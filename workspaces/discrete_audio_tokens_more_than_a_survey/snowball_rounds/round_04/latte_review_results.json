[
  {
    "title": "SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
    "metadata": {
      "title": "SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge",
      "summary": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
      "doi": "https://doi.org/10.1609/aaai.v38i18.29991",
      "openalex_id": "https://openalex.org/W4393160744",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Topic unrelated to 'Discrete Audio Tokens'; the paper discusses combining LLMs with heuristic planning using domain knowledge, not discrete audio token representations or token generation/quantization, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Topic unrelated to 'Discrete Audio Tokens'; the paper discusses combining LLMs with heuristic planning using domain knowledge, not discrete audio token representations or token generation/quantization, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a planning method integrating large language models and heuristic search, focusing on action planning and evaluation in a symbolic domain. There is no indication that the study involves discrete audio tokens, vector quantization, or any form of audio signal processing or modeling related to audio tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a planning method integrating large language models and heuristic search, focusing on action planning and evaluation in a symbolic domain. There is no indication that the study involves discrete audio tokens, vector quantization, or any form of audio signal processing or modeling related to audio tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multimodal deep ensemble classification system with wearable vibration sensor for detecting throat-related events",
    "abstract": "Dysphagia, a swallowing disorder, requires continuous monitoring of throat-related events to obtain comprehensive insights into the patient's pharyngeal and laryngeal functions. However, conventional assessments were performed by medical professionals in clinical settings, limiting persistent monitoring. We demonstrate feasibility of a ubiquitous monitoring system for autonomously detecting throat-related events utilizing a soft skin-attachable throat vibration sensor (STVS). The STVS accurately records throat vibrations without interference from surrounding noise, enabling measurement of subtle sounds such as swallowing. Out of the continuous data stream, we automatically classify events of interest using an ensemble-based deep learning model. The proposed model integrates multiple deep neural networks based on multi-modal acoustic features of throat-related events to enhance robustness and accuracy of classification. The performance of our model outperforms previous studies with a classification accuracy of 95.96%. These results show the potential of wearable solutions for improving dysphagia management and patient outcomes outside of clinical environments.",
    "metadata": {
      "title": "Multimodal deep ensemble classification system with wearable vibration sensor for detecting throat-related events",
      "summary": "Dysphagia, a swallowing disorder, requires continuous monitoring of throat-related events to obtain comprehensive insights into the patient's pharyngeal and laryngeal functions. However, conventional assessments were performed by medical professionals in clinical settings, limiting persistent monitoring. We demonstrate feasibility of a ubiquitous monitoring system for autonomously detecting throat-related events utilizing a soft skin-attachable throat vibration sensor (STVS). The STVS accurately records throat vibrations without interference from surrounding noise, enabling measurement of subtle sounds such as swallowing. Out of the continuous data stream, we automatically classify events of interest using an ensemble-based deep learning model. The proposed model integrates multiple deep neural networks based on multi-modal acoustic features of throat-related events to enhance robustness and accuracy of classification. The performance of our model outperforms previous studies with a classification accuracy of 95.96%. These results show the potential of wearable solutions for improving dysphagia management and patient outcomes outside of clinical environments.",
      "abstract": "Dysphagia, a swallowing disorder, requires continuous monitoring of throat-related events to obtain comprehensive insights into the patient's pharyngeal and laryngeal functions. However, conventional assessments were performed by medical professionals in clinical settings, limiting persistent monitoring. We demonstrate feasibility of a ubiquitous monitoring system for autonomously detecting throat-related events utilizing a soft skin-attachable throat vibration sensor (STVS). The STVS accurately records throat vibrations without interference from surrounding noise, enabling measurement of subtle sounds such as swallowing. Out of the continuous data stream, we automatically classify events of interest using an ensemble-based deep learning model. The proposed model integrates multiple deep neural networks based on multi-modal acoustic features of throat-related events to enhance robustness and accuracy of classification. The performance of our model outperforms previous studies with a classification accuracy of 95.96%. These results show the potential of wearable solutions for improving dysphagia management and patient outcomes outside of clinical environments.",
      "doi": "https://doi.org/10.1038/s41746-024-01417-w",
      "openalex_id": "https://openalex.org/W4406120141",
      "arxiv_id": "",
      "publication_date": "2025-01-07",
      "published": "2025-01-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets a wearable throat vibration sensing system for dysphagia detection with an ensemble deep learning classifier, but it does not address discrete audio tokens or tokenization design (VQ/RVQ/quantization) required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets a wearable throat vibration sensing system for dysphagia detection with an ensemble deep learning classifier, but it does not address discrete audio tokens or tokenization design (VQ/RVQ/quantization) required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting throat-related events using a wearable vibration sensor and multimodal deep learning for classification, but it does not mention generating or using discrete audio tokens derived from vector quantization or related methods, nor does it discuss tokenization processes for audio language modeling or sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting throat-related events using a wearable vibration sensor and multimodal deep learning for classification, but it does not mention generating or using discrete audio tokens derived from vector quantization or related methods, nor does it discuss tokenization processes for audio language modeling or sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "What Do Children and Parents Want and Perceive in Conversational Agents? Towards Transparent, Trustworthy, Democratized Agents",
    "abstract": "Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of \"ideal future agents\" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.",
    "metadata": {
      "title": "What Do Children and Parents Want and Perceive in Conversational Agents? Towards Transparent, Trustworthy, Democratized Agents",
      "summary": "Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of \"ideal future agents\" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.",
      "abstract": "Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of \"ideal future agents\" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.",
      "doi": "https://doi.org/10.1145/3585088.3589353",
      "openalex_id": "https://openalex.org/W4362707416",
      "arxiv_id": "",
      "publication_date": "2023-06-14",
      "published": "2023-06-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies children/parent perspectives on conversational agents and trust, not discrete audio token representations or their generation/quantization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies children/parent perspectives on conversational agents and trust, not discrete audio token representations or their generation/quantization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the perceptions and trust of conversational agents among children and parents, with no mention of discrete audio tokens, vector quantization, token generation, or related technical methods for audio processing and modeling as per the inclusion criteria. Also, it lacks detailed technical content on token formation or audio sequence modeling, thus not meeting the core research focus required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the perceptions and trust of conversational agents among children and parents, with no mention of discrete audio tokens, vector quantization, token generation, or related technical methods for audio processing and modeling as per the inclusion criteria. Also, it lacks detailed technical content on token formation or audio sequence modeling, thus not meeting the core research focus required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "‘All possible sounds’: speech, music, and the emergence of machine listening",
    "abstract": "\"Machine listening\" is one common term for a fast-growing interdisciplinary field of science and engineering that \"uses signal processing and machine learning to extract useful information from sound\". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards \"more fundamental problems of audition\". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader \"auditory scene\". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is \"to cover all possible sounds\". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.",
    "metadata": {
      "title": "‘All possible sounds’: speech, music, and the emergence of machine listening",
      "summary": "\"Machine listening\" is one common term for a fast-growing interdisciplinary field of science and engineering that \"uses signal processing and machine learning to extract useful information from sound\". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards \"more fundamental problems of audition\". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader \"auditory scene\". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is \"to cover all possible sounds\". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.",
      "abstract": "\"Machine listening\" is one common term for a fast-growing interdisciplinary field of science and engineering that \"uses signal processing and machine learning to extract useful information from sound\". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards \"more fundamental problems of audition\". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader \"auditory scene\". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is \"to cover all possible sounds\". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.",
      "doi": "https://doi.org/10.1080/20551940.2023.2195057",
      "openalex_id": "https://openalex.org/W4363676804",
      "arxiv_id": "",
      "publication_date": "2023-04-10",
      "published": "2023-04-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is a historical/critical overview of machine listening and its emergence; it does not present discrete audio token design, token generation, or quantization methods, hence fails the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is a historical/critical overview of machine listening and its emergence; it does not present discrete audio token design, token generation, or quantization methods, hence fails the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses the history and conceptual evolution of machine listening, covering speech, music, and auditory scene analysis broadly without focusing on discrete audio token representation or the technical details of token generation, quantization, or usage within language models or sequence models. It lacks methodological descriptions of token generation processes, vector quantization techniques, or any core contribution related to discrete audio tokens, which are critical for inclusion. Hence, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it describe technical implementations or evaluations required for reproducibility and comparison.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses the history and conceptual evolution of machine listening, covering speech, music, and auditory scene analysis broadly without focusing on discrete audio token representation or the technical details of token generation, quantization, or usage within language models or sequence models. It lacks methodological descriptions of token generation processes, vector quantization techniques, or any core contribution related to discrete audio tokens, which are critical for inclusion. Hence, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it describe technical implementations or evaluations required for reproducibility and comparison.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals",
    "abstract": "Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.",
    "metadata": {
      "title": "Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals",
      "summary": "Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.",
      "abstract": "Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.",
      "doi": "https://doi.org/10.1101/2024.03.11.584533",
      "openalex_id": "https://openalex.org/W4392791724",
      "arxiv_id": "",
      "publication_date": "2024-03-14",
      "published": "2024-03-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural speech decoding from intracranial signals using a transformer-based architecture, which does not address discrete audio token representations/quantization-based token design required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural speech decoding from intracranial signals using a transformer-based architecture, which does not address discrete audio token representations/quantization-based token design required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech decoding from neural signals using a transformer-based model but does not involve the generation or use of discrete audio tokens derived from vector quantization or related quantization mechanisms. There is no mention of tokenization of audio signals for sequence modeling or language modeling, which is essential based on the inclusion criteria. Therefore, it does not meet the criteria of focusing on discrete audio token generation or use as a core method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech decoding from neural signals using a transformer-based model but does not involve the generation or use of discrete audio tokens derived from vector quantization or related quantization mechanisms. There is no mention of tokenization of audio signals for sequence modeling or language modeling, which is essential based on the inclusion criteria. Therefore, it does not meet the criteria of focusing on discrete audio token generation or use as a core method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
    "abstract": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
    "metadata": {
      "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
      "summary": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
      "abstract": "Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1182",
      "openalex_id": "https://openalex.org/W3190032417",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract discusses discrete token discovery via clustering of SSL features and training LM on the resulting units, the item lacks a clear publication/update date, triggering exclusion under the timestamp criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract discusses discrete token discovery via clustering of SSL features and training LM on the resulting units, the item lacks a clear publication/update date, triggering exclusion under the timestamp criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using CPC features and proposes a speaker normalization step to improve acoustic unit discovery through K-means clustering and evaluates a language model trained on these units. However, it does not explicitly mention producing discrete audio tokens through quantization or clustering as a core method aimed at sequence modeling or generative tasks, nor does it detail the token generation process or verify usage of tokens suitable for language model input, which are necessary inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using CPC features and proposes a speaker normalization step to improve acoustic unit discovery through K-means clustering and evaluates a language model trained on these units. However, it does not explicitly mention producing discrete audio tokens through quantization or clustering as a core method aimed at sequence modeling or generative tasks, nor does it detail the token generation process or verify usage of tokens suitable for language model input, which are necessary inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Self-Supervised Inference in State-Space Models",
    "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.",
    "metadata": {
      "title": "Self-Supervised Inference in State-Space Models",
      "summary": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.",
      "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.",
      "doi": "https://doi.org/10.48550/arxiv.2107.13349",
      "openalex_id": "https://openalex.org/W3201728305",
      "arxiv_id": "",
      "publication_date": "2021-07-28",
      "published": "2021-07-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於狀態空間模型的自監督推理與非參數化的 Bayes 更新，未涉及離散音訊 token 的設計、量化或作為語言模型/序列模型的離散表徵，因此不符合主題與方法學的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於狀態空間模型的自監督推理與非參數化的 Bayes 更新，未涉及離散音訊 token 的設計、量化或作為語言模型/序列模型的離散表徵，因此不符合主題與方法學的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on approximate inference in nonlinear state-space models and does not involve discrete audio tokens or their generation through vector quantization or related techniques; it also lacks detailed description of token generation methods linked to audio language modeling or semantic/acoustic disentanglement as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on approximate inference in nonlinear state-space models and does not involve discrete audio tokens or their generation through vector quantization or related techniques; it also lacks detailed description of token generation methods linked to audio language modeling or semantic/acoustic disentanglement as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How BPE Affects Memorization in Transformers",
    "abstract": "Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.",
    "metadata": {
      "title": "How BPE Affects Memorization in Transformers",
      "summary": "Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.",
      "abstract": "Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02782",
      "openalex_id": "https://openalex.org/W3204516855",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria target discrete audio tokens as a core research contribution, but this item studies BPE subword vocabulary and memorization in Transformer-based NLP models, which is text-based rather than discrete audio token representations. Therefore it does not meet the scope and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria target discrete audio tokens as a core research contribution, but this item studies BPE subword vocabulary and memorization in Transformer-based NLP models, which is text-based rather than discrete audio token representations. Therefore it does not meet the scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the effects of Byte-Pair Encoding (BPE) on memorization in Transformer models within NLP, specifically addressing subword vocabulary size and model behavior—it does not involve discrete audio tokens, neural audio codecs, or related audio tokenization methods as defined in the inclusion criteria. Therefore, it does not meet the core thematic requirements and focuses solely on text-based NLP modeling, which leads to exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the effects of Byte-Pair Encoding (BPE) on memorization in Transformer models within NLP, specifically addressing subword vocabulary size and model behavior—it does not involve discrete audio tokens, neural audio codecs, or related audio tokenization methods as defined in the inclusion criteria. Therefore, it does not meet the core thematic requirements and focuses solely on text-based NLP modeling, which leads to exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units",
    "abstract": "Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.",
    "metadata": {
      "title": "Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units",
      "summary": "Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.",
      "abstract": "Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.",
      "doi": "https://doi.org/10.48550/arxiv.2111.00610",
      "openalex_id": "https://openalex.org/W3210088932",
      "arxiv_id": "",
      "publication_date": "2021-10-31",
      "published": "2021-10-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item uses sub-word linguistic units (syllables/phonemes) for speech-domain language modeling but does not describe discrete audio token generation via quantization (VQ/RVQ, hierarchical quantization) or provide the requisite token-level design/process details, so it does not meet the core inclusion criteria for discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item uses sub-word linguistic units (syllables/phonemes) for speech-domain language modeling but does not describe discrete audio token generation via quantization (VQ/RVQ, hierarchical quantization) or provide the requisite token-level design/process details, so it does not meet the core inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes an LSTM-based generative speech language model using sub-word linguistic units such as syllables and phonemes to improve acoustic consistency, but it does not describe the use or design of discrete audio tokens generated by vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization methods. Also, there is no mention of token generation processes, quantization methods, codebook or hierarchical structures, or how discrete tokens are explicitly utilized for sequence modeling, which are necessary per inclusion criteria. Thus, while related to speech language modeling, it does not meet the core requirement of discrete audio tokens as defined, nor does it clarify token formation or quantization techniques required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes an LSTM-based generative speech language model using sub-word linguistic units such as syllables and phonemes to improve acoustic consistency, but it does not describe the use or design of discrete audio tokens generated by vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization methods. Also, there is no mention of token generation processes, quantization methods, codebook or hierarchical structures, or how discrete tokens are explicitly utilized for sequence modeling, which are necessary per inclusion criteria. Thus, while related to speech language modeling, it does not meet the core requirement of discrete audio tokens as defined, nor does it clarify token formation or quantization techniques required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Recent Advances in Synthesis and Interaction of Speech, Text, and Vision",
    "abstract": "In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.",
    "metadata": {
      "title": "Recent Advances in Synthesis and Interaction of Speech, Text, and Vision",
      "summary": "In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.",
      "abstract": "In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.",
      "doi": "https://doi.org/10.3390/electronics13091726",
      "openalex_id": "https://openalex.org/W4396510248",
      "arxiv_id": "",
      "publication_date": "2024-04-30",
      "published": "2024-04-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is a broad survey of image-to-audio and speech synthesis but does not focus on discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization) or provide the tokenization methodology required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is a broad survey of image-to-audio and speech synthesis but does not focus on discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization) or provide the tokenization methodology required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on recent advances in synthesizing speech from text and images and converting images into audio descriptions, involving speech synthesis and cross-modal interactions, but do not specifically mention discrete audio tokens, vector quantization, or token generation methods necessary for the inclusion criteria. The study is a review covering broad techniques rather than focusing on discrete token design as core contributions, nor detailing token generation, quantization processes, or evaluation of discrete audio tokens. Therefore, it does not meet the inclusion criteria focused strictly on discrete audio token methodologies and representation for language or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on recent advances in synthesizing speech from text and images and converting images into audio descriptions, involving speech synthesis and cross-modal interactions, but do not specifically mention discrete audio tokens, vector quantization, or token generation methods necessary for the inclusion criteria. The study is a review covering broad techniques rather than focusing on discrete token design as core contributions, nor detailing token generation, quantization processes, or evaluation of discrete audio tokens. Therefore, it does not meet the inclusion criteria focused strictly on discrete audio token methodologies and representation for language or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Common brain activity features discretization for predicting perceived speech quality",
    "abstract": "The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.",
    "metadata": {
      "title": "Common brain activity features discretization for predicting perceived speech quality",
      "summary": "The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.",
      "abstract": "The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.",
      "doi": "https://doi.org/10.1016/j.procs.2022.12.195",
      "openalex_id": "https://openalex.org/W4319997228",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discretizing brain activity features to predict perceived speech quality, not on discrete audio tokens or tokenization methods (VQ/RVQ, codebooks, SSL-based token generation) required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discretizing brain activity features to predict perceived speech quality, not on discrete audio tokens or tokenization methods (VQ/RVQ, codebooks, SSL-based token generation) required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting perceived speech quality based on common brain activity features during listening, without discussing any method to generate discrete audio tokens or quantization methods for audio signal representation, nor does it mention usage of such tokens for sequence or language modeling. The abstract lacks information on token generation processes, vector quantization, or any discrete token design as required by the inclusion criteria. Hence, it does not align with the core requirements regarding discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting perceived speech quality based on common brain activity features during listening, without discussing any method to generate discrete audio tokens or quantization methods for audio signal representation, nor does it mention usage of such tokens for sequence or language modeling. The abstract lacks information on token generation processes, vector quantization, or any discrete token design as required by the inclusion criteria. Hence, it does not align with the core requirements regarding discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Penerapan Optical Character Recognition (OCR) Dengan Text-To-Speech (TTS) dalam Konversi Gambar ke Suara",
    "abstract": "Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning",
    "metadata": {
      "title": "Penerapan Optical Character Recognition (OCR) Dengan Text-To-Speech (TTS) dalam Konversi Gambar ke Suara",
      "summary": "Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning",
      "abstract": "Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning",
      "doi": "https://doi.org/10.24036/voteteknika.v11i4.125218",
      "openalex_id": "https://openalex.org/W4393389449",
      "arxiv_id": "",
      "publication_date": "2023-12-02",
      "published": "2023-12-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on OCR plus VQ-VAE and gTTS for image-to-speech conversion to aid accessibility, without outlining discrete audio token design or tokenization methods (the inclusion criteria require discrete audio token contributions and explicit token generation/quantization details). Additionally, there is no clear publication/update date provided, which triggers an exclusion criterion. Therefore it does not meet ALL inclusion criteria and fulfills at least one exclusion criterion, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on OCR plus VQ-VAE and gTTS for image-to-speech conversion to aid accessibility, without outlining discrete audio token design or tokenization methods (the inclusion criteria require discrete audio token contributions and explicit token generation/quantization details). Additionally, there is no clear publication/update date provided, which triggers an exclusion criterion. Therefore it does not meet ALL inclusion criteria and fulfills at least one exclusion criterion, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study combines Optical Character Recognition (OCR) with a Text-to-Speech (TTS) system, incorporating a Vector Quantized Variational Autoencoder (VQ-VAE) for voice conversion, which suggests the use of vector quantization for discrete audio token representation. However, the abstract does not explicitly describe the discrete audio token generation process, token structure, or how the tokens are used in sequence or language models, which are key inclusion criteria. The study focuses on accessibility and the quality of voice conversion, without detailed method descriptions or experimental validation specifically targeting discrete audio tokens as defined. Therefore, it partially aligns with the theme but lacks sufficient methodological detail for full inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study combines Optical Character Recognition (OCR) with a Text-to-Speech (TTS) system, incorporating a Vector Quantized Variational Autoencoder (VQ-VAE) for voice conversion, which suggests the use of vector quantization for discrete audio token representation. However, the abstract does not explicitly describe the discrete audio token generation process, token structure, or how the tokens are used in sequence or language models, which are key inclusion criteria. The study focuses on accessibility and the quality of voice conversion, without detailed method descriptions or experimental validation specifically targeting discrete audio tokens as defined. Therefore, it partially aligns with the theme but lacks sufficient methodological detail for full inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted this work uses VQ‑VAE and gTTS for OCR-to-speech accessibility but the abstract lacks any clear description of discrete audio token design, token generation/quantization procedures, codebook/layer details, or how tokens are used by sequence/language models (required inclusion elements); additionally, no verifiable publication date or sufficient methodological detail is provided, which meets the exclusion criteria. Therefore this item should be excluded. One-sentence explanation: Exclude (1) — the paper does not provide the required discrete-token design/quantization details or reproducible methodological information and also lacks a clear publication timestamp.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted this work uses VQ‑VAE and gTTS for OCR-to-speech accessibility but the abstract lacks any clear description of discrete audio token design, token generation/quantization procedures, codebook/layer details, or how tokens are used by sequence/language models (required inclusion elements); additionally, no verifiable publication date or sufficient methodological detail is provided, which meets the exclusion criteria. Therefore this item should be excluded. One-sentence explanation: Exclude (1) — the paper does not provide the required discrete-token design/quantization details or reproducible methodological information and also lacks a clear publication timestamp.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Double Articulation Analyzer with Prosody for Unsupervised Word and Phoneme Discovery",
    "abstract": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
    "metadata": {
      "title": "Double Articulation Analyzer with Prosody for Unsupervised Word and Phoneme Discovery",
      "summary": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "abstract": "Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.",
      "doi": "https://doi.org/10.48550/arxiv.2103.08199",
      "openalex_id": "https://openalex.org/W3136576491",
      "arxiv_id": "",
      "publication_date": "2021-03-15",
      "published": "2021-03-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper develops a probabilistic model for unsupervised word/phoneme discovery using prosody, but it does not present discrete audio token generation/quantization (e.g., VQ/RVQ, token vocab, or explicit discretized token representation) as its main contribution, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper develops a probabilistic model for unsupervised word/phoneme discovery using prosody, but it does not present discrete audio token generation/quantization (e.g., VQ/RVQ, token vocab, or explicit discretized token representation) as its main contribution, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a probabilistic generative model for discovering words and phonemes in an unsupervised manner from continuous speech, focusing on prosodic and distributional cues for word segmentation. However, it does not explicitly discuss or describe a method that produces discrete audio tokens suitable for use in language or sequence models, such as vector quantization or semantic-driven discretization mechanisms. The focus is on word and phoneme discovery rather than the generation of discrete audio tokens for downstream modeling tasks, and the token generation process is not clearly described as per the inclusion criteria, nor does it address the design or evaluation of discrete tokens for speech representation and modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a probabilistic generative model for discovering words and phonemes in an unsupervised manner from continuous speech, focusing on prosodic and distributional cues for word segmentation. However, it does not explicitly discuss or describe a method that produces discrete audio tokens suitable for use in language or sequence models, such as vector quantization or semantic-driven discretization mechanisms. The focus is on word and phoneme discovery rather than the generation of discrete audio tokens for downstream modeling tasks, and the token generation process is not clearly described as per the inclusion criteria, nor does it address the design or evaluation of discrete tokens for speech representation and modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Non-Intrusive Binaural Speech Intelligibility Prediction From Discrete Latent Representations",
    "abstract": "Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.",
    "metadata": {
      "title": "Non-Intrusive Binaural Speech Intelligibility Prediction From Discrete Latent Representations",
      "summary": "Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.",
      "abstract": "Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.",
      "doi": "https://doi.org/10.1109/lsp.2022.3161115",
      "openalex_id": "https://openalex.org/W3217035088",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study leverages discrete latent representations via vector quantization (VQ) combined with CPC (VQ-CPC) to form features for binaural speech intelligibility prediction, aligning with the discrete audio tokens/discrete representations theme; it presents methodological details and evaluation against benchmarks rather than mere demo, indicating it contributes a discrete-token-like approach suitable for inclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The study leverages discrete latent representations via vector quantization (VQ) combined with CPC (VQ-CPC) to form features for binaural speech intelligibility prediction, aligning with the discrete audio tokens/discrete representations theme; it presents methodological details and evaluation against benchmarks rather than mere demo, indicating it contributes a discrete-token-like approach suitable for inclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a non-intrusive speech intelligibility prediction method using vector quantization (VQ) and contrastive predictive coding (CPC) features extracted from binaural input signals. While VQ is employed to generate discrete latent representations, the primary focus is SI prediction rather than designing discrete audio tokens as core representations for audio language modeling or downstream generation/understanding tasks. The paper lacks details on token generation processes, codebook design, semantic disentanglement, or usage of discrete tokens specifically for sequence model input. Thus, it only partially matches the inclusion criteria and does not satisfy the requirement of using discrete tokens as a principal contribution for sequence models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a non-intrusive speech intelligibility prediction method using vector quantization (VQ) and contrastive predictive coding (CPC) features extracted from binaural input signals. While VQ is employed to generate discrete latent representations, the primary focus is SI prediction rather than designing discrete audio tokens as core representations for audio language modeling or downstream generation/understanding tasks. The paper lacks details on token generation processes, codebook design, semantic disentanglement, or usage of discrete tokens specifically for sequence model input. Thus, it only partially matches the inclusion criteria and does not satisfy the requirement of using discrete tokens as a principal contribution for sequence models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the use of VQ-based discrete latent representations (VQ-CPC) but disagreed on inclusion: JuniorNano leaned to include, citing the discrete representation aspect; JuniorMini recommended exclusion, noting the work focuses on SI prediction rather than designing discrete tokens for sequence/language modeling and lacks token-generation details. My assessment: although the paper uses vector quantization to produce discrete-like features, its core contribution is a speech-intelligibility predictor, not the design, documentation, or intended use of discrete audio tokens for sequence/audio-language modeling (no clear codebook/tokenization pipeline, hierarchical/RVQ design, or downstream sequence-model use is described), so it does not meet the inclusion requirements. Score explanation (1-sentence): Because the VQ usage is incidental to an SI prediction task and the paper lacks token-generation details and positioning of those discrete features as tokens for sequence models, it should be excluded (better to exclude).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the use of VQ-based discrete latent representations (VQ-CPC) but disagreed on inclusion: JuniorNano leaned to include, citing the discrete representation aspect; JuniorMini recommended exclusion, noting the work focuses on SI prediction rather than designing discrete tokens for sequence/language modeling and lacks token-generation details. My assessment: although the paper uses vector quantization to produce discrete-like features, its core contribution is a speech-intelligibility predictor, not the design, documentation, or intended use of discrete audio tokens for sequence/audio-language modeling (no clear codebook/tokenization pipeline, hierarchical/RVQ design, or downstream sequence-model use is described), so it does not meet the inclusion requirements. Score explanation (1-sentence): Because the VQ usage is incidental to an SI prediction task and the paper lacks token-generation details and positioning of those discrete features as tokens for sequence models, it should be excluded (better to exclude).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Whole brain Probabilistic Generative Model toward Realizing Cognitive Architecture for Developmental Robots",
    "abstract": "Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.",
    "metadata": {
      "title": "Whole brain Probabilistic Generative Model toward Realizing Cognitive Architecture for Developmental Robots",
      "summary": "Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.",
      "abstract": "Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3138466558",
      "arxiv_id": "",
      "publication_date": "2021-03-15",
      "published": "2021-03-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a whole-brain probabilistic generative model for cognitive architecture in developmental robots, with no discussion of discrete audio tokens or tokenization methods, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a whole-brain probabilistic generative model for cognitive architecture in developmental robots, with no discussion of discrete audio tokens or tokenization methods, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on developing a whole-brain probabilistic generative model for cognitive architecture in developmental robots, which is related to artificial cognitive systems and not to discrete audio tokens or tokenization of continuous audio signals as specified in the inclusion criteria. It lacks discussion on discrete audio token generation processes, vector quantization, or any method to transform continuous audio into discrete tokens for language or sequence modeling. Therefore, it does not meet the inclusion criteria around audio tokenization and does not provide relevant technical details about audio token representations as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on developing a whole-brain probabilistic generative model for cognitive architecture in developmental robots, which is related to artificial cognitive systems and not to discrete audio tokens or tokenization of continuous audio signals as specified in the inclusion criteria. It lacks discussion on discrete audio token generation processes, vector quantization, or any method to transform continuous audio into discrete tokens for language or sequence modeling. Therefore, it does not meet the inclusion criteria around audio tokenization and does not provide relevant technical details about audio token representations as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder",
    "abstract": "In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.",
    "metadata": {
      "title": "crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder",
      "summary": "In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.",
      "abstract": "In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.",
      "doi": "https://doi.org/10.48550/arxiv.2103.02858",
      "openalex_id": "https://openalex.org/W3134286091",
      "arxiv_id": "",
      "publication_date": "2021-03-04",
      "published": "2021-03-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on an open-source nonparallel voice conversion system using vector-quantized VAE, not primarily on designing or detailing discrete audio tokens as a representation for language/modeling, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on an open-source nonparallel voice conversion system using vector-quantized VAE, not primarily on designing or detailing discrete audio tokens as a representation for language/modeling, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an open-source software 'crank' implementing voice conversion based on Vector-Quantized Variational Autoencoder (VQVAE), which involves vector quantization to generate discrete audio tokens for modeling nonparallel voice conversion without requiring parallel data. The methodology clearly details the use of vector quantization and the generation of discrete latent representations (tokens) used for sequence modeling in voice conversion, aligning with the key inclusion criterion of discrete audio token design and usage. The abstract also describes incorporation of recent autoencoder-based VC techniques and objective evaluations, indicating enough technical detail to assess the method. Thus, it meets the inclusion criteria of focusing on discrete audio token design with explicit token generation and use for sequence modeling in audio. No exclusion criteria appear to be met as the study provides detailed method descriptions with measurable evaluations rather than a mere demo or codec for transmission purposes. The availability of an open-source tool and comparisons suggests reproducibility, supporting inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents an open-source software 'crank' implementing voice conversion based on Vector-Quantized Variational Autoencoder (VQVAE), which involves vector quantization to generate discrete audio tokens for modeling nonparallel voice conversion without requiring parallel data. The methodology clearly details the use of vector quantization and the generation of discrete latent representations (tokens) used for sequence modeling in voice conversion, aligning with the key inclusion criterion of discrete audio token design and usage. The abstract also describes incorporation of recent autoencoder-based VC techniques and objective evaluations, indicating enough technical detail to assess the method. Thus, it meets the inclusion criteria of focusing on discrete audio token design with explicit token generation and use for sequence modeling in audio. No exclusion criteria appear to be met as the study provides detailed method descriptions with measurable evaluations rather than a mere demo or codec for transmission purposes. The availability of an open-source tool and comparisons suggests reproducibility, supporting inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagreed: JuniorNano recommended exclusion (not focused on discrete audio tokens), while JuniorMini recommended inclusion (points to VQVAE-based vector quantization). After review, I agree with JuniorNano — the paper presents an open-source VQVAE-based voice conversion toolkit whose main contribution is nonparallel VC software rather than a clear, detailed proposal of discrete audio token design or their use for sequence/language modeling; the abstract lacks sufficient technical detail about the tokenization/quantization scheme, codebook/layering, or use of tokens in audio language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagreed: JuniorNano recommended exclusion (not focused on discrete audio tokens), while JuniorMini recommended inclusion (points to VQVAE-based vector quantization). After review, I agree with JuniorNano — the paper presents an open-source VQVAE-based voice conversion toolkit whose main contribution is nonparallel VC software rather than a clear, detailed proposal of discrete audio token design or their use for sequence/language modeling; the abstract lacks sufficient technical detail about the tokenization/quantization scheme, codebook/layering, or use of tokens in audio language modeling, so it does not meet the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words",
    "abstract": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
    "metadata": {
      "title": "Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words",
      "summary": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
      "abstract": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746102",
      "openalex_id": "https://openalex.org/W3209993061",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": " The abstract focuses on CPC-based self-supervised phoneme/word segmentation and categorization without discussing discrete audio tokens, vector quantization, or token-level representations for downstream language-model-like processing, so it does not meet the discrete audio token criterion. ",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": " The abstract focuses on CPC-based self-supervised phoneme/word segmentation and categorization without discussing discrete audio tokens, vector quantization, or token-level representations for downstream language-model-like processing, so it does not meet the discrete audio token criterion. ",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on self-supervised learning methods for phoneme categorization and segmentation but does not clearly describe generating discrete audio tokens via vector quantization or similar methods for use in sequence or language modeling, nor does it detail token formation processes or quantization, which are key inclusion criteria. Hence, it does not sufficiently address the core theme of discrete audio token design and usage required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on self-supervised learning methods for phoneme categorization and segmentation but does not clearly describe generating discrete audio tokens via vector quantization or similar methods for use in sequence or language modeling, nor does it detail token formation processes or quantization, which are key inclusion criteria. Hence, it does not sufficiently address the core theme of discrete audio token design and usage required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Paralinguistic singing attribute recognition using supervised machine learning for describing the classical tenor solo singing voice in vocal pedagogy",
    "abstract": "Abstract Humans can recognize someone’s identity through their voice and describe the timbral phenomena of voices. Likewise, the singing voice also has timbral phenomena. In vocal pedagogy, vocal teachers listen and then describe the timbral phenomena of their student’s singing voice. In this study, in order to enable machines to describe the singing voice from the vocal pedagogy point of view, we perform a task called paralinguistic singing attribute recognition. To achieve this goal, we first construct and publish an open source dataset named Singing Voice Quality and Technique Database (SVQTD) for supervised learning. All the audio clips in SVQTD are downloaded from YouTube and processed by music source separation and silence detection. For annotation, seven paralinguistic singing attributes commonly used in vocal pedagogy are adopted as the labels. Furthermore, to explore the different supervised machine learning algorithm for classifying each paralinguistic singing attribute, we adopt three main frameworks, namely openSMILE features with support vector machine (SF-SVM), end-to-end deep learning (E2EDL), and deep embedding with support vector machine (DE-SVM). Our methods are based on existing frameworks commonly employed in other paralinguistic speech attribute recognition tasks. In SF-SVM, we separately use the feature set of the INTERSPEECH 2009 Challenge and that of the INTERSPEECH 2016 Challenge as the SVM classifier’s input. In E2EDL, the end-to-end framework separately utilizes the ResNet and transformer encoder as feature extractors. In particular, to handle two-dimensional spectrogram input for a transformer, we adopt a sliced multi-head self-attention (SMSA) mechanism. In the DE-SVM, we use the representation extracted from the E2EDL model as the input of the SVM classifier. Experimental results on SVQTD show no absolute winner between E2EDL and the DE-SVM, which means that the back-end SVM classifier with the representation learned by E2E as input does not necessarily improve the performance. However, the DE-SVM that utilizes the ResNet as the feature extractor achieves the best average UAR, with an average 16% improvement over that of the SF-SVM with INTERSPEECH’s hand-crafted feature set.",
    "metadata": {
      "title": "Paralinguistic singing attribute recognition using supervised machine learning for describing the classical tenor solo singing voice in vocal pedagogy",
      "summary": "Abstract Humans can recognize someone’s identity through their voice and describe the timbral phenomena of voices. Likewise, the singing voice also has timbral phenomena. In vocal pedagogy, vocal teachers listen and then describe the timbral phenomena of their student’s singing voice. In this study, in order to enable machines to describe the singing voice from the vocal pedagogy point of view, we perform a task called paralinguistic singing attribute recognition. To achieve this goal, we first construct and publish an open source dataset named Singing Voice Quality and Technique Database (SVQTD) for supervised learning. All the audio clips in SVQTD are downloaded from YouTube and processed by music source separation and silence detection. For annotation, seven paralinguistic singing attributes commonly used in vocal pedagogy are adopted as the labels. Furthermore, to explore the different supervised machine learning algorithm for classifying each paralinguistic singing attribute, we adopt three main frameworks, namely openSMILE features with support vector machine (SF-SVM), end-to-end deep learning (E2EDL), and deep embedding with support vector machine (DE-SVM). Our methods are based on existing frameworks commonly employed in other paralinguistic speech attribute recognition tasks. In SF-SVM, we separately use the feature set of the INTERSPEECH 2009 Challenge and that of the INTERSPEECH 2016 Challenge as the SVM classifier’s input. In E2EDL, the end-to-end framework separately utilizes the ResNet and transformer encoder as feature extractors. In particular, to handle two-dimensional spectrogram input for a transformer, we adopt a sliced multi-head self-attention (SMSA) mechanism. In the DE-SVM, we use the representation extracted from the E2EDL model as the input of the SVM classifier. Experimental results on SVQTD show no absolute winner between E2EDL and the DE-SVM, which means that the back-end SVM classifier with the representation learned by E2E as input does not necessarily improve the performance. However, the DE-SVM that utilizes the ResNet as the feature extractor achieves the best average UAR, with an average 16% improvement over that of the SF-SVM with INTERSPEECH’s hand-crafted feature set.",
      "abstract": "Abstract Humans can recognize someone’s identity through their voice and describe the timbral phenomena of voices. Likewise, the singing voice also has timbral phenomena. In vocal pedagogy, vocal teachers listen and then describe the timbral phenomena of their student’s singing voice. In this study, in order to enable machines to describe the singing voice from the vocal pedagogy point of view, we perform a task called paralinguistic singing attribute recognition. To achieve this goal, we first construct and publish an open source dataset named Singing Voice Quality and Technique Database (SVQTD) for supervised learning. All the audio clips in SVQTD are downloaded from YouTube and processed by music source separation and silence detection. For annotation, seven paralinguistic singing attributes commonly used in vocal pedagogy are adopted as the labels. Furthermore, to explore the different supervised machine learning algorithm for classifying each paralinguistic singing attribute, we adopt three main frameworks, namely openSMILE features with support vector machine (SF-SVM), end-to-end deep learning (E2EDL), and deep embedding with support vector machine (DE-SVM). Our methods are based on existing frameworks commonly employed in other paralinguistic speech attribute recognition tasks. In SF-SVM, we separately use the feature set of the INTERSPEECH 2009 Challenge and that of the INTERSPEECH 2016 Challenge as the SVM classifier’s input. In E2EDL, the end-to-end framework separately utilizes the ResNet and transformer encoder as feature extractors. In particular, to handle two-dimensional spectrogram input for a transformer, we adopt a sliced multi-head self-attention (SMSA) mechanism. In the DE-SVM, we use the representation extracted from the E2EDL model as the input of the SVM classifier. Experimental results on SVQTD show no absolute winner between E2EDL and the DE-SVM, which means that the back-end SVM classifier with the representation learned by E2E as input does not necessarily improve the performance. However, the DE-SVM that utilizes the ResNet as the feature extractor achieves the best average UAR, with an average 16% improvement over that of the SF-SVM with INTERSPEECH’s hand-crafted feature set.",
      "doi": "https://doi.org/10.1186/s13636-022-00240-z",
      "openalex_id": "https://openalex.org/W4223988985",
      "arxiv_id": "",
      "publication_date": "2022-04-15",
      "published": "2022-04-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about supervised ML for paralinguistic singing attribute recognition and dataset construction, with no discussion of discrete audio tokens or tokenization/quantization methods, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about supervised ML for paralinguistic singing attribute recognition and dataset construction, with no discussion of discrete audio tokens or tokenization/quantization methods, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on supervised machine learning methods to recognize paralinguistic singing attributes from continuous audio signals but does not describe any methodology or contribution related to generating discrete audio tokens through vector quantization or similar discrete representation techniques, which is central to the inclusion criteria. It lacks details on token generation, quantization, or usage of discrete tokens for sequence modeling, and does not aim to produce discrete token representations for downstream generation or understanding tasks as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on supervised machine learning methods to recognize paralinguistic singing attributes from continuous audio signals but does not describe any methodology or contribution related to generating discrete audio tokens through vector quantization or similar discrete representation techniques, which is central to the inclusion criteria. It lacks details on token generation, quantization, or usage of discrete tokens for sequence modeling, and does not aim to produce discrete token representations for downstream generation or understanding tasks as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "18. Artificial Intelligence and the Symphony Orchestra",
    "abstract": "This chapter documents a process of practice-based research concerning the relationship between artificial intelligence and classical music. I argue that classical music (as an industry) is well placed o answer salient questions that the age of artificial intelligence demands we consider. The relationship is explored through three themes, which are: 1. The relationship between the future and the past 2. The idea of “authenticity” 3. The notion of music as an abstract artform that can or cannot be reduced to data alone Using these thematic areas as a bedrock, the case study will discuss the three- movement work Silicon, written for the BBC Philharmonic Orchestra. Drawing from my own practice and views the case study will explore how this new technology affects, and will affect, the way an orchestra interacts with a composer, and how orchestral music can be used to explore technology that has an increasingly profound effect on all aspects of our day-to-day lives. Alongside theoretical and aesthetic ties between classical music and artificial intelligence, practical methodologies for utilising artificial intelligence as part of the creative process will be explored. This includes the benefits and limitations of using artificial intelligence to create or develop musical material, long-term structures, or novel synthesized instruments, and also some compositional methodologies I have developed to magnify or mitigate the effect of artificial intelligence on a large-scale work.",
    "metadata": {
      "title": "18. Artificial Intelligence and the Symphony Orchestra",
      "summary": "This chapter documents a process of practice-based research concerning the relationship between artificial intelligence and classical music. I argue that classical music (as an industry) is well placed o answer salient questions that the age of artificial intelligence demands we consider. The relationship is explored through three themes, which are: 1. The relationship between the future and the past 2. The idea of “authenticity” 3. The notion of music as an abstract artform that can or cannot be reduced to data alone Using these thematic areas as a bedrock, the case study will discuss the three- movement work Silicon, written for the BBC Philharmonic Orchestra. Drawing from my own practice and views the case study will explore how this new technology affects, and will affect, the way an orchestra interacts with a composer, and how orchestral music can be used to explore technology that has an increasingly profound effect on all aspects of our day-to-day lives. Alongside theoretical and aesthetic ties between classical music and artificial intelligence, practical methodologies for utilising artificial intelligence as part of the creative process will be explored. This includes the benefits and limitations of using artificial intelligence to create or develop musical material, long-term structures, or novel synthesized instruments, and also some compositional methodologies I have developed to magnify or mitigate the effect of artificial intelligence on a large-scale work.",
      "abstract": "This chapter documents a process of practice-based research concerning the relationship between artificial intelligence and classical music. I argue that classical music (as an industry) is well placed o answer salient questions that the age of artificial intelligence demands we consider. The relationship is explored through three themes, which are: 1. The relationship between the future and the past 2. The idea of “authenticity” 3. The notion of music as an abstract artform that can or cannot be reduced to data alone Using these thematic areas as a bedrock, the case study will discuss the three- movement work Silicon, written for the BBC Philharmonic Orchestra. Drawing from my own practice and views the case study will explore how this new technology affects, and will affect, the way an orchestra interacts with a composer, and how orchestral music can be used to explore technology that has an increasingly profound effect on all aspects of our day-to-day lives. Alongside theoretical and aesthetic ties between classical music and artificial intelligence, practical methodologies for utilising artificial intelligence as part of the creative process will be explored. This includes the benefits and limitations of using artificial intelligence to create or develop musical material, long-term structures, or novel synthesized instruments, and also some compositional methodologies I have developed to magnify or mitigate the effect of artificial intelligence on a large-scale work.",
      "doi": "https://doi.org/10.11647/obp.0353.18",
      "openalex_id": "https://openalex.org/W4391350578",
      "arxiv_id": "",
      "publication_date": "2024-01-30",
      "published": "2024-01-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss AI in classical music practice and case study music composition, but do not address discrete audio tokens, token generation/quantization, or audio-language modeling, so they do not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss AI in classical music practice and case study music composition, but do not address discrete audio tokens, token generation/quantization, or audio-language modeling, so they do not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the use of artificial intelligence in the context of classical music composition and orchestral interaction, centering on thematic and aesthetic considerations rather than the technical development or use of discrete audio tokens, vector quantization methods, or sequence modeling relevant to Audio Language Modeling. It lacks specific methodological details on token generation, quantization, or their application in language or sequence models, which are core to the inclusion criteria, and does not provide technical descriptions or quantitative analyses of audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the use of artificial intelligence in the context of classical music composition and orchestral interaction, centering on thematic and aesthetic considerations rather than the technical development or use of discrete audio tokens, vector quantization methods, or sequence modeling relevant to Audio Language Modeling. It lacks specific methodological details on token generation, quantization, or their application in language or sequence models, which are core to the inclusion criteria, and does not provide technical descriptions or quantitative analyses of audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview",
    "abstract": "We present a structured overview of adaptation algorithms for neural\\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\\nneural network systems and end-to-end neural network systems, with a focus on\\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\\ncharacterizes adaptation algorithms as based on embeddings, model parameter\\nadaptation, or data augmentation. We present a meta-analysis of the performance\\nof speech recognition adaptation algorithms, based on relative error rate\\nreductions as reported in the literature.\\n",
    "metadata": {
      "title": "Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview",
      "summary": "We present a structured overview of adaptation algorithms for neural\\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\\nneural network systems and end-to-end neural network systems, with a focus on\\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\\ncharacterizes adaptation algorithms as based on embeddings, model parameter\\nadaptation, or data augmentation. We present a meta-analysis of the performance\\nof speech recognition adaptation algorithms, based on relative error rate\\nreductions as reported in the literature.\\n",
      "abstract": "We present a structured overview of adaptation algorithms for neural\\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\\nneural network systems and end-to-end neural network systems, with a focus on\\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\\ncharacterizes adaptation algorithms as based on embeddings, model parameter\\nadaptation, or data augmentation. We present a meta-analysis of the performance\\nof speech recognition adaptation algorithms, based on relative error rate\\nreductions as reported in the literature.\\n",
      "doi": "https://doi.org/10.1109/ojsp.2020.3045349",
      "openalex_id": "https://openalex.org/W3112702554",
      "arxiv_id": "",
      "publication_date": "2020-12-17",
      "published": "2020-12-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses adaptation algorithms for neural network-based speech recognition and does not address discrete audio tokens or tokenization workflows; thus does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses adaptation algorithms for neural network-based speech recognition and does not address discrete audio tokens or tokenization workflows; thus does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper provides an overview of adaptation algorithms for neural network-based speech recognition and does not focus on the generation or use of discrete audio tokens through vector quantization or related discretization mechanisms, nor does it detail token generation processes or usages in sequence or language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper provides an overview of adaptation algorithms for neural network-based speech recognition and does not focus on the generation or use of discrete audio tokens through vector quantization or related discretization mechanisms, nor does it detail token generation processes or usages in sequence or language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transforming English language learning: Advanced speech recognition with MLP-LSTM for personalized education",
    "abstract": "Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.",
    "metadata": {
      "title": "Transforming English language learning: Advanced speech recognition with MLP-LSTM for personalized education",
      "summary": "Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.",
      "abstract": "Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.",
      "doi": "https://doi.org/10.1016/j.aej.2024.10.065",
      "openalex_id": "https://openalex.org/W4403558103",
      "arxiv_id": "",
      "publication_date": "2024-10-19",
      "published": "2024-10-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses an MLP-LSTM based speech recognition system and performance metrics but does not address discrete audio tokens or tokenization methods as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses an MLP-LSTM based speech recognition system and performance metrics but does not address discrete audio tokens or tokenization methods as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on English language learning using an advanced speech recognition model MLP-LSTM, but does not mention any discrete audio token representation, vector quantization, or a token generation process that interfaces with language or sequence models as defined in the inclusion criteria. The primary contribution is speech recognition accuracy improvement rather than discrete token-based audio representation, and there is no clear explanation of token formation or usage for sequence modeling. Thus, it does not meet the inclusion criteria related to discrete audio tokens nor does it detail methodology as required, leading to exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on English language learning using an advanced speech recognition model MLP-LSTM, but does not mention any discrete audio token representation, vector quantization, or a token generation process that interfaces with language or sequence models as defined in the inclusion criteria. The primary contribution is speech recognition accuracy improvement rather than discrete token-based audio representation, and there is no clear explanation of token formation or usage for sequence modeling. Thus, it does not meet the inclusion criteria related to discrete audio tokens nor does it detail methodology as required, leading to exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
    "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.",
    "metadata": {
      "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
      "summary": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.",
      "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.",
      "doi": "https://doi.org/10.1613/jair.1.13083",
      "openalex_id": "https://openalex.org/W3166631396",
      "arxiv_id": "",
      "publication_date": "2022-07-13",
      "published": "2022-07-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses multilingual task‑oriented dialogue systems but does not discuss discrete audio tokens or related tokenization/quantization methods; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses multilingual task‑oriented dialogue systems but does not discuss discrete audio tokens or related tokenization/quantization methods; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses multilingual task-oriented dialogue systems, focusing on natural language processing challenges and data limitations, without any mention of discrete audio tokens, vector quantization, or related audio tokenization techniques as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses multilingual task-oriented dialogue systems, focusing on natural language processing challenges and data limitations, without any mention of discrete audio tokens, vector quantization, or related audio tokenization techniques as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Temporal Knowledge Distillation for on-device Audio Classification",
    "abstract": "Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",
    "metadata": {
      "title": "Temporal Knowledge Distillation for on-device Audio Classification",
      "summary": "Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",
      "abstract": "Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747908",
      "openalex_id": "https://openalex.org/W3210574093",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or tokenization/quantization as a core method; it focuses on temporal knowledge distillation for on-device audio classification, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or tokenization/quantization as a core method; it focuses on temporal knowledge distillation for on-device audio classification, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on knowledge distillation methods for on-device audio classification, emphasizing temporal knowledge transfer in attention-based transformer models and compatibility with various architectures. However, there is no mention of discrete audio token generation, vector quantization, or related methods to convert continuous audio signals into discrete symbols for sequence modeling, which is central to the inclusion criteria. Thus, it does not meet the key inclusion criteria regarding discrete audio tokens, nor is it excluded based on other criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on knowledge distillation methods for on-device audio classification, emphasizing temporal knowledge transfer in attention-based transformer models and compatibility with various architectures. However, there is no mention of discrete audio token generation, vector quantization, or related methods to convert continuous audio signals into discrete symbols for sequence modeling, which is central to the inclusion criteria. Thus, it does not meet the key inclusion criteria regarding discrete audio tokens, nor is it excluded based on other criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
    "abstract": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.",
    "metadata": {
      "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
      "summary": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.",
      "abstract": "One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1390",
      "openalex_id": "https://openalex.org/W3110524561",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multilingual ASR with adapters for long-tail data, not on discrete audio token representations or tokenization methods, thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multilingual ASR with adapters for long-tail data, not on discrete audio token representations or tokenization methods, thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on overcoming the long-tail problem in multilingual speech recognition using transformer-based multi-task learning and adaptation strategies, but it does not describe any discrete audio token generation, vector quantization, or tokenization process for audio signals as core methods. The abstract lacks details about producing discrete audio tokens or a quantization mechanism that supports sequence/language modeling, and thus it does not meet the inclusion criteria requiring discrete token design as a main contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on overcoming the long-tail problem in multilingual speech recognition using transformer-based multi-task learning and adaptation strategies, but it does not describe any discrete audio token generation, vector quantization, or tokenization process for audio signals as core methods. The abstract lacks details about producing discrete audio tokens or a quantization mechanism that supports sequence/language modeling, and thus it does not meet the inclusion criteria requiring discrete token design as a main contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Validity, feasibility, and effectiveness of a voice‐recognition based digital cognitive screener for dementia and mild cognitive impairment in community‐dwelling older Chinese adults: A large‐scale implementation study",
    "abstract": "Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.",
    "metadata": {
      "title": "Validity, feasibility, and effectiveness of a voice‐recognition based digital cognitive screener for dementia and mild cognitive impairment in community‐dwelling older Chinese adults: A large‐scale implementation study",
      "summary": "Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.",
      "abstract": "Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.",
      "doi": "https://doi.org/10.1002/alz.13668",
      "openalex_id": "https://openalex.org/W4391436317",
      "arxiv_id": "",
      "publication_date": "2024-02-01",
      "published": "2024-02-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or token-based representations; it's a clinical screening study using voice recognition, lacking the required tokenization methodology and token-level design details.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or token-based representations; it's a clinical screening study using voice recognition, lacking the required tokenization methodology and token-level design details.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the use of a voice recognition-based digital cognitive screener for dementia and mild cognitive impairment detection. However, it does not describe the use or design of discrete audio tokens that are vector-quantized or discretized audio representations for sequence or language modeling, nor does it present technical details on token generation or quantization methods. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens as core research contributions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the use of a voice recognition-based digital cognitive screener for dementia and mild cognitive impairment detection. However, it does not describe the use or design of discrete audio tokens that are vector-quantized or discretized audio representations for sequence or language modeling, nor does it present technical details on token generation or quantization methods. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens as core research contributions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic Acoustic Unit Augmentation with BPE-Dropout for Low-Resource End-to-End Speech Recognition",
    "abstract": "With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.",
    "metadata": {
      "title": "Dynamic Acoustic Unit Augmentation with BPE-Dropout for Low-Resource End-to-End Speech Recognition",
      "summary": "With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.",
      "abstract": "With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.",
      "doi": "https://doi.org/10.3390/s21093063",
      "openalex_id": "https://openalex.org/W3136717460",
      "arxiv_id": "",
      "publication_date": "2021-04-28",
      "published": "2021-04-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses BPE-dropout for tokenization in a low-resource end-to-end ASR setting, focusing on discrete subword tokens to handle OOV, but it does not present discrete audio token design via neural codecs (VQ/RVQ) or semantic-discretization of audio as a core methodological contribution, thus it does not fully meet the specified criteria for Discrete Audio Tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work uses BPE-dropout for tokenization in a low-resource end-to-end ASR setting, focusing on discrete subword tokens to handle OOV, but it does not present discrete audio token design via neural codecs (VQ/RVQ) or semantic-discretization of audio as a core methodological contribution, thus it does not fully meet the specified criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on end-to-end speech recognition using dynamic acoustic unit augmentation with BPE-dropout, dealing primarily with subword tokenization strategies rather than discrete audio tokens generated from vector quantization or similar discrete audio representation methods. It does not describe producing discrete audio tokens from continuous audio signals via vector quantization or related techniques, nor does it detail token generation for sequence/language modeling based on discrete audio representations as defined by the inclusion criteria. Therefore, it fails to meet the core inclusion requirement of discrete audio token design as the main contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on end-to-end speech recognition using dynamic acoustic unit augmentation with BPE-dropout, dealing primarily with subword tokenization strategies rather than discrete audio tokens generated from vector quantization or similar discrete audio representation methods. It does not describe producing discrete audio tokens from continuous audio signals via vector quantization or related techniques, nor does it detail token generation for sequence/language modeling based on discrete audio representations as defined by the inclusion criteria. Therefore, it fails to meet the core inclusion requirement of discrete audio token design as the main contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
    "abstract": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.",
    "metadata": {
      "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
      "summary": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.",
      "abstract": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.",
      "doi": "https://doi.org/10.21437/interspeech.2022-547",
      "openalex_id": "https://openalex.org/W3207547029",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on adapting Wav2vec 2.0 for Korean ASR with joint decoding of graphemes and syllables and cross-lingual pretraining, but it does not present or emphasize discrete audio token generation/quantization (e.g., VQ/RVQ) as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adapting Wav2vec 2.0 for Korean ASR with joint decoding of graphemes and syllables and cross-lingual pretraining, but it does not present or emphasize discrete audio token generation/quantization (e.g., VQ/RVQ) as a core contribution.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes K-Wav2vec 2.0, a modified Wav2vec 2.0 framework for Korean ASR, focusing on self-supervised learning and hierarchical decoding of graphemes and syllables, but it does not clearly describe generation or usage of discrete audio tokens through vector quantization or similar mechanisms critical to the inclusion criteria. The work primarily focuses on ASR improvement for Korean, without detailed mention of discrete token generation or their role in sequence/language modeling beyond standard self-supervised speech representation. Therefore, it does not meet the inclusion requirements related to discrete audio token design and usage as defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes K-Wav2vec 2.0, a modified Wav2vec 2.0 framework for Korean ASR, focusing on self-supervised learning and hierarchical decoding of graphemes and syllables, but it does not clearly describe generation or usage of discrete audio tokens through vector quantization or similar mechanisms critical to the inclusion criteria. The work primarily focuses on ASR improvement for Korean, without detailed mention of discrete token generation or their role in sequence/language modeling beyond standard self-supervised speech representation. Therefore, it does not meet the inclusion requirements related to discrete audio token design and usage as defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging",
    "abstract": "Machine Learning (ML) is more than just training models, the whole workflow must be considered. Once deployed, a ML model needs to be watched and constantly supervised and debugged to guarantee its validity and robustness in unexpected situations. Debugging in ML aims to identify (and address) the model weaknesses in not trivial contexts. Several techniques have been proposed to identify different types of model weaknesses, such as bias in classification, model decay, adversarial attacks, etc., yet there is not a generic framework that allows them to work in a collaborative, modular, portable, iterative way and, more importantly, flexible enough to allow both human- and machine-driven techniques. In this paper, we propose a novel containerized directed graph framework to support and accelerate end-to-end ML workflow management, supervision, and debugging. The framework allows defining and deploying ML workflows in containers, tracking their metadata, checking their behavior in production, and improving the models by using both learned and human-provided knowledge. We demonstrate these capabilities by integrating in the framework two hybrid systems to detect data drift distribution which identify the samples that are far from the latent space of the original distribution, ask for human intervention, and whether retrain the model or wrap it with a filter to remove the noise of corrupted data at inference time. We test these systems on MNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy results with the help of human involvement.",
    "metadata": {
      "title": "Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging",
      "summary": "Machine Learning (ML) is more than just training models, the whole workflow must be considered. Once deployed, a ML model needs to be watched and constantly supervised and debugged to guarantee its validity and robustness in unexpected situations. Debugging in ML aims to identify (and address) the model weaknesses in not trivial contexts. Several techniques have been proposed to identify different types of model weaknesses, such as bias in classification, model decay, adversarial attacks, etc., yet there is not a generic framework that allows them to work in a collaborative, modular, portable, iterative way and, more importantly, flexible enough to allow both human- and machine-driven techniques. In this paper, we propose a novel containerized directed graph framework to support and accelerate end-to-end ML workflow management, supervision, and debugging. The framework allows defining and deploying ML workflows in containers, tracking their metadata, checking their behavior in production, and improving the models by using both learned and human-provided knowledge. We demonstrate these capabilities by integrating in the framework two hybrid systems to detect data drift distribution which identify the samples that are far from the latent space of the original distribution, ask for human intervention, and whether retrain the model or wrap it with a filter to remove the noise of corrupted data at inference time. We test these systems on MNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy results with the help of human involvement.",
      "abstract": "Machine Learning (ML) is more than just training models, the whole workflow must be considered. Once deployed, a ML model needs to be watched and constantly supervised and debugged to guarantee its validity and robustness in unexpected situations. Debugging in ML aims to identify (and address) the model weaknesses in not trivial contexts. Several techniques have been proposed to identify different types of model weaknesses, such as bias in classification, model decay, adversarial attacks, etc., yet there is not a generic framework that allows them to work in a collaborative, modular, portable, iterative way and, more importantly, flexible enough to allow both human- and machine-driven techniques. In this paper, we propose a novel containerized directed graph framework to support and accelerate end-to-end ML workflow management, supervision, and debugging. The framework allows defining and deploying ML workflows in containers, tracking their metadata, checking their behavior in production, and improving the models by using both learned and human-provided knowledge. We demonstrate these capabilities by integrating in the framework two hybrid systems to detect data drift distribution which identify the samples that are far from the latent space of the original distribution, ask for human intervention, and whether retrain the model or wrap it with a filter to remove the noise of corrupted data at inference time. We test these systems on MNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy results with the help of human involvement.",
      "doi": "https://doi.org/10.1016/j.eswa.2022.117232",
      "openalex_id": "https://openalex.org/W3211110040",
      "arxiv_id": "",
      "publication_date": "2022-04-19",
      "published": "2022-04-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a general ML workflow management framework for supervision and debugging, not any discrete audio token design or tokenization methodology (e.g., VQ/RVQ, semantic/discrete tokens), so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a general ML workflow management framework for supervision and debugging, not any discrete audio token design or tokenization methodology (e.g., VQ/RVQ, semantic/discrete tokens), so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a multi-graph framework for machine learning workflow management, supervision, and debugging rather than on discrete audio token generation or design, which is the core inclusion criterion. There is no mention of audio signal processing, vector quantization, or the transformation of continuous audio signals into discrete tokens for sequence or language modeling tasks, nor any reference to the methodologies required for inclusion. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a multi-graph framework for machine learning workflow management, supervision, and debugging rather than on discrete audio token generation or design, which is the core inclusion criterion. There is no mention of audio signal processing, vector quantization, or the transformation of continuous audio signals into discrete tokens for sequence or language modeling tasks, nor any reference to the methodologies required for inclusion. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals",
    "abstract": "Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based \"AI Listener\" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",
    "metadata": {
      "title": "Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals",
      "summary": "Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based \"AI Listener\" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",
      "abstract": "Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based \"AI Listener\" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",
      "doi": "https://doi.org/10.1109/ner52421.2023.10123751",
      "openalex_id": "https://openalex.org/W4377089543",
      "arxiv_id": "",
      "publication_date": "2023-04-24",
      "published": "2023-04-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on evaluating intelligibility of brain-signal–synthesized speech using ASR models and human listener comparisons, and does not address discrete audio token representations, token generation/quantization, or token-based language/modeling; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on evaluating intelligibility of brain-signal–synthesized speech using ASR models and human listener comparisons, and does not address discrete audio token representations, token generation/quantization, or token-based language/modeling; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using Automatic Speech Recognition models to evaluate the intelligibility of speech synthesized from brain-computer interfaces, which relates to speech synthesis and evaluation but does not address the creation or use of discrete audio tokens derived from vector quantization or similar methods. The abstract lacks mention of discrete token generation, quantization processes, or use of tokens as a core method for audio language modeling or sequencing, which are essential inclusion criteria. Therefore, it does not meet the inclusion criteria for studies on discrete audio tokens and related techniques.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using Automatic Speech Recognition models to evaluate the intelligibility of speech synthesized from brain-computer interfaces, which relates to speech synthesis and evaluation but does not address the creation or use of discrete audio tokens derived from vector quantization or similar methods. The abstract lacks mention of discrete token generation, quantization processes, or use of tokens as a core method for audio language modeling or sequencing, which are essential inclusion criteria. Therefore, it does not meet the inclusion criteria for studies on discrete audio tokens and related techniques.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Speech Recognition Initiative for African Languages",
    "abstract": "Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.",
    "metadata": {
      "title": "Multilingual Speech Recognition Initiative for African Languages",
      "summary": "Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.",
      "abstract": "Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.",
      "doi": "https://doi.org/10.21203/rs.3.rs-2708355/v1",
      "openalex_id": "https://openalex.org/W4327858845",
      "arxiv_id": "",
      "publication_date": "2023-03-20",
      "published": "2023-03-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses multilingual ASR using self-supervised models and language identification but does not discuss discrete audio tokens or tokenization/quantization methods, which are required for inclusion under the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses multilingual ASR using self-supervised models and language identification but does not discuss discrete audio tokens or tokenization/quantization methods, which are required for inclusion under the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses speech recognition methods for multiple African languages using self-supervised pre-trained models but does not mention discrete audio tokens, vector quantization, or any explicit process of converting continuous audio signals into discrete token representations. It also lacks technical details about token generation or quantization methods required by the inclusion criteria, and thus does not meet the core requirement of focusing on discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses speech recognition methods for multiple African languages using self-supervised pre-trained models but does not mention discrete audio tokens, vector quantization, or any explicit process of converting continuous audio signals into discrete token representations. It also lacks technical details about token generation or quantization methods required by the inclusion criteria, and thus does not meet the core requirement of focusing on discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CDPAM: Contrastive Learning for Perceptual Audio Similarity",
    "abstract": "Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.",
    "metadata": {
      "title": "CDPAM: Contrastive Learning for Perceptual Audio Similarity",
      "summary": "Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.",
      "abstract": "Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9413711",
      "openalex_id": "https://openalex.org/W3128311069",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a perceptual audio similarity metric (CDPAM) using contrastive learning and human judgments; it does not discuss discrete audio tokens, vector quantization, or token-based representations, which are required for inclusion. Therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a perceptual audio similarity metric (CDPAM) using contrastive learning and human judgments; it does not discuss discrete audio tokens, vector quantization, or token-based representations, which are required for inclusion. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces CDPAM, a metric for perceptual audio similarity using contrastive learning to improve generalization and correlates with human judgments; however, it does not mention any discrete audio token generation, quantization methods, or token usage in sequence models, which is the core requirement for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces CDPAM, a metric for perceptual audio similarity using contrastive learning to improve generalization and correlates with human judgments; however, it does not mention any discrete audio token generation, quantization methods, or token usage in sequence models, which is the core requirement for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The CogALex shared task on monolingual and multilingual identification of semantic relations",
    "abstract": "The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.",
    "metadata": {
      "title": "The CogALex shared task on monolingual and multilingual identification of semantic relations",
      "summary": "The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.",
      "abstract": "The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3120335524",
      "arxiv_id": "",
      "publication_date": "2020-12-01",
      "published": "2020-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a CogALex shared task on monolingual and multilingual identification of semantic relations in text, not about discrete audio tokens or audio token representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a CogALex shared task on monolingual and multilingual identification of semantic relations in text, not about discrete audio tokens or audio token representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semantic relation identification between word pairs using linguistic and transformer-based models, and does not discuss any discrete audio tokens, audio representation discretization methods, or vector quantization related to audio signals as defined in the inclusion criteria. It lacks any content involving audio signal processing or discrete token generation for audio language modeling, thus failing all inclusion requirements and meeting exclusion criteria implicitly.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semantic relation identification between word pairs using linguistic and transformer-based models, and does not discuss any discrete audio tokens, audio representation discretization methods, or vector quantization related to audio signals as defined in the inclusion criteria. It lacks any content involving audio signal processing or discrete token generation for audio language modeling, thus failing all inclusion requirements and meeting exclusion criteria implicitly.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer Based Speech to Text Translation for Indic Languages",
    "abstract": "In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.",
    "metadata": {
      "title": "Transformer Based Speech to Text Translation for Indic Languages",
      "summary": "In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.",
      "abstract": "In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.",
      "doi": "https://doi.org/10.1109/icaisc58445.2023.10200222",
      "openalex_id": "https://openalex.org/W4385695089",
      "arxiv_id": "",
      "publication_date": "2023-06-16",
      "published": "2023-06-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses transformer-based speech-to-text translation and LM improvements but does not address discrete audio tokens or tokenization methods (e.g., VQ/RVQ, semantic discrete tokens) as required by the inclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses transformer-based speech-to-text translation and LM improvements but does not address discrete audio tokens or tokenization methods (e.g., VQ/RVQ, semantic discrete tokens) as required by the inclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech recognition accuracy using transformer models but does not describe generating discrete audio tokens via vector quantization or related methods, nor does it provide details on token generation or usage as required for inclusion; hence it fails to meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech recognition accuracy using transformer models but does not describe generating discrete audio tokens via vector quantization or related methods, nor does it provide details on token generation or usage as required for inclusion; hence it fails to meet the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
    "abstract": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.",
    "metadata": {
      "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
      "summary": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.",
      "abstract": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1683",
      "openalex_id": "https://openalex.org/W3145189018",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates wav2vec 2.0 fine-tuning and ASR training criteria (CTC vs LFMMI) under out-of-domain data, not any discrete audio token generation/quantization mechanism, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates wav2vec 2.0 fine-tuning and ASR training criteria (CTC vs LFMMI) under out-of-domain data, not any discrete audio token generation/quantization mechanism, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on comparing CTC and LFMMI methods for adapting wav2vec 2.0 acoustic models in speech recognition scenarios, without mentioning any discrete audio token design or generation mechanisms such as vector quantization, clustering, or semantic disentanglement; it lacks details on token formation, quantization, or the use of discrete tokens as representations for sequence modeling, thus not meeting the inclusion criteria and not related to the core topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on comparing CTC and LFMMI methods for adapting wav2vec 2.0 acoustic models in speech recognition scenarios, without mentioning any discrete audio token design or generation mechanisms such as vector quantization, clustering, or semantic disentanglement; it lacks details on token formation, quantization, or the use of discrete tokens as representations for sequence modeling, thus not meeting the inclusion criteria and not related to the core topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data",
    "abstract": "We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.",
    "metadata": {
      "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data",
      "summary": "We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.",
      "abstract": "We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.591",
      "openalex_id": "https://openalex.org/W3153532013",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with multilingual/cross-lingual intent detection from spoken data and uses MT/LaBSE; it does not address discrete audio tokens or token-based representations, hence fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with multilingual/cross-lingual intent detection from spoken data and uses MT/LaBSE; it does not address discrete audio tokens or token-based representations, hence fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual and cross-lingual intent detection from spoken data using machine translation and multilingual sentence encoders, but it does not mention the design, generation, or use of discrete audio tokens through vector quantization or similar methods as required by the inclusion criteria. There is no clear description of token generation, quantization, or use of discrete audio token representations for language or sequence modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio token techniques.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual and cross-lingual intent detection from spoken data using machine translation and multilingual sentence encoders, but it does not mention the design, generation, or use of discrete audio tokens through vector quantization or similar methods as required by the inclusion criteria. There is no clear description of token generation, quantization, or use of discrete audio token representations for language or sequence modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio token techniques.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
    "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
    "metadata": {
      "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
      "summary": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
      "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2011.01403",
      "openalex_id": "https://openalex.org/W3096565276",
      "arxiv_id": "",
      "publication_date": "2020-11-03",
      "published": "2020-11-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on supervised contrastive learning for fine-tuning language models (NLP) and does not involve discrete audio tokens or audio token design, thus it does not meet the inclusion criteria for the discrete audio token topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on supervised contrastive learning for fine-tuning language models (NLP) and does not involve discrete audio tokens or audio token design, thus it does not meet the inclusion criteria for the discrete audio token topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on supervised contrastive learning for fine-tuning pre-trained language models on natural language understanding tasks, which does not relate to discrete audio tokens or audio signal processing as defined in the inclusion criteria. There is no mention of audio token discretization, vector quantization, or audio-related sequence modeling, thus failing to meet the main technical coverage requirements and the core research focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on supervised contrastive learning for fine-tuning pre-trained language models on natural language understanding tasks, which does not relate to discrete audio tokens or audio signal processing as defined in the inclusion criteria. There is no mention of audio token discretization, vector quantization, or audio-related sequence modeling, thus failing to meet the main technical coverage requirements and the core research focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data",
    "abstract": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.",
    "metadata": {
      "title": "UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data",
      "summary": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.",
      "abstract": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.",
      "doi": "https://doi.org/10.48550/arxiv.2101.07597",
      "openalex_id": "https://openalex.org/W3121299949",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or tokenization (VQ/RVQ/semantic tokens); focuses on unified speech representations with supervised/SSL objectives, so it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or tokenization (VQ/RVQ/semantic tokens); focuses on unified speech representations with supervised/SSL objectives, so it fails the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper UniSpeech proposes a unified speech representation learning approach integrating labeled and unlabeled data, focusing on multi-task learning of phonetic CTC and contrastive self-supervised learning for improved speech recognition. However, there is no indication that the method involves creating discrete audio tokens or tokenization via vector quantization, residual vector quantization, or similar discrete token generation techniques as defined in the inclusion criteria. The abstract does not describe the generation of discrete tokens or quantization mechanisms central to the topic, nor does it emphasize token design or their use as input to language or sequence models for audio understanding or generation. Therefore, the work does not meet the criteria of focusing on discrete audio tokens and their generation, and only targets learning of continuous speech representations and transfer tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper UniSpeech proposes a unified speech representation learning approach integrating labeled and unlabeled data, focusing on multi-task learning of phonetic CTC and contrastive self-supervised learning for improved speech recognition. However, there is no indication that the method involves creating discrete audio tokens or tokenization via vector quantization, residual vector quantization, or similar discrete token generation techniques as defined in the inclusion criteria. The abstract does not describe the generation of discrete tokens or quantization mechanisms central to the topic, nor does it emphasize token design or their use as input to language or sequence models for audio understanding or generation. Therefore, the work does not meet the criteria of focusing on discrete audio tokens and their generation, and only targets learning of continuous speech representations and transfer tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptation Algorithms for Speech Recognition: An Overview.",
    "abstract": "We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.",
    "metadata": {
      "title": "Adaptation Algorithms for Speech Recognition: An Overview.",
      "summary": "We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.",
      "abstract": "We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3049397270",
      "arxiv_id": "",
      "publication_date": "2020-08-14",
      "published": "2020-08-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on adaptation algorithms for speech recognition and does not address discrete audio tokens or tokenization designs, failing the inclusion criteria focused on discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on adaptation algorithms for speech recognition and does not address discrete audio tokens or tokenization designs, failing the inclusion criteria focused on discrete token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article provides an overview of adaptation algorithms for neural network-based speech recognition, focusing on speaker, domain, and accent adaptation, without mentioning any discrete audio token design, token generation processes, or vector quantization methods relevant to the inclusion criteria for discrete audio tokens; thus, it does not meet the core inclusion requirement related to discrete token methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article provides an overview of adaptation algorithms for neural network-based speech recognition, focusing on speaker, domain, and accent adaptation, without mentioning any discrete audio token design, token generation processes, or vector quantization methods relevant to the inclusion criteria for discrete audio tokens; thus, it does not meet the core inclusion requirement related to discrete token methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset",
    "abstract": "Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.",
    "metadata": {
      "title": "UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset",
      "summary": "Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.",
      "abstract": "Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.",
      "doi": "https://doi.org/10.48550/arxiv.2107.05233",
      "openalex_id": "https://openalex.org/W3178203035",
      "arxiv_id": "",
      "publication_date": "2021-07-12",
      "published": "2021-07-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised pre-training for large-scale ASR and does not discuss discrete audio tokens, token generation, or token-based representations required by the inclusion criteria, nor does it present appropriate methodological detail for token-based evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised pre-training for large-scale ASR and does not discuss discrete audio tokens, token generation, or token-based representations required by the inclusion criteria, nor does it present appropriate methodological detail for token-based evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on large-scale self-supervised learning for speech recognition using a Transformer-Transducer model pre-trained on large labeled datasets; however, it does not describe the use or generation of discrete audio tokens, nor any vector quantization or similar tokenization methods. Therefore, it does not meet the core inclusion criterion of involving discrete audio token design or usage as a central contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on large-scale self-supervised learning for speech recognition using a Transformer-Transducer model pre-trained on large labeled datasets; however, it does not describe the use or generation of discrete audio tokens, nor any vector quantization or similar tokenization methods. Therefore, it does not meet the core inclusion criterion of involving discrete audio token design or usage as a central contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatically Identifying Language Family from Acoustic Examples in Low Resource Scenarios",
    "abstract": "Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.",
    "metadata": {
      "title": "Automatically Identifying Language Family from Acoustic Examples in Low Resource Scenarios",
      "summary": "Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.",
      "abstract": "Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.",
      "doi": "https://doi.org/10.48550/arxiv.2012.00876",
      "openalex_id": "https://openalex.org/W3109527089",
      "arxiv_id": "",
      "publication_date": "2020-12-01",
      "published": "2020-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on analyzing language similarity via a deep learning latent space for cross-lingual augmentation, with no discussion of discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on analyzing language similarity via a deep learning latent space for cross-lingual augmentation, with no discussion of discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training a deep learning model for analyzing language similarity in speech using the Wilderness dataset, but it does not mention the use of discrete audio token representations generated by vector quantization or related methods as defined in the inclusion criteria. The abstract lacks details about token generation processes, quantization techniques, or use of discrete tokens for language modeling or audio generation, which are key required elements. Therefore, it does not fulfill the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training a deep learning model for analyzing language similarity in speech using the Wilderness dataset, but it does not mention the use of discrete audio token representations generated by vector quantization or related methods as defined in the inclusion criteria. The abstract lacks details about token generation processes, quantization techniques, or use of discrete tokens for language modeling or audio generation, which are key required elements. Therefore, it does not fulfill the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition",
    "abstract": "Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often contain incorrect phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classification task to distinguish Anglicisms from native German words. With this approach, the model learns to generate different pronunciations depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries to be added to an existing German speech recognition model. Tested on a special Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by a relative 1 % and the Anglicism error rate by a relative 3 %. With our experiment, we show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.",
    "metadata": {
      "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition",
      "summary": "Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often contain incorrect phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classification task to distinguish Anglicisms from native German words. With this approach, the model learns to generate different pronunciations depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries to be added to an existing German speech recognition model. Tested on a special Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by a relative 1 % and the Anglicism error rate by a relative 3 %. With our experiment, we show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.",
      "abstract": "Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often contain incorrect phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classification task to distinguish Anglicisms from native German words. With this approach, the model learns to generate different pronunciations depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries to be added to an existing German speech recognition model. Tested on a special Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by a relative 1 % and the Anglicism error rate by a relative 3 %. With our experiment, we show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.",
      "doi": "https://doi.org/10.24406/publica-380",
      "openalex_id": "https://openalex.org/W3165619625",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not discuss discrete audio tokens or tokenization; it targets grapheme-to-phoneme conversion and pronunciation dictionaries for Anglicisms in German ASR, which is not about discrete token representations used for audio-language modeling or downstream token-based models.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not discuss discrete audio tokens or tokenization; it targets grapheme-to-phoneme conversion and pronunciation dictionaries for Anglicisms in German ASR, which is not about discrete token representations used for audio-language modeling or downstream token-based models.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving grapheme-to-phoneme conversion for Anglicisms in German speech recognition through a multitask learning approach, but it does not address discrete audio token generation, vector quantization, or any form of discrete representation of continuous audio signals, which are core requirements of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving grapheme-to-phoneme conversion for Anglicisms in German speech recognition through a multitask learning approach, but it does not address discrete audio token generation, vector quantization, or any form of discrete representation of continuous audio signals, which are core requirements of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19",
    "abstract": "Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.",
    "metadata": {
      "title": "Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19",
      "summary": "Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.",
      "abstract": "Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.",
      "doi": "https://doi.org/10.48550/arxiv.2108.10791",
      "openalex_id": "https://openalex.org/W3196174113",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on inclusive NLP in the global COVID-19 response and does not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on inclusive NLP in the global COVID-19 response and does not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the inclusive application of NLP in the global COVID-19 response, discussing low-resource languages and societal impacts, but do not mention discrete audio tokens or any related process such as vector quantization or token generation. Therefore, the study does not meet the core inclusion criteria regarding discrete audio token design or methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the inclusive application of NLP in the global COVID-19 response, discussing low-resource languages and societal impacts, but do not mention discrete audio tokens or any related process such as vector quantization or token generation. Therefore, the study does not meet the core inclusion criteria regarding discrete audio token design or methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ASR4REAL: An extended benchmark for speech models",
    "abstract": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",
    "metadata": {
      "title": "ASR4REAL: An extended benchmark for speech models",
      "summary": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",
      "abstract": "Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",
      "doi": "https://doi.org/10.48550/arxiv.2110.08583",
      "openalex_id": "https://openalex.org/W3206083773",
      "arxiv_id": "",
      "publication_date": "2021-10-16",
      "published": "2021-10-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on ASR benchmarks and biases in speech models, not on discrete audio tokens or tokenization methods; it fails to meet the inclusion criteria and is not about token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on ASR benchmarks and biases in speech models, not on discrete audio tokens or tokenization methods; it fails to meet the inclusion criteria and is not about token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses an extended benchmark for speech models focusing on real-life conditions and biases, without mentioning any method for generating discrete audio tokens or utilizing vector quantization or related techniques to produce such tokens for sequence modeling or audio language modeling. Therefore, it does not meet the inclusion criteria centered around discrete audio token design and usage, nor does it provide relevant methodological details about token formation or quantization settings.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses an extended benchmark for speech models focusing on real-life conditions and biases, without mentioning any method for generating discrete audio tokens or utilizing vector quantization or related techniques to produce such tokens for sequence modeling or audio language modeling. Therefore, it does not meet the inclusion criteria centered around discrete audio token design and usage, nor does it provide relevant methodological details about token formation or quantization settings.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals",
    "abstract": "Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.",
    "metadata": {
      "title": "Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals",
      "summary": "Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.",
      "abstract": "Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.",
      "doi": "https://doi.org/10.48550/arxiv.2103.11011",
      "openalex_id": "https://openalex.org/W3136649840",
      "arxiv_id": "",
      "publication_date": "2021-03-19",
      "published": "2021-03-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multilingual captioning of cardiac signals and generating clinical reports, but it does not address discrete audio tokens, vector quantization, or token-design for audio-language modelling; it lacks the required methodological focus on discrete audio token representations, so it should be excluded under the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multilingual captioning of cardiac signals and generating clinical reports, but it does not address discrete audio tokens, vector quantization, or token-design for audio-language modelling; it lacks the required methodological focus on discrete audio token representations, so it should be excluded under the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on captioning cardiac signals (electrocardiogram) by generating clinical reports using deep neural networks with multilingual capabilities. However, it does not address the creation or use of discrete audio tokens through vector quantization or similar methods to transform continuous audio signals into discrete tokens suitable for language or sequence models, which is the core inclusion criterion. The work is about signal captioning and report generation, not about discrete audio token design or generation for audio language modeling or understanding. Therefore, it does not meet the main inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on captioning cardiac signals (electrocardiogram) by generating clinical reports using deep neural networks with multilingual capabilities. However, it does not address the creation or use of discrete audio tokens through vector quantization or similar methods to transform continuous audio signals into discrete tokens suitable for language or sequence models, which is the core inclusion criterion. The work is about signal captioning and report generation, not about discrete audio token design or generation for audio language modeling or understanding. Therefore, it does not meet the main inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pretext Tasks selection for multitask self-supervised speech\\n representation learning",
    "abstract": "Through solving pretext tasks, self-supervised learning leverages unlabeled\\ndata to extract useful latent representations replacing traditional input\\nfeatures in the downstream task. In audio/speech signal processing, a wide\\nrange of features where engineered through decades of research efforts. As it\\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\\nto be a particularly relevant pretext task, leading to useful self-supervised\\nrepresentations which prove to be effective for downstream tasks. However,\\nmethods and common practices for combining such pretext tasks for better\\nperformance on the downstream task have not been explored and understood\\nproperly. In fact, the process relies almost exclusively on a computationally\\nheavy experimental procedure, which becomes intractable with the increase of\\nthe number of pretext tasks. This paper introduces a method to select a group\\nof pretext tasks among a set of candidates. The method we propose estimates\\ncalibrated weights for the partial losses corresponding to the considered\\npretext tasks during the self-supervised training process. The experiments\\nconducted on automatic speech recognition, speaker and emotion recognition\\nvalidate our approach, as the groups selected and weighted with our method\\nperform better than classic baselines, thus facilitating the selection and\\ncombination of relevant pseudo-labels for self-supervised representation\\nlearning.\\n",
    "metadata": {
      "title": "Pretext Tasks selection for multitask self-supervised speech\\n representation learning",
      "summary": "Through solving pretext tasks, self-supervised learning leverages unlabeled\\ndata to extract useful latent representations replacing traditional input\\nfeatures in the downstream task. In audio/speech signal processing, a wide\\nrange of features where engineered through decades of research efforts. As it\\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\\nto be a particularly relevant pretext task, leading to useful self-supervised\\nrepresentations which prove to be effective for downstream tasks. However,\\nmethods and common practices for combining such pretext tasks for better\\nperformance on the downstream task have not been explored and understood\\nproperly. In fact, the process relies almost exclusively on a computationally\\nheavy experimental procedure, which becomes intractable with the increase of\\nthe number of pretext tasks. This paper introduces a method to select a group\\nof pretext tasks among a set of candidates. The method we propose estimates\\ncalibrated weights for the partial losses corresponding to the considered\\npretext tasks during the self-supervised training process. The experiments\\nconducted on automatic speech recognition, speaker and emotion recognition\\nvalidate our approach, as the groups selected and weighted with our method\\nperform better than classic baselines, thus facilitating the selection and\\ncombination of relevant pseudo-labels for self-supervised representation\\nlearning.\\n",
      "abstract": "Through solving pretext tasks, self-supervised learning leverages unlabeled\\ndata to extract useful latent representations replacing traditional input\\nfeatures in the downstream task. In audio/speech signal processing, a wide\\nrange of features where engineered through decades of research efforts. As it\\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\\nto be a particularly relevant pretext task, leading to useful self-supervised\\nrepresentations which prove to be effective for downstream tasks. However,\\nmethods and common practices for combining such pretext tasks for better\\nperformance on the downstream task have not been explored and understood\\nproperly. In fact, the process relies almost exclusively on a computationally\\nheavy experimental procedure, which becomes intractable with the increase of\\nthe number of pretext tasks. This paper introduces a method to select a group\\nof pretext tasks among a set of candidates. The method we propose estimates\\ncalibrated weights for the partial losses corresponding to the considered\\npretext tasks during the self-supervised training process. The experiments\\nconducted on automatic speech recognition, speaker and emotion recognition\\nvalidate our approach, as the groups selected and weighted with our method\\nperform better than classic baselines, thus facilitating the selection and\\ncombination of relevant pseudo-labels for self-supervised representation\\nlearning.\\n",
      "doi": "https://doi.org/10.1109/jstsp.2022.3195430",
      "openalex_id": "https://openalex.org/W3176481516",
      "arxiv_id": "",
      "publication_date": "2021-07-01",
      "published": "2021-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on selecting and weighting pretext tasks for self-supervised speech representations, not on discrete audio tokens or tokenization methods (e.g., VQ/RVQ, vector quantization, or token-based modeling). It therefore does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on selecting and weighting pretext tasks for self-supervised speech representations, not on discrete audio tokens or tokenization methods (e.g., VQ/RVQ, vector quantization, or token-based modeling). It therefore does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on pretext task selection for multitask self-supervised speech representation learning but does not mention the generation of discrete audio tokens via vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization; nor does it detail token formation processes for sequence or language model usage, which are central to the inclusion criteria. Therefore, it does not meet the core requirement related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on pretext task selection for multitask self-supervised speech representation learning but does not mention the generation of discrete audio tokens via vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization; nor does it detail token formation processes for sequence or language model usage, which are central to the inclusion criteria. Therefore, it does not meet the core requirement related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "IMS’ Systems for the IWSLT 2021 Low-Resource Speech Translation Task",
    "abstract": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.",
    "metadata": {
      "title": "IMS’ Systems for the IWSLT 2021 Low-Resource Speech Translation Task",
      "summary": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.",
      "abstract": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.",
      "doi": "https://doi.org/10.18653/v1/2021.iwslt-1.21",
      "openalex_id": "https://openalex.org/W3177457454",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on a low-resource speech translation system with ASR/MT components and data augmentation; it does not discuss discrete audio tokens, tokenization methods (VQ/RVQ, hierarchical quantization, semantic discrete tokens), or their use in audio-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a low-resource speech translation system with ASR/MT components and data augmentation; it does not discuss discrete audio tokens, tokenization methods (VQ/RVQ, hierarchical quantization, semantic discrete tokens), or their use in audio-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a speech translation system using state-of-the-art models and learning approaches but does not mention any discrete audio token design, quantization methods, or token generation processes that align with the inclusion criteria. It focuses on speech translation performance rather than on discrete audio token representations or related quantization-based methods, thus lacking the core methodological content required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a speech translation system using state-of-the-art models and learning approaches but does not mention any discrete audio token design, quantization methods, or token generation processes that align with the inclusion criteria. It focuses on speech translation performance rather than on discrete audio token representations or related quantization-based methods, thus lacking the core methodological content required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-lingual Transfer for Speech Processing using Acoustic Language Similarity",
    "abstract": "Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.",
    "metadata": {
      "title": "Cross-lingual Transfer for Speech Processing using Acoustic Language Similarity",
      "summary": "Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.",
      "abstract": "Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.",
      "doi": "https://doi.org/10.48550/arxiv.2111.01326",
      "openalex_id": "https://openalex.org/W3211231337",
      "arxiv_id": "",
      "publication_date": "2021-11-02",
      "published": "2021-11-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on cross-lingual transfer for speech processing via acoustic similarity and does not present discrete audio token representations or tokenization details for language-model style processing, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on cross-lingual transfer for speech processing via acoustic similarity and does not present discrete audio token representations or tokenization details for language-model style processing, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on cross-lingual transfer for speech processing by leveraging acoustic language similarity to improve low-resource language systems. However, it does not mention any discrete audio token generation process such as vector quantization or similar techniques, nor does it detail any token formation or use as inputs for sequence or language models. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and lacks sufficient technical detail to evaluate token design or usage for speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on cross-lingual transfer for speech processing by leveraging acoustic language similarity to improve low-resource language systems. However, it does not mention any discrete audio token generation process such as vector quantization or similar techniques, nor does it detail any token formation or use as inputs for sequence or language models. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and lacks sufficient technical detail to evaluate token design or usage for speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Emotion Recognition in Italian Using Wav2Vec 2.0 and the Novel Crowdsourced Emotional Speech Corpus Emozionalmente",
    "abstract": "&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;",
    "metadata": {
      "title": "Speech Emotion Recognition in Italian Using Wav2Vec 2.0 and the Novel Crowdsourced Emotional Speech Corpus Emozionalmente",
      "summary": "&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;",
      "doi": "https://doi.org/10.36227/techrxiv.22821992",
      "openalex_id": "https://openalex.org/W4377088036",
      "arxiv_id": "",
      "publication_date": "2023-05-19",
      "published": "2023-05-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses speech emotion recognition using wav2vec 2.0 and a crowdsourced Italian corpus, but it does not involve discrete audio tokens or tokenization strategies, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses speech emotion recognition using wav2vec 2.0 and a crowdsourced Italian corpus, but it does not involve discrete audio tokens or tokenization strategies, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech emotion recognition using the wav2vec 2.0 model trained on an Italian emotional speech corpus, but it does not describe generating discrete audio tokens or vector quantization processes central to the inclusion criteria. It also lacks details about token formation, quantization settings, or using discrete tokens for sequence modeling, which are necessary for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech emotion recognition using the wav2vec 2.0 model trained on an Italian emotional speech corpus, but it does not describe generating discrete audio tokens or vector quantization processes central to the inclusion criteria. It also lacks details about token formation, quantization settings, or using discrete tokens for sequence modeling, which are necessary for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Building a Language-Independent Speech Scoring Assessment",
    "abstract": "Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.",
    "metadata": {
      "title": "Towards Building a Language-Independent Speech Scoring Assessment",
      "summary": "Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.",
      "abstract": "Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.",
      "doi": "https://doi.org/10.1609/aaai.v38i21.30366",
      "openalex_id": "https://openalex.org/W4393157347",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on language-independent speech scoring using multilingual representations; it does not address discrete audio token design, token formation, or quantization necessary to meet the inclusion criteria for 'Discrete Audio Tokens', so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on language-independent speech scoring using multilingual representations; it does not address discrete audio token design, token formation, or quantization necessary to meet the inclusion criteria for 'Discrete Audio Tokens', so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a language-independent speech scoring system leveraging multilingual representations from wav2vec 2.0 XLSR and force-alignment, aiming to predict pronunciation and fluency scores. However, it does not describe the generation or design of discrete audio tokens through vector quantization, residual vector quantization, or similar discrete quantization methods that produce a tokenized audio representation for language or sequence modeling, nor does it address token formation or provide technical details of token quantization as required by the inclusion criteria. It primarily targets automatic speech assessment rather than discrete audio token-based modeling or generation, thus not meeting the inclusion criteria related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a language-independent speech scoring system leveraging multilingual representations from wav2vec 2.0 XLSR and force-alignment, aiming to predict pronunciation and fluency scores. However, it does not describe the generation or design of discrete audio tokens through vector quantization, residual vector quantization, or similar discrete quantization methods that produce a tokenized audio representation for language or sequence modeling, nor does it address token formation or provide technical details of token quantization as required by the inclusion criteria. It primarily targets automatic speech assessment rather than discrete audio token-based modeling or generation, thus not meeting the inclusion criteria related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Realistic and broad-scope learning simulations: first results and challenges",
    "abstract": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
    "metadata": {
      "title": "Realistic and broad-scope learning simulations: first results and challenges",
      "summary": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "abstract": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "doi": "https://doi.org/10.1017/s0305000923000272",
      "openalex_id": "https://openalex.org/W4378619943",
      "arxiv_id": "",
      "publication_date": "2023-05-29",
      "published": "2023-05-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on language learning simulations for language acquisition and does not address Discrete Audio Tokens (tokenization, VQ/RVQ, semantic/disentanglement) nor provide the methodological details needed for token-based audio representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on language learning simulations for language acquisition and does not address Discrete Audio Tokens (tokenization, VQ/RVQ, semantic/disentanglement) nor provide the methodological details needed for token-based audio representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on language acquisition research and language learning simulations; they do not address discrete audio tokens, vector quantization, or related tokenization techniques as defined in the inclusion criteria. There is no mention of discrete token design, token generation process, or the use of such tokens for audio language modeling or sequence modeling. Therefore, the study does not meet the inclusion criteria and falls outside the domain of discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on language acquisition research and language learning simulations; they do not address discrete audio tokens, vector quantization, or related tokenization techniques as defined in the inclusion criteria. There is no mention of discrete token design, token generation process, or the use of such tokens for audio language modeling or sequence modeling. Therefore, the study does not meet the inclusion criteria and falls outside the domain of discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
    "abstract": "We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.",
    "metadata": {
      "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
      "summary": "We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.",
      "abstract": "We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1503",
      "openalex_id": "https://openalex.org/W3178584664",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此題透過 CPC 的自監督學習結合深度聚類以產生 pseudo-labels，並以聚類得到的離散表示供後續序列模型使用，屬於離散音訊 token 的設計與應用，且具學術研究之方法論內容；雖摘要未完整展現 token 生成與碼本細節，但核心貢獻指向離散表示的建立與評估，符合納入標準。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "此題透過 CPC 的自監督學習結合深度聚類以產生 pseudo-labels，並以聚類得到的離散表示供後續序列模型使用，屬於離散音訊 token 的設計與應用，且具學術研究之方法論內容；雖摘要未完整展現 token 生成與碼本細節，但核心貢獻指向離散表示的建立與評估，符合納入標準。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech representation learning combining Contrastive Predictive Coding (CPC) with deep clustering to generate phoneme-discriminative representations. It involves clustering of continuous speech embeddings to produce pseudo-labels, effectively creating a form of discrete tokenization used for further modeling. However, the abstract lacks explicit details about token generation processes such as vector quantization or a clear explanation that these clusters or labels are used as discrete audio tokens for sequence or language model understanding or generation. While it shows improvements in phonetic, lexical, and syntactic metrics, it does not fully meet the inclusion criteria requiring clear token generation mechanisms and their usage as discrete tokens for downstream tasks. Therefore, inclusion is uncertain without more detailed methodological descriptions about the tokenization strategies and their roles in downstream tasks.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech representation learning combining Contrastive Predictive Coding (CPC) with deep clustering to generate phoneme-discriminative representations. It involves clustering of continuous speech embeddings to produce pseudo-labels, effectively creating a form of discrete tokenization used for further modeling. However, the abstract lacks explicit details about token generation processes such as vector quantization or a clear explanation that these clusters or labels are used as discrete audio tokens for sequence or language model understanding or generation. While it shows improvements in phonetic, lexical, and syntactic metrics, it does not fully meet the inclusion criteria requiring clear token generation mechanisms and their usage as discrete tokens for downstream tasks. Therefore, inclusion is uncertain without more detailed methodological descriptions about the tokenization strategies and their roles in downstream tasks.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: JuniorNano recommended inclusion (4) observing this work produces pseudo-labels via clustering of CPC outputs and thus designs discrete representations; JuniorMini was cautious (3) because the abstract lacks low-level tokenization/quantization details. The paper describes using CPC embeddings clustered with k-means to produce pseudo-labels (i.e., discrete labels/tokens) and then trains an autoregressive model on those labels, with a second-round clustering to obtain phoneme-discriminative discrete representations — this fits the inclusion pathway of SSL representation + clustering to form discrete semantic tokens used for sequence modeling. Although the abstract does not spell out implementation-level codebook/quantization details, it clearly presents clustering-derived discrete labels as a core method and uses them for downstream sequence modeling, which satisfies the stated inclusion criteria (SSL + clustering → discrete tokens for modeling); it is not a demo or codec-standard paper and thus does not meet exclusion criteria. Therefore I judge it as better to include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: JuniorNano recommended inclusion (4) observing this work produces pseudo-labels via clustering of CPC outputs and thus designs discrete representations; JuniorMini was cautious (3) because the abstract lacks low-level tokenization/quantization details. The paper describes using CPC embeddings clustered with k-means to produce pseudo-labels (i.e., discrete labels/tokens) and then trains an autoregressive model on those labels, with a second-round clustering to obtain phoneme-discriminative discrete representations — this fits the inclusion pathway of SSL representation + clustering to form discrete semantic tokens used for sequence modeling. Although the abstract does not spell out implementation-level codebook/quantization details, it clearly presents clustering-derived discrete labels as a core method and uses them for downstream sequence modeling, which satisfies the stated inclusion criteria (SSL + clustering → discrete tokens for modeling); it is not a demo or codec-standard paper and thus does not meet exclusion criteria. Therefore I judge it as better to include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modeling the initial state of early phonetic learning in infants",
    "abstract": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
    "metadata": {
      "title": "Modeling the initial state of early phonetic learning in infants",
      "summary": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
      "abstract": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
      "doi": "https://doi.org/10.31234/osf.io/gc5kp",
      "openalex_id": "https://openalex.org/W4395036961",
      "arxiv_id": "",
      "publication_date": "2024-04-23",
      "published": "2024-04-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses innate factors in infant phonetic learning and modeling trajectories, but it does not address discrete audio tokens, tokenization methods, or token-based representations needed by the inclusion criteria; therefore it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses innate factors in infant phonetic learning and modeling trajectories, but it does not address discrete audio tokens, tokenization methods, or token-based representations needed by the inclusion criteria; therefore it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on modeling infant phonetic learning, discussing innate factors versus statistical learning in speech perception, but it does not address discrete audio tokens, their generation, vector quantization, or their use in sequence or language modeling, which is the core inclusion criterion. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and lacks methodological details on token formation or quantization strategies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on modeling infant phonetic learning, discussing innate factors versus statistical learning in speech perception, but it does not address discrete audio tokens, their generation, vector quantization, or their use in sequence or language modeling, which is the core inclusion criterion. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and lacks methodological details on token formation or quantization strategies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating Content-Aware Neural Text-to-Speech MOS Prediction Using Prosodic and Linguistic Features",
    "abstract": "Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are beneficial in the MOS prediction task, by improving the predicted MOS scores' correlation with the ground truths, both at utterance-level and system-level predictions.",
    "metadata": {
      "title": "Investigating Content-Aware Neural Text-to-Speech MOS Prediction Using Prosodic and Linguistic Features",
      "summary": "Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are beneficial in the MOS prediction task, by improving the predicted MOS scores' correlation with the ground truths, both at utterance-level and system-level predictions.",
      "abstract": "Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are beneficial in the MOS prediction task, by improving the predicted MOS scores' correlation with the ground truths, both at utterance-level and system-level predictions.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096255",
      "openalex_id": "https://openalex.org/W4372263384",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract focuses on MOS prediction for neural TTS using prosodic and linguistic features and does not present discrete audio token design or tokenization methods, so it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract focuses on MOS prediction for neural TTS using prosodic and linguistic features and does not present discrete audio token design or tokenization methods, so it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on MOS prediction for neural TTS systems using prosodic and linguistic features but does not involve generating or designing discrete audio tokens through vector quantization or similar mechanisms, nor does it detail a token generation process or propose a discrete token-based method for audio modeling. Hence, it does not meet the inclusion criteria related to discrete audio token design or usage as core methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on MOS prediction for neural TTS systems using prosodic and linguistic features but does not involve generating or designing discrete audio tokens through vector quantization or similar mechanisms, nor does it detail a token generation process or propose a discrete token-based method for audio modeling. Hence, it does not meet the inclusion criteria related to discrete audio token design or usage as core methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control",
    "abstract": "In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.",
    "metadata": {
      "title": "Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control",
      "summary": "In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.",
      "abstract": "In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.",
      "doi": "https://doi.org/10.21437/ssw.2021-21",
      "openalex_id": "https://openalex.org/W3193323418",
      "arxiv_id": "",
      "publication_date": "2021-08-24",
      "published": "2021-08-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a Tacotron-based rapping/singing synthesis system with phoneme-level prosody control and fine-tuning, but it does not address discrete audio tokens or tokenization methods/quantization essential to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a Tacotron-based rapping/singing synthesis system with phoneme-level prosody control and fine-tuning, but it does not address discrete audio tokens or tokenization methods/quantization essential to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a text-to-rapping/singing voice synthesis system that adapts Tacotron-based multispeaker acoustic models with prosody control at the phoneme level, but it does not mention the use or generation of discrete audio tokens through vector quantization or similar mechanisms, nor does it describe a discrete token representation intended for use in language or sequence modeling; therefore, it does not fit the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a text-to-rapping/singing voice synthesis system that adapts Tacotron-based multispeaker acoustic models with prosody control at the phoneme level, but it does not mention the use or generation of discrete audio tokens through vector quantization or similar mechanisms, nor does it describe a discrete token representation intended for use in language or sequence modeling; therefore, it does not fit the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep learning in electron microscopy",
    "abstract": "Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.",
    "metadata": {
      "title": "Deep learning in electron microscopy",
      "summary": "Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.",
      "abstract": "Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.",
      "doi": "https://doi.org/10.1088/2632-2153/abd614",
      "openalex_id": "https://openalex.org/W3084979415",
      "arxiv_id": "",
      "publication_date": "2020-12-22",
      "published": "2020-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study topic is deep learning in electron microscopy, which is not about discrete audio tokens or token-based audio representations; it does not meet the stated inclusion criteria and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study topic is deep learning in electron microscopy, which is not about discrete audio tokens or token-based audio representations; it does not meet the stated inclusion criteria and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on deep learning applications in electron microscopy, which is unrelated to the defined topic of discrete audio tokens that involve audio signal discretization and language model integration. There is no mention of audio, tokenization, vector quantization, or related methods, and thus it does not meet any inclusion criteria nor is it relevant to the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on deep learning applications in electron microscopy, which is unrelated to the defined topic of discrete audio tokens that involve audio signal discretization and language model integration. There is no mention of audio, tokenization, vector quantization, or related methods, and thus it does not meet any inclusion criteria nor is it relevant to the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them\\n on Images",
    "abstract": "We present a hierarchical VAE that, for the first time, generates samples\\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\\nautoregressive models, as well as faster, better models if they exist, when\\nmade sufficiently deep. Despite this, autoregressive models have historically\\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\\nby scaling a VAE to greater stochastic depth than previously explored and\\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\\nsamples thousands of times faster, and are more easily applied to\\nhigh-resolution images. Qualitative studies suggest this is because the VAE\\nlearns efficient hierarchical visual representations. We release our source\\ncode and models at https://github.com/openai/vdvae.\\n",
    "metadata": {
      "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them\\n on Images",
      "summary": "We present a hierarchical VAE that, for the first time, generates samples\\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\\nautoregressive models, as well as faster, better models if they exist, when\\nmade sufficiently deep. Despite this, autoregressive models have historically\\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\\nby scaling a VAE to greater stochastic depth than previously explored and\\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\\nsamples thousands of times faster, and are more easily applied to\\nhigh-resolution images. Qualitative studies suggest this is because the VAE\\nlearns efficient hierarchical visual representations. We release our source\\ncode and models at https://github.com/openai/vdvae.\\n",
      "abstract": "We present a hierarchical VAE that, for the first time, generates samples\\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\\nautoregressive models, as well as faster, better models if they exist, when\\nmade sufficiently deep. Despite this, autoregressive models have historically\\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\\nby scaling a VAE to greater stochastic depth than previously explored and\\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\\nsamples thousands of times faster, and are more easily applied to\\nhigh-resolution images. Qualitative studies suggest this is because the VAE\\nlearns efficient hierarchical visual representations. We release our source\\ncode and models at https://github.com/openai/vdvae.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2011.10650",
      "openalex_id": "https://openalex.org/W3120243996",
      "arxiv_id": "",
      "publication_date": "2020-11-20",
      "published": "2020-11-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a hierarchical VAE for images outperforming autoregressive models, with no discussion of discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a hierarchical VAE for images outperforming autoregressive models, with no discussion of discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on deep Variational Autoencoders (VAEs) for image generation, comparing them to autoregressive models. It does not address discrete audio tokens, audio representation, or any audio-related quantization or tokenization mechanism as required by the inclusion criteria. Therefore, it falls outside the defined scope concerning discrete audio token design and use for language or sequence modeling of audio data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on deep Variational Autoencoders (VAEs) for image generation, comparing them to autoregressive models. It does not address discrete audio tokens, audio representation, or any audio-related quantization or tokenization mechanism as required by the inclusion criteria. Therefore, it falls outside the defined scope concerning discrete audio token design and use for language or sequence modeling of audio data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "POP909: A Pop-song Dataset for Music Arrangement Generation",
    "abstract": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
    "metadata": {
      "title": "POP909: A Pop-song Dataset for Music Arrangement Generation",
      "summary": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
      "abstract": "Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
      "doi": "https://doi.org/10.48550/arxiv.2008.07142",
      "openalex_id": "https://openalex.org/W3049247973",
      "arxiv_id": "",
      "publication_date": "2020-08-17",
      "published": "2020-08-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a dataset for music arrangement generation with MIDI data and annotations, but it does not present discrete audio token design, token generation, or related methodology required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a dataset for music arrangement generation with MIDI data and annotations, but it does not present discrete audio token design, token generation, or related methodology required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a dataset for music arrangement generation, providing MIDI data, annotations, and baseline experiments. However, it does not describe any method for converting continuous audio signals into discrete audio tokens via vector quantization or semantic-driven discretization. There is no focus on generating or utilizing discrete audio tokens for sequence or language modeling, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria focused on discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a dataset for music arrangement generation, providing MIDI data, annotations, and baseline experiments. However, it does not describe any method for converting continuous audio signals into discrete audio tokens via vector quantization or semantic-driven discretization. There is no focus on generating or utilizing discrete audio tokens for sequence or language modeling, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria focused on discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Convergence of Adam and Adagrad",
    "abstract": "We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\\ell_\\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.",
    "metadata": {
      "title": "On the Convergence of Adam and Adagrad",
      "summary": "We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\\ell_\\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.",
      "abstract": "We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\\ell_\\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\\alpha=1/\\sqrt{N}$ and a momentum parameter on squared gradients $\\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\\ln(N)/\\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3009948090",
      "arxiv_id": "",
      "publication_date": "2020-03-05",
      "published": "2020-03-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on mathematical convergence of optimization algorithms (Adam/Adagrad) rather than discrete audio token design or token-based audio modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on mathematical convergence of optimization algorithms (Adam/Adagrad) rather than discrete audio token design or token-based audio modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on theoretical convergence proofs for the optimization algorithms Adam and Adagrad, without discussing discrete audio tokens, vector quantization, or audio language modeling; thus it does not meet the inclusion criteria relating to discrete audio token generation or usage, nor does it evidence relevant methodology or applications for audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on theoretical convergence proofs for the optimization algorithms Adam and Adagrad, without discussing discrete audio tokens, vector quantization, or audio language modeling; thus it does not meet the inclusion criteria relating to discrete audio token generation or usage, nor does it evidence relevant methodology or applications for audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stochastic Restoration of Heavily Compressed Musical Audio Using Generative Adversarial Networks",
    "abstract": "Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.",
    "metadata": {
      "title": "Stochastic Restoration of Heavily Compressed Musical Audio Using Generative Adversarial Networks",
      "summary": "Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.",
      "abstract": "Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.",
      "doi": "https://doi.org/10.3390/electronics10111349",
      "openalex_id": "https://openalex.org/W3169882120",
      "arxiv_id": "",
      "publication_date": "2021-06-05",
      "published": "2021-06-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on GAN-based restoration of heavily compressed music and does not discuss discrete audio tokens, vector quantization, or token-based modeling, thus it fails the discrete-audio-token inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on GAN-based restoration of heavily compressed music and does not discuss discrete audio tokens, vector quantization, or token-based modeling, thus it fails the discrete-audio-token inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the restoration of heavily compressed musical audio using GANs but does not mention the generation or use of discrete audio tokens through vector quantization or related methods as defined in the inclusion criteria; it primarily deals with enhancement of MP3 compressed audio, which is a traditional codec, not involving discrete token design or usage for language or sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the restoration of heavily compressed musical audio using GANs but does not mention the generation or use of discrete audio tokens through vector quantization or related methods as defined in the inclusion criteria; it primarily deals with enhancement of MP3 compressed audio, which is a traditional codec, not involving discrete token design or usage for language or sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based\\n Models",
    "abstract": "Energy-based models (EBMs) have recently been successful in representing\\ncomplex distributions of small images. However, sampling from them requires\\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\\nsamples quickly and are equipped with a latent space that enables fast\\ntraversal of the data manifold. However, VAEs tend to assign high probability\\ndensity to regions in data space outside the actual data distribution and often\\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\\ncaptures the overall mode structure of the data distribution using a\\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\\nnon-data-like regions from the model and refine the image samples. Moreover,\\nthe VAE component in VAEBM allows us to speed up MCMC updates by\\nreparameterizing them in the VAE's latent space. Our experimental results show\\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\\nseveral benchmark image datasets by a large margin. It can generate\\nhigh-quality images as large as 256$\\\\times$256 pixels with short MCMC chains.\\nWe also demonstrate that VAEBM provides complete mode coverage and performs\\nwell in out-of-distribution detection. The source code is available at\\nhttps://github.com/NVlabs/VAEBM\\n",
    "metadata": {
      "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based\\n Models",
      "summary": "Energy-based models (EBMs) have recently been successful in representing\\ncomplex distributions of small images. However, sampling from them requires\\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\\nsamples quickly and are equipped with a latent space that enables fast\\ntraversal of the data manifold. However, VAEs tend to assign high probability\\ndensity to regions in data space outside the actual data distribution and often\\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\\ncaptures the overall mode structure of the data distribution using a\\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\\nnon-data-like regions from the model and refine the image samples. Moreover,\\nthe VAE component in VAEBM allows us to speed up MCMC updates by\\nreparameterizing them in the VAE's latent space. Our experimental results show\\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\\nseveral benchmark image datasets by a large margin. It can generate\\nhigh-quality images as large as 256$\\\\times$256 pixels with short MCMC chains.\\nWe also demonstrate that VAEBM provides complete mode coverage and performs\\nwell in out-of-distribution detection. The source code is available at\\nhttps://github.com/NVlabs/VAEBM\\n",
      "abstract": "Energy-based models (EBMs) have recently been successful in representing\\ncomplex distributions of small images. However, sampling from them requires\\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\\nsamples quickly and are equipped with a latent space that enables fast\\ntraversal of the data manifold. However, VAEs tend to assign high probability\\ndensity to regions in data space outside the actual data distribution and often\\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\\ncaptures the overall mode structure of the data distribution using a\\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\\nnon-data-like regions from the model and refine the image samples. Moreover,\\nthe VAE component in VAEBM allows us to speed up MCMC updates by\\nreparameterizing them in the VAE's latent space. Our experimental results show\\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\\nseveral benchmark image datasets by a large margin. It can generate\\nhigh-quality images as large as 256$\\\\times$256 pixels with short MCMC chains.\\nWe also demonstrate that VAEBM provides complete mode coverage and performs\\nwell in out-of-distribution detection. The source code is available at\\nhttps://github.com/NVlabs/VAEBM\\n",
      "doi": "https://doi.org/10.48550/arxiv.2010.00654",
      "openalex_id": "https://openalex.org/W3118605064",
      "arxiv_id": "",
      "publication_date": "2020-10-01",
      "published": "2020-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses VAEs/EBMs for image generation, not discrete audio tokens or audio tokenization, so it does not meet the stated inclusion criteria focused on discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses VAEs/EBMs for image generation, not discrete audio tokens or audio tokenization, so it does not meet the stated inclusion criteria focused on discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on the integration of Variational Autoencoders and Energy-based Models for image generation tasks, without any mention or consideration of discrete audio tokens, audio signal processing, or representations related to audio language modeling or cross-modal audio-text generation. Hence, it does not meet any inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on the integration of Variational Autoencoders and Energy-based Models for image generation tasks, without any mention or consideration of discrete audio tokens, audio signal processing, or representations related to audio language modeling or cross-modal audio-text generation. Hence, it does not meet any inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Environmental Sound Classification with Tiny Transformers in Noisy Edge Environments",
    "abstract": "The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.",
    "metadata": {
      "title": "Environmental Sound Classification with Tiny Transformers in Noisy Edge Environments",
      "summary": "The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.",
      "abstract": "The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.",
      "doi": "https://doi.org/10.1109/wf-iot51360.2021.9596007",
      "openalex_id": "https://openalex.org/W3212947338",
      "arxiv_id": "",
      "publication_date": "2021-06-14",
      "published": "2021-06-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on Environmental Sound Classification with a tiny Transformer for edge environments and does not involve discrete audio tokens or token-based representations (e.g., VQ/RVQ, semantic tokenization) as the main contribution, so it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on Environmental Sound Classification with a tiny Transformer for edge environments and does not involve discrete audio tokens or token-based representations (e.g., VQ/RVQ, semantic tokenization) as the main contribution, so it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on environmental sound classification using a tiny Transformer on noisy edge devices, but it does not discuss or propose any discrete audio tokens or their generation processes such as vector quantization or semantic disentanglement, nor does it mention treating audio as discrete tokens for language/sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on environmental sound classification using a tiny Transformer on noisy edge devices, but it does not discuss or propose any discrete audio tokens or their generation processes such as vector quantization or semantic disentanglement, nor does it mention treating audio as discrete tokens for language/sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "One Billion Audio Sounds from GPU-Enabled Modular Synthesis",
    "abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.",
    "metadata": {
      "title": "One Billion Audio Sounds from GPU-Enabled Modular Synthesis",
      "summary": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.",
      "abstract": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.",
      "doi": "https://doi.org/10.23919/dafx51585.2021.9768246",
      "openalex_id": "https://openalex.org/W3159239022",
      "arxiv_id": "",
      "publication_date": "2021-09-08",
      "published": "2021-09-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on large-scale synthetic audio datasets and a GPU-enabled modular synthesizer; it does not address discrete audio tokens, tokenization methods, or VQ/RVQ-based representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on large-scale synthetic audio datasets and a GPU-enabled modular synthesizer; it does not address discrete audio tokens, tokenization methods, or VQ/RVQ-based representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The dataset and tools released are focused on generating a large amount of synthesized audio and evaluating representations, but the abstract lacks mention of the production or use of discrete audio tokens via vector quantization, residual quantization, or similar methods tailored for sequence or language model processing. There is no clear description of discrete token generation processes, quantization, or their use for audio language modeling or downstream semantic tasks. Hence, it doesn't meet key inclusion criteria about discrete audio token design and usage as a representation for sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The dataset and tools released are focused on generating a large amount of synthesized audio and evaluating representations, but the abstract lacks mention of the production or use of discrete audio tokens via vector quantization, residual quantization, or similar methods tailored for sequence or language model processing. There is no clear description of discrete token generation processes, quantization, or their use for audio language modeling or downstream semantic tasks. Hence, it doesn't meet key inclusion criteria about discrete audio token design and usage as a representation for sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "AI song contest: Human-AI co-creation in songwriting",
    "abstract": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",
    "metadata": {
      "title": "AI song contest: Human-AI co-creation in songwriting",
      "summary": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",
      "abstract": "Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",
      "doi": "https://doi.org/10.5281/zenodo.4245530",
      "openalex_id": "https://openalex.org/W3092135915",
      "arxiv_id": "",
      "publication_date": "2020-10-11",
      "published": "2020-10-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses human–AI co-creation in songwriting and interface design, not discrete audio token representations or tokenization methods required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses human–AI co-creation in songwriting and interface design, not discrete audio token representations or tokenization methods required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on AI-assisted songwriting and the challenges musicians face when co-creating music with AI generative models, but it does not describe the generation of discrete audio tokens, token quantization methods, or their use in sequence models, which are core to the inclusion criteria; it lacks methodological details related to token formation or neural audio codec-based discrete token representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on AI-assisted songwriting and the challenges musicians face when co-creating music with AI generative models, but it does not describe the generation of discrete audio tokens, token quantization methods, or their use in sequence models, which are core to the inclusion criteria; it lacks methodological details related to token formation or neural audio codec-based discrete token representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks",
    "abstract": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
    "metadata": {
      "title": "DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks",
      "summary": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
      "abstract": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",
      "doi": "https://doi.org/10.5281/zenodo.4245503",
      "openalex_id": "https://openalex.org/W3081378361",
      "arxiv_id": "",
      "publication_date": "2020-10-11",
      "published": "2020-10-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The DrumGAN paper focuses on GAN-based drum-sound synthesis conditioned on perceptual features and does not involve discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or token-based sequence modeling, so it fails the discrete audio tokens inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The DrumGAN paper focuses on GAN-based drum-sound synthesis conditioned on perceptual features and does not involve discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or token-based sequence modeling, so it fails the discrete audio tokens inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on synthesizing drum sounds using GANs conditioned on perceptual features, but it does not discuss or propose a method involving discrete audio tokens, vector quantization, or semantic tokenization as required by the inclusion criteria. The described approach is based on continuous feature conditioning rather than generating or using discrete tokens suitable for language or sequence modeling, and does not provide token generation details or quantization mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on synthesizing drum sounds using GANs conditioned on perceptual features, but it does not discuss or propose a method involving discrete audio tokens, vector quantization, or semantic tokenization as required by the inclusion criteria. The described approach is based on continuous feature conditioning rather than generating or using discrete tokens suitable for language or sequence modeling, and does not provide token generation details or quantization mechanisms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Generative Models in Engineering Design: A Review",
    "abstract": "Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.",
    "metadata": {
      "title": "Deep Generative Models in Engineering Design: A Review",
      "summary": "Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.",
      "abstract": "Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.",
      "doi": "https://doi.org/10.1115/1.4053859",
      "openalex_id": "https://openalex.org/W3206790237",
      "arxiv_id": "",
      "publication_date": "2022-02-16",
      "published": "2022-02-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item covers deep generative models in engineering design; no discussion of discrete audio tokens, tokenization, or audio-language modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item covers deep generative models in engineering design; no discussion of discrete audio tokens, tokenization, or audio-language modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on deep generative models applied in engineering design, without any indication or mention of discrete audio tokens, vector quantization for audio, or audio signal processing related to transforming continuous audio into discrete tokens for language modelling or generation tasks. Thus, it does not meet the inclusion criteria focused on discrete audio token research and clearly does not align with the relevant domain or methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on deep generative models applied in engineering design, without any indication or mention of discrete audio tokens, vector quantization for audio, or audio signal processing related to transforming continuous audio into discrete tokens for language modelling or generation tasks. Thus, it does not meet the inclusion criteria focused on discrete audio token research and clearly does not align with the relevant domain or methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BassNet: A Variational Gated Autoencoder for Conditional Generation of Bass Guitar Tracks with Learned Interactive Control",
    "abstract": "Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.",
    "metadata": {
      "title": "BassNet: A Variational Gated Autoencoder for Conditional Generation of Bass Guitar Tracks with Learned Interactive Control",
      "summary": "Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.",
      "abstract": "Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.",
      "doi": "https://doi.org/10.3390/app10186627",
      "openalex_id": "https://openalex.org/W3088329082",
      "arxiv_id": "",
      "publication_date": "2020-09-22",
      "published": "2020-09-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a variational autoencoder for generating bass guitar tracks with interactive control, but it does not discuss discrete audio tokens, vector quantization, or token-based representations required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a variational autoencoder for generating bass guitar tracks with interactive control, but it does not discuss discrete audio tokens, vector quantization, or token-based representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a deep learning model (BassNet) for conditional generation of bass guitar tracks using a variational gated autoencoder with learned interactive control. The abstract does not mention the use of discrete audio tokens or any vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization methods to convert continuous audio signals into discrete token representations. There is also no mention of these tokens being used for sequence or language modeling for audio understanding or generation, which is a core requirement based on the inclusion criteria. Therefore, it does not meet the inclusion criteria regarding discrete audio token design and usage, and does not provide adequate details about token generation or quantization approaches. There is also no indication of violating exclusion criteria but lacking inclusion criteria is sufficient to exclude.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a deep learning model (BassNet) for conditional generation of bass guitar tracks using a variational gated autoencoder with learned interactive control. The abstract does not mention the use of discrete audio tokens or any vector quantization, residual vector quantization, hierarchical quantization, or semantic-driven discretization methods to convert continuous audio signals into discrete token representations. There is also no mention of these tokens being used for sequence or language modeling for audio understanding or generation, which is a core requirement based on the inclusion criteria. Therefore, it does not meet the inclusion criteria regarding discrete audio token design and usage, and does not provide adequate details about token generation or quantization approaches. There is also no indication of violating exclusion criteria but lacking inclusion criteria is sufficient to exclude.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Symbolic Music Generation with Diffusion Models",
    "abstract": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
    "metadata": {
      "title": "Symbolic Music Generation with Diffusion Models",
      "summary": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
      "abstract": "Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",
      "doi": "https://doi.org/10.5281/zenodo.5624362",
      "openalex_id": "https://openalex.org/W3148695041",
      "arxiv_id": "",
      "publication_date": "2021-11-07",
      "published": "2021-11-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item uses diffusion on symbolic music via the continuous latent space of a pre-trained VAE rather than focusing on discrete audio token design or explicit token quantization; therefore it does not meet the inclusion criterion that requires discrete audio tokens as the main research contribution.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item uses diffusion on symbolic music via the continuous latent space of a pre-trained VAE rather than focusing on discrete audio token design or explicit token quantization; therefore it does not meet the inclusion criterion that requires discrete audio tokens as the main research contribution.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses symbolic music generation using diffusion models with continuous latent embeddings from a variational autoencoder, focusing on sequence generation in a continuous space rather than discrete tokenization of audio signals such as vector quantization-based discrete audio tokens; it does not clearly describe producing discrete token representations for audio that are used for language or sequence modeling, and the methodology is centered on latent continuous embeddings, which do not meet the inclusion criteria's emphasis on discrete audio tokens and their generation processes.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses symbolic music generation using diffusion models with continuous latent embeddings from a variational autoencoder, focusing on sequence generation in a continuous space rather than discrete tokenization of audio signals such as vector quantization-based discrete audio tokens; it does not clearly describe producing discrete token representations for audio that are used for language or sequence modeling, and the methodology is centered on latent continuous embeddings, which do not meet the inclusion criteria's emphasis on discrete audio tokens and their generation processes.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Transformer Neural Networks for Automated Rhythm Generation",
    "abstract": "Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.",
    "metadata": {
      "title": "Transformer Neural Networks for Automated Rhythm Generation",
      "summary": "Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.",
      "abstract": "Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.",
      "doi": "https://doi.org/10.21428/92fbeb44.fe9a0d82",
      "openalex_id": "https://openalex.org/W3185297410",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns symbolic MIDI rhythm generation with Transformer-XL and does not address discrete audio token representations or tokenization mechanisms, and it also lacks a publication date, thus failing the inclusion criteria and triggering an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns symbolic MIDI rhythm generation with Transformer-XL and does not address discrete audio token representations or tokenization mechanisms, and it also lacks a publication date, thus failing the inclusion criteria and triggering an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Transformer-XL model for automated symbolic rhythm generation using a MIDI dataset, but does not mention any discrete audio tokens, vector quantization, or related tokenization methods as defined in the inclusion criteria, nor does it describe token generation processes or usage for sequence modeling of discrete audio tokens; therefore, it does not meet the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Transformer-XL model for automated symbolic rhythm generation using a MIDI dataset, but does not mention any discrete audio tokens, vector quantization, or related tokenization methods as defined in the inclusion criteria, nor does it describe token generation processes or usage for sequence modeling of discrete audio tokens; therefore, it does not meet the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders",
    "abstract": "The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.",
    "metadata": {
      "title": "Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders",
      "summary": "The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.",
      "abstract": "The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.",
      "doi": "https://doi.org/10.18653/v1/2020.inlg-1.42",
      "openalex_id": "https://openalex.org/W3112789166",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text-based rap lyric generation using a denoising autoencoder and paraphrasing to adjust rhyme, with no discussion of discrete audio tokens, vector-quantization, or audio-language modelling; therefore it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text-based rap lyric generation using a denoising autoencoder and paraphrasing to adjust rhyme, with no discussion of discrete audio tokens, vector-quantization, or audio-language modelling; therefore it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generating rap lyrics using a Transformer-based denoising autoencoder and BERT-based paraphrasing for rhyme enhancement, but it does not discuss discrete audio tokenization or any vector quantization methods related to converting continuous audio into discrete tokens, nor does it describe token generation procedures or usage for audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and their usage as main contributions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generating rap lyrics using a Transformer-based denoising autoencoder and BERT-based paraphrasing for rhyme enhancement, but it does not discuss discrete audio tokenization or any vector quantization methods related to converting continuous audio into discrete tokens, nor does it describe token generation procedures or usage for audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and their usage as main contributions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Watching a Language Model Learning Chess",
    "abstract": "We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.",
    "metadata": {
      "title": "Watching a Language Model Learning Chess",
      "summary": "We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.",
      "abstract": "We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.",
      "doi": "https://doi.org/10.26615/978-954-452-072-4_153",
      "openalex_id": "https://openalex.org/W3211354215",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies how a language model learns chess from text data, focusing on model capacity, training data, and how information is stored, with no discussion of discrete audio tokens or audio tokenization methods; it does not meet the inclusion criteria centered on discrete audio token research, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies how a language model learns chess from text data, focusing on model capacity, training data, and how information is stored, with no discussion of discrete audio tokens or audio tokenization methods; it does not meet the inclusion criteria centered on discrete audio token research, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a study on how a language model learns chess from text data, focusing on model capacity, training data, and neuron activations related to board states. There is no indication that the study involves discrete audio tokens or methods related to audio tokenization, vector quantization, or audio language modeling. The content is unrelated to audio signal processing or discrete audio token design, and therefore does not meet the inclusion criteria. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a study on how a language model learns chess from text data, focusing on model capacity, training data, and neuron activations related to board states. There is no indication that the study involves discrete audio tokens or methods related to audio tokenization, vector quantization, or audio language modeling. The content is unrelated to audio signal processing or discrete audio token design, and therefore does not meet the inclusion criteria. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Crispr2vec: Machine Learning Model Predicts Off-Target Cuts of CRISPR systems",
    "abstract": "1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.",
    "metadata": {
      "title": "Crispr2vec: Machine Learning Model Predicts Off-Target Cuts of CRISPR systems",
      "summary": "1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.",
      "abstract": "1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.",
      "doi": "https://doi.org/10.1101/2020.10.28.359885",
      "openalex_id": "https://openalex.org/W3097306158",
      "arxiv_id": "",
      "publication_date": "2020-10-29",
      "published": "2020-10-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses CRISPR off-target prediction using a deep learning model; it has no relation to discrete audio tokens or audio tokenization, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses CRISPR off-target prediction using a deep learning model; it has no relation to discrete audio tokens or audio tokenization, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on CRISPR off-target prediction using deep metric learning, which is a genetic editing and bioinformatics topic unrelated to discrete audio tokens, audio representation, or any aspects of audio signal processing or modeling; thus, it does not meet any inclusion criteria related to audio token generation or usage and does not involve methods such as vector quantization or audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on CRISPR off-target prediction using deep metric learning, which is a genetic editing and bioinformatics topic unrelated to discrete audio tokens, audio representation, or any aspects of audio signal processing or modeling; thus, it does not meet any inclusion criteria related to audio token generation or usage and does not involve methods such as vector quantization or audio language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures",
    "abstract": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
    "metadata": {
      "title": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures",
      "summary": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
      "abstract": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
      "doi": "https://doi.org/10.48550/arxiv.2006.12878",
      "openalex_id": "https://openalex.org/W3036165773",
      "arxiv_id": "",
      "publication_date": "2020-06-23",
      "published": "2020-06-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns Direct Feedback Alignment and training without weight transport across DL tasks, not about discrete audio tokens or tokenization methods; it fails the specified inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns Direct Feedback Alignment and training without weight transport across DL tasks, not about discrete audio tokens or tokenization methods; it fails the specified inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Direct Feedback Alignment in deep learning architectures, without any mention of discrete audio tokens, vector quantization, or audio-related tokenization methods required by the inclusion criteria. It does not address audio signal processing, token generation for language or sequence modeling, or any relevant audio tasks, thus it fails to meet the thematic requirements and core methodological focus related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Direct Feedback Alignment in deep learning architectures, without any mention of discrete audio tokens, vector quantization, or audio-related tokenization methods required by the inclusion criteria. It does not address audio signal processing, token generation for language or sequence modeling, or any relevant audio tasks, thus it fails to meet the thematic requirements and core methodological focus related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Upsampling Layers for Music Source Separation",
    "abstract": "Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.",
    "metadata": {
      "title": "Upsampling Layers for Music Source Separation",
      "summary": "Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.",
      "abstract": "Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10289768",
      "openalex_id": "https://openalex.org/W3216722162",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates upsampling layers for music source separation with artifacts discussion, but it does not address discrete audio tokens or tokenization methods, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates upsampling layers for music source separation with artifacts discussion, but it does not address discrete audio tokens or tokenization methods, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on upsampling layers for music source separation and evaluates different upsampling methods and their effects on audio quality and model performance. However, it does not mention or involve generating discrete audio tokens via vector quantization or similar methods, nor does it focus on token design or use in language or sequence modeling. Therefore, it does not meet the inclusion criteria which require discrete token design or generation as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on upsampling layers for music source separation and evaluates different upsampling methods and their effects on audio quality and model performance. However, it does not mention or involve generating discrete audio tokens via vector quantization or similar methods, nor does it focus on token design or use in language or sequence modeling. Therefore, it does not meet the inclusion criteria which require discrete token design or generation as a core contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "What to Play and How to Play it: Guiding Generative Music Models with Multiple Demonstrations",
    "abstract": "We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both \"what to play\" (via scores in MIDI format) and \"how to play it\" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.",
    "metadata": {
      "title": "What to Play and How to Play it: Guiding Generative Music Models with Multiple Demonstrations",
      "summary": "We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both \"what to play\" (via scores in MIDI format) and \"how to play it\" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.",
      "abstract": "We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both \"what to play\" (via scores in MIDI format) and \"how to play it\" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.",
      "doi": "https://doi.org/10.21428/92fbeb44.06e2d5f4",
      "openalex_id": "https://openalex.org/W3209221083",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes guiding generative music models via multiple demonstrations (MIDI and rhythmic inputs) for co-created drum loops, but it does not address discrete audio tokens, token generation/quantization, or token-based representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes guiding generative music models via multiple demonstrations (MIDI and rhythmic inputs) for co-created drum loops, but it does not address discrete audio tokens, token generation/quantization, or token-based representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on guiding generative music models using multiple user-provided demonstrations, emphasizing musical characteristics and human-AI co-creation of drum loops, but does not mention discrete audio token design, vector quantization, token generation process, or use of tokens for language or sequence modeling, which are core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on guiding generative music models using multiple user-provided demonstrations, emphasizing musical characteristics and human-AI co-creation of drum loops, but does not mention discrete audio token design, vector quantization, token generation process, or use of tokens for language or sequence modeling, which are core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models",
    "abstract": "Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",
    "metadata": {
      "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models",
      "summary": "Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",
      "abstract": "Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",
      "doi": "https://doi.org/10.5281/zenodo.5624597",
      "openalex_id": "https://openalex.org/W3189022627",
      "arxiv_id": "",
      "publication_date": "2021-11-07",
      "published": "2021-11-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work presents a dataset of tokenized GuitarPro scores and an encoder/decoder to convert GuitarPro to tokens for sequence models; however, it targets symbolic music notation rather than discrete audio tokens derived from audio signals via quantization or SSL, so it does not meet the 'Discrete Audio Tokens' inclusion criteria (which require discretized audio representations for audio-language modeling); thus better to exclude.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work presents a dataset of tokenized GuitarPro scores and an encoder/decoder to convert GuitarPro to tokens for sequence models; however, it targets symbolic music notation rather than discrete audio tokens derived from audio signals via quantization or SSL, so it does not meet the 'Discrete Audio Tokens' inclusion criteria (which require discretized audio representations for audio-language modeling); thus better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The dataset and model presented focus on tokenized GuitarPro music scores for sequence models, but these tokens represent symbolic music notation events rather than discretized audio tokens derived from continuous audio signals via quantization or learned representation methods. Therefore, it does not meet the inclusion criteria of discrete audio tokens generated from continuous audio signals for language or sequence models, as it deals with symbolic music data rather than audio data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The dataset and model presented focus on tokenized GuitarPro music scores for sequence models, but these tokens represent symbolic music notation events rather than discretized audio tokens derived from continuous audio signals via quantization or learned representation methods. Therefore, it does not meet the inclusion criteria of discrete audio tokens generated from continuous audio signals for language or sequence models, as it deals with symbolic music data rather than audio data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis With GANs",
    "abstract": "Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called \"soft labels\") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",
    "metadata": {
      "title": "DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis With GANs",
      "summary": "Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called \"soft labels\") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",
      "abstract": "Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called \"soft labels\") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",
      "doi": "https://doi.org/10.5281/zenodo.5624507",
      "openalex_id": "https://openalex.org/W3191340970",
      "arxiv_id": "",
      "publication_date": "2021-11-07",
      "published": "2021-11-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work does not involve discrete audio tokens or tokenization; it focuses on distilling knowledge from an audio tagging system into a GAN-based synthesizer, which does not meet the discrete audio token criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work does not involve discrete audio tokens or tokenization; it focuses on distilling knowledge from an audio tagging system into a GAN-based synthesizer, which does not meet the discrete audio token criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio synthesis using GANs and knowledge distillation from an audio tagging system, but does not describe producing discrete audio tokens through vector quantization or other quantization mechanisms crucial for sequence modeling, nor does it clarify the process of token generation or usage for language model-based understanding or generation. Therefore, it does not meet the core inclusion criteria regarding discrete audio token representation development.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio synthesis using GANs and knowledge distillation from an audio tagging system, but does not describe producing discrete audio tokens through vector quantization or other quantization mechanisms crucial for sequence modeling, nor does it clarify the process of token generation or usage for language model-based understanding or generation. Therefore, it does not meet the core inclusion criteria regarding discrete audio token representation development.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Geometry-Free View Synthesis: Transformers and no 3D Priors",
    "abstract": "Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn",
    "metadata": {
      "title": "Geometry-Free View Synthesis: Transformers and no 3D Priors",
      "summary": "Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn",
      "abstract": "Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn",
      "doi": "https://doi.org/10.1109/iccv48922.2021.01409",
      "openalex_id": "https://openalex.org/W3154159596",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses geometry-free view synthesis in computer vision using transformers, with no discussion of discrete audio tokens, quantization, or audio-language modeling; it does not meet the audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses geometry-free view synthesis in computer vision using transformers, with no discussion of discrete audio tokens, quantization, or audio-language modeling; it does not meet the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on geometry-free view synthesis using transformers without any 3D priors, which is a computer vision and 3D image synthesis topic; it does not mention audio signals, discrete audio tokens, vector quantization related to audio, or any audio representation or modeling techniques. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their generation methods, nor does it align with the scope of transforming audio signals into discrete tokens for downstream tasks in audio language modeling or audio understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on geometry-free view synthesis using transformers without any 3D priors, which is a computer vision and 3D image synthesis topic; it does not mention audio signals, discrete audio tokens, vector quantization related to audio, or any audio representation or modeling techniques. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their generation methods, nor does it align with the scope of transforming audio signals into discrete tokens for downstream tasks in audio language modeling or audio understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical timbre-painting and articulation generation",
    "abstract": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
    "metadata": {
      "title": "Hierarchical timbre-painting and articulation generation",
      "summary": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
      "abstract": "We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",
      "doi": "https://doi.org/10.5281/zenodo.4245584",
      "openalex_id": "https://openalex.org/W3082087924",
      "arxiv_id": "",
      "publication_date": "2020-10-11",
      "published": "2020-10-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract targets timbre transfer and articulation generation using source-filtering networks without mentioning discrete audio tokens, tokenization methods (VQ/RVQ), or token-based modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract targets timbre transfer and articulation generation using source-filtering networks without mentioning discrete audio tokens, tokenization methods (VQ/RVQ), or token-based modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fast and high-fidelity music generation using specified continuous features (f0 and loudness) and learned source-filtering networks to reconstruct audio signals, but it does not mention the use of discrete audio tokens, vector quantization, or any discrete tokenization mechanisms to represent audio for language or sequence modeling; thus, it does not meet the inclusion criteria that require discrete token-based representation as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fast and high-fidelity music generation using specified continuous features (f0 and loudness) and learned source-filtering networks to reconstruct audio signals, but it does not mention the use of discrete audio tokens, vector quantization, or any discrete tokenization mechanisms to represent audio for language or sequence modeling; thus, it does not meet the inclusion criteria that require discrete token-based representation as a core contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
    "metadata": {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "summary": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
      "doi": "https://doi.org/10.48550/arxiv.2103.00020",
      "openalex_id": "https://openalex.org/W3135367836",
      "arxiv_id": "",
      "publication_date": "2021-02-26",
      "published": "2021-02-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns learning transferable visual models from natural language supervision (CLIP) using image-caption data, which does not address discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns learning transferable visual models from natural language supervision (CLIP) using image-caption data, which does not address discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning visual models from natural language supervision for image and text pairs and does not address discrete audio tokens or audio signal processing, which is the core inclusion criterion; it lacks any token generation process or use of discrete audio tokens for sequence models in audio contexts.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning visual models from natural language supervision for image and text pairs and does not address discrete audio tokens or audio signal processing, which is the core inclusion criterion; it lacks any token generation process or use of discrete audio tokens for sequence models in audio contexts.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diffusion Models Beat GANs on Image Synthesis",
    "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
    "metadata": {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "summary": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
      "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
      "doi": "https://doi.org/10.48550/arxiv.2105.05233",
      "openalex_id": "https://openalex.org/W3162926177",
      "arxiv_id": "",
      "publication_date": "2021-05-11",
      "published": "2021-05-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on diffusion models for image synthesis and classifier guidance, with no discussion of discrete audio tokens or token-based audio representations, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on diffusion models for image synthesis and classifier guidance, with no discussion of discrete audio tokens or token-based audio representations, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on diffusion models for image synthesis and do not discuss any topics related to discrete audio tokens or audio processing methods as defined in the inclusion criteria. No methodology or tokenization related to audio is presented, and the study centers on image generation models, which excludes it based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on diffusion models for image synthesis and do not discuss any topics related to discrete audio tokens or audio processing methods as defined in the inclusion criteria. No methodology or tokenization related to audio is presented, and the study centers on image generation models, which excludes it based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Text-to-Image Generation",
    "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
    "metadata": {
      "title": "Zero-Shot Text-to-Image Generation",
      "summary": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
      "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
      "doi": "https://doi.org/10.48550/arxiv.2102.12092",
      "openalex_id": "https://openalex.org/W3129576130",
      "arxiv_id": "",
      "publication_date": "2021-02-24",
      "published": "2021-02-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses zero-shot text-to-image generation using a transformer on text and image tokens and has no discrete audio token design or audio modeling details, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses zero-shot text-to-image generation using a transformer on text and image tokens and has no discrete audio token design or audio modeling details, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on zero-shot text-to-image generation using a transformer model and do not discuss discrete audio tokens, processes for generating them, or any related audio signal quantization or modeling. Therefore, the study does not meet any inclusion criteria related to discrete audio tokens, nor does it relate to audio domain specifics needed for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on zero-shot text-to-image generation using a transformer model and do not discuss discrete audio tokens, processes for generating them, or any related audio signal quantization or modeling. Therefore, the study does not meet any inclusion criteria related to discrete audio tokens, nor does it relate to audio domain specifics needed for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Laws for Autoregressive Generative Modeling",
    "abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
    "metadata": {
      "title": "Scaling Laws for Autoregressive Generative Modeling",
      "summary": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
      "abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
      "doi": "https://doi.org/10.48550/arxiv.2010.14701",
      "openalex_id": "https://openalex.org/W3095645723",
      "arxiv_id": "",
      "publication_date": "2020-10-28",
      "published": "2020-10-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about scaling laws for autoregressive generative modeling across image, video, multimodal text, and math tasks, with no discussion of discrete audio tokens, tokenization processes, or related methods; thus it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about scaling laws for autoregressive generative modeling across image, video, multimodal text, and math tasks, with no discussion of discrete audio tokens, tokenization processes, or related methods; thus it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on empirical scaling laws for autoregressive generative modeling across image, video, multimodal, and mathematical problem-solving domains. The content centers around autoregressive Transformers and their scaling behaviors, without any mention or analysis involving discrete audio tokens, audio signal processing, vector quantization of audio, or related audio language modeling as required by the inclusion criteria. Therefore, it does not meet the main topical inclusion criteria related to discrete audio tokens, their generation, or use for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on empirical scaling laws for autoregressive generative modeling across image, video, multimodal, and mathematical problem-solving domains. The content centers around autoregressive Transformers and their scaling behaviors, without any mention or analysis involving discrete audio tokens, audio signal processing, vector quantization of audio, or related audio language modeling as required by the inclusion criteria. Therefore, it does not meet the main topical inclusion criteria related to discrete audio tokens, their generation, or use for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
    "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
    "metadata": {
      "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
      "summary": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
      "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
      "doi": "https://doi.org/10.48550/arxiv.2104.10157",
      "openalex_id": "https://openalex.org/W3152733922",
      "arxiv_id": "",
      "publication_date": "2021-04-20",
      "published": "2021-04-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on video generation using VQ-VAE and transformers with discrete latent video tokens, not on discrete audio tokens, so it does not meet the inclusion criteria centered on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on video generation using VQ-VAE and transformers with discrete latent video tokens, not on discrete audio tokens, so it does not meet the inclusion criteria centered on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on video generation using VQ-VAE and transformers, with the primary modality being video rather than audio. Although it employs vector quantization, it does not address discrete audio token generation, representation, or use in audio language modeling or audio-related generative tasks, which are central to the inclusion criteria. Furthermore, the study does not provide relevant token generation or quantization methods in the audio domain, thus not meeting the topic requirements related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on video generation using VQ-VAE and transformers, with the primary modality being video rather than audio. Although it employs vector quantization, it does not address discrete audio token generation, representation, or use in audio language modeling or audio-related generative tasks, which are central to the inclusion criteria. Furthermore, the study does not provide relevant token generation or quantization methods in the audio domain, thus not meeting the topic requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis",
    "abstract": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",
    "metadata": {
      "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis",
      "summary": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",
      "abstract": "Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",
      "doi": "https://doi.org/10.48550/arxiv.2108.08827",
      "openalex_id": "https://openalex.org/W3196163807",
      "arxiv_id": "",
      "publication_date": "2021-08-19",
      "published": "2021-08-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript focuses on image autoregressive synthesis with diffusion (ImageBART) and does not address discrete audio tokens or audio token quantization, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript focuses on image autoregressive synthesis with diffusion (ImageBART) and does not address discrete audio tokens or audio token quantization, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image synthesis using an autoregressive model combined with multinomial diffusion, targeting image editing and generation tasks with latent image representations. It does not pertain to audio signal processing, discrete audio token generation, or audio language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is about a different modality (image rather than audio).",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image synthesis using an autoregressive model combined with multinomial diffusion, targeting image editing and generation tasks with latent image representations. It does not pertain to audio signal processing, discrete audio token generation, or audio language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is about a different modality (image rather than audio).",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
    "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
    "metadata": {
      "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
      "summary": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
      "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
      "doi": "https://doi.org/10.48550/arxiv.2110.03675",
      "openalex_id": "https://openalex.org/W3204896549",
      "arxiv_id": "",
      "publication_date": "2021-10-07",
      "published": "2021-10-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目與離散音訊 token 主題不符，該論文聚焦室內場景合成與 3D 物件佈局，與本主題的離散音訊 token 或聲學編碼無關，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目與離散音訊 token 主題不符，該論文聚焦室內場景合成與 3D 物件佈局，與本主題的離散音訊 token 或聲學編碼無關，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on indoor scene synthesis using autoregressive transformers to generate furniture layouts based on room type and floor plan, with no mention of discrete audio tokens, audio signal processing, vector quantization of audio, or any related audio language modeling topics as defined in the inclusion criteria. Thus, it does not meet the inclusion criteria for studies about discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on indoor scene synthesis using autoregressive transformers to generate furniture layouts based on room type and floor plan, with no mention of discrete audio tokens, audio signal processing, vector quantization of audio, or any related audio language modeling topics as defined in the inclusion criteria. Thus, it does not meet the inclusion criteria for studies about discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Computer-Aided Design as Language",
    "abstract": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
    "metadata": {
      "title": "Computer-Aided Design as Language",
      "summary": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
      "abstract": "Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.",
      "doi": "https://doi.org/10.48550/arxiv.2105.02769",
      "openalex_id": "https://openalex.org/W3159309302",
      "arxiv_id": "",
      "publication_date": "2021-05-06",
      "published": "2021-05-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper deals with generating 2D CAD sketches using language-modeling approaches; it does not address discrete audio tokens or audio-language modeling, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper deals with generating 2D CAD sketches using language-modeling approaches; it does not address discrete audio tokens or audio-language modeling, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on computer-aided design and machine learning models for generating 2D sketches, without mentioning any discrete audio token representation, audio signal processing, or related quantization methods for audio as defined in the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and their use in language or sequence modeling, and it does not fit the scope of the review task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on computer-aided design and machine learning models for generating 2D sketches, without mentioning any discrete audio token representation, audio signal processing, or related quantization methods for audio as defined in the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and their use in language or sequence modeling, and it does not fit the scope of the review task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Simple and Effective VAE Training with Calibrated Decoders",
    "abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/",
    "metadata": {
      "title": "Simple and Effective VAE Training with Calibrated Decoders",
      "summary": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/",
      "abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/",
      "doi": "https://doi.org/10.48550/arxiv.2006.13202",
      "openalex_id": "https://openalex.org/W3036520878",
      "arxiv_id": "",
      "publication_date": "2020-06-23",
      "published": "2020-06-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses VAE training with calibrated decoders and does not discuss discrete audio tokens or tokenization methods for audio language modeling, so it does not meet the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses VAE training with calibrated decoders and does not discuss discrete audio tokens or tokenization methods for audio language modeling, so it does not meet the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training variational autoencoders (VAEs) using calibrated decoders to improve model performance on image and video datasets, without mentioning any audio-related discrete token representation or audio-domain applications. There is no description or evidence of discrete audio token generation, vector quantization, or related audio signal processing methods as required by the inclusion criteria, and the work is purely on generic VAE training methods for images and videos. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training variational autoencoders (VAEs) using calibrated decoders to improve model performance on image and video datasets, without mentioning any audio-related discrete token representation or audio-domain applications. There is no description or evidence of discrete audio token generation, vector quantization, or related audio signal processing methods as required by the inclusion criteria, and the work is purely on generic VAE training methods for images and videos. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Simple Convergence Proof of Adam and Adagrad",
    "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.",
    "metadata": {
      "title": "A Simple Convergence Proof of Adam and Adagrad",
      "summary": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.",
      "abstract": "We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\\ln(N)/\\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.",
      "doi": "https://doi.org/10.48550/arxiv.2003.02395",
      "openalex_id": "https://openalex.org/W3096312061",
      "arxiv_id": "",
      "publication_date": "2020-03-05",
      "published": "2020-03-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此標題/摘要聚焦於 Adam 與 Adagrad 的收斂證明，與離散音訊 token 的研究主題（離散化表示、向量量化等）無關，未滿足包含條件，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此標題/摘要聚焦於 Adam 與 Adagrad 的收斂證明，與離散音訊 token 的研究主題（離散化表示、向量量化等）無關，未滿足包含條件，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on providing a convergence proof for the Adam and Adagrad optimization algorithms applied to smooth non-convex functions, without mentioning any discrete audio tokens, token production processes, or applications related to audio language modeling, sequence modeling, or audio representation learning. Therefore, it does not meet the inclusion criteria related to \"Discrete Audio Tokens\" and their design or usage, nor does it fall under any exclusion criteria beyond irrelevance to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on providing a convergence proof for the Adam and Adagrad optimization algorithms applied to smooth non-convex functions, without mentioning any discrete audio tokens, token production processes, or applications related to audio language modeling, sequence modeling, or audio representation learning. Therefore, it does not meet the inclusion criteria related to \"Discrete Audio Tokens\" and their design or usage, nor does it fall under any exclusion criteria beyond irrelevance to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Predictive Coding: a Theoretical and Experimental Review",
    "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
    "metadata": {
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "summary": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "doi": "https://doi.org/10.48550/arxiv.2107.12979",
      "openalex_id": "https://openalex.org/W3186883833",
      "arxiv_id": "",
      "publication_date": "2021-07-27",
      "published": "2021-07-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss predictive coding in neuroscience and do not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and does not satisfy the required research topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss predictive coding in neuroscience and do not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and does not satisfy the required research topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article reviews predictive coding theory related to cortical function and Bayesian brain framework with no mention or relevance to discrete audio tokens, neural audio codecs, vector quantization, or audio language modeling, which are the core inclusion criteria. Therefore, it does not meet any inclusion criteria and is unrelated to the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article reviews predictive coding theory related to cortical function and Bayesian brain framework with no mention or relevance to discrete audio tokens, neural audio codecs, vector quantization, or audio language modeling, which are the core inclusion criteria. Therefore, it does not meet any inclusion criteria and is unrelated to the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Tiny Transformers for Environmental Sound Classification at the Edge",
    "abstract": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
    "metadata": {
      "title": "Tiny Transformers for Environmental Sound Classification at the Edge",
      "summary": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
      "abstract": "With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.",
      "doi": "https://doi.org/10.48550/arxiv.2103.12157",
      "openalex_id": "https://openalex.org/W3136991969",
      "arxiv_id": "",
      "publication_date": "2021-03-22",
      "published": "2021-03-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on edge-efficient Transformers for environmental sound classification using Mel spectrograms and does not discuss discrete audio tokens, tokenization processes, or VQ/RVQ-like discretization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on edge-efficient Transformers for environmental sound classification using Mel spectrograms and does not discuss discrete audio tokens, tokenization processes, or VQ/RVQ-like discretization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on designing and training efficient Transformer-based models for environmental sound classification at the edge, leveraging Mel spectrograms and model compression, without discussing conversion of continuous audio signals into discrete audio tokens via vector quantization or similar methods, nor detailing token generation or usage for language/sequential modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens as a core contribution, nor does it provide the necessary technical detail about token formation or quantization approaches.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on designing and training efficient Transformer-based models for environmental sound classification at the edge, leveraging Mel spectrograms and model compression, without discussing conversion of continuous audio signals into discrete audio tokens via vector quantization or similar methods, nor detailing token generation or usage for language/sequential modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens as a core contribution, nor does it provide the necessary technical detail about token formation or quantization approaches.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Score-based Generative Modeling in Latent Space",
    "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
    "metadata": {
      "title": "Score-based Generative Modeling in Latent Space",
      "summary": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
      "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
      "doi": "https://doi.org/10.48550/arxiv.2106.05931",
      "openalex_id": "https://openalex.org/W3168452307",
      "arxiv_id": "",
      "publication_date": "2021-06-10",
      "published": "2021-06-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes latent-space score-based generative modeling for image data (LSGM) and does not address discrete audio tokens or tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes latent-space score-based generative modeling for image data (LSGM) and does not address discrete audio tokens or tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on score-based generative modeling in latent space primarily for image datasets (CIFAR-10, CelebA-HQ-256, binarized OMNIGLOT), with no mention of audio or the process of generating discrete audio tokens through vector quantization or related methods; thus, it does not meet the inclusion criteria related to discrete audio token generation or usage for audio language modeling or downstream audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on score-based generative modeling in latent space primarily for image datasets (CIFAR-10, CelebA-HQ-256, binarized OMNIGLOT), with no mention of audio or the process of generating discrete audio tokens through vector quantization or related methods; thus, it does not meet the inclusion criteria related to discrete audio token generation or usage for audio language modeling or downstream audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain",
    "abstract": "The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",
    "metadata": {
      "title": "Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain",
      "summary": "The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",
      "abstract": "The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",
      "doi": "https://doi.org/10.48550/arxiv.2009.05359",
      "openalex_id": "https://openalex.org/W3086298921",
      "arxiv_id": "",
      "publication_date": "2020-09-11",
      "published": "2020-09-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses backpropagation in neural circuits and neurobiological plausibility, not discrete audio token design or tokenization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses backpropagation in neural circuits and neurobiological plausibility, not discrete audio token design or tokenization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a novel algorithm for approximating backpropagation in the brain, which relates to neural network training and computational neuroscience rather than discrete audio token generation or audio signal processing. There is no indication that the work involves audio data, vector quantization, discrete token generation, or sequence modeling related to audio, thus it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a novel algorithm for approximating backpropagation in the brain, which relates to neural network training and computational neuroscience rather than discrete audio token generation or audio signal processing. There is no indication that the work involves audio data, vector quantization, discrete token generation, or sequence modeling related to audio, thus it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generating Images with Sparse Representations",
    "abstract": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.",
    "metadata": {
      "title": "Generating Images with Sparse Representations",
      "summary": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.",
      "abstract": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.",
      "doi": "https://doi.org/10.48550/arxiv.2103.03841",
      "openalex_id": "https://openalex.org/W3135058862",
      "arxiv_id": "",
      "publication_date": "2021-03-05",
      "published": "2021-03-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes image generation using sparse DCT-based representations, not discrete audio tokens or audio-language modeling; it does not meet the audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes image generation using sparse DCT-based representations, not discrete audio tokens or audio-language modeling; it does not meet the audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image generation using sparse representations of image data, specifically quantized discrete cosine transform (DCT) blocks, with no mention of audio signals, discrete audio tokens, or related audio processing methods such as vector quantization for audio content. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and applications, and falls strictly outside the scope of the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image generation using sparse representations of image data, specifically quantized discrete cosine transform (DCT) blocks, with no mention of audio signals, discrete audio tokens, or related audio processing methods such as vector quantization for audio content. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and applications, and falls strictly outside the scope of the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Contrastive Learning Approach for Training Variational Autoencoder Priors",
    "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
    "metadata": {
      "title": "A Contrastive Learning Approach for Training Variational Autoencoder Priors",
      "summary": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
      "abstract": "Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",
      "doi": "https://doi.org/10.48550/arxiv.2010.02917",
      "openalex_id": "https://openalex.org/W3209501356",
      "arxiv_id": "",
      "publication_date": "2020-10-06",
      "published": "2020-10-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses variational autoencoder priors and energy-based/noise-contrastive methods, not discrete audio token representations or tokenization for audio-language modeling, thus it does not meet the inclusion criteria targeting discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses variational autoencoder priors and energy-based/noise-contrastive methods, not discrete audio token representations or tokenization for audio-language modeling, thus it does not meet the inclusion criteria targeting discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on variational autoencoders and their prior distributions for image generation, with no mention of discrete audio tokens, audio signal processing, or tokenization methods relevant to audio language modeling or related tasks, failing to meet the inclusion criteria and unrelated to the discrete audio tokens research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on variational autoencoders and their prior distributions for image generation, with no mention of discrete audio tokens, audio signal processing, or tokenization methods relevant to audio language modeling or related tasks, failing to meet the inclusion criteria and unrelated to the discrete audio tokens research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention",
    "abstract": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
    "metadata": {
      "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention",
      "summary": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
      "abstract": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
      "doi": "https://doi.org/10.48550/arxiv.2105.03928",
      "openalex_id": "https://openalex.org/W3163120468",
      "arxiv_id": "",
      "publication_date": "2021-05-09",
      "published": "2021-05-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates Transformer architecture and a 'vocabulary bottleneck' concept but does not address discrete audio tokens or token design/quantization for audio-language modeling; thus it does not meet the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates Transformer architecture and a 'vocabulary bottleneck' concept but does not address discrete audio tokens or token design/quantization for audio-language modeling; thus it does not meet the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on Transformer architecture optimization related to embedding size and vocabulary bottleneck in self-attention across various data modalities, but do not mention discrete audio tokens, audio signal processing, or methods like vector quantization for generating tokens from audio. There is no description nor evidence of producing discrete audio tokens for audio language modeling or related applications as required in the inclusion criteria. Hence, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on Transformer architecture optimization related to embedding size and vocabulary bottleneck in self-attention across various data modalities, but do not mention discrete audio tokens, audio signal processing, or methods like vector quantization for generating tokens from audio. There is no description nor evidence of producing discrete audio tokens for audio language modeling or related applications as required in the inclusion criteria. Hence, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Models as a Data Source for Multiview Representation Learning",
    "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
    "metadata": {
      "title": "Generative Models as a Data Source for Multiview Representation Learning",
      "summary": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
      "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.",
      "doi": "https://doi.org/10.48550/arxiv.2106.05258",
      "openalex_id": "https://openalex.org/W3171895902",
      "arxiv_id": "",
      "publication_date": "2021-06-09",
      "published": "2021-06-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets visual representation learning with generative image models and latent-space views, with no discussion of discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets visual representation learning with generative image models and latent-space views, with no discussion of discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generative models for learning visual representations from images generated by such models; it does not discuss discrete audio tokens, audio signal processing, vector quantization of audio, or any audio-related tokenization methods. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and application for audio-related sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generative models for learning visual representations from images generated by such models; it does not discuss discrete audio tokens, audio signal processing, vector quantization of audio, or any audio-related tokenization methods. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and application for audio-related sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis",
    "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.",
    "metadata": {
      "title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis",
      "summary": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.",
      "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.",
      "doi": "https://doi.org/10.48550/arxiv.2110.10139",
      "openalex_id": "https://openalex.org/W3206916870",
      "arxiv_id": "",
      "publication_date": "2021-10-19",
      "published": "2021-10-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on chunked autoregressive GAN for waveform synthesis and pitch/periodicity artifacts, with no consideration of discrete audio token representations or tokenization methods, thus failing the discrete token inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on chunked autoregressive GAN for waveform synthesis and pitch/periodicity artifacts, with no consideration of discrete audio token representations or tokenization methods, thus failing the discrete token inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a GAN model for conditional waveform synthesis that improves pitch accuracy and generation speed but does not discuss the generation or use of discrete audio tokens through vector quantization, residual quantization, or a similar discrete tokenization process; thus, it does not meet the key inclusion criterion of discrete audio token design or usage as core methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a GAN model for conditional waveform synthesis that improves pitch accuracy and generation speed but does not discuss the generation or use of discrete audio tokens through vector quantization, residual quantization, or a similar discrete tokenization process; thus, it does not meet the key inclusion criterion of discrete audio token design or usage as core methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
    "abstract": "We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.",
    "metadata": {
      "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
      "summary": "We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.",
      "abstract": "We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.",
      "doi": "https://doi.org/10.48550/arxiv.2101.04785",
      "openalex_id": "https://openalex.org/W3119914886",
      "arxiv_id": "",
      "publication_date": "2021-01-12",
      "published": "2021-01-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a GAN-based music generation approach using MDCT and phase information and does not describe discrete audio token designs or tokenization; thus it fails the inclusion criteria for discrete audio tokens and should be excluded (the abstract also lacks a clear publication date).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on a GAN-based music generation approach using MDCT and phase information and does not describe discrete audio token designs or tokenization; thus it fails the inclusion criteria for discrete audio tokens and should be excluded (the abstract also lacks a clear publication date).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a convolutional GAN for coherent music generation from raw audio with an MDCT representation involving phase information; however, it does not mention generating discrete audio tokens through vector quantization, residual quantization, or any discrete token mechanism intended for sequence or language model processing, nor does it detail token generation or quantization processes. It focuses on generating audio waveforms directly rather than producing discrete token representations usable for audio language modeling, which is central to the inclusion criteria; hence, it does not meet the core requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces a convolutional GAN for coherent music generation from raw audio with an MDCT representation involving phase information; however, it does not mention generating discrete audio tokens through vector quantization, residual quantization, or any discrete token mechanism intended for sequence or language model processing, nor does it detail token generation or quantization processes. It focuses on generating audio waveforms directly rather than producing discrete token representations usable for audio language modeling, which is central to the inclusion criteria; hence, it does not meet the core requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Catch-A-Waveform: Learning to Generate Audio from a Single Short Example",
    "abstract": "Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.",
    "metadata": {
      "title": "Catch-A-Waveform: Learning to Generate Audio from a Single Short Example",
      "summary": "Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.",
      "abstract": "Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.",
      "doi": "https://doi.org/10.48550/arxiv.2106.06426",
      "openalex_id": "https://openalex.org/W3171443854",
      "arxiv_id": "",
      "publication_date": "2021-06-11",
      "published": "2021-06-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns GAN-based audio generation from a single short example and does not present discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or token-based audio modeling, so it does not meet the inclusion criteria focused on discrete audio tokens, and is not aligned with the exclusion notes provided.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns GAN-based audio generation from a single short example and does not present discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or token-based audio modeling, so it does not meet the inclusion criteria focused on discrete audio tokens, and is not aligned with the exclusion notes provided.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a GAN-based generative model trained on a single short audio example to generate audio samples but does not describe any method for converting continuous audio into discrete tokens or vector-quantized representations; its main contribution is audio waveform generation without leveraging discrete audio token representations or explicit token generation mechanisms that can be used for sequence modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a GAN-based generative model trained on a single short audio example to generate audio samples but does not describe any method for converting continuous audio into discrete tokens or vector-quantized representations; its main contribution is audio waveform generation without leveraging discrete audio token representations or explicit token generation mechanisms that can be used for sequence modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DeepSinger: Singing Voice Synthesis with Data Mined From the Web",
    "abstract": "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)",
    "metadata": {
      "title": "DeepSinger: Singing Voice Synthesis with Data Mined From the Web",
      "summary": "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)",
      "abstract": "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)",
      "doi": "https://doi.org/10.48550/arxiv.2007.04590",
      "openalex_id": "https://openalex.org/W3041199652",
      "arxiv_id": "",
      "publication_date": "2020-07-09",
      "published": "2020-07-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes DeepSinger SVS with data mining and alignment; it does not address discrete audio token representations or tokenization for audio modeling, hence it does not meet inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes DeepSinger SVS with data mining and alignment; it does not address discrete audio token representations or tokenization for audio modeling, hence it does not meet inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a multi-lingual multi-singer singing voice synthesis system built from web-mined training data, employing a feed-forward Transformer model to generate linear-spectrograms which are then synthesized into audio. However, it lacks any mention of discrete audio tokens, vector quantization, residual vector quantization, or other discrete representations of audio signals used as tokens for sequence or language modeling. There is no indication of token generation processes or discrete representations used in downstream tasks such as audio language modeling or cross-modal generation. Hence, it does not meet the inclusion criteria centered on discrete audio tokens as a primary contribution and method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a multi-lingual multi-singer singing voice synthesis system built from web-mined training data, employing a feed-forward Transformer model to generate linear-spectrograms which are then synthesized into audio. However, it lacks any mention of discrete audio tokens, vector quantization, residual vector quantization, or other discrete representations of audio signals used as tokens for sequence or language modeling. There is no indication of token generation processes or discrete representations used in downstream tasks such as audio language modeling or cross-modal generation. Hence, it does not meet the inclusion criteria centered on discrete audio tokens as a primary contribution and method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer",
    "abstract": "Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",
    "metadata": {
      "title": "AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer",
      "summary": "Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",
      "abstract": "Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",
      "doi": "https://doi.org/10.48550/arxiv.2108.11213",
      "openalex_id": "https://openalex.org/W3196114627",
      "arxiv_id": "",
      "publication_date": "2021-08-25",
      "published": "2021-08-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes an accompaniment generation system (AccoMontage) using phrase selection and style transfer, not a discrete audio token/discretization method, so it fails the inclusion criteria focused on discrete audio tokens; despite methodological details, it does not contribute token design or VQ/RVQ-style discretization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work describes an accompaniment generation system (AccoMontage) using phrase selection and style transfer, not a discrete audio token/discretization method, so it fails the inclusion criteria focused on discrete audio tokens; despite methodological details, it does not contribute token design or VQ/RVQ-style discretization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on accompaniment arrangement for music generation using phrase selection and neural style transfer, but it does not mention generating or using discrete audio tokens through vector quantization or related methods, nor does it describe token generation processes or their use in sequence or language models for audio understanding or generation. Therefore, it does not meet the inclusion criteria regarding discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on accompaniment arrangement for music generation using phrase selection and neural style transfer, but it does not mention generating or using discrete audio tokens through vector quantization or related methods, nor does it describe token generation processes or their use in sequence or language models for audio understanding or generation. Therefore, it does not meet the inclusion criteria regarding discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks",
    "abstract": "As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.",
    "metadata": {
      "title": "Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks",
      "summary": "As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.",
      "abstract": "As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.",
      "doi": "https://doi.org/10.48550/arxiv.2101.08919",
      "openalex_id": "https://openalex.org/W3209092256",
      "arxiv_id": "",
      "publication_date": "2021-01-22",
      "published": "2021-01-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on client-side privacy for downstream speech tasks and privacy-preserving techniques, not on discrete audio tokens or tokenization methods, so it fails the inclusion criteria requiring discrete audio token design or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on client-side privacy for downstream speech tasks and privacy-preserving techniques, not on discrete audio tokens or tokenization methods, so it fails the inclusion criteria requiring discrete audio token design or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on client-side privacy challenges and solutions for speech data but does not discuss discrete audio tokens, their generation process, or usage in sequence/language models. It lacks details on vector quantization or token-based representations, which are core to the inclusion criteria. Therefore, it does not meet the criteria of having discrete token design or token formation as a main research contribution, and it does not satisfy any of the technical coverage requirements related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on client-side privacy challenges and solutions for speech data but does not discuss discrete audio tokens, their generation process, or usage in sequence/language models. It lacks details on vector quantization or token-based representations, which are core to the inclusion criteria. Therefore, it does not meet the criteria of having discrete token design or token formation as a main research contribution, and it does not satisfy any of the technical coverage requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics",
    "abstract": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
    "metadata": {
      "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics",
      "summary": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
      "abstract": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
      "doi": "https://doi.org/10.48550/arxiv.2105.08164",
      "openalex_id": "https://openalex.org/W3160695487",
      "arxiv_id": "",
      "publication_date": "2021-05-17",
      "published": "2021-05-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes parallel and flexible sampling for autoregressive models using Langevin dynamics, applied to audio domains, but it does not address discrete audio tokens, token quantization/design, or token-based Audio Language Modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes parallel and flexible sampling for autoregressive models using Langevin dynamics, applied to audio domains, but it does not address discrete audio tokens, token quantization/design, or token-based Audio Language Modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a novel sampling approach for autoregressive models using Langevin dynamics and applies it to audio tasks like source separation, super-resolution, and inpainting; however, it does not describe any discrete audio token generation (e.g., vector quantization or related tokenization methods) nor does it address token design or usage as input to sequence or language models, which is central to the inclusion criteria. Hence, it does not meet the main inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a novel sampling approach for autoregressive models using Langevin dynamics and applies it to audio tasks like source separation, super-resolution, and inpainting; however, it does not describe any discrete audio token generation (e.g., vector quantization or related tokenization methods) nor does it address token design or usage as input to sequence or language models, which is central to the inclusion criteria. Hence, it does not meet the main inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models",
    "abstract": "Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
    "metadata": {
      "title": "Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models",
      "summary": "Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
      "abstract": "Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02891",
      "openalex_id": "https://openalex.org/W3201970400",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on unsupervised style transfer for controllable sequence models (TTS/handwriting) and does not discuss discrete audio token representations or tokenization schemes; thus it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on unsupervised style transfer for controllable sequence models (TTS/handwriting) and does not discuss discrete audio token representations or tokenization schemes; thus it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised learning of controllable generative sequence models for style replication in text-to-speech and handwriting synthesis, but it does not mention the use or generation of discrete audio tokens through vector quantization or similar techniques; it mainly addresses style transfer and inference mismatch rather than discrete token representation or production critical to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised learning of controllable generative sequence models for style replication in text-to-speech and handwriting synthesis, but it does not mention the use or generation of discrete audio tokens through vector quantization or similar techniques; it mainly addresses style transfer and inference mismatch rather than discrete token representation or production critical to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech-to-Singing Conversion based on Boundary Equilibrium GAN",
    "abstract": "This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.",
    "metadata": {
      "title": "Speech-to-Singing Conversion based on Boundary Equilibrium GAN",
      "summary": "This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.",
      "abstract": "This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.",
      "doi": "https://doi.org/10.48550/arxiv.2005.13835",
      "openalex_id": "https://openalex.org/W3028988798",
      "arxiv_id": "",
      "publication_date": "2020-05-28",
      "published": "2020-05-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "No discrete audio token design or tokenization mechanism is described; the work targets continuous spectrogram conversion (speech to singing) using GANs, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "No discrete audio token design or tokenization mechanism is described; the work targets continuous spectrogram conversion (speech to singing) using GANs, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech-to-singing conversion using a GAN-based style transfer model but does not mention the use of discrete audio tokens, vector quantization, or related tokenization methods needed to convert continuous audio into discrete tokens for downstream language or sequence modeling. The study's main contribution is on signal conversion rather than discrete token representation, which is a core inclusion criterion. Additionally, it lacks details on token generation, quantization, or usage as intermediate representations for audio language modeling or related tasks, which are essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech-to-singing conversion using a GAN-based style transfer model but does not mention the use of discrete audio tokens, vector quantization, or related tokenization methods needed to convert continuous audio into discrete tokens for downstream language or sequence modeling. The study's main contribution is on signal conversion rather than discrete token representation, which is a core inclusion criterion. Additionally, it lacks details on token generation, quantization, or usage as intermediate representations for audio language modeling or related tasks, which are essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Melody Classifier with Stacked-LSTM",
    "abstract": "Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.",
    "metadata": {
      "title": "Melody Classifier with Stacked-LSTM",
      "summary": "Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.",
      "abstract": "Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.",
      "doi": "https://doi.org/10.48550/arxiv.2010.08123",
      "openalex_id": "https://openalex.org/W3092769093",
      "arxiv_id": "",
      "publication_date": "2020-10-16",
      "published": "2020-10-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes a melody/human-vs-machine classifier using a stacked-LSTM, but it does not involve discrete audio tokens or tokenization-based representations, nor describe token generation/quantization methods; hence it does not satisfy the inclusion criteria and fails all required token-centric aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work describes a melody/human-vs-machine classifier using a stacked-LSTM, but it does not involve discrete audio tokens or tokenization-based representations, nor describe token generation/quantization methods; hence it does not satisfy the inclusion criteria and fails all required token-centric aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a stacked-LSTM binary classifier to distinguish human-composed melodies from machine-generated ones based on pitch, position, and duration in MIDI files. However, it does not involve producing or designing discrete audio tokens through methods such as vector quantization, nor does it provide details on token generation or usage as input for language or sequence models, which is a key requirement in the inclusion criteria. Therefore, it does not meet the core inclusion criterion related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a stacked-LSTM binary classifier to distinguish human-composed melodies from machine-generated ones based on pitch, position, and duration in MIDI files. However, it does not involve producing or designing discrete audio tokens through methods such as vector quantization, nor does it provide details on token generation or usage as input for language or sequence models, which is a key requirement in the inclusion criteria. Therefore, it does not meet the core inclusion criterion related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at Pitch",
    "abstract": "Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.",
    "metadata": {
      "title": "I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at Pitch",
      "summary": "Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.",
      "abstract": "Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.",
      "doi": "https://doi.org/10.48550/arxiv.2012.04572",
      "openalex_id": "https://openalex.org/W3110955493",
      "arxiv_id": "",
      "publication_date": "2020-12-08",
      "published": "2020-12-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on evaluating audio-to-audio losses and pitch distance for self-supervised audio learning, not on discrete audio tokens or tokenization design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on evaluating audio-to-audio losses and pitch distance for self-supervised audio learning, not on discrete audio tokens or tokenization design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss evaluating audio-to-audio losses based on pitch distance between sinusoidal tones, focusing on the shortcomings of existing audio distance metrics and implications for self-supervised learning in audio. However, there is no indication that the study involves discrete audio tokens, vector quantization, or any tokenization process designed for use with sequence models or language models as required by the inclusion criteria. The paper seems to focus on audio loss metrics evaluation rather than proposing or analyzing discrete token representations for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss evaluating audio-to-audio losses based on pitch distance between sinusoidal tones, focusing on the shortcomings of existing audio distance metrics and implications for self-supervised learning in audio. However, there is no indication that the study involves discrete audio tokens, vector quantization, or any tokenization process designed for use with sequence models or language models as required by the inclusion criteria. The paper seems to focus on audio loss metrics evaluation rather than proposing or analyzing discrete token representations for audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset",
    "abstract": "This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.",
    "metadata": {
      "title": "A Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset",
      "summary": "This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.",
      "abstract": "This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.",
      "doi": "https://doi.org/10.48550/arxiv.2108.01576",
      "openalex_id": "https://openalex.org/W3191781868",
      "arxiv_id": "",
      "publication_date": "2021-08-03",
      "published": "2021-08-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio tokens or tokenization (e.g., VQ/RVQ, hierarchical/semantic discrete tokens) and focuses on benchmarking GAN-based drum-loop generation for audio-domain music using Freesound/Looperman datasets, which does not meet the inclusion criteria but may be a valid study among others.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio tokens or tokenization (e.g., VQ/RVQ, hierarchical/semantic discrete tokens) and focuses on benchmarking GAN-based drum-loop generation for audio-domain music using Freesound/Looperman datasets, which does not meet the inclusion criteria but may be a valid study among others.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a benchmarking task for audio-domain music generation using drum loops but does not mention the use of discrete audio tokens, vector quantization, or any form of tokenization method essential for sequence modeling or language-model processing as required by the inclusion criteria. The focus is primarily on generative adversarial networks and evaluation metrics rather than on discrete token design or token generation processes. Furthermore, the abstract does not provide detailed methods or mechanisms regarding discrete token formation or usage, which is critical for inclusion. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a benchmarking task for audio-domain music generation using drum loops but does not mention the use of discrete audio tokens, vector quantization, or any form of tokenization method essential for sequence modeling or language-model processing as required by the inclusion criteria. The focus is primarily on generative adversarial networks and evaluation metrics rather than on discrete token design or token generation processes. Furthermore, the abstract does not provide detailed methods or mechanisms regarding discrete token formation or usage, which is critical for inclusion. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Network Bending: Manipulating The Inner Representations of Deep Generative Models.",
    "abstract": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.",
    "metadata": {
      "title": "Network Bending: Manipulating The Inner Representations of Deep Generative Models.",
      "summary": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.",
      "abstract": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3030072033",
      "arxiv_id": "",
      "publication_date": "2020-05-25",
      "published": "2020-05-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses manipulating inner representations of image-generative models (StyleGAN2) and does not involve discrete audio tokens or audio-domain discrete representations, so it fails the Audio Discrete Token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses manipulating inner representations of image-generative models (StyleGAN2) and does not involve discrete audio tokens or audio-domain discrete representations, so it fails the Audio Discrete Token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a framework for manipulating inner representations of deep generative image models, specifically StyleGAN2, focusing on visual feature transformations; there is no mention of audio signals, discrete audio tokens, or any form of audio representation or processing, which are core to the inclusion criteria. The study focuses on image generation and interpretation, not audio tokenization for language or sequence modeling, and thus does not meet the topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a framework for manipulating inner representations of deep generative image models, specifically StyleGAN2, focusing on visual feature transformations; there is no mention of audio signals, discrete audio tokens, or any form of audio representation or processing, which are core to the inclusion criteria. The study focuses on image generation and interpretation, not audio tokenization for language or sequence modeling, and thus does not meet the topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation",
    "abstract": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
    "metadata": {
      "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation",
      "summary": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
      "abstract": "Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
      "doi": "https://doi.org/10.48550/arxiv.2106.06819",
      "openalex_id": "https://openalex.org/W3170636531",
      "arxiv_id": "",
      "publication_date": "2021-06-12",
      "published": "2021-06-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about diffusion-denoising models for few-shot conditional image generation and diffusion-based VAEs, with no discussion of discrete audio tokens, token design, VQ/RVQ, or audio-language modeling; it fails to meet the Audio Discrete Token topic inclusion criteria and does not fit any exclusion criteria other than irrelevance to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about diffusion-denoising models for few-shot conditional image generation and diffusion-based VAEs, with no discussion of discrete audio tokens, token design, VQ/RVQ, or audio-language modeling; it fails to meet the Audio Discrete Token topic inclusion criteria and does not fit any exclusion criteria other than irrelevance to the topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on diffusion-denoising models for few-shot conditional image generation, which relates to image data rather than audio; it does not discuss discrete audio tokens, vector quantization, or audio language modeling, nor does it address generating or processing discrete tokens for audio signals, thus it does not meet the inclusion criteria centered on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on diffusion-denoising models for few-shot conditional image generation, which relates to image data rather than audio; it does not discuss discrete audio tokens, vector quantization, or audio language modeling, nor does it address generating or processing discrete tokens for audio signals, thus it does not meet the inclusion criteria centered on discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Source Separation via Bayesian Inference in the Latent Domain",
    "abstract": "State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.",
    "metadata": {
      "title": "Unsupervised Source Separation via Bayesian Inference in the Latent Domain",
      "summary": "State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.",
      "abstract": "State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.",
      "doi": "https://doi.org/10.48550/arxiv.2110.05313",
      "openalex_id": "https://openalex.org/W3204992386",
      "arxiv_id": "",
      "publication_date": "2021-10-11",
      "published": "2021-10-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work uses a discrete latent space, it does not present discrete audio tokens as its main contribution nor provide token generation/quantization details suitable for token-based audio modeling, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work uses a discrete latent space, it does not present discrete audio tokens as its main contribution nor provide token generation/quantization details suitable for token-based audio modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on an unsupervised source separation algorithm operating on a latent representation of time-domain signals, relying on deep Bayesian priors and pre-trained autoregressive networks. However, there is no clear indication that the latent representations are discrete audio tokens generated by vector quantization or similar discrete tokenization methods that preserve acoustic and semantic information to enable sequence modeling or audio language modeling. The method does not clearly describe the token generation process, quantization, or discrete codebook design, nor does it explicitly state the intended use of tokens for language model-based understanding or generation. Therefore, it does not fully meet the inclusion criteria requiring discrete token design and detailed token generation methodology, nor does it fall under the exclusion criteria such as lack of method detail or being a traditional codec. Due to the insufficient evidence for discrete token usage, the evaluation leans towards exclusion but not absolute.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on an unsupervised source separation algorithm operating on a latent representation of time-domain signals, relying on deep Bayesian priors and pre-trained autoregressive networks. However, there is no clear indication that the latent representations are discrete audio tokens generated by vector quantization or similar discrete tokenization methods that preserve acoustic and semantic information to enable sequence modeling or audio language modeling. The method does not clearly describe the token generation process, quantization, or discrete codebook design, nor does it explicitly state the intended use of tokens for language model-based understanding or generation. Therefore, it does not fully meet the inclusion criteria requiring discrete token design and detailed token generation methodology, nor does it fall under the exclusion criteria such as lack of method detail or being a traditional codec. Due to the insufficient evidence for discrete token usage, the evaluation leans towards exclusion but not absolute.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers",
    "abstract": "Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.",
    "metadata": {
      "title": "Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers",
      "summary": "Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.",
      "abstract": "Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.",
      "doi": "https://doi.org/10.48550/arxiv.2101.00169",
      "openalex_id": "https://openalex.org/W3120142428",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on GAN-based generative music and comparison to master composers, with no discussion of discrete audio tokens or token-based representations, and lacks the required methodological details for token generation/quantization; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on GAN-based generative music and comparison to master composers, with no discussion of discrete audio tokens or token-based representations, and lacks the required methodological details for token generation/quantization; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses generative adversarial networks applied to AI-generated classical music aiming to achieve compositional quality near master composers, but it does not mention any method of representing or generating discrete audio tokens via vector quantization or related techniques as defined in the inclusion criteria. There is no detailed technical description of token generation, quantization, or usage in sequence models, nor does it show the token as a core contribution. Thus, it does not meet the required inclusion criteria related to discrete audio tokens and fails to provide methodological details for reproducibility.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses generative adversarial networks applied to AI-generated classical music aiming to achieve compositional quality near master composers, but it does not mention any method of representing or generating discrete audio tokens via vector quantization or related techniques as defined in the inclusion criteria. There is no detailed technical description of token generation, quantization, or usage in sequence models, nor does it show the token as a core contribution. Thus, it does not meet the required inclusion criteria related to discrete audio tokens and fails to provide methodological details for reproducibility.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral",
    "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
    "metadata": {
      "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral",
      "summary": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
      "abstract": "The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3170685585",
      "arxiv_id": "",
      "publication_date": "2021-06-15",
      "published": "2021-06-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses statistical properties and estimation of divergence frontiers for generative models, with no focus on discrete audio token design, quantization techniques for audio tokens, or methods enabling audio-language modeling; thus it fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses statistical properties and estimation of divergence frontiers for generative models, with no focus on discrete audio token design, quantization techniques for audio tokens, or methods enabling audio-language modeling; thus it fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the theoretical analysis of divergence frontiers in generative models, specifically studying sample complexity and quantization levels related to statistical estimation in deep generative modeling. However, it does not discuss or contribute to discrete audio token representation, token generation processes, vector quantization for audio, or their use in sequence or language models for audio understanding or generation, which are critical inclusion criteria. Therefore, it does not meet the core inclusion requirements centered on discrete audio tokens and their design/application in audio sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the theoretical analysis of divergence frontiers in generative models, specifically studying sample complexity and quantization levels related to statistical estimation in deep generative modeling. However, it does not discuss or contribute to discrete audio token representation, token generation processes, vector quantization for audio, or their use in sequence or language models for audio understanding or generation, which are critical inclusion criteria. Therefore, it does not meet the core inclusion requirements centered on discrete audio tokens and their design/application in audio sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PocketVAE: A Two-step Model for Groove Generation and Control",
    "abstract": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.",
    "metadata": {
      "title": "PocketVAE: A Two-step Model for Groove Generation and Control",
      "summary": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.",
      "abstract": "Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.",
      "doi": "https://doi.org/10.48550/arxiv.2107.05009",
      "openalex_id": "https://openalex.org/W3178354449",
      "arxiv_id": "",
      "publication_date": "2021-07-11",
      "published": "2021-07-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work mentions a discrete latent representation via VQ-VAE for drum notes (discrete tokens), the item only provides an abstract and does not supply full English text with reproducible methodological details (token generation, quantization, model architecture, training/inference settings) required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work mentions a discrete latent representation via VQ-VAE for drum notes (discrete tokens), the item only provides an abstract and does not supply full English text with reproducible methodological details (token generation, quantization, model architecture, training/inference settings) required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces PocketVAE, which uses Vector Quantized Variational Autoencoder (VQ-VAE) to model drum notes, implying the use of discrete latent representations through vector quantization. It discusses a two-step approach for groove generation and control, emphasizing discrete token generation and usage for improving learning music groove distributions. However, the abstract primarily focuses on music generation tasks, especially in rhythm and groove control. It appears to satisfy the inclusion criteria of using vector quantization to produce discrete tokens for sequence modeling in audio generation. The methodology regarding token generation and usage is described clearly, and it is situated within research on discrete audio tokens for music. No exclusion criteria such as lack of method details or traditional codec use seem present. Therefore, it aligns well with the topic of discrete audio token design and application in sequence modeling for audio understanding/generation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces PocketVAE, which uses Vector Quantized Variational Autoencoder (VQ-VAE) to model drum notes, implying the use of discrete latent representations through vector quantization. It discusses a two-step approach for groove generation and control, emphasizing discrete token generation and usage for improving learning music groove distributions. However, the abstract primarily focuses on music generation tasks, especially in rhythm and groove control. It appears to satisfy the inclusion criteria of using vector quantization to produce discrete tokens for sequence modeling in audio generation. The methodology regarding token generation and usage is described clearly, and it is situated within research on discrete audio tokens for music. No exclusion criteria such as lack of method details or traditional codec use seem present. Therefore, it aligns well with the topic of discrete audio token design and application in sequence modeling for audio understanding/generation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the use of VQ‑VAE; however, they disagreed — one flagged lack of full methodological detail while the other treated the VQ usage as sufficient. Upon review I exclude this item: the work operates on symbolic MIDI/drum-note representations (not the conversion of continuous audio to discrete audio tokens as defined by the inclusion criteria) and only an abstract is provided so there are no reproducible method/tokenization details to assess. One-sentence explanation: Exclude (score=1) because the paper focuses on MIDI-level groove modeling with VQ‑VAE rather than producing discrete tokens from continuous audio, and only an abstract is available lacking necessary methodological details.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the use of VQ‑VAE; however, they disagreed — one flagged lack of full methodological detail while the other treated the VQ usage as sufficient. Upon review I exclude this item: the work operates on symbolic MIDI/drum-note representations (not the conversion of continuous audio to discrete audio tokens as defined by the inclusion criteria) and only an abstract is provided so there are no reproducible method/tokenization details to assess. One-sentence explanation: Exclude (score=1) because the paper focuses on MIDI-level groove modeling with VQ‑VAE rather than producing discrete tokens from continuous audio, and only an abstract is available lacking necessary methodological details.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "BumbleBee: A Transformer for Music",
    "abstract": "We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)",
    "metadata": {
      "title": "BumbleBee: A Transformer for Music",
      "summary": "We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)",
      "abstract": "We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)",
      "doi": "https://doi.org/10.48550/arxiv.2107.03443",
      "openalex_id": "https://openalex.org/W3181311346",
      "arxiv_id": "",
      "publication_date": "2021-07-07",
      "published": "2021-07-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a transformer for music generation using MIDI data and long-sequence attention, but it does not address discrete audio token representations or tokenization designs, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a transformer for music generation using MIDI data and long-sequence attention, but it does not address discrete audio token representations or tokenization designs, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes BumbleBee, a transformer model for generating MIDI music data using a longformer architecture. However, the abstract does not describe the generation or use of discrete audio tokens produced by vector quantization or related quantization methods; it rather focuses on sequence modeling of symbolic MIDI data directly. There is no mention of token generation mechanisms, vector quantization, or discrete token-based audio representations as per the inclusion criteria, hence it does not meet the main research contribution requirements related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes BumbleBee, a transformer model for generating MIDI music data using a longformer architecture. However, the abstract does not describe the generation or use of discrete audio tokens produced by vector quantization or related quantization methods; it rather focuses on sequence modeling of symbolic MIDI data directly. There is no mention of token generation mechanisms, vector quantization, or discrete token-based audio representations as per the inclusion criteria, hence it does not meet the main research contribution requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Source Separation By Steering Pretrained Music Models",
    "abstract": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
    "metadata": {
      "title": "Unsupervised Source Separation By Steering Pretrained Music Models",
      "summary": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "abstract": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.",
      "doi": "https://doi.org/10.48550/arxiv.2110.13071",
      "openalex_id": "https://openalex.org/W3209109096",
      "arxiv_id": "",
      "publication_date": "2021-10-25",
      "published": "2021-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses latent-space optimization of a generative model and taggers for source separation and does not involve discrete audio tokens or token-based representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work uses latent-space optimization of a generative model and taggers for source separation and does not involve discrete audio tokens or token-based representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes an unsupervised source separation method leveraging pretrained music models (generative and taggers) and operating in latent space, but does not mention any discrete audio token generation, vector quantization, or quantized semantic representations as defined in the inclusion criteria. The method focuses on source separation using latent representations and gradient ascent without detailing any discrete tokenization or token design that supports sequence/language modeling. Therefore, it does not meet the core inclusion criterion of discrete audio token design or usage as a main research contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes an unsupervised source separation method leveraging pretrained music models (generative and taggers) and operating in latent space, but does not mention any discrete audio token generation, vector quantization, or quantized semantic representations as defined in the inclusion criteria. The method focuses on source separation using latent representations and gradient ascent without detailing any discrete tokenization or token design that supports sequence/language modeling. Therefore, it does not meet the core inclusion criterion of discrete audio token design or usage as a main research contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Ownership and Creativity in Generative Models",
    "abstract": "Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.",
    "metadata": {
      "title": "Ownership and Creativity in Generative Models",
      "summary": "Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.",
      "abstract": "Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.",
      "doi": "https://doi.org/10.48550/arxiv.2112.01516",
      "openalex_id": "https://openalex.org/W3217025373",
      "arxiv_id": "",
      "publication_date": "2021-12-02",
      "published": "2021-12-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract address ownership and creativity in generative models, with no discussion of discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria requiring discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract address ownership and creativity in generative models, with no discussion of discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria requiring discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on ownership and creativity issues related to generative models producing image, text, and music content, without any mention or analysis of discrete audio tokens, token generation processes, vector quantization, or related audio representation techniques necessary to meet the inclusion criteria focused on discrete audio tokens for speech, music, or general audio modeling. Thus, it does not discuss production or usage of discrete audio tokens in sequence or language modeling contexts, nor does it provide technical details or evaluation of such tokens as required, and so should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on ownership and creativity issues related to generative models producing image, text, and music content, without any mention or analysis of discrete audio tokens, token generation processes, vector quantization, or related audio representation techniques necessary to meet the inclusion criteria focused on discrete audio tokens for speech, music, or general audio modeling. Thus, it does not discuss production or usage of discrete audio tokens in sequence or language modeling contexts, nor does it provide technical details or evaluation of such tokens as required, and so should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language",
    "abstract": "Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.",
    "metadata": {
      "title": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language",
      "summary": "Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.",
      "abstract": "Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01759",
      "openalex_id": "https://openalex.org/W4386057807",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on CLIP-guided zero-shot 3D shape generation from natural language, not on discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria centered on discrete audio token design and related methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on CLIP-guided zero-shot 3D shape generation from natural language, not on discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria centered on discrete audio token design and related methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method for generating 3D shapes from natural language using a multi-resolution approach and transformer models, focusing on visual 3D shape synthesis rather than audio processing. There is no indication of discrete audio tokens, vector quantization of audio signals, or audio language modeling, which are essential inclusion criteria. Therefore, this work does not meet the topic of 'Discrete Audio Tokens' and related methods for audio tokenization and audio sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method for generating 3D shapes from natural language using a multi-resolution approach and transformer models, focusing on visual 3D shape synthesis rather than audio processing. There is no indication of discrete audio tokens, vector quantization of audio signals, or audio language modeling, which are essential inclusion criteria. Therefore, this work does not meet the topic of 'Discrete Audio Tokens' and related methods for audio tokenization and audio sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Deep Vector Quantization Clustering Method for Polarimetric SAR Images",
    "abstract": "Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.",
    "metadata": {
      "title": "A Deep Vector Quantization Clustering Method for Polarimetric SAR Images",
      "summary": "Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.",
      "abstract": "Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.",
      "doi": "https://doi.org/10.3390/rs13112127",
      "openalex_id": "https://openalex.org/W3168380356",
      "arxiv_id": "",
      "publication_date": "2021-05-28",
      "published": "2021-05-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe unsupervised PolSAR image clustering using vector quantization, which does not pertain to discrete audio tokens or audio tokenization for language models, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe unsupervised PolSAR image clustering using vector quantization, which does not pertain to discrete audio tokens or audio tokenization for language models, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a vector quantization clustering method applied to polarimetric SAR images, which are remote sensing data, not audio signals; it does not address discrete audio token generation, audio language modeling, or related methods in audio signal processing, and thus does not fulfill the inclusion criteria focused on discrete audio tokens for audio understanding or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a vector quantization clustering method applied to polarimetric SAR images, which are remote sensing data, not audio signals; it does not address discrete audio token generation, audio language modeling, or related methods in audio signal processing, and thus does not fulfill the inclusion criteria focused on discrete audio tokens for audio understanding or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AbNatiV: VQ-VAE-based assessment of antibody and nanobody nativeness for hit selection, humanisation, and engineering",
    "abstract": "Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.",
    "metadata": {
      "title": "AbNatiV: VQ-VAE-based assessment of antibody and nanobody nativeness for hit selection, humanisation, and engineering",
      "summary": "Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.",
      "abstract": "Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.",
      "doi": "https://doi.org/10.1101/2023.04.28.538712",
      "openalex_id": "https://openalex.org/W4367366153",
      "arxiv_id": "",
      "publication_date": "2023-04-29",
      "published": "2023-04-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item discusses AbNatiV, a VQ-VAE–based tool for assessing antibody/nanobody nativeness, which is unrelated to discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item discusses AbNatiV, a VQ-VAE–based tool for assessing antibody/nanobody nativeness, which is unrelated to discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a deep-learning tool for assessing antibody and nanobody nativeness using a VQ-VAE based method, but its application domain is biological and immunological rather than audio processing. Although it mentions vector quantization (VQ), it does not involve discrete audio tokens or their use in audio language modeling, speech, or music processing, which are the core focus of the inclusion criteria. Therefore, it does not meet the subject definition of discrete audio tokens used in audio-related sequence modeling and generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a deep-learning tool for assessing antibody and nanobody nativeness using a VQ-VAE based method, but its application domain is biological and immunological rather than audio processing. Although it mentions vector quantization (VQ), it does not involve discrete audio tokens or their use in audio language modeling, speech, or music processing, which are the core focus of the inclusion criteria. Therefore, it does not meet the subject definition of discrete audio tokens used in audio-related sequence modeling and generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attributable and Scalable Opinion Summarization",
    "abstract": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.",
    "metadata": {
      "title": "Attributable and Scalable Opinion Summarization",
      "summary": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.",
      "abstract": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.473",
      "openalex_id": "https://openalex.org/W4385570501",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text-based opinion summarization using hierarchical discrete latent representations, not discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text-based opinion summarization using hierarchical discrete latent representations, not discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an unsupervised opinion summarization method focused on textual data from customer reviews, without any mention of audio data, discrete audio tokens, vector quantization, or any audio-related tokenization process. Therefore, the study does not align with the inclusion criteria related to discrete audio tokens, nor does it meet the specific technical coverage needed. It also does not describe any audio token generation process or usage in sequence or language models for audio. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an unsupervised opinion summarization method focused on textual data from customer reviews, without any mention of audio data, discrete audio tokens, vector quantization, or any audio-related tokenization process. Therefore, the study does not align with the inclusion criteria related to discrete audio tokens, nor does it meet the specific technical coverage needed. It also does not describe any audio token generation process or usage in sequence or language models for audio. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Tokenized and Continuous Embedding Compressions of Protein Sequence and Structure",
    "abstract": "Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.",
    "metadata": {
      "title": "Tokenized and Continuous Embedding Compressions of Protein Sequence and Structure",
      "summary": "Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.",
      "abstract": "Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.",
      "doi": "https://doi.org/10.1101/2024.08.06.606920",
      "openalex_id": "https://openalex.org/W4401409675",
      "arxiv_id": "",
      "publication_date": "2024-08-08",
      "published": "2024-08-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not a discrete audio token study: the work discusses tokenized and continuous embeddings for protein sequences/structures (CHEAP) and HPCT, not discrete audio representations or tokenization suitable for audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not a discrete audio token study: the work discusses tokenized and continuous embeddings for protein sequences/structures (CHEAP) and HPCT, not discrete audio representations or tokenization suitable for audio language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on protein sequence and structure embeddings and compression, with no focus on audio signals, audio tokens, or audio language modeling. The discrete tokens discussed are related to protein structure, not discrete audio tokens as defined in the inclusion criteria, and there is no indication of acoustic or semantic disentanglement for audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on protein sequence and structure embeddings and compression, with no focus on audio signals, audio tokens, or audio language modeling. The discrete tokens discussed are related to protein structure, not discrete audio tokens as defined in the inclusion criteria, and there is no indication of acoustic or semantic disentanglement for audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PCVAE: Generating Prior Context for Dialogue Response Generation",
    "abstract": "Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.",
    "metadata": {
      "title": "PCVAE: Generating Prior Context for Dialogue Response Generation",
      "summary": "Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.",
      "abstract": "Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.",
      "doi": "https://doi.org/10.24963/ijcai.2022/564",
      "openalex_id": "https://openalex.org/W4285599837",
      "arxiv_id": "",
      "publication_date": "2022-07-01",
      "published": "2022-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on dialogue generation with PCVAE and prior context; it does not involve discrete audio tokens, so it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on dialogue generation with PCVAE and prior context; it does not involve discrete audio tokens, so it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on dialogue response generation using a novel discrete variable called prior context within a variational autoencoder framework, without any mention of discrete audio tokens derived from continuous audio signals or relevant vector quantization methods; therefore, it does not meet the inclusion criteria focused on discrete audio token design and use for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on dialogue response generation using a novel discrete variable called prior context within a variational autoencoder framework, without any mention of discrete audio tokens derived from continuous audio signals or relevant vector quantization methods; therefore, it does not meet the inclusion criteria focused on discrete audio token design and use for audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phase-Aware Speech Enhancement With Complex Wiener Filter",
    "abstract": "In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.",
    "metadata": {
      "title": "Phase-Aware Speech Enhancement With Complex Wiener Filter",
      "summary": "In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.",
      "abstract": "In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.",
      "doi": "https://doi.org/10.1109/access.2023.3341919",
      "openalex_id": "https://openalex.org/W4389633906",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於相位感知語音增強與複雜韋納濾波器，提出的模型與損失函數雖具創新性，但未涉及離散音訊 token 的生成、量化或可用於語言模型的離散表徵，因此不符合主題中的離散 token 要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於相位感知語音增強與複雜韋納濾波器，提出的模型與損失函數雖具創新性，但未涉及離散音訊 token 的生成、量化或可用於語言模型的離散表徵，因此不符合主題中的離散 token 要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phase-aware speech enhancement using complex Wiener filter and speech-variance estimation models but does not involve any process of discretizing audio signals into tokens through vector quantization or related discrete token representations for use with language or sequence models, which is the core requirement in the inclusion criteria. There is no mention of discrete audio token generation, quantization, or token usage for downstream sequence modeling or audio language modeling. Thus, it does not meet the key inclusion criteria related to discrete audio token design and usage, nor are there grounds for exclusion based on the exclusion criteria other than irrelevant topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phase-aware speech enhancement using complex Wiener filter and speech-variance estimation models but does not involve any process of discretizing audio signals into tokens through vector quantization or related discrete token representations for use with language or sequence models, which is the core requirement in the inclusion criteria. There is no mention of discrete audio token generation, quantization, or token usage for downstream sequence modeling or audio language modeling. Thus, it does not meet the key inclusion criteria related to discrete audio token design and usage, nor are there grounds for exclusion based on the exclusion criteria other than irrelevant topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes",
    "abstract": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
    "metadata": {
      "title": "GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes",
      "summary": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
      "abstract": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
      "doi": "https://doi.org/10.1109/ijcb57857.2023.10449102",
      "openalex_id": "https://openalex.org/W4392411961",
      "arxiv_id": "",
      "publication_date": "2023-09-25",
      "published": "2023-09-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets transforming gait sequences using a discrete latent codebook and optimal transport in a vision/biometrics setting, not discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets transforming gait sequences using a discrete latent codebook and optimal transport in a vision/biometrics setting, not discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on modifying gait sequences using discrete codes derived from a high-compression model; however, its domain is gait recognition (biometric walking patterns), and it does not pertain to audio signals or discrete audio tokens as defined in the inclusion criteria. The tokens discussed are not derived from continuous audio signals like speech or music, nor used for audio language modeling or related audio downstream tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and cross-modal audio-language modeling, and no exclusion criteria apply otherwise.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on modifying gait sequences using discrete codes derived from a high-compression model; however, its domain is gait recognition (biometric walking patterns), and it does not pertain to audio signals or discrete audio tokens as defined in the inclusion criteria. The tokens discussed are not derived from continuous audio signals like speech or music, nor used for audio language modeling or related audio downstream tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and cross-modal audio-language modeling, and no exclusion criteria apply otherwise.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration",
    "abstract": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",
    "metadata": {
      "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration",
      "summary": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",
      "abstract": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",
      "doi": "https://doi.org/10.1109/wacv57701.2024.00508",
      "openalex_id": "https://openalex.org/W4394625601",
      "arxiv_id": "",
      "publication_date": "2024-01-03",
      "published": "2024-01-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image-based blind face restoration with texture transfer, not on discrete audio token design or audio token-based modeling, so it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image-based blind face restoration with texture transfer, not on discrete audio token design or audio token-based modeling, so it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on blind face restoration using neural texture extraction and reference images; it does not involve discrete audio tokens, audio signal quantization, or any audio-related sequence modeling, which are the core of the inclusion criteria. The paper is entirely unrelated to audio tokenization or sequence modeling for audio data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on blind face restoration using neural texture extraction and reference images; it does not involve discrete audio tokens, audio signal quantization, or any audio-related sequence modeling, which are the core of the inclusion criteria. The paper is entirely unrelated to audio tokenization or sequence modeling for audio data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
    "abstract": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.",
    "metadata": {
      "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
      "summary": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.",
      "abstract": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.893",
      "openalex_id": "https://openalex.org/W4389524339",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vector-quantized prompts for paraphrase generation in NLP, not on discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria centered on Discrete Audio Tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vector-quantized prompts for paraphrase generation in NLP, not on discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria centered on Discrete Audio Tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on \"Vector-Quantized Prompt Learning for Paraphrase Generation,\" which involves pre-trained language models and prompt learning to generate paraphrases, without addressing discrete audio tokens or audio signal processing; thus, it does not meet the inclusion criteria regarding discrete audio token generation or usage in audio language modeling or audio understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on \"Vector-Quantized Prompt Learning for Paraphrase Generation,\" which involves pre-trained language models and prompt learning to generate paraphrases, without addressing discrete audio tokens or audio signal processing; thus, it does not meet the inclusion criteria regarding discrete audio token generation or usage in audio language modeling or audio understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TAG2G: A Diffusion-Based Approach to Interlocutor-Aware Co-Speech Gesture Generation",
    "abstract": "Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.",
    "metadata": {
      "title": "TAG2G: A Diffusion-Based Approach to Interlocutor-Aware Co-Speech Gesture Generation",
      "summary": "Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.",
      "abstract": "Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.",
      "doi": "https://doi.org/10.3390/electronics13173364",
      "openalex_id": "https://openalex.org/W4401892801",
      "arxiv_id": "",
      "publication_date": "2024-08-24",
      "published": "2024-08-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses diffusion-based co-speech gesture generation and dyadic multimodal inputs; it does not propose discrete audio token representations, vector quantization, or token-based modeling essential to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses diffusion-based co-speech gesture generation and dyadic multimodal inputs; it does not propose discrete audio token representations, vector quantization, or token-based modeling essential to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on co-speech gesture generation using diffusion models and vector-quantized variational auto-encoders (VQVAE) considering multimodal inputs such as text, audio, and gestures. However, it does not specify the design or generation of discrete audio tokens from continuous audio signals as a primary contribution or core method, nor does it describe token generation processes for use in language or sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, and does not bring forward relevant tokenization or quantization workflows for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on co-speech gesture generation using diffusion models and vector-quantized variational auto-encoders (VQVAE) considering multimodal inputs such as text, audio, and gestures. However, it does not specify the design or generation of discrete audio tokens from continuous audio signals as a primary contribution or core method, nor does it describe token generation processes for use in language or sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, and does not bring forward relevant tokenization or quantization workflows for audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
    "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
    "metadata": {
      "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
      "summary": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
      "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1990",
      "openalex_id": "https://openalex.org/W3198082505",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on disentangled speech representations for multi-factor voice conversion and adversarial learning, but it does not discuss discrete audio tokens, vector quantization, or any tokenization method, so it does not meet the discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on disentangled speech representations for multi-factor voice conversion and adversarial learning, but it does not discuss discrete audio tokens, vector quantization, or any tokenization method, so it does not meet the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on disentangled speech representation learning via adversarial methods to improve voice conversion quality, without mentioning or involving discrete audio tokens generated by quantization methods or tokenization processes that enables modeling with language or sequence models; thus, it does not meet the inclusion criteria related to discrete token design and usage for modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on disentangled speech representation learning via adversarial methods to improve voice conversion quality, without mentioning or involving discrete audio tokens generated by quantization methods or tokenization processes that enables modeling with language or sequence models; thus, it does not meet the inclusion criteria related to discrete token design and usage for modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Contrastive Learning for Extracting Radar Word in the Hierarchical Model of Multifunction Radar",
    "abstract": "The analysis of intercepted multifunction radar (MFR) signals has attracted considerable attention in the field of cognitive electronic reconnaissance. The agility of pulse parameters makes it difficult to recognize their behavior states. Currently, most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a noncooperative way. This study develops a contrastive self-supervised method to adaptively segment and cluster MFR pulse sequences into radar words with little prior information. First, a convolutional neural network (CNN) is trained in a self-supervised manner to differentiate between adjacent and nonadjacent pulse group embeddings to learn robust feature representations from sequences of pulse descriptor words (PDWs). Next, based on the learned embeddings, test statistics are calculated for each PDW sequence to detect change points, which are, in turn, used to segment the sequence into multiple subsegments, each containing only one radar word. Finally, feature vectors are produced for clustering the subsegments into groups of radar words. Simulation results show that without using any labeled data, the proposed method can effectively extract the radar words of a hypothetical MFR under corrupted and overlapped pulse parameters and performs only slightly worse than the state-of-the-art fully supervised method.",
    "metadata": {
      "title": "Self-Supervised Contrastive Learning for Extracting Radar Word in the Hierarchical Model of Multifunction Radar",
      "summary": "The analysis of intercepted multifunction radar (MFR) signals has attracted considerable attention in the field of cognitive electronic reconnaissance. The agility of pulse parameters makes it difficult to recognize their behavior states. Currently, most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a noncooperative way. This study develops a contrastive self-supervised method to adaptively segment and cluster MFR pulse sequences into radar words with little prior information. First, a convolutional neural network (CNN) is trained in a self-supervised manner to differentiate between adjacent and nonadjacent pulse group embeddings to learn robust feature representations from sequences of pulse descriptor words (PDWs). Next, based on the learned embeddings, test statistics are calculated for each PDW sequence to detect change points, which are, in turn, used to segment the sequence into multiple subsegments, each containing only one radar word. Finally, feature vectors are produced for clustering the subsegments into groups of radar words. Simulation results show that without using any labeled data, the proposed method can effectively extract the radar words of a hypothetical MFR under corrupted and overlapped pulse parameters and performs only slightly worse than the state-of-the-art fully supervised method.",
      "abstract": "The analysis of intercepted multifunction radar (MFR) signals has attracted considerable attention in the field of cognitive electronic reconnaissance. The agility of pulse parameters makes it difficult to recognize their behavior states. Currently, most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a noncooperative way. This study develops a contrastive self-supervised method to adaptively segment and cluster MFR pulse sequences into radar words with little prior information. First, a convolutional neural network (CNN) is trained in a self-supervised manner to differentiate between adjacent and nonadjacent pulse group embeddings to learn robust feature representations from sequences of pulse descriptor words (PDWs). Next, based on the learned embeddings, test statistics are calculated for each PDW sequence to detect change points, which are, in turn, used to segment the sequence into multiple subsegments, each containing only one radar word. Finally, feature vectors are produced for clustering the subsegments into groups of radar words. Simulation results show that without using any labeled data, the proposed method can effectively extract the radar words of a hypothetical MFR under corrupted and overlapped pulse parameters and performs only slightly worse than the state-of-the-art fully supervised method.",
      "doi": "https://doi.org/10.1109/taes.2023.3323443",
      "openalex_id": "https://openalex.org/W4387609229",
      "arxiv_id": "",
      "publication_date": "2023-10-13",
      "published": "2023-10-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets radar signal processing and extraction of radar words from MFR pulses, not discrete audio tokens or tokenization for audio-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets radar signal processing and extraction of radar words from MFR pulses, not discrete audio tokens or tokenization for audio-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning and clustering methods applied to radar pulse sequences for extracting radar words in multifunction radar signal analysis. It does not relate to the domain of discrete audio tokens derived from continuous audio signals (speech, music, general audio) nor addresses token generation processes such as vector quantization for use in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their usage in sequence modeling or generation, nor does it fit exclusion criteria as it is unrelated.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning and clustering methods applied to radar pulse sequences for extracting radar words in multifunction radar signal analysis. It does not relate to the domain of discrete audio tokens derived from continuous audio signals (speech, music, general audio) nor addresses token generation processes such as vector quantization for use in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their usage in sequence modeling or generation, nor does it fit exclusion criteria as it is unrelated.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Promptvc: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
    "abstract": "Stylistic voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
    "metadata": {
      "title": "Promptvc: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
      "summary": "Stylistic voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
      "abstract": "Stylistic voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445804",
      "openalex_id": "https://openalex.org/W4392903591",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions using discrete tokens via HuBERT and K-Means as linguistic content in a latent-diffusion style voice-conversion system, but it does not present discrete audio token design as a core, well-delineated contribution with concrete token-generation/quantization details, and it lacks a clear publication date per the exclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions using discrete tokens via HuBERT and K-Means as linguistic content in a latent-diffusion style voice-conversion system, but it does not present discrete audio token design as a core, well-delineated contribution with concrete token-generation/quantization details, and it lacks a clear publication date per the exclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a style voice conversion method that uses HuBERT to extract discrete tokens, which are then processed and modified to serve as linguistic content minimizing residual style information. This indicates the use of discrete audio tokens generated via self-supervised learning (HuBERT) and clustering (K-Means), forming a discrete representation used for further modeling. The method description includes token extraction, replacement with cluster embeddings, and duration prediction, which meets the inclusion criteria of a detailed token generation process and usage for sequence modeling. The paper does not seem to rely on traditional codecs nor does it lack technical detail. Hence, it satisfies the inclusion criteria and excludes those mentioned for exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study proposes a style voice conversion method that uses HuBERT to extract discrete tokens, which are then processed and modified to serve as linguistic content minimizing residual style information. This indicates the use of discrete audio tokens generated via self-supervised learning (HuBERT) and clustering (K-Means), forming a discrete representation used for further modeling. The method description includes token extraction, replacement with cluster embeddings, and duration prediction, which meets the inclusion criteria of a detailed token generation process and usage for sequence modeling. The paper does not seem to rely on traditional codecs nor does it lack technical detail. Hence, it satisfies the inclusion criteria and excludes those mentioned for exclusion.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses HuBERT-derived discrete tokens with K‑Means clustering and subsequent processing (deduplication and duration prediction); JuniorMini judged this meets the SSL+clustering discrete-token pathway in the inclusion criteria, while JuniorNano excluded largely due to a missing publication date which is not provided in the input but is an external metadata check. The abstract explicitly describes extracting discrete tokens from HuBERT, replacing them with cluster-center embeddings as linguistic content, and using a differentiable duration predictor—this aligns with the inclusion path of SSL-based token extraction + clustering used for sequence modeling, although token-design is not the sole novelty and the abstract lacks full technical detail. Verdict (one-sentence): I lean to include (4) because the work uses SSL-derived discrete tokens (HuBERT + K‑Means) as a core component for sequence-style modeling and describes the token generation/usage pipeline sufficiently to meet the inclusion theme, but it is not an unambiguous yes (not a 5) due to limited detail in the abstract and uncertainty about publication metadata.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses HuBERT-derived discrete tokens with K‑Means clustering and subsequent processing (deduplication and duration prediction); JuniorMini judged this meets the SSL+clustering discrete-token pathway in the inclusion criteria, while JuniorNano excluded largely due to a missing publication date which is not provided in the input but is an external metadata check. The abstract explicitly describes extracting discrete tokens from HuBERT, replacing them with cluster-center embeddings as linguistic content, and using a differentiable duration predictor—this aligns with the inclusion path of SSL-based token extraction + clustering used for sequence modeling, although token-design is not the sole novelty and the abstract lacks full technical detail. Verdict (one-sentence): I lean to include (4) because the work uses SSL-derived discrete tokens (HuBERT + K‑Means) as a core component for sequence-style modeling and describes the token generation/usage pipeline sufficiently to meet the inclusion theme, but it is not an unambiguous yes (not a 5) due to limited detail in the abstract and uncertainty about publication metadata.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data",
    "abstract": "We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at https://mindslab-ai.github.io/cotatron, and the code with a pre-trained model will be made available soon.",
    "metadata": {
      "title": "Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data",
      "summary": "We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at https://mindslab-ai.github.io/cotatron, and the code with a pre-trained model will be made available soon.",
      "abstract": "We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at https://mindslab-ai.github.io/cotatron, and the code with a pre-trained model will be made available soon.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1542",
      "openalex_id": "https://openalex.org/W3020975377",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents a transcription-guided speech encoder for voice conversion using continuous features and does not describe discrete audio tokens, vector quantization, or token-level design necessary for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper presents a transcription-guided speech encoder for voice conversion using continuous features and does not describe discrete audio tokens, vector quantization, or token-level design necessary for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Cotatron, a transcription-guided speech encoder trained on multispeaker TTS datasets, focusing on voice conversion without requiring parallel data. Although it deals with speech representation and voice conversion, it does not mention the production or use of discrete audio tokens via vector quantization, residual quantization, or similar discrete representation mechanisms that are central to the inclusion criteria. The abstract lacks details on token generation, quantization methods, or their use as tokens for sequence or language modeling. Therefore, it does not meet the core inclusion criterion related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes Cotatron, a transcription-guided speech encoder trained on multispeaker TTS datasets, focusing on voice conversion without requiring parallel data. Although it deals with speech representation and voice conversion, it does not mention the production or use of discrete audio tokens via vector quantization, residual quantization, or similar discrete representation mechanisms that are central to the inclusion criteria. The abstract lacks details on token generation, quantization methods, or their use as tokens for sequence or language modeling. Therefore, it does not meet the core inclusion criterion related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Noisy-to-Noisy Voice Conversion Under Variations of Noisy Condition",
    "abstract": "Voiceconversion (VC) refers to the transformation of the speaker identity of a speech to the target one without altering the linguistic content. As recent VC techniques have made significant progress, implementing them in real-world scenarios is also considered, where speech data have some inevitable interferences, the most common of which are background sounds. On the other hand, background sounds are informative and need to be retained in some applications, such as VC in movies/videos. To address these issues, we have proposed a noisy-to-noisy (N2N) VC framework that does not rely on clean VC data and models the noisy speech directly by using noise as conditions. Previous experimental results have proven its effectiveness. In this article, we further improve its performance by introducing the pre-trained noise-conditioned VC model. Moreover, to further explore the impacts of introducing noise conditions, the performance in more realistic situations is evaluated in which the training set possesses speaker-dependent noisy conditions. The experimental results demonstrate the effectiveness of the pre-training strategy and the degradation of its performance under strict noisy conditions. We then proposed a noise augmentation method to overcome the limitation. Further experiments showed the effectiveness of the augmentation method.",
    "metadata": {
      "title": "Noisy-to-Noisy Voice Conversion Under Variations of Noisy Condition",
      "summary": "Voiceconversion (VC) refers to the transformation of the speaker identity of a speech to the target one without altering the linguistic content. As recent VC techniques have made significant progress, implementing them in real-world scenarios is also considered, where speech data have some inevitable interferences, the most common of which are background sounds. On the other hand, background sounds are informative and need to be retained in some applications, such as VC in movies/videos. To address these issues, we have proposed a noisy-to-noisy (N2N) VC framework that does not rely on clean VC data and models the noisy speech directly by using noise as conditions. Previous experimental results have proven its effectiveness. In this article, we further improve its performance by introducing the pre-trained noise-conditioned VC model. Moreover, to further explore the impacts of introducing noise conditions, the performance in more realistic situations is evaluated in which the training set possesses speaker-dependent noisy conditions. The experimental results demonstrate the effectiveness of the pre-training strategy and the degradation of its performance under strict noisy conditions. We then proposed a noise augmentation method to overcome the limitation. Further experiments showed the effectiveness of the augmentation method.",
      "abstract": "Voiceconversion (VC) refers to the transformation of the speaker identity of a speech to the target one without altering the linguistic content. As recent VC techniques have made significant progress, implementing them in real-world scenarios is also considered, where speech data have some inevitable interferences, the most common of which are background sounds. On the other hand, background sounds are informative and need to be retained in some applications, such as VC in movies/videos. To address these issues, we have proposed a noisy-to-noisy (N2N) VC framework that does not rely on clean VC data and models the noisy speech directly by using noise as conditions. Previous experimental results have proven its effectiveness. In this article, we further improve its performance by introducing the pre-trained noise-conditioned VC model. Moreover, to further explore the impacts of introducing noise conditions, the performance in more realistic situations is evaluated in which the training set possesses speaker-dependent noisy conditions. The experimental results demonstrate the effectiveness of the pre-training strategy and the degradation of its performance under strict noisy conditions. We then proposed a noise augmentation method to overcome the limitation. Further experiments showed the effectiveness of the augmentation method.",
      "doi": "https://doi.org/10.1109/taslp.2023.3313426",
      "openalex_id": "https://openalex.org/W4386902717",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on noisy-to-noisy voice conversion and noise-conditioned models, not on discrete audio tokens or tokenization/quantization methods; thus it does not meet the inclusion criteria focused on token-based discrete audio representations and methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on noisy-to-noisy voice conversion and noise-conditioned models, not on discrete audio tokens or tokenization/quantization methods; thus it does not meet the inclusion criteria focused on token-based discrete audio representations and methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on noisy-to-noisy voice conversion, primarily dealing with speech transformation under noise conditions rather than generating or utilizing discrete audio tokens derived from vector quantization or similar discrete representations for language or sequence modeling. The abstract lacks any description of discrete token generation processes, vector quantization techniques, or the use of discrete tokens as intermediate representations for audio language modeling or cross-modal generation, which are essential inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token design and use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on noisy-to-noisy voice conversion, primarily dealing with speech transformation under noise conditions rather than generating or utilizing discrete audio tokens derived from vector quantization or similar discrete representations for language or sequence modeling. The abstract lacks any description of discrete token generation processes, vector quantization techniques, or the use of discrete tokens as intermediate representations for audio language modeling or cross-modal generation, which are essential inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token design and use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Many-to-Many Unsupervised Speech Conversion From Nonparallel Corpora",
    "abstract": "We address a nonparallel data-driven many-to-many speech modeling and multimodal style conversion method. In this work, we train a speech conversion model for multiple domains rather than a specific source and target domain pair, and we generate diverse output speech signals from a given source domain speech by transferring some speech style-related characteristics while preserving its linguistic content information. The proposed method comprises a variational autoencoder (VAE)-based many-to-many speech conversion network with a Wasserstein generative adversarial network (WGAN) and a skip-connected autoencoder-based self-supervised learning network. The proposed conversion network trains the models by decomposing the spectral features of the input speech signal into a content factor that represents domain-invariant information and a style factor that represents domain-related information to automatically estimate the various speech styles of each domain, and the network converts the input speech signal to another domain using the computed content factor with the target style factor we want to change. Diverse and multimodal outputs can be generated by sampling different style factors. We also train models in a stable manner and improve the quality of generated outputs by sharing the discriminator of the VAE-based speech conversion network and that of the self-supervised learning network. We apply the proposed method to speaker conversion and perform the perceptual evaluations. Experimental results revealed that the proposed method obtained high accuracy of converted spectra, significantly improved the sound quality and speaker similarity of the converted speech, and contributed to stable model training.",
    "metadata": {
      "title": "Many-to-Many Unsupervised Speech Conversion From Nonparallel Corpora",
      "summary": "We address a nonparallel data-driven many-to-many speech modeling and multimodal style conversion method. In this work, we train a speech conversion model for multiple domains rather than a specific source and target domain pair, and we generate diverse output speech signals from a given source domain speech by transferring some speech style-related characteristics while preserving its linguistic content information. The proposed method comprises a variational autoencoder (VAE)-based many-to-many speech conversion network with a Wasserstein generative adversarial network (WGAN) and a skip-connected autoencoder-based self-supervised learning network. The proposed conversion network trains the models by decomposing the spectral features of the input speech signal into a content factor that represents domain-invariant information and a style factor that represents domain-related information to automatically estimate the various speech styles of each domain, and the network converts the input speech signal to another domain using the computed content factor with the target style factor we want to change. Diverse and multimodal outputs can be generated by sampling different style factors. We also train models in a stable manner and improve the quality of generated outputs by sharing the discriminator of the VAE-based speech conversion network and that of the self-supervised learning network. We apply the proposed method to speaker conversion and perform the perceptual evaluations. Experimental results revealed that the proposed method obtained high accuracy of converted spectra, significantly improved the sound quality and speaker similarity of the converted speech, and contributed to stable model training.",
      "abstract": "We address a nonparallel data-driven many-to-many speech modeling and multimodal style conversion method. In this work, we train a speech conversion model for multiple domains rather than a specific source and target domain pair, and we generate diverse output speech signals from a given source domain speech by transferring some speech style-related characteristics while preserving its linguistic content information. The proposed method comprises a variational autoencoder (VAE)-based many-to-many speech conversion network with a Wasserstein generative adversarial network (WGAN) and a skip-connected autoencoder-based self-supervised learning network. The proposed conversion network trains the models by decomposing the spectral features of the input speech signal into a content factor that represents domain-invariant information and a style factor that represents domain-related information to automatically estimate the various speech styles of each domain, and the network converts the input speech signal to another domain using the computed content factor with the target style factor we want to change. Diverse and multimodal outputs can be generated by sampling different style factors. We also train models in a stable manner and improve the quality of generated outputs by sharing the discriminator of the VAE-based speech conversion network and that of the self-supervised learning network. We apply the proposed method to speaker conversion and perform the perceptual evaluations. Experimental results revealed that the proposed method obtained high accuracy of converted spectra, significantly improved the sound quality and speaker similarity of the converted speech, and contributed to stable model training.",
      "doi": "https://doi.org/10.1109/access.2021.3058382",
      "openalex_id": "https://openalex.org/W3132220150",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study addresses nonparallel many-to-many speech conversion using VAE/WGAN and disentangles content/style from spectral features; it does not propose or employ discrete audio tokens, vector quantization, or any tokenization mechanism, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study addresses nonparallel many-to-many speech conversion using VAE/WGAN and disentangles content/style from spectral features; it does not propose or employ discrete audio tokens, vector quantization, or any tokenization mechanism, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on many-to-many speech conversion using variational autoencoders and GANs to convert speech styles while preserving linguistic content, but does not describe methods that produce discrete audio tokens via vector quantization or other discrete representation mechanisms as defined in the inclusion criteria. The paper lacks explicit mention of discrete token generation processes, token usage for sequence or language modeling, or quantization details. Therefore, it does not meet the key inclusion criteria related to discrete audio token representation or design, nor does it provide sufficient methodological detail about token formation to support inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on many-to-many speech conversion using variational autoencoders and GANs to convert speech styles while preserving linguistic content, but does not describe methods that produce discrete audio tokens via vector quantization or other discrete representation mechanisms as defined in the inclusion criteria. The paper lacks explicit mention of discrete token generation processes, token usage for sequence or language modeling, or quantization details. Therefore, it does not meet the key inclusion criteria related to discrete audio token representation or design, nor does it provide sufficient methodological detail about token formation to support inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Parallel Synthesis for Autoregressive Speech Generation",
    "abstract": "Autoregressive neural vocoders have achieved outstanding performance and are widely used in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it can generate highly natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech time sequence in parallel and then proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is first split into different frequency subbands. The proposed model generates a subband conditioned on the previously generated one. A full-band speech can then be reconstructed from these generated subbands. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance's length but to the number of subbands/bits. The inference efficiency is hence significantly increased. Besides, a post-filter is employed to sample audio signals from output posteriors, and its training objective is designed based on the characteristics of the proposed autoregressive methods. The experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with the baseline autoregressive and non-autoregressive vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability while synthesizing 44 kHz speech or utterances from unseen speakers.",
    "metadata": {
      "title": "Parallel Synthesis for Autoregressive Speech Generation",
      "summary": "Autoregressive neural vocoders have achieved outstanding performance and are widely used in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it can generate highly natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech time sequence in parallel and then proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is first split into different frequency subbands. The proposed model generates a subband conditioned on the previously generated one. A full-band speech can then be reconstructed from these generated subbands. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance's length but to the number of subbands/bits. The inference efficiency is hence significantly increased. Besides, a post-filter is employed to sample audio signals from output posteriors, and its training objective is designed based on the characteristics of the proposed autoregressive methods. The experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with the baseline autoregressive and non-autoregressive vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability while synthesizing 44 kHz speech or utterances from unseen speakers.",
      "abstract": "Autoregressive neural vocoders have achieved outstanding performance and are widely used in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it can generate highly natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech time sequence in parallel and then proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is first split into different frequency subbands. The proposed model generates a subband conditioned on the previously generated one. A full-band speech can then be reconstructed from these generated subbands. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance's length but to the number of subbands/bits. The inference efficiency is hence significantly increased. Besides, a post-filter is employed to sample audio signals from output posteriors, and its training objective is designed based on the characteristics of the proposed autoregressive methods. The experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with the baseline autoregressive and non-autoregressive vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability while synthesizing 44 kHz speech or utterances from unseen speakers.",
      "doi": "https://doi.org/10.1109/taslp.2023.3301212",
      "openalex_id": "https://openalex.org/W4385569627",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on frequency-wise and bit-wise autoregressive generation for speech (FAR/BAR) with 8-bit quantization, aiming at faster synthesis; it does not present discrete audio tokens or token-based modeling (e.g., VQ/RVQ, hierarchical tokenization) as the core representation or provide a reproducible token-generation workflow for downstream audio-language tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on frequency-wise and bit-wise autoregressive generation for speech (FAR/BAR) with 8-bit quantization, aiming at faster synthesis; it does not present discrete audio tokens or token-based modeling (e.g., VQ/RVQ, hierarchical tokenization) as the core representation or provide a reproducible token-generation workflow for downstream audio-language tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on autoregressive neural vocoders and proposes novel frequency-wise and bit-wise autoregressive generation methods to improve synthesis speed, but it does not discuss the generation or use of discrete audio tokens obtained through learned quantization or vector quantization methods essential for audio language modeling or semantic disentanglement as required by the inclusion criteria. Therefore, the core contributions do not align with discrete token design or usage central to the research theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on autoregressive neural vocoders and proposes novel frequency-wise and bit-wise autoregressive generation methods to improve synthesis speed, but it does not discuss the generation or use of discrete audio tokens obtained through learned quantization or vector quantization methods essential for audio language modeling or semantic disentanglement as required by the inclusion criteria. Therefore, the core contributions do not align with discrete token design or usage central to the research theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Self-supervised Learning for Multi-function Radar Behavior State Detection and Recognition",
    "abstract": "&lt;div&gt;The analysis of intercepted multi-function radar (MFR) signals has gained considerable attention in the field of cognitive electronic reconnaissance. With the rapid development of MFR, the switch between different work modes is becoming more flexible, increasing the agility of pulse parameters. Most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a non-cooperative way. This study develops a novel hierarchical contrastive self-supervise-based method for segmenting and clustering MFR pulse sequences. First, a convolutional neural network (CNN) with a limited receptive field is trained in a contrastive way to distinguish between pulse descriptor words (PDW) in the original order and the samples created by random permutations to detect the boundary between each radar word and perform segmentation. Afterward, the K-means++ algorithm with cosine distances is established to cluster the segmented PDWs according to the output vectors of the CNN’s last layer for radar words extraction. This segmenting and clustering process continues to go in the extracted radar word sequence, radar phase sequence, and so on, finishing the automatic extraction of MFR behavior states in the MFR hierarchical model. Simulation results show that without using any labeled data, the proposed method can effectively mine distinguishable patterns in the sequentially arriving PDWs and recognize the MFR behavior states under corrupted, overlapped pulse parameters.&lt;/div&gt;",
    "metadata": {
      "title": "Towards Self-supervised Learning for Multi-function Radar Behavior State Detection and Recognition",
      "summary": "&lt;div&gt;The analysis of intercepted multi-function radar (MFR) signals has gained considerable attention in the field of cognitive electronic reconnaissance. With the rapid development of MFR, the switch between different work modes is becoming more flexible, increasing the agility of pulse parameters. Most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a non-cooperative way. This study develops a novel hierarchical contrastive self-supervise-based method for segmenting and clustering MFR pulse sequences. First, a convolutional neural network (CNN) with a limited receptive field is trained in a contrastive way to distinguish between pulse descriptor words (PDW) in the original order and the samples created by random permutations to detect the boundary between each radar word and perform segmentation. Afterward, the K-means++ algorithm with cosine distances is established to cluster the segmented PDWs according to the output vectors of the CNN’s last layer for radar words extraction. This segmenting and clustering process continues to go in the extracted radar word sequence, radar phase sequence, and so on, finishing the automatic extraction of MFR behavior states in the MFR hierarchical model. Simulation results show that without using any labeled data, the proposed method can effectively mine distinguishable patterns in the sequentially arriving PDWs and recognize the MFR behavior states under corrupted, overlapped pulse parameters.&lt;/div&gt;",
      "abstract": "&lt;div&gt;The analysis of intercepted multi-function radar (MFR) signals has gained considerable attention in the field of cognitive electronic reconnaissance. With the rapid development of MFR, the switch between different work modes is becoming more flexible, increasing the agility of pulse parameters. Most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a non-cooperative way. This study develops a novel hierarchical contrastive self-supervise-based method for segmenting and clustering MFR pulse sequences. First, a convolutional neural network (CNN) with a limited receptive field is trained in a contrastive way to distinguish between pulse descriptor words (PDW) in the original order and the samples created by random permutations to detect the boundary between each radar word and perform segmentation. Afterward, the K-means++ algorithm with cosine distances is established to cluster the segmented PDWs according to the output vectors of the CNN’s last layer for radar words extraction. This segmenting and clustering process continues to go in the extracted radar word sequence, radar phase sequence, and so on, finishing the automatic extraction of MFR behavior states in the MFR hierarchical model. Simulation results show that without using any labeled data, the proposed method can effectively mine distinguishable patterns in the sequentially arriving PDWs and recognize the MFR behavior states under corrupted, overlapped pulse parameters.&lt;/div&gt;",
      "doi": "https://doi.org/10.36227/techrxiv.17976062",
      "openalex_id": "https://openalex.org/W4206192208",
      "arxiv_id": "",
      "publication_date": "2022-01-11",
      "published": "2022-01-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on self-supervised learning for multi-function radar state detection, which is unrelated to discrete audio tokens or audio language modeling; therefore it does not meet the stated inclusion criteria for discrete audio token design and associated methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on self-supervised learning for multi-function radar state detection, which is unrelated to discrete audio tokens or audio language modeling; therefore it does not meet the stated inclusion criteria for discrete audio token design and associated methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised learning methods for radar signal segmentation and recognition, specifically on multi-function radar behavior states, which does not relate to discrete audio tokens or audio representation as defined in the inclusion criteria. There is no mention of converting continuous audio signals into discrete tokens, nor details on token generation, vector quantization, or usage in language models. This topic is outside the scope of audio signal processing and discrete audio token generation, thus not meeting inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning methods for radar signal segmentation and recognition, specifically on multi-function radar behavior states, which does not relate to discrete audio tokens or audio representation as defined in the inclusion criteria. There is no mention of converting continuous audio signals into discrete tokens, nor details on token generation, vector quantization, or usage in language models. This topic is outside the scope of audio signal processing and discrete audio token generation, thus not meeting inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High-Quality Many-to-Many Voice Conversion Using Transitive Star Generative Adversarial Networks with Adaptive Instance Normalization",
    "abstract": "This paper proposes a novel high-quality nonparallel many-to-many voice conversion method based on transitive star generative adversarial networks with adaptive instance normalization (Trans-StarGAN-VC with AdaIN). First, we improve the structure of generator with TransNets to make full use of hierarchical features associated with speech naturalness. In TransNets, many shortcut connections share hierarchical features between encoding and decoding part to capture sufficient linguistic and semantic information, which helps to provide natural sounding converted speech and accelerate the convergence of training process. Second, by incorporating AdaIN for style transfer, we enable the generator to learn sufficient speaker characteristic information directly from speech instead of using attribute labels, which also provides a promising framework for one-shot VC. Objective and subjective experiments with nonparallel training data show that our method significantly outperforms StarGAN-VC in both speech naturalness and speaker similarity. The mean values of mean opinion score (MOS) and ABX are increased by 24.5% and 10.7%, respectively. The comparison of spectrogram also shows that our method can provide more complete harmonic structures and details, and effectively bridge the gap between converted speech and target speech.",
    "metadata": {
      "title": "High-Quality Many-to-Many Voice Conversion Using Transitive Star Generative Adversarial Networks with Adaptive Instance Normalization",
      "summary": "This paper proposes a novel high-quality nonparallel many-to-many voice conversion method based on transitive star generative adversarial networks with adaptive instance normalization (Trans-StarGAN-VC with AdaIN). First, we improve the structure of generator with TransNets to make full use of hierarchical features associated with speech naturalness. In TransNets, many shortcut connections share hierarchical features between encoding and decoding part to capture sufficient linguistic and semantic information, which helps to provide natural sounding converted speech and accelerate the convergence of training process. Second, by incorporating AdaIN for style transfer, we enable the generator to learn sufficient speaker characteristic information directly from speech instead of using attribute labels, which also provides a promising framework for one-shot VC. Objective and subjective experiments with nonparallel training data show that our method significantly outperforms StarGAN-VC in both speech naturalness and speaker similarity. The mean values of mean opinion score (MOS) and ABX are increased by 24.5% and 10.7%, respectively. The comparison of spectrogram also shows that our method can provide more complete harmonic structures and details, and effectively bridge the gap between converted speech and target speech.",
      "abstract": "This paper proposes a novel high-quality nonparallel many-to-many voice conversion method based on transitive star generative adversarial networks with adaptive instance normalization (Trans-StarGAN-VC with AdaIN). First, we improve the structure of generator with TransNets to make full use of hierarchical features associated with speech naturalness. In TransNets, many shortcut connections share hierarchical features between encoding and decoding part to capture sufficient linguistic and semantic information, which helps to provide natural sounding converted speech and accelerate the convergence of training process. Second, by incorporating AdaIN for style transfer, we enable the generator to learn sufficient speaker characteristic information directly from speech instead of using attribute labels, which also provides a promising framework for one-shot VC. Objective and subjective experiments with nonparallel training data show that our method significantly outperforms StarGAN-VC in both speech naturalness and speaker similarity. The mean values of mean opinion score (MOS) and ABX are increased by 24.5% and 10.7%, respectively. The comparison of spectrogram also shows that our method can provide more complete harmonic structures and details, and effectively bridge the gap between converted speech and target speech.",
      "doi": "https://doi.org/10.1142/s0218126621501887",
      "openalex_id": "https://openalex.org/W3114104342",
      "arxiv_id": "",
      "publication_date": "2020-12-24",
      "published": "2020-12-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於使用 Trans-StarGAN-VC 與 AdaIN 進行非平行的多對多語音轉換，未涉及離散音訊 token 的產生、量化或語義層次的離散表示，與所定義的離散音訊 token 主題與方法要求不符，因而應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於使用 Trans-StarGAN-VC 與 AdaIN 進行非平行的多對多語音轉換，未涉及離散音訊 token 的產生、量化或語義層次的離散表示，與所定義的離散音訊 token 主題與方法要求不符，因而應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a many-to-many voice conversion method using generative adversarial networks and adaptive instance normalization, aiming to improve speech naturalness and speaker similarity. However, it does not mention converting continuous audio signals into discrete audio tokens or using vector quantization or related discrete token generation techniques, which are key aspects of the inclusion criteria. Therefore, it does not meet the primary focus on discrete audio token design as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a many-to-many voice conversion method using generative adversarial networks and adaptive instance normalization, aiming to improve speech naturalness and speaker similarity. However, it does not mention converting continuous audio signals into discrete audio tokens or using vector quantization or related discrete token generation techniques, which are key aspects of the inclusion criteria. Therefore, it does not meet the primary focus on discrete audio token design as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Policy learning with partial observation and mechanical constraints for multi-person modeling",
    "abstract": "Extracting the rules of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. Biological agents generally have limited observation and mechanical constraints; however, most of the conventional data-driven models ignore such assumptions, resulting in lack of biological plausibility and model interpretability for behavioral analyses in biological and cognitive science. Here we propose sequential generative models with partial observation and mechanical constraints, which can visualize whose information the agents utilize and can generate biologically plausible actions. We formulate this as a decentralized multi-agent imitation learning problem, leveraging binary partial observation models with a Gumbel-Softmax reparameterization and policy models based on hierarchical variational recurrent neural networks with physical and biomechanical constraints. We investigate the empirical performances using real-world multi-person motion datasets from basketball and soccer games.",
    "metadata": {
      "title": "Policy learning with partial observation and mechanical constraints for multi-person modeling",
      "summary": "Extracting the rules of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. Biological agents generally have limited observation and mechanical constraints; however, most of the conventional data-driven models ignore such assumptions, resulting in lack of biological plausibility and model interpretability for behavioral analyses in biological and cognitive science. Here we propose sequential generative models with partial observation and mechanical constraints, which can visualize whose information the agents utilize and can generate biologically plausible actions. We formulate this as a decentralized multi-agent imitation learning problem, leveraging binary partial observation models with a Gumbel-Softmax reparameterization and policy models based on hierarchical variational recurrent neural networks with physical and biomechanical constraints. We investigate the empirical performances using real-world multi-person motion datasets from basketball and soccer games.",
      "abstract": "Extracting the rules of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. Biological agents generally have limited observation and mechanical constraints; however, most of the conventional data-driven models ignore such assumptions, resulting in lack of biological plausibility and model interpretability for behavioral analyses in biological and cognitive science. Here we propose sequential generative models with partial observation and mechanical constraints, which can visualize whose information the agents utilize and can generate biologically plausible actions. We formulate this as a decentralized multi-agent imitation learning problem, leveraging binary partial observation models with a Gumbel-Softmax reparameterization and policy models based on hierarchical variational recurrent neural networks with physical and biomechanical constraints. We investigate the empirical performances using real-world multi-person motion datasets from basketball and soccer games.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3042203388",
      "arxiv_id": "",
      "publication_date": "2020-07-07",
      "published": "2020-07-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets policy learning with partial observation and mechanical constraints for multi-person multi-agent modeling, using imitation learning and RNN-based dynamics, which is unrelated to discrete audio tokens and token-based audio modeling described in the inclusion criteria; thus it does not meet the required criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets policy learning with partial observation and mechanical constraints for multi-person multi-agent modeling, using imitation learning and RNN-based dynamics, which is unrelated to discrete audio tokens and token-based audio modeling described in the inclusion criteria; thus it does not meet the required criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on multi-agent modeling with partial observation and mechanical constraints in biological systems and sports motion data but do not mention discrete audio tokens, audio quantization, or neural audio codecs related to audio language modeling or token design, thus failing to meet the inclusion criteria and not addressing any aspect of discrete audio representations or token generation needed for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on multi-agent modeling with partial observation and mechanical constraints in biological systems and sports motion data but do not mention discrete audio tokens, audio quantization, or neural audio codecs related to audio language modeling or token design, thus failing to meet the inclusion criteria and not addressing any aspect of discrete audio representations or token generation needed for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input",
    "abstract": "Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.",
    "metadata": {
      "title": "Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input",
      "summary": "Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.",
      "abstract": "Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.",
      "doi": "https://doi.org/10.1073/pnas.2001844118",
      "openalex_id": "https://openalex.org/W3125087428",
      "arxiv_id": "",
      "publication_date": "2021-01-28",
      "published": "2021-01-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on mechanism-driven phonetic learning and large-scale simulations with realistic input, not on discrete audio token representations or token generation processes.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on mechanism-driven phonetic learning and large-scale simulations with realistic input, not on discrete audio token representations or token generation processes.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses early phonetic learning in infants through large-scale simulations and mechanisms of learning native language sounds, without mention of discrete audio token representations, vector quantization, or related methods for generating discrete audio tokens for sequence modeling; thus it does not meet the inclusion criteria focused on discrete audio token methods nor descriptions of token generation processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses early phonetic learning in infants through large-scale simulations and mechanisms of learning native language sounds, without mention of discrete audio token representations, vector quantization, or related methods for generating discrete audio tokens for sequence modeling; thus it does not meet the inclusion criteria focused on discrete audio token methods nor descriptions of token generation processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Early phonetic learning without phonetic categories -- Insights from large-scale simulations on realistic input",
    "abstract": "Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.",
    "metadata": {
      "title": "Early phonetic learning without phonetic categories -- Insights from large-scale simulations on realistic input",
      "summary": "Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.",
      "abstract": "Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.",
      "doi": "https://doi.org/10.31234/osf.io/fc4wh",
      "openalex_id": "https://openalex.org/W4230289889",
      "arxiv_id": "",
      "publication_date": "2019-05-01",
      "published": "2019-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses early phonetic learning using a distributional learning model but does not present or analyze discrete audio token representations (e.g., VQ/RVQ or semantic-disentangled tokens) or token-based audio language modeling, hence it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article addresses early phonetic learning using a distributional learning model but does not present or analyze discrete audio token representations (e.g., VQ/RVQ or semantic-disentangled tokens) or token-based audio language modeling, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on early phonetic learning mechanisms in infants using distributional learning algorithms on naturalistic speech, without discussing the design, generation, or use of discrete audio tokens or quantization methods relevant to audio language modeling or sequence models, which are the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on early phonetic learning mechanisms in infants using distributional learning algorithms on naturalistic speech, without discussing the design, generation, or use of discrete audio tokens or quantization methods relevant to audio language modeling or sequence models, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised Feature Learning for Speech Using Correspondence and Siamese Networks",
    "abstract": "In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.",
    "metadata": {
      "title": "Unsupervised Feature Learning for Speech Using Correspondence and Siamese Networks",
      "summary": "In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.",
      "abstract": "In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.",
      "doi": "https://doi.org/10.1109/lsp.2020.2973798",
      "openalex_id": "https://openalex.org/W3006358483",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on unsupervised frame-level acoustic feature learning (CAE, Siamese/CTriamese) with weak supervision for speech, and does not discuss discrete audio token generation/quantization (VQ/RVQ) or token-based representations for downstream language-model style processing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on unsupervised frame-level acoustic feature learning (CAE, Siamese/CTriamese) with weak supervision for speech, and does not discuss discrete audio token generation/quantization (VQ/RVQ) or token-based representations for downstream language-model style processing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised feature learning for speech using correspondence and Siamese networks to improve acoustic feature representations, but it does not mention the generation or use of discrete audio tokens, vector quantization, or any explicit tokenization process for use in language or sequence models, which is a core inclusion criterion. Therefore, it does not meet the main topic requirement of discrete audio token design and usage as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised feature learning for speech using correspondence and Siamese networks to improve acoustic feature representations, but it does not mention the generation or use of discrete audio tokens, vector quantization, or any explicit tokenization process for use in language or sequence models, which is a core inclusion criterion. Therefore, it does not meet the main topic requirement of discrete audio token design and usage as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
    "abstract": "This study tackles unsupervised subword modeling in the zero-resource\\nscenario, learning frame-level speech representation that is phonetically\\ndiscriminative and speaker-invariant, using only untranscribed speech for\\ntarget languages. Frame label acquisition is an essential step in solving this\\nproblem. High quality frame labels should be in good consistency with golden\\ntranscriptions and robust to speaker variation. We propose to improve frame\\nlabel acquisition in our previously adopted deep neural network-bottleneck\\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\\nand speaker identity information encoded in speech. By discarding or unifying\\nspeaker information, speaker-invariant features are learned and fed as inputs\\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\\\%$ and $0.6\\\\%$\\nabsolute ABX error rate reductions in across- and within-speaker conditions,\\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\\napproaches significantly outperform vocal tract length normalization in\\nimproving frame labeling and subword modeling.\\n",
    "metadata": {
      "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
      "summary": "This study tackles unsupervised subword modeling in the zero-resource\\nscenario, learning frame-level speech representation that is phonetically\\ndiscriminative and speaker-invariant, using only untranscribed speech for\\ntarget languages. Frame label acquisition is an essential step in solving this\\nproblem. High quality frame labels should be in good consistency with golden\\ntranscriptions and robust to speaker variation. We propose to improve frame\\nlabel acquisition in our previously adopted deep neural network-bottleneck\\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\\nand speaker identity information encoded in speech. By discarding or unifying\\nspeaker information, speaker-invariant features are learned and fed as inputs\\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\\\%$ and $0.6\\\\%$\\nabsolute ABX error rate reductions in across- and within-speaker conditions,\\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\\napproaches significantly outperform vocal tract length normalization in\\nimproving frame labeling and subword modeling.\\n",
      "abstract": "This study tackles unsupervised subword modeling in the zero-resource\\nscenario, learning frame-level speech representation that is phonetically\\ndiscriminative and speaker-invariant, using only untranscribed speech for\\ntarget languages. Frame label acquisition is an essential step in solving this\\nproblem. High quality frame labels should be in good consistency with golden\\ntranscriptions and robust to speaker variation. We propose to improve frame\\nlabel acquisition in our previously adopted deep neural network-bottleneck\\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\\nand speaker identity information encoded in speech. By discarding or unifying\\nspeaker information, speaker-invariant features are learned and fed as inputs\\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\\\%$ and $0.6\\\\%$\\nabsolute ABX error rate reductions in across- and within-speaker conditions,\\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\\napproaches significantly outperform vocal tract length normalization in\\nimproving frame labeling and subword modeling.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-1338",
      "openalex_id": "https://openalex.org/W2949510815",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets unsupervised subword modeling with disentangled speech representations for speaker-invariant features, but it does not present discrete audio tokens or a tokenization/quantiﬁcation pipeline as its main contribution, failing the primary inclusion criterion for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets unsupervised subword modeling with disentangled speech representations for speaker-invariant features, but it does not present discrete audio tokens or a tokenization/quantiﬁcation pipeline as its main contribution, failing the primary inclusion criterion for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning frame-level speech representations that are phonetically discriminative and speaker-invariant using untranscribed speech, applying a disentangled speech representation learning method (FHVAE) to enhance frame label acquisition; however, it does not clearly describe any discrete audio token generation process such as vector quantization, residual quantization, or an explicit tokenization method for audio to be used as discrete units in sequence or language modeling. The work seems centered on feature extraction and clustering for subword modeling rather than explicitly producing discrete tokens for downstream generative or understanding models as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning frame-level speech representations that are phonetically discriminative and speaker-invariant using untranscribed speech, applying a disentangled speech representation learning method (FHVAE) to enhance frame label acquisition; however, it does not clearly describe any discrete audio token generation process such as vector quantization, residual quantization, or an explicit tokenization method for audio to be used as discrete units in sequence or language modeling. The work seems centered on feature extraction and clustering for subword modeling rather than explicitly producing discrete tokens for downstream generative or understanding models as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling",
    "abstract": "This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n",
    "metadata": {
      "title": "Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling",
      "summary": "This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n",
      "abstract": "This research addresses the problem of acoustic modeling of low-resource\\nlanguages for which transcribed training data is absent. The goal is to learn\\nrobust frame-level feature representations that can be used to identify and\\ndistinguish subword-level speech units. The proposed feature representations\\ncomprise various types of multilingual bottleneck features (BNFs) that are\\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\\nkey problems is how to acquire high-quality frame labels for untranscribed\\ntraining data to facilitate supervised DNN training. It is shown that learning\\nof robust BNF representations can be achieved by effectively leveraging\\ntranscribed speech data and well-trained automatic speech recognition (ASR)\\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\\nASR systems can be applied to perform speaker adaptation with untranscribed\\ntraining data of the target language, and to decode the training speech into\\nframe-level labels for DNN training. It is also found that better frame labels\\ncan be generated by considering temporal dependency in speech when performing\\nframe clustering. The proposed methods of feature learning are evaluated on the\\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\\n2017 Challenge. The best performance achieved by our system is $9.7\\\\%$ in terms\\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\\nthe best systems reported recently. Lastly, our investigation reveals that the\\ncloseness between target languages and out-of-domain languages and the amount\\nof available training data for individual target languages could have\\nsignificant impact on the goodness of learned features.\\n",
      "doi": "https://doi.org/10.1109/taslp.2019.2937953",
      "openalex_id": "https://openalex.org/W2971041032",
      "arxiv_id": "",
      "publication_date": "2019-08-28",
      "published": "2019-08-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss multilingual bottleneck features and unsupervised subword modeling with out-of-domain ASR for frame-level representations, but do not address discrete audio tokens or tokenization (VQ/RVQ/semantic tokens) as the core method or contribution, so it should be excluded under the discrete audio tokens criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss multilingual bottleneck features and unsupervised subword modeling with out-of-domain ASR for frame-level representations, but do not address discrete audio tokens or tokenization (VQ/RVQ/semantic tokens) as the core method or contribution, so it should be excluded under the discrete audio tokens criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning frame-level feature representations (multilingual bottleneck features obtained via multi-task deep neural network training) to identify subword-level speech units for low-resource languages without transcribed data, but it does not explicitly describe a discrete audio token design or generation process involving vector quantization, residual quantization, hierarchical quantization, or semantic discretization that produces discrete tokens for sequence/language modeling as required by the inclusion criteria. The paper emphasizes feature learning rather than discrete token generation or usage in language models; thus, it does not meet the core requirement of having 'discrete audio tokens' as a main contribution or method. Hence, it should likely be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning frame-level feature representations (multilingual bottleneck features obtained via multi-task deep neural network training) to identify subword-level speech units for low-resource languages without transcribed data, but it does not explicitly describe a discrete audio token design or generation process involving vector quantization, residual quantization, hierarchical quantization, or semantic discretization that produces discrete tokens for sequence/language modeling as required by the inclusion criteria. The paper emphasizes feature learning rather than discrete token generation or usage in language models; thus, it does not meet the core requirement of having 'discrete audio tokens' as a main contribution or method. Hence, it should likely be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Acquiring language from speech by learning to remember and predict",
    "abstract": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.",
    "metadata": {
      "title": "Acquiring language from speech by learning to remember and predict",
      "summary": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.",
      "abstract": "Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.",
      "doi": "https://doi.org/10.18653/v1/2020.conll-1.15",
      "openalex_id": "https://openalex.org/W3102519966",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract studies language acquisition from speech via memory and prediction, it does not describe discrete audio token generation or token-based representations (no mention of VQ/RVQ, quantization, or token workflows), so it fails the discrete audio token core criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract studies language acquisition from speech via memory and prediction, it does not describe discrete audio token generation or token-based representations (no mention of VQ/RVQ, quantization, or token workflows), so it fails the discrete audio token core criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network model for language acquisition from speech, emphasizing memory and prediction in learning phonemic structures from unlabeled speech. However, it does not mention any discrete audio token generation, vector quantization, or similar discrete representation techniques central to the provided inclusion criteria. The core contribution is modeling linguistic representation learning rather than producing or utilizing discrete audio tokens for downstream sequence modeling or generation tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token design or usage, nor does it provide details on token generation or quantization processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network model for language acquisition from speech, emphasizing memory and prediction in learning phonemic structures from unlabeled speech. However, it does not mention any discrete audio token generation, vector quantization, or similar discrete representation techniques central to the provided inclusion criteria. The core contribution is modeling linguistic representation learning rather than producing or utilizing discrete audio tokens for downstream sequence modeling or generation tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token design or usage, nor does it provide details on token generation or quantization processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
    "abstract": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
    "metadata": {
      "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
      "summary": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "abstract": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1671",
      "openalex_id": "https://openalex.org/W3093121832",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a speech perception benchmark and comparing human perception with model representations, not on discrete audio token design, vector-quantization, or token-level workflows required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a speech perception benchmark and comparing human perception with model representations, not on discrete audio token design, vector-quantization, or token-level workflows required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a dataset and methods for comparing speech models and human perception on a phone discrimination task but does not focus on generating or designing discrete audio tokens via vector quantization or related discrete representation methods; it mainly evaluates model spaces against human perception without detailing token generation processes or usage for sequence modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a dataset and methods for comparing speech models and human perception on a phone discrimination task but does not focus on generating or designing discrete audio tokens via vector quantization or related discrete representation methods; it mainly evaluates model spaces against human perception without detailing token generation processes or usage for sequence modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Slowness Regularized Contrastive Predictive Coding for Acoustic Unit Discovery",
    "abstract": "Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.",
    "metadata": {
      "title": "Slowness Regularized Contrastive Predictive Coding for Acoustic Unit Discovery",
      "summary": "Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.",
      "abstract": "Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.",
      "doi": "https://doi.org/10.1109/taslp.2024.3350888",
      "openalex_id": "https://openalex.org/W4390887450",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it tackles acoustic unit discovery, the abstract does not specify any discrete tokenization/quantization mechanism or token-level modeling, so it likely does not meet the discrete audio token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it tackles acoustic unit discovery, the abstract does not specify any discrete tokenization/quantization mechanism or token-level modeling, so it likely does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised learning of acoustic representations and applies regularization techniques to improve Contrastive Predictive Coding for acoustic unit discovery. However, the abstract does not mention any discretization processes such as vector quantization, residual vector quantization, or any discrete token representations. The focus is on improving continuous representations for acoustic unit discovery, with no clear description or contribution regarding discrete audio token generation or usage for language/sequential modeling as defined in the inclusion criteria. Therefore, it does not meet the core requirement of discrete audio token design or clear token generation workflows.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning of acoustic representations and applies regularization techniques to improve Contrastive Predictive Coding for acoustic unit discovery. However, the abstract does not mention any discretization processes such as vector quantization, residual vector quantization, or any discrete token representations. The focus is on improving continuous representations for acoustic unit discovery, with no clear description or contribution regarding discrete audio token generation or usage for language/sequential modeling as defined in the inclusion criteria. Therefore, it does not meet the core requirement of discrete audio token design or clear token generation workflows.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling",
    "abstract": "We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.",
    "metadata": {
      "title": "Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling",
      "summary": "We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.",
      "abstract": "We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.",
      "doi": "https://doi.org/10.1109/taslp.2018.2852500",
      "openalex_id": "https://openalex.org/W2810166208",
      "arxiv_id": "",
      "publication_date": "2018-07-02",
      "published": "2018-07-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it discusses subword units in unsupervised modeling, the paper does not clearly describe a discrete audio token generation/quantization workflow as required, and the source page does not provide a clear publication/update date, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it discusses subword units in unsupervised modeling, the paper does not clearly describe a discrete audio token generation/quantization workflow as required, and the source page does not provide a clear publication/update date, thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a Dirichlet process mixture of mixtures model for unsupervised subword modeling, focusing on clustering and inference of components from speech feature vectors. However, the abstract does not describe any discrete audio token generation process, vector quantization, or other related quantization/representation methods which produce discrete tokens for audio language modeling or downstream sequence modeling tasks. There is no explicit mention of forming discrete symbols usable as tokens for language models, or related codebook/quantization details aligning with the inclusion criteria emphasizing discrete audio tokens and their usage in language/sequential modeling. Thus, it does not clearly meet the inclusion criteria centered on discrete audio token design and usage for language or sequence models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a Dirichlet process mixture of mixtures model for unsupervised subword modeling, focusing on clustering and inference of components from speech feature vectors. However, the abstract does not describe any discrete audio token generation process, vector quantization, or other related quantization/representation methods which produce discrete tokens for audio language modeling or downstream sequence modeling tasks. There is no explicit mention of forming discrete symbols usable as tokens for language models, or related codebook/quantization details aligning with the inclusion criteria emphasizing discrete audio tokens and their usage in language/sequential modeling. Thus, it does not clearly meet the inclusion criteria centered on discrete audio token design and usage for language or sequence models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
    "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.",
    "metadata": {
      "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
      "summary": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.",
      "abstract": "This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1664",
      "openalex_id": "https://openalex.org/W3145811386",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work tackles unsupervised discovery of phone-like discrete acoustic units via a two-stage approach with clustering, aligning with the discrete audio token concept (AUDs), and provides methodological details and experimental results; it goes beyond a mere demo, though it does not explicitly describe a vector-quantization-based tokenization in the abstract, which keeps the fit to token-discretization criteria uncertain but plausible.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work tackles unsupervised discovery of phone-like discrete acoustic units via a two-stage approach with clustering, aligning with the discrete audio token concept (AUDs), and provides methodological details and experimental results; it goes beyond a mere demo, though it does not explicitly describe a vector-quantization-based tokenization in the abstract, which keeps the fit to token-discretization criteria uncertain but plausible.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a two-stage approach to discover phone-like acoustic units from unlabeled speech data, focusing on learning subword-discriminative features and clustering them into discrete units. However, the method does not explicitly describe the production of discrete audio tokens (e.g., via vector quantization or similar methods) intended for use in language or sequence models, nor does it clarify token generation processes or usage for downstream modeling as defined in the inclusion criteria. The focus is on acoustic unit discovery rather than the design or deployment of discrete audio tokens as core contributions, and detailed token generation or quantization mechanisms for embedding in language models are not clear.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a two-stage approach to discover phone-like acoustic units from unlabeled speech data, focusing on learning subword-discriminative features and clustering them into discrete units. However, the method does not explicitly describe the production of discrete audio tokens (e.g., via vector quantization or similar methods) intended for use in language or sequence models, nor does it clarify token generation processes or usage for downstream modeling as defined in the inclusion criteria. The focus is on acoustic unit discovery rather than the design or deployment of discrete audio tokens as core contributions, and detailed token generation or quantization mechanisms for embedding in language models are not clear.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers diverged (one gave 4 noting alignment with AUD/token discretization; the other gave 2 noting absence of explicit VQ/usage for sequence models). The paper proposes a two-stage unsupervised AUD pipeline that learns subword-discriminative features (via multilingual OOD ASR) and then clusters segment-level representations (k-means) to produce phone-like clusters — i.e., discrete acoustic units. This fits the inclusion theme of forming discrete audio tokens via SSL representation + clustering/quantization, even though the abstract does not detail vector-quantization/RVQ architectures or explicitly demonstrate use of these tokens in downstream language/sequence models; it nonetheless presents a clear token-formation pipeline (feature learning + clustering) and evaluation of the discrete units. Given the match to the core requirement (discrete token formation through learned representations and clustering) but the limited methodological detail in the abstract about quantization specifics and downstream use, I recommend inclusion but not as an absolute certainty.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers diverged (one gave 4 noting alignment with AUD/token discretization; the other gave 2 noting absence of explicit VQ/usage for sequence models). The paper proposes a two-stage unsupervised AUD pipeline that learns subword-discriminative features (via multilingual OOD ASR) and then clusters segment-level representations (k-means) to produce phone-like clusters — i.e., discrete acoustic units. This fits the inclusion theme of forming discrete audio tokens via SSL representation + clustering/quantization, even though the abstract does not detail vector-quantization/RVQ architectures or explicitly demonstrate use of these tokens in downstream language/sequence models; it nonetheless presents a clear token-formation pipeline (feature learning + clustering) and evaluation of the discrete units. Given the match to the core requirement (discrete token formation through learned representations and clustering) but the limited methodological detail in the abstract about quantization specifics and downstream use, I recommend inclusion but not as an absolute certainty.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Feature Representation Learning using Sequence-to-sequence Autoencoder Architecture for Low-resource Language",
    "abstract": "In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.",
    "metadata": {
      "title": "Unsupervised Feature Representation Learning using Sequence-to-sequence Autoencoder Architecture for Low-resource Language",
      "summary": "In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.",
      "abstract": "In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.",
      "doi": "https://doi.org/10.1109/ccai50917.2021.9447504",
      "openalex_id": "https://openalex.org/W3171005929",
      "arxiv_id": "",
      "publication_date": "2021-05-07",
      "published": "2021-05-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe unsupervised continuous latent feature learning (FHVAE) and do not present discrete audio token design, token quantization, or a pathway to token-based language-modeling; thus it does not satisfy inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe unsupervised continuous latent feature learning (FHVAE) and do not present discrete audio token design, token quantization, or a pathway to token-based language-modeling; thus it does not satisfy inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised feature representation learning using a sequence-to-sequence autoencoder for low-resource languages, employing methods like FHVAE and attention mechanisms. However, it does not explicitly discuss the generation of discrete audio tokens via vector quantization or similar methods, nor does it detail token formation or usage for sequence/language models. Thus, it does not meet the key inclusion criterion regarding discrete audio token design and representation for downstream modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised feature representation learning using a sequence-to-sequence autoencoder for low-resource languages, employing methods like FHVAE and attention mechanisms. However, it does not explicitly discuss the generation of discrete audio tokens via vector quantization or similar methods, nor does it detail token formation or usage for sequence/language models. Thus, it does not meet the key inclusion criterion regarding discrete audio token design and representation for downstream modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages",
    "abstract": "(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.",
    "metadata": {
      "title": "Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages",
      "summary": "(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.",
      "abstract": "(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.",
      "doi": "https://doi.org/10.48550/arxiv.2007.15074",
      "openalex_id": "https://openalex.org/W3045592404",
      "arxiv_id": "",
      "publication_date": "2020-07-29",
      "published": "2020-07-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns unsupervised acoustic modeling and cross-lingual subword discovery but does not describe discrete audio token generation/quantization or token-based representations for language models, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns unsupervised acoustic modeling and cross-lingual subword discovery but does not describe discrete audio token generation/quantization or token-based representations for language models, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic modeling for zero-resource speech data, addressing subword unit discovery and frame-level feature representation that is linguistically discriminative and robust, with extensive use of cross-lingual knowledge. However, the abstract does not clearly indicate the use or production of discrete audio tokens via vector quantization or other discrete representation mechanisms for sequence modeling, nor does it detail the token generation process, quantization methods, or model structures that would meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic modeling for zero-resource speech data, addressing subword unit discovery and frame-level feature representation that is linguistically discriminative and robust, with extensive use of cross-lingual knowledge. However, the abstract does not clearly indicate the use or production of discrete audio tokens via vector quantization or other discrete representation mechanisms for sequence modeling, nor does it detail the token generation process, quantization methods, or model structures that would meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised Spoken Term Discovery on Untranscribed Speech",
    "abstract": "(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived \"phonemes\". The audio are labelled with these \"phonemes\" to obtain \"phoneme\" sequences. Unsupervised pattern discovery searches for repetitive patterns in the \"phoneme\" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.",
    "metadata": {
      "title": "Unsupervised Spoken Term Discovery on Untranscribed Speech",
      "summary": "(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived \"phonemes\". The audio are labelled with these \"phonemes\" to obtain \"phoneme\" sequences. Unsupervised pattern discovery searches for repetitive patterns in the \"phoneme\" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.",
      "abstract": "(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived \"phonemes\". The audio are labelled with these \"phonemes\" to obtain \"phoneme\" sequences. Unsupervised pattern discovery searches for repetitive patterns in the \"phoneme\" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.",
      "doi": "https://doi.org/10.48550/arxiv.2011.14060",
      "openalex_id": "https://openalex.org/W3110585608",
      "arxiv_id": "",
      "publication_date": "2020-11-28",
      "published": "2020-11-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on unsupervised discovery of discrete, phoneme-like tokens from untranscribed speech (via Acoustic Segment Modelling) and their use in keyword/topic extraction with downstream embeddings, matching the idea of discrete audio tokens for language-model-like processing rather than a mere demo.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on unsupervised discovery of discrete, phoneme-like tokens from untranscribed speech (via Acoustic Segment Modelling) and their use in keyword/topic extraction with downstream embeddings, matching the idea of discrete audio tokens for language-model-like processing rather than a mere demo.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised spoken term discovery based on acoustic segment modelling that produces 'phoneme' like discrete units from raw speech without phonetic knowledge, aligning with the concept of producing discrete audio tokens from continuous speech. However, the abstract lacks explicit details on the token generation process such as quantization, codebook design, or use within language models, and it emphasizes pattern discovery and information retrieval rather than modeling or generating sequences via discrete tokens. Without clear methodological details on tokenization or use of tokens for sequence modeling, it does not fully meet the inclusion criteria requiring discrete token generation design as a core contribution with detailed technical descriptions.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised spoken term discovery based on acoustic segment modelling that produces 'phoneme' like discrete units from raw speech without phonetic knowledge, aligning with the concept of producing discrete audio tokens from continuous speech. However, the abstract lacks explicit details on the token generation process such as quantization, codebook design, or use within language models, and it emphasizes pattern discovery and information retrieval rather than modeling or generating sequences via discrete tokens. Without clear methodological details on tokenization or use of tokens for sequence modeling, it does not fully meet the inclusion criteria requiring discrete token generation design as a core contribution with detailed technical descriptions.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one (eval=4) viewed the work as producing discrete, phoneme-like tokens from untranscribed speech and useful for downstream retrieval; the other (eval=3) flagged the lack of explicit tokenization details (quantization, codebook, VQ/RVQ, hierarchical design) and emphasis on pattern discovery/IR rather than explicit discrete-token design. My assessment: the abstract describes ASM-derived “phonemes” and phoneme sequences, but it does not describe a clear, modern discrete-token generation mechanism (e.g., vector quantization/codebook design, multi-level quantization, or how tokens are used in sequence/language models), nor does it provide full technical content needed for reproducibility. These omissions mean it fails the inclusion requirements that discrete audio token design and detailed token-generation procedures be a core contribution and reported in reviewable technical detail.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one (eval=4) viewed the work as producing discrete, phoneme-like tokens from untranscribed speech and useful for downstream retrieval; the other (eval=3) flagged the lack of explicit tokenization details (quantization, codebook, VQ/RVQ, hierarchical design) and emphasis on pattern discovery/IR rather than explicit discrete-token design. My assessment: the abstract describes ASM-derived “phonemes” and phoneme sequences, but it does not describe a clear, modern discrete-token generation mechanism (e.g., vector quantization/codebook design, multi-level quantization, or how tokens are used in sequence/language models), nor does it provide full technical content needed for reproducibility. These omissions mean it fails the inclusion requirements that discrete audio token design and detailed token-generation procedures be a core contribution and reported in reviewable technical detail.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery",
    "abstract": "Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.",
    "metadata": {
      "title": "Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery",
      "summary": "Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.",
      "abstract": "Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.",
      "doi": "https://doi.org/10.48550/arxiv.2105.01786",
      "openalex_id": "https://openalex.org/W3158457675",
      "arxiv_id": "",
      "publication_date": "2021-05-04",
      "published": "2021-05-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets speaker normalization and acoustic unit discovery rather than discrete audio token design/quantization for downstream language or sequence modeling, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets speaker normalization and acoustic unit discovery rather than discrete audio token design/quantization for downstream language or sequence modeling, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an unsupervised speaker normalization technique to improve acoustic unit discovery, which implies it deals with acoustic units but does not explicitly mention producing discrete audio tokens for use in language or sequence models; the method does not clarify the token generation process, quantization, or how these units serve as discrete tokens for downstream tasks, thus lacking sufficient detail on token formation and usage as per inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an unsupervised speaker normalization technique to improve acoustic unit discovery, which implies it deals with acoustic units but does not explicitly mention producing discrete audio tokens for use in language or sequence models; the method does not clarify the token generation process, quantization, or how these units serve as discrete tokens for downstream tasks, thus lacking sufficient detail on token formation and usage as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "The effectiveness of self-supervised representation learning in zero-resource subword modeling",
    "abstract": "For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.",
    "metadata": {
      "title": "The effectiveness of self-supervised representation learning in zero-resource subword modeling",
      "summary": "For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.",
      "abstract": "For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.",
      "doi": "https://doi.org/10.1109/ieeeconf53345.2021.9723318",
      "openalex_id": "https://openalex.org/W4214942696",
      "arxiv_id": "",
      "publication_date": "2021-10-31",
      "published": "2021-10-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates self-supervised representations for zero-resource subword modeling but does not address discrete audio tokens/quantization or token design, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates self-supervised representations for zero-resource subword modeling but does not address discrete audio tokens/quantization or token design, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised representation learning for zero-resource subword modeling, which is about learning feature representations for speech units, but it does not explicitly mention discrete audio tokens formed by quantization or clustering used for downstream sequence modeling; the paper emphasizes feature extraction and acoustic modeling rather than token discretization processes or semantic disentanglement for audio language modeling as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised representation learning for zero-resource subword modeling, which is about learning feature representations for speech units, but it does not explicitly mention discrete audio tokens formed by quantization or clustering used for downstream sequence modeling; the paper emphasizes feature extraction and acoustic modeling rather than token discretization processes or semantic disentanglement for audio language modeling as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MIPAD: Mini Program Analysis for Clone Detection using Static Analysis Techniques",
    "abstract": "In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.",
    "metadata": {
      "title": "MIPAD: Mini Program Analysis for Clone Detection using Static Analysis Techniques",
      "summary": "In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.",
      "abstract": "In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.",
      "doi": "https://doi.org/10.1109/frse58934.2023.00052",
      "openalex_id": "https://openalex.org/W4386597471",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is about clone detection for mini programs using static analysis and fuzzy hashing, not about discrete audio tokens or token-based audio representations, so it does not satisfy the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is about clone detection for mini programs using static analysis and fuzzy hashing, not about discrete audio tokens or token-based audio representations, so it does not satisfy the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on clone detection for mini programs using static analysis and does not relate to discrete audio tokenization, vector quantization, or audio language modeling, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on clone detection for mini programs using static analysis and does not relate to discrete audio tokenization, vector quantization, or audio language modeling, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
    "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
    "metadata": {
      "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
      "summary": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447523",
      "openalex_id": "https://openalex.org/W4392903389",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on FunCodec toolkit for neural speech codecs and discusses compression and downstream tasks; it does not present/discuss discrete audio token representations or tokenization design, which is the core topic of the inclusion criteria. Therefore it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on FunCodec toolkit for neural speech codecs and discusses compression and downstream tasks; it does not present/discuss discrete audio token representations or tokenization design, which is the core topic of the inclusion criteria. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit that includes reproducible training recipes and pretrained models for neural speech codec models such as SoundStream and Encodec. It emphasizes frequency-domain codec models achieving good compression quality with low computation and mentions integration with downstream tasks like speech recognition and personalized TTS. However, the abstract does not explicitly describe the discrete token generation process, vector quantization, or semantic disentanglement mechanisms fundamental to the inclusion criteria, nor does it clarify that the output tokens serve as discrete representations for language or sequence models. The description mostly focuses on codec performance and integration rather than the design of discrete audio tokens as a core contribution supporting audio language modeling or cross-modal generation. Hence, it lacks sufficient methodological detail on token discretization and usage for generation or understanding tasks as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit that includes reproducible training recipes and pretrained models for neural speech codec models such as SoundStream and Encodec. It emphasizes frequency-domain codec models achieving good compression quality with low computation and mentions integration with downstream tasks like speech recognition and personalized TTS. However, the abstract does not explicitly describe the discrete token generation process, vector quantization, or semantic disentanglement mechanisms fundamental to the inclusion criteria, nor does it clarify that the output tokens serve as discrete representations for language or sequence models. The description mostly focuses on codec performance and integration rather than the design of discrete audio tokens as a core contribution supporting audio language modeling or cross-modal generation. Hence, it lacks sufficient methodological detail on token discretization and usage for generation or understanding tasks as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry",
    "abstract": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
    "metadata": {
      "title": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry",
      "summary": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "abstract": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "doi": "https://doi.org/10.1016/j.enbenv.2024.03.010",
      "openalex_id": "https://openalex.org/W4393236964",
      "arxiv_id": "",
      "publication_date": "2024-03-27",
      "published": "2024-03-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article deals with LLMs in HVAC knowledge assessed via an exam and does not address discrete audio tokens or token design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article deals with LLMs in HVAC knowledge assessed via an exam and does not address discrete audio tokens or token design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating LLMs' performance in HVAC knowledge and skills, with no mention of audio signals, discrete audio tokenization, vector quantization, or any audio-related discrete token design methods required by the inclusion criteria. It neither discusses audio token generation nor any relevant signal processing or modeling techniques related to discrete audio tokens. Therefore, the study does not meet the inclusion criteria and falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating LLMs' performance in HVAC knowledge and skills, with no mention of audio signals, discrete audio tokenization, vector quantization, or any audio-related discrete token design methods required by the inclusion criteria. It neither discusses audio token generation nor any relevant signal processing or modeling techniques related to discrete audio tokens. Therefore, the study does not meet the inclusion criteria and falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
    "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
    "metadata": {
      "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
      "summary": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446991",
      "openalex_id": "https://openalex.org/W4392931320",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on the adverse impact of synthetic speech on ASR and mitigation strategies (fake audio detection, semantic prompts) rather than on discrete audio token design or tokenization methods (e.g., VQ/RVQ, hierarchical quantization) used for audio-language modeling; thus it does not meet the inclusion criteria focused on discrete audio token representations and is better excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on the adverse impact of synthetic speech on ASR and mitigation strategies (fake audio detection, semantic prompts) rather than on discrete audio token design or tokenization methods (e.g., VQ/RVQ, hierarchical quantization) used for audio-language modeling; thus it does not meet the inclusion criteria focused on discrete audio token representations and is better excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the impact of synthetic speech on automatic speech recognition and methods to differentiate synthetic from real speech using prompt labels and fake audio detection techniques. However, there is no mention of discrete audio token representation, vector quantization, token generation workflows, or any discrete token design, which are core inclusion criteria. Additionally, the study does not describe any process related to producing or using discrete tokens for sequence/language modeling, which is essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the impact of synthetic speech on automatic speech recognition and methods to differentiate synthetic from real speech using prompt labels and fake audio detection techniques. However, there is no mention of discrete audio token representation, vector quantization, token generation workflows, or any discrete token design, which are core inclusion criteria. Additionally, the study does not describe any process related to producing or using discrete tokens for sequence/language modeling, which is essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CMM: Code-Switching with Manifold Mixup for Cross-Lingual Spoken Language Understanding",
    "abstract": "Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.",
    "metadata": {
      "title": "CMM: Code-Switching with Manifold Mixup for Cross-Lingual Spoken Language Understanding",
      "summary": "Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.",
      "abstract": "Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.",
      "doi": "https://doi.org/10.1109/smc53992.2023.10393998",
      "openalex_id": "https://openalex.org/W4391331299",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on code-switching with mixup for cross-lingual SLU and does not present discrete audio token design or token-based representations (no VQ/RVQ/quantization details), so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on code-switching with mixup for cross-lingual SLU and does not present discrete audio token design or token-based representations (no VQ/RVQ/quantization details), so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cross-lingual spoken language understanding using code-switching and manifold mixup techniques, without mentioning discrete audio tokens or their quantization and generation processes, which are essential for inclusion based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cross-lingual spoken language understanding using code-switching and manifold mixup techniques, without mentioning discrete audio tokens or their quantization and generation processes, which are essential for inclusion based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Automatic Speech Recognition for Low-Resource Language by Data Augmentation",
    "abstract": "Automatic speech recognition (ASR) is one of the emergency tasks in human-computer interaction. There are many studies work in the field of building network architecture to deal with this task. While data augmentation was deeply discovered in computer vision, it is a big lag behind in the field of speech. Large data collection is not trivial, and in some cases it is impossible. The problem with data size is even more serious in some low-resource languages, such as Vietnamese. This study focuses on the data augmentation approach to deal with the small-size datasets to help the deep learning network better coverage in the ASR task. The experiment results on various configures of the VIVOS dataset, and two variations of the Conformer network architecture show that our proposed method gets promising improvement.",
    "metadata": {
      "title": "Improving Automatic Speech Recognition for Low-Resource Language by Data Augmentation",
      "summary": "Automatic speech recognition (ASR) is one of the emergency tasks in human-computer interaction. There are many studies work in the field of building network architecture to deal with this task. While data augmentation was deeply discovered in computer vision, it is a big lag behind in the field of speech. Large data collection is not trivial, and in some cases it is impossible. The problem with data size is even more serious in some low-resource languages, such as Vietnamese. This study focuses on the data augmentation approach to deal with the small-size datasets to help the deep learning network better coverage in the ASR task. The experiment results on various configures of the VIVOS dataset, and two variations of the Conformer network architecture show that our proposed method gets promising improvement.",
      "abstract": "Automatic speech recognition (ASR) is one of the emergency tasks in human-computer interaction. There are many studies work in the field of building network architecture to deal with this task. While data augmentation was deeply discovered in computer vision, it is a big lag behind in the field of speech. Large data collection is not trivial, and in some cases it is impossible. The problem with data size is even more serious in some low-resource languages, such as Vietnamese. This study focuses on the data augmentation approach to deal with the small-size datasets to help the deep learning network better coverage in the ASR task. The experiment results on various configures of the VIVOS dataset, and two variations of the Conformer network architecture show that our proposed method gets promising improvement.",
      "doi": "https://doi.org/10.1109/nics56915.2022.10013370",
      "openalex_id": "https://openalex.org/W4317564564",
      "arxiv_id": "",
      "publication_date": "2022-10-31",
      "published": "2022-10-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses data augmentation for ASR in a low-resource language and does not present discrete audio token design/formation or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses data augmentation for ASR in a low-resource language and does not present discrete audio token design/formation or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving automatic speech recognition through data augmentation methods, targeting low-resource languages. However, it does not mention or describe the design or usage of discrete audio tokens, vector quantization, or any form of tokenization that transforms continuous audio into discrete tokens for language modeling or sequence modeling, which is the core inclusion criterion. The method description also lacks detail about token generation processes or quantization, and no evidence suggests the involvement of discrete token representations for downstream tasks. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving automatic speech recognition through data augmentation methods, targeting low-resource languages. However, it does not mention or describe the design or usage of discrete audio tokens, vector quantization, or any form of tokenization that transforms continuous audio into discrete tokens for language modeling or sequence modeling, which is the core inclusion criterion. The method description also lacks detail about token generation processes or quantization, and no evidence suggests the involvement of discrete token representations for downstream tasks. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gloss Attention for Gloss-free Sign Language Translation",
    "abstract": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.",
    "metadata": {
      "title": "Gloss Attention for Gloss-free Sign Language Translation",
      "summary": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.",
      "abstract": "Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.00251",
      "openalex_id": "https://openalex.org/W4386076575",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on gloss-based sign language translation and a gloss attention mechanism, not on creating or evaluating discrete audio tokens (e.g., VQ/RVQ, semantic tokenization) for audio-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on gloss-based sign language translation and a gloss attention mechanism, not on creating or evaluating discrete audio tokens (e.g., VQ/RVQ, semantic tokenization) for audio-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language translation using gloss annotations and attention mechanisms to improve translation performance, but it does not involve discrete audio tokens, vector quantization, or related audio tokenization methods required by the inclusion criteria. Therefore, it does not meet the core topic of designing or utilizing discrete audio token representations for audio language modeling or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language translation using gloss annotations and attention mechanisms to improve translation performance, but it does not involve discrete audio tokens, vector quantization, or related audio tokenization methods required by the inclusion criteria. Therefore, it does not meet the core topic of designing or utilizing discrete audio token representations for audio language modeling or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation",
    "abstract": "Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \\url{https://SingGAN.github.io/}",
    "metadata": {
      "title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation",
      "summary": "Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \\url{https://SingGAN.github.io/}",
      "abstract": "Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \\url{https://SingGAN.github.io/}",
      "doi": "https://doi.org/10.1145/3503161.3547854",
      "openalex_id": "https://openalex.org/W3206191467",
      "arxiv_id": "",
      "publication_date": "2022-10-10",
      "published": "2022-10-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets high-fidelity singing voice generation using GAN/vocoder techniques, but does not present discrete audio tokens or a tokenization/quantization framework, thus fails the inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets high-fidelity singing voice generation using GAN/vocoder techniques, but does not present discrete audio tokens or a tokenization/quantization framework, thus fails the inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice synthesis using a generative adversarial network for high-fidelity audio generation, but does not address discrete audio token representation, vector quantization, or discrete token design, which are core to the inclusion criteria. It lacks discussion on token generation, quantization methods, or usage as tokens for language or sequence modeling, so it does not meet the inclusion criteria and does not meet any of the technical coverage needs related to discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice synthesis using a generative adversarial network for high-fidelity audio generation, but does not address discrete audio token representation, vector quantization, or discrete token design, which are core to the inclusion criteria. It lacks discussion on token generation, quantization methods, or usage as tokens for language or sequence modeling, so it does not meet the inclusion criteria and does not meet any of the technical coverage needs related to discrete tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation",
    "abstract": "While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).",
    "metadata": {
      "title": "DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation",
      "summary": "While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).",
      "abstract": "While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-main.709",
      "openalex_id": "https://openalex.org/W4389519423",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "While the work uses a discrete speech unit space in diffusion, the abstract does not detail a discrete audio token design, token generation flow, or quantization scheme as a core methodological contribution, making it unclear whether it satisfies the discrete-token inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "While the work uses a discrete speech unit space in diffusion, the abstract does not detail a discrete audio token design, token generation flow, or quantization scheme as a core methodological contribution, making it unclear whether it satisfies the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a novel diffusion model for textless direct speech-to-speech translation, highlighting the use of continuous and discrete speech unit spaces in the diffusion process, indicating that discrete speech units are central to the method. However, there is no detailed description of the token generation process, quantization approach, or explicit token design as discrete audio tokens with vector quantization or related methods, which are core to the inclusion criteria. The abstract focuses more on the modeling approach and performance rather than the discrete token generation design or explanation necessary for evaluation against the criteria. Therefore, it partially matches the theme but lacks clear details on discrete token design and usage as per the inclusion criteria, leaning towards exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a novel diffusion model for textless direct speech-to-speech translation, highlighting the use of continuous and discrete speech unit spaces in the diffusion process, indicating that discrete speech units are central to the method. However, there is no detailed description of the token generation process, quantization approach, or explicit token design as discrete audio tokens with vector quantization or related methods, which are core to the inclusion criteria. The abstract focuses more on the modeling approach and performance rather than the discrete token generation design or explanation necessary for evaluation against the criteria. Therefore, it partially matches the theme but lacks clear details on discrete token design and usage as per the inclusion criteria, leaning towards exclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract describes a diffusion-based model that operates in continuous speech representation space and a discrete speech-unit space for the backward process, and two junior reviewers flagged uncertainty because the abstract does not specify how the discrete tokens are produced (e.g., VQ/RVQ/codebook design, quantization/encoding pipeline) or whether discrete-token design is a core, reproducible contribution; the inclusion criteria require the paper to make discrete audio token design/production a primary methodological contribution with sufficient methodological detail. Therefore, based on the provided abstract alone (which lacks token-generation/quantization details and reproducibility information), it does not meet the inclusion requirements.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract describes a diffusion-based model that operates in continuous speech representation space and a discrete speech-unit space for the backward process, and two junior reviewers flagged uncertainty because the abstract does not specify how the discrete tokens are produced (e.g., VQ/RVQ/codebook design, quantization/encoding pipeline) or whether discrete-token design is a core, reproducible contribution; the inclusion criteria require the paper to make discrete audio token design/production a primary methodological contribution with sufficient methodological detail. Therefore, based on the provided abstract alone (which lacks token-generation/quantization details and reproducibility information), it does not meet the inclusion requirements.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "P D N: A Priori Dictionary Network for Fashion Parsing",
    "abstract": "The task of fashion parsing aims to assign pixel-level labels to clothing targets; thereby, parsing models are required to have good contextual recognition ability. However, the shapes of clothing components are complex, and the types are difficult to distinguish. Recent solutions focus on improving datasets and supplying abundant priori information, but the utilization of features by more efficient methods is rarely explored. In this paper, we propose a multi-scale fashion parsing model called the Priori Dictionary Network (PDN), which includes a priori attention module and a multi-scale backbone. The priori attention module extracts high dimensional features from our designed clothing average template as a priori information dictionary (priori dictionary, PD), and the PD is utilized to activate the feature maps of a CNN from a multi-scale attention mechanism. The backbone is derived from classical models, and five side paths are designed to leverage the richer features of local and global contextual representations. To measure the performance of our method, we evaluated the model on four public datasets, the CFPD, UTFR-SBD3, ModaNet and LIP, and the experimental results show that our model stands out from other State of the Art in all four datasets. This method can assist with the labeling problem of clothing datasets.",
    "metadata": {
      "title": "P D N: A Priori Dictionary Network for Fashion Parsing",
      "summary": "The task of fashion parsing aims to assign pixel-level labels to clothing targets; thereby, parsing models are required to have good contextual recognition ability. However, the shapes of clothing components are complex, and the types are difficult to distinguish. Recent solutions focus on improving datasets and supplying abundant priori information, but the utilization of features by more efficient methods is rarely explored. In this paper, we propose a multi-scale fashion parsing model called the Priori Dictionary Network (PDN), which includes a priori attention module and a multi-scale backbone. The priori attention module extracts high dimensional features from our designed clothing average template as a priori information dictionary (priori dictionary, PD), and the PD is utilized to activate the feature maps of a CNN from a multi-scale attention mechanism. The backbone is derived from classical models, and five side paths are designed to leverage the richer features of local and global contextual representations. To measure the performance of our method, we evaluated the model on four public datasets, the CFPD, UTFR-SBD3, ModaNet and LIP, and the experimental results show that our model stands out from other State of the Art in all four datasets. This method can assist with the labeling problem of clothing datasets.",
      "abstract": "The task of fashion parsing aims to assign pixel-level labels to clothing targets; thereby, parsing models are required to have good contextual recognition ability. However, the shapes of clothing components are complex, and the types are difficult to distinguish. Recent solutions focus on improving datasets and supplying abundant priori information, but the utilization of features by more efficient methods is rarely explored. In this paper, we propose a multi-scale fashion parsing model called the Priori Dictionary Network (PDN), which includes a priori attention module and a multi-scale backbone. The priori attention module extracts high dimensional features from our designed clothing average template as a priori information dictionary (priori dictionary, PD), and the PD is utilized to activate the feature maps of a CNN from a multi-scale attention mechanism. The backbone is derived from classical models, and five side paths are designed to leverage the richer features of local and global contextual representations. To measure the performance of our method, we evaluated the model on four public datasets, the CFPD, UTFR-SBD3, ModaNet and LIP, and the experimental results show that our model stands out from other State of the Art in all four datasets. This method can assist with the labeling problem of clothing datasets.",
      "doi": "https://doi.org/10.3390/app14083509",
      "openalex_id": "https://openalex.org/W4395010962",
      "arxiv_id": "",
      "publication_date": "2024-04-22",
      "published": "2024-04-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a vision-based fashion parsing model (PDN) using a priori dictionary; there is no discrete audio tokenization, audio signal processing, or quantization content, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a vision-based fashion parsing model (PDN) using a priori dictionary; there is no discrete audio tokenization, audio signal processing, or quantization content, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fashion parsing using a priori dictionary networks for image segmentation of clothing, with no mention or focus on discrete audio tokens, audio signal processing, quantization methods for audio, or language/sequence modeling of audio tokens, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fashion parsing using a priori dictionary networks for image segmentation of clothing, with no mention or focus on discrete audio tokens, audio signal processing, quantization methods for audio, or language/sequence modeling of audio tokens, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-Concept Learning for Scene Graph Generation",
    "abstract": "Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model's performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.",
    "metadata": {
      "title": "Multi-Concept Learning for Scene Graph Generation",
      "summary": "Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model's performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.",
      "abstract": "Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model's performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.",
      "doi": "https://doi.org/10.1109/tip.2025.3540296",
      "openalex_id": "https://openalex.org/W4408100020",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses scene graph generation and concept-level learning for vision models, not discrete audio tokens or tokenization pipelines, so it fails to meet inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses scene graph generation and concept-level learning for vision models, not discrete audio tokens or tokenization pipelines, so it fails to meet inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on scene graph generation and concept-level imbalance in visual predicates, with no mention or indication of discrete audio tokens, vector quantization, audio sequence modeling, or related audio domain topics as outlined in the inclusion criteria. Therefore, it clearly does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on scene graph generation and concept-level imbalance in visual predicates, with no mention or indication of discrete audio tokens, vector quantization, audio sequence modeling, or related audio domain topics as outlined in the inclusion criteria. Therefore, it clearly does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data Redaction from Conditional Generative Models",
    "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
    "metadata": {
      "title": "Data Redaction from Conditional Generative Models",
      "summary": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "doi": "https://doi.org/10.1109/satml59370.2024.00035",
      "openalex_id": "https://openalex.org/W4396815655",
      "arxiv_id": "",
      "publication_date": "2024-04-09",
      "published": "2024-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses redacting conditioning information in generative models and does not propose or analyze discrete audio token representations or tokenization methods, so it does not meet the 'Discrete Audio Tokens' criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses redacting conditioning information in generative models and does not propose or analyze discrete audio token representations or tokenization methods, so it does not meet the 'Discrete Audio Tokens' criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on post-editing conditional generative models to redact undesirable content, specifically in text-to-image and text-to-speech tasks; however, it does not describe designing or generating discrete audio tokens from continuous audio signals using vector quantization or semantic-driven discretization mechanisms, nor does it provide details on token generation processes or their use as sequence model inputs. Therefore, it does not meet the inclusion criteria related to discrete audio token design and application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on post-editing conditional generative models to redact undesirable content, specifically in text-to-image and text-to-speech tasks; however, it does not describe designing or generating discrete audio tokens from continuous audio signals using vector quantization or semantic-driven discretization mechanisms, nor does it provide details on token generation processes or their use as sequence model inputs. Therefore, it does not meet the inclusion criteria related to discrete audio token design and application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
    "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
    "metadata": {
      "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
      "summary": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "doi": "https://doi.org/10.1051/itmconf/20246901003",
      "openalex_id": "https://openalex.org/W4405379347",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a text-to-text chatbot and database curation for Darija STT/TTS; it does not present discrete audio token design, vector quantization, or token-based modeling necessary for the inclusion criteria, so it does not meet the requirements.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a text-to-text chatbot and database curation for Darija STT/TTS; it does not present discrete audio token design, vector quantization, or token-based modeling necessary for the inclusion criteria, so it does not meet the requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the development of an intelligent chatbot system for Moroccan agriculture with emphasis on Darija language interactions, involving NLP techniques and database curation; however, it does not discuss or propose any discrete audio token representations or tokenization processes involving vector quantization or related methods as core research contributions. The abstract lacks details on token generation processes, quantization mechanisms, or use of discrete audio tokens for audio language modeling, which are central to the inclusion criteria. No description of audio token design, discrete representations, or associated experimental results is provided, making it not aligned with the inclusion requirements and fits none of the exclusion criteria besides the lack of token focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the development of an intelligent chatbot system for Moroccan agriculture with emphasis on Darija language interactions, involving NLP techniques and database curation; however, it does not discuss or propose any discrete audio token representations or tokenization processes involving vector quantization or related methods as core research contributions. The abstract lacks details on token generation processes, quantization mechanisms, or use of discrete audio tokens for audio language modeling, which are central to the inclusion criteria. No description of audio token design, discrete representations, or associated experimental results is provided, making it not aligned with the inclusion requirements and fits none of the exclusion criteria besides the lack of token focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
    "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
    "metadata": {
      "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
      "summary": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "doi": "https://doi.org/10.18653/v1/2023.iwslt-1.33",
      "openalex_id": "https://openalex.org/W4385571610",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes a S2S translation pipeline and does not address discrete audio tokens or token quantization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes a S2S translation pipeline and does not address discrete audio tokens or token quantization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a speech-to-speech translation system composed of speech-to-text translation and text-to-speech synthesis models, but it does not mention the use or design of discrete audio tokens, vector quantization, or any tokenization process that serves as the core contribution or main methodology. There is also no clear description of a discrete token generation process applicable to language or sequence modeling, nor sufficient technical detail on tokenization or quantization as required. Therefore, the study does not meet the inclusion criteria focusing on discrete audio token representations, nor does it provide detailed technical methods that can be inspected or reproduced.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a speech-to-speech translation system composed of speech-to-text translation and text-to-speech synthesis models, but it does not mention the use or design of discrete audio tokens, vector quantization, or any tokenization process that serves as the core contribution or main methodology. There is also no clear description of a discrete token generation process applicable to language or sequence modeling, nor sufficient technical detail on tokenization or quantization as required. Therefore, the study does not meet the inclusion criteria focusing on discrete audio token representations, nor does it provide detailed technical methods that can be inspected or reproduced.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LightCodec: A High Fidelity Neural Audio Codec with Low Computation Complexity",
    "abstract": "The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).",
    "metadata": {
      "title": "LightCodec: A High Fidelity Neural Audio Codec with Low Computation Complexity",
      "summary": "The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).",
      "abstract": "The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447532",
      "openalex_id": "https://openalex.org/W4392902628",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item presents a high-fidelity neural audio codec with low computation and quantization improvements, but it does not introduce discrete audio tokens or a token-based pipeline for downstream models, thus failing the inclusion criteria for discrete audio token design.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item presents a high-fidelity neural audio codec with low computation and quantization improvements, but it does not introduce discrete audio tokens or a token-based pipeline for downstream models, thus failing the inclusion criteria for discrete audio token design.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a neural audio codec designed for low computation complexity and high fidelity, focusing on subband interaction and quantization compensation to improve efficiency and reduce error; however, it does not explicitly describe producing discrete audio tokens or their use for sequence modeling, nor does it detail token generation processes or semantic disentanglement aimed at language/audio model integration, which are core to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a neural audio codec designed for low computation complexity and high fidelity, focusing on subband interaction and quantization compensation to improve efficiency and reduce error; however, it does not explicitly describe producing discrete audio tokens or their use for sequence modeling, nor does it detail token generation processes or semantic disentanglement aimed at language/audio model integration, which are core to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Maskmark: Robust Neuralwatermarking for Real and Synthetic Speech",
    "abstract": "High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.",
    "metadata": {
      "title": "Maskmark: Robust Neuralwatermarking for Real and Synthetic Speech",
      "summary": "High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.",
      "abstract": "High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447253",
      "openalex_id": "https://openalex.org/W4392904158",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於神經水印的魯棒性與檢測，與離散音訊 token 的設計與量化流程無直接關聯，且未提供可審查的離散 token 形成機制，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於神經水印的魯棒性與檢測，與離散音訊 token 的設計與量化流程無直接關聯，且未提供可審查的離散 token 形成機制，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a neural watermarking technique for speech to embed secure signatures in audio for traceability and robustness against transformations, but it does not mention generating discrete audio tokens, quantization, or any tokenization mechanism intended for sequence or language modeling, which is essential for inclusion. Therefore, it does not meet the core inclusion criteria defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a neural watermarking technique for speech to embed secure signatures in audio for traceability and robustness against transformations, but it does not mention generating discrete audio tokens, quantization, or any tokenization mechanism intended for sequence or language modeling, which is essential for inclusion. Therefore, it does not meet the core inclusion criteria defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Determination of optimal formats for digital image compression",
    "abstract": "Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.",
    "metadata": {
      "title": "Determination of optimal formats for digital image compression",
      "summary": "Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.",
      "abstract": "Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.",
      "doi": "https://doi.org/10.17163/ings.n33.2025.01",
      "openalex_id": "https://openalex.org/W4406233769",
      "arxiv_id": "",
      "publication_date": "2025-01-10",
      "published": "2025-01-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns image compression formats and tools, not discrete audio tokens or token-based representations for audio modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns image compression formats and tools, not discrete audio tokens or token-based representations for audio modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on determining optimal digital image compression formats and tools, specifically analyzing image file types and compression outcomes. There is no mention of audio, discrete audio tokens, vector quantization, or any related audio signal processing methods. Therefore, this study does not meet the inclusion criteria centered on discrete audio token generation and usage for audio language modeling or related tasks, nor does it meet any advanced audio representation requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on determining optimal digital image compression formats and tools, specifically analyzing image file types and compression outcomes. There is no mention of audio, discrete audio tokens, vector quantization, or any related audio signal processing methods. Therefore, this study does not meet the inclusion criteria centered on discrete audio token generation and usage for audio language modeling or related tasks, nor does it meet any advanced audio representation requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Expressive TTS Training With Frame and Style Reconstruction Loss",
    "abstract": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.",
    "metadata": {
      "title": "Expressive TTS Training With Frame and Style Reconstruction Loss",
      "summary": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.",
      "abstract": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.",
      "doi": "https://doi.org/10.1109/taslp.2021.3076369",
      "openalex_id": "https://openalex.org/W3168542456",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Tacotron-based TTS training with frame-level and utterance-level perceptual losses for expressiveness, and does not discuss discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Tacotron-based TTS training with frame-level and utterance-level perceptual losses for expressiveness, and does not discuss discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving training strategies for Tacotron-based text-to-speech systems, emphasizing prosody and style reconstruction loss, but it does not involve the creation or use of discrete audio tokens such as vector quantization or similar quantization techniques that transform continuous audio into discrete token sets typically used for language or sequence modeling. Moreover, it does not describe token generation processes or discrete representation learning, which are core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving training strategies for Tacotron-based text-to-speech systems, emphasizing prosody and style reconstruction loss, but it does not involve the creation or use of discrete audio tokens such as vector quantization or similar quantization techniques that transform continuous audio into discrete token sets typically used for language or sequence modeling. Moreover, it does not describe token generation processes or discrete representation learning, which are core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Naturalness and Controllability of Sequence-to-Sequence Speech Synthesis by Learning Local Prosody Representations",
    "abstract": "State-of-the-art neural text-to-speech (TTS) networks are trained with a large amount of speech data, which significantly improves the quality of synthetic speech compared with traditional approaches. However, the prosody and controllability of the generated speech is still insufficient, especially in tonal languages. Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence or words. In this study, we extended Tacotron2 with a pitch prediction task to capture discrete pitch-related representations. Specifically, the learned pitch-related suprasegmental information is fed simultaneously with traditional character features into the decoder to generate final Mel spectrogram. Experiments show that the proposed method can improve the quality of the generated speech (mean opinion score of 4.37 vs. 4.22). Moreover, we demonstrated that we can easily achieve word-level pitch control during generation by changing local pitch-related representations before passing them to the decoder network.",
    "metadata": {
      "title": "Improving Naturalness and Controllability of Sequence-to-Sequence Speech Synthesis by Learning Local Prosody Representations",
      "summary": "State-of-the-art neural text-to-speech (TTS) networks are trained with a large amount of speech data, which significantly improves the quality of synthetic speech compared with traditional approaches. However, the prosody and controllability of the generated speech is still insufficient, especially in tonal languages. Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence or words. In this study, we extended Tacotron2 with a pitch prediction task to capture discrete pitch-related representations. Specifically, the learned pitch-related suprasegmental information is fed simultaneously with traditional character features into the decoder to generate final Mel spectrogram. Experiments show that the proposed method can improve the quality of the generated speech (mean opinion score of 4.37 vs. 4.22). Moreover, we demonstrated that we can easily achieve word-level pitch control during generation by changing local pitch-related representations before passing them to the decoder network.",
      "abstract": "State-of-the-art neural text-to-speech (TTS) networks are trained with a large amount of speech data, which significantly improves the quality of synthetic speech compared with traditional approaches. However, the prosody and controllability of the generated speech is still insufficient, especially in tonal languages. Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence or words. In this study, we extended Tacotron2 with a pitch prediction task to capture discrete pitch-related representations. Specifically, the learned pitch-related suprasegmental information is fed simultaneously with traditional character features into the decoder to generate final Mel spectrogram. Experiments show that the proposed method can improve the quality of the generated speech (mean opinion score of 4.37 vs. 4.22). Moreover, we demonstrated that we can easily achieve word-level pitch control during generation by changing local pitch-related representations before passing them to the decoder network.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414720",
      "openalex_id": "https://openalex.org/W3160844600",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It targets local prosody representations and pitch control in seq2seq TTS, not discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "It targets local prosody representations and pitch control in seq2seq TTS, not discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving naturalness and controllability of TTS by predicting pitch-related representations, but it does not explicitly involve transforming continuous audio signals into discrete tokens through vector quantization or similar methods, nor does it describe a token generation process for use in sequence or language models; thus, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving naturalness and controllability of TTS by predicting pitch-related representations, but it does not explicitly involve transforming continuous audio signals into discrete tokens through vector quantization or similar methods, nor does it describe a token generation process for use in sequence or language models; thus, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech",
    "abstract": "Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.",
    "metadata": {
      "title": "Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech",
      "summary": "Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.",
      "abstract": "Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383526",
      "openalex_id": "https://openalex.org/W3097112431",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses emotional voice conversion and disentanglement of emotion and prosody using VAW-GAN, without proposing or detailing discrete audio token representations or tokenization suitable for language-model-style downstream tasks, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses emotional voice conversion and disentanglement of emotion and prosody using VAW-GAN, without proposing or detailing discrete audio token representations or tokenization suitable for language-model-style downstream tasks, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on emotional voice conversion using a VAW-GAN framework to disentangle and recompose emotional elements in speech, but it does not mention any discrete audio token generation, vector quantization, or related discrete representations intended for use by language or sequence models. The emphasis is on spectral and prosodic feature conversion rather than producing discrete token representations for downstream audio language modeling or generation tasks. Therefore, it does not meet the inclusion criteria which require discrete token design as a main contribution, nor does it address token formation mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on emotional voice conversion using a VAW-GAN framework to disentangle and recompose emotional elements in speech, but it does not mention any discrete audio token generation, vector quantization, or related discrete representations intended for use by language or sequence models. The emphasis is on spectral and prosodic feature conversion rather than producing discrete token representations for downstream audio language modeling or generation tasks. Therefore, it does not meet the inclusion criteria which require discrete token design as a main contribution, nor does it address token formation mechanisms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Srcodec: Split-Residual Vector Quantization for Neural Speech Codec",
    "abstract": "End-to-end neural speech coding achieves state-of-the-art performance by using residual vector quantization. However, it is a challenge to quantize the latent variables with as few bits as possible. In this paper, we propose SRCodec, a neural speech codec that relies on a fully convolutional encoder/decoder network with specifically proposed split-residual vector quantization. In particular, it divides the latent representation into two parts with the same dimensions. We utilize two different quantizers to quantize the low-dimensional features and the residual between the low- and high-dimensional features. Meanwhile, we propose a dual attention module in split-residual vector quantization to improve information sharing along both dimensions. Both subjective and objective evaluations demonstrate that the effectiveness of our proposed method can achieve a higher quality of reconstructed speech at 0.95 kbps than Lyra-v1 at 3 kbps and Encodec at 3 kbps.",
    "metadata": {
      "title": "Srcodec: Split-Residual Vector Quantization for Neural Speech Codec",
      "summary": "End-to-end neural speech coding achieves state-of-the-art performance by using residual vector quantization. However, it is a challenge to quantize the latent variables with as few bits as possible. In this paper, we propose SRCodec, a neural speech codec that relies on a fully convolutional encoder/decoder network with specifically proposed split-residual vector quantization. In particular, it divides the latent representation into two parts with the same dimensions. We utilize two different quantizers to quantize the low-dimensional features and the residual between the low- and high-dimensional features. Meanwhile, we propose a dual attention module in split-residual vector quantization to improve information sharing along both dimensions. Both subjective and objective evaluations demonstrate that the effectiveness of our proposed method can achieve a higher quality of reconstructed speech at 0.95 kbps than Lyra-v1 at 3 kbps and Encodec at 3 kbps.",
      "abstract": "End-to-end neural speech coding achieves state-of-the-art performance by using residual vector quantization. However, it is a challenge to quantize the latent variables with as few bits as possible. In this paper, we propose SRCodec, a neural speech codec that relies on a fully convolutional encoder/decoder network with specifically proposed split-residual vector quantization. In particular, it divides the latent representation into two parts with the same dimensions. We utilize two different quantizers to quantize the low-dimensional features and the residual between the low- and high-dimensional features. Meanwhile, we propose a dual attention module in split-residual vector quantization to improve information sharing along both dimensions. Both subjective and objective evaluations demonstrate that the effectiveness of our proposed method can achieve a higher quality of reconstructed speech at 0.95 kbps than Lyra-v1 at 3 kbps and Encodec at 3 kbps.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445966",
      "openalex_id": "https://openalex.org/W4392903887",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses split-residual vector quantization in a neural speech codec, implying discrete latent representations; however it does not explicitly frame these discrete representations as tokens for downstream sequence modeling or language-model style tasks, nor detail a token-generation workflow for such use, so it's uncertain whether it fully meets the discrete audio tokens inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The paper uses split-residual vector quantization in a neural speech codec, implying discrete latent representations; however it does not explicitly frame these discrete representations as tokens for downstream sequence modeling or language-model style tasks, nor detail a token-generation workflow for such use, so it's uncertain whether it fully meets the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a neural speech codec using split-residual vector quantization to quantize latent variables efficiently, which aligns with vector quantization approaches generating discrete audio tokens. However, the abstract does not clearly state whether the generated tokens are explicitly used for downstream sequence or language modeling tasks (such as audio language modeling or generation), nor does it explicate semantic or acoustic disentanglement aspects or usage beyond compression/codecs. The method details regarding token generation and usage in language models are insufficiently described in the abstract, making it unclear if it meets all inclusion criteria fully.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a neural speech codec using split-residual vector quantization to quantize latent variables efficiently, which aligns with vector quantization approaches generating discrete audio tokens. However, the abstract does not clearly state whether the generated tokens are explicitly used for downstream sequence or language modeling tasks (such as audio language modeling or generation), nor does it explicate semantic or acoustic disentanglement aspects or usage beyond compression/codecs. The method details regarding token generation and usage in language models are insufficiently described in the abstract, making it unclear if it meets all inclusion criteria fully.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I examined the two junior reviews (both rated 3/uncertain) and the abstract: the paper proposes a split-residual vector quantization neural codec (so it likely produces discrete latent codes via RVQ), but the abstract focuses on compression quality at very low bitrates and does not describe token-generation workflow, codebook/layer design, or any use of the discrete codes for sequence/language modeling or downstream semantic tasks; it therefore does not meet the inclusion requirement that discrete audio token design and explicit token usage for sequence/LM tasks be a core contribution and be described. Brief explanation: although VQ/RVQ is used, the abstract lacks explicit tokenization details and downstream/token-use framing required for inclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I examined the two junior reviews (both rated 3/uncertain) and the abstract: the paper proposes a split-residual vector quantization neural codec (so it likely produces discrete latent codes via RVQ), but the abstract focuses on compression quality at very low bitrates and does not describe token-generation workflow, codebook/layer design, or any use of the discrete codes for sequence/language modeling or downstream semantic tasks; it therefore does not meet the inclusion requirement that discrete audio token design and explicit token usage for sequence/LM tasks be a core contribution and be described. Brief explanation: although VQ/RVQ is used, the abstract lacks explicit tokenization details and downstream/token-use framing required for inclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Lace: A Light-Weight, Causal Model for Enhancing Coded Speech Through Adaptive Convolutions",
    "abstract": "Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.",
    "metadata": {
      "title": "Lace: A Light-Weight, Causal Model for Enhancing Coded Speech Through Adaptive Convolutions",
      "summary": "Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.",
      "abstract": "Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.",
      "doi": "https://doi.org/10.1109/waspaa58266.2023.10248150",
      "openalex_id": "https://openalex.org/W4386764386",
      "arxiv_id": "",
      "publication_date": "2023-09-15",
      "published": "2023-09-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets lightweight causal modeling to enhance encoded speech (Opus) rather than forming or using discrete audio tokens through vector quantization or semantic-driven tokenization, so it does not satisfy the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets lightweight causal modeling to enhance encoded speech (Opus) rather than forming or using discrete audio tokens through vector quantization or semantic-driven tokenization, so it does not satisfy the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a deep neural network model for enhancing coded speech via adaptive convolutions, targeting low complexity and integration with an existing codec (Opus). However, it does not describe generating discrete audio tokens through vector quantization or related discretization mechanisms, nor does it address token design for sequence models or language modeling. It mainly addresses signal enhancement within traditional audio coding frameworks without producing discrete token representations for downstream generation or understanding tasks, thus not meeting the inclusion criterion of discrete token design as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a deep neural network model for enhancing coded speech via adaptive convolutions, targeting low complexity and integration with an existing codec (Opus). However, it does not describe generating discrete audio tokens through vector quantization or related discretization mechanisms, nor does it address token design for sequence models or language modeling. It mainly addresses signal enhancement within traditional audio coding frameworks without producing discrete token representations for downstream generation or understanding tasks, thus not meeting the inclusion criterion of discrete token design as a core contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An AudioCodec Based on the Perceptual Equality between the Original and Restored Audio Signals",
    "abstract": "A method for lossy audio data compression (AudioCodec) is presented. It allows for improving objective quality of the restored audio signal by 25% at a bitrate of 390 kbps and 55% at a bitrate of 64 kbps compared to the AAC MPEG-4 format. The proposed method of audio data compression is based on an advanced theory of lossy audio data compression (TLAC), which is also introduced in the article. The improvement in the objective quality of the reconstructed audio signal (according to the standardized PEAQ measure) is achieved because the TLAC overcomes issues in modern lossy audio data compression methods related to the use of psychoacoustic principles of human sound perception, including after overcoming the \"psychoacoustic compression limit\" of the audio signal (i.e. the moment in perceptual coding when the available bit budget is insufficient to encode all spectral components with the accuracy required from a psychoacoustic perspective). This allows for achieving perceptual equality between the original and reconstructed audio signals. As an analysis of the state of the art, solutions for both lossless and lossy audio data compression, as well as those using artificial intelligence, are considered. In all modern lossy audio data compression methods, the procedure for selecting the spectral components to be preserved, as well as the permissible quantization error, is carried out through a series of highly complex procedures collectively referred to as the \"psychoacoustic model of the lossy audio compression method\". In a strict sense, perceptual equality between the spectra of the original and restored signals has not been proven by any research group and, therefore, cannot be guaranteed by them. Independent experts regularly publish tests demonstrating that modern audio codecs have issues with certain audio signals. The article proposes an AudioCodec based on the perceptual equality between the original and restored audio signals, which is based on the new ideas of the theory of lossy audio compression (TLAC). These ideas guarantee the achievement of perceptual equality between the original and restored audio signals at different bitrates, therefore, the AudioCodec built on its basis is free from the above-mentioned issues and, as a result, significantly outperforms modern AudioCodecs in terms of the objective quality of the restored audio signal, as measured by PEAQ.",
    "metadata": {
      "title": "An AudioCodec Based on the Perceptual Equality between the Original and Restored Audio Signals",
      "summary": "A method for lossy audio data compression (AudioCodec) is presented. It allows for improving objective quality of the restored audio signal by 25% at a bitrate of 390 kbps and 55% at a bitrate of 64 kbps compared to the AAC MPEG-4 format. The proposed method of audio data compression is based on an advanced theory of lossy audio data compression (TLAC), which is also introduced in the article. The improvement in the objective quality of the reconstructed audio signal (according to the standardized PEAQ measure) is achieved because the TLAC overcomes issues in modern lossy audio data compression methods related to the use of psychoacoustic principles of human sound perception, including after overcoming the \"psychoacoustic compression limit\" of the audio signal (i.e. the moment in perceptual coding when the available bit budget is insufficient to encode all spectral components with the accuracy required from a psychoacoustic perspective). This allows for achieving perceptual equality between the original and reconstructed audio signals. As an analysis of the state of the art, solutions for both lossless and lossy audio data compression, as well as those using artificial intelligence, are considered. In all modern lossy audio data compression methods, the procedure for selecting the spectral components to be preserved, as well as the permissible quantization error, is carried out through a series of highly complex procedures collectively referred to as the \"psychoacoustic model of the lossy audio compression method\". In a strict sense, perceptual equality between the spectra of the original and restored signals has not been proven by any research group and, therefore, cannot be guaranteed by them. Independent experts regularly publish tests demonstrating that modern audio codecs have issues with certain audio signals. The article proposes an AudioCodec based on the perceptual equality between the original and restored audio signals, which is based on the new ideas of the theory of lossy audio compression (TLAC). These ideas guarantee the achievement of perceptual equality between the original and restored audio signals at different bitrates, therefore, the AudioCodec built on its basis is free from the above-mentioned issues and, as a result, significantly outperforms modern AudioCodecs in terms of the objective quality of the restored audio signal, as measured by PEAQ.",
      "abstract": "A method for lossy audio data compression (AudioCodec) is presented. It allows for improving objective quality of the restored audio signal by 25% at a bitrate of 390 kbps and 55% at a bitrate of 64 kbps compared to the AAC MPEG-4 format. The proposed method of audio data compression is based on an advanced theory of lossy audio data compression (TLAC), which is also introduced in the article. The improvement in the objective quality of the reconstructed audio signal (according to the standardized PEAQ measure) is achieved because the TLAC overcomes issues in modern lossy audio data compression methods related to the use of psychoacoustic principles of human sound perception, including after overcoming the \"psychoacoustic compression limit\" of the audio signal (i.e. the moment in perceptual coding when the available bit budget is insufficient to encode all spectral components with the accuracy required from a psychoacoustic perspective). This allows for achieving perceptual equality between the original and reconstructed audio signals. As an analysis of the state of the art, solutions for both lossless and lossy audio data compression, as well as those using artificial intelligence, are considered. In all modern lossy audio data compression methods, the procedure for selecting the spectral components to be preserved, as well as the permissible quantization error, is carried out through a series of highly complex procedures collectively referred to as the \"psychoacoustic model of the lossy audio compression method\". In a strict sense, perceptual equality between the spectra of the original and restored signals has not been proven by any research group and, therefore, cannot be guaranteed by them. Independent experts regularly publish tests demonstrating that modern audio codecs have issues with certain audio signals. The article proposes an AudioCodec based on the perceptual equality between the original and restored audio signals, which is based on the new ideas of the theory of lossy audio compression (TLAC). These ideas guarantee the achievement of perceptual equality between the original and restored audio signals at different bitrates, therefore, the AudioCodec built on its basis is free from the above-mentioned issues and, as a result, significantly outperforms modern AudioCodecs in terms of the objective quality of the restored audio signal, as measured by PEAQ.",
      "doi": "https://doi.org/10.15622/ia.24.2.3",
      "openalex_id": "https://openalex.org/W4409222133",
      "arxiv_id": "",
      "publication_date": "2025-04-01",
      "published": "2025-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a perceptual-equality audio codec and TLAC, but does not discuss discrete audio tokens, token generation, VQ/RVQ, or token-based representations for language-model-style processing; it lacks the required token-centric methodological details and criteria to be included under the discrete-audio-token theme.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a perceptual-equality audio codec and TLAC, but does not discuss discrete audio tokens, token generation, VQ/RVQ, or token-based representations for language-model-style processing; it lacks the required token-centric methodological details and criteria to be included under the discrete-audio-token theme.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a lossy audio compression method aiming to improve the objective quality of restored audio signals compared to existing codecs, based on theoretical advances in psychoacoustic modeling. However, it does not explicitly discuss discrete audio tokens, vector quantization, or token generation processes aligned with language or sequence modeling tasks. It primarily addresses traditional audio codec improvement rather than proposing or evaluating discrete token representations for audio understanding or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a lossy audio compression method aiming to improve the objective quality of restored audio signals compared to existing codecs, based on theoretical advances in psychoacoustic modeling. However, it does not explicitly discuss discrete audio tokens, vector quantization, or token generation processes aligned with language or sequence modeling tasks. It primarily addresses traditional audio codec improvement rather than proposing or evaluating discrete token representations for audio understanding or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic generation of subword units for speech recognition systems",
    "abstract": "Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.",
    "metadata": {
      "title": "Automatic generation of subword units for speech recognition systems",
      "summary": "Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.",
      "abstract": "Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.",
      "doi": "https://doi.org/10.1109/89.985546",
      "openalex_id": "https://openalex.org/W2167655920",
      "arxiv_id": "",
      "publication_date": "2002-01-01",
      "published": "2002-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss automatic design of subword units for ASR, which aligns with the idea of discrete audio tokens used by language/sequence models, but the abstract does not explicitly detail a tokenization/quantization mechanism (e.g., VQ/RVQ, codebooks) or provide full methodological specifics.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss automatic design of subword units for ASR, which aligns with the idea of discrete audio tokens used by language/sequence models, but the abstract does not explicitly detail a tokenization/quantization mechanism (e.g., VQ/RVQ, codebooks) or provide full methodological specifics.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses the automatic generation of subword units for LVCSR systems, focusing on probabilistic modeling of subword units and dictionaries from acoustic data and transcriptions. However, it does not mention discrete audio token architectures like vector quantization or related quantization methods to generate discrete tokens for language or sequence models, nor does it describe token generation or usage for audio language modeling, generation, or understanding. Thus, it does not meet the inclusion criteria centered on discrete audio tokens as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses the automatic generation of subword units for LVCSR systems, focusing on probabilistic modeling of subword units and dictionaries from acoustic data and transcriptions. However, it does not mention discrete audio token architectures like vector quantization or related quantization methods to generate discrete tokens for language or sequence models, nor does it describe token generation or usage for audio language modeling, generation, or understanding. Thus, it does not meet the inclusion criteria centered on discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Reasoning: The abstract describes learning subword units and a dictionary for LVCSR from acoustic data and transcriptions using a probabilistic framework — i.e., inducing linguistic subword/class labels for ASR — but it does not describe producing discrete audio tokens via vector quantization, neural audio codecs, SSL-derived semantic quantization, or any token-generation/quantization mechanism nor does it position the outputs as discrete tokens for audio language modeling or generation. Junior reviewer A noted some conceptual overlap but flagged the lack of explicit tokenization/quantization methodology; junior reviewer B judged it correctly as not meeting the discrete-audio-token criteria. Given the inclusion criteria require explicit methods for producing discrete audio tokens (VQ/RVQ/codebooks, etc.) or clear use of such tokens for sequence/language modeling, this paper does not meet them. Brief explanation (1 sentence): Exclude — the paper focuses on linguistically motivated subword unit induction for ASR (dictionaries and labels) rather than on creating discrete audio tokens via quantization or codecs for use in audio language/sequence modeling.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Reasoning: The abstract describes learning subword units and a dictionary for LVCSR from acoustic data and transcriptions using a probabilistic framework — i.e., inducing linguistic subword/class labels for ASR — but it does not describe producing discrete audio tokens via vector quantization, neural audio codecs, SSL-derived semantic quantization, or any token-generation/quantization mechanism nor does it position the outputs as discrete tokens for audio language modeling or generation. Junior reviewer A noted some conceptual overlap but flagged the lack of explicit tokenization/quantization methodology; junior reviewer B judged it correctly as not meeting the discrete-audio-token criteria. Given the inclusion criteria require explicit methods for producing discrete audio tokens (VQ/RVQ/codebooks, etc.) or clear use of such tokens for sequence/language modeling, this paper does not meet them. Brief explanation (1 sentence): Exclude — the paper focuses on linguistically motivated subword unit induction for ASR (dictionaries and labels) rather than on creating discrete audio tokens via quantization or codecs for use in audio language/sequence modeling.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Lexicon-building methods for an acoustic sub-word based speech recognizer",
    "abstract": "The use of an acoustic subword unit (ASWU)-based speech recognition system for the recognition of isolated words is discussed. Some methods are proposed for generating the deterministic and the statistical types of word lexicon. It is shown that the use of a modified k-means algorithm on the likelihoods derived through the Viterbi algorithm provides the best deterministic-type of word lexicon. However, the ASWU-based speech recognizer leads to better performance with the statistical type of word lexicon than with the deterministic type. Improving the design of the word lexicon makes it possible to narrow the gap in the recognition performances of the whole word unit (WWU)-based and the ASWU-based speech recognizers considerably. Further improvements are expected by designing the word lexicon better.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "Lexicon-building methods for an acoustic sub-word based speech recognizer",
      "summary": "The use of an acoustic subword unit (ASWU)-based speech recognition system for the recognition of isolated words is discussed. Some methods are proposed for generating the deterministic and the statistical types of word lexicon. It is shown that the use of a modified k-means algorithm on the likelihoods derived through the Viterbi algorithm provides the best deterministic-type of word lexicon. However, the ASWU-based speech recognizer leads to better performance with the statistical type of word lexicon than with the deterministic type. Improving the design of the word lexicon makes it possible to narrow the gap in the recognition performances of the whole word unit (WWU)-based and the ASWU-based speech recognizers considerably. Further improvements are expected by designing the word lexicon better.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The use of an acoustic subword unit (ASWU)-based speech recognition system for the recognition of isolated words is discussed. Some methods are proposed for generating the deterministic and the statistical types of word lexicon. It is shown that the use of a modified k-means algorithm on the likelihoods derived through the Viterbi algorithm provides the best deterministic-type of word lexicon. However, the ASWU-based speech recognizer leads to better performance with the statistical type of word lexicon than with the deterministic type. Improving the design of the word lexicon makes it possible to narrow the gap in the recognition performances of the whole word unit (WWU)-based and the ASWU-based speech recognizers considerably. Further improvements are expected by designing the word lexicon better.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1990.115888",
      "openalex_id": "https://openalex.org/W2125142492",
      "arxiv_id": "",
      "publication_date": "2002-12-04",
      "published": "2002-12-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses lexicon design for acoustic subword units in ASR and compares deterministic vs statistical lexicons; it does not present discrete audio token generation/quantization methods (like VQ/RVQ) or a reproducible token-level pipeline, hence it does not align with the discrete audio token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses lexicon design for acoustic subword units in ASR and compares deterministic vs statistical lexicons; it does not present discrete audio token generation/quantization methods (like VQ/RVQ) or a reproducible token-level pipeline, hence it does not align with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses acoustic subword units (ASWU) in speech recognition and methods for lexicon building, focusing on deterministic and statistical lexicons and their impact on recognition performance. However, it does not describe methods for generating discrete audio tokens through vector quantization or similar processes forming discrete token representations intended for use in language or sequence models. The work is centered on speech recognition lexicon design rather than discrete token generation for audio language modeling or cross-modal applications, and lacks detailed description of token formation or quantization, thus not meeting the inclusion criteria related to discrete audio token design or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses acoustic subword units (ASWU) in speech recognition and methods for lexicon building, focusing on deterministic and statistical lexicons and their impact on recognition performance. However, it does not describe methods for generating discrete audio tokens through vector quantization or similar processes forming discrete token representations intended for use in language or sequence models. The work is centered on speech recognition lexicon design rather than discrete token generation for audio language modeling or cross-modal applications, and lacks detailed description of token formation or quantization, thus not meeting the inclusion criteria related to discrete audio token design or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An improved sub-word based speech recognizer",
    "abstract": "The authors describe a system for speaker-dependent speech recognition based on acoustic subword units. Several strategies for automatic generation of an acoustic lexicon are outlined. Preliminary tests have been performed on a small vocabulary. In these tests, the proposed system showed results comparable to those of whole-word-based systems.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "An improved sub-word based speech recognizer",
      "summary": "The authors describe a system for speaker-dependent speech recognition based on acoustic subword units. Several strategies for automatic generation of an acoustic lexicon are outlined. Preliminary tests have been performed on a small vocabulary. In these tests, the proposed system showed results comparable to those of whole-word-based systems.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The authors describe a system for speaker-dependent speech recognition based on acoustic subword units. Several strategies for automatic generation of an acoustic lexicon are outlined. Preliminary tests have been performed on a small vocabulary. In these tests, the proposed system showed results comparable to those of whole-word-based systems.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1989.266375",
      "openalex_id": "https://openalex.org/W2128780426",
      "arxiv_id": "",
      "publication_date": "2003-01-13",
      "published": "2003-01-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses speaker-dependent speech recognition using acoustic subword units and lexicon generation, but does not specify discrete audio tokens via VQ/RVQ or token-based modeling; it lacks explicit tokenization/discretization methodology and reproducible experimental details required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses speaker-dependent speech recognition using acoustic subword units and lexicon generation, but does not specify discrete audio tokens via VQ/RVQ or token-based modeling; it lacks explicit tokenization/discretization methodology and reproducible experimental details required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speaker-dependent speech recognition system using acoustic subword units and describes strategies for automatic generation of an acoustic lexicon; however, it does not explicitly discuss the use of discrete audio tokens generated via vector quantization or similar techniques, nor does it clarify token generation processes or usage for sequence or language modeling. The study also lacks detailed method descriptions, token formation mechanisms, or validation in the context of audio language modeling or downstream tasks, and appears preliminary with a small vocabulary test. Therefore, it does not meet the strict inclusion criteria focused on discrete audio token design and usage, nor does it provide sufficient technical details for reproducibility or comparison as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speaker-dependent speech recognition system using acoustic subword units and describes strategies for automatic generation of an acoustic lexicon; however, it does not explicitly discuss the use of discrete audio tokens generated via vector quantization or similar techniques, nor does it clarify token generation processes or usage for sequence or language modeling. The study also lacks detailed method descriptions, token formation mechanisms, or validation in the context of audio language modeling or downstream tasks, and appears preliminary with a small vocabulary test. Therefore, it does not meet the strict inclusion criteria focused on discrete audio token design and usage, nor does it provide sufficient technical details for reproducibility or comparison as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Automatic detection of acoustic sub-word boundaries for single digit recognition",
    "abstract": "This paper investigates the use of a spectral variation function to automatically detect acoustic sub-word boundaries in single digits. The developed algorithm generates sub-words for single-digit recognition system using an RBF neural network.",
    "metadata": {
      "title": "Automatic detection of acoustic sub-word boundaries for single digit recognition",
      "summary": "This paper investigates the use of a spectral variation function to automatically detect acoustic sub-word boundaries in single digits. The developed algorithm generates sub-words for single-digit recognition system using an RBF neural network.",
      "abstract": "This paper investigates the use of a spectral variation function to automatically detect acoustic sub-word boundaries in single digits. The developed algorithm generates sub-words for single-digit recognition system using an RBF neural network.",
      "doi": "https://doi.org/10.1109/ccece.1999.808034",
      "openalex_id": "https://openalex.org/W2099878906",
      "arxiv_id": "",
      "publication_date": "2003-01-20",
      "published": "2003-01-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on detecting acoustic sub-word boundaries for single-digit recognition, not on constructing or analyzing discrete audio tokens (e.g., via quantization or token-based modeling for language/sequence tasks), so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on detecting acoustic sub-word boundaries for single-digit recognition, not on constructing or analyzing discrete audio tokens (e.g., via quantization or token-based modeling for language/sequence tasks), so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on detecting acoustic sub-word boundaries using spectral variation and employs an RBF neural network for single-digit recognition. However, it does not clearly address the generation of discrete audio tokens via vector quantization or similar methods intended for sequence or language model processing. There is no description of token generation process or the use of tokens as intermediate representations in audio language modeling or generation tasks, nor details supporting reproducibility or comparison. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it appear to violate exclusion criteria related to insufficient detail, but its key contributions do not align with the thematic requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on detecting acoustic sub-word boundaries using spectral variation and employs an RBF neural network for single-digit recognition. However, it does not clearly address the generation of discrete audio tokens via vector quantization or similar methods intended for sequence or language model processing. There is no description of token generation process or the use of tokens as intermediate representations in audio language modeling or generation tasks, nor details supporting reproducibility or comparison. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it appear to violate exclusion criteria related to insufficient detail, but its key contributions do not align with the thematic requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Combined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units",
    "abstract": "A major challenge in speech recognition is creating a lexicon which is robust to inter and intra speaker variations. This is even more so in speech recognisers based on non linguistic units, e.g., acoustic subword units (ASWUs), since no standard pronunciation dictionaries are available. Thus the baseforms describing the vocabulary words in terms of the recognition units need to be generated from training data. We propose an algorithm for ASWU based speech recognition which performs a combined optimisation of the baseforms and the subword models. The resulting system has been tested on the DARPA Resource Management task, and is shown to perform comparably to a baseline phoneme based system.",
    "metadata": {
      "title": "Combined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units",
      "summary": "A major challenge in speech recognition is creating a lexicon which is robust to inter and intra speaker variations. This is even more so in speech recognisers based on non linguistic units, e.g., acoustic subword units (ASWUs), since no standard pronunciation dictionaries are available. Thus the baseforms describing the vocabulary words in terms of the recognition units need to be generated from training data. We propose an algorithm for ASWU based speech recognition which performs a combined optimisation of the baseforms and the subword models. The resulting system has been tested on the DARPA Resource Management task, and is shown to perform comparably to a baseline phoneme based system.",
      "abstract": "A major challenge in speech recognition is creating a lexicon which is robust to inter and intra speaker variations. This is even more so in speech recognisers based on non linguistic units, e.g., acoustic subword units (ASWUs), since no standard pronunciation dictionaries are available. Thus the baseforms describing the vocabulary words in terms of the recognition units need to be generated from training data. We propose an algorithm for ASWU based speech recognition which performs a combined optimisation of the baseforms and the subword models. The resulting system has been tested on the DARPA Resource Management task, and is shown to perform comparably to a baseline phoneme based system.",
      "doi": "https://doi.org/10.1109/asru.1997.659006",
      "openalex_id": "https://openalex.org/W2158598479",
      "arxiv_id": "",
      "publication_date": "2002-11-22",
      "published": "2002-11-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses optimisation of baseforms and acoustic subword units for ASR but does not present discrete audio token design or quantization methods (no token generation/quantization/ VQ-style discretization mentioned), thus it does not meet the inclusion criteria focused on discrete audio tokens; therefore exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses optimisation of baseforms and acoustic subword units for ASR but does not present discrete audio token design or quantization methods (no token generation/quantization/ VQ-style discretization mentioned), thus it does not meet the inclusion criteria focused on discrete audio tokens; therefore exclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech recognition using acoustic subword units but does not explicitly mention the generation or use of discrete audio tokens derived via vector quantization, residual vector quantization, or semantic-driven discretization as defined in the inclusion criteria. It centers on combined optimization of baseforms and subword models for recognition, without detailing token generation processes that support sequence or language model usage. Therefore, it does not meet the core requirement of advancing discrete audio token design or usage for downstream modeling and lacks detailed token formation methods.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech recognition using acoustic subword units but does not explicitly mention the generation or use of discrete audio tokens derived via vector quantization, residual vector quantization, or semantic-driven discretization as defined in the inclusion criteria. It centers on combined optimization of baseforms and subword models for recognition, without detailing token generation processes that support sequence or language model usage. Therefore, it does not meet the core requirement of advancing discrete audio token design or usage for downstream modeling and lacks detailed token formation methods.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On the robust automatic segmentation of spontaneous speech",
    "abstract": "The results from applying an improved algorithm in the task of automatic segmentationof spontaneoustelephonequality speechare presented, and compared to the results from those resulting from superimposing white noise. Three segmentation algorithms are compared which are all based on variants of the Spectral Variation Function. Experimental results are obtained on the OGI multi-language telephonespeechcorpus (OGI TS). We show that the use of the auditory forward andbackwardmaskingeffects prior to the SVFcomputation increases the robustness of the algorithm to white noise. When the average signal-to-noise ratio (SNR) is decreased to 10dB the peak ratio (defined as the ratio of the number of peaks measured at the target over the original SNRs) is increased by 16%, 12%, and 11% for the MFC (Mel-Frequency Cepstra), RASTA (RelAtive SpecTrAl processing), and the FBDYN (Forward-Backward auditory masking DYNamic cepstra) SVF segmentation algorithms, respectively.",
    "metadata": {
      "title": "On the robust automatic segmentation of spontaneous speech",
      "summary": "The results from applying an improved algorithm in the task of automatic segmentationof spontaneoustelephonequality speechare presented, and compared to the results from those resulting from superimposing white noise. Three segmentation algorithms are compared which are all based on variants of the Spectral Variation Function. Experimental results are obtained on the OGI multi-language telephonespeechcorpus (OGI TS). We show that the use of the auditory forward andbackwardmaskingeffects prior to the SVFcomputation increases the robustness of the algorithm to white noise. When the average signal-to-noise ratio (SNR) is decreased to 10dB the peak ratio (defined as the ratio of the number of peaks measured at the target over the original SNRs) is increased by 16%, 12%, and 11% for the MFC (Mel-Frequency Cepstra), RASTA (RelAtive SpecTrAl processing), and the FBDYN (Forward-Backward auditory masking DYNamic cepstra) SVF segmentation algorithms, respectively.",
      "abstract": "The results from applying an improved algorithm in the task of automatic segmentationof spontaneoustelephonequality speechare presented, and compared to the results from those resulting from superimposing white noise. Three segmentation algorithms are compared which are all based on variants of the Spectral Variation Function. Experimental results are obtained on the OGI multi-language telephonespeechcorpus (OGI TS). We show that the use of the auditory forward andbackwardmaskingeffects prior to the SVFcomputation increases the robustness of the algorithm to white noise. When the average signal-to-noise ratio (SNR) is decreased to 10dB the peak ratio (defined as the ratio of the number of peaks measured at the target over the original SNRs) is increased by 16%, 12%, and 11% for the MFC (Mel-Frequency Cepstra), RASTA (RelAtive SpecTrAl processing), and the FBDYN (Forward-Backward auditory masking DYNamic cepstra) SVF segmentation algorithms, respectively.",
      "doi": "https://doi.org/10.1109/icslp.1996.607750",
      "openalex_id": "https://openalex.org/W2107105119",
      "arxiv_id": "",
      "publication_date": "2002-12-24",
      "published": "2002-12-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該文題為自動分割語音的魯棒性與算法比較研究，聚焦於基於SVF的分割與噪聲魯棒性，未涉及離散音訊 token 的設計、量化流程或可直接用於語言模型/序列模型的離散表示，因此不符合包含條件，屬於排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該文題為自動分割語音的魯棒性與算法比較研究，聚焦於基於SVF的分割與噪聲魯棒性，未涉及離散音訊 token 的設計、量化流程或可直接用於語言模型/序列模型的離散表示，因此不符合包含條件，屬於排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on robust automatic segmentation of spontaneous speech using variants of the Spectral Variation Function with auditory masking effects to improve noise robustness. However, it does not describe any discrete audio token design, vector quantization, or token generation processes for use in sequence or language modeling, which are essential to meet the inclusion criteria focused on discrete audio token generation methods. Thus, it does not meet the core research contribution requirements related to discrete audio tokens and does not describe token formation mechanisms or downstream usage as language tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on robust automatic segmentation of spontaneous speech using variants of the Spectral Variation Function with auditory masking effects to improve noise robustness. However, it does not describe any discrete audio token design, vector quantization, or token generation processes for use in sequence or language modeling, which are essential to meet the inclusion criteria focused on discrete audio token generation methods. Thus, it does not meet the core research contribution requirements related to discrete audio tokens and does not describe token formation mechanisms or downstream usage as language tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transform representation of the spectra of acoustic speech segments with applications. I. General approach and application to speech recognition",
    "abstract": "An approach to modeling and capturing the time-varying structure of the spectral envelope of speech is reported. Acoustic subword decomposition and the Karhunen-Loeve transform (KLT) are used to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the KLT with acoustic subword modeling provides concise representation of both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition are presented. The performance of the recognition algorithm based on this approach compares favorably with that of other techniques.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "Transform representation of the spectra of acoustic speech segments with applications. I. General approach and application to speech recognition",
      "summary": "An approach to modeling and capturing the time-varying structure of the spectral envelope of speech is reported. Acoustic subword decomposition and the Karhunen-Loeve transform (KLT) are used to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the KLT with acoustic subword modeling provides concise representation of both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition are presented. The performance of the recognition algorithm based on this approach compares favorably with that of other techniques.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "An approach to modeling and capturing the time-varying structure of the spectral envelope of speech is reported. Acoustic subword decomposition and the Karhunen-Loeve transform (KLT) are used to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the KLT with acoustic subword modeling provides concise representation of both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition are presented. The performance of the recognition algorithm based on this approach compares favorably with that of other techniques.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/89.222877",
      "openalex_id": "https://openalex.org/W2033431959",
      "arxiv_id": "",
      "publication_date": "1993-04-01",
      "published": "1993-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此文著重於語音頻譜包絡的時變結構建模與KLT在語音辨識中的應用，未涉及離散音訊 token 的生成、量化或與序列模型的直接關聯，因此不符合離散音訊 token 的主題與研究貢獻要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此文著重於語音頻譜包絡的時變結構建模與KLT在語音辨識中的應用，未涉及離散音訊 token 的生成、量化或與序列模型的直接關聯，因此不符合離散音訊 token 的主題與研究貢獻要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on representing speech spectra using the Karhunen-Loeve transform and acoustic subword modeling aimed at speech recognition, but does not describe generating discrete audio tokens via vector quantization or similar methods, nor does it discuss producing token representations for sequence or language modeling; therefore, it does not meet the core inclusion criterion related to discrete token design and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on representing speech spectra using the Karhunen-Loeve transform and acoustic subword modeling aimed at speech recognition, but does not describe generating discrete audio tokens via vector quantization or similar methods, nor does it discuss producing token representations for sequence or language modeling; therefore, it does not meet the core inclusion criterion related to discrete token design and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Pronunciation modeling for speech technology",
    "abstract": "Written text is based on an orthographic representation of words, i.e. linear sequences of letters. Modern speech technology (automatic speech recognition and text-to-speech synthesis) is based on phonetic units representing realization of sounds. A mapping between the orthographic form and phonetic forms representing the pronunciation is thus required. This may be obtained by creating pronunciation lexica and/or rule-based systems for grapheme-to-phoneme conversion. Traditionally, this mapping has been obtained manually, based on phonetic and linguistic knowledge. This approach has a number of drawbacks: i) the pronunciations represent typical pronunciations and will have a limited capacity for describing pronunciation variation due to speaking style and dialectical/accent variations; ii) if multiple pronunciation variants are included, it does not indicate which variants are more significant for the specific application; iii) the description is based on phonetic-knowledge and does not take into account that the units used in speech technology may deviate from the phonetic interpretation; and iv) the description is limited to units with a linguistic interpretation. The paper will present and discuss methods for modeling pronunciation and pronunciation variation specifically for applications in speech technology.",
    "metadata": {
      "title": "Pronunciation modeling for speech technology",
      "summary": "Written text is based on an orthographic representation of words, i.e. linear sequences of letters. Modern speech technology (automatic speech recognition and text-to-speech synthesis) is based on phonetic units representing realization of sounds. A mapping between the orthographic form and phonetic forms representing the pronunciation is thus required. This may be obtained by creating pronunciation lexica and/or rule-based systems for grapheme-to-phoneme conversion. Traditionally, this mapping has been obtained manually, based on phonetic and linguistic knowledge. This approach has a number of drawbacks: i) the pronunciations represent typical pronunciations and will have a limited capacity for describing pronunciation variation due to speaking style and dialectical/accent variations; ii) if multiple pronunciation variants are included, it does not indicate which variants are more significant for the specific application; iii) the description is based on phonetic-knowledge and does not take into account that the units used in speech technology may deviate from the phonetic interpretation; and iv) the description is limited to units with a linguistic interpretation. The paper will present and discuss methods for modeling pronunciation and pronunciation variation specifically for applications in speech technology.",
      "abstract": "Written text is based on an orthographic representation of words, i.e. linear sequences of letters. Modern speech technology (automatic speech recognition and text-to-speech synthesis) is based on phonetic units representing realization of sounds. A mapping between the orthographic form and phonetic forms representing the pronunciation is thus required. This may be obtained by creating pronunciation lexica and/or rule-based systems for grapheme-to-phoneme conversion. Traditionally, this mapping has been obtained manually, based on phonetic and linguistic knowledge. This approach has a number of drawbacks: i) the pronunciations represent typical pronunciations and will have a limited capacity for describing pronunciation variation due to speaking style and dialectical/accent variations; ii) if multiple pronunciation variants are included, it does not indicate which variants are more significant for the specific application; iii) the description is based on phonetic-knowledge and does not take into account that the units used in speech technology may deviate from the phonetic interpretation; and iv) the description is limited to units with a linguistic interpretation. The paper will present and discuss methods for modeling pronunciation and pronunciation variation specifically for applications in speech technology.",
      "doi": "https://doi.org/10.1109/spcom.2004.1458347",
      "openalex_id": "https://openalex.org/W2107988889",
      "arxiv_id": "",
      "publication_date": "2005-06-28",
      "published": "2005-06-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss pronunciation modeling and grapheme-to-phoneme mappings for speech technology, focusing on traditional phonetic/linguistic representations rather than discrete audio tokens (e.g., VQ/RVQ-based tokenization, semantic disentanglement) and do not describe a discrete-token design or experiments for audio language modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss pronunciation modeling and grapheme-to-phoneme mappings for speech technology, focusing on traditional phonetic/linguistic representations rather than discrete audio tokens (e.g., VQ/RVQ-based tokenization, semantic disentanglement) and do not describe a discrete-token design or experiments for audio language modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses pronunciation modeling for speech technology, focusing on mapping orthographic to phonetic forms and handling pronunciation variations. However, it does not present any methods involving discrete audio tokenization through vector quantization or similar techniques, nor does it describe any token generation processes or usage for sequence modeling as required by the inclusion criteria. Therefore, it does not meet the core requirement of studying discrete audio tokens as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses pronunciation modeling for speech technology, focusing on mapping orthographic to phonetic forms and handling pronunciation variations. However, it does not present any methods involving discrete audio tokenization through vector quantization or similar techniques, nor does it describe any token generation processes or usage for sequence modeling as required by the inclusion criteria. Therefore, it does not meet the core requirement of studying discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voice activity detection based on deep neural networks and Viterbi",
    "abstract": "Voice Activity Detection (VAD) is important in speech processing. In the applications, the systems usually need to separate speech/non-speech parts, so that only the speech part can be dealt with. How to improve the performances of VAD in different noisy environments is an important issue in speech processing. Deep Neural network, which proves its efficiency in speech recognition, has been widely used in recent years. This paper studies the present typical VAD algorithms, and presents a new VAD algorithm based on deep neural networks and Viterbi algorithm. The result demonstrates the effectiveness of the deep neural network with Viterbi used in VAD. In addition, it shows the flexibility and the real-time performance of the algorithms.",
    "metadata": {
      "title": "Voice activity detection based on deep neural networks and Viterbi",
      "summary": "Voice Activity Detection (VAD) is important in speech processing. In the applications, the systems usually need to separate speech/non-speech parts, so that only the speech part can be dealt with. How to improve the performances of VAD in different noisy environments is an important issue in speech processing. Deep Neural network, which proves its efficiency in speech recognition, has been widely used in recent years. This paper studies the present typical VAD algorithms, and presents a new VAD algorithm based on deep neural networks and Viterbi algorithm. The result demonstrates the effectiveness of the deep neural network with Viterbi used in VAD. In addition, it shows the flexibility and the real-time performance of the algorithms.",
      "abstract": "Voice Activity Detection (VAD) is important in speech processing. In the applications, the systems usually need to separate speech/non-speech parts, so that only the speech part can be dealt with. How to improve the performances of VAD in different noisy environments is an important issue in speech processing. Deep Neural network, which proves its efficiency in speech recognition, has been widely used in recent years. This paper studies the present typical VAD algorithms, and presents a new VAD algorithm based on deep neural networks and Viterbi algorithm. The result demonstrates the effectiveness of the deep neural network with Viterbi used in VAD. In addition, it shows the flexibility and the real-time performance of the algorithms.",
      "doi": "https://doi.org/10.1088/1757-899x/231/1/012042",
      "openalex_id": "https://openalex.org/W2755708162",
      "arxiv_id": "",
      "publication_date": "2017-09-01",
      "published": "2017-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Voice Activity Detection using DNN and Viterbi, which is not about discrete audio tokens or tokenization designed for downstream language/sequence models; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Voice Activity Detection using DNN and Viterbi, which is not about discrete audio tokens or tokenization designed for downstream language/sequence models; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice activity detection using deep neural networks and Viterbi algorithm, primarily targeting speech/non-speech segmentation in noisy environments, but does not describe any discrete audio token generation methods such as vector quantization or semantic-driven discretization, nor does it discuss token production for sequence or language modeling applications. Therefore, it does not meet the inclusion criteria related to discrete audio token design and usage, and lacks details on token formation mechanisms required for the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice activity detection using deep neural networks and Viterbi algorithm, primarily targeting speech/non-speech segmentation in noisy environments, but does not describe any discrete audio token generation methods such as vector quantization or semantic-driven discretization, nor does it discuss token production for sequence or language modeling applications. Therefore, it does not meet the inclusion criteria related to discrete audio token design and usage, and lacks details on token formation mechanisms required for the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Labelling of Hindi Speech",
    "abstract": "The goal of this paper is to obtain segmented and labelled speech at syllable level and also that the reasonable number of syllables may suffice the need for travel domain applications. A base-line group delay-based segmentation technique is applied on spoken speech sentences to generate labelled database at syllable level. The system is validated against 50 manually segmented speech utterances. The segmentation accuracy was evaluated by performing time-error analysis. It is observed that 63.07% syllables have time-error less than 30 ms. It is observed that vowels are more accurately segmented as compared to fricatives. The confidence interval is found to be 0.1147 ms for confidence level of 95%. This paper also presents implementation of algorithm for identifying syllables based on linguistic rules for Hindi words. After survey of the relevant literature, a set of rules are identified and implemented as a simple easy-to-implement algorithm. The text segmentation algorithm is tested on 2400 distinct words and algorithm performs with 99.5% accuracy for segmentation of written text.",
    "metadata": {
      "title": "Labelling of Hindi Speech",
      "summary": "The goal of this paper is to obtain segmented and labelled speech at syllable level and also that the reasonable number of syllables may suffice the need for travel domain applications. A base-line group delay-based segmentation technique is applied on spoken speech sentences to generate labelled database at syllable level. The system is validated against 50 manually segmented speech utterances. The segmentation accuracy was evaluated by performing time-error analysis. It is observed that 63.07% syllables have time-error less than 30 ms. It is observed that vowels are more accurately segmented as compared to fricatives. The confidence interval is found to be 0.1147 ms for confidence level of 95%. This paper also presents implementation of algorithm for identifying syllables based on linguistic rules for Hindi words. After survey of the relevant literature, a set of rules are identified and implemented as a simple easy-to-implement algorithm. The text segmentation algorithm is tested on 2400 distinct words and algorithm performs with 99.5% accuracy for segmentation of written text.",
      "abstract": "The goal of this paper is to obtain segmented and labelled speech at syllable level and also that the reasonable number of syllables may suffice the need for travel domain applications. A base-line group delay-based segmentation technique is applied on spoken speech sentences to generate labelled database at syllable level. The system is validated against 50 manually segmented speech utterances. The segmentation accuracy was evaluated by performing time-error analysis. It is observed that 63.07% syllables have time-error less than 30 ms. It is observed that vowels are more accurately segmented as compared to fricatives. The confidence interval is found to be 0.1147 ms for confidence level of 95%. This paper also presents implementation of algorithm for identifying syllables based on linguistic rules for Hindi words. After survey of the relevant literature, a set of rules are identified and implemented as a simple easy-to-implement algorithm. The text segmentation algorithm is tested on 2400 distinct words and algorithm performs with 99.5% accuracy for segmentation of written text.",
      "doi": "https://doi.org/10.1080/03772063.2015.1075914",
      "openalex_id": "https://openalex.org/W2324819908",
      "arxiv_id": "",
      "publication_date": "2015-09-07",
      "published": "2015-09-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concentrates on syllable-level labeling and rule-based Hindi speech segmentation rather than producing discrete audio tokens for language/sequence modeling; it also lacks a publication date, triggering exclusion per the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concentrates on syllable-level labeling and rule-based Hindi speech segmentation rather than producing discrete audio tokens for language/sequence modeling; it also lacks a publication date, triggering exclusion per the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on segmenting and labelling Hindi speech at the syllable level using signal processing methods, without discussing any discrete audio token generation, vector quantization, or use of tokens for sequence modeling or downstream tasks; thus, it does not meet the inclusion criteria related to discrete audio tokens as core research contributions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on segmenting and labelling Hindi speech at the syllable level using signal processing methods, without discussing any discrete audio token generation, vector quantization, or use of tokens for sequence modeling or downstream tasks; thus, it does not meet the inclusion criteria related to discrete audio tokens as core research contributions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech analysis and segmentation by parametric filtering",
    "abstract": "A new set of digital signal processing techniques for detecting changes in a speech signal is considered. The overall approach is called parametric filtering, and it yields several promising new diagnostics for speech analysis and segmentation including in particular, the demodulated lag-one autocorrelation /spl gamma//sub /spl theta//(/spl eta/), the time-correlation analysis plot, and the /spl gamma//sub /spl theta//(/spl eta/)-based distortion measures. Initial experiments described in this paper establish the potential significance of the parametric filtering method and these new diagnostics for speech analysis and segmentation.",
    "metadata": {
      "title": "Speech analysis and segmentation by parametric filtering",
      "summary": "A new set of digital signal processing techniques for detecting changes in a speech signal is considered. The overall approach is called parametric filtering, and it yields several promising new diagnostics for speech analysis and segmentation including in particular, the demodulated lag-one autocorrelation /spl gamma//sub /spl theta//(/spl eta/), the time-correlation analysis plot, and the /spl gamma//sub /spl theta//(/spl eta/)-based distortion measures. Initial experiments described in this paper establish the potential significance of the parametric filtering method and these new diagnostics for speech analysis and segmentation.",
      "abstract": "A new set of digital signal processing techniques for detecting changes in a speech signal is considered. The overall approach is called parametric filtering, and it yields several promising new diagnostics for speech analysis and segmentation including in particular, the demodulated lag-one autocorrelation /spl gamma//sub /spl theta//(/spl eta/), the time-correlation analysis plot, and the /spl gamma//sub /spl theta//(/spl eta/)-based distortion measures. Initial experiments described in this paper establish the potential significance of the parametric filtering method and these new diagnostics for speech analysis and segmentation.",
      "doi": "https://doi.org/10.1109/89.496216",
      "openalex_id": "https://openalex.org/W2099714852",
      "arxiv_id": "",
      "publication_date": "1996-05-01",
      "published": "1996-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on parametric filtering for speech analysis and segmentation, not on discrete audio tokens or token generation/discretization methods required by the inclusion criteria (e.g., VQ/RVQ, layered quantization, semantic/discrete token representations for language models). Therefore it fails to meet the inclusion criteria and does not satisfy the exclusion reasons as it simply describes DSP diagnostics rather than discrete token design/processes.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on parametric filtering for speech analysis and segmentation, not on discrete audio tokens or token generation/discretization methods required by the inclusion criteria (e.g., VQ/RVQ, layered quantization, semantic/discrete token representations for language models). Therefore it fails to meet the inclusion criteria and does not satisfy the exclusion reasons as it simply describes DSP diagnostics rather than discrete token design/processes.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses speech analysis and segmentation using parametric filtering techniques without mentioning discrete audio token generation, vector quantization, or related tokenization mechanisms. It lacks details about producing discrete tokens for sequence or language modeling and does not address token generation processes or use in audio language models, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses speech analysis and segmentation using parametric filtering techniques without mentioning discrete audio token generation, vector quantization, or related tokenization mechanisms. It lacks details about producing discrete tokens for sequence or language modeling and does not address token generation processes or use in audio language models, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A HMM-based approach for segmenting continuous speech",
    "abstract": "Several algorithms used for automatically segmenting an input speech signal are reviewed. It is shown that they either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. Another approach to automatically segmentating continuous speech is presented. To verify this approach, experimental results from a database of 30 speakers whose speech has been recorded over the public switched telephone network are presented. The results benchmark the algorithm against a state-of-the-art approach and show a 4* reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "A HMM-based approach for segmenting continuous speech",
      "summary": "Several algorithms used for automatically segmenting an input speech signal are reviewed. It is shown that they either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. Another approach to automatically segmentating continuous speech is presented. To verify this approach, experimental results from a database of 30 speakers whose speech has been recorded over the public switched telephone network are presented. The results benchmark the algorithm against a state-of-the-art approach and show a 4* reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Several algorithms used for automatically segmenting an input speech signal are reviewed. It is shown that they either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. Another approach to automatically segmentating continuous speech is presented. To verify this approach, experimental results from a database of 30 speakers whose speech has been recorded over the public switched telephone network are presented. The results benchmark the algorithm against a state-of-the-art approach and show a 4* reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/acssc.1992.269127",
      "openalex_id": "https://openalex.org/W2162595709",
      "arxiv_id": "",
      "publication_date": "2003-01-02",
      "published": "2003-01-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not meeting inclusion criteria: the abstract focuses on HMM-based segmentation of continuous speech and evaluation of segmentation performance, with no discussion of discrete audio tokens or tokenization methods for downstream language/sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not meeting inclusion criteria: the abstract focuses on HMM-based segmentation of continuous speech and evaluation of segmentation performance, with no discussion of discrete audio tokens or tokenization methods for downstream language/sequence modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an HMM-based approach for segmenting continuous speech and benchmarking segmentation algorithms, but it does not describe any discrete audio token generation process, vector quantization, or semantic-driven discrete representation relevant to discrete audio tokens as defined in the inclusion criteria. It lacks details on token formation, quantization methods, or usage of tokens in sequence or language modeling, and thus does not meet the inclusion criteria nor does it discuss discrete audio tokens as the research core.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an HMM-based approach for segmenting continuous speech and benchmarking segmentation algorithms, but it does not describe any discrete audio token generation process, vector quantization, or semantic-driven discrete representation relevant to discrete audio tokens as defined in the inclusion criteria. It lacks details on token formation, quantization methods, or usage of tokens in sequence or language modeling, and thus does not meet the inclusion criteria nor does it discuss discrete audio tokens as the research core.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A new method for segmenting continuous speech",
    "abstract": "Speech recognition systems are increasingly utilized in various applications like telephone services where a user places a call by uttering the digits or the name of the person. One of the main problems in this application is the segmentation of the input utterance into speech and nonspeech portions. Current approaches typically suffer from two problems. They either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. The authors present another approach to automatically segment continuous speech and create speaker dependent models. To verify the hypothesis, they use a database of 30 speakers whose speech has been recorded over the public switched telephone network. With this database, they benchmark their algorithm against a state of the art approach and show a 4/spl times/ reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "A new method for segmenting continuous speech",
      "summary": "Speech recognition systems are increasingly utilized in various applications like telephone services where a user places a call by uttering the digits or the name of the person. One of the main problems in this application is the segmentation of the input utterance into speech and nonspeech portions. Current approaches typically suffer from two problems. They either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. The authors present another approach to automatically segment continuous speech and create speaker dependent models. To verify the hypothesis, they use a database of 30 speakers whose speech has been recorded over the public switched telephone network. With this database, they benchmark their algorithm against a state of the art approach and show a 4/spl times/ reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Speech recognition systems are increasingly utilized in various applications like telephone services where a user places a call by uttering the digits or the name of the person. One of the main problems in this application is the segmentation of the input utterance into speech and nonspeech portions. Current approaches typically suffer from two problems. They either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. The authors present another approach to automatically segment continuous speech and create speaker dependent models. To verify the hypothesis, they use a database of 30 speakers whose speech has been recorded over the public switched telephone network. With this database, they benchmark their algorithm against a state of the art approach and show a 4/spl times/ reduction in the error rate of the recognition system.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1994.389357",
      "openalex_id": "https://openalex.org/W1947057191",
      "arxiv_id": "",
      "publication_date": "2002-12-17",
      "published": "2002-12-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses segmenting continuous speech and speaker-dependent models, with no mention of discrete audio tokens or token-based representations, so it does not meet the discrete-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses segmenting continuous speech and speaker-dependent models, with no mention of discrete audio tokens or token-based representations, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on segmentation of continuous speech for speech recognition improvement without mentioning any discrete audio token generation, vector quantization, or related mechanisms for producing tokens that serve as input for language or sequence models. They do not describe processes or contributions related to discrete audio token design or use for downstream tasks as required by the inclusion criteria. Additionally, no method details about token generation, quantization, or acoustic/semantic disentanglement are presented. Therefore, the study does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on segmentation of continuous speech for speech recognition improvement without mentioning any discrete audio token generation, vector quantization, or related mechanisms for producing tokens that serve as input for language or sequence models. They do not describe processes or contributions related to discrete audio token design or use for downstream tasks as required by the inclusion criteria. Additionally, no method details about token generation, quantization, or acoustic/semantic disentanglement are presented. Therefore, the study does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speaker recognition based on SOINN and incremental learning Gaussian mixture model",
    "abstract": "Gaussian Mixture Models has been widely used in speaker recognition during the last decades. To deal with the dynamic growth of datasets, initial clustering problem and achieving the results of clustering effectively on incremental data, an incremental adaptation method called incremental learning Gaussian mixture model (IGMM) is proposed in this paper. It was applied to speaker recognition system based on Self Organization Incremental Learning Neural Network (SOINN) and improved EM algorithm. SOINN is a Neural Network which can reach a suitable mixture number and appropriate initial cluster for each model. First, the initial training is conducted by SOINN and EM algorithm only need a limited amount of data. Then, the model would adapt to the data available in each session to enrich itself incrementally and recursively. Experiments were taken on the 1st speech separation challenge database. The results show that IGMM outperforms GMM and classical Bayesian adaptation in most of the cases.",
    "metadata": {
      "title": "Speaker recognition based on SOINN and incremental learning Gaussian mixture model",
      "summary": "Gaussian Mixture Models has been widely used in speaker recognition during the last decades. To deal with the dynamic growth of datasets, initial clustering problem and achieving the results of clustering effectively on incremental data, an incremental adaptation method called incremental learning Gaussian mixture model (IGMM) is proposed in this paper. It was applied to speaker recognition system based on Self Organization Incremental Learning Neural Network (SOINN) and improved EM algorithm. SOINN is a Neural Network which can reach a suitable mixture number and appropriate initial cluster for each model. First, the initial training is conducted by SOINN and EM algorithm only need a limited amount of data. Then, the model would adapt to the data available in each session to enrich itself incrementally and recursively. Experiments were taken on the 1st speech separation challenge database. The results show that IGMM outperforms GMM and classical Bayesian adaptation in most of the cases.",
      "abstract": "Gaussian Mixture Models has been widely used in speaker recognition during the last decades. To deal with the dynamic growth of datasets, initial clustering problem and achieving the results of clustering effectively on incremental data, an incremental adaptation method called incremental learning Gaussian mixture model (IGMM) is proposed in this paper. It was applied to speaker recognition system based on Self Organization Incremental Learning Neural Network (SOINN) and improved EM algorithm. SOINN is a Neural Network which can reach a suitable mixture number and appropriate initial cluster for each model. First, the initial training is conducted by SOINN and EM algorithm only need a limited amount of data. Then, the model would adapt to the data available in each session to enrich itself incrementally and recursively. Experiments were taken on the 1st speech separation challenge database. The results show that IGMM outperforms GMM and classical Bayesian adaptation in most of the cases.",
      "doi": "https://doi.org/10.1109/ijcnn.2013.6706863",
      "openalex_id": "https://openalex.org/W2004740791",
      "arxiv_id": "",
      "publication_date": "2013-08-01",
      "published": "2013-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on incremental learning Gaussian mixture models with SOINN for speaker recognition and lacks any discrete audio token representation or tokenization method, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on incremental learning Gaussian mixture models with SOINN for speaker recognition and lacks any discrete audio token representation or tokenization method, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker recognition using an incremental learning Gaussian mixture model combined with a neural network, without describing any discrete audio token generation process, token quantization, or usage of tokens for sequence or language modeling; thus, it does not meet the core inclusion criteria regarding discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker recognition using an incremental learning Gaussian mixture model combined with a neural network, without describing any discrete audio token generation process, token quantization, or usage of tokens for sequence or language modeling; thus, it does not meet the core inclusion criteria regarding discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Characterization of spectral transitions with applications to acoustic sub-word segmentation and automatic speech recognition",
    "abstract": "A mathematical model has been developed for tracking spectral transitions within the spectral envelope of a speech signal. This technique incorporates linguistic knowledge into a mathematical framework to determine time-varying acoustic-phonetic features and describe formant transitions. The proposed model is quite robust and is capable of extracting not only rapid spectral movement, but also smoother spectral transitions that occur in vowel and sonorant sequences. This basic approach has been previously used to extract steady-state acoustic-phonetic features across spectrally homogeneous regions and to perform speaker dependent recognition in which quite successful results were attained in clean as well as noisy speech. It has now been augmented to capture the dynamics of spectral acoustic-phonetic features.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "Characterization of spectral transitions with applications to acoustic sub-word segmentation and automatic speech recognition",
      "summary": "A mathematical model has been developed for tracking spectral transitions within the spectral envelope of a speech signal. This technique incorporates linguistic knowledge into a mathematical framework to determine time-varying acoustic-phonetic features and describe formant transitions. The proposed model is quite robust and is capable of extracting not only rapid spectral movement, but also smoother spectral transitions that occur in vowel and sonorant sequences. This basic approach has been previously used to extract steady-state acoustic-phonetic features across spectrally homogeneous regions and to perform speaker dependent recognition in which quite successful results were attained in clean as well as noisy speech. It has now been augmented to capture the dynamics of spectral acoustic-phonetic features.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "A mathematical model has been developed for tracking spectral transitions within the spectral envelope of a speech signal. This technique incorporates linguistic knowledge into a mathematical framework to determine time-varying acoustic-phonetic features and describe formant transitions. The proposed model is quite robust and is capable of extracting not only rapid spectral movement, but also smoother spectral transitions that occur in vowel and sonorant sequences. This basic approach has been previously used to extract steady-state acoustic-phonetic features across spectrally homogeneous regions and to perform speaker dependent recognition in which quite successful results were attained in clean as well as noisy speech. It has now been augmented to capture the dynamics of spectral acoustic-phonetic features.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1989.266374",
      "openalex_id": "https://openalex.org/W1540061949",
      "arxiv_id": "",
      "publication_date": "2003-01-13",
      "published": "2003-01-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article develops a mathematical model for tracking spectral transitions to describe time-varying acoustic-phonetic features, focusing on formant transitions and spectral dynamics, and does not discuss discrete audio tokens (VQ/RVQ, hierarchical quantization, SSL-derived tokenization) or token-based representations for language/sequence modeling, hence it does not meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article develops a mathematical model for tracking spectral transitions to describe time-varying acoustic-phonetic features, focusing on formant transitions and spectral dynamics, and does not discuss discrete audio tokens (VQ/RVQ, hierarchical quantization, SSL-derived tokenization) or token-based representations for language/sequence modeling, hence it does not meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on tracking spectral transitions and acoustic-phonetic features in speech signals for sub-word segmentation and automatic speech recognition, but it does not describe any discrete audio token generation methods (e.g., vector quantization, quantization, or token usage in language or sequence models), nor does it address token design or usage for sequence modeling; therefore, it does not meet the inclusion criteria regarding discrete token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on tracking spectral transitions and acoustic-phonetic features in speech signals for sub-word segmentation and automatic speech recognition, but it does not describe any discrete audio token generation methods (e.g., vector quantization, quantization, or token usage in language or sequence models), nor does it address token design or usage for sequence modeling; therefore, it does not meet the inclusion criteria regarding discrete token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The ICSI 2007 Language Recognition System",
    "abstract": "In this paper, we describe the ICSI 2007 language recognition system. The system constitutes a variant of the classic PPRLM (parallel phone recognizer followed by language modeling) approach. We used a combination of frame-by-frame multilayer perceptron (MLP) phone classifiers for English, Arabic, and Mandarin and one open loop hidden Markov Model (HMM) phone recognizer (trained on English data). The maximum likelihood language modeling is substituted by support-vectormachines (SVMs) as a more powerful, discriminative classifi cation method. Rank normalization is used as a normalization method superior to mean-variance normalization. Results are presented on the NIST 2005 language recognition evaluation (LRE05) set and a test set taken from the LRE07 training corpus. The average NIST cost of the system on the LRE05 set is 0.0886.",
    "metadata": {
      "title": "The ICSI 2007 Language Recognition System",
      "summary": "In this paper, we describe the ICSI 2007 language recognition system. The system constitutes a variant of the classic PPRLM (parallel phone recognizer followed by language modeling) approach. We used a combination of frame-by-frame multilayer perceptron (MLP) phone classifiers for English, Arabic, and Mandarin and one open loop hidden Markov Model (HMM) phone recognizer (trained on English data). The maximum likelihood language modeling is substituted by support-vectormachines (SVMs) as a more powerful, discriminative classifi cation method. Rank normalization is used as a normalization method superior to mean-variance normalization. Results are presented on the NIST 2005 language recognition evaluation (LRE05) set and a test set taken from the LRE07 training corpus. The average NIST cost of the system on the LRE05 set is 0.0886.",
      "abstract": "In this paper, we describe the ICSI 2007 language recognition system. The system constitutes a variant of the classic PPRLM (parallel phone recognizer followed by language modeling) approach. We used a combination of frame-by-frame multilayer perceptron (MLP) phone classifiers for English, Arabic, and Mandarin and one open loop hidden Markov Model (HMM) phone recognizer (trained on English data). The maximum likelihood language modeling is substituted by support-vectormachines (SVMs) as a more powerful, discriminative classifi cation method. Rank normalization is used as a normalization method superior to mean-variance normalization. Results are presented on the NIST 2005 language recognition evaluation (LRE05) set and a test set taken from the LRE07 training corpus. The average NIST cost of the system on the LRE05 set is 0.0886.",
      "doi": "",
      "openalex_id": "https://openalex.org/W165254982",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract describes a language recognition system (PPRLM with MLP/HMM/SVM, NIST data) and provides no discrete audio token design or tokenization method, so it fails to meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a language recognition system (PPRLM with MLP/HMM/SVM, NIST data) and provides no discrete audio token design or tokenization method, so it fails to meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a language recognition system using frame-by-frame multilayer perceptron phone classifiers and SVM classifiers; it does not describe the generation of discrete audio tokens via vector quantization or related methods, nor does it focus on token design or usage in sequence/language models for audio understanding or generation as defined in the inclusion criteria. Thus, it neither meets the inclusion criteria nor violates the exclusion criteria explicitly but lacks the core components to be included.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a language recognition system using frame-by-frame multilayer perceptron phone classifiers and SVM classifiers; it does not describe the generation of discrete audio tokens via vector quantization or related methods, nor does it focus on token design or usage in sequence/language models for audio understanding or generation as defined in the inclusion criteria. Thus, it neither meets the inclusion criteria nor violates the exclusion criteria explicitly but lacks the core components to be included.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A digital neural network approach to speech recognition",
    "abstract": "recognition based on sub-word component&amp;quot;. A digital neural network is the fundamental processing strategy in beth methods. The first design is based on the &amp;apos;Separate Segmentation &amp;amp; Labelling &amp;apos; (SS&amp;amp;L) approach. The spectral data of the input utterance is first segmented into phoneme-like units which are then time normalised by linear time normalisation. The neural network labels the time-normalised phoneme-like segments./8.36 % recognition accuracy is achieved for the phoneme-like unit. In the second design, no time no-malisation is required. After segmentation, recognition is performed by classifying the data in a window as it is slid one frame at a time, from the start to the end of of each phoneme-like segment in the utterance. 73.97 % recognition accuracy for the phoneme-like unit is achieved in this application. The parameters of the neural net have been optimised for maximum recognition performance. A segmentation strategy using the sum of the difference in filterbank channel energy over successive spectra produced 80.27 % correct segmentation of isolated utterances into phoneme-like units. A linguistic processor based on that of Kashyap &amp;amp; Mittal [84] enables 93.11 % and 93.49 % word recognition accuracy to be achieved for the SS&amp;amp;L and &amp;apos;Sliding Window &amp;apos; recognisers respectively. The linguistic processor has been redesigned to make it portable so that it can be easily applied to any phoneme based isolated word speech recogruser, To my Parents, brothers and sisters, Aunt Afifa, Unc1e Ghazanfer and Cousin Fahad. ACKNOWLEDGEMENT I would like to express heartfelt thanks to my Supervisor, Dr. T.J. Stonham for his guidance, help, and encouragement throughout this",
    "metadata": {
      "title": "A digital neural network approach to speech recognition",
      "summary": "recognition based on sub-word component&amp;quot;. A digital neural network is the fundamental processing strategy in beth methods. The first design is based on the &amp;apos;Separate Segmentation &amp;amp; Labelling &amp;apos; (SS&amp;amp;L) approach. The spectral data of the input utterance is first segmented into phoneme-like units which are then time normalised by linear time normalisation. The neural network labels the time-normalised phoneme-like segments./8.36 % recognition accuracy is achieved for the phoneme-like unit. In the second design, no time no-malisation is required. After segmentation, recognition is performed by classifying the data in a window as it is slid one frame at a time, from the start to the end of of each phoneme-like segment in the utterance. 73.97 % recognition accuracy for the phoneme-like unit is achieved in this application. The parameters of the neural net have been optimised for maximum recognition performance. A segmentation strategy using the sum of the difference in filterbank channel energy over successive spectra produced 80.27 % correct segmentation of isolated utterances into phoneme-like units. A linguistic processor based on that of Kashyap &amp;amp; Mittal [84] enables 93.11 % and 93.49 % word recognition accuracy to be achieved for the SS&amp;amp;L and &amp;apos;Sliding Window &amp;apos; recognisers respectively. The linguistic processor has been redesigned to make it portable so that it can be easily applied to any phoneme based isolated word speech recogruser, To my Parents, brothers and sisters, Aunt Afifa, Unc1e Ghazanfer and Cousin Fahad. ACKNOWLEDGEMENT I would like to express heartfelt thanks to my Supervisor, Dr. T.J. Stonham for his guidance, help, and encouragement throughout this",
      "abstract": "recognition based on sub-word component&amp;quot;. A digital neural network is the fundamental processing strategy in beth methods. The first design is based on the &amp;apos;Separate Segmentation &amp;amp; Labelling &amp;apos; (SS&amp;amp;L) approach. The spectral data of the input utterance is first segmented into phoneme-like units which are then time normalised by linear time normalisation. The neural network labels the time-normalised phoneme-like segments./8.36 % recognition accuracy is achieved for the phoneme-like unit. In the second design, no time no-malisation is required. After segmentation, recognition is performed by classifying the data in a window as it is slid one frame at a time, from the start to the end of of each phoneme-like segment in the utterance. 73.97 % recognition accuracy for the phoneme-like unit is achieved in this application. The parameters of the neural net have been optimised for maximum recognition performance. A segmentation strategy using the sum of the difference in filterbank channel energy over successive spectra produced 80.27 % correct segmentation of isolated utterances into phoneme-like units. A linguistic processor based on that of Kashyap &amp;amp; Mittal [84] enables 93.11 % and 93.49 % word recognition accuracy to be achieved for the SS&amp;amp;L and &amp;apos;Sliding Window &amp;apos; recognisers respectively. The linguistic processor has been redesigned to make it portable so that it can be easily applied to any phoneme based isolated word speech recogruser, To my Parents, brothers and sisters, Aunt Afifa, Unc1e Ghazanfer and Cousin Fahad. ACKNOWLEDGEMENT I would like to express heartfelt thanks to my Supervisor, Dr. T.J. Stonham for his guidance, help, and encouragement throughout this",
      "doi": "",
      "openalex_id": "https://openalex.org/W1997183752",
      "arxiv_id": "",
      "publication_date": "1989-01-01",
      "published": "1989-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes phoneme-like unit segmentation for speech recognition but does not present discrete audio tokens via VQ/RVQ or token-based language modeling, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes phoneme-like unit segmentation for speech recognition but does not present discrete audio tokens via VQ/RVQ or token-based language modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on speech recognition using digital neural networks and phoneme-like segmentation and labeling, without discussing methods for generating discrete audio tokens, vector quantization, or encoding audio into a finite set of discrete tokens as required by the inclusion criteria. There is no mention of token generation processes or their use in sequence modeling or language models, which are core to the inclusion criteria. The content aligns with speech recognition algorithms rather than discrete audio token design, so it does not meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on speech recognition using digital neural networks and phoneme-like segmentation and labeling, without discussing methods for generating discrete audio tokens, vector quantization, or encoding audio into a finite set of discrete tokens as required by the inclusion criteria. There is no mention of token generation processes or their use in sequence modeling or language models, which are core to the inclusion criteria. The content aligns with speech recognition algorithms rather than discrete audio token design, so it does not meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Query-by-Example Spoken Document Retrieval : The Star Challenge 2008",
    "abstract": "In this paper, we give an update of recent research activities in HLT department of I2R in query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the Star Challenge 2008, which was organized by A*STAR, Singapore. It is suggested that low-level feature-based approach, which does not rely on error-prone speech transcripts, is a promising solution to query-by-example multilingual spoken document retrieval.",
    "metadata": {
      "title": "Query-by-Example Spoken Document Retrieval : The Star Challenge 2008",
      "summary": "In this paper, we give an update of recent research activities in HLT department of I2R in query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the Star Challenge 2008, which was organized by A*STAR, Singapore. It is suggested that low-level feature-based approach, which does not rely on error-prone speech transcripts, is a promising solution to query-by-example multilingual spoken document retrieval.",
      "abstract": "In this paper, we give an update of recent research activities in HLT department of I2R in query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the Star Challenge 2008, which was organized by A*STAR, Singapore. It is suggested that low-level feature-based approach, which does not rely on error-prone speech transcripts, is a promising solution to query-by-example multilingual spoken document retrieval.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2125222687",
      "arxiv_id": "",
      "publication_date": "2009-10-04",
      "published": "2009-10-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses query-by-example spoken document retrieval and a challenge evaluation, with no mention of discrete audio tokens or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses query-by-example spoken document retrieval and a challenge evaluation, with no mention of discrete audio tokens or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses query-by-example spoken document retrieval using low-level feature-based approaches, but it does not mention the design or generation of discrete audio tokens using vector quantization or similar mechanisms as a core method, nor detailed descriptions of token formation or usage for sequence/language modeling; therefore, it does not meet the inclusion criteria regarding discrete audio token research and lacks sufficient method detail for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses query-by-example spoken document retrieval using low-level feature-based approaches, but it does not mention the design or generation of discrete audio tokens using vector quantization or similar mechanisms as a core method, nor detailed descriptions of token formation or usage for sequence/language modeling; therefore, it does not meet the inclusion criteria regarding discrete audio token research and lacks sufficient method detail for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks",
    "abstract": "Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery. Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",
    "metadata": {
      "title": "Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks",
      "summary": "Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery. Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",
      "abstract": "Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery. Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",
      "doi": "https://doi.org/10.48550/arxiv.2011.14062",
      "openalex_id": "https://openalex.org/W3110206221",
      "arxiv_id": "",
      "publication_date": "2020-11-28",
      "published": "2020-11-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on unsupervised spoken term discovery with hypothesized subword sequences and Siamese/Triplet-based re-clustering, but it does not describe discrete audio token generation/quantization (e.g., VQ/RVQ) or token-level representations for language-modeling, so it fails the discrete-token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on unsupervised spoken term discovery with hypothesized subword sequences and Siamese/Triplet-based re-clustering, but it does not describe discrete audio token generation/quantization (e.g., VQ/RVQ) or token-level representations for language-modeling, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses unsupervised spoken term discovery from untranscribed speech by decoding speech into subword units and performing re-clustering via Siamese and Triplet networks on hypothesized segments. However, it does not explicitly mention producing or using discrete audio tokens generated through vector quantization, residual quantization, or semantic-driven discretization as core contributions. The methods focus on segment similarity learning and clustering rather than the generation of discrete tokens designed for sequence/language model input or cross-modal tasks. Therefore, the core method does not meet the inclusion criteria requiring discrete audio token design and clear token generation processes for sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study addresses unsupervised spoken term discovery from untranscribed speech by decoding speech into subword units and performing re-clustering via Siamese and Triplet networks on hypothesized segments. However, it does not explicitly mention producing or using discrete audio tokens generated through vector quantization, residual quantization, or semantic-driven discretization as core contributions. The methods focus on segment similarity learning and clustering rather than the generation of discrete tokens designed for sequence/language model input or cross-modal tasks. Therefore, the core method does not meet the inclusion criteria requiring discrete audio token design and clear token generation processes for sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A sub-word based speaker independent speech recognizer using a two-pass segmentation scheme",
    "abstract": "An isolated-word speech recognizer based on acoustically defined subwords is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the subword boundaries are estimated. The second pass divides each subword into three subsegments, thus matching the states in the three-state HMMs used to model the subwords. On the basis of both the segmental and subsegmental information, the acoustic lexicon and the HMMs are created. Used in a speaker-independent mode the recognizer is slightly inferior to a corresponding whole-word-based recognizer.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "A sub-word based speaker independent speech recognizer using a two-pass segmentation scheme",
      "summary": "An isolated-word speech recognizer based on acoustically defined subwords is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the subword boundaries are estimated. The second pass divides each subword into three subsegments, thus matching the states in the three-state HMMs used to model the subwords. On the basis of both the segmental and subsegmental information, the acoustic lexicon and the HMMs are created. Used in a speaker-independent mode the recognizer is slightly inferior to a corresponding whole-word-based recognizer.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "An isolated-word speech recognizer based on acoustically defined subwords is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the subword boundaries are estimated. The second pass divides each subword into three subsegments, thus matching the states in the three-state HMMs used to model the subwords. On the basis of both the segmental and subsegmental information, the acoustic lexicon and the HMMs are created. Used in a speaker-independent mode the recognizer is slightly inferior to a corresponding whole-word-based recognizer.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/icassp.1989.266429",
      "openalex_id": "https://openalex.org/W1488766199",
      "arxiv_id": "",
      "publication_date": "2003-01-13",
      "published": "2003-01-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study discusses a subword-based isolated-word ASR with two-pass segmentation and HMMs, but it does not present discrete audio token design (e.g., VQ/RVQ, hierarchical quantization) for audio-language modeling or downstream token-based tasks, so it fails the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study discusses a subword-based isolated-word ASR with two-pass segmentation and HMMs, but it does not present discrete audio token design (e.g., VQ/RVQ, hierarchical quantization) for audio-language modeling or downstream token-based tasks, so it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a two-pass segmentation scheme for speech recognition using acoustically defined subwords, but it does not describe generating discrete audio tokens via vector quantization or other quantization mechanisms intended for sequence or language model processing. The work appears to target speech recognition performance rather than proposing a discrete token representation for audio modeling or generation, and lacks detailed methodology on tokenization or quantization processes that would support use in downstream audio language or multimodal models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a two-pass segmentation scheme for speech recognition using acoustically defined subwords, but it does not describe generating discrete audio tokens via vector quantization or other quantization mechanisms intended for sequence or language model processing. The work appears to target speech recognition performance rather than proposing a discrete token representation for audio modeling or generation, and lacks detailed methodology on tokenization or quantization processes that would support use in downstream audio language or multimodal models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Edge Detection With The Parametric Filtering Method (Comparison With Canny Method)",
    "abstract": "In this paper, a new method of image edge-detection and characterization is presented. \"Parametric Filtering method\" uses a judicious defined filter, which preserves the signal correlation structure as input in the autocorrelation of the output. This leads, showing the evolution of the image correlation structure as well as various distortion measures which quantify the deviation between two zones of the signal (the two Hamming signals) for the protection of an image edge.",
    "metadata": {
      "title": "Edge Detection With The Parametric Filtering Method (Comparison With Canny Method)",
      "summary": "In this paper, a new method of image edge-detection and characterization is presented. \"Parametric Filtering method\" uses a judicious defined filter, which preserves the signal correlation structure as input in the autocorrelation of the output. This leads, showing the evolution of the image correlation structure as well as various distortion measures which quantify the deviation between two zones of the signal (the two Hamming signals) for the protection of an image edge.",
      "abstract": "In this paper, a new method of image edge-detection and characterization is presented. \"Parametric Filtering method\" uses a judicious defined filter, which preserves the signal correlation structure as input in the autocorrelation of the output. This leads, showing the evolution of the image correlation structure as well as various distortion measures which quantify the deviation between two zones of the signal (the two Hamming signals) for the protection of an image edge.",
      "doi": "https://doi.org/10.5281/zenodo.1062766",
      "openalex_id": "https://openalex.org/W1492135023",
      "arxiv_id": "",
      "publication_date": "2007-10-28",
      "published": "2007-10-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses image edge detection using a parametric filtering method, with no mention of discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses image edge detection using a parametric filtering method, with no mention of discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses an image edge-detection method and does not involve audio signals, discrete audio tokens, or any audio-related token generation processes; hence, it does not meet the inclusion criteria which focus on discrete audio token design and usage in audio language modeling or related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses an image edge-detection method and does not involve audio signals, discrete audio tokens, or any audio-related token generation processes; hence, it does not meet the inclusion criteria which focus on discrete audio token design and usage in audio language modeling or related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Segments with Applications-I: General Approach and Application to Speech Recognition",
    "abstract": "Absmcr- We present in this series of two papers a new approach for modeling and capturing the time-varying structure of the spectral envelope of speech. In this approach, we use an acoustic subword decomposition and the Karhunen-Loeve transform (UT) to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the UT with acoustic subword modeling is a novel approach that concisely represents both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The organization of these two papers is as follows: the first paper, Part I presents the physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition. The performance of the recognition algorithm based on this approach compares favorably to other existing techniques. Part I1 will present a frequency-domain coding technique by analysidsynthesis. This application of the new method produces good quality speech at low bit rates.",
    "metadata": {
      "title": "Speech Segments with Applications-I: General Approach and Application to Speech Recognition",
      "summary": "Absmcr- We present in this series of two papers a new approach for modeling and capturing the time-varying structure of the spectral envelope of speech. In this approach, we use an acoustic subword decomposition and the Karhunen-Loeve transform (UT) to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the UT with acoustic subword modeling is a novel approach that concisely represents both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The organization of these two papers is as follows: the first paper, Part I presents the physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition. The performance of the recognition algorithm based on this approach compares favorably to other existing techniques. Part I1 will present a frequency-domain coding technique by analysidsynthesis. This application of the new method produces good quality speech at low bit rates.",
      "abstract": "Absmcr- We present in this series of two papers a new approach for modeling and capturing the time-varying structure of the spectral envelope of speech. In this approach, we use an acoustic subword decomposition and the Karhunen-Loeve transform (UT) to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the UT with acoustic subword modeling is a novel approach that concisely represents both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The organization of these two papers is as follows: the first paper, Part I presents the physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition. The performance of the recognition algorithm based on this approach compares favorably to other existing techniques. Part I1 will present a frequency-domain coding technique by analysidsynthesis. This application of the new method produces good quality speech at low bit rates.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2187576911",
      "arxiv_id": "",
      "publication_date": "1993-01-01",
      "published": "1993-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on modeling the spectral envelope and acoustic-subword representations for speech recognition, not on discretizing audio into tokens via VQ/RVQ or related tokenization methods, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on modeling the spectral envelope and acoustic-subword representations for speech recognition, not on discretizing audio into tokens via VQ/RVQ or related tokenization methods, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a novel approach to capturing the time-varying structure of the spectral envelope of speech using acoustic subword decomposition and Karhunen-Loeve transform, applied to speaker-dependent speech recognition. However, it does not explicitly address the generation of discrete audio tokens through vector quantization or similar methods, nor does it mention producing tokens for use in sequence or language models. The focus is on spectral representation and speech recognition performance, lacking description of token formation, quantization schemes, or discrete token usage as required in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a novel approach to capturing the time-varying structure of the spectral envelope of speech using acoustic subword decomposition and Karhunen-Loeve transform, applied to speaker-dependent speech recognition. However, it does not explicitly address the generation of discrete audio tokens through vector quantization or similar methods, nor does it mention producing tokens for use in sequence or language models. The focus is on spectral representation and speech recognition performance, lacking description of token formation, quantization schemes, or discrete token usage as required in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "USING A TWO-PASS SEGMENTATION SCHEME.",
    "abstract": "An isolated word speech recognizer based on acoustically defined sub-words is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the sub-word boundaries are estimated. The second pass divides each sub- word into three subsegments, thus matching the statcs in the 3-state HMMs used to model the sub-words. Based on both the segmental and subsegmental information, the acoustic lexicon and the HMMs are mated. Used in a speaker independent mode the recognizer is slightly inferior to a corresponding whole-word based recognizer.",
    "metadata": {
      "title": "USING A TWO-PASS SEGMENTATION SCHEME.",
      "summary": "An isolated word speech recognizer based on acoustically defined sub-words is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the sub-word boundaries are estimated. The second pass divides each sub- word into three subsegments, thus matching the statcs in the 3-state HMMs used to model the sub-words. Based on both the segmental and subsegmental information, the acoustic lexicon and the HMMs are mated. Used in a speaker independent mode the recognizer is slightly inferior to a corresponding whole-word based recognizer.",
      "abstract": "An isolated word speech recognizer based on acoustically defined sub-words is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the sub-word boundaries are estimated. The second pass divides each sub- word into three subsegments, thus matching the statcs in the 3-state HMMs used to model the sub-words. Based on both the segmental and subsegmental information, the acoustic lexicon and the HMMs are mated. Used in a speaker independent mode the recognizer is slightly inferior to a corresponding whole-word based recognizer.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2188541342",
      "arxiv_id": "",
      "publication_date": "1989-01-01",
      "published": "1989-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes a two-pass segmentation for sub-words with HMM-based isolated word recognition and does not present discrete audio tokens (VQ/RVQ, hierarchical quantization, or semantic discretization) used for language-model style sequence modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes a two-pass segmentation for sub-words with HMM-based isolated word recognition and does not present discrete audio tokens (VQ/RVQ, hierarchical quantization, or semantic discretization) used for language-model style sequence modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a speech recognition system using a two-pass segmentation algorithm based on acoustically defined sub-words. However, there is no mention of discrete audio tokens generated via neural codecs, vector quantization, or any form of semantic-driven discretization. The focus is on segmenting speech for HMM-based recognition, without clear evidence of producing discrete tokens for downstream sequence modeling or language modeling as per the inclusion criteria. Therefore, the study does not meet the main inclusion criteria related to discrete token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a speech recognition system using a two-pass segmentation algorithm based on acoustically defined sub-words. However, there is no mention of discrete audio tokens generated via neural codecs, vector quantization, or any form of semantic-driven discretization. The focus is on segmenting speech for HMM-based recognition, without clear evidence of producing discrete tokens for downstream sequence modeling or language modeling as per the inclusion criteria. Therefore, the study does not meet the main inclusion criteria related to discrete token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AI-Based Affective Music Generation Systems: A Review of Methods and Challenges",
    "abstract": "Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.",
    "metadata": {
      "title": "AI-Based Affective Music Generation Systems: A Review of Methods and Challenges",
      "summary": "Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.",
      "abstract": "Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.",
      "doi": "https://doi.org/10.1145/3672554",
      "openalex_id": "https://openalex.org/W4399743538",
      "arxiv_id": "",
      "publication_date": "2024-06-17",
      "published": "2024-06-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a general review of AI-based affective music generation and does not focus on discrete audio tokens or tokenization design as a core contribution, thus it fails the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a general review of AI-based affective music generation and does not focus on discrete audio tokens or tokenization design as a core contribution, thus it fails the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper reviews AI-based affective music generation systems but does not mention discrete audio tokens, vector quantization, or any related tokenization techniques as core methods or contributions, nor does it describe token generation processes or methodology details to support reproducibility. Therefore, it does not meet the inclusion criteria focusing on discrete audio token techniques, and it lacks the necessary methodological specifics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper reviews AI-based affective music generation systems but does not mention discrete audio tokens, vector quantization, or any related tokenization techniques as core methods or contributions, nor does it describe token generation processes or methodology details to support reproducibility. Therefore, it does not meet the inclusion criteria focusing on discrete audio token techniques, and it lacks the necessary methodological specifics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audiosr: Versatile Audio Super-Resolution at Scale",
    "abstract": "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.",
    "metadata": {
      "title": "Audiosr: Versatile Audio Super-Resolution at Scale",
      "summary": "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.",
      "abstract": "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447246",
      "openalex_id": "https://openalex.org/W4392903177",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on diffusion-based audio super-resolution (AudioSR) without discussing discrete audio tokens or tokenization methods, so it does not meet the discrete token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on diffusion-based audio super-resolution (AudioSR) without discussing discrete audio tokens or tokenization methods, so it does not meet the discrete token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on audio super-resolution using a diffusion-based generative model to enhance audio quality across various audio types and bandwidths. However, it does not mention generating discrete audio tokens, nor does it describe token formation processes such as vector quantization or semantic-driven discretization mechanisms. The work emphasizes audio enhancement rather than representing audio as discrete tokens for language or sequence modeling, so it does not meet the inclusion criteria of discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on audio super-resolution using a diffusion-based generative model to enhance audio quality across various audio types and bandwidths. However, it does not mention generating discrete audio tokens, nor does it describe token formation processes such as vector quantization or semantic-driven discretization mechanisms. The work emphasizes audio enhancement rather than representing audio as discrete tokens for language or sequence modeling, so it does not meet the inclusion criteria of discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning",
    "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.",
    "metadata": {
      "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning",
      "summary": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.",
      "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447027",
      "openalex_id": "https://openalex.org/W4392909390",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text-to-music generation, music question answering and captioning, with no discussion of discrete audio tokens or tokenization/quantization mechanisms; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text-to-music generation, music question answering and captioning, with no discussion of discrete audio tokens or tokenization/quantization mechanisms; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-to-music generation and music question answering using pretrained audio models but does not discuss the generation or use of discrete audio tokens derived through vector quantization, residual quantization, or similar methods; it lacks detailed methodology on tokenization or discrete audio representation critical for inclusion under the specified criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-to-music generation and music question answering using pretrained audio models but does not discuss the generation or use of discrete audio tokens derived through vector quantization, residual quantization, or similar methods; it lacks detailed methodology on tokenization or discrete audio representation critical for inclusion under the specified criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Investigating Personalization Methods in Text to Music Generation",
    "abstract": "In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "Investigating Personalization Methods in Text to Music Generation",
      "summary": "In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446869",
      "openalex_id": "https://openalex.org/W4393138539",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於文本到音樂生成的個性化方法，而非以離散音訊 token 的設計、量化或其作為序列模型中介表示，因此不符合本研究的離散音訊 token 納入標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於文本到音樂生成的個性化方法，而非以離散音訊 token 的設計、量化或其作為序列模型中介表示，因此不符合本研究的離散音訊 token 納入標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on personalization methods for text-to-music generation using diffusion models but does not explicitly mention the use or design of discrete audio tokens, vector quantization, or any quantization methods for token generation as required by the inclusion criteria. The abstract lacks details on token generation mechanisms or quantization processes and does not clearly position discrete audio tokens as a core part of the methodology, thus failing to meet the main thematic requirement for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on personalization methods for text-to-music generation using diffusion models but does not explicitly mention the use or design of discrete audio tokens, vector quantization, or any quantization methods for token generation as required by the inclusion criteria. The abstract lacks details on token generation mechanisms or quantization processes and does not clearly position discrete audio tokens as a core part of the methodology, thus failing to meet the main thematic requirement for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Harmonizing AI-Generated Music: Integrating Symbolic and Audio Models for Text-to-Music Generation",
    "abstract": "The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.",
    "metadata": {
      "title": "Harmonizing AI-Generated Music: Integrating Symbolic and Audio Models for Text-to-Music Generation",
      "summary": "The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.",
      "abstract": "The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.",
      "doi": "https://doi.org/10.1109/aiiip61647.2023.00030",
      "openalex_id": "https://openalex.org/W4391584535",
      "arxiv_id": "",
      "publication_date": "2023-10-27",
      "published": "2023-10-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes text-to-music generation using symbolic and audio models but does not discuss Discrete Audio Tokens, tokenization/quantization methods, or token-based representations, so it fails to meet the inclusion criteria requiring discrete audio token design and related technical details.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes text-to-music generation using symbolic and audio models but does not discuss Discrete Audio Tokens, tokenization/quantization methods, or token-based representations, so it fails to meet the inclusion criteria requiring discrete audio token design and related technical details.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study integrates symbolic and audio models for text-to-music generation, focusing on alignment between generated music and input text through a two-step process but does not mention discrete audio tokens, vector quantization, or any tokenization mechanisms as defined in the inclusion criteria. There is no description of token generation processes, quantization methods, or the use of discrete tokens for sequence modeling, which are core requirements for inclusion. Thus, it does not meet the inclusion criteria related to discrete audio token design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study integrates symbolic and audio models for text-to-music generation, focusing on alignment between generated music and input text through a two-step process but does not mention discrete audio tokens, vector quantization, or any tokenization mechanisms as defined in the inclusion criteria. There is no description of token generation processes, quantization methods, or the use of discrete tokens for sequence modeling, which are core requirements for inclusion. Thus, it does not meet the inclusion criteria related to discrete audio token design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AI for Musical Discovery",
    "abstract": "What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.",
    "metadata": {
      "title": "AI for Musical Discovery",
      "summary": "What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.",
      "abstract": "What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.",
      "doi": "https://doi.org/10.21428/e4baedd9.8fa181e9",
      "openalex_id": "https://openalex.org/W4393228744",
      "arxiv_id": "",
      "publication_date": "2024-03-27",
      "published": "2024-03-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on AI for musical discovery at a conceptual level and does not present discrete audio token representations, token generation methods, or other required technical details; it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on AI for musical discovery at a conceptual level and does not present discrete audio token representations, token generation methods, or other required technical details; it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss the role of generative AI in musical discovery, focusing on expanding creativity and community engagement in music, but do not mention discrete audio token design, vector quantization, token generation processes, or sequence/language modeling of audio tokens as required by the inclusion criteria. There is no technical detail about token formation, quantization, or audio language modeling, and thus it lacks the core methodological contribution and details needed for inclusion. Hence, it falls outside the defined scope and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss the role of generative AI in musical discovery, focusing on expanding creativity and community engagement in music, but do not mention discrete audio token design, vector quantization, token generation processes, or sequence/language modeling of audio tokens as required by the inclusion criteria. There is no technical detail about token formation, quantization, or audio language modeling, and thus it lacks the core methodological contribution and details needed for inclusion. Hence, it falls outside the defined scope and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How voice and helpfulness shape perceptions in human–agent teams",
    "abstract": "Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.",
    "metadata": {
      "title": "How voice and helpfulness shape perceptions in human–agent teams",
      "summary": "Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.",
      "abstract": "Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.",
      "doi": "https://doi.org/10.1016/j.chbah.2024.100101",
      "openalex_id": "https://openalex.org/W4404135183",
      "arxiv_id": "",
      "publication_date": "2024-08-01",
      "published": "2024-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item exclusively discusses how voice type and perceived helpfulness affect perceptions in human–agent teams, with no focus on discrete audio tokens, token generation, or quantized representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item exclusively discusses how voice type and perceived helpfulness affect perceptions in human–agent teams, with no focus on discrete audio tokens, token generation, or quantized representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on human-agent interaction, specifically voice assistants and their perceived helpfulness and anthropomorphism. It does not mention discrete audio tokens, vector quantization, or any form of converting continuous audio into discrete token representations for sequence modeling, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on human-agent interaction, specifically voice assistants and their perceived helpfulness and anthropomorphism. It does not mention discrete audio tokens, vector quantization, or any form of converting continuous audio into discrete token representations for sequence modeling, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation",
    "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
    "metadata": {
      "title": "Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation",
      "summary": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448249",
      "openalex_id": "https://openalex.org/W4392902987",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on Multi-view MidiVAE for long multi-track symbolic music generation using MIDI representations; it does not discuss discrete audio tokens, vector quantization, or token-based representations for audio/language modeling, thus it does not meet the inclusion criteria related to Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on Multi-view MidiVAE for long multi-track symbolic music generation using MIDI representations; it does not discuss discrete audio tokens, vector quantization, or token-based representations for audio/language modeling, thus it does not meet the inclusion criteria related to Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on variational autoencoders for long multi-track symbolic music generation, but it does not describe generating discrete audio tokens through vector quantization or similar methods as a primary contribution. The paper lacks detailed mention of token generation processes or discrete token design used for language model-based sequence modeling. Hence, it does not fulfill the required inclusion criteria emphasizing discrete audio token production for audio language modeling or related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on variational autoencoders for long multi-track symbolic music generation, but it does not describe generating discrete audio tokens through vector quantization or similar methods as a primary contribution. The paper lacks detailed mention of token generation processes or discrete token design used for language model-based sequence modeling. Hence, it does not fulfill the required inclusion criteria emphasizing discrete audio token production for audio language modeling or related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Learning for Style Transfer and Experimentation with Audio Effects and Music Creation",
    "abstract": "Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.",
    "metadata": {
      "title": "Deep Learning for Style Transfer and Experimentation with Audio Effects and Music Creation",
      "summary": "Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.",
      "abstract": "Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.",
      "doi": "https://doi.org/10.1609/aaai.v38i21.30558",
      "openalex_id": "https://openalex.org/W4393145938",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on deep learning for music style transfer and audio effects rather than discrete audio tokenization or token-based representations, and lacks details on token generation, quantization, or evaluation needed by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on deep learning for music style transfer and audio effects rather than discrete audio tokenization or token-based representations, and lacks details on token generation, quantization, or evaluation needed by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss deep learning methods for style transfer, audio effects, and music creation, but do not mention discrete audio tokens, vector quantization, or token generation processes required by the inclusion criteria. There is no indication that a discrete token representation or quantization-based tokenization is a core method or contribution. The description is more general and lacks the technical details and tokenization specifics necessary for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss deep learning methods for style transfer, audio effects, and music creation, but do not mention discrete audio tokens, vector quantization, or token generation processes required by the inclusion criteria. There is no indication that a discrete token representation or quantization-based tokenization is a core method or contribution. The description is more general and lacks the technical details and tokenization specifics necessary for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input",
    "abstract": "Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.",
    "metadata": {
      "title": "Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input",
      "summary": "Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.",
      "abstract": "Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.",
      "doi": "https://doi.org/10.1109/nice61972.2024.10549580",
      "openalex_id": "https://openalex.org/W4399530928",
      "arxiv_id": "",
      "publication_date": "2024-04-23",
      "published": "2024-04-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses synthetic event camera streams from text prompts (visual modality) and does not discuss discrete audio token design, quantization, or audio-language modeling; it fails to meet the inclusion criteria focused on discrete audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses synthetic event camera streams from text prompts (visual modality) and does not discuss discrete audio token design, quantization, or audio-language modeling; it fails to meet the inclusion criteria focused on discrete audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on synthesizing event camera data from text prompts and does not discuss discrete audio tokens, vector quantization, or any audio-related tokenization methods. It lacks details about discrete audio token generation or usage in language/sequence models and thus does not meet the inclusion criteria targeting discrete audio token research, making it irrelevant for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on synthesizing event camera data from text prompts and does not discuss discrete audio tokens, vector quantization, or any audio-related tokenization methods. It lacks details about discrete audio token generation or usage in language/sequence models and thus does not meet the inclusion criteria targeting discrete audio token research, making it irrelevant for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning latent representations for controllable combinational creativity and game design",
    "abstract": "Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract",
    "metadata": {
      "title": "Learning latent representations for controllable combinational creativity and game design",
      "summary": "Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract",
      "abstract": "Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract",
      "doi": "https://doi.org/10.17760/d20581905",
      "openalex_id": "https://openalex.org/W4388761715",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns learning latent representations for game design and combinational creativity, not discrete audio tokens or tokenization methods, therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns learning latent representations for game design and combinational creativity, not discrete audio tokens or tokenization methods, therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the use of deep latent variable models for game design applications, including game blending and level design; however, it does not mention any discrete audio token representations, vector quantization, or related audio signal processing methods as defined in the inclusion criteria. Thus, the core topic does not align with discrete audio token research and does not meet the required criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the use of deep latent variable models for game design applications, including game blending and level design; however, it does not mention any discrete audio token representations, vector quantization, or related audio signal processing methods as defined in the inclusion criteria. Thus, the core topic does not align with discrete audio token research and does not meet the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation",
    "abstract": "Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/",
    "metadata": {
      "title": "Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation",
      "summary": "Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/",
      "abstract": "Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/",
      "doi": "https://doi.org/10.1109/bigdata59044.2023.10386748",
      "openalex_id": "https://openalex.org/W4391093895",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image-to-music generation with a dual-path network for preserving image content and musicality, but it does not involve discrete audio tokens, token generation, or quantization mechanisms, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image-to-music generation with a dual-path network for preserving image content and musicality, but it does not involve discrete audio tokens, token generation, or quantization mechanisms, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a network for image to music generation that focuses on musical element enhancement and image content preservation; however, there is no mention of discrete audio tokens, their generation, vector quantization or related methods, nor is there a description of token formation or usage as input for language or sequence models. The paper does not meet the core inclusion criteria related to discrete audio token design, nor does it provide details on token generation or quantization methods. Therefore, it does not fit within the scope defined by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a network for image to music generation that focuses on musical element enhancement and image content preservation; however, there is no mention of discrete audio tokens, their generation, vector quantization or related methods, nor is there a description of token formation or usage as input for language or sequence models. The paper does not meet the core inclusion criteria related to discrete audio token design, nor does it provide details on token generation or quantization methods. Therefore, it does not fit within the scope defined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm",
    "abstract": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.",
    "metadata": {
      "title": "Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm",
      "summary": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.",
      "abstract": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.",
      "doi": "https://doi.org/10.1109/wacv57701.2024.00702",
      "openalex_id": "https://openalex.org/W4394625798",
      "arxiv_id": "",
      "publication_date": "2024-01-03",
      "published": "2024-01-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not center on discrete audio token representations or tokenization methods (e.g., VQ/RVQ, SSL-derived discrete tokens) as the core contribution; it describes learning-based real-time translation from body motion to polyphonic drum sounds, which is not aligned with the discrete-audio-token criteria. Additionally, the provided metadata lacks an explicit publication/update date, raising traceability concerns per the exclusion rules.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not center on discrete audio token representations or tokenization methods (e.g., VQ/RVQ, SSL-derived discrete tokens) as the core contribution; it describes learning-based real-time translation from body motion to polyphonic drum sounds, which is not aligned with the discrete-audio-token criteria. Additionally, the provided metadata lacks an explicit publication/update date, raising traceability concerns per the exclusion rules.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a system that generates drum sounds based on body rhythm captured by video, using sequence models and a conditional drum generation network; however, it does not mention the use of discrete audio tokens, vector quantization, residual quantization, or related mechanisms to produce discrete token representations for audio modeling or generation. The focus is more on real-time rhythm generation from body movements rather than on discrete audio token design or usage as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a system that generates drum sounds based on body rhythm captured by video, using sequence models and a conditional drum generation network; however, it does not mention the use of discrete audio tokens, vector quantization, residual quantization, or related mechanisms to produce discrete token representations for audio modeling or generation. The focus is more on real-time rhythm generation from body movements rather than on discrete audio token design or usage as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Raging with the Machine in the Uncanny Valley: Human–AI Cocreativity in the Eurovision-Themed AI Song Contest",
    "abstract": "Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.",
    "metadata": {
      "title": "Raging with the Machine in the Uncanny Valley: Human–AI Cocreativity in the Eurovision-Themed AI Song Contest",
      "summary": "Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.",
      "abstract": "Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.",
      "doi": "https://doi.org/10.1162/comj_a_00674",
      "openalex_id": "https://openalex.org/W4396606329",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes human–AI cocreation in an AI song contest and technical highlights (melody indexing, DSP timbre) but provides no discrete audio token design/quantization methodology, thus not meeting the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes human–AI cocreation in an AI song contest and technical highlights (melody indexing, DSP timbre) but provides no discrete audio token design/quantization methodology, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a human–AI cocreativity process for song generation using technical innovations like stress-based melody indexing and novel timbre creation, but it does not mention any discrete audio tokens, vector quantization, or related mechanisms used as sequence modeling inputs. There is no detailed method for generating or using discrete audio tokens as per the inclusion criteria, nor is there evidence of supporting experimental or technical detail on token quantization or usage. Thus, it does not meet the core inclusion criteria focused on discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a human–AI cocreativity process for song generation using technical innovations like stress-based melody indexing and novel timbre creation, but it does not mention any discrete audio tokens, vector quantization, or related mechanisms used as sequence modeling inputs. There is no detailed method for generating or using discrete audio tokens as per the inclusion criteria, nor is there evidence of supporting experimental or technical detail on token quantization or usage. Thus, it does not meet the core inclusion criteria focused on discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms",
    "abstract": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
    "metadata": {
      "title": "Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms",
      "summary": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "abstract": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "doi": "https://doi.org/10.14569/ijacsa.2024.01505101",
      "openalex_id": "https://openalex.org/W4399260034",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on deep-learning-based music generation using text-based representations and MT-CHSE-GAN, with no description of discrete audio tokens, vector quantization, or token-based modeling; thus it does not meet the Discrete Audio Tokens criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on deep-learning-based music generation using text-based representations and MT-CHSE-GAN, with no description of discrete audio tokens, vector quantization, or token-based modeling; thus it does not meet the Discrete Audio Tokens criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on automatic music generation using deep learning and GAN architectures but does not mention discrete audio tokens, vector quantization, or any tokenization process designed for sequence or language models, nor does it describe token generation or quantization mechanisms. Hence, it does not meet the inclusion criteria around discrete audio token design or methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on automatic music generation using deep learning and GAN architectures but does not mention discrete audio tokens, vector quantization, or any tokenization process designed for sequence or language models, nor does it describe token generation or quantization mechanisms. Hence, it does not meet the inclusion criteria around discrete audio token design or methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Benchmarking Music Generation Models and Metrics via Human Preference Studies",
    "abstract": "Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.",
    "metadata": {
      "title": "Benchmarking Music Generation Models and Metrics via Human Preference Studies",
      "summary": "Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.",
      "abstract": "Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.",
      "doi": "https://doi.org/10.1109/icassp49660.2025.10887745",
      "openalex_id": "https://openalex.org/W4408345698",
      "arxiv_id": "",
      "publication_date": "2025-03-12",
      "published": "2025-03-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on benchmarking music generation models and metrics using human preference studies, with no discussion of discrete audio tokens, tokenization methods, or VQ/RVQ-based discrete representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on benchmarking music generation models and metrics using human preference studies, with no discussion of discrete audio tokens, tokenization methods, or VQ/RVQ-based discrete representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on benchmarking music generation models via human preference studies, but it does not mention the use or design of discrete audio tokens or their generation process via vector quantization or similar methods, which is a core inclusion criterion; also, no detailed methodology about token formation or quantization is provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on benchmarking music generation models via human preference studies, but it does not mention the use or design of discrete audio tokens or their generation process via vector quantization or similar methods, which is a core inclusion criterion; also, no detailed methodology about token formation or quantization is provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Confidence Estimation and Deletion Prediction Using Bidirectional Recurrent Neural Networks",
    "abstract": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
    "metadata": {
      "title": "Confidence Estimation and Deletion Prediction Using Bidirectional Recurrent Neural Networks",
      "summary": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
      "abstract": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
      "doi": "https://doi.org/10.17863/cam.35236",
      "openalex_id": "https://openalex.org/W2898630520",
      "arxiv_id": "",
      "publication_date": "2018-12-21",
      "published": "2018-12-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on ASR confidence estimation and deletions using BiRNNs, but it does not address discrete audio tokens or tokenization methods, failing the inclusion criterion that requires discrete audio token design/quantization as the core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on ASR confidence estimation and deletions using BiRNNs, but it does not address discrete audio tokens or tokenization methods, failing the inclusion criterion that requires discrete audio token design/quantization as the core contribution.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on confidence estimation and deletion prediction in speech transcription using bidirectional recurrent neural networks, without any mention of generating or using discrete audio tokens, vector quantization, or any quantization processes to produce discrete symbolic representations of audio for downstream sequence or language modeling tasks, which is critical according to the inclusion criteria. The method centers on confidence scoring rather than discrete token design or generation, and no technical details regarding tokenization or quantization are provided. Therefore, it does not meet the inclusion criteria related to discrete audio token research and does not provide relevant methodological details required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on confidence estimation and deletion prediction in speech transcription using bidirectional recurrent neural networks, without any mention of generating or using discrete audio tokens, vector quantization, or any quantization processes to produce discrete symbolic representations of audio for downstream sequence or language modeling tasks, which is critical according to the inclusion criteria. The method centers on confidence scoring rather than discrete token design or generation, and no technical details regarding tokenization or quantization are provided. Therefore, it does not meet the inclusion criteria related to discrete audio token research and does not provide relevant methodological details required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stimulated training for automatic speech recognition and keyword search in limited resource conditions",
    "abstract": "Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.",
    "metadata": {
      "title": "Stimulated training for automatic speech recognition and keyword search in limited resource conditions",
      "summary": "Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.",
      "abstract": "Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953074",
      "openalex_id": "https://openalex.org/W2696253854",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates stimulated training for ASR and keyword search in limited-resource conditions, focusing on training techniques and ensembles rather than discrete audio token representations; it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates stimulated training for ASR and keyword search in limited-resource conditions, focusing on training techniques and ensembles rather than discrete audio token representations; it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on stimulated training for neural network acoustic models to improve recognition performance on limited data, but it does not mention producing discrete audio tokens via vector quantization or similar quantization-based methods, nor does it discuss token generation, token usage for language modeling or sequence modeling, or semantic/acoustic disentanglement of tokens. Therefore, it does not meet the inclusion criteria centered on discrete token design and usage, and it is outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on stimulated training for neural network acoustic models to improve recognition performance on limited data, but it does not mention producing discrete audio tokens via vector quantization or similar quantization-based methods, nor does it discuss token generation, token usage for language modeling or sequence modeling, or semantic/acoustic disentanglement of tokens. Therefore, it does not meet the inclusion criteria centered on discrete token design and usage, and it is outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The 2016 BBN Georgian telephone speech keyword spotting system",
    "abstract": "In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.",
    "metadata": {
      "title": "The 2016 BBN Georgian telephone speech keyword spotting system",
      "summary": "In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.",
      "abstract": "In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953259",
      "openalex_id": "https://openalex.org/W2671812860",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a keyword spotting system for Georgian telephone speech and does not discuss discrete audio token representations (VQ/RVQ/semantic tokenization) or any token-generation/quantization methods, so it fails all inclusion criteria and meets at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a keyword spotting system for Georgian telephone speech and does not discuss discrete audio token representations (VQ/RVQ/semantic tokenization) or any token-generation/quantization methods, so it fails all inclusion criteria and meets at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a keyword spotting system for telephone speech focusing on system performance under limited data and noisy conditions; however, it does not mention the design or use of discrete audio tokens, their generation process, quantization methods, or usage in sequence/language models, which are crucial for inclusion according to the criteria. The work appears to be a system description without emphasizing discrete token representations or detailed method disclosure about tokenization strategies, thus failing to meet the inclusion criteria and instead aligns more with systems evaluation rather than token-based audio representation research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a keyword spotting system for telephone speech focusing on system performance under limited data and noisy conditions; however, it does not mention the design or use of discrete audio tokens, their generation process, quantization methods, or usage in sequence/language models, which are crucial for inclusion according to the criteria. The work appears to be a system description without emphasizing discrete token representations or detailed method disclosure about tokenization strategies, thus failing to meet the inclusion criteria and instead aligns more with systems evaluation rather than token-based audio representation research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Speech Recognition for Under-Resourced Languages: Application to Vietnamese Language",
    "abstract": "This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.",
    "metadata": {
      "title": "Automatic Speech Recognition for Under-Resourced Languages: Application to Vietnamese Language",
      "summary": "This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.",
      "abstract": "This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.",
      "doi": "https://doi.org/10.1109/tasl.2009.2021723",
      "openalex_id": "https://openalex.org/W2141820854",
      "arxiv_id": "",
      "publication_date": "2009-04-28",
      "published": "2009-04-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with ASR for under-resourced languages and cross-lingual/grapheme-based modeling, but it does not discuss creating discrete audio tokens (e.g., VQ/RVQ, SSL-generated discrete tokens) or token-based audio representations, so it does not meet the specified inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with ASR for under-resourced languages and cross-lingual/grapheme-based modeling, but it does not discuss creating discrete audio tokens (e.g., VQ/RVQ, SSL-generated discrete tokens) or token-based audio representations, so it does not meet the specified inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on automatic speech recognition (ASR) for an under-resourced language (Vietnamese) and details various acoustic modeling techniques; however, it does not mention the creation or use of discrete audio tokens through vector quantization or similar discrete representation mechanisms, nor does it describe token generation processes or their applications in sequence or language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on automatic speech recognition (ASR) for an under-resourced language (Vietnamese) and details various acoustic modeling techniques; however, it does not mention the creation or use of discrete audio tokens through vector quantization or similar discrete representation mechanisms, nor does it describe token generation processes or their applications in sequence or language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An End-to-End Language-Tracking Speech Recognizer for Mixed-Language Speech",
    "abstract": "End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.",
    "metadata": {
      "title": "An End-to-End Language-Tracking Speech Recognizer for Mixed-Language Speech",
      "summary": "End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.",
      "abstract": "End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.",
      "doi": "https://doi.org/10.1109/icassp.2018.8462180",
      "openalex_id": "https://openalex.org/W2891616026",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses end-to-end ASR with mixed-language speech and language-tracking within utterances, but does not present discrete audio tokens or tokenization/quantization methods as core contributions, thus failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses end-to-end ASR with mixed-language speech and language-tracking within utterances, but does not present discrete audio tokens or tokenization/quantization methods as core contributions, thus failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an end-to-end multilingual speech recognition model that can handle mixed-language speech but does not mention generating or utilizing discrete audio tokens, vector quantization, or any tokenization process that converts continuous audio into discrete token representations for language modeling or generation tasks as required by the inclusion criteria. Therefore, it does not meet the core criterion of discrete audio token design or usage, nor does it describe any token generation or quantization process relevant to the theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an end-to-end multilingual speech recognition model that can handle mixed-language speech but does not mention generating or utilizing discrete audio tokens, vector quantization, or any tokenization process that converts continuous audio into discrete token representations for language modeling or generation tasks as required by the inclusion criteria. Therefore, it does not meet the core criterion of discrete audio token design or usage, nor does it describe any token generation or quantization process relevant to the theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
    "abstract": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).",
    "metadata": {
      "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
      "summary": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).",
      "abstract": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).",
      "doi": "https://doi.org/10.21437/interspeech.2019-2858",
      "openalex_id": "https://openalex.org/W2971840980",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is about a streaming end-to-end multilingual ASR system with language conditioning and adapters; it does not involve discrete audio tokens or tokenization schemes as a core contribution, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is about a streaming end-to-end multilingual ASR system with language conditioning and adapters; it does not involve discrete audio tokens or tokenization schemes as a core contribution, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses a large-scale multilingual speech recognition model but does not mention the use or design of discrete audio tokens, vector quantization, or related discrete representations of audio signals which are central to the inclusion criteria. It focuses on end-to-end multilingual ASR performance and language-specific adaptation without detailing any tokenization or discrete representation techniques that could be evaluated or reproduced.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses a large-scale multilingual speech recognition model but does not mention the use or design of discrete audio tokens, vector quantization, or related discrete representations of audio signals which are central to the inclusion criteria. It focuses on end-to-end multilingual ASR performance and language-specific adaptation without detailing any tokenization or discrete representation techniques that could be evaluated or reproduced.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semi-supervised training in low-resource ASR and KWS",
    "abstract": "In particular for \"low resource\" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.",
    "metadata": {
      "title": "Semi-supervised training in low-resource ASR and KWS",
      "summary": "In particular for \"low resource\" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.",
      "abstract": "In particular for \"low resource\" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178862",
      "openalex_id": "https://openalex.org/W1524956127",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "该项聚焦低资源场景下的半监督训练及测试数据上的适配/说话人自适应，未涉及离散音频 token 的生成、量化或用于序列模型的离散表征，因此不符合以离散 token 设计为核心研究贡献的纳入条件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "该项聚焦低资源场景下的半监督训练及测试数据上的适配/说话人自适应，未涉及离散音频 token 的生成、量化或用于序列模型的离散表征，因此不符合以离散 token 设计为核心研究贡献的纳入条件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semi-supervised training techniques for low-resource ASR and KWS, with experiments on various languages. However, it does not discuss the creation or use of discrete audio tokens through vector quantization or other discrete representations as defined in the inclusion criteria. The study focuses on speech recognition accuracy improvements rather than discrete token generation or modeling, thus failing to meet the main inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semi-supervised training techniques for low-resource ASR and KWS, with experiments on various languages. However, it does not discuss the creation or use of discrete audio tokens through vector quantization or other discrete representations as defined in the inclusion criteria. The study focuses on speech recognition accuracy improvements rather than discrete token generation or modeling, thus failing to meet the main inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Speech Recognition with a Single End-to-End Model",
    "abstract": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.",
    "metadata": {
      "title": "Multilingual Speech Recognition with a Single End-to-End Model",
      "summary": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.",
      "abstract": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461972",
      "openalex_id": "https://openalex.org/W2964309797",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses multilingual end-to-end ASR with grapheme sets and optional language-id input, not discrete audio tokenization or token-based representations as core contribution, so it fails inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work discusses multilingual end-to-end ASR with grapheme sets and optional language-id input, not discrete audio tokenization or token-based representations as core contribution, so it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses multilingual speech recognition using sequence-to-sequence models trained on grapheme sets without involving discrete audio token generation or their quantization processes, and it lacks details on token formation, vector quantization, or semantic disentanglement required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses multilingual speech recognition using sequence-to-sequence models trained on grapheme sets without involving discrete audio token generation or their quantization processes, and it lacks details on token formation, vector quantization, or semantic disentanglement required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling",
    "abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
    "metadata": {
      "title": "Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling",
      "summary": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
      "abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
      "doi": "https://doi.org/10.1109/slt.2018.8639655",
      "openalex_id": "https://openalex.org/W2894835365",
      "arxiv_id": "",
      "publication_date": "2018-12-01",
      "published": "2018-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper addresses multilingual seq2seq ASR and language modeling with transfer learning, it does not discuss discrete audio token representations (e.g., VQ/RVQ, vector quantization, or token-based audio-language modeling), so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper addresses multilingual seq2seq ASR and language modeling with transfer learning, it does not discuss discrete audio token representations (e.g., VQ/RVQ, vector quantization, or token-based audio-language modeling), so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on multilingual seq2seq speech recognition using transfer learning and language modeling for ASR systems, but it does not mention the use or design of discrete audio tokens (such as vector quantization or semantic-driven discretization) as a core contribution or method. The paper emphasizes seq2seq modeling improvements rather than discrete token generation or usage for downstream sequence modeling, thus it does not meet the inclusion criteria centered on discrete audio token design and use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on multilingual seq2seq speech recognition using transfer learning and language modeling for ASR systems, but it does not mention the use or design of discrete audio tokens (such as vector quantization or semantic-driven discretization) as a core contribution or method. The paper emphasizes seq2seq modeling improvements rather than discrete token generation or usage for downstream sequence modeling, thus it does not meet the inclusion criteria centered on discrete audio token design and use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Cross-lingual Representation Learning at Scale",
    "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
    "metadata": {
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "summary": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
      "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.747",
      "openalex_id": "https://openalex.org/W2983040767",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on multilingual text representation learning (XLM-R) and cross-lingual NLP tasks, not discrete audio token representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on multilingual text representation learning (XLM-R) and cross-lingual NLP tasks, not discrete audio token representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised cross-lingual representation learning for text data across multiple languages, using Transformer-based masked language models. It does not discuss discrete audio token generation, vector quantization of audio signals, or any audio-related tokenization methods, which are mandatory inclusion criteria. The study does not mention any audio or speech signal processing, token quantization processes for audio, or related downstream audio generation or understanding tasks, and thus does not meet the inclusion criteria. Therefore, despite being a strong NLP paper, it is out of scope for the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised cross-lingual representation learning for text data across multiple languages, using Transformer-based masked language models. It does not discuss discrete audio token generation, vector quantization of audio signals, or any audio-related tokenization methods, which are mandatory inclusion criteria. The study does not mention any audio or speech signal processing, token quantization processes for audio, or related downstream audio generation or understanding tasks, and thus does not meet the inclusion criteria. Therefore, despite being a strong NLP paper, it is out of scope for the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual acoustic modeling for speech recognition based on subspace Gaussian Mixture Models",
    "abstract": "Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.",
    "metadata": {
      "title": "Multilingual acoustic modeling for speech recognition based on subspace Gaussian Mixture Models",
      "summary": "Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.",
      "abstract": "Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.",
      "doi": "https://doi.org/10.1109/icassp.2010.5495646",
      "openalex_id": "https://openalex.org/W2123798005",
      "arxiv_id": "",
      "publication_date": "2010-03-01",
      "published": "2010-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on multilingual acoustic modeling with Subspace Gaussian Mixture Models and does not involve discrete audio tokens or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multilingual acoustic modeling with Subspace Gaussian Mixture Models and does not involve discrete audio tokens or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multilingual acoustic modeling using Subspace Gaussian Mixture Models for speech recognition, without addressing the creation or use of discrete audio tokens formed by vector quantization or similar discrete representation methods that are crucial for language or sequence model processing as defined in the inclusion criteria; therefore, it does not meet the core requirement of discrete token design or usage as a primary contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multilingual acoustic modeling using Subspace Gaussian Mixture Models for speech recognition, without addressing the creation or use of discrete audio tokens formed by vector quantization or similar discrete representation methods that are crucial for language or sequence model processing as defined in the inclusion criteria; therefore, it does not meet the core requirement of discrete token design or usage as a primary contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion",
    "abstract": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
    "metadata": {
      "title": "Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion",
      "summary": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
      "abstract": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682918",
      "openalex_id": "https://openalex.org/W2963027641",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on language-independent end-to-end ASR with external LM fusion for adaptation; it does not discuss discrete audio tokens or tokenization as a core representation, so it does not meet the inclusion criteria that require discrete token-based audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on language-independent end-to-end ASR with external LM fusion for adaptation; it does not discuss discrete audio tokens or tokenization as a core representation, so it does not meet the inclusion criteria that require discrete token-based audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on transfer learning for an end-to-end ASR system using language model fusion, which deals with speech recognition adaptation but does not mention the use or generation of discrete audio tokens or vector quantization methods for tokenizing continuous audio into discrete representations; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on transfer learning for an end-to-end ASR system using language model fusion, which deals with speech recognition adaptation but does not mention the use or generation of discrete audio tokens or vector quantization methods for tokenizing continuous audio into discrete representations; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS",
    "abstract": "The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",
    "metadata": {
      "title": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS",
      "summary": "The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",
      "abstract": "The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",
      "doi": "https://doi.org/10.1111/j.1469-1809.1936.tb02137.x",
      "openalex_id": "https://openalex.org/W2001619934",
      "arxiv_id": "",
      "publication_date": "1936-09-01",
      "published": "1936-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses historical eugenics archives and lacks any content on Discrete Audio Tokens or the specified methodological criteria, so it should be excluded based on the inclusion/exclusion rules.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses historical eugenics archives and lacks any content on Discrete Audio Tokens or the specified methodological criteria, so it should be excluded based on the inclusion/exclusion rules.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract do not discuss discrete audio tokens, their generation, or related methods; instead, the abstract describes an online archive of historical articles unrelated to audio tokenization technology or methodology, hence it does not meet any inclusion criteria and fails key content relevance requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract do not discuss discrete audio tokens, their generation, or related methods; instead, the abstract describes an online archive of historical articles unrelated to audio tokenization technology or methodology, hence it does not meet any inclusion criteria and fails key content relevance requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Maximum likelihood modeling with Gaussian distributions for classification",
    "abstract": "Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.",
    "metadata": {
      "title": "Maximum likelihood modeling with Gaussian distributions for classification",
      "summary": "Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.",
      "abstract": "Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.",
      "doi": "https://doi.org/10.1109/icassp.1998.675351",
      "openalex_id": "https://openalex.org/W2124629003",
      "arxiv_id": "",
      "publication_date": "2002-11-27",
      "published": "2002-11-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses ML with Gaussian distributions for classification and applications to speech recognition, but it does not address discrete audio tokens or token-based representations; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses ML with Gaussian distributions for classification and applications to speech recognition, but it does not address discrete audio tokens or token-based representations; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on maximum likelihood modeling with Gaussian distributions for classification, discussing parameter sharing and applications to speech recognition. However, it does not describe the discrete audio token generation process, the use of vector quantization or related techniques, nor the use of discrete tokens as core representations for audio language modeling or downstream audio understanding/generation tasks. Therefore, it does not meet the core inclusion criteria regarding discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on maximum likelihood modeling with Gaussian distributions for classification, discussing parameter sharing and applications to speech recognition. However, it does not describe the discrete audio token generation process, the use of vector quantization or related techniques, nor the use of discrete tokens as core representations for audio language modeling or downstream audio understanding/generation tasks. Therefore, it does not meet the core inclusion criteria regarding discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semi-tied covariance matrices for hidden Markov models",
    "abstract": "There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few \"full\" covariance matrices to be shared over many distributions, whilst each distribution maintains its own \"diagonal\" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.",
    "metadata": {
      "title": "Semi-tied covariance matrices for hidden Markov models",
      "summary": "There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few \"full\" covariance matrices to be shared over many distributions, whilst each distribution maintains its own \"diagonal\" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.",
      "abstract": "There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few \"full\" covariance matrices to be shared over many distributions, whilst each distribution maintains its own \"diagonal\" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.",
      "doi": "https://doi.org/10.1109/89.759034",
      "openalex_id": "https://openalex.org/W2106554350",
      "arxiv_id": "",
      "publication_date": "1999-05-01",
      "published": "1999-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on semi-tied covariance matrices for hidden Markov models, not on discrete audio token representations or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on semi-tied covariance matrices for hidden Markov models, not on discrete audio token representations or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a novel covariance matrix approach for hidden Markov models in speech recognition, focusing on parameter sharing and recognition performance. However, it does not mention the generation or use of discrete audio tokens, vector quantization, or any form of discrete token representation as defined in the inclusion criteria. The work centers on improving statistical model parameters rather than discrete token-based audio representations or their applications in language modeling or audio generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a novel covariance matrix approach for hidden Markov models in speech recognition, focusing on parameter sharing and recognition performance. However, it does not mention the generation or use of discrete audio tokens, vector quantization, or any form of discrete token representation as defined in the inclusion criteria. The work centers on improving statistical model parameters rather than discrete token-based audio representations or their applications in language modeling or audio generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A compact model for speaker-adaptive training",
    "abstract": "We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.",
    "metadata": {
      "title": "A compact model for speaker-adaptive training",
      "summary": "We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.",
      "abstract": "We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.",
      "doi": "https://doi.org/10.1109/icslp.1996.607807",
      "openalex_id": "https://openalex.org/W1599512239",
      "arxiv_id": "",
      "publication_date": "2002-12-24",
      "published": "2002-12-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on estimating HMM parameters for speaker-independent ASR and speaker adaptation, not on discrete audio tokenization or token-based representations, so it does not meet the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on estimating HMM parameters for speaker-independent ASR and speaker adaptation, not on discrete audio tokenization or token-based representations, so it does not meet the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speaker-adaptive training for continuous density HMMs in speech recognition and does not describe any method involving the generation or use of discrete audio tokens through vector quantization or similar discrete representation methods intended for sequence or language model processing as defined in the inclusion criteria. It lacks description of token generation processes, use of discrete tokens for audio language modeling, or related quantization mechanisms. Therefore, it does not meet the thematic inclusion criteria for studies about discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speaker-adaptive training for continuous density HMMs in speech recognition and does not describe any method involving the generation or use of discrete audio tokens through vector quantization or similar discrete representation methods intended for sequence or language model processing as defined in the inclusion criteria. It lacks description of token generation processes, use of discrete tokens for audio language modeling, or related quantization mechanisms. Therefore, it does not meet the thematic inclusion criteria for studies about discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Iterative training of a DPGMM-HMM acoustic unit recognizer in a zero resource scenario",
    "abstract": "In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.",
    "metadata": {
      "title": "Iterative training of a DPGMM-HMM acoustic unit recognizer in a zero resource scenario",
      "summary": "In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.",
      "abstract": "In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.",
      "doi": "https://doi.org/10.1109/slt.2016.7846245",
      "openalex_id": "https://openalex.org/W2586754519",
      "arxiv_id": "",
      "publication_date": "2016-12-01",
      "published": "2016-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes clustering acoustic features into a dynamically sized set of discrete acoustic units (tokens) via DPGMM and using them as transcriptions for acoustic model and language model training, which aligns with discrete audio token research; it presents a tokenization/quantization-like approach and an evaluation pipeline, and is not evidently just a demo, so it meets the inclusion criteria in spirit though abstract-level detail may be limited.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper describes clustering acoustic features into a dynamically sized set of discrete acoustic units (tokens) via DPGMM and using them as transcriptions for acoustic model and language model training, which aligns with discrete audio token research; it presents a tokenization/quantization-like approach and an evaluation pipeline, and is not evidently just a demo, so it meets the inclusion criteria in spirit though abstract-level detail may be limited.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an unsupervised acoustic unit recognizer that iteratively generates cluster labels from continuous audio features, which serve as discrete acoustic units used to train models including language models, aligning with the concept of deriving discrete audio tokens for sequence modeling. However, the paper does not explicitly clarify the token generation process as vector quantization or similar techniques, nor is there detailed methodological transparency on the token formation and their semantic properties as required by the inclusion criteria. While it fits the theme of discrete acoustic units and usage in language modeling, the lack of explicit tokenization details and quantization mechanisms leaves some ambiguity about full compliance with inclusion requirements.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes an unsupervised acoustic unit recognizer that iteratively generates cluster labels from continuous audio features, which serve as discrete acoustic units used to train models including language models, aligning with the concept of deriving discrete audio tokens for sequence modeling. However, the paper does not explicitly clarify the token generation process as vector quantization or similar techniques, nor is there detailed methodological transparency on the token formation and their semantic properties as required by the inclusion criteria. While it fits the theme of discrete acoustic units and usage in language modeling, the lack of explicit tokenization details and quantization mechanisms leaves some ambiguity about full compliance with inclusion requirements.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers observed this paper proposes unsupervised clustering into discrete acoustic units (DPGMM) which are explicitly used as frame-level labels/tokens to train acoustic and n-gram language models—this aligns with the ‘‘Discrete Audio Tokens’’ topic (generation of discrete units via clustering/quantization and use in sequence modeling) and it is not a standard codec/demo; although the abstract is brief and lacks full reproduction details, the core contribution is the design and iterative training of discrete acoustic units for LM use, so it should be included for full-text review.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers observed this paper proposes unsupervised clustering into discrete acoustic units (DPGMM) which are explicitly used as frame-level labels/tokens to train acoustic and n-gram language models—this aligns with the ‘‘Discrete Audio Tokens’’ topic (generation of discrete units via clustering/quantization and use in sequence modeling) and it is not a standard codec/demo; although the abstract is brief and lacks full reproduction details, the core contribution is the design and iterative training of discrete acoustic units for LM use, so it should be included for full-text review.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression",
    "abstract": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression",
      "summary": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/18.87000",
      "openalex_id": "https://openalex.org/W2113641473",
      "arxiv_id": "",
      "publication_date": "1991-07-01",
      "published": "1991-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract pertain to zero-frequency problems in adaptive text compression, not to discrete audio tokens or audio tokenization methods; it lacks any audio-domain discrete token design, quantization, or audio-language modeling content, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract pertain to zero-frequency problems in adaptive text compression, not to discrete audio tokens or audio tokenization methods; it lacks any audio-domain discrete token design, quantization, or audio-language modeling content, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses the zero-frequency problem in adaptive text compression and proposes a Poisson process model for predicting novel tokens, but it does not involve discrete audio tokens or methods related to audio signal quantization and tokenization as specified in the inclusion criteria. Therefore, it does not meet the main thematic requirement of discrete audio token representation or token design in audio modeling contexts.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study addresses the zero-frequency problem in adaptive text compression and proposes a Poisson process model for predicting novel tokens, but it does not involve discrete audio tokens or methods related to audio signal quantization and tokenization as specified in the inclusion criteria. Therefore, it does not meet the main thematic requirement of discrete audio token representation or token design in audio modeling contexts.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Parallel Sampling of DP Mixture Models using Sub-Cluster Splits",
    "abstract": "We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
    "metadata": {
      "title": "Parallel Sampling of DP Mixture Models using Sub-Cluster Splits",
      "summary": "We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
      "abstract": "We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2128032727",
      "arxiv_id": "",
      "publication_date": "2013-12-05",
      "published": "2013-12-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a parallel MCMC sampler for Dirichlet process mixture models, with no reference to discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria and fails the criteria for inclusion while not triggering any exclusion notes beyond irrelevance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a parallel MCMC sampler for Dirichlet process mixture models, with no reference to discrete audio tokens or audio-language modeling; it does not meet the inclusion criteria and fails the criteria for inclusion while not triggering any exclusion notes beyond irrelevance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents an MCMC sampler for Dirichlet process mixture models focusing on parallelization and convergence improvements, but it does not discuss discrete audio tokens, vector quantization, or any audio-related tokenization methods relevant to audio language modeling or discrete audio token generation as specified; hence it doesn't meet the inclusion criteria nor relate to the subject of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents an MCMC sampler for Dirichlet process mixture models focusing on parallelization and convergence improvements, but it does not discuss discrete audio tokens, vector quantization, or any audio-related tokenization methods relevant to audio language modeling or discrete audio token generation as specified; hence it doesn't meet the inclusion criteria nor relate to the subject of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario",
    "abstract": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
    "metadata": {
      "title": "Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario",
      "summary": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "abstract": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.032",
      "openalex_id": "https://openalex.org/W2345811097",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses unsupervised LDA for acoustic unit discovery and clustering in zero-resource settings, using a first-iteration DPGMM and feature transformation, but it does not describe or evaluate discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization, or token-based modeling). Therefore it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses unsupervised LDA for acoustic unit discovery and clustering in zero-resource settings, using a first-iteration DPGMM and feature transformation, but it does not describe or evaluate discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization, or token-based modeling). Therefore it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using unsupervised linear discriminant analysis to improve clustering of acoustic units in zero resource scenarios but does not clearly describe the generation of discrete audio tokens through vector quantization, residual quantization, hierarchical quantization, or semantic-driven discretization methods. The work primarily deals with continuous feature transformation and clustering rather than producing discrete token representations intended for sequence or language modeling. Therefore, it does not meet the core inclusion criteria related to developing or using discrete audio token representations as the main contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using unsupervised linear discriminant analysis to improve clustering of acoustic units in zero resource scenarios but does not clearly describe the generation of discrete audio tokens through vector quantization, residual quantization, hierarchical quantization, or semantic-driven discretization methods. The work primarily deals with continuous feature transformation and clustering rather than producing discrete token representations intended for sequence or language modeling. Therefore, it does not meet the core inclusion criteria related to developing or using discrete audio token representations as the main contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Audio Embeddings by Adjacency-Based Clustering with Applications in Spoken Term Detection",
    "abstract": "Embedding audio signal segments into vectors with fixed dimensionality is attractive because all following processing will be easier and more efficient, for example modeling, classifying or indexing. Audio Word2Vec previously proposed was shown to be able to represent audio segments for spoken words as such vectors carrying information about the phonetic structures of the signal segments. However, each linguistic unit (word, syllable, phoneme in text form) corresponds to unlimited number of audio segments with vector representations inevitably spread over the embedding space, which causes some confusion. It is therefore desired to better cluster the audio embeddings such that those corresponding to the same linguistic unit can be more compactly distributed. In this paper, inspired by Siamese networks, we propose some approaches to achieve the above goal. This includes identifying positive and negative pairs from unlabeled data for Siamese style training, disentangling acoustic factors such as speaker characteristics from the audio embedding, handling unbalanced data distribution, and having the embedding processes learn from the adjacency relationships among data points. All these can be done in an unsupervised way. Improved performance was obtained in preliminary experiments on the LibriSpeech data set, including clustering characteristics analysis and applications of spoken term detection.",
    "metadata": {
      "title": "Improved Audio Embeddings by Adjacency-Based Clustering with Applications in Spoken Term Detection",
      "summary": "Embedding audio signal segments into vectors with fixed dimensionality is attractive because all following processing will be easier and more efficient, for example modeling, classifying or indexing. Audio Word2Vec previously proposed was shown to be able to represent audio segments for spoken words as such vectors carrying information about the phonetic structures of the signal segments. However, each linguistic unit (word, syllable, phoneme in text form) corresponds to unlimited number of audio segments with vector representations inevitably spread over the embedding space, which causes some confusion. It is therefore desired to better cluster the audio embeddings such that those corresponding to the same linguistic unit can be more compactly distributed. In this paper, inspired by Siamese networks, we propose some approaches to achieve the above goal. This includes identifying positive and negative pairs from unlabeled data for Siamese style training, disentangling acoustic factors such as speaker characteristics from the audio embedding, handling unbalanced data distribution, and having the embedding processes learn from the adjacency relationships among data points. All these can be done in an unsupervised way. Improved performance was obtained in preliminary experiments on the LibriSpeech data set, including clustering characteristics analysis and applications of spoken term detection.",
      "abstract": "Embedding audio signal segments into vectors with fixed dimensionality is attractive because all following processing will be easier and more efficient, for example modeling, classifying or indexing. Audio Word2Vec previously proposed was shown to be able to represent audio segments for spoken words as such vectors carrying information about the phonetic structures of the signal segments. However, each linguistic unit (word, syllable, phoneme in text form) corresponds to unlimited number of audio segments with vector representations inevitably spread over the embedding space, which causes some confusion. It is therefore desired to better cluster the audio embeddings such that those corresponding to the same linguistic unit can be more compactly distributed. In this paper, inspired by Siamese networks, we propose some approaches to achieve the above goal. This includes identifying positive and negative pairs from unlabeled data for Siamese style training, disentangling acoustic factors such as speaker characteristics from the audio embedding, handling unbalanced data distribution, and having the embedding processes learn from the adjacency relationships among data points. All these can be done in an unsupervised way. Improved performance was obtained in preliminary experiments on the LibriSpeech data set, including clustering characteristics analysis and applications of spoken term detection.",
      "doi": "https://doi.org/10.48550/arxiv.1811.02775",
      "openalex_id": "https://openalex.org/W2899518769",
      "arxiv_id": "",
      "publication_date": "2018-11-07",
      "published": "2018-11-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on unsupervised audio embeddings and adjacency-based clustering for spoken term detection, without explicit discrete token design or token generation/quantization mechanisms, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper centers on unsupervised audio embeddings and adjacency-based clustering for spoken term detection, without explicit discrete token design or token generation/quantization mechanisms, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving audio embeddings through clustering and unsupervised learning techniques applied to spoken term detection; however, it does not explicitly address the generation of discrete audio tokens via vector quantization or similar discrete tokenization methods for use in sequence or language models, which is central to the inclusion criteria. The paper lacks clear description of token generation processes and how these embeddings serve as discrete tokens integrating semantic/acoustic disentanglement as required by the criteria. Therefore, it does not meet the inclusion criteria focused on discrete audio token design for audio language modeling or sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving audio embeddings through clustering and unsupervised learning techniques applied to spoken term detection; however, it does not explicitly address the generation of discrete audio tokens via vector quantization or similar discrete tokenization methods for use in sequence or language models, which is central to the inclusion criteria. The paper lacks clear description of token generation processes and how these embeddings serve as discrete tokens integrating semantic/acoustic disentanglement as required by the criteria. Therefore, it does not meet the inclusion criteria focused on discrete audio token design for audio language modeling or sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Decoupled Learning for Conditional Adversarial Networks",
    "abstract": "Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network. This paper gives the first attempt to relax the need of manual balancing by proposing the concept of decoupled learning, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses. In existing works, the encoding-decoding nets and GANs are integrated by sharing weights on the generator/decoder, thus the two losses are backpropagated to the generator/decoder simultaneously, where a weighting factor is needed to balance the interaction between the two losses. The decoupled learning avoids the interaction and thus removes the requirement of the weighting factor, essentially improving the generalization capacity of the designed model to different applications. The decoupled learning framework could be easily adapted to most existing encoding-decoding-based generative networks and achieve competitive performance without the need of weight adjustment. Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called normalized relative discriminative score (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.",
    "metadata": {
      "title": "Decoupled Learning for Conditional Adversarial Networks",
      "summary": "Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network. This paper gives the first attempt to relax the need of manual balancing by proposing the concept of decoupled learning, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses. In existing works, the encoding-decoding nets and GANs are integrated by sharing weights on the generator/decoder, thus the two losses are backpropagated to the generator/decoder simultaneously, where a weighting factor is needed to balance the interaction between the two losses. The decoupled learning avoids the interaction and thus removes the requirement of the weighting factor, essentially improving the generalization capacity of the designed model to different applications. The decoupled learning framework could be easily adapted to most existing encoding-decoding-based generative networks and achieve competitive performance without the need of weight adjustment. Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called normalized relative discriminative score (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.",
      "abstract": "Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network. This paper gives the first attempt to relax the need of manual balancing by proposing the concept of decoupled learning, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses. In existing works, the encoding-decoding nets and GANs are integrated by sharing weights on the generator/decoder, thus the two losses are backpropagated to the generator/decoder simultaneously, where a weighting factor is needed to balance the interaction between the two losses. The decoupled learning avoids the interaction and thus removes the requirement of the weighting factor, essentially improving the generalization capacity of the designed model to different applications. The decoupled learning framework could be easily adapted to most existing encoding-decoding-based generative networks and achieve competitive performance without the need of weight adjustment. Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called normalized relative discriminative score (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.",
      "doi": "https://doi.org/10.1109/wacv.2018.00082",
      "openalex_id": "https://openalex.org/W2962974898",
      "arxiv_id": "",
      "publication_date": "2018-03-01",
      "published": "2018-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses decoupled learning for conditional adversarial networks in image generation and proposes a new evaluation metric; it does not address discrete audio tokens or methods for converting audio to discrete tokens, so it does not meet the inclusion criteria for the Discrete Audio Tokens topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses decoupled learning for conditional adversarial networks in image generation and proposes a new evaluation metric; it does not address discrete audio tokens or methods for converting audio to discrete tokens, so it does not meet the inclusion criteria for the Discrete Audio Tokens topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a novel learning framework for conditional adversarial networks applied mainly in image generation tasks, without any indication of work related to discrete audio tokens, audio signal processing, or language modeling of audio. The content does not describe generating or using discrete audio tokens through vector quantization or similar methods, nor does it involve semantic or acoustic disentanglement of audio signals. Therefore, the study does not meet the inclusion criteria focused on discrete audio token research and instead falls outside the specified theme entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a novel learning framework for conditional adversarial networks applied mainly in image generation tasks, without any indication of work related to discrete audio tokens, audio signal processing, or language modeling of audio. The content does not describe generating or using discrete audio tokens through vector quantization or similar methods, nor does it involve semantic or acoustic disentanglement of audio signals. Therefore, the study does not meet the inclusion criteria focused on discrete audio token research and instead falls outside the specified theme entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Conditional Image Synthesis With Auxiliary Classifier GANs",
    "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.",
    "metadata": {
      "title": "Conditional Image Synthesis With Auxiliary Classifier GANs",
      "summary": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.",
      "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.",
      "doi": "https://doi.org/10.48550/arxiv.1610.09585",
      "openalex_id": "https://openalex.org/W2548275288",
      "arxiv_id": "",
      "publication_date": "2016-10-30",
      "published": "2016-10-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on conditional image synthesis with GANs and image-level results, not on discrete audio tokens or related audio-language modeling; thus it does not meet the target topic criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on conditional image synthesis with GANs and image-level results, not on discrete audio tokens or related audio-language modeling; thus it does not meet the target topic criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The provided title and abstract focus on image synthesis using generative adversarial networks (GANs) to generate high-resolution photorealistic images, with no mention or indication of discrete audio tokens, vector quantization of audio, or any related audio processing or language modeling of audio sequences. Therefore, it does not meet the inclusion criteria which require discrete audio token design or usage as a core research contribution related to audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The provided title and abstract focus on image synthesis using generative adversarial networks (GANs) to generate high-resolution photorealistic images, with no mention or indication of discrete audio tokens, vector quantization of audio, or any related audio processing or language modeling of audio sequences. Therefore, it does not meet the inclusion criteria which require discrete audio token design or usage as a core research contribution related to audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "<i>Tabula</i>Nearly<i>Rasa:</i>Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text",
    "abstract": "Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our “near tabula rasa” RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.",
    "metadata": {
      "title": "<i>Tabula</i>Nearly<i>Rasa:</i>Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text",
      "summary": "Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our “near tabula rasa” RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.",
      "abstract": "Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our “near tabula rasa” RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.",
      "doi": "https://doi.org/10.1162/tacl_a_00283",
      "openalex_id": "https://openalex.org/W2972447203",
      "arxiv_id": "",
      "publication_date": "2019-09-11",
      "published": "2019-09-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on character-level language models trained on unsegmented text and tests linguistic knowledge, not on discrete audio tokens or audio tokenization methods for speech/music; it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on character-level language models trained on unsegmented text and tests linguistic knowledge, not on discrete audio tokens or audio tokenization methods for speech/music; it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on linguistic knowledge in character-level neural language models trained on unsegmented text, dealing exclusively with textual data and tokenization in natural language processing. It does not relate to discrete audio tokens, audio representation, or audio language modeling, which are core to the inclusion criteria. The study does not discuss any form of audio signal processing, vector quantization, or discrete audio token generation methods, thus failing to meet the inclusion criteria and instead falling outside the domain of interest.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on linguistic knowledge in character-level neural language models trained on unsegmented text, dealing exclusively with textual data and tokenization in natural language processing. It does not relate to discrete audio tokens, audio representation, or audio language modeling, which are core to the inclusion criteria. The study does not discuss any form of audio signal processing, vector quantization, or discrete audio token generation methods, thus failing to meet the inclusion criteria and instead falling outside the domain of interest.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluation methods for unsupervised word embeddings",
    "abstract": "We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.We present new evaluation techniques that directly compare embeddings with respect to specific queries.These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.",
    "metadata": {
      "title": "Evaluation methods for unsupervised word embeddings",
      "summary": "We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.We present new evaluation techniques that directly compare embeddings with respect to specific queries.These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.",
      "abstract": "We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.We present new evaluation techniques that directly compare embeddings with respect to specific queries.These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.",
      "doi": "https://doi.org/10.18653/v1/d15-1036",
      "openalex_id": "https://openalex.org/W2252211741",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about text-based word embeddings and evaluation methods, not about discrete audio tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about text-based word embeddings and evaluation methods, not about discrete audio tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluation methods for unsupervised word embeddings derived from text, without any mention or indication of discrete audio tokens, vector quantization, or related audio processing techniques described in the inclusion criteria. It does not involve any audio-based discrete token generation, nor does it target audio language modeling, cross-modal generation, or speech/audio understanding tasks. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria for lacking relevant methodology and domain focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluation methods for unsupervised word embeddings derived from text, without any mention or indication of discrete audio tokens, vector quantization, or related audio processing techniques described in the inclusion criteria. It does not involve any audio-based discrete token generation, nor does it target audio language modeling, cross-modal generation, or speech/audio understanding tasks. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria for lacking relevant methodology and domain focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
    "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
    "metadata": {
      "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
      "summary": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
      "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
      "doi": "https://doi.org/10.1162/tacl_a_00115",
      "openalex_id": "https://openalex.org/W2549835527",
      "arxiv_id": "",
      "publication_date": "2016-12-01",
      "published": "2016-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on LSTMs learning syntax-sensitive dependencies in English (textual language modeling), not on discrete audio tokens or audio tokenization methods; it does not meet the inclusion criteria centered on discrete audio token representations and related methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on LSTMs learning syntax-sensitive dependencies in English (textual language modeling), not on discrete audio tokens or audio tokenization methods; it does not meet the inclusion criteria centered on discrete audio token representations and related methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on assessing LSTM neural networks' ability to learn syntactic dependencies in language, specifically English subject-verb agreement, without any mention of discrete audio token generation, vector quantization, or audio-related sequence modeling. It does not involve methods or contributions related to discrete audio representations or tokenization of continuous audio signals, which is central to the inclusion criteria. Therefore, it does not meet the necessary technical coverage or thematic relevance to discrete audio tokens or their use in audio language modeling or cross-modal generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on assessing LSTM neural networks' ability to learn syntactic dependencies in language, specifically English subject-verb agreement, without any mention of discrete audio token generation, vector quantization, or audio-related sequence modeling. It does not involve methods or contributions related to discrete audio representations or tokenization of continuous audio signals, which is central to the inclusion criteria. Therefore, it does not meet the necessary technical coverage or thematic relevance to discrete audio tokens or their use in audio language modeling or cross-modal generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Can LSTM Learn to Capture Agreement? The Case of Basque",
    "abstract": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system. Analyzing experimental results from two syntactic prediction tasks – verb number prediction and suffix recovery – we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English. Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence. We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.",
    "metadata": {
      "title": "Can LSTM Learn to Capture Agreement? The Case of Basque",
      "summary": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system. Analyzing experimental results from two syntactic prediction tasks – verb number prediction and suffix recovery – we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English. Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence. We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.",
      "abstract": "Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system. Analyzing experimental results from two syntactic prediction tasks – verb number prediction and suffix recovery – we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English. Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence. We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.",
      "doi": "https://doi.org/10.18653/v1/w18-5412",
      "openalex_id": "https://openalex.org/W2889947987",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article examines LSTM-based language modeling and agreement in Basque, focusing on hierarchical structure and morphology, but it does not address discrete audio tokens or any tokenization/quantization methods for audio models, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article examines LSTM-based language modeling and agreement in Basque, focusing on hierarchical structure and morphology, but it does not address discrete audio tokens or any tokenization/quantization methods for audio models, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sequential neural network models for agreement prediction in Basque language, investigating syntactic and morphological understanding, which is unrelated to the generation or use of discrete audio tokens or their design, production, or application in audio language modeling or downstream audio tasks, violating the inclusion criteria and aligning with exclusion criteria related to irrelevant topic focus and lack of discrete audio token methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sequential neural network models for agreement prediction in Basque language, investigating syntactic and morphological understanding, which is unrelated to the generation or use of discrete audio tokens or their design, production, or application in audio language modeling or downstream audio tasks, violating the inclusion criteria and aligning with exclusion criteria related to irrelevant topic focus and lack of discrete audio token methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Random sampling with a reservoir",
    "abstract": "We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.",
    "metadata": {
      "title": "Random sampling with a reservoir",
      "summary": "We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.",
      "abstract": "We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.",
      "doi": "https://doi.org/10.1145/3147.3165",
      "openalex_id": "https://openalex.org/W2119885577",
      "arxiv_id": "",
      "publication_date": "1985-03-01",
      "published": "1985-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss reservoir sampling algorithms without any focus on discrete audio token design or token-based audio modeling; it fails all inclusion criteria related to discrete audio tokens (VQ, RVQ, SSL-derived discrete representations) and has no relevant methodology for token production or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss reservoir sampling algorithms without any focus on discrete audio token design or token-based audio modeling; it fails all inclusion criteria related to discrete audio tokens (VQ, RVQ, SSL-derived discrete representations) and has no relevant methodology for token production or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an algorithm for random sampling with a reservoir but do not mention any aspect of discrete audio tokens, vector quantization, or audio language modeling, nor do they involve the generation or use of discrete audio representations as defined in the criteria. Therefore, the study does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an algorithm for random sampling with a reservoir but do not mention any aspect of discrete audio tokens, vector quantization, or audio language modeling, nor do they involve the generation or use of discrete audio representations as defined in the criteria. Therefore, the study does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The language-independent bottleneck features",
    "abstract": "In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.",
    "metadata": {
      "title": "The language-independent bottleneck features",
      "summary": "In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.",
      "abstract": "In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.",
      "doi": "https://doi.org/10.1109/slt.2012.6424246",
      "openalex_id": "https://openalex.org/W1970890968",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on language-independent bottleneck features using a multilingual ANN, with no mention of discrete audio tokens, tokenization, or quantization methods as required by the inclusion criteria for Discrete Audio Tokens; thus it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on language-independent bottleneck features using a multilingual ANN, with no mention of discrete audio tokens, tokenization, or quantization methods as required by the inclusion criteria for Discrete Audio Tokens; thus it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a language-independent bottleneck feature extraction method using a multilingual ANN but does not involve discrete audio tokenization methods such as vector quantization or semantic-driven discretization of continuous audio signals into finite token sets for language or sequence modeling; hence it does not meet the core inclusion criteria for discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a language-independent bottleneck feature extraction method using a multilingual ANN but does not involve discrete audio tokenization methods such as vector quantization or semantic-driven discretization of continuous audio signals into finite token sets for language or sequence modeling; hence it does not meet the core inclusion criteria for discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Acceleration of Stochastic Approximation by Averaging",
    "abstract": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
    "metadata": {
      "title": "Acceleration of Stochastic Approximation by Averaging",
      "summary": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
      "abstract": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.",
      "doi": "https://doi.org/10.1137/0330046",
      "openalex_id": "https://openalex.org/W2086161653",
      "arxiv_id": "",
      "publication_date": "1992-07-01",
      "published": "1992-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study concerns stochastic approximation with averaging, not discrete audio token design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study concerns stochastic approximation with averaging, not discrete audio token design, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss a stochastic approximation algorithm focused on convergence and optimization problems without any mention of discrete audio tokens, vector quantization, or audio language modeling, which are the primary inclusion criteria; there is no indication that the study involves audio token generation or usage in sequence models for audio processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss a stochastic approximation algorithm focused on convergence and optimization problems without any mention of discrete audio tokens, vector quantization, or audio language modeling, which are the primary inclusion criteria; there is no indication that the study involves audio token generation or usage in sequence models for audio processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
    "abstract": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
    "metadata": {
      "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models",
      "summary": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
      "abstract": "Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",
      "doi": "https://doi.org/10.48550/arxiv.1703.07370",
      "openalex_id": "https://openalex.org/W2602076750",
      "arxiv_id": "",
      "publication_date": "2017-03-21",
      "published": "2017-03-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes unbiased gradient estimates for discrete latent variable models and does not address discrete audio tokens or tokenization methods for audio; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes unbiased gradient estimates for discrete latent variable models and does not address discrete audio tokens or tokenization methods for audio; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a method for low-variance, unbiased gradient estimates for discrete latent variable models without any mention of discrete audio tokens, audio signal processing, vector quantization, or audio language modeling. It does not describe a discrete token generation process, nor target audio applications or audio representations. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens as defined and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a method for low-variance, unbiased gradient estimates for discrete latent variable models without any mention of discrete audio tokens, audio signal processing, vector quantization, or audio language modeling. It does not describe a discrete token generation process, nor target audio applications or audio representations. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens as defined and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Estimating or Propagating Gradients Through Stochastic Neurons",
    "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",
    "metadata": {
      "title": "Estimating or Propagating Gradients Through Stochastic Neurons",
      "summary": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",
      "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.",
      "doi": "https://doi.org/10.48550/arxiv.1305.2982",
      "openalex_id": "https://openalex.org/W1583776211",
      "arxiv_id": "",
      "publication_date": "2013-05-14",
      "published": "2013-05-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses gradient estimators for stochastic neurons, not discrete audio tokenization or token-based audio representations, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses gradient estimators for stochastic neurons, not discrete audio tokenization or token-based audio representations, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gradient estimation through stochastic neurons in deep learning and does not address the generation or design of discrete audio tokens, nor does it discuss audio representation, vector quantization, or related audio sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gradient estimation through stochastic neurons in deep learning and does not address the generation or design of discrete audio tokens, nor does it discuss audio representation, vector quantization, or related audio sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Composing graphical models with neural networks for structured representations and fast inference",
    "abstract": "We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.",
    "metadata": {
      "title": "Composing graphical models with neural networks for structured representations and fast inference",
      "summary": "We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.",
      "abstract": "We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.",
      "doi": "https://doi.org/10.48550/arxiv.1603.06277",
      "openalex_id": "https://openalex.org/W2464234964",
      "arxiv_id": "",
      "publication_date": "2016-03-20",
      "published": "2016-03-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a general framework for composing probabilistic graphical models with deep learning, variational inference, and neural networks, with an application to mouse behavior, but it has no focus on discrete audio tokens or tokenization for audio-language modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a general framework for composing probabilistic graphical models with deep learning, variational inference, and neural networks, with an application to mouse behavior, but it has no focus on discrete audio tokens or tokenization for audio-language modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe composing graphical models with neural networks for structured representations and inference, without mention of discrete audio tokens, audio signal quantization, or token generation for audio modeling. The work is focused on probabilistic graphical models and deep learning broadly, with one example application in mouse behavioral phenotyping rather than audio data or discrete audio token design. Therefore, this item does not meet the inclusion criteria related to discrete audio token methods or applications in speech/music/general audio processing, nor does it describe a token generation process for sequence models as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe composing graphical models with neural networks for structured representations and inference, without mention of discrete audio tokens, audio signal quantization, or token generation for audio modeling. The work is focused on probabilistic graphical models and deep learning broadly, with one example application in mouse behavioral phenotyping rather than audio data or discrete audio token design. Therefore, this item does not meet the inclusion criteria related to discrete audio token methods or applications in speech/music/general audio processing, nor does it describe a token generation process for sequence models as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The “ScribbleLens” Dutch Historical Handwriting Corpus",
    "abstract": "Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.",
    "metadata": {
      "title": "The “ScribbleLens” Dutch Historical Handwriting Corpus",
      "summary": "Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.",
      "abstract": "Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.",
      "doi": "https://doi.org/10.1109/icfhr2020.2020.00023",
      "openalex_id": "https://openalex.org/W3049315473",
      "arxiv_id": "",
      "publication_date": "2020-09-01",
      "published": "2020-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets a historical handwriting corpus for OCR and does not involve discrete audio token representations or audio-language modeling, so it fails all inclusion criteria for the topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets a historical handwriting corpus for OCR and does not involve discrete audio token representations or audio-language modeling, so it fails all inclusion criteria for the topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a corpus of historical handwritten Dutch documents aimed at facilitating machine learning for manuscript recognition, without any discussion or methodology related to discrete audio tokens, vector quantization, or audio signal processing; thus, it does not match the inclusion criteria focused on discrete audio token design and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a corpus of historical handwritten Dutch documents aimed at facilitating machine learning for manuscript recognition, without any discussion or methodology related to discrete audio tokens, vector quantization, or audio signal processing; thus, it does not match the inclusion criteria focused on discrete audio token design and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
    "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
    "metadata": {
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "summary": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
      "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
      "doi": "https://doi.org/10.48550/arxiv.1512.02595",
      "openalex_id": "https://openalex.org/W2193413348",
      "arxiv_id": "",
      "publication_date": "2015-12-08",
      "published": "2015-12-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe end-to-end speech recognition with HPC speedups, not discrete audio token representations (no tokenization/quantization mechanism discussed), so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe end-to-end speech recognition with HPC speedups, not discrete audio token representations (no tokenization/quantization mechanism discussed), so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on end-to-end speech recognition using deep learning, replacing traditional pipelines with neural networks to improve efficiency and accuracy. However, it does not describe the generation or use of discrete audio tokens, vector quantization, or related mechanisms central to the inclusion criteria. The abstract lacks details on discrete token generation, quantization processes, or usage of tokens for sequence modeling, which are essential for inclusion. Therefore, it does not meet the specific thematic and methodological requirements outlined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on end-to-end speech recognition using deep learning, replacing traditional pipelines with neural networks to improve efficiency and accuracy. However, it does not describe the generation or use of discrete audio tokens, vector quantization, or related mechanisms central to the inclusion criteria. The abstract lacks details on discrete token generation, quantization processes, or usage of tokens for sequence modeling, which are essential for inclusion. Therefore, it does not meet the specific thematic and methodological requirements outlined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Theory and Experiments on Vector Quantized Autoencoders",
    "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",
    "metadata": {
      "title": "Theory and Experiments on Vector Quantized Autoencoders",
      "summary": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",
      "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",
      "doi": "https://doi.org/10.48550/arxiv.1805.11063",
      "openalex_id": "https://openalex.org/W2804145368",
      "arxiv_id": "",
      "publication_date": "2018-05-28",
      "published": "2018-05-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Vector Quantized Autoencoders (VQ-VAE) for image generation and non-autoregressive MT, not on discrete audio tokens or audio-language modeling; therefore it does not meet the inclusion criteria focused on discrete audio token representations and related methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on Vector Quantized Autoencoders (VQ-VAE) for image generation and non-autoregressive MT, not on discrete audio tokens or audio-language modeling; therefore it does not meet the inclusion criteria focused on discrete audio token representations and related methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantized autoencoders (VQ-VAE) and their training methods, primarily targeting image generation and machine translation tasks, without involving audio signals or discrete audio token generation from speech, music, or general audio; it lacks discussion on audio-specific vector quantization or token usage for sequence/language modeling in audio domains, therefore it does not meet the inclusion criteria focused strictly on discrete audio tokens and their related applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantized autoencoders (VQ-VAE) and their training methods, primarily targeting image generation and machine translation tasks, without involving audio signals or discrete audio token generation from speech, music, or general audio; it lacks discussion on audio-specific vector quantization or token usage for sequence/language modeling in audio domains, therefore it does not meet the inclusion criteria focused strictly on discrete audio tokens and their related applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The information bottleneck method",
    "abstract": "We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. This approach yields an exact set of self consistent equations for the coding rules X → ˜ X and ˜ X → Y. Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1 1",
    "metadata": {
      "title": "The information bottleneck method",
      "summary": "We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. This approach yields an exact set of self consistent equations for the coding rules X → ˜ X and ˜ X → Y. Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1 1",
      "abstract": "We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. This approach yields an exact set of self consistent equations for the coding rules X → ˜ X and ˜ X → Y. Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2979454998",
      "arxiv_id": "",
      "publication_date": "2000-04-24",
      "published": "2000-04-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses the information bottleneck method in general signal processing and learning, not discrete audio tokens or token-based representations for audio-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses the information bottleneck method in general signal processing and learning, not discrete audio tokens or token-based representations for audio-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the information bottleneck method, a theoretical framework for optimizing information representation for predicting related variables, without any mention or indication of discrete audio tokens, vector quantization, or audio-specific token generation and usage as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the information bottleneck method, a theoretical framework for optimizing information representation for predicting related variables, without any mention or indication of discrete audio tokens, vector quantization, or audio-specific token generation and usage as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Variational Information Bottleneck on Vector Quantized Autoencoders",
    "abstract": "In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.",
    "metadata": {
      "title": "Variational Information Bottleneck on Vector Quantized Autoencoders",
      "summary": "In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.",
      "abstract": "In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.",
      "doi": "https://doi.org/10.48550/arxiv.1808.01048",
      "openalex_id": "https://openalex.org/W2887927938",
      "arxiv_id": "",
      "publication_date": "2018-08-02",
      "published": "2018-08-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses Variational Information Bottleneck on Vector Quantized Autoencoders in general (VQ-VAE) and does not focus on discrete audio tokens, token generation, or audio-domain experiments. It lacks a clearly defined contribution about discrete audio representations or token-based modeling for audio, so it does not meet the inclusion criteria. It may be relevant to quantized representations, but not specifically to discrete audio tokens as defined, and there are no explicit audio-domain methods described to assess reproducibility in that context.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses Variational Information Bottleneck on Vector Quantized Autoencoders in general (VQ-VAE) and does not focus on discrete audio tokens, token generation, or audio-domain experiments. It lacks a clearly defined contribution about discrete audio representations or token-based modeling for audio, so it does not meet the inclusion criteria. It may be relevant to quantized representations, but not specifically to discrete audio tokens as defined, and there are no explicit audio-domain methods described to assess reproducibility in that context.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses Vector Quantized Variational Autoencoders (VQ-VAE) and interprets their loss functions from an information bottleneck perspective. However, the abstract does not clarify the generation or use of discrete audio tokens specifically for speech, music, or general audio, nor does it detail the token formation processes, semantic or acoustic disentanglement, or downstream applications like audio language modeling or audio-text generation. The core contribution seems focused on theoretical interpretation rather than discrete audio token design with practical application for sequence models, thus not fully meeting the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study discusses Vector Quantized Variational Autoencoders (VQ-VAE) and interprets their loss functions from an information bottleneck perspective. However, the abstract does not clarify the generation or use of discrete audio tokens specifically for speech, music, or general audio, nor does it detail the token formation processes, semantic or acoustic disentanglement, or downstream applications like audio language modeling or audio-text generation. The core contribution seems focused on theoretical interpretation rather than discrete audio token design with practical application for sequence models, thus not fully meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
    "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
    "metadata": {
      "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "summary": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
      "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
      "doi": "https://doi.org/10.21437/interspeech.2014-80",
      "openalex_id": "https://openalex.org/W2293634267",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses LSTM architectures for acoustic modeling and distributed training, not discrete audio token representations; it fails the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses LSTM architectures for acoustic modeling and distributed training, not discrete audio token representations; it fails the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on LSTM recurrent neural network architectures for acoustic modeling in speech recognition, without discussing any discrete audio tokenization, vector quantization, or related discrete token generation methods that are central to the inclusion criteria. It does not mention producing or using discrete audio tokens or similar representations for downstream sequence modeling or generation tasks. Therefore, it does not meet the primary inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on LSTM recurrent neural network architectures for acoustic modeling in speech recognition, without discussing any discrete audio tokenization, vector quantization, or related discrete token generation methods that are central to the inclusion criteria. It does not mention producing or using discrete audio tokens or similar representations for downstream sequence modeling or generation tasks. Therefore, it does not meet the primary inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Supervised Feature Transformations on Zero Resources for Improved Acoustic Unit Discovery",
    "abstract": "In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.",
    "metadata": {
      "title": "Learning Supervised Feature Transformations on Zero Resources for Improved Acoustic Unit Discovery",
      "summary": "In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.",
      "abstract": "In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.",
      "doi": "https://doi.org/10.1587/transinf.2017edp7175",
      "openalex_id": "https://openalex.org/W2780786457",
      "arxiv_id": "",
      "publication_date": "2017-12-31",
      "published": "2017-12-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "While the paper explores zero-resource acoustic unit discovery using pseudo-labels to learn feature transforms, it does not propose discrete audio token design (e.g., VQ/RVQ, token quantization) or token-based representations for downstream language/sequence models, thus it fails the discrete audio tokens inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "While the paper explores zero-resource acoustic unit discovery using pseudo-labels to learn feature transforms, it does not propose discrete audio token design (e.g., VQ/RVQ, token quantization) or token-based representations for downstream language/sequence models, thus it fails the discrete audio tokens inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using feature transformations to improve acoustic unit discovery via clustering methods, specifically DPGMM, without involving explicit discrete token generation or vector quantization mechanisms for audio representation as defined by the inclusion criteria. It does not clearly define the generation of discrete audio tokens used for language or sequence modeling nor elaborates on token formation processes or quantization strategies. Therefore, it does not meet the inclusion criteria emphasizing discrete token design and use for downstream modeling tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using feature transformations to improve acoustic unit discovery via clustering methods, specifically DPGMM, without involving explicit discrete token generation or vector quantization mechanisms for audio representation as defined by the inclusion criteria. It does not clearly define the generation of discrete audio tokens used for language or sequence modeling nor elaborates on token formation processes or quantization strategies. Therefore, it does not meet the inclusion criteria emphasizing discrete token design and use for downstream modeling tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche",
    "abstract": "Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.",
    "metadata": {
      "title": "Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche",
      "summary": "Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.",
      "abstract": "Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.",
      "doi": "https://doi.org/10.1126/sciadv.aaw2594",
      "openalex_id": "https://openalex.org/W2971775690",
      "arxiv_id": "",
      "publication_date": "2019-09-04",
      "published": "2019-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses cross-language information rates and does not present discrete audio token design/formation methods (no VQ/RVQ, quantization, or semantic tokenization), hence fails the inclusion criteria requiring token-level representations for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses cross-language information rates and does not present discrete audio token design/formation methods (no VQ/RVQ, quantization, or semantic tokenization), hence fails the inclusion criteria requiring token-level representations for audio modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss the information rates across different human languages without mentioning any process of converting continuous audio signals into discrete audio tokens, nor do they describe methods involving vector quantization, residual vector quantization, or similar discrete token generation techniques. There is no clear indication of token formation, use in language models, or discrete audio representations, and the content focuses on linguistic information rates rather than audio token discrete representations for modeling or generation tasks. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens and lacks relevant methodological detail, warranting exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss the information rates across different human languages without mentioning any process of converting continuous audio signals into discrete audio tokens, nor do they describe methods involving vector quantization, residual vector quantization, or similar discrete token generation techniques. There is no clear indication of token formation, use in language models, or discrete audio representations, and the content focuses on linguistic information rates rather than audio token discrete representations for modeling or generation tasks. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens and lacks relevant methodological detail, warranting exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "End-to-End Speech Recognition Sequence Training With Reinforcement Learning",
    "abstract": "End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.",
    "metadata": {
      "title": "End-to-End Speech Recognition Sequence Training With Reinforcement Learning",
      "summary": "End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.",
      "abstract": "End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.",
      "doi": "https://doi.org/10.1109/access.2019.2922617",
      "openalex_id": "https://openalex.org/W2951444698",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on training ASR with reinforcement learning and does not discuss discrete audio tokens or tokenization/quantization designs; it fails to meet the inclusion requirement for discrete audio token representations and lacks methodology details on token-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on training ASR with reinforcement learning and does not discuss discrete audio tokens or tokenization/quantization designs; it fails to meet the inclusion requirement for discrete audio token representations and lacks methodology details on token-based modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on end-to-end speech recognition training with reinforcement learning, without mentioning discrete audio tokens, vector quantization, or tokenization processes. There is no description of generating or using discrete tokens as intermediate representations for sequence modeling or language modeling, which is central to the inclusion criteria. The method centers on reinforcement learning to optimize recognition accuracy rather than on discrete token modeling or design. Hence, it does not meet the inclusion criteria, nor does it discuss any token generation or quantization methods for discrete audio token representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on end-to-end speech recognition training with reinforcement learning, without mentioning discrete audio tokens, vector quantization, or tokenization processes. There is no description of generating or using discrete tokens as intermediate representations for sequence modeling or language modeling, which is central to the inclusion criteria. The method centers on reinforcement learning to optimize recognition accuracy rather than on discrete token modeling or design. Hence, it does not meet the inclusion criteria, nor does it discuss any token generation or quantization methods for discrete audio token representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
    "abstract": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
    "metadata": {
      "title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
      "summary": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "abstract": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "doi": "https://doi.org/10.1109/icassp.2011.5947338",
      "openalex_id": "https://openalex.org/W2170659185",
      "arxiv_id": "",
      "publication_date": "2011-05-01",
      "published": "2011-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses unsupervised acoustic sub-word unit detection via a diarization-based approach for query-by-example STD, but it does not present a discrete tokenization/quantization design (e.g., VQ/RVQ, codebooks) suitable for use as discrete tokens in language-model-like pipelines, so it does not meet the core 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper discusses unsupervised acoustic sub-word unit detection via a diarization-based approach for query-by-example STD, but it does not present a discrete tokenization/quantization design (e.g., VQ/RVQ, codebooks) suitable for use as discrete tokens in language-model-like pipelines, so it does not meet the core 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a method for generating acoustic sub-word units for query-by-example spoken term detection using a modified speaker diarization system, which is an unsupervised approach for segmenting and modeling speech. However, the study does not clearly describe the generation of discrete audio tokens via vector quantization or similar quantization methods, nor does it indicate the use of these tokens as intermediate representations for audio language modeling or sequence modeling; rather, it focuses on speaker-dependent sub-word units primarily for retrieval tasks without explicit mention of tokenization suitable for language models. Moreover, the description lacks details on the quantization process, model structure, or token usage for downstream generation or understanding tasks, which are critical inclusion criteria. Therefore, it does not meet the main inclusion criteria for discrete audio token research, though it does contribute to sub-word unit detection in speech.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a method for generating acoustic sub-word units for query-by-example spoken term detection using a modified speaker diarization system, which is an unsupervised approach for segmenting and modeling speech. However, the study does not clearly describe the generation of discrete audio tokens via vector quantization or similar quantization methods, nor does it indicate the use of these tokens as intermediate representations for audio language modeling or sequence modeling; rather, it focuses on speaker-dependent sub-word units primarily for retrieval tasks without explicit mention of tokenization suitable for language models. Moreover, the description lacks details on the quantization process, model structure, or token usage for downstream generation or understanding tasks, which are critical inclusion criteria. Therefore, it does not meet the main inclusion criteria for discrete audio token research, though it does contribute to sub-word unit detection in speech.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An improved speech segmentation quality measure: the r-value",
    "abstract": "Phone segmentation in ASR is usually performed indirectly by Viterbi decoding of HMM output. Direct approaches also exist, e.g., blind speech segmentation algorithms. In either case, performance of automatic speech segmentation algorithms is often measured using automated evaluation algorithms and used to optimize a segmentation system’s performance. However, evaluation approaches reported in literature were found to be lacking. Also, we have determined that increases in phone boundary location detection rates are often due to increased over-segmentation levels and not to algorithmic improvements, i.e., by simply adding random boundaries a better hit-rate can be achieved when using current quality measures. Since established measures were found to be insensitive to this type of random boundary insertion, a new R-value quality measure is introduced that indicates how close a segmentation algorithm’s performance is to an ideal point of operation.",
    "metadata": {
      "title": "An improved speech segmentation quality measure: the r-value",
      "summary": "Phone segmentation in ASR is usually performed indirectly by Viterbi decoding of HMM output. Direct approaches also exist, e.g., blind speech segmentation algorithms. In either case, performance of automatic speech segmentation algorithms is often measured using automated evaluation algorithms and used to optimize a segmentation system’s performance. However, evaluation approaches reported in literature were found to be lacking. Also, we have determined that increases in phone boundary location detection rates are often due to increased over-segmentation levels and not to algorithmic improvements, i.e., by simply adding random boundaries a better hit-rate can be achieved when using current quality measures. Since established measures were found to be insensitive to this type of random boundary insertion, a new R-value quality measure is introduced that indicates how close a segmentation algorithm’s performance is to an ideal point of operation.",
      "abstract": "Phone segmentation in ASR is usually performed indirectly by Viterbi decoding of HMM output. Direct approaches also exist, e.g., blind speech segmentation algorithms. In either case, performance of automatic speech segmentation algorithms is often measured using automated evaluation algorithms and used to optimize a segmentation system’s performance. However, evaluation approaches reported in literature were found to be lacking. Also, we have determined that increases in phone boundary location detection rates are often due to increased over-segmentation levels and not to algorithmic improvements, i.e., by simply adding random boundaries a better hit-rate can be achieved when using current quality measures. Since established measures were found to be insensitive to this type of random boundary insertion, a new R-value quality measure is introduced that indicates how close a segmentation algorithm’s performance is to an ideal point of operation.",
      "doi": "https://doi.org/10.21437/interspeech.2009-538",
      "openalex_id": "https://openalex.org/W130754613",
      "arxiv_id": "",
      "publication_date": "2009-09-06",
      "published": "2009-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a new quality metric (R-value) for phone segmentation in ASR and does not address discrete audio tokens or tokenization methods (VQ/RVQ/etc.) required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a new quality metric (R-value) for phone segmentation in ASR and does not address discrete audio tokens or tokenization methods (VQ/RVQ/etc.) required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech segmentation quality measurement, specifically introducing a new R-value measure to assess segmentation performance, without addressing discrete audio token generation, vector quantization, or any form of token design used for audio language modeling or sequence modeling. The content does not meet the inclusion criteria of involving discrete token representations or their generation mechanisms, and rather focuses on evaluation metrics for segmentation algorithms in ASR.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech segmentation quality measurement, specifically introducing a new R-value measure to assess segmentation performance, without addressing discrete audio token generation, vector quantization, or any form of token design used for audio language modeling or sequence modeling. The content does not meet the inclusion criteria of involving discrete token representations or their generation mechanisms, and rather focuses on evaluation metrics for segmentation algorithms in ASR.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Phoneme segmentation with Recurrent Neural Networks.",
    "abstract": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.",
    "metadata": {
      "title": "Improving Phoneme segmentation with Recurrent Neural Networks.",
      "summary": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.",
      "abstract": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2478415332",
      "arxiv_id": "",
      "publication_date": "2016-08-01",
      "published": "2016-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on phoneme segmentation using RNNs and an unsupervised error-based method, but it does not discuss discrete audio token representations (e.g., VQ/RVQ, semantic discretization) or token-based downstream modeling; thus it fails all inclusion criteria and does not meet the topic definition for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on phoneme segmentation using RNNs and an unsupervised error-based method, but it does not discuss discrete audio token representations (e.g., VQ/RVQ, semantic discretization) or token-based downstream modeling; thus it fails all inclusion criteria and does not meet the topic definition for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phoneme segmentation using recurrent neural networks and Markov chains to analyze speech features and identify boundaries, but it does not mention generating or using discrete audio tokens, vector quantization, or related discrete token representations intended for language or sequence modeling, which are core to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phoneme segmentation using recurrent neural networks and Markov chains to analyze speech features and identify boundaries, but it does not mention generating or using discrete audio tokens, vector quantization, or related discrete token representations intended for language or sequence modeling, which are core to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rapid evaluation of speech representations for spoken term discovery",
    "abstract": "Acoustic front-ends are typically developed for supervised learning tasks and are thus optimized to minimize word error rate, phone error rate, etc. However, in recent efforts to develop zero-resource speech technologies, the goal is not to use transcribed speech to train systems but instead to discover the acoustic structure of the spoken language automatically. For this new setting, we require a framework for evaluating the quality of speech representations without coupling to a particular recognition architecture. Motivated by the spoken term discovery task, we present a dynamic time warping-based framework for quantifying how well a representation can associate words of the same type spoken by different speakers. We benchmark the quality of a wide range of speech representations using multiple frame-level distance metrics and demonstrate that our performance metrics can also accurately predict phone recognition accuracies.",
    "metadata": {
      "title": "Rapid evaluation of speech representations for spoken term discovery",
      "summary": "Acoustic front-ends are typically developed for supervised learning tasks and are thus optimized to minimize word error rate, phone error rate, etc. However, in recent efforts to develop zero-resource speech technologies, the goal is not to use transcribed speech to train systems but instead to discover the acoustic structure of the spoken language automatically. For this new setting, we require a framework for evaluating the quality of speech representations without coupling to a particular recognition architecture. Motivated by the spoken term discovery task, we present a dynamic time warping-based framework for quantifying how well a representation can associate words of the same type spoken by different speakers. We benchmark the quality of a wide range of speech representations using multiple frame-level distance metrics and demonstrate that our performance metrics can also accurately predict phone recognition accuracies.",
      "abstract": "Acoustic front-ends are typically developed for supervised learning tasks and are thus optimized to minimize word error rate, phone error rate, etc. However, in recent efforts to develop zero-resource speech technologies, the goal is not to use transcribed speech to train systems but instead to discover the acoustic structure of the spoken language automatically. For this new setting, we require a framework for evaluating the quality of speech representations without coupling to a particular recognition architecture. Motivated by the spoken term discovery task, we present a dynamic time warping-based framework for quantifying how well a representation can associate words of the same type spoken by different speakers. We benchmark the quality of a wide range of speech representations using multiple frame-level distance metrics and demonstrate that our performance metrics can also accurately predict phone recognition accuracies.",
      "doi": "https://doi.org/10.21437/interspeech.2011-304",
      "openalex_id": "https://openalex.org/W2407151108",
      "arxiv_id": "",
      "publication_date": "2011-08-27",
      "published": "2011-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on evaluating speech representations for spoken term discovery using dynamic time warping and does not address discrete audio tokens or token quantization design, so it does not meet the token-centric inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on evaluating speech representations for spoken term discovery using dynamic time warping and does not address discrete audio tokens or token quantization design, so it does not meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating acoustic speech representations for spoken term discovery using dynamic time warping but does not mention generating discrete audio tokens through vector quantization or similar discrete tokenization methods, nor does it specify a token generation process or use of such tokens in sequence or language models, which are key inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating acoustic speech representations for spoken term discovery using dynamic time warping but does not mention generating discrete audio tokens through vector quantization or similar discrete tokenization methods, nor does it specify a token generation process or use of such tokens in sequence or language models, which are key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A deep scattering spectrum — Deep Siamese network pipeline for unsupervised acoustic modeling",
    "abstract": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
    "metadata": {
      "title": "A deep scattering spectrum — Deep Siamese network pipeline for unsupervised acoustic modeling",
      "summary": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
      "abstract": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472622",
      "openalex_id": "https://openalex.org/W2400549570",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on unsupervised acoustic modeling and the use of scattering-spectrum features with a Siamese network; it does not discuss discrete audio tokens, vector quantization, or tokenization as a core research contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on unsupervised acoustic modeling and the use of scattering-spectrum features with a Siamese network; it does not discuss discrete audio tokens, vector quantization, or tokenization as a core research contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on replacing traditional mel-scaled filterbanks with scattering spectrum features in an unsupervised acoustic modeling pipeline using a Siamese network, but it does not explicitly discuss the generation of discrete audio tokens, vector quantization, or a discrete token design for sequence modeling as required by the inclusion criteria. The abstract lacks details on token generation workflows, quantization methods, or use of discrete tokens for language or sequence modeling, and instead primarily addresses feature representation for acoustic modeling. Therefore, it does not meet the inclusion requirements focused on discrete audio tokens as a core contribution or method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on replacing traditional mel-scaled filterbanks with scattering spectrum features in an unsupervised acoustic modeling pipeline using a Siamese network, but it does not explicitly discuss the generation of discrete audio tokens, vector quantization, or a discrete token design for sequence modeling as required by the inclusion criteria. The abstract lacks details on token generation workflows, quantization methods, or use of discrete tokens for language or sequence modeling, and instead primarily addresses feature representation for acoustic modeling. Therefore, it does not meet the inclusion requirements focused on discrete audio tokens as a core contribution or method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models",
    "abstract": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
    "metadata": {
      "title": "Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models",
      "summary": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
      "abstract": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
      "doi": "https://doi.org/10.7551/mitpress/7503.003.0085",
      "openalex_id": "https://openalex.org/W2117126688",
      "arxiv_id": "",
      "publication_date": "2007-09-07",
      "published": "2007-09-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses adaptor grammars for probabilistic language models, not discrete audio token representations or audio tokenization; it fails the Audio Token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses adaptor grammars for probabilistic language models, not discrete audio token representations or audio tokenization; it fails the Audio Token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptor grammars which are probabilistic models of language and grammars used for inference of language models, but it does not mention audio signals, discrete audio tokens, vector quantization, or any form of discretization of continuous audio signals into tokens for audio language modeling or related audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptor grammars which are probabilistic models of language and grammars used for inference of language models, but it does not mention audio signals, discrete audio tokens, vector quantization, or any form of discretization of continuous audio signals into tokens for audio language modeling or related audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Co‐occurrence statistics as a language‐dependent cue for speech segmentation",
    "abstract": "Abstract To what extent can language acquisition be explained in terms of different associative learning mechanisms? It has been hypothesized that distributional regularities in spoken languages are strong enough to elicit statistical learning about dependencies among speech units. Distributional regularities could be a useful cue for word learning even without rich language‐specific knowledge. However, it is not clear how strong and reliable the distributional cues are that humans might use to segment speech. We investigate cross‐linguistic viability of different statistical learning strategies by analyzing child‐directed speech corpora from nine languages and by modeling possible statistics‐based speech segmentations. We show that languages vary as to which statistical segmentation strategies are most successful. The variability of the results can be partially explained by systematic differences between languages, such as rhythmical differences. The results confirm previous findings that different statistical learning strategies are successful in different languages and suggest that infants may have to primarily rely on non‐statistical cues when they begin their process of speech segmentation.",
    "metadata": {
      "title": "Co‐occurrence statistics as a language‐dependent cue for speech segmentation",
      "summary": "Abstract To what extent can language acquisition be explained in terms of different associative learning mechanisms? It has been hypothesized that distributional regularities in spoken languages are strong enough to elicit statistical learning about dependencies among speech units. Distributional regularities could be a useful cue for word learning even without rich language‐specific knowledge. However, it is not clear how strong and reliable the distributional cues are that humans might use to segment speech. We investigate cross‐linguistic viability of different statistical learning strategies by analyzing child‐directed speech corpora from nine languages and by modeling possible statistics‐based speech segmentations. We show that languages vary as to which statistical segmentation strategies are most successful. The variability of the results can be partially explained by systematic differences between languages, such as rhythmical differences. The results confirm previous findings that different statistical learning strategies are successful in different languages and suggest that infants may have to primarily rely on non‐statistical cues when they begin their process of speech segmentation.",
      "abstract": "Abstract To what extent can language acquisition be explained in terms of different associative learning mechanisms? It has been hypothesized that distributional regularities in spoken languages are strong enough to elicit statistical learning about dependencies among speech units. Distributional regularities could be a useful cue for word learning even without rich language‐specific knowledge. However, it is not clear how strong and reliable the distributional cues are that humans might use to segment speech. We investigate cross‐linguistic viability of different statistical learning strategies by analyzing child‐directed speech corpora from nine languages and by modeling possible statistics‐based speech segmentations. We show that languages vary as to which statistical segmentation strategies are most successful. The variability of the results can be partially explained by systematic differences between languages, such as rhythmical differences. The results confirm previous findings that different statistical learning strategies are successful in different languages and suggest that infants may have to primarily rely on non‐statistical cues when they begin their process of speech segmentation.",
      "doi": "https://doi.org/10.1111/desc.12390",
      "openalex_id": "https://openalex.org/W2345913943",
      "arxiv_id": "",
      "publication_date": "2016-05-04",
      "published": "2016-05-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on cross-linguistic statistical learning for speech segmentation and does not present discrete audio token representations or a tokenization method (VQ/RVQ/semantic tokens) required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on cross-linguistic statistical learning for speech segmentation and does not present discrete audio token representations or a tokenization method (VQ/RVQ/semantic tokens) required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper investigates statistical learning strategies for speech segmentation across languages, focusing on distributional regularities and associative learning mechanisms rather than proposing or analyzing discrete audio token generation, vector quantization, or related tokenization techniques that can be utilized as input for language or sequence models, which are central to the inclusion criteria. It lacks discussion on discrete token design, generation process, or usage of tokens in audio language modeling or generation, and thus does not meet the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper investigates statistical learning strategies for speech segmentation across languages, focusing on distributional regularities and associative learning mechanisms rather than proposing or analyzing discrete audio token generation, vector quantization, or related tokenization techniques that can be utilized as input for language or sequence models, which are central to the inclusion criteria. It lacks discussion on discrete token design, generation process, or usage of tokens in audio language modeling or generation, and thus does not meet the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
    "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.",
    "metadata": {
      "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
      "summary": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.",
      "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.",
      "doi": "https://doi.org/10.21437/interspeech.2017-877",
      "openalex_id": "https://openalex.org/W2962799131",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper analyzes gate activation signals and phoneme boundaries without discussing discrete audio token representations or tokenization methods for downstream language-model-like processing, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes gate activation signals and phoneme boundaries without discussing discrete audio token representations or tokenization methods for downstream language-model-like processing, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper analyzes gate activation signals in gated recurrent neural networks related to phoneme boundaries and phoneme segmentation, but it does not discuss discrete audio token design, generation processes, or the use of vector quantization or related discrete representation mechanisms for audio. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper analyzes gate activation signals in gated recurrent neural networks related to phoneme boundaries and phoneme segmentation, but it does not discuss discrete audio token design, generation processes, or the use of vector quantization or related discrete representation mechanisms for audio. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level",
    "abstract": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level Okko Rasanen (okko.rasanen@aalto.fi) Department of Signal Processing and Acoustics, Aalto University PO Box 13000, 00076 Aalto, FINLAND Abstract Considerable effort has been put to understand how infants may utilize statistical regularities of speech in early word segmentation. Some studies suggest that infants are able to discover word boundaries at the points of high unpredictability across subsequent linguistic units such as phonemes or syllables. Meanwhile, the possible role of the statistical regularities in the temporal organization of the speech at a pre-linguistic acoustic level has not been widely addressed. The current work examines how the short-term temporal predictability of the acoustic speech signal correlates with linguistically motivated phone-, syllable-, and word-level units. The results indicate that the points of low predictability correlate mainly with the boundaries between phone-like segments. This suggests that the same statistical learning mechanisms hypothesized to operate at the word level can also aid in temporal organization of the speech stream into phone-like temporal segments before knowing the phonemic or syllabic units of the language. Keywords: distributional learning; language acquisition; phone segmentation; speech segmentation; statistical learning Introduction Segmentation of continuous speech into linguistically relevant units is essential for successful language acquisition (LA). Segmentation can take place at a number of levels, as the speech can be linguistically characterized in terms of units such as phones, syllables, and words, and with the latter always consisting of the former. In the early LA research, infants’ ability to segment words from speech has received a large amount of attention as the words are the main functional units of the language, standing for entities, events, actions, and states of the surrounding world. In the word segmentation studies, one of the major findings is that the infants can use statistical regularities in the speech input in order to discover boundaries between words (Saffran, Aslin & Newport, 1996). Also, these statistical learning mechanisms do not seem to be specific to words or even language faculty but operate across many levels of representation and perceptual domains (see, e.g., Romberg & Saffran, 2010, for a recent review). Importantly, a large body of the existing work on statistical word learning assumes that the infants are capable of representing speech input in terms of linguistically relevant units such as phones or syllables. Given the representational units, the infants are supposedly tracking transitional probabilities (TPs) between these units across time and use low-probability transitions as indications for word boundaries while the high-probability regions form representational units (Saffran et al., 1996). This strategy is valid as long as the TPs within words are higher than the TPs across word boundaries. However, the infant’s access to linguistic units such as phones or syllables and their statistics cannot be taken for granted. It is still unclear whether early adaptation to phonetic units drives lexical learning (c.f., NLM-e theory by Kuhl et al., 2008) or whether early lexical learning actually precedes, or at least parallels, the acquisition of sub-word representation of spoken language (e.g., Werker & Curtin, 2005). The “sub- word units –first” approach is challenged by the fact that the bottom-up organization of speech signal into temporally and categorically discrete units is far from trivial. Learning a phonetic or syllabic representation of the spoken language includes both the segmentation problem (division of the signal in time) and the categorization problem (assigning context-, talker-, and speaking style-dependent acoustic observations into a correct number of linguistic categories). Importantly, infants do not have access to any ground truth in either of the two tasks while learning the native language, suggesting that some speech-external factors such as feedback from lexical level or social interaction are required for successful learning. Still, it seems that even the basic problem of segmenting speech into sub-word units has been largely overlooked in the existing LA research. For example, it is unclear how well natural co-articulated speech can be segmented into sub-word units before learning the phonetic or lexical units of the language, and whether infants actually do such segmentation. Possibly the most concrete reference to early sub-word segmentation in the existing literature is the Kuhl’s concept of basic cuts: a perceptual mechanism that provides an initial low-level chunking of the speech stream into primitive phone-like units and which then gradually improves towards native language phone system through language exposure (Kuhl, 2004, and references therein). Segmentation into syllabic units is also central to many theories of LA (e.g., Jusczyk, 1993) although explicit and well-controlled studies on the segmentation process itself are few. In the speech engineering community, both phone- and syllable-level segmentation have been widely studied. The general finding is that the spectral changes (or “jumps”) in speech are good candidates for phone boundaries as they correlate with the changes in articulator positions (e.g., Almpanidis & Kotropulos, 2008; Esposito & Aversano, 2005; ten Bosch & Cranen, 2007; Scharenborg et al., 2007). On the other hand, it is known that syllabic segmentation",
    "metadata": {
      "title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level",
      "summary": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level Okko Rasanen (okko.rasanen@aalto.fi) Department of Signal Processing and Acoustics, Aalto University PO Box 13000, 00076 Aalto, FINLAND Abstract Considerable effort has been put to understand how infants may utilize statistical regularities of speech in early word segmentation. Some studies suggest that infants are able to discover word boundaries at the points of high unpredictability across subsequent linguistic units such as phonemes or syllables. Meanwhile, the possible role of the statistical regularities in the temporal organization of the speech at a pre-linguistic acoustic level has not been widely addressed. The current work examines how the short-term temporal predictability of the acoustic speech signal correlates with linguistically motivated phone-, syllable-, and word-level units. The results indicate that the points of low predictability correlate mainly with the boundaries between phone-like segments. This suggests that the same statistical learning mechanisms hypothesized to operate at the word level can also aid in temporal organization of the speech stream into phone-like temporal segments before knowing the phonemic or syllabic units of the language. Keywords: distributional learning; language acquisition; phone segmentation; speech segmentation; statistical learning Introduction Segmentation of continuous speech into linguistically relevant units is essential for successful language acquisition (LA). Segmentation can take place at a number of levels, as the speech can be linguistically characterized in terms of units such as phones, syllables, and words, and with the latter always consisting of the former. In the early LA research, infants’ ability to segment words from speech has received a large amount of attention as the words are the main functional units of the language, standing for entities, events, actions, and states of the surrounding world. In the word segmentation studies, one of the major findings is that the infants can use statistical regularities in the speech input in order to discover boundaries between words (Saffran, Aslin & Newport, 1996). Also, these statistical learning mechanisms do not seem to be specific to words or even language faculty but operate across many levels of representation and perceptual domains (see, e.g., Romberg & Saffran, 2010, for a recent review). Importantly, a large body of the existing work on statistical word learning assumes that the infants are capable of representing speech input in terms of linguistically relevant units such as phones or syllables. Given the representational units, the infants are supposedly tracking transitional probabilities (TPs) between these units across time and use low-probability transitions as indications for word boundaries while the high-probability regions form representational units (Saffran et al., 1996). This strategy is valid as long as the TPs within words are higher than the TPs across word boundaries. However, the infant’s access to linguistic units such as phones or syllables and their statistics cannot be taken for granted. It is still unclear whether early adaptation to phonetic units drives lexical learning (c.f., NLM-e theory by Kuhl et al., 2008) or whether early lexical learning actually precedes, or at least parallels, the acquisition of sub-word representation of spoken language (e.g., Werker & Curtin, 2005). The “sub- word units –first” approach is challenged by the fact that the bottom-up organization of speech signal into temporally and categorically discrete units is far from trivial. Learning a phonetic or syllabic representation of the spoken language includes both the segmentation problem (division of the signal in time) and the categorization problem (assigning context-, talker-, and speaking style-dependent acoustic observations into a correct number of linguistic categories). Importantly, infants do not have access to any ground truth in either of the two tasks while learning the native language, suggesting that some speech-external factors such as feedback from lexical level or social interaction are required for successful learning. Still, it seems that even the basic problem of segmenting speech into sub-word units has been largely overlooked in the existing LA research. For example, it is unclear how well natural co-articulated speech can be segmented into sub-word units before learning the phonetic or lexical units of the language, and whether infants actually do such segmentation. Possibly the most concrete reference to early sub-word segmentation in the existing literature is the Kuhl’s concept of basic cuts: a perceptual mechanism that provides an initial low-level chunking of the speech stream into primitive phone-like units and which then gradually improves towards native language phone system through language exposure (Kuhl, 2004, and references therein). Segmentation into syllabic units is also central to many theories of LA (e.g., Jusczyk, 1993) although explicit and well-controlled studies on the segmentation process itself are few. In the speech engineering community, both phone- and syllable-level segmentation have been widely studied. The general finding is that the spectral changes (or “jumps”) in speech are good candidates for phone boundaries as they correlate with the changes in articulator positions (e.g., Almpanidis & Kotropulos, 2008; Esposito & Aversano, 2005; ten Bosch & Cranen, 2007; Scharenborg et al., 2007). On the other hand, it is known that syllabic segmentation",
      "abstract": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level Okko Rasanen (okko.rasanen@aalto.fi) Department of Signal Processing and Acoustics, Aalto University PO Box 13000, 00076 Aalto, FINLAND Abstract Considerable effort has been put to understand how infants may utilize statistical regularities of speech in early word segmentation. Some studies suggest that infants are able to discover word boundaries at the points of high unpredictability across subsequent linguistic units such as phonemes or syllables. Meanwhile, the possible role of the statistical regularities in the temporal organization of the speech at a pre-linguistic acoustic level has not been widely addressed. The current work examines how the short-term temporal predictability of the acoustic speech signal correlates with linguistically motivated phone-, syllable-, and word-level units. The results indicate that the points of low predictability correlate mainly with the boundaries between phone-like segments. This suggests that the same statistical learning mechanisms hypothesized to operate at the word level can also aid in temporal organization of the speech stream into phone-like temporal segments before knowing the phonemic or syllabic units of the language. Keywords: distributional learning; language acquisition; phone segmentation; speech segmentation; statistical learning Introduction Segmentation of continuous speech into linguistically relevant units is essential for successful language acquisition (LA). Segmentation can take place at a number of levels, as the speech can be linguistically characterized in terms of units such as phones, syllables, and words, and with the latter always consisting of the former. In the early LA research, infants’ ability to segment words from speech has received a large amount of attention as the words are the main functional units of the language, standing for entities, events, actions, and states of the surrounding world. In the word segmentation studies, one of the major findings is that the infants can use statistical regularities in the speech input in order to discover boundaries between words (Saffran, Aslin & Newport, 1996). Also, these statistical learning mechanisms do not seem to be specific to words or even language faculty but operate across many levels of representation and perceptual domains (see, e.g., Romberg & Saffran, 2010, for a recent review). Importantly, a large body of the existing work on statistical word learning assumes that the infants are capable of representing speech input in terms of linguistically relevant units such as phones or syllables. Given the representational units, the infants are supposedly tracking transitional probabilities (TPs) between these units across time and use low-probability transitions as indications for word boundaries while the high-probability regions form representational units (Saffran et al., 1996). This strategy is valid as long as the TPs within words are higher than the TPs across word boundaries. However, the infant’s access to linguistic units such as phones or syllables and their statistics cannot be taken for granted. It is still unclear whether early adaptation to phonetic units drives lexical learning (c.f., NLM-e theory by Kuhl et al., 2008) or whether early lexical learning actually precedes, or at least parallels, the acquisition of sub-word representation of spoken language (e.g., Werker & Curtin, 2005). The “sub- word units –first” approach is challenged by the fact that the bottom-up organization of speech signal into temporally and categorically discrete units is far from trivial. Learning a phonetic or syllabic representation of the spoken language includes both the segmentation problem (division of the signal in time) and the categorization problem (assigning context-, talker-, and speaking style-dependent acoustic observations into a correct number of linguistic categories). Importantly, infants do not have access to any ground truth in either of the two tasks while learning the native language, suggesting that some speech-external factors such as feedback from lexical level or social interaction are required for successful learning. Still, it seems that even the basic problem of segmenting speech into sub-word units has been largely overlooked in the existing LA research. For example, it is unclear how well natural co-articulated speech can be segmented into sub-word units before learning the phonetic or lexical units of the language, and whether infants actually do such segmentation. Possibly the most concrete reference to early sub-word segmentation in the existing literature is the Kuhl’s concept of basic cuts: a perceptual mechanism that provides an initial low-level chunking of the speech stream into primitive phone-like units and which then gradually improves towards native language phone system through language exposure (Kuhl, 2004, and references therein). Segmentation into syllabic units is also central to many theories of LA (e.g., Jusczyk, 1993) although explicit and well-controlled studies on the segmentation process itself are few. In the speech engineering community, both phone- and syllable-level segmentation have been widely studied. The general finding is that the spectral changes (or “jumps”) in speech are good candidates for phone boundaries as they correlate with the changes in articulator positions (e.g., Almpanidis & Kotropulos, 2008; Esposito & Aversano, 2005; ten Bosch & Cranen, 2007; Scharenborg et al., 2007). On the other hand, it is known that syllabic segmentation",
      "doi": "",
      "openalex_id": "https://openalex.org/W2404952642",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item studies segmentation into phone-like units and pre-linguistic statistical learning, but does not present discrete audio token generation/quantization methods (e.g., VQ/RVQ, hierarchical/token-based representations) or use discrete tokens for language-model-type downstream tasks, so it fails the core inclusion criteria and does not demonstrate the required tokenized representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item studies segmentation into phone-like units and pre-linguistic statistical learning, but does not present discrete audio token generation/quantization methods (e.g., VQ/RVQ, hierarchical/token-based representations) or use discrete tokens for language-model-type downstream tasks, so it fails the core inclusion criteria and does not demonstrate the required tokenized representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on statistical learning mechanisms for segmenting continuous speech into phone-like units at a pre-linguistic level, rather than designing or producing discrete audio tokens through vector quantization or similar methods for sequence/language modeling. It emphasizes the cognitive and acoustic segmentation process from an infant language acquisition perspective and does not describe a discrete token generation process, explicit quantization mechanisms, or usage of tokens for computational modeling such as audio language modeling or generative tasks. Therefore, it does not meet the inclusion criteria related to discrete audio token design and technical content, nor does it fall into any exclusion criteria regarding missing dates or demo-level content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on statistical learning mechanisms for segmenting continuous speech into phone-like units at a pre-linguistic level, rather than designing or producing discrete audio tokens through vector quantization or similar methods for sequence/language modeling. It emphasizes the cognitive and acoustic segmentation process from an infant language acquisition perspective and does not describe a discrete token generation process, explicit quantization mechanisms, or usage of tokens for computational modeling such as audio language modeling or generative tasks. Therefore, it does not meet the inclusion criteria related to discrete audio token design and technical content, nor does it fall into any exclusion criteria regarding missing dates or demo-level content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonetics embedding learning with side information",
    "abstract": "We show that it is possible to learn an efficient acoustic model using only a<br>small amount of easily available word-level similarity annotations. In contrast<br>to the detailed phonetic labeling required by classical speech recognition<br>technologies, the only information our method requires are pairs of<br>speech excerpts which are known to be similar (same word) and pairs of<br>speech excerpts which are known to be different (different words). An acoustic model is obtained by training shallow and deep neural networks, using an<br>architecture and a cost function well-adapted to the nature of the provided information. The resulting model is evaluated on an ABX minimal-pair discrimination task and is shown to perform much better (11.8% ABX error<br>rate) than raw speech features (19.6%), not far from a fully supervised baseline (best neural network: 9.2%, HMM-GMM: 11%).",
    "metadata": {
      "title": "Phonetics embedding learning with side information",
      "summary": "We show that it is possible to learn an efficient acoustic model using only a<br>small amount of easily available word-level similarity annotations. In contrast<br>to the detailed phonetic labeling required by classical speech recognition<br>technologies, the only information our method requires are pairs of<br>speech excerpts which are known to be similar (same word) and pairs of<br>speech excerpts which are known to be different (different words). An acoustic model is obtained by training shallow and deep neural networks, using an<br>architecture and a cost function well-adapted to the nature of the provided information. The resulting model is evaluated on an ABX minimal-pair discrimination task and is shown to perform much better (11.8% ABX error<br>rate) than raw speech features (19.6%), not far from a fully supervised baseline (best neural network: 9.2%, HMM-GMM: 11%).",
      "abstract": "We show that it is possible to learn an efficient acoustic model using only a<br>small amount of easily available word-level similarity annotations. In contrast<br>to the detailed phonetic labeling required by classical speech recognition<br>technologies, the only information our method requires are pairs of<br>speech excerpts which are known to be similar (same word) and pairs of<br>speech excerpts which are known to be different (different words). An acoustic model is obtained by training shallow and deep neural networks, using an<br>architecture and a cost function well-adapted to the nature of the provided information. The resulting model is evaluated on an ABX minimal-pair discrimination task and is shown to perform much better (11.8% ABX error<br>rate) than raw speech features (19.6%), not far from a fully supervised baseline (best neural network: 9.2%, HMM-GMM: 11%).",
      "doi": "https://doi.org/10.1109/slt.2014.7078558",
      "openalex_id": "https://openalex.org/W2052697931",
      "arxiv_id": "",
      "publication_date": "2014-12-01",
      "published": "2014-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on learning acoustic models using word-level similarity annotations and ABX discrimination, not on creating or using discrete audio tokens via quantization or semantic tokenization, so it does not meet the core 'Discrete Audio Tokens' criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on learning acoustic models using word-level similarity annotations and ABX discrimination, not on creating or using discrete audio tokens via quantization or semantic tokenization, so it does not meet the core 'Discrete Audio Tokens' criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning an acoustic model using word-level similarity annotations without detailed phonetic labeling, and evaluates performance on a minimal-pair discrimination task. However, it does not explicitly describe the generation or use of discrete audio tokens through vector quantization or related discrete representation mechanisms, nor does it clarify that these tokens are designed for sequence or language model processing, which is essential to meet the inclusion criteria. The paper centers more on acoustic model training from similarity annotations rather than on discrete token design or usage for audio language modeling as defined in the criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning an acoustic model using word-level similarity annotations without detailed phonetic labeling, and evaluates performance on a minimal-pair discrimination task. However, it does not explicitly describe the generation or use of discrete audio tokens through vector quantization or related discrete representation mechanisms, nor does it clarify that these tokens are designed for sequence or language model processing, which is essential to meet the inclusion criteria. The paper centers more on acoustic model training from similarity annotations rather than on discrete token design or usage for audio language modeling as defined in the criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS",
    "abstract": "The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech.To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information.In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms.It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain.And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information.We conduct subjective tests to evaluate the proposed approach.Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity.When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder.Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multiscale discriminating.",
    "metadata": {
      "title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS",
      "summary": "The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech.To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information.In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms.It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain.And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information.We conduct subjective tests to evaluate the proposed approach.Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity.When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder.Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multiscale discriminating.",
      "abstract": "The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech.To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information.In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms.It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain.And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information.We conduct subjective tests to evaluate the proposed approach.Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity.When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder.Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multiscale discriminating.",
      "doi": "https://doi.org/10.21437/interspeech.2022-52",
      "openalex_id": "https://openalex.org/W4297841320",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a multi-scale spectrogram discriminator for GAN-based non-autoregressive TTS and does not address discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria about discrete audio token design.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a multi-scale spectrogram discriminator for GAN-based non-autoregressive TTS and does not address discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria about discrete audio token design.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a GAN-based discriminator for Non-Autoregressive TTS to improve Mel-spectrogram generation quality, without proposing or detailing any discrete audio token generation or vector quantization processes. It lacks discussion on discrete token formation, quantization methods, or their use in sequence modeling, which are core to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a GAN-based discriminator for Non-Autoregressive TTS to improve Mel-spectrogram generation quality, without proposing or detailing any discrete audio token generation or vector quantization processes. It lacks discussion on discrete token formation, quantization methods, or their use in sequence modeling, which are core to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Learning Deep Neural Networks for High Dimensional Output Problems",
    "abstract": "State-of-the-art pattern recognition methods have difficulties dealing with problems where the dimension of the output space is large. In this article, we propose a framework based on deep architectures (e. g. Deep Neural Networks) in order to deal with this issue. Deep architectures have proven to be efficient for high dimensional input problems such as image classification, due to their ability to embed the input space. The main contribution of this article is the extension of the embedding procedure to both the input and output spaces to easily handle complex outputs. Using this extension, inter-output dependencies can be modelled efficiently. This provides an interesting alternative to probabilistic models such as HMM and CRF. Preliminary experiments on toy datasets and USPS character reconstruction show promising results.",
    "metadata": {
      "title": "Learning Deep Neural Networks for High Dimensional Output Problems",
      "summary": "State-of-the-art pattern recognition methods have difficulties dealing with problems where the dimension of the output space is large. In this article, we propose a framework based on deep architectures (e. g. Deep Neural Networks) in order to deal with this issue. Deep architectures have proven to be efficient for high dimensional input problems such as image classification, due to their ability to embed the input space. The main contribution of this article is the extension of the embedding procedure to both the input and output spaces to easily handle complex outputs. Using this extension, inter-output dependencies can be modelled efficiently. This provides an interesting alternative to probabilistic models such as HMM and CRF. Preliminary experiments on toy datasets and USPS character reconstruction show promising results.",
      "abstract": "State-of-the-art pattern recognition methods have difficulties dealing with problems where the dimension of the output space is large. In this article, we propose a framework based on deep architectures (e. g. Deep Neural Networks) in order to deal with this issue. Deep architectures have proven to be efficient for high dimensional input problems such as image classification, due to their ability to embed the input space. The main contribution of this article is the extension of the embedding procedure to both the input and output spaces to easily handle complex outputs. Using this extension, inter-output dependencies can be modelled efficiently. This provides an interesting alternative to probabilistic models such as HMM and CRF. Preliminary experiments on toy datasets and USPS character reconstruction show promising results.",
      "doi": "https://doi.org/10.1109/icmla.2009.48",
      "openalex_id": "https://openalex.org/W2039225946",
      "arxiv_id": "",
      "publication_date": "2009-12-01",
      "published": "2009-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on deep neural nets for high-dimensional outputs and does not address discrete audio tokens, quantization, or token-based representations required by inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on deep neural nets for high-dimensional outputs and does not address discrete audio tokens, quantization, or token-based representations required by inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on deep neural networks for high dimensional output problems and embedding output and input spaces, without mentioning any discrete audio tokens, vector quantization, or audio token generation/usage relevant to audio language modeling or related applications. It does not satisfy the inclusion criteria related to discrete audio token design or application, nor is there indication of token generation methods or their use in sequence modeling tasks from audio data. Therefore, it does not meet the essential inclusion criteria and is not relevant to the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on deep neural networks for high dimensional output problems and embedding output and input spaces, without mentioning any discrete audio tokens, vector quantization, or audio token generation/usage relevant to audio language modeling or related applications. It does not satisfy the inclusion criteria related to discrete audio token design or application, nor is there indication of token generation methods or their use in sequence modeling tasks from audio data. Therefore, it does not meet the essential inclusion criteria and is not relevant to the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TeNC: Low Bit-Rate Speech Coding with VQ-VAE and GAN",
    "abstract": "Speech coding aims at compressing digital speech signals with fewer bits and reconstructing it back to raw signals, maintaining the speech quality as much as possible. But conventional codecs usually need a high bit-rate to achieve reconstructed speech with reasonable high quality. In this paper, we propose an end-to-end neural generative codec with a VQ-VAE based auto-encoder and the generative adversarial network (GAN), which achieves reconstructed speech with high-fidelity at a low bit-rate about 2 kb/s. The compression process of speech coding is carried out by a down-sampling module of the encoder and a learnable discrete codebook. GAN is used to further improve the reconstructed quality. Our experiments confirm the effectiveness of the proposed model in both objective and subjective tests, which significantly outperforms the conventional codecs at low bit-rate in terms of speech quality and speaker similarity.",
    "metadata": {
      "title": "TeNC: Low Bit-Rate Speech Coding with VQ-VAE and GAN",
      "summary": "Speech coding aims at compressing digital speech signals with fewer bits and reconstructing it back to raw signals, maintaining the speech quality as much as possible. But conventional codecs usually need a high bit-rate to achieve reconstructed speech with reasonable high quality. In this paper, we propose an end-to-end neural generative codec with a VQ-VAE based auto-encoder and the generative adversarial network (GAN), which achieves reconstructed speech with high-fidelity at a low bit-rate about 2 kb/s. The compression process of speech coding is carried out by a down-sampling module of the encoder and a learnable discrete codebook. GAN is used to further improve the reconstructed quality. Our experiments confirm the effectiveness of the proposed model in both objective and subjective tests, which significantly outperforms the conventional codecs at low bit-rate in terms of speech quality and speaker similarity.",
      "abstract": "Speech coding aims at compressing digital speech signals with fewer bits and reconstructing it back to raw signals, maintaining the speech quality as much as possible. But conventional codecs usually need a high bit-rate to achieve reconstructed speech with reasonable high quality. In this paper, we propose an end-to-end neural generative codec with a VQ-VAE based auto-encoder and the generative adversarial network (GAN), which achieves reconstructed speech with high-fidelity at a low bit-rate about 2 kb/s. The compression process of speech coding is carried out by a down-sampling module of the encoder and a learnable discrete codebook. GAN is used to further improve the reconstructed quality. Our experiments confirm the effectiveness of the proposed model in both objective and subjective tests, which significantly outperforms the conventional codecs at low bit-rate in terms of speech quality and speaker similarity.",
      "doi": "https://doi.org/10.1145/3461615.3491114",
      "openalex_id": "https://openalex.org/W4200219715",
      "arxiv_id": "",
      "publication_date": "2021-10-18",
      "published": "2021-10-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper uses a VQ-VAE with a discrete codebook to produce discrete audio tokens, satisfying the inclusion focus on discrete audio token representations, the source lacks a clear publication/update date, triggering exclusion per the specified exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper uses a VQ-VAE with a discrete codebook to produce discrete audio tokens, satisfying the inclusion focus on discrete audio token representations, the source lacks a clear publication/update date, triggering exclusion per the specified exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a neural generative codec using VQ-VAE and GAN for low bit-rate speech coding, emphasizing a learnable discrete codebook via vector quantization, which aligns with the inclusion criteria for discrete audio tokens as a core method with an explained token generation process; it also provides experimental evidence for quality and speaker similarity improvements, suggesting methodological transparency and applicability beyond traditional codecs which mainly focus on compression, fulfilling required criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a neural generative codec using VQ-VAE and GAN for low bit-rate speech coding, emphasizing a learnable discrete codebook via vector quantization, which aligns with the inclusion criteria for discrete audio tokens as a core method with an explained token generation process; it also provides experimental evidence for quality and speaker similarity improvements, suggesting methodological transparency and applicability beyond traditional codecs which mainly focus on compression, fulfilling required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorMini correctly notes the paper’s core method aligns with the “Discrete Audio Tokens” topic (VQ‑VAE with a learnable discrete codebook and experiments showing quality/similarity), and JuniorNano correctly flagged a missing publication/update date; because the item (as provided) lacks a clear YYYY‑MM‑DD publication or update timestamp — an explicit exclusion criterion — I must exclude it despite meeting the technical inclusion aspects. Explanation: Excluded because no clear publication/update date is provided, which violates the exclusion criteria even though the method fits the discrete-token topic.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorMini correctly notes the paper’s core method aligns with the “Discrete Audio Tokens” topic (VQ‑VAE with a learnable discrete codebook and experiments showing quality/similarity), and JuniorNano correctly flagged a missing publication/update date; because the item (as provided) lacks a clear YYYY‑MM‑DD publication or update timestamp — an explicit exclusion criterion — I must exclude it despite meeting the technical inclusion aspects. Explanation: Excluded because no clear publication/update date is provided, which violates the exclusion criteria even though the method fits the discrete-token topic.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Enhancing Hybrid Self-attention Structure with Relative-position-aware Bias for Speech Synthesis",
    "abstract": "Compared with the conventional \"front-end\"-\"back-end\"- \"vocoder\" structure, based on the attention mechanism, end-to-end speech synthesis systems directly train and synthesize from text sequence to the acoustic feature sequence as a whole. Recently, a more calculation efficient end-to-end architecture named transformer, which is solely based on self-attention, was proposed to model global dependencies between the input and output sequences. However, although with many advantages, transformer lacks position information in its structure. Moreover, the weighted sum form in self-attention may disperse the attention to the whole input sequence other than focusing on the more important neighbouring positions. In order to solve the above problems, this paper introduces a hybrid self-attention structure which combines self-attention with the recurrent neural networks (RNNs). We further enhance the proposed structure with relative-position-aware biases. Mean opinion score (MOS) test results indicate that by enhancing hybrid self-attention structure with relative-position-aware biases, the proposed system achieves the best performance with only 0.11 MOS score lower than natural recording.",
    "metadata": {
      "title": "Enhancing Hybrid Self-attention Structure with Relative-position-aware Bias for Speech Synthesis",
      "summary": "Compared with the conventional \"front-end\"-\"back-end\"- \"vocoder\" structure, based on the attention mechanism, end-to-end speech synthesis systems directly train and synthesize from text sequence to the acoustic feature sequence as a whole. Recently, a more calculation efficient end-to-end architecture named transformer, which is solely based on self-attention, was proposed to model global dependencies between the input and output sequences. However, although with many advantages, transformer lacks position information in its structure. Moreover, the weighted sum form in self-attention may disperse the attention to the whole input sequence other than focusing on the more important neighbouring positions. In order to solve the above problems, this paper introduces a hybrid self-attention structure which combines self-attention with the recurrent neural networks (RNNs). We further enhance the proposed structure with relative-position-aware biases. Mean opinion score (MOS) test results indicate that by enhancing hybrid self-attention structure with relative-position-aware biases, the proposed system achieves the best performance with only 0.11 MOS score lower than natural recording.",
      "abstract": "Compared with the conventional \"front-end\"-\"back-end\"- \"vocoder\" structure, based on the attention mechanism, end-to-end speech synthesis systems directly train and synthesize from text sequence to the acoustic feature sequence as a whole. Recently, a more calculation efficient end-to-end architecture named transformer, which is solely based on self-attention, was proposed to model global dependencies between the input and output sequences. However, although with many advantages, transformer lacks position information in its structure. Moreover, the weighted sum form in self-attention may disperse the attention to the whole input sequence other than focusing on the more important neighbouring positions. In order to solve the above problems, this paper introduces a hybrid self-attention structure which combines self-attention with the recurrent neural networks (RNNs). We further enhance the proposed structure with relative-position-aware biases. Mean opinion score (MOS) test results indicate that by enhancing hybrid self-attention structure with relative-position-aware biases, the proposed system achieves the best performance with only 0.11 MOS score lower than natural recording.",
      "doi": "https://doi.org/10.1109/icassp.2019.8682861",
      "openalex_id": "https://openalex.org/W2937909162",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a hybrid self-attention architecture with relative-position biases for end-to-end speech synthesis and does not address discrete audio tokens or tokenization/quantization methods, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a hybrid self-attention architecture with relative-position biases for end-to-end speech synthesis and does not address discrete audio tokens or tokenization/quantization methods, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving a hybrid self-attention structure for speech synthesis using relative-position-aware biases, but it does not describe any process of generating discrete audio tokens, vector quantization, or any form of tokenization of audio signals. The work centers on architectural enhancements in speech synthesis rather than the design or use of discrete audio tokens as a primary research contribution, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving a hybrid self-attention structure for speech synthesis using relative-position-aware biases, but it does not describe any process of generating discrete audio tokens, vector quantization, or any form of tokenization of audio signals. The work centers on architectural enhancements in speech synthesis rather than the design or use of discrete audio tokens as a primary research contribution, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic recognition of continuously spoken sentences from a finite state grammer",
    "abstract": "We report performance results on the recognition of continuously spoken sentences from the finite state grammar for the \"New Raleigh Language\" (vocabulary-250 words; average sentence length-8 words; entropy-2.86 bits/word; perplexity-7.27 words). Sentence and word error rates of 5% and 0.6% , respectively, are achieved, using a new centisecond-level model for the acoustic processor. We also report results for the \"CMU-AIX05 Language\" (vocabulary-1011 words; average sentence length-about 7 words; entropy-2.18 bits/word; perplexity-4.53 words), using both our earlier phone-level model and the centisecond-level model. With the phone-level acoustic-processor model, sentence and word error rates of 2% and 0.8%, respectively, are achieved. With the centisecond-level model, sentence and word error rates are 1% and 0.1%, respectively.",
    "metadata": {
      "title": "Automatic recognition of continuously spoken sentences from a finite state grammer",
      "summary": "We report performance results on the recognition of continuously spoken sentences from the finite state grammar for the \"New Raleigh Language\" (vocabulary-250 words; average sentence length-8 words; entropy-2.86 bits/word; perplexity-7.27 words). Sentence and word error rates of 5% and 0.6% , respectively, are achieved, using a new centisecond-level model for the acoustic processor. We also report results for the \"CMU-AIX05 Language\" (vocabulary-1011 words; average sentence length-about 7 words; entropy-2.18 bits/word; perplexity-4.53 words), using both our earlier phone-level model and the centisecond-level model. With the phone-level acoustic-processor model, sentence and word error rates of 2% and 0.8%, respectively, are achieved. With the centisecond-level model, sentence and word error rates are 1% and 0.1%, respectively.",
      "abstract": "We report performance results on the recognition of continuously spoken sentences from the finite state grammar for the \"New Raleigh Language\" (vocabulary-250 words; average sentence length-8 words; entropy-2.86 bits/word; perplexity-7.27 words). Sentence and word error rates of 5% and 0.6% , respectively, are achieved, using a new centisecond-level model for the acoustic processor. We also report results for the \"CMU-AIX05 Language\" (vocabulary-1011 words; average sentence length-about 7 words; entropy-2.18 bits/word; perplexity-4.53 words), using both our earlier phone-level model and the centisecond-level model. With the phone-level acoustic-processor model, sentence and word error rates of 2% and 0.8%, respectively, are achieved. With the centisecond-level model, sentence and word error rates are 1% and 0.1%, respectively.",
      "doi": "https://doi.org/10.1109/icassp.1978.1170404",
      "openalex_id": "https://openalex.org/W1919801718",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes ASR performance for sentences from a finite-state grammar and discusses models/entropy but does not present discrete audio token design, token generation/process, quantization (VQ/RVQ), or SSL-based discrete representations for downstream models, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes ASR performance for sentences from a finite-state grammar and discusses models/entropy but does not present discrete audio token design, token generation/process, quantization (VQ/RVQ), or SSL-based discrete representations for downstream models, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article reports on speech recognition performance using finite state grammar and acoustic modeling, but does not discuss the generation or use of discrete audio tokens through vector quantization or similar discrete representation mechanisms, nor does it detail token formation processes or their use in sequence/language models as required by the inclusion criteria. It focuses on speech recognition error rates without addressing discrete token design or representation relevant to audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article reports on speech recognition performance using finite state grammar and acoustic modeling, but does not discuss the generation or use of discrete audio tokens through vector quantization or similar discrete representation mechanisms, nor does it detail token formation processes or their use in sequence/language models as required by the inclusion criteria. It focuses on speech recognition error rates without addressing discrete token design or representation relevant to audio language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluation of a word recognition system using syntax analysis",
    "abstract": "A speech recognition system has been implemented which accepts reasonably natural English sentences spoken as isolated words. The major components of the system are a speaker dependent word recognizer and a syntax analyzer. The set of sentences selected for investigation is intended for use as requests in an automated flight information and reservation system. Results are presented of evaluations for speakers using their own stored reference patterns, the reference patterns of other speakers and reference patterns averaged over several speakers. For speakers using their own reference pattern the median word recognition error rate fell from 11.7% to 0.4% with the use of syntax analysis.",
    "metadata": {
      "title": "Evaluation of a word recognition system using syntax analysis",
      "summary": "A speech recognition system has been implemented which accepts reasonably natural English sentences spoken as isolated words. The major components of the system are a speaker dependent word recognizer and a syntax analyzer. The set of sentences selected for investigation is intended for use as requests in an automated flight information and reservation system. Results are presented of evaluations for speakers using their own stored reference patterns, the reference patterns of other speakers and reference patterns averaged over several speakers. For speakers using their own reference pattern the median word recognition error rate fell from 11.7% to 0.4% with the use of syntax analysis.",
      "abstract": "A speech recognition system has been implemented which accepts reasonably natural English sentences spoken as isolated words. The major components of the system are a speaker dependent word recognizer and a syntax analyzer. The set of sentences selected for investigation is intended for use as requests in an automated flight information and reservation system. Results are presented of evaluations for speakers using their own stored reference patterns, the reference patterns of other speakers and reference patterns averaged over several speakers. For speakers using their own reference pattern the median word recognition error rate fell from 11.7% to 0.4% with the use of syntax analysis.",
      "doi": "https://doi.org/10.1109/icassp.1977.1170183",
      "openalex_id": "https://openalex.org/W2169248114",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on a speaker-dependent word recognizer and a syntax analyzer for isolated-word speech recognition; it does not propose or evaluate discrete audio tokens, token generation/quantization, or token-based representations for language/sequence models, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on a speaker-dependent word recognizer and a syntax analyzer for isolated-word speech recognition; it does not propose or evaluate discrete audio tokens, token generation/quantization, or token-based representations for language/sequence models, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a speech recognition system incorporating syntax analysis that reduces word recognition error rates but do not mention any discrete audio token generation, vector quantization, or related methodologies required by the inclusion criteria. The study focuses on word recognition performance rather than the creation or use of discrete audio tokens for sequence modeling or language modeling, and lacks methodological details on tokenization or quantization processes. Therefore, it does not meet the inclusion criteria nor does it explicitly fall under any exclusion criteria; however, it is unrelated to discrete audio token research as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a speech recognition system incorporating syntax analysis that reduces word recognition error rates but do not mention any discrete audio token generation, vector quantization, or related methodologies required by the inclusion criteria. The study focuses on word recognition performance rather than the creation or use of discrete audio tokens for sequence modeling or language modeling, and lacks methodological details on tokenization or quantization processes. Therefore, it does not meet the inclusion criteria nor does it explicitly fall under any exclusion criteria; however, it is unrelated to discrete audio token research as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A database for speaker-independent digit recognition",
    "abstract": "A large speech database has been collected for use in designing and evaluating algorithms for speaker independent recognition of connected digit sequences. This dialect balanced database consists of more than 25 thousand digit sequences spoken by over 300 men, women, and children. The data were collected in a quiet environment and digitized at 20 KHz. Formal human listening tests on this database provided certification of the labelling of the digit sequences, and also provided information about human recognition performance and the inherent recognizability of the data.",
    "metadata": {
      "title": "A database for speaker-independent digit recognition",
      "summary": "A large speech database has been collected for use in designing and evaluating algorithms for speaker independent recognition of connected digit sequences. This dialect balanced database consists of more than 25 thousand digit sequences spoken by over 300 men, women, and children. The data were collected in a quiet environment and digitized at 20 KHz. Formal human listening tests on this database provided certification of the labelling of the digit sequences, and also provided information about human recognition performance and the inherent recognizability of the data.",
      "abstract": "A large speech database has been collected for use in designing and evaluating algorithms for speaker independent recognition of connected digit sequences. This dialect balanced database consists of more than 25 thousand digit sequences spoken by over 300 men, women, and children. The data were collected in a quiet environment and digitized at 20 KHz. Formal human listening tests on this database provided certification of the labelling of the digit sequences, and also provided information about human recognition performance and the inherent recognizability of the data.",
      "doi": "https://doi.org/10.1109/icassp.1984.1172716",
      "openalex_id": "https://openalex.org/W2152131029",
      "arxiv_id": "",
      "publication_date": "2005-03-24",
      "published": "2005-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a dataset paper about speaker-independent digit recognition and contains no discussion of discrete audio tokens, tokenization methods (e.g., VQ/RVQ/hierarchical/semantic tokens), or reproducible token-generation/quantization techniques, so it does not meet the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a dataset paper about speaker-independent digit recognition and contains no discussion of discrete audio tokens, tokenization methods (e.g., VQ/RVQ/hierarchical/semantic tokens), or reproducible token-generation/quantization techniques, so it does not meet the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study presents a large speech database for speaker-independent connected digit recognition with details about data collection and human recognition performance; however, it does not describe any discrete audio token generation method, vector quantization, or use of tokens for sequence/language models, which is the core inclusion criteria for this topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study presents a large speech database for speaker-independent connected digit recognition with details about data collection and human recognition performance; however, it does not describe any discrete audio token generation method, vector quantization, or use of tokens for sequence/language models, which is the core inclusion criteria for this topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Distance measures for speech processing",
    "abstract": "The properties and interrelationships among four measures of distance in speech processing are theoretically and experimentally discussed. The root mean square (rms) log spectral distance, cepstral distance, likelihood ratio (minimum residual principle or delta coding (DELCO) algorithm), and a cosh measure (based upon two nonsymmetrical likelihood ratios) are considered. It is shown that the cepstral measure bounds the rms log spectral measure from below, while the cosh measure bounds it from above. A simple nonlinear transformation of the likelihood ratio is shown to be highly correlated with the rms log spectral measure over expected ranges. Relationships between distance measure values and perception are also considered. The likelihood ratio, cepstral measure, and cosh measure are easily evaluated recursively from linear prediction filter coefficients, and each has a meaningful and interrelated frequency domain interpretation. Fortran programs are presented for computing the recursively evaluated distance measures.",
    "metadata": {
      "title": "Distance measures for speech processing",
      "summary": "The properties and interrelationships among four measures of distance in speech processing are theoretically and experimentally discussed. The root mean square (rms) log spectral distance, cepstral distance, likelihood ratio (minimum residual principle or delta coding (DELCO) algorithm), and a cosh measure (based upon two nonsymmetrical likelihood ratios) are considered. It is shown that the cepstral measure bounds the rms log spectral measure from below, while the cosh measure bounds it from above. A simple nonlinear transformation of the likelihood ratio is shown to be highly correlated with the rms log spectral measure over expected ranges. Relationships between distance measure values and perception are also considered. The likelihood ratio, cepstral measure, and cosh measure are easily evaluated recursively from linear prediction filter coefficients, and each has a meaningful and interrelated frequency domain interpretation. Fortran programs are presented for computing the recursively evaluated distance measures.",
      "abstract": "The properties and interrelationships among four measures of distance in speech processing are theoretically and experimentally discussed. The root mean square (rms) log spectral distance, cepstral distance, likelihood ratio (minimum residual principle or delta coding (DELCO) algorithm), and a cosh measure (based upon two nonsymmetrical likelihood ratios) are considered. It is shown that the cepstral measure bounds the rms log spectral measure from below, while the cosh measure bounds it from above. A simple nonlinear transformation of the likelihood ratio is shown to be highly correlated with the rms log spectral measure over expected ranges. Relationships between distance measure values and perception are also considered. The likelihood ratio, cepstral measure, and cosh measure are easily evaluated recursively from linear prediction filter coefficients, and each has a meaningful and interrelated frequency domain interpretation. Fortran programs are presented for computing the recursively evaluated distance measures.",
      "doi": "https://doi.org/10.1109/tassp.1976.1162849",
      "openalex_id": "https://openalex.org/W1989337816",
      "arxiv_id": "",
      "publication_date": "1976-10-01",
      "published": "1976-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses distance measures for speech processing rather than discrete audio token representations or tokenization methods (e.g., VQ/RVQ, SSL-based discrete tokens), hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses distance measures for speech processing rather than discrete audio token representations or tokenization methods (e.g., VQ/RVQ, SSL-based discrete tokens), hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses theoretical and experimental aspects of various distance measures in speech processing but does not present discrete audio token generation methods, quantization strategies, or cover discrete token design for language or sequence models. Hence, it does not meet the inclusion criteria focused on discrete audio token production and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses theoretical and experimental aspects of various distance measures in speech processing but does not present discrete audio token generation methods, quantization strategies, or cover discrete token design for language or sequence models. Hence, it does not meet the inclusion criteria focused on discrete audio token production and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A modified K-means clustering algorithm for use in isolated work recognition",
    "abstract": "Abstract-Studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the per-formance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophis-ticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but was impossible to reproduce exactly because it was highly dependent on decisions made by the experimenter. Subsequent work led to an auto-matic clustering procedure which, given only a set of clustering param-eters, clustered patterns with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since a naive user of such a statistical clus-tering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recogni-tion system. It is the purpose of this paper to present a clustering al-gorithm based on a standard K-means approach which requires no user parameter specification. Experimental data show that this new algo-rithm performs as well or better than the previously used clustering techniques when tested as part of a speaker-independent isolated word recognition system. P I.",
    "metadata": {
      "title": "A modified K-means clustering algorithm for use in isolated work recognition",
      "summary": "Abstract-Studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the per-formance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophis-ticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but was impossible to reproduce exactly because it was highly dependent on decisions made by the experimenter. Subsequent work led to an auto-matic clustering procedure which, given only a set of clustering param-eters, clustered patterns with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since a naive user of such a statistical clus-tering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recogni-tion system. It is the purpose of this paper to present a clustering al-gorithm based on a standard K-means approach which requires no user parameter specification. Experimental data show that this new algo-rithm performs as well or better than the previously used clustering techniques when tested as part of a speaker-independent isolated word recognition system. P I.",
      "abstract": "Abstract-Studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the per-formance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophis-ticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but was impossible to reproduce exactly because it was highly dependent on decisions made by the experimenter. Subsequent work led to an auto-matic clustering procedure which, given only a set of clustering param-eters, clustered patterns with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since a naive user of such a statistical clus-tering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recogni-tion system. It is the purpose of this paper to present a clustering al-gorithm based on a standard K-means approach which requires no user parameter specification. Experimental data show that this new algo-rithm performs as well or better than the previously used clustering techniques when tested as part of a speaker-independent isolated word recognition system. P I.",
      "doi": "https://doi.org/10.1109/tassp.1985.1164581",
      "openalex_id": "https://openalex.org/W2048648518",
      "arxiv_id": "",
      "publication_date": "1985-06-01",
      "published": "1985-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents a modified K-means clustering algorithm for isolated word recognition focusing on template clustering, not on discrete audio token design (e.g., VQ/RVQ-based tokens for language-model-style sequence modeling), so it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper presents a modified K-means clustering algorithm for isolated word recognition focusing on template clustering, not on discrete audio token design (e.g., VQ/RVQ-based tokens for language-model-style sequence modeling), so it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a modified K-means clustering algorithm for isolated word recognition systems, primarily discussing clustering parameters and pattern recognition; however, it does not clearly address the generation of discrete audio tokens via vector quantization, residual quantization, or similar methods, nor does it describe a token generation process or its use for sequence/language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token design and its application for audio language modeling or audio understanding/generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a modified K-means clustering algorithm for isolated word recognition systems, primarily discussing clustering parameters and pattern recognition; however, it does not clearly address the generation of discrete audio tokens via vector quantization, residual quantization, or similar methods, nor does it describe a token generation process or its use for sequence/language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token design and its application for audio language modeling or audio understanding/generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An introduction to hidden Markov models",
    "abstract": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.",
    "metadata": {
      "title": "An introduction to hidden Markov models",
      "summary": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.",
      "abstract": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.",
      "doi": "https://doi.org/10.1109/massp.1986.1165342",
      "openalex_id": "https://openalex.org/W2105594594",
      "arxiv_id": "",
      "publication_date": "1986-01-01",
      "published": "1986-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe an introduction to hidden Markov models for speech recognition, not discrete audio tokens or tokenization methods; it lacks the required tokenization/quantization details and core contributions about discrete audio token representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe an introduction to hidden Markov models for speech recognition, not discrete audio tokens or tokenization methods; it lacks the required tokenization/quantization details and core contributions about discrete audio token representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces the theory of hidden Markov models applied to speech processing but does not focus on discrete audio token design or discrete representation generation via vector quantization or related approaches; it lacks discussion on token generation processes or usage in sequence/language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces the theory of hidden Markov models applied to speech processing but does not focus on discrete audio token design or discrete representation generation via vector quantization or related approaches; it lacks discussion on token generation processes or usage in sequence/language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Computers: Speech recognition: Turning theory to practice: New ICs have brought the requisite computer power to speech technology; an evaluation of equipment shows where it stands today",
    "abstract": "Presents an evaluation of the equipment now available for turning the theory of electronic speech recognition into practice. The fulfilment of this goal seems much closer than it did because of the pace of advance in IC technology.",
    "metadata": {
      "title": "Computers: Speech recognition: Turning theory to practice: New ICs have brought the requisite computer power to speech technology; an evaluation of equipment shows where it stands today",
      "summary": "Presents an evaluation of the equipment now available for turning the theory of electronic speech recognition into practice. The fulfilment of this goal seems much closer than it did because of the pace of advance in IC technology.",
      "abstract": "Presents an evaluation of the equipment now available for turning the theory of electronic speech recognition into practice. The fulfilment of this goal seems much closer than it did because of the pace of advance in IC technology.",
      "doi": "https://doi.org/10.1109/mspec.1981.6369809",
      "openalex_id": "https://openalex.org/W2065625684",
      "arxiv_id": "",
      "publication_date": "1981-09-01",
      "published": "1981-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses hardware evaluation for speech recognition and does not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses hardware evaluation for speech recognition and does not address discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on evaluating equipment and the practical realization of speech recognition enabled by new ICs, without mentioning discrete audio tokens, tokenization processes, quantization mechanisms, or their use for language or sequence modeling; therefore, it fails to meet the inclusion criteria centered around discrete audio token design and description, and there is no detailed methodological disclosure to support replicability or comparison.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on evaluating equipment and the practical realization of speech recognition enabled by new ICs, without mentioning discrete audio tokens, tokenization processes, quantization mechanisms, or their use for language or sequence modeling; therefore, it fails to meet the inclusion criteria centered around discrete audio token design and description, and there is no detailed methodological disclosure to support replicability or comparison.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous speech recognition by statistical methods",
    "abstract": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",
    "metadata": {
      "title": "Continuous speech recognition by statistical methods",
      "summary": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",
      "abstract": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",
      "doi": "https://doi.org/10.1109/proc.1976.10159",
      "openalex_id": "https://openalex.org/W1990005915",
      "arxiv_id": "",
      "publication_date": "1976-01-01",
      "published": "1976-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on statistical methods for continuous speech recognition and does not discuss discrete audio tokens or tokenization; thus it does not satisfy the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on statistical methods for continuous speech recognition and does not discuss discrete audio tokens or tokenization; thus it does not satisfy the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses statistical methods for continuous speech recognition, focusing on acoustic modeling and linguistic decoding without mentioning any discrete audio token generation, vector quantization, or related discrete representation methods central to current research on discrete audio tokens; thus, it lacks the core contribution of discrete token design or usage as defined in the inclusion criteria, and does not provide details on token generation or usage for language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses statistical methods for continuous speech recognition, focusing on acoustic modeling and linguistic decoding without mentioning any discrete audio token generation, vector quantization, or related discrete representation methods central to current research on discrete audio tokens; thus, it lacks the core contribution of discrete token design or usage as defined in the inclusion criteria, and does not provide details on token generation or usage for language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Structural methods in automatic speech recognition",
    "abstract": "The past decade has witnessed substantial progress toward the goal of constructing a machine capable of understanding colloquial discourse. Central to this progress has been the development and application of mathematical methods that permit modeling the speech signal as a complex code with several coexisting levels of structure. The most successful of these are \"template matching,\" stochastic modeling, and probabilistic parsing. The manifestation of common themes such as dynamic programming and finite-state descriptions accentuates a superficial likeness amongst the methods which is often mistaken for the deeper similarity arising from their shared Bayesian foundation. In this paper, we outline the mathematical bases of these methods, invariant metrics, hidden Markov chains, and formal grammars, respectively. We then recount and briefly interpret the results of experiments in speech recognition to which the various methods were applied. Since these mathematical principles seem to bear little resemblance to traditional linguistic characterizations of speech, the success of the experiments is occasionally attributed, even by their authors, merely to excellent engineering. We conclude by speculating that, quite to the contrary, these methods actually constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers.",
    "metadata": {
      "title": "Structural methods in automatic speech recognition",
      "summary": "The past decade has witnessed substantial progress toward the goal of constructing a machine capable of understanding colloquial discourse. Central to this progress has been the development and application of mathematical methods that permit modeling the speech signal as a complex code with several coexisting levels of structure. The most successful of these are \"template matching,\" stochastic modeling, and probabilistic parsing. The manifestation of common themes such as dynamic programming and finite-state descriptions accentuates a superficial likeness amongst the methods which is often mistaken for the deeper similarity arising from their shared Bayesian foundation. In this paper, we outline the mathematical bases of these methods, invariant metrics, hidden Markov chains, and formal grammars, respectively. We then recount and briefly interpret the results of experiments in speech recognition to which the various methods were applied. Since these mathematical principles seem to bear little resemblance to traditional linguistic characterizations of speech, the success of the experiments is occasionally attributed, even by their authors, merely to excellent engineering. We conclude by speculating that, quite to the contrary, these methods actually constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers.",
      "abstract": "The past decade has witnessed substantial progress toward the goal of constructing a machine capable of understanding colloquial discourse. Central to this progress has been the development and application of mathematical methods that permit modeling the speech signal as a complex code with several coexisting levels of structure. The most successful of these are \"template matching,\" stochastic modeling, and probabilistic parsing. The manifestation of common themes such as dynamic programming and finite-state descriptions accentuates a superficial likeness amongst the methods which is often mistaken for the deeper similarity arising from their shared Bayesian foundation. In this paper, we outline the mathematical bases of these methods, invariant metrics, hidden Markov chains, and formal grammars, respectively. We then recount and briefly interpret the results of experiments in speech recognition to which the various methods were applied. Since these mathematical principles seem to bear little resemblance to traditional linguistic characterizations of speech, the success of the experiments is occasionally attributed, even by their authors, merely to excellent engineering. We conclude by speculating that, quite to the contrary, these methods actually constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers.",
      "doi": "https://doi.org/10.1109/proc.1985.13344",
      "openalex_id": "https://openalex.org/W1975598412",
      "arxiv_id": "",
      "publication_date": "1985-01-01",
      "published": "1985-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses structural methods in ASR and Bayesian foundations but does not address discrete audio tokens, tokenization methods, or token-based downstream modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses structural methods in ASR and Bayesian foundations but does not address discrete audio tokens, tokenization methods, or token-based downstream modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on mathematical methods in automatic speech recognition, discussing stochastic modeling and probabilistic parsing, but it does not describe the creation or use of discrete audio tokens through vector quantization or related techniques, nor does it provide details on token generation or their application in sequence modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on mathematical methods in automatic speech recognition, discussing stochastic modeling and probabilistic parsing, but it does not describe the creation or use of discrete audio tokens through vector quantization or related techniques, nor does it provide details on token generation or their application in sequence modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Practical applications of voice input to machines",
    "abstract": "Voice input to machine is the most natural form of man-machine communications. In this type of system the machine responds to the mode of communications preferred by the user, rather than vice versa. Many practical applications exist today for limited capability voice input systems. The first operational voice input systems have taken place with limited vocabulary, isolated word voice input systems. Most of these initial systems were for industrial applications in which the users' hands or eyes were already busy with their normal work requirements. Future developments in both new applications and increased capability voice input systems can be expected to considerably expand the usage of this form of man-machine communications.",
    "metadata": {
      "title": "Practical applications of voice input to machines",
      "summary": "Voice input to machine is the most natural form of man-machine communications. In this type of system the machine responds to the mode of communications preferred by the user, rather than vice versa. Many practical applications exist today for limited capability voice input systems. The first operational voice input systems have taken place with limited vocabulary, isolated word voice input systems. Most of these initial systems were for industrial applications in which the users' hands or eyes were already busy with their normal work requirements. Future developments in both new applications and increased capability voice input systems can be expected to considerably expand the usage of this form of man-machine communications.",
      "abstract": "Voice input to machine is the most natural form of man-machine communications. In this type of system the machine responds to the mode of communications preferred by the user, rather than vice versa. Many practical applications exist today for limited capability voice input systems. The first operational voice input systems have taken place with limited vocabulary, isolated word voice input systems. Most of these initial systems were for industrial applications in which the users' hands or eyes were already busy with their normal work requirements. Future developments in both new applications and increased capability voice input systems can be expected to considerably expand the usage of this form of man-machine communications.",
      "doi": "https://doi.org/10.1109/proc.1976.10157",
      "openalex_id": "https://openalex.org/W2144195083",
      "arxiv_id": "",
      "publication_date": "1976-01-01",
      "published": "1976-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract discusses practical voice input applications and future developments, but it does not describe discrete audio token representations or tokenization methods, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract discusses practical voice input applications and future developments, but it does not describe discrete audio token representations or tokenization methods, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on practical applications of voice input systems primarily for man-machine communication and industrial scenarios, without mentioning discrete audio token generation, vector quantization, or related methods defined in the inclusion criteria; it lacks details on tokenization processes or use of discrete tokens for sequence modeling which are essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on practical applications of voice input systems primarily for man-machine communication and industrial scenarios, without mentioning discrete audio token generation, vector quantization, or related methods defined in the inclusion criteria; it lacks details on tokenization processes or use of discrete tokens for sequence modeling which are essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Isolated and Connected Word Recognition--Theory and Selected Applications",
    "abstract": "The art and science of speech recognition have been advanced to the state where it is now possible to communicate reliably with a computer by speaking to it in a disciplined manner using a vocabulary of moderate size. It is the purpose of this paper to outline two aspects of speech-recognition research. First, we discuss word recognition as a classical pattern-recognition problem and show how some fundamental concepts of signal processing, information theory, and computer science can be combined to give us the capability of robust recognition of isolated words and simple connected word sequences. We then describe methods whereby these principles, augmented by modern theories of formal language and semantic analysis, can be used to study some of the more general problems in speech recognition. It is anticipated that these methods will ultimately lead to accurate mechanical recognition of fluent speech under certain controlled conditions.",
    "metadata": {
      "title": "Isolated and Connected Word Recognition--Theory and Selected Applications",
      "summary": "The art and science of speech recognition have been advanced to the state where it is now possible to communicate reliably with a computer by speaking to it in a disciplined manner using a vocabulary of moderate size. It is the purpose of this paper to outline two aspects of speech-recognition research. First, we discuss word recognition as a classical pattern-recognition problem and show how some fundamental concepts of signal processing, information theory, and computer science can be combined to give us the capability of robust recognition of isolated words and simple connected word sequences. We then describe methods whereby these principles, augmented by modern theories of formal language and semantic analysis, can be used to study some of the more general problems in speech recognition. It is anticipated that these methods will ultimately lead to accurate mechanical recognition of fluent speech under certain controlled conditions.",
      "abstract": "The art and science of speech recognition have been advanced to the state where it is now possible to communicate reliably with a computer by speaking to it in a disciplined manner using a vocabulary of moderate size. It is the purpose of this paper to outline two aspects of speech-recognition research. First, we discuss word recognition as a classical pattern-recognition problem and show how some fundamental concepts of signal processing, information theory, and computer science can be combined to give us the capability of robust recognition of isolated words and simple connected word sequences. We then describe methods whereby these principles, augmented by modern theories of formal language and semantic analysis, can be used to study some of the more general problems in speech recognition. It is anticipated that these methods will ultimately lead to accurate mechanical recognition of fluent speech under certain controlled conditions.",
      "doi": "https://doi.org/10.1109/tcom.1981.1095031",
      "openalex_id": "https://openalex.org/W2057833190",
      "arxiv_id": "",
      "publication_date": "1981-05-01",
      "published": "1981-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns classical speech recognition and word recognition using signal processing and information theory, but it does not present discrete audio token representations (VQ/RVQ/semantic discretization) or token-based models as core contributions; therefore it fails the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns classical speech recognition and word recognition using signal processing and information theory, but it does not present discrete audio token representations (VQ/RVQ/semantic discretization) or token-based models as core contributions; therefore it fails the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses classical approaches to isolated and connected word recognition focused on speech recognition using fundamental signal processing and pattern recognition concepts; however, it does not mention or focus on discrete audio token generation, vector quantization, or related discrete token representations suitable for language or sequence modeling as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria of studying or proposing discrete audio token mechanisms with detailed methodology or usage in downstream generative or understanding models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses classical approaches to isolated and connected word recognition focused on speech recognition using fundamental signal processing and pattern recognition concepts; however, it does not mention or focus on discrete audio token generation, vector quantization, or related discrete token representations suitable for language or sequence modeling as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria of studying or proposing discrete audio token mechanisms with detailed methodology or usage in downstream generative or understanding models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A modified K-means clustering algorithm for use in speaker-independent isolated word recognition",
    "abstract": "Recent studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the performance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophisticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but it was impossible to reproduce exactly, because it was highly dependent on decisions made by the experimenter. Subsequent work led to an automatic clustering procedure which, given only a set of clustering parameters, clustered tokens with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since the user of such a statistical clustering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recognition system. It is the purpose of this paper to present a new clustering algorithm based on a K-means approach which requires no user parameter specification. Experimental data show that this new algorithm performs as well or better than the previously used clustering techniques when tested as part of a speaker independent isolated word recognition system.",
    "metadata": {
      "title": "A modified K-means clustering algorithm for use in speaker-independent isolated word recognition",
      "summary": "Recent studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the performance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophisticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but it was impossible to reproduce exactly, because it was highly dependent on decisions made by the experimenter. Subsequent work led to an automatic clustering procedure which, given only a set of clustering parameters, clustered tokens with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since the user of such a statistical clustering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recognition system. It is the purpose of this paper to present a new clustering algorithm based on a K-means approach which requires no user parameter specification. Experimental data show that this new algorithm performs as well or better than the previously used clustering techniques when tested as part of a speaker independent isolated word recognition system.",
      "abstract": "Recent studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the performance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophisticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but it was impossible to reproduce exactly, because it was highly dependent on decisions made by the experimenter. Subsequent work led to an automatic clustering procedure which, given only a set of clustering parameters, clustered tokens with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since the user of such a statistical clustering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recognition system. It is the purpose of this paper to present a new clustering algorithm based on a K-means approach which requires no user parameter specification. Experimental data show that this new algorithm performs as well or better than the previously used clustering techniques when tested as part of a speaker independent isolated word recognition system.",
      "doi": "https://doi.org/10.1121/1.2021691",
      "openalex_id": "https://openalex.org/W1994859265",
      "arxiv_id": "",
      "publication_date": "1984-05-01",
      "published": "1984-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper discusses a modified K-means clustering approach for speaker-independent isolated word recognition with templates, but it does not address discrete audio token design, quantization, or token-based modeling as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper discusses a modified K-means clustering approach for speaker-independent isolated word recognition with templates, but it does not address discrete audio token design, quantization, or token-based modeling as defined in the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a modified K-means clustering algorithm for speaker-independent isolated word recognition and does not describe the generation or use of discrete audio tokens transformed from continuous audio signals for language or sequence modeling, nor does it specify token quantization or semantic disentanglement. Therefore, it does not meet the inclusion criteria relating to discrete audio token design and usage, and it is not primarily about discrete token representation for audio understanding or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a modified K-means clustering algorithm for speaker-independent isolated word recognition and does not describe the generation or use of discrete audio tokens transformed from continuous audio signals for language or sequence modeling, nor does it specify token quantization or semantic disentanglement. Therefore, it does not meet the inclusion criteria relating to discrete audio token design and usage, and it is not primarily about discrete token representation for audio understanding or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sequence to Sequence Neural Speech Synthesis with Prosody Modification Capabilities",
    "abstract": "Modern sequence to sequence neural TTS systems provide close to natural speech quality. Such systems usually comprise a network converting linguistic/phonetic features sequence to an acoustic features sequence, cascaded with a neural vocoder. The generated speech prosody (i.e. phoneme durations, pitch and loudness) is implicitly present in the acoustic features, being mixed with spectral information. Although the speech sounds natural, its prosody realization is randomly chosen and cannot be easily altered. The prosody control becomes an even more difficult task if no prosodic labeling is present in the training data. Recently, much progress has been achieved in unsupervised speaking style learning and generation, however human inspection is still required after the training for discovery and interpretation of the speaking styles learned by the system. In this work we introduce a fully automatic method that makes the system aware of the prosody and enables sentence-wise speaking pace and expressiveness control on a continuous scale. While being useful by itself in many applications, the proposed prosody control can also improve the overall quality and expressiveness of the synthesized speech, as demonstrated by subjective listening evaluations. We also propose a novel augmented attention mechanism, that facilitates better pace control sensitivity and faster attention convergence.",
    "metadata": {
      "title": "Sequence to Sequence Neural Speech Synthesis with Prosody Modification Capabilities",
      "summary": "Modern sequence to sequence neural TTS systems provide close to natural speech quality. Such systems usually comprise a network converting linguistic/phonetic features sequence to an acoustic features sequence, cascaded with a neural vocoder. The generated speech prosody (i.e. phoneme durations, pitch and loudness) is implicitly present in the acoustic features, being mixed with spectral information. Although the speech sounds natural, its prosody realization is randomly chosen and cannot be easily altered. The prosody control becomes an even more difficult task if no prosodic labeling is present in the training data. Recently, much progress has been achieved in unsupervised speaking style learning and generation, however human inspection is still required after the training for discovery and interpretation of the speaking styles learned by the system. In this work we introduce a fully automatic method that makes the system aware of the prosody and enables sentence-wise speaking pace and expressiveness control on a continuous scale. While being useful by itself in many applications, the proposed prosody control can also improve the overall quality and expressiveness of the synthesized speech, as demonstrated by subjective listening evaluations. We also propose a novel augmented attention mechanism, that facilitates better pace control sensitivity and faster attention convergence.",
      "abstract": "Modern sequence to sequence neural TTS systems provide close to natural speech quality. Such systems usually comprise a network converting linguistic/phonetic features sequence to an acoustic features sequence, cascaded with a neural vocoder. The generated speech prosody (i.e. phoneme durations, pitch and loudness) is implicitly present in the acoustic features, being mixed with spectral information. Although the speech sounds natural, its prosody realization is randomly chosen and cannot be easily altered. The prosody control becomes an even more difficult task if no prosodic labeling is present in the training data. Recently, much progress has been achieved in unsupervised speaking style learning and generation, however human inspection is still required after the training for discovery and interpretation of the speaking styles learned by the system. In this work we introduce a fully automatic method that makes the system aware of the prosody and enables sentence-wise speaking pace and expressiveness control on a continuous scale. While being useful by itself in many applications, the proposed prosody control can also improve the overall quality and expressiveness of the synthesized speech, as demonstrated by subjective listening evaluations. We also propose a novel augmented attention mechanism, that facilitates better pace control sensitivity and faster attention convergence.",
      "doi": "https://doi.org/10.21437/ssw.2019-49",
      "openalex_id": "https://openalex.org/W2974194285",
      "arxiv_id": "",
      "publication_date": "2019-09-14",
      "published": "2019-09-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文主旨為在 TTS 中加入韻律控制與增強注意力機制，未提出離散音訊 token 的設計、量化流程或能被序列模型使用的離散表徵，因此不符合本主題之離散 token 研究納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文主旨為在 TTS 中加入韻律控制與增強注意力機制，未提出離散音訊 token 的設計、量化流程或能被序列模型使用的離散表徵，因此不符合本主題之離散 token 研究納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses sequence to sequence neural TTS with prosody modification, focusing on prosody control and a novel attention mechanism. However, it does not mention the generation or use of discrete audio tokens derived via vector quantization or similar methods as a core contribution, nor does it detail any tokenization or discrete representation process that fits the inclusion criteria. The work appears to be about continuous acoustic feature prediction and prosody modification rather than discrete token creation or usage for language modeling or sequence modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token design or utilization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses sequence to sequence neural TTS with prosody modification, focusing on prosody control and a novel attention mechanism. However, it does not mention the generation or use of discrete audio tokens derived via vector quantization or similar methods as a core contribution, nor does it detail any tokenization or discrete representation process that fits the inclusion criteria. The work appears to be about continuous acoustic feature prediction and prosody modification rather than discrete token creation or usage for language modeling or sequence modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token design or utilization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
    "abstract": "Modern neural text-to-speech (TTS) synthesis can generate speech that is indistinguishable from natural speech.However, the prosody of generated utterances often represents the average prosodic style of the database instead of having wide prosodic variation.Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence.In this work, we train a sequence-to-sequence neural network conditioned on acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions.Experiments show that a model conditioned on sentencewise pitch, pitch range, phone duration, energy, and spectral tilt can effectively control each prosodic dimension and generate a wide variety of speaking styles, while maintaining similar mean opinion score (4.23) to our Tacotron baseline (4.26).",
    "metadata": {
      "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
      "summary": "Modern neural text-to-speech (TTS) synthesis can generate speech that is indistinguishable from natural speech.However, the prosody of generated utterances often represents the average prosodic style of the database instead of having wide prosodic variation.Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence.In this work, we train a sequence-to-sequence neural network conditioned on acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions.Experiments show that a model conditioned on sentencewise pitch, pitch range, phone duration, energy, and spectral tilt can effectively control each prosodic dimension and generate a wide variety of speaking styles, while maintaining similar mean opinion score (4.23) to our Tacotron baseline (4.26).",
      "abstract": "Modern neural text-to-speech (TTS) synthesis can generate speech that is indistinguishable from natural speech.However, the prosody of generated utterances often represents the average prosodic style of the database instead of having wide prosodic variation.Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence.In this work, we train a sequence-to-sequence neural network conditioned on acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions.Experiments show that a model conditioned on sentencewise pitch, pitch range, phone duration, energy, and spectral tilt can effectively control each prosodic dimension and generate a wide variety of speaking styles, while maintaining similar mean opinion score (4.23) to our Tacotron baseline (4.26).",
      "doi": "https://doi.org/10.21437/interspeech.2020-2861",
      "openalex_id": "https://openalex.org/W3097892637",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets controllable TTS prosody via a latent continuous prosody space and explicit acoustic features, but it does not introduce discrete audio tokens or tokenization/quantization suitable for audio language modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets controllable TTS prosody via a latent continuous prosody space and explicit acoustic features, but it does not introduce discrete audio tokens or tokenization/quantization suitable for audio language modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural text-to-speech synthesis conditioned on acoustic features to control prosody, but it does not mention any discrete audio tokens, vector quantization, or tokenization processes used for sequence or language modeling; it also lacks description of token generation methods or usage relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural text-to-speech synthesis conditioned on acoustic features to control prosody, but it does not mention any discrete audio tokens, vector quantization, or tokenization processes used for sequence or language modeling; it also lacks description of token generation methods or usage relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonemic-level Duration Control Using Attention Alignment for Natural Speech Synthesis",
    "abstract": "Recent attention-based end-to-end speech synthesis from text systems have achieved human-level performance. However, many approaches cause a sequence-to-sequence model to generate only averaged results of the input text, making it difficult to control the duration of utterance. In this study, we present a novel mechanism for phonemic-level duration control (PDC) in a nearly end-to-end manner in order to solve this problem. We used a teacher attention alignment generated by an annotation speech analyzer program. Our method is inspired by the idea that the duration of a phoneme is highly related to its phonemic features. These phonemic features are saved on the attention alignment by adding duration embedding to it. This enables the model to learn and control the phonemic and rhythmic features of speech. We also show that providing alignment information as a teacher loss term improves training speed and notably, makes the model better at controlling the speed of dramatic change in phonemic-level duration with subjective demonstration. As a result, we show that our PDC speech synthesis with alignment loss outperforms other baseline methods without losing the ability to control the duration of phonemes in extremely adjusted environments with faster convergence.",
    "metadata": {
      "title": "Phonemic-level Duration Control Using Attention Alignment for Natural Speech Synthesis",
      "summary": "Recent attention-based end-to-end speech synthesis from text systems have achieved human-level performance. However, many approaches cause a sequence-to-sequence model to generate only averaged results of the input text, making it difficult to control the duration of utterance. In this study, we present a novel mechanism for phonemic-level duration control (PDC) in a nearly end-to-end manner in order to solve this problem. We used a teacher attention alignment generated by an annotation speech analyzer program. Our method is inspired by the idea that the duration of a phoneme is highly related to its phonemic features. These phonemic features are saved on the attention alignment by adding duration embedding to it. This enables the model to learn and control the phonemic and rhythmic features of speech. We also show that providing alignment information as a teacher loss term improves training speed and notably, makes the model better at controlling the speed of dramatic change in phonemic-level duration with subjective demonstration. As a result, we show that our PDC speech synthesis with alignment loss outperforms other baseline methods without losing the ability to control the duration of phonemes in extremely adjusted environments with faster convergence.",
      "abstract": "Recent attention-based end-to-end speech synthesis from text systems have achieved human-level performance. However, many approaches cause a sequence-to-sequence model to generate only averaged results of the input text, making it difficult to control the duration of utterance. In this study, we present a novel mechanism for phonemic-level duration control (PDC) in a nearly end-to-end manner in order to solve this problem. We used a teacher attention alignment generated by an annotation speech analyzer program. Our method is inspired by the idea that the duration of a phoneme is highly related to its phonemic features. These phonemic features are saved on the attention alignment by adding duration embedding to it. This enables the model to learn and control the phonemic and rhythmic features of speech. We also show that providing alignment information as a teacher loss term improves training speed and notably, makes the model better at controlling the speed of dramatic change in phonemic-level duration with subjective demonstration. As a result, we show that our PDC speech synthesis with alignment loss outperforms other baseline methods without losing the ability to control the duration of phonemes in extremely adjusted environments with faster convergence.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683827",
      "openalex_id": "https://openalex.org/W2938102059",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses phonemic-level duration control in TTS using attention alignment and duration embeddings, but does not address discrete audio token generation/quantization or token-based representations; thus it does not satisfy the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses phonemic-level duration control in TTS using attention alignment and duration embeddings, but does not address discrete audio token generation/quantization or token-based representations; thus it does not satisfy the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonemic-level duration control in speech synthesis using attention alignment and duration embeddings, but it does not mention any process of converting continuous audio signals into discrete tokens or vector quantization mechanisms used to produce discrete audio tokens for language or sequence modeling, which is the core inclusion criterion; thus, the paper does not meet the main inclusion requirements concerning discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonemic-level duration control in speech synthesis using attention alignment and duration embeddings, but it does not mention any process of converting continuous audio signals into discrete tokens or vector quantization mechanisms used to produce discrete audio tokens for language or sequence modeling, which is the core inclusion criterion; thus, the paper does not meet the main inclusion requirements concerning discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency",
    "abstract": "This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests.",
    "metadata": {
      "title": "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency",
      "summary": "This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests.",
      "abstract": "This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2464",
      "openalex_id": "https://openalex.org/W3095389792",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets low-latency streaming TTS using Tacotron-like models and LPCNet, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets low-latency streaming TTS using Tacotron-like models and LPCNet, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a low-latency streaming text-to-speech system using an autoregressive attention-based acoustic model and LPCNet vocoder, but does not discuss the generation or use of discrete audio tokens (e.g., vector quantization, residual vector quantization, or semantic disentanglement) as core methodology, nor does it provide details of tokenization or quantization processes relevant to discrete audio tokens as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria of having discrete token design or generation as a main contribution, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a low-latency streaming text-to-speech system using an autoregressive attention-based acoustic model and LPCNet vocoder, but does not discuss the generation or use of discrete audio tokens (e.g., vector quantization, residual vector quantization, or semantic disentanglement) as core methodology, nor does it provide details of tokenization or quantization processes relevant to discrete audio tokens as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria of having discrete token design or generation as a main contribution, and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ACCURATE SHORT-TERM ANALYSIS OF THE FUNDAMENTAL FREQUENCY AND THE HARMONICS-TO-NOISE RATIO OF A SAMPLED SOUND",
    "abstract": "We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for pitch detection, and to the exclusive use of frequency-domain methods for the determination of the harmonics-to-noise ratio.",
    "metadata": {
      "title": "ACCURATE SHORT-TERM ANALYSIS OF THE FUNDAMENTAL FREQUENCY AND THE HARMONICS-TO-NOISE RATIO OF A SAMPLED SOUND",
      "summary": "We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for pitch detection, and to the exclusive use of frequency-domain methods for the determination of the harmonics-to-noise ratio.",
      "abstract": "We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for pitch detection, and to the exclusive use of frequency-domain methods for the determination of the harmonics-to-noise ratio.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2107831318",
      "arxiv_id": "",
      "publication_date": "1993-01-01",
      "published": "1993-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on autocorrelation-based pitch and harmonics-to-noise analysis without any discrete audio tokenization or token design for language/sequence modeling, so it does not meet the inclusion criteria centered on discrete audio tokens; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on autocorrelation-based pitch and harmonics-to-noise analysis without any discrete audio tokenization or token design for language/sequence modeling, so it does not meet the inclusion criteria centered on discrete audio tokens; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an algorithm for detecting periodicity, fundamental frequency, and harmonics-to-noise ratio in sampled sounds using autocorrelation methods, without discussing discrete audio tokenization processes such as vector quantization or the generation of discrete tokens for language/sequential models as defined in the inclusion criteria. There is no mention of token generation, quantization steps, token usage for audio language modeling or cross-modal tasks. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and their generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an algorithm for detecting periodicity, fundamental frequency, and harmonics-to-noise ratio in sampled sounds using autocorrelation methods, without discussing discrete audio tokenization processes such as vector quantization or the generation of discrete tokens for language/sequential models as defined in the inclusion criteria. There is no mention of token generation, quantization steps, token usage for audio language modeling or cross-modal tasks. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and their generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis",
    "abstract": "We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. Audio samples are available on the web.",
    "metadata": {
      "title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis",
      "summary": "We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. Audio samples are available on the web.",
      "abstract": "We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. Audio samples are available on the web.",
      "doi": "https://doi.org/10.48550/arxiv.1910.01709",
      "openalex_id": "https://openalex.org/W2977311057",
      "arxiv_id": "",
      "publication_date": "2019-10-03",
      "published": "2019-10-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address Discrete Audio Tokens or token-based discretization (no tokenization/quantization method described), so it fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address Discrete Audio Tokens or token-based discretization (no tokenization/quantization method described), so it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a semi-supervised generative model for controllable speech synthesis but does not mention generating discrete audio tokens or the process of converting continuous audio into discrete tokens using vector quantization or similar techniques. There is no clear explanation of token generation, quantization, or how tokens are used for language or sequence modeling, which are core to the inclusion criteria. Therefore, it does not meet the main requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a semi-supervised generative model for controllable speech synthesis but does not mention generating discrete audio tokens or the process of converting continuous audio into discrete tokens using vector quantization or similar techniques. There is no clear explanation of token generation, quantization, or how tokens are used for language or sequence modeling, which are core to the inclusion criteria. Therefore, it does not meet the main requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Prosody Transfer in Neural Text to Speech Using Global Pitch and Loudness Features",
    "abstract": "This paper presents a simple yet effective method to achieve prosody transfer from a reference speech signal to synthesized speech. The main idea is to incorporate well-known acoustic correlates of prosody such as pitch and loudness contours of the reference speech into a modern neural text-to-speech (TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of acoustic features are extracted from reference audio and then used to condition a TC2 synthesizer. The trained model is evaluated using subjective listening tests and a novel objective evaluation of prosody transfer is proposed. Listening tests show that the synthesized speech is rated as highly natural and that prosody is successfully transferred from the reference speech signal to the synthesized signal.",
    "metadata": {
      "title": "Prosody Transfer in Neural Text to Speech Using Global Pitch and Loudness Features",
      "summary": "This paper presents a simple yet effective method to achieve prosody transfer from a reference speech signal to synthesized speech. The main idea is to incorporate well-known acoustic correlates of prosody such as pitch and loudness contours of the reference speech into a modern neural text-to-speech (TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of acoustic features are extracted from reference audio and then used to condition a TC2 synthesizer. The trained model is evaluated using subjective listening tests and a novel objective evaluation of prosody transfer is proposed. Listening tests show that the synthesized speech is rated as highly natural and that prosody is successfully transferred from the reference speech signal to the synthesized signal.",
      "abstract": "This paper presents a simple yet effective method to achieve prosody transfer from a reference speech signal to synthesized speech. The main idea is to incorporate well-known acoustic correlates of prosody such as pitch and loudness contours of the reference speech into a modern neural text-to-speech (TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of acoustic features are extracted from reference audio and then used to condition a TC2 synthesizer. The trained model is evaluated using subjective listening tests and a novel objective evaluation of prosody transfer is proposed. Listening tests show that the synthesized speech is rated as highly natural and that prosody is successfully transferred from the reference speech signal to the synthesized signal.",
      "doi": "https://doi.org/10.48550/arxiv.1911.09645",
      "openalex_id": "https://openalex.org/W2991417167",
      "arxiv_id": "",
      "publication_date": "2019-11-21",
      "published": "2019-11-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or token-based representations; focuses on prosody transfer in neural TTS using pitch and loudness conditioning rather than tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or token-based representations; focuses on prosody transfer in neural TTS using pitch and loudness conditioning rather than tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on prosody transfer in neural text-to-speech synthesis using pitch and loudness features, without addressing the generation or design of discrete audio tokens such as vector quantization-based representations, or discrete token formation processes that can be used in sequence modeling or audio language modeling. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on prosody transfer in neural text-to-speech synthesis using pitch and loudness features, without addressing the generation or design of discrete audio tokens such as vector quantization-based representations, or discrete token formation processes that can be used in sequence modeling or audio language modeling. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LOL: An Investigation into Cybernetic Humor, or: Can Machines Laugh?",
    "abstract": "The mechanisms of humour have been the subject of much study and investigation, starting with and up to our days. Much of this work is based on literary theories, put forward by some of the most eminent philosophers and thinkers of all times, or medical theories, investigating the impact of humor on brain activity or behaviour. Recent functional neuroimaging studies, for instance, have investigated the process of comprehending and appreciating humor by examining functional activity in distinctive regions of brains stimulated by joke corpora. Yet, there is precious little work on the computational side, possibly due to the less hilarious nature of computer scientists as compared to men of letters and sawbones. In this paper, we set to investigate whether literary theories of humour can stand the test of algorithmic laughter. Or, in other words, we ask ourselves the vexed question: Can machines laugh? We attempt to answer that question by testing whether an algorithm - namely, a neural network - can \"understand\" humour, and in particular whether it is possible to automatically identify abstractions that are predicted to be relevant by established literary theories about the mechanisms of humor. Notice that we do not focus here on distinguishing humorous from serious statements - a feat that is clearly way beyond the capabilities of the average human voter, not to mention the average machine - but rather on identifying the underlying mechanisms and triggers that are postulated to exist by literary theories, by verifying if similar mechanisms can be learned by machines.",
    "metadata": {
      "title": "LOL: An Investigation into Cybernetic Humor, or: Can Machines Laugh?",
      "summary": "The mechanisms of humour have been the subject of much study and investigation, starting with and up to our days. Much of this work is based on literary theories, put forward by some of the most eminent philosophers and thinkers of all times, or medical theories, investigating the impact of humor on brain activity or behaviour. Recent functional neuroimaging studies, for instance, have investigated the process of comprehending and appreciating humor by examining functional activity in distinctive regions of brains stimulated by joke corpora. Yet, there is precious little work on the computational side, possibly due to the less hilarious nature of computer scientists as compared to men of letters and sawbones. In this paper, we set to investigate whether literary theories of humour can stand the test of algorithmic laughter. Or, in other words, we ask ourselves the vexed question: Can machines laugh? We attempt to answer that question by testing whether an algorithm - namely, a neural network - can \"understand\" humour, and in particular whether it is possible to automatically identify abstractions that are predicted to be relevant by established literary theories about the mechanisms of humor. Notice that we do not focus here on distinguishing humorous from serious statements - a feat that is clearly way beyond the capabilities of the average human voter, not to mention the average machine - but rather on identifying the underlying mechanisms and triggers that are postulated to exist by literary theories, by verifying if similar mechanisms can be learned by machines.",
      "abstract": "The mechanisms of humour have been the subject of much study and investigation, starting with and up to our days. Much of this work is based on literary theories, put forward by some of the most eminent philosophers and thinkers of all times, or medical theories, investigating the impact of humor on brain activity or behaviour. Recent functional neuroimaging studies, for instance, have investigated the process of comprehending and appreciating humor by examining functional activity in distinctive regions of brains stimulated by joke corpora. Yet, there is precious little work on the computational side, possibly due to the less hilarious nature of computer scientists as compared to men of letters and sawbones. In this paper, we set to investigate whether literary theories of humour can stand the test of algorithmic laughter. Or, in other words, we ask ourselves the vexed question: Can machines laugh? We attempt to answer that question by testing whether an algorithm - namely, a neural network - can \"understand\" humour, and in particular whether it is possible to automatically identify abstractions that are predicted to be relevant by established literary theories about the mechanisms of humor. Notice that we do not focus here on distinguishing humorous from serious statements - a feat that is clearly way beyond the capabilities of the average human voter, not to mention the average machine - but rather on identifying the underlying mechanisms and triggers that are postulated to exist by literary theories, by verifying if similar mechanisms can be learned by machines.",
      "doi": "https://doi.org/10.4230/lipics.fun.2016.3",
      "openalex_id": "https://openalex.org/W1810943226",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper discusses cybernetic humor and neural networks for understanding humor, with no focus on discrete audio token representations or tokenization methods, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper discusses cybernetic humor and neural networks for understanding humor, with no focus on discrete audio token representations or tokenization methods, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on investigating humor comprehension by machines via neural networks but does not discuss discrete audio tokenization, vector quantization, or related audio representation techniques outlined in the inclusion criteria. It lacks any mention of transforming continuous audio signals into discrete tokens for sequence modeling or downstream audio tasks, which is essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on investigating humor comprehension by machines via neural networks but does not discuss discrete audio tokenization, vector quantization, or related audio representation techniques outlined in the inclusion criteria. It lacks any mention of transforming continuous audio signals into discrete tokens for sequence modeling or downstream audio tasks, which is essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis",
    "abstract": "Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.",
    "metadata": {
      "title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis",
      "summary": "Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.",
      "abstract": "Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.",
      "doi": "https://doi.org/10.48550/arxiv.1906.03402",
      "openalex_id": "https://openalex.org/W2948238043",
      "arxiv_id": "",
      "publication_date": "2019-06-08",
      "published": "2019-06-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses variational embedding capacity in expressive speech synthesis but does not address discrete audio tokens or tokenization for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses variational embedding capacity in expressive speech synthesis but does not address discrete audio tokens or tokenization for audio modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on variational latent variable models for expressive speech synthesis, discussing embedding capacity and prosody/style transfer, but it does not discuss or propose discrete audio tokens generated through vector quantization or any form of discrete tokenization as required in the inclusion criteria; hence it does not meet the essential condition of discrete token design or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on variational latent variable models for expressive speech synthesis, discussing embedding capacity and prosody/style transfer, but it does not discuss or propose discrete audio tokens generated through vector quantization or any form of discrete tokenization as required in the inclusion criteria; hence it does not meet the essential condition of discrete token design or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Finding temporal structure in music: blues improvisation with LSTM recurrent networks",
    "abstract": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
    "metadata": {
      "title": "Finding temporal structure in music: blues improvisation with LSTM recurrent networks",
      "summary": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
      "abstract": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
      "doi": "https://doi.org/10.1109/nnsp.2002.1030094",
      "openalex_id": "https://openalex.org/W2137619888",
      "arxiv_id": "",
      "publication_date": "2003-06-25",
      "published": "2003-06-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on using LSTM networks to learn blues music structure and generate melodies, not on discrete audio token representations or tokenization methods for audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on using LSTM networks to learn blues music structure and generate melodies, not on discrete audio token representations or tokenization methods for audio language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on learning temporal structure in music using LSTM recurrent networks to generate blues music, but it does not discuss or propose any method for generating discrete audio tokens through vector quantization or related techniques, nor does it mention using these tokens for sequence modeling or representation. It primarily addresses music generation via LSTM but lacks the core aspect of discrete audio token representation, which is essential according to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on learning temporal structure in music using LSTM recurrent networks to generate blues music, but it does not discuss or propose any method for generating discrete audio tokens through vector quantization or related techniques, nor does it mention using these tokens for sequence modeling or representation. It primarily addresses music generation via LSTM but lacks the core aspect of discrete audio token representation, which is essential according to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MorpheuS: Generating Structured Music with Constrained Patterns and Tension",
    "abstract": "Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS' novel framework has the ability to generate polyphonic pieces with a given tension profile and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantifies the tension profile and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efficient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best fit the prescribed tension profile to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with specific tension profile and long-term structure is particularly useful in a game or film music context. Music generated by the MorpheuS system has been performed live in concerts.",
    "metadata": {
      "title": "MorpheuS: Generating Structured Music with Constrained Patterns and Tension",
      "summary": "Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS' novel framework has the ability to generate polyphonic pieces with a given tension profile and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantifies the tension profile and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efficient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best fit the prescribed tension profile to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with specific tension profile and long-term structure is particularly useful in a game or film music context. Music generated by the MorpheuS system has been performed live in concerts.",
      "abstract": "Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS' novel framework has the ability to generate polyphonic pieces with a given tension profile and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantifies the tension profile and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efficient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best fit the prescribed tension profile to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with specific tension profile and long-term structure is particularly useful in a game or film music context. Music generated by the MorpheuS system has been performed live in concerts.",
      "doi": "https://doi.org/10.1109/taffc.2017.2737984",
      "openalex_id": "https://openalex.org/W2744457411",
      "arxiv_id": "",
      "publication_date": "2017-08-10",
      "published": "2017-08-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not address discrete audio tokens or token-based representations (no VQ/RVQ, hierarchical quantization, SSL-derived discrete tokens, etc.). It describes a music generation system using tension profiles and long-term structure via pattern detection and an optimization metaheuristic, without token-level or discrete token methodology, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item does not address discrete audio tokens or token-based representations (no VQ/RVQ, hierarchical quantization, SSL-derived discrete tokens, etc.). It describes a music generation system using tension profiles and long-term structure via pattern detection and an optimization metaheuristic, without token-level or discrete token methodology, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper 'MorpheuS: Generating Structured Music with Constrained Patterns and Tension' focuses on automatic music generation through mathematical models and optimization for tonal tension and pattern structures without mentioning any discrete audio token design, vector quantization, or tokenization mechanism for audio representation as required by the inclusion criteria. It lacks details on token generation or the use of discrete tokens for sequence modeling relevant to speech, music, or general audio. Therefore, it does not meet the inclusion criteria and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper 'MorpheuS: Generating Structured Music with Constrained Patterns and Tension' focuses on automatic music generation through mathematical models and optimization for tonal tension and pattern structures without mentioning any discrete audio token design, vector quantization, or tokenization mechanism for audio representation as required by the inclusion criteria. It lacks details on token generation or the use of discrete tokens for sequence modeling relevant to speech, music, or general audio. Therefore, it does not meet the inclusion criteria and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An Emotion-Based Method to Perform Algorithmic Composition",
    "abstract": "The generative music using algorithmic composition techniques has been developed in many years. However it usually lacks of emotion-based mechanism to generate music with specific affective features. In this article the automated music algorithm will be performed based on Prof. Phil Winosr’s “MusicSculptor” software with proper emotion parameter mapping to drive the music content with specific context using various music pa-rameters distribution with different probability control, in order to generate the necessary music emotion automatically. When the emotion scenario varies, the generative music will be logically made via the emotion and context control based on the emotion music classification method. This innovative technique not only generates the emotion music according to the scenario, but also plays the different content of the music every time to make listeners feel “fresh”. The emotion music classification method and the automated music development can be analyzed as the reference for the input of the automated music program. The result shows the proposed method generating music emotions successfully such as happy, angry, sad, and joy, with the correspondent parameter mapping between music and emotion. Although this paper only demonstrates the possibility of emotion-based algorithmic composition, hopefully the proposed idea can be extended to apply into the fields including multimedia and game, to make the background music automatically generated any time according to the context changed by the interaction between human and machine.",
    "metadata": {
      "title": "An Emotion-Based Method to Perform Algorithmic Composition",
      "summary": "The generative music using algorithmic composition techniques has been developed in many years. However it usually lacks of emotion-based mechanism to generate music with specific affective features. In this article the automated music algorithm will be performed based on Prof. Phil Winosr’s “MusicSculptor” software with proper emotion parameter mapping to drive the music content with specific context using various music pa-rameters distribution with different probability control, in order to generate the necessary music emotion automatically. When the emotion scenario varies, the generative music will be logically made via the emotion and context control based on the emotion music classification method. This innovative technique not only generates the emotion music according to the scenario, but also plays the different content of the music every time to make listeners feel “fresh”. The emotion music classification method and the automated music development can be analyzed as the reference for the input of the automated music program. The result shows the proposed method generating music emotions successfully such as happy, angry, sad, and joy, with the correspondent parameter mapping between music and emotion. Although this paper only demonstrates the possibility of emotion-based algorithmic composition, hopefully the proposed idea can be extended to apply into the fields including multimedia and game, to make the background music automatically generated any time according to the context changed by the interaction between human and machine.",
      "abstract": "The generative music using algorithmic composition techniques has been developed in many years. However it usually lacks of emotion-based mechanism to generate music with specific affective features. In this article the automated music algorithm will be performed based on Prof. Phil Winosr’s “MusicSculptor” software with proper emotion parameter mapping to drive the music content with specific context using various music pa-rameters distribution with different probability control, in order to generate the necessary music emotion automatically. When the emotion scenario varies, the generative music will be logically made via the emotion and context control based on the emotion music classification method. This innovative technique not only generates the emotion music according to the scenario, but also plays the different content of the music every time to make listeners feel “fresh”. The emotion music classification method and the automated music development can be analyzed as the reference for the input of the automated music program. The result shows the proposed method generating music emotions successfully such as happy, angry, sad, and joy, with the correspondent parameter mapping between music and emotion. Although this paper only demonstrates the possibility of emotion-based algorithmic composition, hopefully the proposed idea can be extended to apply into the fields including multimedia and game, to make the background music automatically generated any time according to the context changed by the interaction between human and machine.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2336031501",
      "arxiv_id": "",
      "publication_date": "2013-01-01",
      "published": "2013-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on emotion-based algorithmic music generation using software and emotion-context control; it does not propose or analyze discrete audio token representations (VQ/RVQ, tokenization, or SSL-based token generation) needed by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on emotion-based algorithmic music generation using software and emotion-context control; it does not propose or analyze discrete audio token representations (VQ/RVQ, tokenization, or SSL-based token generation) needed by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on an emotion-based algorithmic composition method for generating music with specific affective features, using parameter mapping and automated music generation techniques. However, it does not discuss or propose any discrete audio tokenization process, vector quantization, or token generation used to transform continuous audio signals into discrete tokens for sequence or language modeling. The study mainly addresses emotional music generation using existing software without providing technical details on token formation, quantization, or a novel discrete token-based representation. Therefore, it does not satisfy the inclusion criteria related to discrete audio tokens, nor does it fall into the exclusion criteria involving lack of method details, since the topic itself is unrelated to discrete token design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on an emotion-based algorithmic composition method for generating music with specific affective features, using parameter mapping and automated music generation techniques. However, it does not discuss or propose any discrete audio tokenization process, vector quantization, or token generation used to transform continuous audio signals into discrete tokens for sequence or language modeling. The study mainly addresses emotional music generation using existing software without providing technical details on token formation, quantization, or a novel discrete token-based representation. Therefore, it does not satisfy the inclusion criteria related to discrete audio tokens, nor does it fall into the exclusion criteria involving lack of method details, since the topic itself is unrelated to discrete token design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An exploratory study of musical emotions and psychophysiology.",
    "abstract": "A basic issue about musical emotions concerns whether music elicits emotional responses in listeners (the 'emotivist' position) or simply expresses emotions that listeners recognize in the music (the 'cognitivist' position). To address this, psychophysiological measures were recorded while listners heard two excerpts chosen to represent each of three emotions: sad, fear, and happy. The measures covered a fairly wide spectrum of cardiac, vascular, electrodermal, and respiratory functions. Other subjects indicated dynamic changes in emotions they experienced while listening to the music on one of four scales: sad, fear, happy, and tension. Both physiological and emotion judgements were made on a second-by-second basis. The physiological measures all showed a significant effect of music compared to the pre-music interval. A number of analyses, including correlations between physiology and emotion judgments, found significant differences among the excerpts. The sad excerpts produced the largest changes in heart rate, blood pressure, skin conductance and temperature. The fear excerpts produced the largest changes in blood transit time and amplitude. The happy excerpts produced the largest changes in the measures of respiration. These emotion-specific physiological changes only partially replicated those found for nonmusical emotions. The physiological effects of music observed generally support the emotivist view of musical emotions.",
    "metadata": {
      "title": "An exploratory study of musical emotions and psychophysiology.",
      "summary": "A basic issue about musical emotions concerns whether music elicits emotional responses in listeners (the 'emotivist' position) or simply expresses emotions that listeners recognize in the music (the 'cognitivist' position). To address this, psychophysiological measures were recorded while listners heard two excerpts chosen to represent each of three emotions: sad, fear, and happy. The measures covered a fairly wide spectrum of cardiac, vascular, electrodermal, and respiratory functions. Other subjects indicated dynamic changes in emotions they experienced while listening to the music on one of four scales: sad, fear, happy, and tension. Both physiological and emotion judgements were made on a second-by-second basis. The physiological measures all showed a significant effect of music compared to the pre-music interval. A number of analyses, including correlations between physiology and emotion judgments, found significant differences among the excerpts. The sad excerpts produced the largest changes in heart rate, blood pressure, skin conductance and temperature. The fear excerpts produced the largest changes in blood transit time and amplitude. The happy excerpts produced the largest changes in the measures of respiration. These emotion-specific physiological changes only partially replicated those found for nonmusical emotions. The physiological effects of music observed generally support the emotivist view of musical emotions.",
      "abstract": "A basic issue about musical emotions concerns whether music elicits emotional responses in listeners (the 'emotivist' position) or simply expresses emotions that listeners recognize in the music (the 'cognitivist' position). To address this, psychophysiological measures were recorded while listners heard two excerpts chosen to represent each of three emotions: sad, fear, and happy. The measures covered a fairly wide spectrum of cardiac, vascular, electrodermal, and respiratory functions. Other subjects indicated dynamic changes in emotions they experienced while listening to the music on one of four scales: sad, fear, happy, and tension. Both physiological and emotion judgements were made on a second-by-second basis. The physiological measures all showed a significant effect of music compared to the pre-music interval. A number of analyses, including correlations between physiology and emotion judgments, found significant differences among the excerpts. The sad excerpts produced the largest changes in heart rate, blood pressure, skin conductance and temperature. The fear excerpts produced the largest changes in blood transit time and amplitude. The happy excerpts produced the largest changes in the measures of respiration. These emotion-specific physiological changes only partially replicated those found for nonmusical emotions. The physiological effects of music observed generally support the emotivist view of musical emotions.",
      "doi": "https://doi.org/10.1037/1196-1961.51.4.336",
      "openalex_id": "https://openalex.org/W1983627329",
      "arxiv_id": "",
      "publication_date": "1997-12-01",
      "published": "1997-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates musical emotions with psychophysiology; it does not address discrete audio token design or tokenization methods, hence it fails all inclusion criteria and does not meet any exclusion criteria related to the token-focused scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates musical emotions with psychophysiology; it does not address discrete audio token design or tokenization methods, hence it fails all inclusion criteria and does not meet any exclusion criteria related to the token-focused scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on measuring psychophysiological responses to musical emotions without mentioning any discrete audio token representation, vector quantization, or related techniques aimed at transforming continuous audio into discrete tokens for sequence modeling. It lacks discussion on token generation processes, neural codecs, or audio language modeling, which are essential for inclusion according to the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on measuring psychophysiological responses to musical emotions without mentioning any discrete audio token representation, vector quantization, or related techniques aimed at transforming continuous audio into discrete tokens for sequence modeling. It lacks discussion on token generation processes, neural codecs, or audio language modeling, which are essential for inclusion according to the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Musical Tension Modeling and Its Application to Dynamic Sonification",
    "abstract": "March 01 2012 Generative Musical Tension Modeling and Its Application to Dynamic Sonification Ryan Nikolaidis, Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Bruce Walker, Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Gil Weinberg Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Author and Article Information Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Online ISSN: 1531-5169 Print ISSN: 0148-9267 © 2012 Massachusetts Institute of Technology.2012 Computer Music Journal (2012) 36 (1): 55–64. https://doi.org/10.1162/COMJ_a_00105 Cite Icon Cite Permissions Share Icon Share Facebook Twitter LinkedIn Email Views Icon Views Article contents Figures & tables Video Audio Supplementary Data Peer Review Search Site Citation Ryan Nikolaidis, Bruce Walker, Gil Weinberg; Generative Musical Tension Modeling and Its Application to Dynamic Sonification. Computer Music Journal 2012; 36 (1): 55–64. doi: https://doi.org/10.1162/COMJ_a_00105 Download citation file: Ris (Zotero) Reference Manager EasyBib Bookends Mendeley Papers EndNote RefWorks BibTex toolbar search Search Dropdown Menu toolbar search search input Search input auto suggest filter your search All ContentAll JournalsComputer Music Journal Search Advanced Search This content is only available as a PDF. © 2012 Massachusetts Institute of Technology.2012 Article PDF first page preview Close Modal You do not currently have access to this content.",
    "metadata": {
      "title": "Generative Musical Tension Modeling and Its Application to Dynamic Sonification",
      "summary": "March 01 2012 Generative Musical Tension Modeling and Its Application to Dynamic Sonification Ryan Nikolaidis, Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Bruce Walker, Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Gil Weinberg Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Author and Article Information Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Online ISSN: 1531-5169 Print ISSN: 0148-9267 © 2012 Massachusetts Institute of Technology.2012 Computer Music Journal (2012) 36 (1): 55–64. https://doi.org/10.1162/COMJ_a_00105 Cite Icon Cite Permissions Share Icon Share Facebook Twitter LinkedIn Email Views Icon Views Article contents Figures & tables Video Audio Supplementary Data Peer Review Search Site Citation Ryan Nikolaidis, Bruce Walker, Gil Weinberg; Generative Musical Tension Modeling and Its Application to Dynamic Sonification. Computer Music Journal 2012; 36 (1): 55–64. doi: https://doi.org/10.1162/COMJ_a_00105 Download citation file: Ris (Zotero) Reference Manager EasyBib Bookends Mendeley Papers EndNote RefWorks BibTex toolbar search Search Dropdown Menu toolbar search search input Search input auto suggest filter your search All ContentAll JournalsComputer Music Journal Search Advanced Search This content is only available as a PDF. © 2012 Massachusetts Institute of Technology.2012 Article PDF first page preview Close Modal You do not currently have access to this content.",
      "abstract": "March 01 2012 Generative Musical Tension Modeling and Its Application to Dynamic Sonification Ryan Nikolaidis, Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Bruce Walker, Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Gil Weinberg Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Author and Article Information Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Online ISSN: 1531-5169 Print ISSN: 0148-9267 © 2012 Massachusetts Institute of Technology.2012 Computer Music Journal (2012) 36 (1): 55–64. https://doi.org/10.1162/COMJ_a_00105 Cite Icon Cite Permissions Share Icon Share Facebook Twitter LinkedIn Email Views Icon Views Article contents Figures & tables Video Audio Supplementary Data Peer Review Search Site Citation Ryan Nikolaidis, Bruce Walker, Gil Weinberg; Generative Musical Tension Modeling and Its Application to Dynamic Sonification. Computer Music Journal 2012; 36 (1): 55–64. doi: https://doi.org/10.1162/COMJ_a_00105 Download citation file: Ris (Zotero) Reference Manager EasyBib Bookends Mendeley Papers EndNote RefWorks BibTex toolbar search Search Dropdown Menu toolbar search search input Search input auto suggest filter your search All ContentAll JournalsComputer Music Journal Search Advanced Search This content is only available as a PDF. © 2012 Massachusetts Institute of Technology.2012 Article PDF first page preview Close Modal You do not currently have access to this content.",
      "doi": "https://doi.org/10.1162/comj_a_00105",
      "openalex_id": "https://openalex.org/W2068627759",
      "arxiv_id": "",
      "publication_date": "2012-02-28",
      "published": "2012-02-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract address generative musical tension modeling and dynamic sonification, with no discussion of discrete audio tokens, vector quantization, or token-based representations, and thus does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract address generative musical tension modeling and dynamic sonification, with no discussion of discrete audio tokens, vector quantization, or token-based representations, and thus does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article titled 'Generative Musical Tension Modeling and Its Application to Dynamic Sonification' does not explicitly mention the use of discrete audio tokens, vector quantization, or related quantization methods essential for the inclusion criteria. It seems focused on musical tension modeling and dynamic sonification rather than the creation or use of discrete tokenized audio representations for language or sequence modeling purposes as required by the criteria. Also, the abstract and metadata do not provide detailed technical descriptions of token generation or quantization processes. Therefore, it lacks sufficient evidence to meet the specific inclusion requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article titled 'Generative Musical Tension Modeling and Its Application to Dynamic Sonification' does not explicitly mention the use of discrete audio tokens, vector quantization, or related quantization methods essential for the inclusion criteria. It seems focused on musical tension modeling and dynamic sonification rather than the creation or use of discrete tokenized audio representations for language or sequence modeling purposes as required by the criteria. Also, the abstract and metadata do not provide detailed technical descriptions of token generation or quantization processes. Therefore, it lacks sufficient evidence to meet the specific inclusion requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Real-Time Music Generation for a Virtual Environment",
    "abstract": "We describe how we are adapting musical techniques used in films to build a computer program to generate atmospheric music suited to an educational virtual environment. The generator produces music to convey fear using suspense and surprise. The paper motivates the search for a mapping between these emotions and musical structure and outlines how the music generator is implemented. It also explains how we intend to evaluate the effects the music has on the users&amp;apos; subjective sense of presence or &amp;quot;being there&amp;quot;.",
    "metadata": {
      "title": "Real-Time Music Generation for a Virtual Environment",
      "summary": "We describe how we are adapting musical techniques used in films to build a computer program to generate atmospheric music suited to an educational virtual environment. The generator produces music to convey fear using suspense and surprise. The paper motivates the search for a mapping between these emotions and musical structure and outlines how the music generator is implemented. It also explains how we intend to evaluate the effects the music has on the users&amp;apos; subjective sense of presence or &amp;quot;being there&amp;quot;.",
      "abstract": "We describe how we are adapting musical techniques used in films to build a computer program to generate atmospheric music suited to an educational virtual environment. The generator produces music to convey fear using suspense and surprise. The paper motivates the search for a mapping between these emotions and musical structure and outlines how the music generator is implemented. It also explains how we intend to evaluate the effects the music has on the users&amp;apos; subjective sense of presence or &amp;quot;being there&amp;quot;.",
      "doi": "",
      "openalex_id": "https://openalex.org/W52081385",
      "arxiv_id": "",
      "publication_date": "1998-01-01",
      "published": "1998-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on real-time music generation and emotional mapping for a virtual education environment and does not discuss discrete audio tokens, tokenization methods (VQ/RVQ/segmented tokens), or token-based modeling; lacking the required token-centric methodological content, it should be excluded per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on real-time music generation and emotional mapping for a virtual education environment and does not discuss discrete audio tokens, tokenization methods (VQ/RVQ/segmented tokens), or token-based modeling; lacking the required token-centric methodological content, it should be excluded per the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on real-time music generation for a virtual environment aimed at conveying emotions such as fear using suspense and surprise. However, they do not discuss any discrete audio token generation process, vector quantization, or use of these tokens for sequence modeling or language modeling. There is no mention of token design, quantization techniques, or a detailed method for discrete token production and usage. The work appears to focus on music generation and evaluation of user experience rather than discrete audio token representations or their modeling, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on real-time music generation for a virtual environment aimed at conveying emotions such as fear using suspense and surprise. However, they do not discuss any discrete audio token generation process, vector quantization, or use of these tokens for sequence modeling or language modeling. There is no mention of token design, quantization techniques, or a detailed method for discrete token production and usage. The work appears to focus on music generation and evaluation of user experience rather than discrete audio token representations or their modeling, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Feeling of Music Past: How Listeners Remember Musical Affect",
    "abstract": "This study was conducted to determine how listeners derive global evaluations of past musical durations from moment-to-moment experience. Participants produced moment-to-moment affective intensity ratings by pressing a pressure-sensitive button while listening to various selections. They later reported the remembered affective intensity of each example. The data suggest that the assumption that remembered affect equals the sum of all momentary affects fundamentally misrepresents how listeners encode and label past affective experiences. The duration of particular rather than uniform episodes contributes minimally to remembered affect (duration neglect). Listeners rely on the peak of affective intensity during a selection, the last moment, and moments that are more emotionally intense than immediately previous moments to determine postperformance ratings. The peak proves to be the strongest predictor of remembered affect. We derive a formula that takes moment-to-moment experience as input and predicts how listeners will remember musical affect. The formula is a better predictor of postperformance affect than any other on-line characteristic considered. Last, the utility of the formula is demonstrated through a brief examination of compositional decisions in a string quartet movement by Borodin and one typical format of four-movement symphonies from the classical period.",
    "metadata": {
      "title": "The Feeling of Music Past: How Listeners Remember Musical Affect",
      "summary": "This study was conducted to determine how listeners derive global evaluations of past musical durations from moment-to-moment experience. Participants produced moment-to-moment affective intensity ratings by pressing a pressure-sensitive button while listening to various selections. They later reported the remembered affective intensity of each example. The data suggest that the assumption that remembered affect equals the sum of all momentary affects fundamentally misrepresents how listeners encode and label past affective experiences. The duration of particular rather than uniform episodes contributes minimally to remembered affect (duration neglect). Listeners rely on the peak of affective intensity during a selection, the last moment, and moments that are more emotionally intense than immediately previous moments to determine postperformance ratings. The peak proves to be the strongest predictor of remembered affect. We derive a formula that takes moment-to-moment experience as input and predicts how listeners will remember musical affect. The formula is a better predictor of postperformance affect than any other on-line characteristic considered. Last, the utility of the formula is demonstrated through a brief examination of compositional decisions in a string quartet movement by Borodin and one typical format of four-movement symphonies from the classical period.",
      "abstract": "This study was conducted to determine how listeners derive global evaluations of past musical durations from moment-to-moment experience. Participants produced moment-to-moment affective intensity ratings by pressing a pressure-sensitive button while listening to various selections. They later reported the remembered affective intensity of each example. The data suggest that the assumption that remembered affect equals the sum of all momentary affects fundamentally misrepresents how listeners encode and label past affective experiences. The duration of particular rather than uniform episodes contributes minimally to remembered affect (duration neglect). Listeners rely on the peak of affective intensity during a selection, the last moment, and moments that are more emotionally intense than immediately previous moments to determine postperformance ratings. The peak proves to be the strongest predictor of remembered affect. We derive a formula that takes moment-to-moment experience as input and predicts how listeners will remember musical affect. The formula is a better predictor of postperformance affect than any other on-line characteristic considered. Last, the utility of the formula is demonstrated through a brief examination of compositional decisions in a string quartet movement by Borodin and one typical format of four-movement symphonies from the classical period.",
      "doi": "https://doi.org/10.1525/mp.2004.22.1.15",
      "openalex_id": "https://openalex.org/W1976814248",
      "arxiv_id": "",
      "publication_date": "2004-01-01",
      "published": "2004-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns memory and perception of musical affect and does not discuss discrete audio token design or tokenization methods, thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns memory and perception of musical affect and does not discuss discrete audio token design or tokenization methods, thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates how listeners remember musical affect based on moment-to-moment experience, but it does not mention any discrete audio token representation, quantization methods, or use in sequence or language models, which are essential inclusion criteria; thus, the study is unrelated to the discrete audio token research topic defined and fails to meet any inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates how listeners remember musical affect based on moment-to-moment experience, but it does not mention any discrete audio token representation, quantization methods, or use in sequence or language models, which are essential inclusion criteria; thus, the study is unrelated to the discrete audio token research topic defined and fails to meet any inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Computer-Generating emotional music: The design of an affective music algorithm",
    "abstract": "This paper explores one way to use music in the context of affective design. We&amp;apos;ve made a real-time music generator that is designed around the concepts of valence and arousal, which are two components of certain models of emotion. When set to a desired valence and arousal, the algorithm plays music corresponding to the intersection of these two parameters. We designed our algorithm using psychological theory of emotion and parametrized features of music which have been tested for affect. The results are a modular algorithm design, in which our parameters can be implemented in other affective music algorithms. We describe our implementation of these parameters, and our strategy for manipulating the parameters to generate musical emotion. Finally we discuss possible applications for these techniques in the fields of the arts, medical systems, and research applications. We believe",
    "metadata": {
      "title": "Computer-Generating emotional music: The design of an affective music algorithm",
      "summary": "This paper explores one way to use music in the context of affective design. We&amp;apos;ve made a real-time music generator that is designed around the concepts of valence and arousal, which are two components of certain models of emotion. When set to a desired valence and arousal, the algorithm plays music corresponding to the intersection of these two parameters. We designed our algorithm using psychological theory of emotion and parametrized features of music which have been tested for affect. The results are a modular algorithm design, in which our parameters can be implemented in other affective music algorithms. We describe our implementation of these parameters, and our strategy for manipulating the parameters to generate musical emotion. Finally we discuss possible applications for these techniques in the fields of the arts, medical systems, and research applications. We believe",
      "abstract": "This paper explores one way to use music in the context of affective design. We&amp;apos;ve made a real-time music generator that is designed around the concepts of valence and arousal, which are two components of certain models of emotion. When set to a desired valence and arousal, the algorithm plays music corresponding to the intersection of these two parameters. We designed our algorithm using psychological theory of emotion and parametrized features of music which have been tested for affect. The results are a modular algorithm design, in which our parameters can be implemented in other affective music algorithms. We describe our implementation of these parameters, and our strategy for manipulating the parameters to generate musical emotion. Finally we discuss possible applications for these techniques in the fields of the arts, medical systems, and research applications. We believe",
      "doi": "",
      "openalex_id": "https://openalex.org/W3158589047",
      "arxiv_id": "",
      "publication_date": "2008-12-01",
      "published": "2008-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a real-time affective music generator based on valence/arousal; it does not address discrete audio tokens or tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a real-time affective music generator based on valence/arousal; it does not address discrete audio tokens or tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a real-time music generator based on affective design principles involving valence and arousal, but it does not mention any discrete audio tokenization method such as vector quantization or other discrete representation techniques, nor does it provide details on token generation, use with sequence or language models, or experimental validation relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a real-time music generator based on affective design principles involving valence and arousal, but it does not mention any discrete audio tokenization method such as vector quantization or other discrete representation techniques, nor does it provide details on token generation, use with sequence or language models, or experimental validation relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mechanics of human voice production and control",
    "abstract": "As the primary means of communication, voice plays an important role in daily life. Voice also conveys personal information such as social status, personal traits, and the emotional state of the speaker. Mechanically, voice production involves complex fluid-structure interaction within the glottis and its control by laryngeal muscle activation. An important goal of voice research is to establish a causal theory linking voice physiology and biomechanics to how speakers use and control voice to communicate meaning and personal information. Establishing such a causal theory has important implications for clinical voice management, voice training, and many speech technology applications. This paper provides a review of voice physiology and biomechanics, the physics of vocal fold vibration and sound production, and laryngeal muscular control of the fundamental frequency of voice, vocal intensity, and voice quality. Current efforts to develop mechanical and computational models of voice production are also critically reviewed. Finally, issues and future challenges in developing a causal theory of voice production and perception are discussed.",
    "metadata": {
      "title": "Mechanics of human voice production and control",
      "summary": "As the primary means of communication, voice plays an important role in daily life. Voice also conveys personal information such as social status, personal traits, and the emotional state of the speaker. Mechanically, voice production involves complex fluid-structure interaction within the glottis and its control by laryngeal muscle activation. An important goal of voice research is to establish a causal theory linking voice physiology and biomechanics to how speakers use and control voice to communicate meaning and personal information. Establishing such a causal theory has important implications for clinical voice management, voice training, and many speech technology applications. This paper provides a review of voice physiology and biomechanics, the physics of vocal fold vibration and sound production, and laryngeal muscular control of the fundamental frequency of voice, vocal intensity, and voice quality. Current efforts to develop mechanical and computational models of voice production are also critically reviewed. Finally, issues and future challenges in developing a causal theory of voice production and perception are discussed.",
      "abstract": "As the primary means of communication, voice plays an important role in daily life. Voice also conveys personal information such as social status, personal traits, and the emotional state of the speaker. Mechanically, voice production involves complex fluid-structure interaction within the glottis and its control by laryngeal muscle activation. An important goal of voice research is to establish a causal theory linking voice physiology and biomechanics to how speakers use and control voice to communicate meaning and personal information. Establishing such a causal theory has important implications for clinical voice management, voice training, and many speech technology applications. This paper provides a review of voice physiology and biomechanics, the physics of vocal fold vibration and sound production, and laryngeal muscular control of the fundamental frequency of voice, vocal intensity, and voice quality. Current efforts to develop mechanical and computational models of voice production are also critically reviewed. Finally, issues and future challenges in developing a causal theory of voice production and perception are discussed.",
      "doi": "https://doi.org/10.1121/1.4964509",
      "openalex_id": "https://openalex.org/W2530921900",
      "arxiv_id": "",
      "publication_date": "2016-10-01",
      "published": "2016-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item reviews voice physiology and biomechanics with a focus on mechanical aspects and modeling of vocal production, and does not address discrete audio tokens, token generation/quantization, or token-based modeling required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item reviews voice physiology and biomechanics with a focus on mechanical aspects and modeling of vocal production, and does not address discrete audio tokens, token generation/quantization, or token-based modeling required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the biomechanics and physiology of human voice production rather than on discrete audio tokens, neural audio codecs, or any form of learned or signal-processed discrete token representation for sequence modeling or downstream audio tasks as required by the inclusion criteria. There is no indication of token generation, vector quantization, or their usage in language models or sequence models, and the content does not meet any of the technical coverage needs specified. Thus, it does not satisfy the inclusion criteria and falls outside the scope of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the biomechanics and physiology of human voice production rather than on discrete audio tokens, neural audio codecs, or any form of learned or signal-processed discrete token representation for sequence modeling or downstream audio tasks as required by the inclusion criteria. There is no indication of token generation, vector quantization, or their usage in language models or sequence models, and the content does not meet any of the technical coverage needs specified. Thus, it does not satisfy the inclusion criteria and falls outside the scope of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders",
    "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
    "metadata": {
      "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders",
      "summary": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
      "abstract": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
      "doi": "https://doi.org/10.48550/arxiv.1704.01279",
      "openalex_id": "https://openalex.org/W2606176153",
      "arxiv_id": "",
      "publication_date": "2017-04-05",
      "published": "2017-04-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper covers WaveNet-style autoencoders for audio synthesis and the NSynth dataset; it does not introduce or analyze discrete audio tokens, token generation, or quantization as a core contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper covers WaveNet-style autoencoders for audio synthesis and the NSynth dataset; it does not introduce or analyze discrete audio tokens, token generation, or quantization as a core contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a WaveNet-style autoencoder for audio modeling and presents a large-scale dataset of musical notes, but it does not explicitly mention the generation or use of discrete audio tokens through quantization techniques like vector quantization, residual vector quantization, or semantic-driven discretization. There is no clear explanation of how discrete tokens are produced or utilized as a core method for sequence or language modeling in the abstract, which is a key inclusion criterion. Without evidence of discrete token design and usage, it does not sufficiently meet the inclusion criteria, though it is related to audio synthesis and modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a WaveNet-style autoencoder for audio modeling and presents a large-scale dataset of musical notes, but it does not explicitly mention the generation or use of discrete audio tokens through quantization techniques like vector quantization, residual vector quantization, or semantic-driven discretization. There is no clear explanation of how discrete tokens are produced or utilized as a core method for sequence or language modeling in the abstract, which is a key inclusion criterion. Without evidence of discrete token design and usage, it does not sufficiently meet the inclusion criteria, though it is related to audio synthesis and modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Isolating the dynamic attributes of musical timbrea)",
    "abstract": "Three experiments examined the dynamic attributes of timbre by evaluating the role of onsets in similarity judgments. In separate experiments, subjects heard complete orchestral instrument tones, the onsets of those tones, and tones with the onsets removed (‘‘remainders’’). Ratings for complete tones corresponded to those for onsets, indicating that the salient acoustic attributes for complete tones are present at the onset. Ratings for complete tones also corresponded to those for remainders, indicating that the salient attributes for complete tones are present also in the absence of onsets. Subsequent acoustic analyses demonstrated that this pattern of similarity was due to the centroid frequencies and amplitude envelopes of the tones. The results indicate that the dynamic attributes of timbre are not only present at the onset, but also throughout, and that multiple acoustic attributes may contribute to the same perceptual dimensions.",
    "metadata": {
      "title": "Isolating the dynamic attributes of musical timbrea)",
      "summary": "Three experiments examined the dynamic attributes of timbre by evaluating the role of onsets in similarity judgments. In separate experiments, subjects heard complete orchestral instrument tones, the onsets of those tones, and tones with the onsets removed (‘‘remainders’’). Ratings for complete tones corresponded to those for onsets, indicating that the salient acoustic attributes for complete tones are present at the onset. Ratings for complete tones also corresponded to those for remainders, indicating that the salient attributes for complete tones are present also in the absence of onsets. Subsequent acoustic analyses demonstrated that this pattern of similarity was due to the centroid frequencies and amplitude envelopes of the tones. The results indicate that the dynamic attributes of timbre are not only present at the onset, but also throughout, and that multiple acoustic attributes may contribute to the same perceptual dimensions.",
      "abstract": "Three experiments examined the dynamic attributes of timbre by evaluating the role of onsets in similarity judgments. In separate experiments, subjects heard complete orchestral instrument tones, the onsets of those tones, and tones with the onsets removed (‘‘remainders’’). Ratings for complete tones corresponded to those for onsets, indicating that the salient acoustic attributes for complete tones are present at the onset. Ratings for complete tones also corresponded to those for remainders, indicating that the salient attributes for complete tones are present also in the absence of onsets. Subsequent acoustic analyses demonstrated that this pattern of similarity was due to the centroid frequencies and amplitude envelopes of the tones. The results indicate that the dynamic attributes of timbre are not only present at the onset, but also throughout, and that multiple acoustic attributes may contribute to the same perceptual dimensions.",
      "doi": "https://doi.org/10.1121/1.407371",
      "openalex_id": "https://openalex.org/W2164767140",
      "arxiv_id": "",
      "publication_date": "1993-11-01",
      "published": "1993-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses timbre dynamics and onset contributions but does not address discrete audio tokens, token generation, or discrete representations suitable for language-model-style processing, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses timbre dynamics and onset contributions but does not address discrete audio tokens, token generation, or discrete representations suitable for language-model-style processing, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper studies the dynamic attributes of musical timbre through perceptual similarity experiments, focusing on acoustic features like onset and centroid frequencies, but it does not mention any method involving discrete audio token generation, vector quantization, or related techniques tied to audio language modeling or sequence modeling; thus it does not meet the inclusion criteria which require discrete token design and usage for sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper studies the dynamic attributes of musical timbre through perceptual similarity experiments, focusing on acoustic features like onset and centroid frequencies, but it does not mention any method involving discrete audio token generation, vector quantization, or related techniques tied to audio language modeling or sequence modeling; thus it does not meet the inclusion criteria which require discrete token design and usage for sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DIMENSION ANALYSIS OF THE PERCEPTION OF INSTRUMENTAL TIMBRE",
    "abstract": "W edin L. &amp; G oude G. Dimension analysis of the perception of instrumental timbre. Scand. J. Psychol ., 1972, 1 3 , 228–240.—The dimensionality of the perception of instrumental timbre for single tones was investigated through multidimensional scaling according to Ekman's vector model of similarity. Three factors were extracted and they were identified with certain characteristics of the spectrum envelopes. It was found that initial transients were important for the identifiability of instrumental tones but their presence or absence did not influence the dimensional structure. The “perceptual structure” was found to be different from the “cognitive structure” (= knowledge about the classification of the instruments), and the results are general for both trained and naive listeners.",
    "metadata": {
      "title": "DIMENSION ANALYSIS OF THE PERCEPTION OF INSTRUMENTAL TIMBRE",
      "summary": "W edin L. &amp; G oude G. Dimension analysis of the perception of instrumental timbre. Scand. J. Psychol ., 1972, 1 3 , 228–240.—The dimensionality of the perception of instrumental timbre for single tones was investigated through multidimensional scaling according to Ekman's vector model of similarity. Three factors were extracted and they were identified with certain characteristics of the spectrum envelopes. It was found that initial transients were important for the identifiability of instrumental tones but their presence or absence did not influence the dimensional structure. The “perceptual structure” was found to be different from the “cognitive structure” (= knowledge about the classification of the instruments), and the results are general for both trained and naive listeners.",
      "abstract": "W edin L. &amp; G oude G. Dimension analysis of the perception of instrumental timbre. Scand. J. Psychol ., 1972, 1 3 , 228–240.—The dimensionality of the perception of instrumental timbre for single tones was investigated through multidimensional scaling according to Ekman's vector model of similarity. Three factors were extracted and they were identified with certain characteristics of the spectrum envelopes. It was found that initial transients were important for the identifiability of instrumental tones but their presence or absence did not influence the dimensional structure. The “perceptual structure” was found to be different from the “cognitive structure” (= knowledge about the classification of the instruments), and the results are general for both trained and naive listeners.",
      "doi": "https://doi.org/10.1111/j.1467-9450.1972.tb00071.x",
      "openalex_id": "https://openalex.org/W2115067604",
      "arxiv_id": "",
      "publication_date": "1972-09-01",
      "published": "1972-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates perceptual dimensionality of instrumental timbre with no discrete audio token design, token generation, or reproducible tokenization methodology, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates perceptual dimensionality of instrumental timbre with no discrete audio token design, token generation, or reproducible tokenization methodology, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses the dimensional analysis of instrumental timbre perception without addressing the creation or use of discrete audio tokens via vector quantization or similar methodologies for audio language modeling or sequence modeling tasks, thus it does not meet the inclusion criteria related to discrete token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses the dimensional analysis of instrumental timbre perception without addressing the creation or use of discrete audio tokens via vector quantization or similar methodologies for audio language modeling or sequence modeling tasks, thus it does not meet the inclusion criteria related to discrete token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Latent Class Approach to Fitting the Weighted Euclidean Model, Clascal",
    "abstract": "A weighted Euclidean distance model for analyzing three-way proximity data is proposed that incorporates a latent class approach. In this latent class weighted Euclidean model, the contribution to the distance function between two stimuli is per dimension weighted identically by all subjects in the same latent class. This model removes the rotational invariance of the classical multidimensional scaling model retaining psychologically meaningful dimensions, and drastically reduces the number of parameters in the traditional INDSCAL model. The probability density function for the data of a subject is posited to be a finite mixture of spherical multivariate normal densities. The maximum likelihood function is optimized by means of an EM algorithm; a modified Fisher scoring method is used to update the parameters in the M-step. A model selection strategy is proposed and illustrated on both real and artificial data.",
    "metadata": {
      "title": "A Latent Class Approach to Fitting the Weighted Euclidean Model, Clascal",
      "summary": "A weighted Euclidean distance model for analyzing three-way proximity data is proposed that incorporates a latent class approach. In this latent class weighted Euclidean model, the contribution to the distance function between two stimuli is per dimension weighted identically by all subjects in the same latent class. This model removes the rotational invariance of the classical multidimensional scaling model retaining psychologically meaningful dimensions, and drastically reduces the number of parameters in the traditional INDSCAL model. The probability density function for the data of a subject is posited to be a finite mixture of spherical multivariate normal densities. The maximum likelihood function is optimized by means of an EM algorithm; a modified Fisher scoring method is used to update the parameters in the M-step. A model selection strategy is proposed and illustrated on both real and artificial data.",
      "abstract": "A weighted Euclidean distance model for analyzing three-way proximity data is proposed that incorporates a latent class approach. In this latent class weighted Euclidean model, the contribution to the distance function between two stimuli is per dimension weighted identically by all subjects in the same latent class. This model removes the rotational invariance of the classical multidimensional scaling model retaining psychologically meaningful dimensions, and drastically reduces the number of parameters in the traditional INDSCAL model. The probability density function for the data of a subject is posited to be a finite mixture of spherical multivariate normal densities. The maximum likelihood function is optimized by means of an EM algorithm; a modified Fisher scoring method is used to update the parameters in the M-step. A model selection strategy is proposed and illustrated on both real and artificial data.",
      "doi": "https://doi.org/10.1007/bf02294578",
      "openalex_id": "https://openalex.org/W2143966220",
      "arxiv_id": "",
      "publication_date": "1993-06-01",
      "published": "1993-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describes a latent class weighted Euclidean model for proximity data (psychometrics) with EM optimization, not about discrete audio tokens, audio language modeling, or tokenization; it does not meet the inclusion criteria focused on Discrete Audio Tokens and hence should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describes a latent class weighted Euclidean model for proximity data (psychometrics) with EM optimization, not about discrete audio tokens, audio language modeling, or tokenization; it does not meet the inclusion criteria focused on Discrete Audio Tokens and hence should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a latent class weighted Euclidean distance model for analyzing proximity data and focuses on multidimensional scaling, model parameters, and EM algorithm optimization, without mentioning any discrete audio token generation, vector quantization, or audio language modeling relevant to the inclusion criteria; thus, it does not address the core topic of discrete audio tokens or their generation and usage in sequence modeling as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a latent class weighted Euclidean distance model for analyzing proximity data and focuses on multidimensional scaling, model parameters, and EM algorithm optimization, without mentioning any discrete audio token generation, vector quantization, or audio language modeling relevant to the inclusion criteria; thus, it does not address the core topic of discrete audio tokens or their generation and usage in sequence modeling as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.",
    "abstract": "A computer program is described that is designed to reconstruct the metric configuration of a set of points in Euclidean space on the basis of essentially nonmetric information about that configuration. A minimum set of Cartesian coordinates for the points is determined when the only available information specifies for each pair of those points—not the distance between them—but some unknown, fixed monotonic function of that distance. The program is proposed as a tool for reductively analyzing several types of psychological data, particularly measures of interstimulus similarity or confusability, by making explicit the multidimensional structure underlying such data.",
    "metadata": {
      "title": "The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.",
      "summary": "A computer program is described that is designed to reconstruct the metric configuration of a set of points in Euclidean space on the basis of essentially nonmetric information about that configuration. A minimum set of Cartesian coordinates for the points is determined when the only available information specifies for each pair of those points—not the distance between them—but some unknown, fixed monotonic function of that distance. The program is proposed as a tool for reductively analyzing several types of psychological data, particularly measures of interstimulus similarity or confusability, by making explicit the multidimensional structure underlying such data.",
      "abstract": "A computer program is described that is designed to reconstruct the metric configuration of a set of points in Euclidean space on the basis of essentially nonmetric information about that configuration. A minimum set of Cartesian coordinates for the points is determined when the only available information specifies for each pair of those points—not the distance between them—but some unknown, fixed monotonic function of that distance. The program is proposed as a tool for reductively analyzing several types of psychological data, particularly measures of interstimulus similarity or confusability, by making explicit the multidimensional structure underlying such data.",
      "doi": "https://doi.org/10.1007/bf02289630",
      "openalex_id": "https://openalex.org/W2169371330",
      "arxiv_id": "",
      "publication_date": "1962-06-01",
      "published": "1962-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multidimensional scaling with nonmetric information and reconstruction of metric configurations, not on discrete audio tokens or tokenization methods for audio data, thus it does not meet the Inclusion criteria nor aligns with the topic of Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multidimensional scaling with nonmetric information and reconstruction of metric configurations, not on discrete audio tokens or tokenization methods for audio data, thus it does not meet the Inclusion criteria nor aligns with the topic of Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses a computer program for reconstructing metric configurations based on nonmetric proximity data, primarily for psychological data analysis of interstimulus similarity; it does not address discrete audio token generation, vector quantization, or any audio language modeling-related methods described in the inclusion criteria, nor does it mention any tokenization or audio representation relevant to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses a computer program for reconstructing metric configurations based on nonmetric proximity data, primarily for psychological data analysis of interstimulus similarity; it does not address discrete audio token generation, vector quantization, or any audio language modeling-related methods described in the inclusion criteria, nor does it mention any tokenization or audio representation relevant to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NONOTO: A Model-agnostic Web Interface for Interactive Music Composition by Inpainting",
    "abstract": "Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.",
    "metadata": {
      "title": "NONOTO: A Model-agnostic Web Interface for Interactive Music Composition by Inpainting",
      "summary": "Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.",
      "abstract": "Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.",
      "doi": "https://doi.org/10.48550/arxiv.1907.10380",
      "openalex_id": "https://openalex.org/W2963411769",
      "arxiv_id": "",
      "publication_date": "2019-07-23",
      "published": "2019-07-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本題未涉及離散音訊 token 的設計/量化與可重現性方法，僅為介面與在音樂生成的工作，未符合納入標的核心要件，因此建議排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "本題未涉及離散音訊 token 的設計/量化與可重現性方法，僅為介面與在音樂生成的工作，未符合納入標的核心要件，因此建議排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents NONOTO, an interface for interactive music generation using inpainting models, but does not describe any discrete audio token design or generation process involving vector quantization or similar techniques meeting the inclusion criteria. It focuses on a user interface and interactivity rather than discrete audio token modeling or representation, and no methodological details about token generation are provided, thus it does not fulfill the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents NONOTO, an interface for interactive music generation using inpainting models, but does not describe any discrete audio token design or generation process involving vector quantization or similar techniques meeting the inclusion criteria. It focuses on a user interface and interactivity rather than discrete audio token modeling or representation, and no methodological details about token generation are provided, thus it does not fulfill the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Breathy, Resonant, Pressed – Automatic Detection of Phonation Mode from Audio Recordings of Singing",
    "abstract": "Abstract In this paper we present an experiment on automatic detection of phonation modes from recordings of sustained sung vowels. We created an open dataset specifically for this experiment, containing recordings of nine vowels from multiple languages, sung by a female singer on all pitches in her vocal range in phonation modes breathy, neutral, flow (resonant) and pressed. The dataset is available under a Creative Commons license at http://www.proutskova.de/phonation-modes. First, glottal flow waveform is estimated via inverse filtering (IAIF) from audio recordings. Then six parameters of the glottal flow waveform are calculated. A 4-class Support Vector Machine classifier is constructed to separate these features into phonation mode classes. We automated the IAIF approach by computing the values of the input arguments – lip radiation and formant count – leading to the best-performing SVM classifiers (average classification accuracy over 60%), yielding a physical model for the articulation of the vowels. We examine the steps needed to generalize and extend the experimental work presented in this paper in order to apply this method in ethnomusicological investigations. Acknowledgments We would like to sincerely thank Victor Grauer, the co-inventor of Cantometrics, for suggesting to focus on the subordination of women hypothesis, which gave us the inspiration and the motivation for the current work. We are grateful to prof. Johan Sundberg for his recommendations, e.g. on the recordings set up and his general support. Our special thanks go to the peer reviewers for their balanced, insightful and fair reviews, which helped us to substantially revise the text. Notes http://www.youtube.com/watch?v=MLU0jndUGg4 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=k4SLSlSmW74 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=7iQQGBfbB0k (last accessed on 30/10/2012) http://www.youtube.com/watch?v=hRyDB4RWJdw (last accessed on 30/10/2012) http://www.youtube.com/watch?v=rgusCINe260 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=XgDrJ5Z2rKw (last accessed on 30/10/2012)",
    "metadata": {
      "title": "Breathy, Resonant, Pressed – Automatic Detection of Phonation Mode from Audio Recordings of Singing",
      "summary": "Abstract In this paper we present an experiment on automatic detection of phonation modes from recordings of sustained sung vowels. We created an open dataset specifically for this experiment, containing recordings of nine vowels from multiple languages, sung by a female singer on all pitches in her vocal range in phonation modes breathy, neutral, flow (resonant) and pressed. The dataset is available under a Creative Commons license at http://www.proutskova.de/phonation-modes. First, glottal flow waveform is estimated via inverse filtering (IAIF) from audio recordings. Then six parameters of the glottal flow waveform are calculated. A 4-class Support Vector Machine classifier is constructed to separate these features into phonation mode classes. We automated the IAIF approach by computing the values of the input arguments – lip radiation and formant count – leading to the best-performing SVM classifiers (average classification accuracy over 60%), yielding a physical model for the articulation of the vowels. We examine the steps needed to generalize and extend the experimental work presented in this paper in order to apply this method in ethnomusicological investigations. Acknowledgments We would like to sincerely thank Victor Grauer, the co-inventor of Cantometrics, for suggesting to focus on the subordination of women hypothesis, which gave us the inspiration and the motivation for the current work. We are grateful to prof. Johan Sundberg for his recommendations, e.g. on the recordings set up and his general support. Our special thanks go to the peer reviewers for their balanced, insightful and fair reviews, which helped us to substantially revise the text. Notes http://www.youtube.com/watch?v=MLU0jndUGg4 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=k4SLSlSmW74 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=7iQQGBfbB0k (last accessed on 30/10/2012) http://www.youtube.com/watch?v=hRyDB4RWJdw (last accessed on 30/10/2012) http://www.youtube.com/watch?v=rgusCINe260 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=XgDrJ5Z2rKw (last accessed on 30/10/2012)",
      "abstract": "Abstract In this paper we present an experiment on automatic detection of phonation modes from recordings of sustained sung vowels. We created an open dataset specifically for this experiment, containing recordings of nine vowels from multiple languages, sung by a female singer on all pitches in her vocal range in phonation modes breathy, neutral, flow (resonant) and pressed. The dataset is available under a Creative Commons license at http://www.proutskova.de/phonation-modes. First, glottal flow waveform is estimated via inverse filtering (IAIF) from audio recordings. Then six parameters of the glottal flow waveform are calculated. A 4-class Support Vector Machine classifier is constructed to separate these features into phonation mode classes. We automated the IAIF approach by computing the values of the input arguments – lip radiation and formant count – leading to the best-performing SVM classifiers (average classification accuracy over 60%), yielding a physical model for the articulation of the vowels. We examine the steps needed to generalize and extend the experimental work presented in this paper in order to apply this method in ethnomusicological investigations. Acknowledgments We would like to sincerely thank Victor Grauer, the co-inventor of Cantometrics, for suggesting to focus on the subordination of women hypothesis, which gave us the inspiration and the motivation for the current work. We are grateful to prof. Johan Sundberg for his recommendations, e.g. on the recordings set up and his general support. Our special thanks go to the peer reviewers for their balanced, insightful and fair reviews, which helped us to substantially revise the text. Notes http://www.youtube.com/watch?v=MLU0jndUGg4 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=k4SLSlSmW74 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=7iQQGBfbB0k (last accessed on 30/10/2012) http://www.youtube.com/watch?v=hRyDB4RWJdw (last accessed on 30/10/2012) http://www.youtube.com/watch?v=rgusCINe260 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=XgDrJ5Z2rKw (last accessed on 30/10/2012)",
      "doi": "https://doi.org/10.1080/09298215.2013.821496",
      "openalex_id": "https://openalex.org/W2099773255",
      "arxiv_id": "",
      "publication_date": "2013-06-01",
      "published": "2013-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on automatic detection of phonation modes in singing using IAIF and SVM; it does not present discrete audio tokens or tokenization methodology for language/sequence modeling, thus fails the inclusion criteria which require discrete token representations as core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on automatic detection of phonation modes in singing using IAIF and SVM; it does not present discrete audio tokens or tokenization methodology for language/sequence modeling, thus fails the inclusion criteria which require discrete token representations as core contribution.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on classifying phonation modes from audio recordings using glottal flow waveform parameters and a support vector machine, without involving discrete audio token generation or quantization methods that convert continuous audio into finite discrete token sets for language or sequence modeling. Thus, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it discuss token generation processes, vector quantization, or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on classifying phonation modes from audio recordings using glottal flow waveform parameters and a support vector machine, without involving discrete audio token generation or quantization methods that convert continuous audio into finite discrete token sets for language or sequence modeling. Thus, it does not meet the inclusion criteria centered on discrete audio token design and usage, nor does it discuss token generation processes, vector quantization, or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population",
    "abstract": "Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of 'musical sophistication' which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.",
    "metadata": {
      "title": "The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population",
      "summary": "Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of 'musical sophistication' which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.",
      "abstract": "Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of 'musical sophistication' which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.",
      "doi": "https://doi.org/10.1371/journal.pone.0089642",
      "openalex_id": "https://openalex.org/W1981455444",
      "arxiv_id": "",
      "publication_date": "2014-02-26",
      "published": "2014-02-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper develops a self-report index (Gold-MSI) to measure musical sophistication in the general population; it does not involve discrete audio tokens, tokenization, or related SSL/quantization methods, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper develops a self-report index (Gold-MSI) to measure musical sophistication in the general population; it does not involve discrete audio tokens, tokenization, or related SSL/quantization methods, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on assessing musical sophistication in the general population using a self-report index and psychometric validation, without addressing the design or generation of discrete audio tokens, vector quantization processes, or related technical methodologies for audio representation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on assessing musical sophistication in the general population using a self-report index and psychometric validation, without addressing the design or generation of discrete audio tokens, vector quantization processes, or related technical methodologies for audio representation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Universal audio synthesizer control with normalizing flows",
    "abstract": "The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders (VAE) and Normalizing Flows (NF) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live",
    "metadata": {
      "title": "Universal audio synthesizer control with normalizing flows",
      "summary": "The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders (VAE) and Normalizing Flows (NF) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live",
      "abstract": "The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders (VAE) and Normalizing Flows (NF) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live",
      "doi": "https://doi.org/10.48550/arxiv.1907.00971",
      "openalex_id": "https://openalex.org/W2955263139",
      "arxiv_id": "",
      "publication_date": "2019-07-01",
      "published": "2019-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on continuous latent-space control of a synthesizer using VAEs and normalizing flows, not on discrete audio token design (e.g., VQ/RVQ, semantic tokenization) or token-based sequence modeling, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on continuous latent-space control of a synthesizer using VAEs and normalizing flows, not on discrete audio token design (e.g., VQ/RVQ, semantic tokenization) or token-based sequence modeling, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on controlling audio synthesizers through latent audio space and invertible mappings using Variational Auto-Encoders and Normalizing Flows, without mentioning any discrete audio tokens, vector quantization, or tokenization methods that convert continuous audio signals into discrete tokens for sequence modeling; thus it does not meet the core inclusion criteria of discrete audio token design and usage for language modeling or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on controlling audio synthesizers through latent audio space and invertible mappings using Variational Auto-Encoders and Normalizing Flows, without mentioning any discrete audio tokens, vector quantization, or tokenization methods that convert continuous audio signals into discrete tokens for sequence modeling; thus it does not meet the core inclusion criteria of discrete audio token design and usage for language modeling or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear\\n Attention",
    "abstract": "Transformers achieve remarkable performance in several tasks but due to their\\nquadratic complexity, with respect to the input's length, they are\\nprohibitively slow for very long sequences. To address this limitation, we\\nexpress the self-attention as a linear dot-product of kernel feature maps and\\nmake use of the associativity property of matrix products to reduce the\\ncomplexity from $\\\\mathcal{O}\\\\left(N^2\\\\right)$ to $\\\\mathcal{O}\\\\left(N\\\\right)$,\\nwhere $N$ is the sequence length. We show that this formulation permits an\\niterative implementation that dramatically accelerates autoregressive\\ntransformers and reveals their relationship to recurrent neural networks. Our\\nlinear transformers achieve similar performance to vanilla transformers and\\nthey are up to 4000x faster on autoregressive prediction of very long\\nsequences.\\n",
    "metadata": {
      "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear\\n Attention",
      "summary": "Transformers achieve remarkable performance in several tasks but due to their\\nquadratic complexity, with respect to the input's length, they are\\nprohibitively slow for very long sequences. To address this limitation, we\\nexpress the self-attention as a linear dot-product of kernel feature maps and\\nmake use of the associativity property of matrix products to reduce the\\ncomplexity from $\\\\mathcal{O}\\\\left(N^2\\\\right)$ to $\\\\mathcal{O}\\\\left(N\\\\right)$,\\nwhere $N$ is the sequence length. We show that this formulation permits an\\niterative implementation that dramatically accelerates autoregressive\\ntransformers and reveals their relationship to recurrent neural networks. Our\\nlinear transformers achieve similar performance to vanilla transformers and\\nthey are up to 4000x faster on autoregressive prediction of very long\\nsequences.\\n",
      "abstract": "Transformers achieve remarkable performance in several tasks but due to their\\nquadratic complexity, with respect to the input's length, they are\\nprohibitively slow for very long sequences. To address this limitation, we\\nexpress the self-attention as a linear dot-product of kernel feature maps and\\nmake use of the associativity property of matrix products to reduce the\\ncomplexity from $\\\\mathcal{O}\\\\left(N^2\\\\right)$ to $\\\\mathcal{O}\\\\left(N\\\\right)$,\\nwhere $N$ is the sequence length. We show that this formulation permits an\\niterative implementation that dramatically accelerates autoregressive\\ntransformers and reveals their relationship to recurrent neural networks. Our\\nlinear transformers achieve similar performance to vanilla transformers and\\nthey are up to 4000x faster on autoregressive prediction of very long\\nsequences.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2006.16236",
      "openalex_id": "https://openalex.org/W3037798801",
      "arxiv_id": "",
      "publication_date": "2020-06-29",
      "published": "2020-06-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes fast autoregressive transformers with linear attention, focusing on computational efficiency rather than discrete audio token representations, tokenization, or audio-language modeling as defined by the inclusion criteria; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes fast autoregressive transformers with linear attention, focusing on computational efficiency rather than discrete audio token representations, tokenization, or audio-language modeling as defined by the inclusion criteria; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving transformer architectures by reformulating self-attention to reduce computational complexity and speed up autoregressive prediction, without mentioning discrete audio tokens, token quantization, or audio-related token generation methods as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving transformer architectures by reformulating self-attention to reduce computational complexity and speed up autoregressive prediction, without mentioning discrete audio tokens, token quantization, or audio-related token generation methods as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Science of the Singing Voice",
    "abstract": "Althought there are numerous books dealing with the science and acoustics of speech, there are relatively few that deal with the singing voice as distinct from the speaking voice. Now, Johan Sundberg's The Science of the Singing Voice illustrated with over a hundred instructive and significant diagrams and drawings thoroughly describes the structure and functions of the vocal organs in singing, from the aerodynamics of respiration through the dynamics of articulation.",
    "metadata": {
      "title": "The Science of the Singing Voice",
      "summary": "Althought there are numerous books dealing with the science and acoustics of speech, there are relatively few that deal with the singing voice as distinct from the speaking voice. Now, Johan Sundberg's The Science of the Singing Voice illustrated with over a hundred instructive and significant diagrams and drawings thoroughly describes the structure and functions of the vocal organs in singing, from the aerodynamics of respiration through the dynamics of articulation.",
      "abstract": "Althought there are numerous books dealing with the science and acoustics of speech, there are relatively few that deal with the singing voice as distinct from the speaking voice. Now, Johan Sundberg's The Science of the Singing Voice illustrated with over a hundred instructive and significant diagrams and drawings thoroughly describes the structure and functions of the vocal organs in singing, from the aerodynamics of respiration through the dynamics of articulation.",
      "doi": "",
      "openalex_id": "https://openalex.org/W1581879603",
      "arxiv_id": "",
      "publication_date": "1987-01-01",
      "published": "1987-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on the science of the singing voice and vocal physiology, not on discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on the science of the singing voice and vocal physiology, not on discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the science and acoustics of the singing voice, describing vocal organ functions and aerodynamics, but do not mention discrete audio tokens, token generation processes, neural audio codecs, or any related methods that produce or utilize discrete tokens for audio modeling or language tasks, thus failing to meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the science and acoustics of the singing voice, describing vocal organ functions and aerodynamics, but do not mention discrete audio tokens, token generation processes, neural audio codecs, or any related methods that produce or utilize discrete tokens for audio modeling or language tasks, thus failing to meet the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analysis of Individual Differences in Multidimensional Scaling Via an N-way Generalization of “Eckart-Young” Decomposition",
    "abstract": "An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.",
    "metadata": {
      "title": "Analysis of Individual Differences in Multidimensional Scaling Via an N-way Generalization of “Eckart-Young” Decomposition",
      "summary": "An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.",
      "abstract": "An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.",
      "doi": "https://doi.org/10.1007/bf02310791",
      "openalex_id": "https://openalex.org/W2000215628",
      "arxiv_id": "",
      "publication_date": "1970-09-01",
      "published": "1970-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with multidimensional scaling and Eckart-Young decomposition, not with discrete audio token representations or token-based audio modeling, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with multidimensional scaling and Eckart-Young decomposition, not with discrete audio token representations or token-based audio modeling, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an individual differences model for multidimensional scaling with a generalized Eckart-Young decomposition method applied to similarity data, focusing on auditory stimuli and perception, without mentioning any discrete audio token generation, vector quantization, or sequence modeling relevant to the defined inclusion criteria; thus, it does not meet the inclusion requirements nor address the discrete audio tokens topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an individual differences model for multidimensional scaling with a generalized Eckart-Young decomposition method applied to similarity data, focusing on auditory stimuli and perception, without mentioning any discrete audio token generation, vector quantization, or sequence modeling relevant to the defined inclusion criteria; thus, it does not meet the inclusion requirements nor address the discrete audio tokens topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Drum Machine : An Interactive System for Real-time Synthesis of\\n Drum Sounds",
    "abstract": "In this work, we introduce a system for real-time generation of drum sounds.\\nThis system is composed of two parts: a generative model for drum sounds\\ntogether with a Max4Live plugin providing intuitive controls on the generative\\nprocess. The generative model consists of a Conditional Wasserstein autoencoder\\n(CWAE), which learns to generate Mel-scaled magnitude spectrograms of short\\npercussion samples, coupled with a Multi-Head Convolutional Neural Network\\n(MCNN) which estimates the corresponding audio signal from the magnitude\\nspectrogram. The design of this model makes it lightweight, so that it allows\\none to perform real-time generation of novel drum sounds on an average CPU,\\nremoving the need for the users to possess dedicated hardware in order to use\\nthis system. We then present our Max4Live interface designed to interact with\\nthis generative model. With this setup, the system can be easily integrated\\ninto a studio-production environment and enhance the creative process. Finally,\\nwe discuss the advantages of our system and how the interaction of music\\nproducers with such tools could change the way drum tracks are composed.\\n",
    "metadata": {
      "title": "Neural Drum Machine : An Interactive System for Real-time Synthesis of\\n Drum Sounds",
      "summary": "In this work, we introduce a system for real-time generation of drum sounds.\\nThis system is composed of two parts: a generative model for drum sounds\\ntogether with a Max4Live plugin providing intuitive controls on the generative\\nprocess. The generative model consists of a Conditional Wasserstein autoencoder\\n(CWAE), which learns to generate Mel-scaled magnitude spectrograms of short\\npercussion samples, coupled with a Multi-Head Convolutional Neural Network\\n(MCNN) which estimates the corresponding audio signal from the magnitude\\nspectrogram. The design of this model makes it lightweight, so that it allows\\none to perform real-time generation of novel drum sounds on an average CPU,\\nremoving the need for the users to possess dedicated hardware in order to use\\nthis system. We then present our Max4Live interface designed to interact with\\nthis generative model. With this setup, the system can be easily integrated\\ninto a studio-production environment and enhance the creative process. Finally,\\nwe discuss the advantages of our system and how the interaction of music\\nproducers with such tools could change the way drum tracks are composed.\\n",
      "abstract": "In this work, we introduce a system for real-time generation of drum sounds.\\nThis system is composed of two parts: a generative model for drum sounds\\ntogether with a Max4Live plugin providing intuitive controls on the generative\\nprocess. The generative model consists of a Conditional Wasserstein autoencoder\\n(CWAE), which learns to generate Mel-scaled magnitude spectrograms of short\\npercussion samples, coupled with a Multi-Head Convolutional Neural Network\\n(MCNN) which estimates the corresponding audio signal from the magnitude\\nspectrogram. The design of this model makes it lightweight, so that it allows\\none to perform real-time generation of novel drum sounds on an average CPU,\\nremoving the need for the users to possess dedicated hardware in order to use\\nthis system. We then present our Max4Live interface designed to interact with\\nthis generative model. With this setup, the system can be easily integrated\\ninto a studio-production environment and enhance the creative process. Finally,\\nwe discuss the advantages of our system and how the interaction of music\\nproducers with such tools could change the way drum tracks are composed.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1907.02637",
      "openalex_id": "https://openalex.org/W3029579848",
      "arxiv_id": "",
      "publication_date": "2019-07-04",
      "published": "2019-07-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets real-time drum-sound synthesis using a CWAE and a neural decoder with a Max4Live interface, and does not discuss discrete audio tokens or token-based representations, so it fails the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets real-time drum-sound synthesis using a CWAE and a neural decoder with a Max4Live interface, and does not discuss discrete audio tokens or token-based representations, so it fails the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a system for real-time drum sound synthesis using a Conditional Wasserstein autoencoder and a neural network to generate audio spectrograms and audio signals. However, it does not describe any discrete audio token discretization process, vector quantization, or token generation mechanisms that serve as linguistic or sequence-modeling tokens. The work focuses on generating audio signals in real-time rather than producing or employing discrete audio tokens for downstream language or sequence modeling tasks as specified by the inclusion criteria. Thus, it does not meet the core requirement of discrete token design and application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a system for real-time drum sound synthesis using a Conditional Wasserstein autoencoder and a neural network to generate audio spectrograms and audio signals. However, it does not describe any discrete audio token discretization process, vector quantization, or token generation mechanisms that serve as linguistic or sequence-modeling tokens. The work focuses on generating audio signals in real-time rather than producing or employing discrete audio tokens for downstream language or sequence modeling tasks as specified by the inclusion criteria. Thus, it does not meet the core requirement of discrete token design and application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Hierarchical Representations for Expressive Speaking Style in End-to-End Speech Synthesis",
    "abstract": "Although Global Style Tokens (GSTs) are a recently-proposed method to uncover expressive factors of variation in speaking style, they are a mixture of style attributes without explicitly considering the factorization of multiple-level speaking styles. In this work, we introduce a hierarchical GST architecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech. We make hierarchical evaluations conditioned on individual tokens from different GST layers. As the number of layers increases, we tend to observe a coarse to fine style decomposition. For example, the first GST layer learns a good representation of speaker IDs while finer speaking style or emotion variations can be found in higher-level layers. Meanwhile, the proposed model shows good performance of style transfer.",
    "metadata": {
      "title": "Learning Hierarchical Representations for Expressive Speaking Style in End-to-End Speech Synthesis",
      "summary": "Although Global Style Tokens (GSTs) are a recently-proposed method to uncover expressive factors of variation in speaking style, they are a mixture of style attributes without explicitly considering the factorization of multiple-level speaking styles. In this work, we introduce a hierarchical GST architecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech. We make hierarchical evaluations conditioned on individual tokens from different GST layers. As the number of layers increases, we tend to observe a coarse to fine style decomposition. For example, the first GST layer learns a good representation of speaker IDs while finer speaking style or emotion variations can be found in higher-level layers. Meanwhile, the proposed model shows good performance of style transfer.",
      "abstract": "Although Global Style Tokens (GSTs) are a recently-proposed method to uncover expressive factors of variation in speaking style, they are a mixture of style attributes without explicitly considering the factorization of multiple-level speaking styles. In this work, we introduce a hierarchical GST architecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech. We make hierarchical evaluations conditioned on individual tokens from different GST layers. As the number of layers increases, we tend to observe a coarse to fine style decomposition. For example, the first GST layer learns a good representation of speaker IDs while finer speaking style or emotion variations can be found in higher-level layers. Meanwhile, the proposed model shows good performance of style transfer.",
      "doi": "https://doi.org/10.1109/asru46091.2019.9003859",
      "openalex_id": "https://openalex.org/W3007067948",
      "arxiv_id": "",
      "publication_date": "2019-12-01",
      "published": "2019-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses hierarchical Global Style Tokens for expressive TTS but does not describe discrete audio token generation/quantization (e.g., VQ/RVQ, hierarchical tokenization) nor a token-centric approach for audio-language modeling, so it fails the discrete-audio-token inclusion criteria while meeting no exclusion triggers.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses hierarchical Global Style Tokens for expressive TTS but does not describe discrete audio token generation/quantization (e.g., VQ/RVQ, hierarchical tokenization) nor a token-centric approach for audio-language modeling, so it fails the discrete-audio-token inclusion criteria while meeting no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a hierarchical Global Style Tokens (GST) architecture to learn disentangled representations for expressive speaking styles in speech synthesis, but it does not explicitly describe any discrete audio token generation process such as vector quantization, residual vector quantization, or similar discrete token design methods that are central to the inclusion criteria. The work focuses on style representation and transfer rather than producing discrete tokens for language or sequence model usage, and lacks detailed discussion of token quantization or formation processes. Therefore, it does not meet the inclusion criteria which require discrete token design as a core contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a hierarchical Global Style Tokens (GST) architecture to learn disentangled representations for expressive speaking styles in speech synthesis, but it does not explicitly describe any discrete audio token generation process such as vector quantization, residual vector quantization, or similar discrete token design methods that are central to the inclusion criteria. The work focuses on style representation and transfer rather than producing discrete tokens for language or sequence model usage, and lacks detailed discussion of token quantization or formation processes. Therefore, it does not meet the inclusion criteria which require discrete token design as a core contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An Effective Style Token Weight Control Technique for End-to-End Emotional Speech Synthesis",
    "abstract": "In this letter, we propose a high-quality emotional speech synthesis system, using emotional vector space, i.e., the weighted sum of global style tokens (GSTs). Our previous research verified the feasibility of GST-based emotional speech synthesis in an end-to-end text-to-speech synthesis framework. However, selecting appropriate reference audio (RA) signals to extract emotion embedding vectors to the specific types of target emotions remains problematic. To ameliorate the selection problem, we propose an effective way of generating emotion embedding vectors by utilizing the trained GSTs. By assuming that the trained GSTs represent an emotional vector space, we first investigate the distribution of all the training samples depending on the type of each emotion. We then regard the centroid of the distribution as an emotion-specific weighting value, which effectively controls the expressiveness of synthesized speech, even without using the RA for guidance, as it did before. Finally, we confirm that the proposed controlled weight-based method is superior to the conventional emotion label-based methods in terms of perceptual quality and emotion classification accuracy.",
    "metadata": {
      "title": "An Effective Style Token Weight Control Technique for End-to-End Emotional Speech Synthesis",
      "summary": "In this letter, we propose a high-quality emotional speech synthesis system, using emotional vector space, i.e., the weighted sum of global style tokens (GSTs). Our previous research verified the feasibility of GST-based emotional speech synthesis in an end-to-end text-to-speech synthesis framework. However, selecting appropriate reference audio (RA) signals to extract emotion embedding vectors to the specific types of target emotions remains problematic. To ameliorate the selection problem, we propose an effective way of generating emotion embedding vectors by utilizing the trained GSTs. By assuming that the trained GSTs represent an emotional vector space, we first investigate the distribution of all the training samples depending on the type of each emotion. We then regard the centroid of the distribution as an emotion-specific weighting value, which effectively controls the expressiveness of synthesized speech, even without using the RA for guidance, as it did before. Finally, we confirm that the proposed controlled weight-based method is superior to the conventional emotion label-based methods in terms of perceptual quality and emotion classification accuracy.",
      "abstract": "In this letter, we propose a high-quality emotional speech synthesis system, using emotional vector space, i.e., the weighted sum of global style tokens (GSTs). Our previous research verified the feasibility of GST-based emotional speech synthesis in an end-to-end text-to-speech synthesis framework. However, selecting appropriate reference audio (RA) signals to extract emotion embedding vectors to the specific types of target emotions remains problematic. To ameliorate the selection problem, we propose an effective way of generating emotion embedding vectors by utilizing the trained GSTs. By assuming that the trained GSTs represent an emotional vector space, we first investigate the distribution of all the training samples depending on the type of each emotion. We then regard the centroid of the distribution as an emotion-specific weighting value, which effectively controls the expressiveness of synthesized speech, even without using the RA for guidance, as it did before. Finally, we confirm that the proposed controlled weight-based method is superior to the conventional emotion label-based methods in terms of perceptual quality and emotion classification accuracy.",
      "doi": "https://doi.org/10.1109/lsp.2019.2931673",
      "openalex_id": "https://openalex.org/W2966387353",
      "arxiv_id": "",
      "publication_date": "2019-07-29",
      "published": "2019-07-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on emotion style tokens (GSTs) and a weighting/control method for end-to-end emotional speech synthesis, but it does not involve discrete audio tokens or a tokenization/quantization process as described in the inclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on emotion style tokens (GSTs) and a weighting/control method for end-to-end emotional speech synthesis, but it does not involve discrete audio tokens or a tokenization/quantization process as described in the inclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on emotional speech synthesis using weighted sums of global style tokens (GSTs) for embedding emotion vectors, but it does not describe a process of discrete audio token generation involving vector quantization, residual quantization, or semantic-driven discretization mechanisms as required by the inclusion criteria. The tokens here appear to be continuous style embeddings used for emotional control rather than discrete tokens designed for sequence modeling or language model use. Additionally, no detailed description of token quantization, token formation, or use in sequence modeling is provided, which is crucial for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on emotional speech synthesis using weighted sums of global style tokens (GSTs) for embedding emotion vectors, but it does not describe a process of discrete audio token generation involving vector quantization, residual quantization, or semantic-driven discretization mechanisms as required by the inclusion criteria. The tokens here appear to be continuous style embeddings used for emotional control rather than discrete tokens designed for sequence modeling or language model use. Additionally, no detailed description of token quantization, token formation, or use in sequence modeling is provided, which is crucial for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Voice: Real-time Neural Text-to-Speech",
    "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
    "metadata": {
      "title": "Deep Voice: Real-time Neural Text-to-Speech",
      "summary": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
      "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",
      "doi": "https://doi.org/10.48550/arxiv.1702.07825",
      "openalex_id": "https://openalex.org/W2591927543",
      "arxiv_id": "",
      "publication_date": "2017-02-25",
      "published": "2017-02-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work presents a real-time neural TTS system with WaveNet-like synthesis and component models, but it does not focus on discrete audio tokens or token-based discretization (e.g., VQ/RVQ, hierarchical quantization) as core contributions, so it fails the specified inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work presents a real-time neural TTS system with WaveNet-like synthesis and component models, but it does not focus on discrete audio tokens or token-based discretization (e.g., VQ/RVQ, hierarchical quantization) as core contributions, so it fails the specified inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a neural text-to-speech (TTS) system composed of several neural network components to synthesize speech from text. However, it does not mention generating or using discrete audio tokens formed by vector quantization or similar methods as defined in the inclusion criteria. The system emphasizes end-to-end neural TTS and optimized inference speed but lacks details on tokenization of audio into discrete tokens used for sequence modeling or downstream tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a neural text-to-speech (TTS) system composed of several neural network components to synthesize speech from text. However, it does not mention generating or using discrete audio tokens formed by vector quantization or similar methods as defined in the inclusion criteria. The system emphasizes end-to-end neural TTS and optimized inference speed but lacks details on tokenization of audio into discrete tokens used for sequence modeling or downstream tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "YIN, a fundamental frequency estimator for speech and music",
    "abstract": "An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.",
    "metadata": {
      "title": "YIN, a fundamental frequency estimator for speech and music",
      "summary": "An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.",
      "abstract": "An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.",
      "doi": "https://doi.org/10.1121/1.1458024",
      "openalex_id": "https://openalex.org/W2091425152",
      "arxiv_id": "",
      "publication_date": "2002-04-01",
      "published": "2002-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a fundamental frequency estimator (F0) for speech/music and does not address discrete audio tokens, token generation/quantization, or related down-stream token-based models.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a fundamental frequency estimator (F0) for speech/music and does not address discrete audio tokens, token generation/quantization, or related down-stream token-based models.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an algorithm for estimating fundamental frequency in speech and music using autocorrelation improvements, focusing on signal processing for pitch detection; there is no mention of generating discrete audio tokens, vector quantization, or any tokenization process intended for sequence or language modeling. Therefore, it does not meet the inclusion criteria centered around discrete audio token design and usage for modeling tasks, nor does it meet exclusion criteria that would automatically exclude it due to lacking technical detail, so the study should be excluded based on non-relevance to the discrete token theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an algorithm for estimating fundamental frequency in speech and music using autocorrelation improvements, focusing on signal processing for pitch detection; there is no mention of generating discrete audio tokens, vector quantization, or any tokenization process intended for sequence or language modeling. Therefore, it does not meet the inclusion criteria centered around discrete audio token design and usage for modeling tasks, nor does it meet exclusion criteria that would automatically exclude it due to lacking technical detail, so the study should be excluded based on non-relevance to the discrete token theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
    "abstract": "India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
    "metadata": {
      "title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages",
      "summary": "India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
      "abstract": "India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",
      "doi": "https://doi.org/10.48550/arxiv.2305.16307",
      "openalex_id": "https://openalex.org/W4378505287",
      "arxiv_id": "",
      "publication_date": "2023-05-25",
      "published": "2023-05-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns MT for 22 Indian languages and datasets/models release; it does not address discrete audio token representations or any tokenization/quantization methods for audio, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns MT for 22 Indian languages and datasets/models release; it does not address discrete audio token representations or any tokenization/quantization methods for audio, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on machine translation models for Indian languages and does not discuss discrete audio token generation, vector quantization, or related audio token designs as defined in the inclusion criteria. It lacks detailed methodology about discrete token formation or usage within language or sequence models, and primarily addresses text-based language models for translation, not audio representation or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on machine translation models for Indian languages and does not discuss discrete audio token generation, vector quantization, or related audio token designs as defined in the inclusion criteria. It lacks detailed methodology about discrete token formation or usage within language or sequence models, and primarily addresses text-based language models for translation, not audio representation or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Machine Translation System for English to Indian Language Translation Using MTIL Parallel Corpus",
    "abstract": "Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.",
    "metadata": {
      "title": "Neural Machine Translation System for English to Indian Language Translation Using MTIL Parallel Corpus",
      "summary": "Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.",
      "abstract": "Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.",
      "doi": "https://doi.org/10.1515/jisys-2019-2510",
      "openalex_id": "https://openalex.org/W2924093092",
      "arxiv_id": "",
      "publication_date": "2019-03-20",
      "published": "2019-03-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about neural machine translation for English–Indian language text data and English–Malayalam/Hindi/Tamil/Punjabi translation, not about discrete audio tokens or audio tokenization; it fails the inclusion criteria focused on Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about neural machine translation for English–Indian language text data and English–Malayalam/Hindi/Tamil/Punjabi translation, not about discrete audio tokens or audio tokenization; it fails the inclusion criteria focused on Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural machine translation for English to Indian languages using parallel corpora, without addressing discrete audio tokens, audio representation quantization, or related audio sequence modeling as required by the inclusion criteria. It lacks any discussion of transforming continuous audio signals into discrete tokens, vector quantization methods, or relevant sequence model usage. Thus, it does not meet inclusion criteria and is unrelated to the specific topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural machine translation for English to Indian languages using parallel corpora, without addressing discrete audio tokens, audio representation quantization, or related audio sequence modeling as required by the inclusion criteria. It lacks any discussion of transforming continuous audio signals into discrete tokens, vector quantization methods, or relevant sequence model usage. Thus, it does not meet inclusion criteria and is unrelated to the specific topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
    "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
    "metadata": {
      "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
      "summary": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
      "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
      "doi": "https://doi.org/10.48550/arxiv.2305.14716",
      "openalex_id": "https://openalex.org/W4378473793",
      "arxiv_id": "",
      "publication_date": "2023-05-24",
      "published": "2023-05-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on GlobalBench and multilingual NLP benchmarks, with no content on discrete audio tokens or methods for converting audio to discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on GlobalBench and multilingual NLP benchmarks, with no content on discrete audio tokens or methods for converting audio to discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article introduces GlobalBench, a benchmark focused on multilingual NLP progress and equitable language technology, but it does not address discrete audio token design, generation, or usage for audio language modeling or cross-modal generation as defined in the inclusion criteria. It also lacks details on tokenization or quantization processes applicable to audio data, which are core to the topic, and instead centers on NLP benchmarking across languages without relevant technical aspects related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article introduces GlobalBench, a benchmark focused on multilingual NLP progress and equitable language technology, but it does not address discrete audio token design, generation, or usage for audio language modeling or cross-modal generation as defined in the inclusion criteria. It also lacks details on tokenization or quantization processes applicable to audio data, which are core to the topic, and instead centers on NLP benchmarking across languages without relevant technical aspects related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Developing a Speech Recognition System for Recognizing Tonal Speech Signals Using a Convolutional Neural Network",
    "abstract": "Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.",
    "metadata": {
      "title": "Developing a Speech Recognition System for Recognizing Tonal Speech Signals Using a Convolutional Neural Network",
      "summary": "Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.",
      "abstract": "Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.",
      "doi": "https://doi.org/10.3390/app12126223",
      "openalex_id": "https://openalex.org/W4283121045",
      "arxiv_id": "",
      "publication_date": "2022-06-19",
      "published": "2022-06-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on CNN-based speech recognition for tonal speech using MFCC features, with no mention of discrete audio tokens, token generation/quantization, or token-based sequence modeling, thus failing the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on CNN-based speech recognition for tonal speech using MFCC features, with no mention of discrete audio tokens, token generation/quantization, or token-based sequence modeling, thus failing the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on developing a CNN-based speech recognition system for tonal speech signals, specifically for recognizing Gurbani hymns, using standard feature extraction techniques (MFCC) without mentioning any discrete audio token generation method such as vector quantization (VQ), residual vector quantization (RVQ), or hierarchical quantization that produce discrete tokens for sequence modeling; hence, it does not meet the inclusion criteria requiring discrete token design and generation process description, nor does it clearly serve as a foundation for audio language modeling or sequence modeling with discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on developing a CNN-based speech recognition system for tonal speech signals, specifically for recognizing Gurbani hymns, using standard feature extraction techniques (MFCC) without mentioning any discrete audio token generation method such as vector quantization (VQ), residual vector quantization (RVQ), or hierarchical quantization that produce discrete tokens for sequence modeling; hence, it does not meet the inclusion criteria requiring discrete token design and generation process description, nor does it clearly serve as a foundation for audio language modeling or sequence modeling with discrete tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
    "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .",
    "metadata": {
      "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
      "summary": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .",
      "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .",
      "doi": "https://doi.org/10.48550/arxiv.2204.02967",
      "openalex_id": "https://openalex.org/W4226543485",
      "arxiv_id": "",
      "publication_date": "2022-04-06",
      "published": "2022-04-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses encoding target speech into discrete representations via the S2UT framework for direct S2ST, with pre-training and decoder design centered on discrete units, aligning with the discrete audio token topic, and it is a parametric research paper rather than a demo, though full methodological details would be in the full text.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses encoding target speech into discrete representations via the S2UT framework for direct S2ST, with pre-training and decoder design centered on discrete units, aligning with the discrete audio token topic, and it is a parametric research paper rather than a demo, though full methodological details would be in the full text.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study uses a speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, which aligns well with the inclusion criteria focusing on discrete audio tokens generated via quantization methods for sequence modeling and generation tasks. However, the abstract does not provide detailed information about the token generation process, quantization method, or explicit description of the discrete token structure and usage beyond mentioning discrete representations. Although the research clearly involves discrete representations and applies self-supervised learning and data augmentation, the lack of detailed methodological transparency regarding token generation and quantitative descriptions makes it difficult to fully assess compliance with all inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study uses a speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, which aligns well with the inclusion criteria focusing on discrete audio tokens generated via quantization methods for sequence modeling and generation tasks. However, the abstract does not provide detailed information about the token generation process, quantization method, or explicit description of the discrete token structure and usage beyond mentioning discrete representations. Although the research clearly involves discrete representations and applies self-supervised learning and data augmentation, the lack of detailed methodological transparency regarding token generation and quantitative descriptions makes it difficult to fully assess compliance with all inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers note this work uses a speech-to-unit translation (S2UT) framework that explicitly encodes target speech into discrete representations, which places discrete audio tokens at the core of the method (JuniorNano: inclines to include; JuniorMini: cautious due to lack of token-generation details in the abstract). While the abstract does not spell out the exact quantization/codec/bitrate or codebook details required by the inclusion checklist, the paper’s focus on S2UT and on pre-training the discrete-unit decoder strongly suggests the discrete-token design and usage are a primary contribution rather than a peripheral detail; moreover this is a formal research contribution (not a demo/blog). Therefore I lean toward inclusion but flag that final inclusion should confirm the full paper contains the required methodological details (token generation/quantization and reproducible settings). Score explanation: The study centers on discrete-unit S2UT and likely meets the topic scope, but the abstract lacks explicit quantization/token-formation details so inclusion is recommended pending confirmation in the full text.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers note this work uses a speech-to-unit translation (S2UT) framework that explicitly encodes target speech into discrete representations, which places discrete audio tokens at the core of the method (JuniorNano: inclines to include; JuniorMini: cautious due to lack of token-generation details in the abstract). While the abstract does not spell out the exact quantization/codec/bitrate or codebook details required by the inclusion checklist, the paper’s focus on S2UT and on pre-training the discrete-unit decoder strongly suggests the discrete-token design and usage are a primary contribution rather than a peripheral detail; moreover this is a formal research contribution (not a demo/blog). Therefore I lean toward inclusion but flag that final inclusion should confirm the full paper contains the required methodological details (token generation/quantization and reproducible settings). Score explanation: The study centers on discrete-unit S2UT and likely meets the topic scope, but the abstract lacks explicit quantization/token-formation details so inclusion is recommended pending confirmation in the full text.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multilingual Speech-to-Speech Translation into Multiple Target Languages",
    "abstract": "Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",
    "metadata": {
      "title": "Multilingual Speech-to-Speech Translation into Multiple Target Languages",
      "summary": "Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",
      "abstract": "Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",
      "doi": "https://doi.org/10.48550/arxiv.2307.08655",
      "openalex_id": "https://openalex.org/W4384648564",
      "arxiv_id": "",
      "publication_date": "2023-07-17",
      "published": "2023-07-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work discusses 'speech-to-unit' concepts and a multilingual vocoder, it does not clearly present discrete audio token design/quantization as its core contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work discusses 'speech-to-unit' concepts and a multilingual vocoder, it does not clearly present discrete audio token design/quantization as its core contribution, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a multilingual speech-to-speech translation system leveraging speech-to-unit (S2U) and vocoder components with multilingual capabilities, including a proposed speech-to-masked-unit extension to reduce language interference. However, it does not clearly describe the process of generating discrete audio tokens, their quantization, codebook design, or how these tokens are used explicitly for sequence or language modeling, which is essential per the inclusion criteria. The focus is more on translation performance rather than on discrete token design or representation. Therefore, the study does not meet the required inclusion criteria around discrete audio token generation and methodology detail.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a multilingual speech-to-speech translation system leveraging speech-to-unit (S2U) and vocoder components with multilingual capabilities, including a proposed speech-to-masked-unit extension to reduce language interference. However, it does not clearly describe the process of generating discrete audio tokens, their quantization, codebook design, or how these tokens are used explicitly for sequence or language modeling, which is essential per the inclusion criteria. The focus is more on translation performance rather than on discrete token design or representation. Therefore, the study does not meet the required inclusion criteria around discrete audio token generation and methodology detail.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Towards Speech to Speech Machine Translation focusing on Indian Languages",
    "abstract": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.",
    "metadata": {
      "title": "Towards Speech to Speech Machine Translation focusing on Indian Languages",
      "summary": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.",
      "abstract": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.",
      "doi": "https://doi.org/10.18653/v1/2023.eacl-demo.19",
      "openalex_id": "https://openalex.org/W4386566860",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a full speech-to-speech translation pipeline for Indian languages with ASR/MT/TTS and human-in-the-loop, not on discrete audio tokens or tokenization methods; it lacks the required tokenization-based methodology and technical detail to support reproducibility.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a full speech-to-speech translation pipeline for Indian languages with ASR/MT/TTS and human-in-the-loop, not on discrete audio tokens or tokenization methods; it lacks the required tokenization-based methodology and technical detail to support reproducibility.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a complete Speech to Speech Machine Translation system for Indian languages, focusing on cascading linguistic modules like ASR, MT, TTS, and human intervention; however, it does not describe any discrete audio tokenization methods, vector quantization, or related processes central to discrete audio tokens, nor does it provide technical details about token generation or their use for sequence modeling. The focus is on system integration and user experience, lacking the methodological details or token-based approach necessary to meet the inclusion criteria and avoid the exclusions related to insufficient methodological transparency.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a complete Speech to Speech Machine Translation system for Indian languages, focusing on cascading linguistic modules like ASR, MT, TTS, and human intervention; however, it does not describe any discrete audio tokenization methods, vector quantization, or related processes central to discrete audio tokens, nor does it provide technical details about token generation or their use for sequence modeling. The focus is on system integration and user experience, lacking the methodological details or token-based approach necessary to meet the inclusion criteria and avoid the exclusions related to insufficient methodological transparency.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ClassRoute: An English to Punjabi Educational Video Translation Pipeline for Supporting Punjabi Mother-Tongue Education",
    "abstract": "Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.",
    "metadata": {
      "title": "ClassRoute: An English to Punjabi Educational Video Translation Pipeline for Supporting Punjabi Mother-Tongue Education",
      "summary": "Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.",
      "abstract": "Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.",
      "doi": "https://doi.org/10.1109/ghtc53159.2021.9612485",
      "openalex_id": "https://openalex.org/W3215465553",
      "arxiv_id": "",
      "publication_date": "2021-10-19",
      "published": "2021-10-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on translating English educational videos into Punjabi and does not address discrete audio tokens or token-based audio representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on translating English educational videos into Punjabi and does not address discrete audio tokens or token-based audio representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an educational video translation pipeline from English to Punjabi to support mother-tongue education, emphasizing translation and cultural adaptation rather than the discrete audio tokenization or vector quantization techniques defined in the inclusion criteria. No technical details about audio token generation, vector quantization, or sequence modeling with discrete tokens are provided. Therefore, the study does not meet the essential inclusion criteria regarding discrete audio token design and usage as a core method or contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an educational video translation pipeline from English to Punjabi to support mother-tongue education, emphasizing translation and cultural adaptation rather than the discrete audio tokenization or vector quantization techniques defined in the inclusion criteria. No technical details about audio token generation, vector quantization, or sequence modeling with discrete tokens are provided. Therefore, the study does not meet the essential inclusion criteria regarding discrete audio token design and usage as a core method or contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
    "abstract": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
    "metadata": {
      "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
      "summary": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
      "abstract": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
      "doi": "https://doi.org/10.48550/arxiv.2212.05409",
      "openalex_id": "https://openalex.org/W4311550865",
      "arxiv_id": "",
      "publication_date": "2022-12-11",
      "published": "2022-12-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses Indic language NLP resources and models (IndicCorp, IndicXTREME, IndicBERT v2) but does not address discrete audio tokens or audio-language modeling; it fails the inclusion criteria focused on discrete audio token representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses Indic language NLP resources and models (IndicCorp, IndicXTREME, IndicBERT v2) but does not address discrete audio tokens or audio-language modeling; it fails the inclusion criteria focused on discrete audio token representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on building monolingual corpora, benchmarks, and multilingual language models for Indic languages, without any discussion or development of discrete audio tokens or related audio tokenization methods as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria and is unrelated to the discrete audio token research scope specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on building monolingual corpora, benchmarks, and multilingual language models for Indic languages, without any discussion or development of discrete audio tokens or related audio tokenization methods as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria and is unrelated to the discrete audio token research scope specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Common Voice: A Massively-Multilingual Speech Corpus",
    "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    "metadata": {
      "title": "Common Voice: A Massively-Multilingual Speech Corpus",
      "summary": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1912.06670",
      "openalex_id": "https://openalex.org/W2995929068",
      "arxiv_id": "",
      "publication_date": "2019-12-13",
      "published": "2019-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a multilingual speech corpus and ASR experiments; it does not present discrete audio token representations or tokenization methods, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a multilingual speech corpus and ASR experiments; it does not present discrete audio token representations or tokenization methods, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes the Common Voice corpus, a large multilingual speech dataset aimed at automatic speech recognition research, but does not focus on discrete audio token design or the generation of tokenized audio representations through vector quantization or similar methods; it lacks details on token generation, quantization, or their use in sequence modeling, which are core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes the Common Voice corpus, a large multilingual speech dataset aimed at automatic speech recognition research, but does not focus on discrete audio token design or the generation of tokenized audio representations through vector quantization or similar methods; it lacks details on token generation, quantization, or their use in sequence modeling, which are core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VOXLINGUA107: A Dataset for Spoken Language Recognition",
    "abstract": "This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.",
    "metadata": {
      "title": "VOXLINGUA107: A Dataset for Spoken Language Recognition",
      "summary": "This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.",
      "abstract": "This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383459",
      "openalex_id": "https://openalex.org/W3106807794",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The VOXLINGUA107 paper describes a dataset for spoken language recognition using automatically collected web audio data, with no emphasis on discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization, or SSL-derived discrete tokens) or methods for token generation/quantization. Since the inclusion criteria require discrete audio token designs as the core contribution, and none of the exclusion criteria apply (no demo-only or non-methodology issue), the item does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The VOXLINGUA107 paper describes a dataset for spoken language recognition using automatically collected web audio data, with no emphasis on discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization, or SSL-derived discrete tokens) or methods for token generation/quantization. Since the inclusion criteria require discrete audio token designs as the core contribution, and none of the exclusion criteria apply (no demo-only or non-methodology issue), the item does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spoken language recognition using a large dataset collected from web audio, but it does not present any method involving discrete audio token generation, vector quantization, or related token design/quantization processes as a core contribution. The dataset and recognition models rely on speech activity detection and speaker diarization rather than discrete audio tokens, thus not meeting the inclusion criteria centered on discrete token design and usage for sequence models. Furthermore, method details about token formation or quantization are absent, aligning it more as a dataset and task paper rather than a discrete audio token design study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spoken language recognition using a large dataset collected from web audio, but it does not present any method involving discrete audio token generation, vector quantization, or related token design/quantization processes as a core contribution. The dataset and recognition models rely on speech activity detection and speaker diarization rather than discrete audio tokens, thus not meeting the inclusion criteria centered on discrete token design and usage for sequence models. Furthermore, method details about token formation or quantization are absent, aligning it more as a dataset and task paper rather than a discrete audio token design study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken\\n Utterances Extracted from the Bible",
    "abstract": "The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\\npublished multilingual speech dataset based on recorded readings of the New\\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\\nthat the source content (the Bible) is the same for all the languages is not\\nexploited to date.Therefore, this article proposes to add multilingual links\\nbetween speech segments in different languages, and shares a large and clean\\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\\nalignment as well as on translation for typologically different language pairs.\\nThe quality of the final corpus is attested by human evaluation performed on a\\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\\nusefulness of the final product on a bilingual speech retrieval task.\\n",
    "metadata": {
      "title": "MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken\\n Utterances Extracted from the Bible",
      "summary": "The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\\npublished multilingual speech dataset based on recorded readings of the New\\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\\nthat the source content (the Bible) is the same for all the languages is not\\nexploited to date.Therefore, this article proposes to add multilingual links\\nbetween speech segments in different languages, and shares a large and clean\\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\\nalignment as well as on translation for typologically different language pairs.\\nThe quality of the final corpus is attested by human evaluation performed on a\\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\\nusefulness of the final product on a bilingual speech retrieval task.\\n",
      "abstract": "The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\\npublished multilingual speech dataset based on recorded readings of the New\\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\\nthat the source content (the Bible) is the same for all the languages is not\\nexploited to date.Therefore, this article proposes to add multilingual links\\nbetween speech segments in different languages, and shares a large and clean\\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\\nalignment as well as on translation for typologically different language pairs.\\nThe quality of the final corpus is attested by human evaluation performed on a\\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\\nusefulness of the final product on a bilingual speech retrieval task.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1907.12895",
      "openalex_id": "https://openalex.org/W2966095117",
      "arxiv_id": "",
      "publication_date": "2019-07-30",
      "published": "2019-07-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "MaSS provides a parallel speech corpus across 8 languages with no discussion of discrete audio token representations, token generation/quantization methods, or token-based modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "MaSS provides a parallel speech corpus across 8 languages with no discussion of discrete audio token representations, token generation/quantization methods, or token-based modeling, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a multilingual corpus of sentence-aligned spoken utterances extracted from the Bible to support speech-to-speech alignment and bilingual speech retrieval tasks but does not describe the design or use of discrete audio tokens, vector quantization, or related tokenization mechanisms for audio language modeling or generation. The main contribution is dataset creation and alignment rather than discrete token generation or modeling, thus it does not meet the inclusion criteria focused on discrete audio token methods and lacks detailed methods on tokenization or quantization processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a multilingual corpus of sentence-aligned spoken utterances extracted from the Bible to support speech-to-speech alignment and bilingual speech retrieval tasks but does not describe the design or use of discrete audio tokens, vector quantization, or related tokenization mechanisms for audio language modeling or generation. The main contribution is dataset creation and alignment rather than discrete token generation or modeling, thus it does not meet the inclusion criteria focused on discrete audio token methods and lacks detailed methods on tokenization or quantization processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations",
    "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available.",
    "metadata": {
      "title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations",
      "summary": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available.",
      "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available.",
      "doi": "https://doi.org/10.48550/arxiv.2211.04508",
      "openalex_id": "https://openalex.org/W4308756394",
      "arxiv_id": "",
      "publication_date": "2022-11-08",
      "published": "2022-11-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a large-scale multilingual speech-to-speech translation corpus rather than discrete audio token representations or tokenization/quantization methods; it lacks required methodological details on token design and quantization, so it does not meet the Inclusion criteria and is not aligned with the Exclusion criteria either.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a large-scale multilingual speech-to-speech translation corpus rather than discrete audio token representations or tokenization/quantization methods; it lacks required methodological details on token design and quantization, so it does not meet the Inclusion criteria and is not aligned with the Exclusion criteria either.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a large-scale mined corpus of multilingual speech-to-speech translations but does not describe or focus on generating discrete audio tokens through vector quantization or similar methods, nor does it elaborate on a token generation process or discrete audio representation for language modeling or generation. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens as a core method or contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a large-scale mined corpus of multilingual speech-to-speech translations but does not describe or focus on generating discrete audio tokens through vector quantization or similar methods, nor does it elaborate on a token generation process or discrete audio representation for language modeling or generation. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens as a core method or contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages",
    "abstract": "A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",
    "metadata": {
      "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages",
      "summary": "A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",
      "abstract": "A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",
      "doi": "https://doi.org/10.48550/arxiv.2208.11761",
      "openalex_id": "https://openalex.org/W4293332626",
      "arxiv_id": "",
      "publication_date": "2022-08-24",
      "published": "2022-08-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes IndicSUPERB as a benchmark for Indian languages and self-supervised speech models; it does not discuss discrete audio tokens or tokenization methods, and thus does not meet the inclusion criteria centered on discrete token representations, nor does it appear to be a mere demo without methodology.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes IndicSUPERB as a benchmark for Indian languages and self-supervised speech models; it does not discuss discrete audio tokens or tokenization methods, and thus does not meet the inclusion criteria centered on discrete token representations, nor does it appear to be a mere demo without methodology.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes the creation of a benchmark dataset and evaluation of speech models for Indic languages, focusing on speech tasks such as ASR, speaker verification, and language identification. However, it does not mention any discretization of audio into discrete tokens via vector quantization or similar methods, nor does it describe any token generation process or usage of discrete tokens for language modeling or audio generation/understanding as defined in the inclusion criteria. There is no technical detail on token formation, quantization scheme, or discrete representation. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes the creation of a benchmark dataset and evaluation of speech models for Indic languages, focusing on speech tasks such as ASR, speaker verification, and language identification. However, it does not mention any discretization of audio into discrete tokens via vector quantization or similar methods, nor does it describe any token generation process or usage of discrete tokens for language modeling or audio generation/understanding as defined in the inclusion criteria. There is no technical detail on token formation, quantization scheme, or discrete representation. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?",
    "abstract": "Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.",
    "metadata": {
      "title": "Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?",
      "summary": "Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.",
      "abstract": "Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054567",
      "openalex_id": "https://openalex.org/W3015315843",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on automatic lyrics alignment and polyphonic ASR using genre-informed acoustic models, with no emphasis on discrete audio tokens, VQ/RVQ, or token-based sequence modeling as core contributions; thus it does not meet the tokenization/discrete-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on automatic lyrics alignment and polyphonic ASR using genre-informed acoustic models, with no emphasis on discrete audio tokens, VQ/RVQ, or token-based sequence modeling as core contributions; thus it does not meet the tokenization/discrete-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses automatic lyrics alignment and transcription in polyphonic music, focusing on acoustic modeling and genre-specific characteristics, but does not mention any discrete audio tokens, vector quantization, or token generation mechanisms for use in language or sequence modeling, which are key inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses automatic lyrics alignment and transcription in polyphonic music, focusing on acoustic modeling and genre-specific characteristics, but does not mention any discrete audio tokens, vector quantization, or token generation mechanisms for use in language or sequence modeling, which are key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
    "abstract": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.",
    "metadata": {
      "title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System",
      "summary": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.",
      "abstract": "This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1410",
      "openalex_id": "https://openalex.org/W3097514409",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目未以離散音訊 token 設計或量化/分層建模為核心貢獻，且僅描述歌唱語音合成系統的架構與實驗，未符合離散 token 主題與研究貢獻，因此不符合 inclusion。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目未以離散音訊 token 設計或量化/分層建模為核心貢獻，且僅描述歌唱語音合成系統的架構與實驗，未符合離散 token 主題與研究貢獻，因此不符合 inclusion。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a singing voice synthesis system focusing on spectrum, F0, and duration modeling without mentioning any discrete audio tokens, vector quantization, or similar discrete representation methods; therefore, it does not meet the inclusion criteria which require discrete audio token design and methodology for sequence or language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a singing voice synthesis system focusing on spectrum, F0, and duration modeling without mentioning any discrete audio tokens, vector quantization, or similar discrete representation methods; therefore, it does not meet the inclusion criteria which require discrete audio token design and methodology for sequence or language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ByteSing: A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders",
    "abstract": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.",
    "metadata": {
      "title": "ByteSing: A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders",
      "summary": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.",
      "abstract": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.",
      "doi": "https://doi.org/10.1109/iscslp49672.2021.9362104",
      "openalex_id": "https://openalex.org/W3133525064",
      "arxiv_id": "",
      "publication_date": "2021-01-24",
      "published": "2021-01-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a Chinese singing voice synthesis system using Tacotron-like models and WaveRNN, with no mention of discrete audio tokens, token formation, quantization, or token-based modeling, which are required to meet the inclusion criteria; thus it does not fit the discrete audio token focus and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a Chinese singing voice synthesis system using Tacotron-like models and WaveRNN, with no mention of discrete audio tokens, token formation, quantization, or token-based modeling, which are required to meet the inclusion criteria; thus it does not fit the discrete audio token focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a Chinese singing voice synthesis system focusing on encoder-decoder acoustic models and neural vocoders to improve voice quality, without mentioning discrete audio token design, generation process, or usage as tokens for sequence/language modeling, and lacks discussion of vector quantization or semantic disentanglement relevant to discrete audio token methods, thus not meeting the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a Chinese singing voice synthesis system focusing on encoder-decoder acoustic models and neural vocoders to improve voice quality, without mentioning discrete audio token design, generation process, or usage as tokens for sequence/language modeling, and lacks discussion of vector quantization or semantic disentanglement relevant to discrete audio token methods, thus not meeting the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
    "abstract": "Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).",
    "metadata": {
      "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
      "summary": "Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).",
      "abstract": "Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).",
      "doi": "https://doi.org/10.21437/interspeech.2016-872",
      "openalex_id": "https://openalex.org/W2516406502",
      "arxiv_id": "",
      "publication_date": "2016-08-29",
      "published": "2016-08-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes unit-selection based expressive singing synthesis and provides no discrete audio token design or reproducible methodological details required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes unit-selection based expressive singing synthesis and provides no discrete audio token design or reproducible methodological details required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The provided title relates to expressive singing synthesis based on unit selection, but the abstract is only a conference presentation notice without detailing any methods or technical content concerning discrete audio tokens, token generation, or quantization processes. It lacks the necessary methodological details and does not clarify the use of discrete token representations for sequence modeling, thus failing to meet key inclusion criteria and partially falling under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The provided title relates to expressive singing synthesis based on unit selection, but the abstract is only a conference presentation notice without detailing any methods or technical content concerning discrete audio tokens, token generation, or quantization processes. It lacks the necessary methodological details and does not clarify the use of discrete token representations for sequence modeling, thus failing to meet key inclusion criteria and partially falling under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An HMM-based singing voice synthesis system",
    "abstract": "Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different",
    "metadata": {
      "title": "An HMM-based singing voice synthesis system",
      "summary": "Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different",
      "abstract": "Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different",
      "doi": "https://doi.org/10.21437/interspeech.2006-584",
      "openalex_id": "https://openalex.org/W29794711",
      "arxiv_id": "",
      "publication_date": "2006-09-17",
      "published": "2006-09-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes an HMM-based corpus singing voice synthesis without any discrete audio tokenization, quantization, or token-level representations, thus it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes an HMM-based corpus singing voice synthesis without any discrete audio tokenization, quantization, or token-level representations, thus it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes an HMM-based singing voice synthesis system focusing on modeling musical information and producing natural-sounding singing voice, but it does not mention any form of discrete audio tokenization, vector quantization, or similar discrete representation methods used for sequence modeling or language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes an HMM-based singing voice synthesis system focusing on modeling musical information and producing natural-sounding singing voice, but it does not mention any form of discrete audio tokenization, vector quantization, or similar discrete representation methods used for sequence modeling or language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy",
    "abstract": "This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.",
    "metadata": {
      "title": "Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy",
      "summary": "This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.",
      "abstract": "This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.",
      "doi": "https://doi.org/10.23919/apsipa.2018.8659797",
      "openalex_id": "https://openalex.org/W2921576841",
      "arxiv_id": "",
      "publication_date": "2018-11-01",
      "published": "2018-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes a DNN-based singing voice synthesis system (Sinsy) and techniques for synthesis quality, but it does not present discrete audio token representations or tokenization methods (e.g., VQ/RVQ, hierarchical quantization, semantic/discrete tokens) required by the inclusion criteria; therefore it does not meet the topic definition and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes a DNN-based singing voice synthesis system (Sinsy) and techniques for synthesis quality, but it does not present discrete audio token representations or tokenization methods (e.g., VQ/RVQ, hierarchical quantization, semantic/discrete tokens) required by the inclusion criteria; therefore it does not meet the topic definition and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a DNN-based singing voice synthesis system but does not mention the use or design of discrete audio tokens derived via vector quantization or similar methods for audio language modeling or sequence modeling. It lacks discussion on token generation processes, quantization schemes, or the use of discrete tokens as intermediate representations, which are essential inclusion criteria. It focuses on synthesis techniques and performance comparison with HMM-based systems, not on discrete token representation or use in downstream tasks as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a DNN-based singing voice synthesis system but does not mention the use or design of discrete audio tokens derived via vector quantization or similar methods for audio language modeling or sequence modeling. It lacks discussion on token generation processes, quantization schemes, or the use of discrete tokens as intermediate representations, which are essential inclusion criteria. It focuses on synthesis techniques and performance comparison with HMM-based systems, not on discrete token representation or use in downstream tasks as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast and High-Quality Singing Voice Synthesis System Based on Convolutional Neural Networks",
    "abstract": "The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.",
    "metadata": {
      "title": "Fast and High-Quality Singing Voice Synthesis System Based on Convolutional Neural Networks",
      "summary": "The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.",
      "abstract": "The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053811",
      "openalex_id": "https://openalex.org/W3015437531",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses CNN-based singing voice synthesis without any discrete audio tokenization or token-generation methods, so it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses CNN-based singing voice synthesis without any discrete audio tokenization or token-generation methods, so it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on singing voice synthesis using convolutional neural networks to improve naturalness and computational efficiency. However, it does not mention the use or generation of discrete audio tokens, vector quantization, or any tokenization mechanism essential for processing audio as discrete elements for language or sequence models, which is the core inclusion criterion. Hence, it does not meet the inclusion requirements related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on singing voice synthesis using convolutional neural networks to improve naturalness and computational efficiency. However, it does not mention the use or generation of discrete audio tokens, vector quantization, or any tokenization mechanism essential for processing audio as discrete elements for language or sequence models, which is the core inclusion criterion. Hence, it does not meet the inclusion requirements related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Singing Voice Synthesis: History, Current Work, and Future Directions",
    "abstract": "This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.",
    "metadata": {
      "title": "Singing Voice Synthesis: History, Current Work, and Future Directions",
      "summary": "This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.",
      "abstract": "This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.",
      "doi": "https://doi.org/10.2307/3680822",
      "openalex_id": "https://openalex.org/W2030149476",
      "arxiv_id": "",
      "publication_date": "1996-01-01",
      "published": "1996-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract discusses singing voice synthesis history and future directions but does not address discrete audio tokens or tokenization methods (no token-generation, quantization, or language-modeling aspects), and the source date is not provided.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract discusses singing voice synthesis history and future directions but does not address discrete audio tokens or tokenization methods (no token-generation, quantization, or language-modeling aspects), and the source date is not provided.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article reviews the history and current research directions of singing voice synthesis but does not detail methods related to discrete audio tokens, such as token generation processes, vector quantization techniques, or their use in sequence/language models, thus it fails to meet the inclusion criteria focused on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article reviews the history and current research directions of singing voice synthesis but does not detail methods related to discrete audio tokens, such as token generation processes, vector quantization techniques, or their use in sequence/language models, thus it fails to meet the inclusion criteria focused on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VOCALOID - Commercial singing synthesizer based on sample concatenation",
    "abstract": "The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.",
    "metadata": {
      "title": "VOCALOID - Commercial singing synthesizer based on sample concatenation",
      "summary": "The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.",
      "abstract": "The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2124097505",
      "arxiv_id": "",
      "publication_date": "2007-01-01",
      "published": "2007-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item presents a Vocaloid product overview and synthesis technique, with no discrete audio token design or reproducible methodology, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item presents a Vocaloid product overview and synthesis technique, with no discrete audio token design or reproducible methodology, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a commercial singing synthesizer Vocaloid based on sample concatenation, without mentioning any discrete audio token generation, vector quantization, or related tokenization methods used for language or sequence modeling. There are no methodological details on token formation, quantization, or usage of tokens for downstream audio language modeling tasks, nor does it indicate it fits the inclusion criteria regarding discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a commercial singing synthesizer Vocaloid based on sample concatenation, without mentioning any discrete audio token generation, vector quantization, or related tokenization methods used for language or sequence modeling. There are no methodological details on token formation, quantization, or usage of tokens for downstream audio language modeling tasks, nor does it indicate it fits the inclusion criteria regarding discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Filterbank learning using Convolutional Restricted Boltzmann Machine for speech recognition",
    "abstract": "Convolutional Restricted Boltzmann Machine (ConvRBM) as a model for speech signal is presented in this paper. We have developed ConvRBM with sampling from noisy rectified linear units (NReLUs). ConvRBM is trained in an unsupervised way to model speech signal of arbitrary lengths. Weights of the model can represent an auditory-like filterbank. Our proposed learned filterbank is also nonlinear with respect to center frequencies of subband filters similar to standard filterbanks (such as Mel, Bark, ERB, etc.). We have used our proposed model as a front-end to learn features and applied to speech recognition task. Performance of ConvRBM features is improved compared to MFCC with relative improvement of 5% on TIMIT test set and 7% on WSJ0 database for both Nov'92 test sets using GMM-HMM systems. With DNN-HMM systems, we achieved relative improvement of 3% on TIMIT test set over MFCC and Mel filterbank (FBANK). On WSJ0 Nov'92 test sets, we achieved relative improvement of 4-14% using ConvRBM features over MFCC features and 3.6-5.6% using ConvRBM filterbank over FBANK features.",
    "metadata": {
      "title": "Filterbank learning using Convolutional Restricted Boltzmann Machine for speech recognition",
      "summary": "Convolutional Restricted Boltzmann Machine (ConvRBM) as a model for speech signal is presented in this paper. We have developed ConvRBM with sampling from noisy rectified linear units (NReLUs). ConvRBM is trained in an unsupervised way to model speech signal of arbitrary lengths. Weights of the model can represent an auditory-like filterbank. Our proposed learned filterbank is also nonlinear with respect to center frequencies of subband filters similar to standard filterbanks (such as Mel, Bark, ERB, etc.). We have used our proposed model as a front-end to learn features and applied to speech recognition task. Performance of ConvRBM features is improved compared to MFCC with relative improvement of 5% on TIMIT test set and 7% on WSJ0 database for both Nov'92 test sets using GMM-HMM systems. With DNN-HMM systems, we achieved relative improvement of 3% on TIMIT test set over MFCC and Mel filterbank (FBANK). On WSJ0 Nov'92 test sets, we achieved relative improvement of 4-14% using ConvRBM features over MFCC features and 3.6-5.6% using ConvRBM filterbank over FBANK features.",
      "abstract": "Convolutional Restricted Boltzmann Machine (ConvRBM) as a model for speech signal is presented in this paper. We have developed ConvRBM with sampling from noisy rectified linear units (NReLUs). ConvRBM is trained in an unsupervised way to model speech signal of arbitrary lengths. Weights of the model can represent an auditory-like filterbank. Our proposed learned filterbank is also nonlinear with respect to center frequencies of subband filters similar to standard filterbanks (such as Mel, Bark, ERB, etc.). We have used our proposed model as a front-end to learn features and applied to speech recognition task. Performance of ConvRBM features is improved compared to MFCC with relative improvement of 5% on TIMIT test set and 7% on WSJ0 database for both Nov'92 test sets using GMM-HMM systems. With DNN-HMM systems, we achieved relative improvement of 3% on TIMIT test set over MFCC and Mel filterbank (FBANK). On WSJ0 Nov'92 test sets, we achieved relative improvement of 4-14% using ConvRBM features over MFCC features and 3.6-5.6% using ConvRBM filterbank over FBANK features.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472808",
      "openalex_id": "https://openalex.org/W2394873997",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets learning a filterbank with ConvRBM for speech recognition, not providing discrete audio tokens or tokenization mechanisms, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets learning a filterbank with ConvRBM for speech recognition, not providing discrete audio tokens or tokenization mechanisms, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a convolutional restricted Boltzmann machine (ConvRBM) model that learns auditory-like filterbanks for speech recognition improvement. However, it focuses on continuous feature learning and filterbank design rather than generating discrete audio tokens through quantization or discretization mechanisms. There is no indication of producing finite discrete symbols or tokens suitable for language or sequence models, nor discussion on token generation processes or their application for audio language modeling or sequence modeling. Therefore, it does not meet the key inclusion criteria related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a convolutional restricted Boltzmann machine (ConvRBM) model that learns auditory-like filterbanks for speech recognition improvement. However, it focuses on continuous feature learning and filterbank design rather than generating discrete audio tokens through quantization or discretization mechanisms. There is no indication of producing finite discrete symbols or tokens suitable for language or sequence models, nor discussion on token generation processes or their application for audio language modeling or sequence modeling. Therefore, it does not meet the key inclusion criteria related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised modulation filter learning for noise-robust speech recognition",
    "abstract": "The modulation filtering approach to robust automatic speech recognition (ASR) is based on enhancing perceptually relevant regions of the modulation spectrum while suppressing the regions susceptible to noise. In this paper, a data-driven unsupervised modulation filter learning scheme is proposed using convolutional restricted Boltzmann machine. The initial filter is learned using the speech spectrogram while subsequent filters are learned using residual spectrograms. The modulation filtered spectrograms are used for ASR experiments on noisy and reverberant speech where these features provide significant improvements over other robust features. Furthermore, the application of the proposed method for semi-supervised learning is investigated.",
    "metadata": {
      "title": "Unsupervised modulation filter learning for noise-robust speech recognition",
      "summary": "The modulation filtering approach to robust automatic speech recognition (ASR) is based on enhancing perceptually relevant regions of the modulation spectrum while suppressing the regions susceptible to noise. In this paper, a data-driven unsupervised modulation filter learning scheme is proposed using convolutional restricted Boltzmann machine. The initial filter is learned using the speech spectrogram while subsequent filters are learned using residual spectrograms. The modulation filtered spectrograms are used for ASR experiments on noisy and reverberant speech where these features provide significant improvements over other robust features. Furthermore, the application of the proposed method for semi-supervised learning is investigated.",
      "abstract": "The modulation filtering approach to robust automatic speech recognition (ASR) is based on enhancing perceptually relevant regions of the modulation spectrum while suppressing the regions susceptible to noise. In this paper, a data-driven unsupervised modulation filter learning scheme is proposed using convolutional restricted Boltzmann machine. The initial filter is learned using the speech spectrogram while subsequent filters are learned using residual spectrograms. The modulation filtered spectrograms are used for ASR experiments on noisy and reverberant speech where these features provide significant improvements over other robust features. Furthermore, the application of the proposed method for semi-supervised learning is investigated.",
      "doi": "https://doi.org/10.1121/1.5001926",
      "openalex_id": "https://openalex.org/W2756577849",
      "arxiv_id": "",
      "publication_date": "2017-09-01",
      "published": "2017-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe unsupervised modulation filter learning for ASR rather than discrete audio tokenization or token-based representations; since the inclusion criteria require discrete audio tokens as the core contribution, this item does not meet the inclusion criteria (and it also does not present a tokenization-based method to be evaluated).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe unsupervised modulation filter learning for ASR rather than discrete audio tokenization or token-based representations; since the inclusion criteria require discrete audio tokens as the core contribution, this item does not meet the inclusion criteria (and it also does not present a tokenization-based method to be evaluated).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised modulation filter learning for noise-robust speech recognition, primarily dealing with feature enhancement in the modulation spectrum and robustness against noise in ASR systems; it does not describe producing discrete audio tokens via vector quantization or other discrete tokenization methods, nor does it mention transforming continuous audio into discrete tokens for sequence or language model processing. Hence, it does not meet the inclusion criteria regarding discrete audio token generation or design, and lacks key methodological details related to tokenization or downstream token use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised modulation filter learning for noise-robust speech recognition, primarily dealing with feature enhancement in the modulation spectrum and robustness against noise in ASR systems; it does not describe producing discrete audio tokens via vector quantization or other discrete tokenization methods, nor does it mention transforming continuous audio into discrete tokens for sequence or language model processing. Hence, it does not meet the inclusion criteria regarding discrete audio token generation or design, and lacks key methodological details related to tokenization or downstream token use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Finding a \"Kneedle\" in a Haystack: Detecting Knee Points in System Behavior",
    "abstract": "Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These \"knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.",
    "metadata": {
      "title": "Finding a \"Kneedle\" in a Haystack: Detecting Knee Points in System Behavior",
      "summary": "Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These \"knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.",
      "abstract": "Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These \"knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.",
      "doi": "https://doi.org/10.1109/icdcsw.2011.20",
      "openalex_id": "https://openalex.org/W2097749765",
      "arxiv_id": "",
      "publication_date": "2011-06-01",
      "published": "2011-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses knee detection in system behavior (Kneedle) and does not address discrete audio tokens or their generation/quantization for language-model-style processing; it fails to meet inclusion criteria and does not trigger any exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses knee detection in system behavior (Kneedle) and does not address discrete audio tokens or their generation/quantization for language-model-style processing; it fails to meet inclusion criteria and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on detecting knee points in system behavior, specifically relating to performance trade-offs in computer systems, without any mention of discrete audio tokens, audio signal processing, token generation, or related methodologies as defined in the inclusion criteria. There is no indication that the study involves neural audio codecs, vector quantization, semantic disentanglement, or any form of discrete audio token design or application for sequence/language modeling. Therefore, the study does not meet any of the inclusion criteria and falls outside the specified research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on detecting knee points in system behavior, specifically relating to performance trade-offs in computer systems, without any mention of discrete audio tokens, audio signal processing, token generation, or related methodologies as defined in the inclusion criteria. There is no indication that the study involves neural audio codecs, vector quantization, semantic disentanglement, or any form of discrete audio token design or application for sequence/language modeling. Therefore, the study does not meet any of the inclusion criteria and falls outside the specified research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Finding the Number of Clusters in a Dataset",
    "abstract": "One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.",
    "metadata": {
      "title": "Finding the Number of Clusters in a Dataset",
      "summary": "One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.",
      "abstract": "One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.",
      "doi": "https://doi.org/10.1198/016214503000000666",
      "openalex_id": "https://openalex.org/W1973041621",
      "arxiv_id": "",
      "publication_date": "2003-09-01",
      "published": "2003-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a nonparametric clustering method based on distortion and rate-distortion theory, focusing on determining the number of clusters in datasets, with no discussion of discrete audio token representations or token design, thus failing the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a nonparametric clustering method based on distortion and rate-distortion theory, focusing on determining the number of clusters in datasets, with no discussion of discrete audio token representations or token design, thus failing the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on methods to determine the number of clusters in datasets via nonparametric techniques, without any mention of discrete audio tokens, vector quantization, or audio-related tokenization processes required by the inclusion criteria. It does not describe token generation from continuous audio signals nor does it address audio language modeling or semantic/acoustic disentanglement. Therefore, it fails to meet the core subject requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on methods to determine the number of clusters in datasets via nonparametric techniques, without any mention of discrete audio tokens, vector quantization, or audio-related tokenization processes required by the inclusion criteria. It does not describe token generation from continuous audio signals nor does it address audio language modeling or semantic/acoustic disentanglement. Therefore, it fails to meet the core subject requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bootstrap estimates for confidence intervals in ASR performance evaluation",
    "abstract": "The field of speech recognition has clearly benefited from precisely defined testing conditions and objective performance measures such as word error rate. In the development and evaluation of new methods, the question arises whether the empirically observed difference in performance is due to a genuine advantage of one system over the other, or just an effect of chance. However, many publications still do not concern themselves with the statistical significance of the results reported. We present a bootstrap method for significance analysis which is, at the same time, intuitive, precise and and easy to use. Unlike some methods, we make no (possibly ill-founded) approximations and the results are immediately interpretable in terms of word error rate.",
    "metadata": {
      "title": "Bootstrap estimates for confidence intervals in ASR performance evaluation",
      "summary": "The field of speech recognition has clearly benefited from precisely defined testing conditions and objective performance measures such as word error rate. In the development and evaluation of new methods, the question arises whether the empirically observed difference in performance is due to a genuine advantage of one system over the other, or just an effect of chance. However, many publications still do not concern themselves with the statistical significance of the results reported. We present a bootstrap method for significance analysis which is, at the same time, intuitive, precise and and easy to use. Unlike some methods, we make no (possibly ill-founded) approximations and the results are immediately interpretable in terms of word error rate.",
      "abstract": "The field of speech recognition has clearly benefited from precisely defined testing conditions and objective performance measures such as word error rate. In the development and evaluation of new methods, the question arises whether the empirically observed difference in performance is due to a genuine advantage of one system over the other, or just an effect of chance. However, many publications still do not concern themselves with the statistical significance of the results reported. We present a bootstrap method for significance analysis which is, at the same time, intuitive, precise and and easy to use. Unlike some methods, we make no (possibly ill-founded) approximations and the results are immediately interpretable in terms of word error rate.",
      "doi": "https://doi.org/10.1109/icassp.2004.1326009",
      "openalex_id": "https://openalex.org/W2107223151",
      "arxiv_id": "",
      "publication_date": "2004-09-28",
      "published": "2004-09-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns bootstrap-based significance analysis for ASR WER; it does not address discrete audio tokens nor token generation/quantization; thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns bootstrap-based significance analysis for ASR WER; it does not address discrete audio tokens nor token generation/quantization; thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on bootstrap methods for confidence interval estimation in ASR performance evaluation and does not discuss discrete audio token generation, their quantization, or their use in language or sequence models, which are required inclusion criteria. Therefore, it does not meet the main subject and technical requirements, and no method details for discrete token generation or application are provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on bootstrap methods for confidence interval estimation in ASR performance evaluation and does not discuss discrete audio token generation, their quantization, or their use in language or sequence models, which are required inclusion criteria. Therefore, it does not meet the main subject and technical requirements, and no method details for discrete token generation or application are provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Conditional Image Generation with PixelCNN Decoders",
    "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
    "metadata": {
      "title": "Conditional Image Generation with PixelCNN Decoders",
      "summary": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
      "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
      "doi": "https://doi.org/10.48550/arxiv.1606.05328",
      "openalex_id": "https://openalex.org/W2963636093",
      "arxiv_id": "",
      "publication_date": "2016-06-16",
      "published": "2016-06-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on conditional image generation with PixelCNN decoders, not on discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on conditional image generation with PixelCNN decoders, not on discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on conditional image generation with PixelCNN decoders, which is entirely within the image domain and does not concern discrete audio tokens, audio vector quantization, or related audio representation and generation techniques as specified in the inclusion criteria. It lacks any discussion or methodological detail related to discretizing audio signals or producing discrete audio tokens for sequence or language models. Therefore, it does not meet the inclusion criteria and falls outside the scope of the targeted audio-related research focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on conditional image generation with PixelCNN decoders, which is entirely within the image domain and does not concern discrete audio tokens, audio vector quantization, or related audio representation and generation techniques as specified in the inclusion criteria. It lacks any discussion or methodological detail related to discretizing audio signals or producing discrete audio tokens for sequence or language models. Therefore, it does not meet the inclusion criteria and falls outside the scope of the targeted audio-related research focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Variational Inference with Normalizing Flows",
    "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
    "metadata": {
      "title": "Variational Inference with Normalizing Flows",
      "summary": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
      "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
      "doi": "https://doi.org/10.48550/arxiv.1505.05770",
      "openalex_id": "https://openalex.org/W2963090522",
      "arxiv_id": "",
      "publication_date": "2015-05-21",
      "published": "2015-05-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses variational inference with normalizing flows and does not address discrete audio tokens or tokenization for audio, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses variational inference with normalizing flows and does not address discrete audio tokens or tokenization for audio, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a new method in variational inference focusing on improving posterior approximations via normalizing flows, which is a probabilistic modeling technique. However, the content neither mentions discrete audio tokens nor any audio-related tokenization, vector quantization, or audio language modeling. It does not align with the inclusion criteria around discrete audio tokens or token design for audio signal representation; hence, it falls outside the scope defined. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a new method in variational inference focusing on improving posterior approximations via normalizing flows, which is a probabilistic modeling technique. However, the content neither mentions discrete audio tokens nor any audio-related tokenization, vector quantization, or audio language modeling. It does not align with the inclusion criteria around discrete audio tokens or token design for audio signal representation; hence, it falls outside the scope defined. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synthesis of the Singing Voice by Performance Sampling and Spectral Models",
    "abstract": "This paper introduces the concept of synthesis based on performance sampling. It explains that although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument. The paper presents a singing voice synthesizer, pointing out the main issues and complexities emerging along its design. Although the current system is able to generate convincing results in certain situations, there is still much room for improvements, especially in the areas of expression, spectral modeling and sonic space design. However, computer singing is definitely coming close to becoming indistinguishable from human performances.",
    "metadata": {
      "title": "Synthesis of the Singing Voice by Performance Sampling and Spectral Models",
      "summary": "This paper introduces the concept of synthesis based on performance sampling. It explains that although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument. The paper presents a singing voice synthesizer, pointing out the main issues and complexities emerging along its design. Although the current system is able to generate convincing results in certain situations, there is still much room for improvements, especially in the areas of expression, spectral modeling and sonic space design. However, computer singing is definitely coming close to becoming indistinguishable from human performances.",
      "abstract": "This paper introduces the concept of synthesis based on performance sampling. It explains that although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument. The paper presents a singing voice synthesizer, pointing out the main issues and complexities emerging along its design. Although the current system is able to generate convincing results in certain situations, there is still much room for improvements, especially in the areas of expression, spectral modeling and sonic space design. However, computer singing is definitely coming close to becoming indistinguishable from human performances.",
      "doi": "https://doi.org/10.1109/msp.2007.323266",
      "openalex_id": "https://openalex.org/W2102870814",
      "arxiv_id": "",
      "publication_date": "2007-03-01",
      "published": "2007-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on singing voice synthesis via performance sampling and spectral models and does not present or analyze discrete audio tokens (tokenization/quantization) or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on singing voice synthesis via performance sampling and spectral models and does not present or analyze discrete audio tokens (tokenization/quantization) or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on synthesizing the singing voice using performance sampling and spectral models without mentioning any discrete audio token generation, vector quantization, or token-based sequence modeling frameworks, which are essential according to the inclusion criteria. It also lacks details on token generation mechanisms or quantization processes and does not discuss usage of discrete tokens for audio language modeling or understanding, thus not satisfying the core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on synthesizing the singing voice using performance sampling and spectral models without mentioning any discrete audio token generation, vector quantization, or token-based sequence modeling frameworks, which are essential according to the inclusion criteria. It also lacks details on token generation mechanisms or quantization processes and does not discuss usage of discrete tokens for audio language modeling or understanding, thus not satisfying the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Aggregated Residual Transformations for Deep Neural Networks",
    "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
    "metadata": {
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "summary": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "doi": "https://doi.org/10.1109/cvpr.2017.634",
      "openalex_id": "https://openalex.org/W2549139847",
      "arxiv_id": "",
      "publication_date": "2017-07-01",
      "published": "2017-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes ResNeXt for image classification and does not involve discrete audio tokens or tokenization methods, so it fails the inclusion criteria focused on discrete audio token research, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes ResNeXt for image classification and does not involve discrete audio tokens or tokenization methods, so it fails the inclusion criteria focused on discrete audio token research, and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on deep neural network architectures for image classification and does not address discrete audio tokens, audio signal processing, or any tokenization related to audio data. It lacks any discussion of quantization or token generation for audio or language modeling related to audio, thus failing to meet the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on deep neural network architectures for image classification and does not address discrete audio tokens, audio signal processing, or any tokenization related to audio data. It lacks any discussion of quantization or token generation for audio or language modeling related to audio, thus failing to meet the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
    "metadata": {
      "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
      "summary": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
      "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
      "doi": "https://doi.org/10.48550/arxiv.1809.11096",
      "openalex_id": "https://openalex.org/W2893749619",
      "arxiv_id": "",
      "publication_date": "2018-09-28",
      "published": "2018-09-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns large-scale GANs for image synthesis (BigGAN) on ImageNet, with no discrete audio token design or audio-language modeling content, so it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns large-scale GANs for image synthesis (BigGAN) on ImageNet, with no discrete audio token design or audio-language modeling content, so it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on large scale GAN training for natural image synthesis and does not address discrete audio token design, token generation processes, or usage for audio language modeling or related tasks; it is unrelated to the defined inclusion criteria focused on audio tokenization and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on large scale GAN training for natural image synthesis and does not address discrete audio token design, token generation processes, or usage for audio language modeling or related tasks; it is unrelated to the defined inclusion criteria focused on audio tokenization and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unit selection in a concatenative speech synthesis system using a large speech database",
    "abstract": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",
    "metadata": {
      "title": "Unit selection in a concatenative speech synthesis system using a large speech database",
      "summary": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",
      "abstract": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",
      "doi": "https://doi.org/10.1109/icassp.1996.541110",
      "openalex_id": "https://openalex.org/W2150658333",
      "arxiv_id": "",
      "publication_date": "2002-12-24",
      "published": "2002-12-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on unit selection in concatenative speech synthesis and a transition-cost framework for concatenating phoneme-like units, with no emphasis on discrete audio tokens (e.g., VQ/RVQ, hierarchical tokenization) or token-based representations for sequence models; therefore it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on unit selection in concatenative speech synthesis and a transition-cost framework for concatenating phoneme-like units, with no emphasis on discrete audio tokens (e.g., VQ/RVQ, hierarchical tokenization) or token-based representations for sequence models; therefore it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unit selection in concatenative speech synthesis, which involves selecting phoneme units from a large speech database to generate speech waveforms. However, it does not involve the creation or utilization of discrete audio tokens generated via vector quantization or similar techniques for use in language or sequence models, which is the core requirement of the inclusion criteria. It also lacks mention of token generation processes, quantization methods, or use of tokens in downstream modeling tasks. Thus, it does not meet the inclusion criteria since it is not about discrete audio token design or usage in language modeling, and does not show related methodology, making it unsuitable for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unit selection in concatenative speech synthesis, which involves selecting phoneme units from a large speech database to generate speech waveforms. However, it does not involve the creation or utilization of discrete audio tokens generated via vector quantization or similar techniques for use in language or sequence models, which is the core requirement of the inclusion criteria. It also lacks mention of token generation processes, quantization methods, or use of tokens in downstream modeling tasks. Thus, it does not meet the inclusion criteria since it is not about discrete audio token design or usage in language modeling, and does not show related methodology, making it unsuitable for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Constraint programming systems for modeling music theories and composition",
    "abstract": "Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.",
    "metadata": {
      "title": "Constraint programming systems for modeling music theories and composition",
      "summary": "Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.",
      "abstract": "Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.",
      "doi": "https://doi.org/10.1145/1978802.1978809",
      "openalex_id": "https://openalex.org/W2116973068",
      "arxiv_id": "",
      "publication_date": "2011-10-01",
      "published": "2011-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses constraint programming approaches to modeling music theories and composition, which does not address discrete audio token representations or their generation/quantization for language-model-style processing; it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses constraint programming approaches to modeling music theories and composition, which does not address discrete audio token representations or their generation/quantization for language-model-style processing; it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on constraint programming systems for music theory modeling and composition but does not address discrete audio token generation or design processes involving quantization or tokenization for language or sequence models; hence, it does not meet the inclusion criteria related to discrete audio token representations nor the token generation methods explicitly required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on constraint programming systems for music theory modeling and composition but does not address discrete audio token generation or design processes involving quantization or tokenization for language or sequence models; hence, it does not meet the inclusion criteria related to discrete audio token representations nor the token generation methods explicitly required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
    "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
    "metadata": {
      "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "summary": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "doi": "https://doi.org/10.48550/arxiv.1912.06680",
      "openalex_id": "https://openalex.org/W2996037775",
      "arxiv_id": "",
      "publication_date": "2019-12-13",
      "published": "2019-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe reinforcement learning for Dota 2 (OpenAI Five); there is no discrete audio token concept, no methods for creating/quantizing audio tokens or audio-language modeling; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe reinforcement learning for Dota 2 (OpenAI Five); there is no discrete audio token concept, no methods for creating/quantizing audio tokens or audio-language modeling; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes the use of reinforcement learning to train an AI system to play Dota 2, an esports game, focusing on gameplay and learning from large-scale data. It does not involve discrete audio tokens or any audio signal processing related to vector quantization or tokenization for use with language or sequence models, nor does it provide methods or experiments related to discrete audio representations as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes the use of reinforcement learning to train an AI system to play Dota 2, an esports game, focusing on gameplay and learning from large-scale data. It does not involve discrete audio tokens or any audio signal processing related to vector quantization or tokenization for use with language or sequence models, nor does it provide methods or experiments related to discrete audio representations as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Axial Attention in Multidimensional Transformers",
    "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
    "metadata": {
      "title": "Axial Attention in Multidimensional Transformers",
      "summary": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
      "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
      "doi": "https://doi.org/10.48550/arxiv.1912.12180",
      "openalex_id": "https://openalex.org/W2998108143",
      "arxiv_id": "",
      "publication_date": "2019-12-20",
      "published": "2019-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not relevant to discrete audio tokens—the abstract discusses axial attention for images and high-dimensional data with no discrete audio tokenization or audio-language modeling components.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not relevant to discrete audio tokens—the abstract discusses axial attention for images and high-dimensional data with no discrete audio tokenization or audio-language modeling components.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on 'Axial Attention in Multidimensional Transformers' applied to images and other high dimensional tensors, with emphasis on model efficiency and state-of-the-art performance on image and video benchmarks. It does not address the creation, quantization, or use of discrete audio tokens, nor does it discuss audio representations, semantic disentanglement, or audio language modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related technologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on 'Axial Attention in Multidimensional Transformers' applied to images and other high dimensional tensors, with emphasis on model efficiency and state-of-the-art performance on image and video benchmarks. It does not address the creation, quantization, or use of discrete audio tokens, nor does it discuss audio representations, semantic disentanglement, or audio language modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related technologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Music and computer composition",
    "abstract": "The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.",
    "metadata": {
      "title": "Music and computer composition",
      "summary": "The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.",
      "abstract": "The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.",
      "doi": "https://doi.org/10.1145/361254.361265",
      "openalex_id": "https://openalex.org/W1997640156",
      "arxiv_id": "",
      "publication_date": "1972-02-01",
      "published": "1972-02-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses early computer-assisted music composition and does not address discrete audio tokens or tokenization methods essential to the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses early computer-assisted music composition and does not address discrete audio tokens or tokenization methods essential to the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss computer composition of Western popular music and do not mention the generation or use of discrete audio tokens, vector quantization, or any related coding or tokenization methods that are central to the inclusion criteria. There is no description of token generation process, quantization techniques, or use of discrete tokens for sequence modeling or audio language modeling, so it does not meet the inclusion criteria, and it falls under exclusion criteria for lacking method details relevant to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss computer composition of Western popular music and do not mention the generation or use of discrete audio tokens, vector quantization, or any related coding or tokenization methods that are central to the inclusion criteria. There is no description of token generation process, quantization techniques, or use of discrete tokens for sequence modeling or audio language modeling, so it does not meet the inclusion criteria, and it falls under exclusion criteria for lacking method details relevant to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Lyrics Transcription in Polyphonic Music: Does Background Music Help?",
    "abstract": "Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",
    "metadata": {
      "title": "Automatic Lyrics Transcription in Polyphonic Music: Does Background Music Help?",
      "summary": "Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",
      "abstract": "Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2973975824",
      "arxiv_id": "",
      "publication_date": "2019-09-23",
      "published": "2019-09-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究主題為背景音樂干擾下的自動歌詞轉錄，聚焦於與語音識別流程和語種/流派特徵的比較，未涉及離散音訊 token 的設計或量化流程，因此不符合主題中的離散 token 要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究主題為背景音樂干擾下的自動歌詞轉錄，聚焦於與語音識別流程和語種/流派特徵的比較，未涉及離散音訊 token 的設計或量化流程，因此不符合主題中的離散 token 要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic lyrics transcription in polyphonic music using music genre-specific acoustic models, without mentioning the use or design of discrete audio tokens, vector quantization, or neural audio codecs. It lacks clear description of token generation processes or their role in sequence/language modeling, so it does not meet the inclusion criteria centered on discrete audio tokens as core methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic lyrics transcription in polyphonic music using music genre-specific acoustic models, without mentioning the use or design of discrete audio tokens, vector quantization, or neural audio codecs. It lacks clear description of token generation processes or their role in sequence/language modeling, so it does not meet the inclusion criteria centered on discrete audio tokens as core methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer",
    "abstract": "We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.",
    "metadata": {
      "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer",
      "summary": "We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.",
      "abstract": "We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.",
      "doi": "https://doi.org/10.3929/ethz-b-000292318",
      "openalex_id": "https://openalex.org/W2892104732",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets MIDI-based symbolic music modeling and style transfer using a VAE, not discrete audio token design or token-based audio language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets MIDI-based symbolic music modeling and style transfer using a VAE, not discrete audio token design or token-based audio language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces MIDI-VAE, a neural network model for modeling polyphonic music and style transfer between music genres; however, it does not address the generation or design of discrete audio tokens via vector quantization or related quantization methods as defined in the inclusion criteria. The paper primarily focuses on symbolic music modeling and style transfer without clearly describing producing discrete token representations for audio language modeling or downstream tasks. Therefore, it fails to meet the main inclusion criteria regarding discrete token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces MIDI-VAE, a neural network model for modeling polyphonic music and style transfer between music genres; however, it does not address the generation or design of discrete audio tokens via vector quantization or related quantization methods as defined in the inclusion criteria. The paper primarily focuses on symbolic music modeling and style transfer without clearly describing producing discrete token representations for audio language modeling or downstream tasks. Therefore, it fails to meet the main inclusion criteria regarding discrete token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Software for a cascade/parallel formant synthesizer",
    "abstract": "A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",
    "metadata": {
      "title": "Software for a cascade/parallel formant synthesizer",
      "summary": "A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",
      "abstract": "A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",
      "doi": "https://doi.org/10.1121/1.383940",
      "openalex_id": "https://openalex.org/W1999885698",
      "arxiv_id": "",
      "publication_date": "1980-03-01",
      "published": "1980-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper covers traditional digital formant synthesis using cascade/parallel resonators and provides code; it does not discuss discrete audio tokens, tokenization methods, or token-based representations for language models, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper covers traditional digital formant synthesis using cascade/parallel resonators and provides code; it does not discuss discrete audio tokens, tokenization methods, or token-based representations for language models, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a software formant synthesizer capable of speech synthesis via digital resonators but does not mention any discrete audio tokenization process, vector quantization, or generation of discrete tokens for sequence or language modeling; thus, it lacks the core characteristics required, such as producing or utilizing discrete audio tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a software formant synthesizer capable of speech synthesis via digital resonators but does not mention any discrete audio tokenization process, vector quantization, or generation of discrete tokens for sequence or language modeling; thus, it lacks the core characteristics required, such as producing or utilizing discrete audio tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Variational Inference with Inverse Autoregressive Flow",
    "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",
    "metadata": {
      "title": "Improved Variational Inference with Inverse Autoregressive Flow",
      "summary": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",
      "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2587284713",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers improved variational inference using inverse autoregressive flow (IAF) and VAEs, with no discussion of discrete audio tokenization or related token design methods; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers improved variational inference using inverse autoregressive flow (IAF) and VAEs, with no discussion of discrete audio tokenization or related token design methods; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improved variational inference techniques using inverse autoregressive flow for high-dimensional latent spaces, without addressing the generation or design of discrete audio tokens or audio representation quantization relevant to speech, music, or audio sequence modeling as required by the inclusion criteria. It neither discusses tokenization methods, vector quantization, audio language modeling, nor sequence modeling of discrete audio tokens. Therefore, it does not meet the inclusion criteria which focus on discrete audio tokens and their applications, nor does it correspond to the exclusion criteria about demos or codec standards, indicating it is outside the scope of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improved variational inference techniques using inverse autoregressive flow for high-dimensional latent spaces, without addressing the generation or design of discrete audio tokens or audio representation quantization relevant to speech, music, or audio sequence modeling as required by the inclusion criteria. It neither discusses tokenization methods, vector quantization, audio language modeling, nor sequence modeling of discrete audio tokens. Therefore, it does not meet the inclusion criteria which focus on discrete audio tokens and their applications, nor does it correspond to the exclusion criteria about demos or codec standards, indicating it is outside the scope of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Hierarchical Recurrent Neural Network for Symbolic Melody Generation",
    "abstract": "In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.",
    "metadata": {
      "title": "A Hierarchical Recurrent Neural Network for Symbolic Melody Generation",
      "summary": "In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.",
      "abstract": "In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.",
      "doi": "https://doi.org/10.1109/tcyb.2019.2953194",
      "openalex_id": "https://openalex.org/W2992790584",
      "arxiv_id": "",
      "publication_date": "2019-12-02",
      "published": "2019-12-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work targets hierarchical LSTM for symbolic melody generation and does not present discrete audio tokens or a tokenization/quantization pipeline; thus it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work targets hierarchical LSTM for symbolic melody generation and does not present discrete audio tokens or a tokenization/quantization pipeline; thus it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a hierarchical recurrent neural network for symbolic melody generation but does not describe any method related to creating discrete audio tokens through vector quantization or similar quantization mechanisms, nor does it mention using such tokens as an intermediary representation for language or sequence models. The focus is on melody generation via neural networks rather than on discrete token design or generation explicitly meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a hierarchical recurrent neural network for symbolic melody generation but does not describe any method related to creating discrete audio tokens through vector quantization or similar quantization mechanisms, nor does it mention using such tokens as an intermediary representation for language or sequence models. The focus is on melody generation via neural networks rather than on discrete token design or generation explicitly meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
    "abstract": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.",
    "metadata": {
      "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
      "summary": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.",
      "abstract": "Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.",
      "doi": "https://doi.org/10.21437/interspeech.2021-349",
      "openalex_id": "https://openalex.org/W3095292526",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses non-autoregressive self-supervised speech representations but does not mention discrete audio tokens, vector quantization, or tokenization methods; hence it does not meet the discrete-token criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses non-autoregressive self-supervised speech representations but does not mention discrete audio tokens, vector quantization, or tokenization methods; hence it does not meet the discrete-token criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Non-Autoregressive Predictive Coding (NPC) for learning speech representations using local dependencies, but it does not mention generating discrete audio tokens via vector quantization or similar discrete tokenization methods; thus, it lacks a clear token generation process or discrete representation essential for inclusion according to the defined criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes Non-Autoregressive Predictive Coding (NPC) for learning speech representations using local dependencies, but it does not mention generating discrete audio tokens via vector quantization or similar discrete tokenization methods; thus, it lacks a clear token generation process or discrete representation essential for inclusion according to the defined criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unified Language Model Pre-training for Natural Language Understanding\\n and Generation",
    "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\\nbe fine-tuned for both natural language understanding and generation tasks. The\\nmodel is pre-trained using three types of language modeling tasks:\\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\\nmodeling is achieved by employing a shared Transformer network and utilizing\\nspecific self-attention masks to control what context the prediction conditions\\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UniLM achieves new\\nstate-of-the-art results on five natural language generation datasets,\\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\\nis 2.65). The code and pre-trained models are available at\\nhttps://github.com/microsoft/unilm.\\n",
    "metadata": {
      "title": "Unified Language Model Pre-training for Natural Language Understanding\\n and Generation",
      "summary": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\\nbe fine-tuned for both natural language understanding and generation tasks. The\\nmodel is pre-trained using three types of language modeling tasks:\\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\\nmodeling is achieved by employing a shared Transformer network and utilizing\\nspecific self-attention masks to control what context the prediction conditions\\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UniLM achieves new\\nstate-of-the-art results on five natural language generation datasets,\\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\\nis 2.65). The code and pre-trained models are available at\\nhttps://github.com/microsoft/unilm.\\n",
      "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\\nbe fine-tuned for both natural language understanding and generation tasks. The\\nmodel is pre-trained using three types of language modeling tasks:\\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\\nmodeling is achieved by employing a shared Transformer network and utilizing\\nspecific self-attention masks to control what context the prediction conditions\\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UniLM achieves new\\nstate-of-the-art results on five natural language generation datasets,\\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\\nis 2.65). The code and pre-trained models are available at\\nhttps://github.com/microsoft/unilm.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1905.03197",
      "openalex_id": "https://openalex.org/W2971274815",
      "arxiv_id": "",
      "publication_date": "2019-05-08",
      "published": "2019-05-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets a unified language model for NLP tasks with no discrete audio token design or audio-domain methodology, thus it does not meet the Inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets a unified language model for NLP tasks with no discrete audio token design or audio-domain methodology, thus it does not meet the Inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a unified language model pre-training approach for natural language understanding and generation, specifically involving text-based language models and does not address discrete audio tokens, their production, quantization, or usage in audio-related sequence modeling, thus not meeting any of the inclusion criteria related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a unified language model pre-training approach for natural language understanding and generation, specifically involving text-based language models and does not address discrete audio tokens, their production, quantization, or usage in audio-related sequence modeling, thus not meeting any of the inclusion criteria related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Virtual Phone Discovery for Speech Synthesis Without Text",
    "abstract": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
    "metadata": {
      "title": "Virtual Phone Discovery for Speech Synthesis Without Text",
      "summary": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "abstract": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "doi": "https://doi.org/10.1109/globalsip45357.2019.8969412",
      "openalex_id": "https://openalex.org/W3003750857",
      "arxiv_id": "",
      "publication_date": "2019-11-01",
      "published": "2019-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes automatic discovery of discrete unit tokens (virtual phones) from speech and using them for speech synthesis with a token-like, one-hot encoding pipeline, including segmentation, clustering, and modeling, which aligns with Discrete Audio Tokens criteria; no evident exclusion factors (e.g., purely demo content) are present.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes automatic discovery of discrete unit tokens (virtual phones) from speech and using them for speech synthesis with a token-like, one-hot encoding pipeline, including segmentation, clustering, and modeling, which aligns with Discrete Audio Tokens criteria; no evident exclusion factors (e.g., purely demo content) are present.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes transforming speech signals into sequences of acoustic subword units or virtual phones discovered automatically via segmentation and clustering methods, which serve as discrete token-like units for speech synthesis. However, it does not specify that these tokens are produced through vector quantization or similar discrete encoding mechanisms emphasized in the inclusion criteria, nor does it clarify their semantic or acoustic disentanglement properties. The usage of these virtual phones as sequences for a neural synthesis model aligns partially with the concept of discrete audio tokens, but the method of token generation and detailed quantization mechanisms are not clearly described, limiting reproducibility and comparability. Therefore, it partially meets the criteria but lacks sufficient detail on token generation and does not explicitly frame the units as discrete tokens used for language/sequential modeling with semantic disentanglement or multi-task generalization, making its inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study describes transforming speech signals into sequences of acoustic subword units or virtual phones discovered automatically via segmentation and clustering methods, which serve as discrete token-like units for speech synthesis. However, it does not specify that these tokens are produced through vector quantization or similar discrete encoding mechanisms emphasized in the inclusion criteria, nor does it clarify their semantic or acoustic disentanglement properties. The usage of these virtual phones as sequences for a neural synthesis model aligns partially with the concept of discrete audio tokens, but the method of token generation and detailed quantization mechanisms are not clearly described, limiting reproducibility and comparability. Therefore, it partially meets the criteria but lacks sufficient detail on token generation and does not explicitly frame the units as discrete tokens used for language/sequential modeling with semantic disentanglement or multi-task generalization, making its inclusion uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: JuniorNano rated 4 noting alignment with discrete-unit discovery and usage; JuniorMini rated 3 noting missing explicit VQ/RVQ quantization details. My judgement: the abstract describes a clear pipeline that converts continuous speech into discrete symbols—‘virtual phones’—via unsupervised segmentation and clustering (with one-hot encoding) and then uses those discrete units for sequence-based synthesis, which satisfies the core ‘‘Discrete Audio Tokens’’ definition (discretization mechanism producing token-like units used for generation) and is not a standard codec or merely a demo; however it does not use or explicitly claim VQ/RVQ and lacks some quantizer-specific details, so it is borderline but nonetheless fits the inclusion scope. 1-sentence explanation for score: It should be included because it produces and uses discrete, clustering-derived token units for audio generation (meeting the topic’s core requirement), despite not using explicit VQ/RVQ quantizers.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: JuniorNano rated 4 noting alignment with discrete-unit discovery and usage; JuniorMini rated 3 noting missing explicit VQ/RVQ quantization details. My judgement: the abstract describes a clear pipeline that converts continuous speech into discrete symbols—‘virtual phones’—via unsupervised segmentation and clustering (with one-hot encoding) and then uses those discrete units for sequence-based synthesis, which satisfies the core ‘‘Discrete Audio Tokens’’ definition (discretization mechanism producing token-like units used for generation) and is not a standard codec or merely a demo; however it does not use or explicitly claim VQ/RVQ and lacks some quantizer-specific details, so it is borderline but nonetheless fits the inclusion scope. 1-sentence explanation for score: It should be included because it produces and uses discrete, clustering-derived token units for audio generation (meeting the topic’s core requirement), despite not using explicit VQ/RVQ quantizers.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
    "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
    "metadata": {
      "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
      "summary": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
      "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
      "doi": "https://doi.org/10.21437/interspeech.2017-343",
      "openalex_id": "https://openalex.org/W2577366047",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract describes end-to-end attention-based seq2seq ASR with LM integration and does not discuss discrete audio tokens or tokenization methods (VQ/RVQ, hierarchical quantization, etc.), thus it does not meet the inclusion criteria centered on Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract describes end-to-end attention-based seq2seq ASR with LM integration and does not discuss discrete audio tokens or tokenization methods (VQ/RVQ, hierarchical quantization, etc.), thus it does not meet the inclusion criteria centered on Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sequence-to-sequence speech recognition using neural networks and language models, but does not discuss the generation or use of discrete audio tokens derived from vector quantization, residual quantization, or semantic disentanglement methods as required by the inclusion criteria. Furthermore, the token generation process and discrete representations used for language or sequence models are not described in the abstract. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sequence-to-sequence speech recognition using neural networks and language models, but does not discuss the generation or use of discrete audio tokens derived from vector quantization, residual quantization, or semantic disentanglement methods as required by the inclusion criteria. Furthermore, the token generation process and discrete representations used for language or sequence models are not described in the abstract. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An argument for basic emotions",
    "abstract": "Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.",
    "metadata": {
      "title": "An argument for basic emotions",
      "summary": "Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.",
      "abstract": "Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.",
      "doi": "https://doi.org/10.1080/02699939208411068",
      "openalex_id": "https://openalex.org/W1966797434",
      "arxiv_id": "",
      "publication_date": "1992-05-01",
      "published": "1992-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on basic emotions and evolutionary psychology, with no discrete audio token design, token generation method, or evaluation suitable for audio language modeling; it does not meet inclusion criteria and clearly falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on basic emotions and evolutionary psychology, with no discrete audio token design, token generation method, or evaluation suitable for audio language modeling; it does not meet inclusion criteria and clearly falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the theoretical understanding of basic emotions from an evolutionary perspective, without any reference to discrete audio tokens, vector quantization, or audio language modeling, and thus do not meet the inclusion criteria of involving discrete audio token design or generation, nor do they provide relevant methodological details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the theoretical understanding of basic emotions from an evolutionary perspective, without any reference to discrete audio tokens, vector quantization, or audio language modeling, and thus do not meet the inclusion criteria of involving discrete audio token design or generation, nor do they provide relevant methodological details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
    "abstract": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.",
    "metadata": {
      "title": "Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network",
      "summary": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.",
      "abstract": "We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1323",
      "openalex_id": "https://openalex.org/W3045354608",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets continuous speech representation and emotion conversion using latent embeddings of F0 and spectrum with LDDMM regularization, and does not involve discrete audio tokens, vector quantization, or tokenization workflows, hence it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets continuous speech representation and emotion conversion using latent embeddings of F0 and spectrum with LDDMM regularization, and does not involve discrete audio tokens, vector quantization, or tokenization workflows, hence it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural network method for emotion conversion in speech focusing on modifying fundamental frequency and spectrum but does not mention producing discrete audio tokens or using vector quantization or related techniques to create tokens for sequence modeling or language modeling; thus, it does not meet the main inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural network method for emotion conversion in speech focusing on modifying fundamental frequency and spectrum but does not mention producing discrete audio tokens or using vector quantization or related techniques to create tokens for sequence modeling or language modeling; thus, it does not meet the main inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Theory behind Controllable Expressive Speech Synthesis: A Cross-Disciplinary Approach",
    "abstract": "As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.",
    "metadata": {
      "title": "The Theory behind Controllable Expressive Speech Synthesis: A Cross-Disciplinary Approach",
      "summary": "As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.",
      "abstract": "As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.",
      "doi": "https://doi.org/10.5772/intechopen.89849",
      "openalex_id": "https://openalex.org/W2979790850",
      "arxiv_id": "",
      "publication_date": "2019-12-02",
      "published": "2019-12-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses general expressive speech synthesis and TTS paradigms, but does not present discrete audio tokens or a tokenization framework as its main contribution, so it does not meet the inclusion criteria; it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses general expressive speech synthesis and TTS paradigms, but does not present discrete audio tokens or a tokenization framework as its main contribution, so it does not meet the inclusion criteria; it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on controllable expressive speech synthesis, discussing paradigms like concatenative, parametric, and neural TTS methods using deep learning; however, there is no mention of discrete audio tokens, vector quantization, residual vector quantization, or any discrete token generation processes as defined in the inclusion criteria. Moreover, the content does not describe any tokenization mechanisms or their use in language or sequence models for audio, nor does it address semantic/acoustic disentanglement or cross-modal generation. Therefore, it does not meet the inclusion criteria concerning discrete audio tokens as a core method or contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on controllable expressive speech synthesis, discussing paradigms like concatenative, parametric, and neural TTS methods using deep learning; however, there is no mention of discrete audio tokens, vector quantization, residual vector quantization, or any discrete token generation processes as defined in the inclusion criteria. Moreover, the content does not describe any tokenization mechanisms or their use in language or sequence models for audio, nor does it address semantic/acoustic disentanglement or cross-modal generation. Therefore, it does not meet the inclusion criteria concerning discrete audio tokens as a core method or contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-Target Emotional Voice Conversion With Neural Vocoders",
    "abstract": "Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.",
    "metadata": {
      "title": "Multi-Target Emotional Voice Conversion With Neural Vocoders",
      "summary": "Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.",
      "abstract": "Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.",
      "doi": "https://doi.org/10.48550/arxiv.2004.03782",
      "openalex_id": "https://openalex.org/W3015669407",
      "arxiv_id": "",
      "publication_date": "2020-04-08",
      "published": "2020-04-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multi-target emotional voice conversion using neural vocoders and does not address discrete audio tokens, tokenization, or discrete representations for downstream language/sequence models, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multi-target emotional voice conversion using neural vocoders and does not address discrete audio tokens, tokenization, or discrete representations for downstream language/sequence models, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on emotional voice conversion using neural vocoders and deep learning models to generate expressive synthetic speech, but it does not mention any process of producing discrete audio tokens, vector quantization-based tokenization, or the use of tokens for sequence modeling or language modeling; thus, it does not meet the inclusion criterion of discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on emotional voice conversion using neural vocoders and deep learning models to generate expressive synthetic speech, but it does not mention any process of producing discrete audio tokens, vector quantization-based tokenization, or the use of tokens for sequence modeling or language modeling; thus, it does not meet the inclusion criterion of discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GMM-Based Emotional Voice Conversion Using Spectrum and Prosody Features",
    "abstract": "We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.",
    "metadata": {
      "title": "GMM-Based Emotional Voice Conversion Using Spectrum and Prosody Features",
      "summary": "We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.",
      "abstract": "We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.",
      "doi": "https://doi.org/10.5923/j.ajsp.20120205.06",
      "openalex_id": "https://openalex.org/W2077801020",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on GMM-based emotional voice conversion using spectrum and prosody; it does not address discrete audio tokens or tokenization/quantization workflows, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on GMM-based emotional voice conversion using spectrum and prosody; it does not address discrete audio tokens or tokenization/quantization workflows, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on GMM-based emotional voice conversion using spectrum and prosody features, without involving discrete audio token generation, vector quantization, or related techniques for producing or utilizing discrete tokens for downstream language or sequence modeling. The abstract lacks details on token formation processes or quantization methods, and the contribution centers on emotional voice conversion rather than discrete audio token research aligned with the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on GMM-based emotional voice conversion using spectrum and prosody features, without involving discrete audio token generation, vector quantization, or related techniques for producing or utilizing discrete tokens for downstream language or sequence modeling. The abstract lacks details on token formation processes or quantization methods, and the contribution centers on emotional voice conversion rather than discrete audio token research aligned with the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emotional Voice Conversion Using Multitask Learning with Text-To-Speech",
    "abstract": "Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.",
    "metadata": {
      "title": "Emotional Voice Conversion Using Multitask Learning with Text-To-Speech",
      "summary": "Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.",
      "abstract": "Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053255",
      "openalex_id": "https://openalex.org/W3015719316",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on emotional voice conversion using multitask learning with TTS and discusses preserving linguistic content, but it does not address discrete audio tokens, vector quantization, or token-based representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on emotional voice conversion using multitask learning with TTS and discusses preserving linguistic content, but it does not address discrete audio tokens, vector quantization, or token-based representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on emotional voice conversion using multitask learning with text-to-speech, without mentioning any discrete audio tokens, vector quantization, or related mechanisms for converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria centered on discrete audio token design and generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on emotional voice conversion using multitask learning with text-to-speech, without mentioning any discrete audio tokens, vector quantization, or related mechanisms for converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria centered on discrete audio token design and generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems",
    "abstract": "In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.",
    "metadata": {
      "title": "The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems",
      "summary": "In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.",
      "abstract": "In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.",
      "doi": "https://doi.org/10.48550/arxiv.1806.09514",
      "openalex_id": "https://openalex.org/W2810914326",
      "arxiv_id": "",
      "publication_date": "2018-06-25",
      "published": "2018-06-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on an emotional speech database for synthesis and a simple conversion from neutral to angry speech; it does not address discrete audio tokens, tokenization methods, or token-based modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on an emotional speech database for synthesis and a simple conversion from neutral to angry speech; it does not address discrete audio tokens, tokenization methods, or token-based modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article introduces an emotional voice database aimed at voice generation systems with emotional control, but it does not describe any discrete audio token generation, vector quantization, or related processes involving discrete token-based audio representations as defined in the inclusion criteria; hence, it does not meet the core topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article introduces an emotional voice database aimed at voice generation systems with emotional control, but it does not describe any discrete audio token generation, vector quantization, or related processes involving discrete token-based audio representations as defined in the inclusion criteria; hence, it does not meet the core topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "End-to-end text-dependent speaker verification",
    "abstract": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications Like ours that require highly accurate, easy-to-maintain systems with a small footprint.",
    "metadata": {
      "title": "End-to-end text-dependent speaker verification",
      "summary": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications Like ours that require highly accurate, easy-to-maintain systems with a small footprint.",
      "abstract": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications Like ours that require highly accurate, easy-to-maintain systems with a small footprint.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472652",
      "openalex_id": "https://openalex.org/W2114925438",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It concerns end-to-end speaker verification with a neural network architecture and direct scoring, but has no discussion of discrete audio tokens, token generation, VQ/RVQ, or token-based representations; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "It concerns end-to-end speaker verification with a neural network architecture and direct scoring, but has no discussion of discrete audio tokens, token generation, VQ/RVQ, or token-based representations; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an end-to-end speaker verification system using neural networks and does not discuss the generation or use of discrete audio tokens through vector quantization or related techniques as defined in the inclusion criteria. There is no mention of token generation, quantization, or using discrete tokens for sequence/language modeling or downstream audio tasks, thus it does not meet the core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an end-to-end speaker verification system using neural networks and does not discuss the generation or use of discrete audio tokens through vector quantization or related techniques as defined in the inclusion criteria. There is no mention of token generation, quantization, or using discrete tokens for sequence/language modeling or downstream audio tasks, thus it does not meet the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Yet Another Algorithm for Pitch Tracking",
    "abstract": "In this paper, we present a pitch detection algorithm that is extremely robust for both high quality and telephone speech. The kernel method for this algorithm is the \"NCCF or Normalized Cross Correlation\" reported by David Talkin [1]. Major innovations include: processing of the original acoustic signal and a nonlinearly processed version of the signal to partially restore very weak F0 components; intelligent peak picking to select multiple F0 candidates and assign merit factors; and, incorporation of highly rohust pitch contours obtained from smoothed versions of low frequency portions of spectrograms. Dynamic programming is used to find the \"best\" pitch track among all the candidates, using both local and transition costs. We evaluated our algorithm using the Keele pitch extraction reference database as \"ground truth\" for both \"high quality\" and \"telephone\" speech. For both types of speech, the error rates obtained are lower than the lowest reported in the literature.",
    "metadata": {
      "title": "Yet Another Algorithm for Pitch Tracking",
      "summary": "In this paper, we present a pitch detection algorithm that is extremely robust for both high quality and telephone speech. The kernel method for this algorithm is the \"NCCF or Normalized Cross Correlation\" reported by David Talkin [1]. Major innovations include: processing of the original acoustic signal and a nonlinearly processed version of the signal to partially restore very weak F0 components; intelligent peak picking to select multiple F0 candidates and assign merit factors; and, incorporation of highly rohust pitch contours obtained from smoothed versions of low frequency portions of spectrograms. Dynamic programming is used to find the \"best\" pitch track among all the candidates, using both local and transition costs. We evaluated our algorithm using the Keele pitch extraction reference database as \"ground truth\" for both \"high quality\" and \"telephone\" speech. For both types of speech, the error rates obtained are lower than the lowest reported in the literature.",
      "abstract": "In this paper, we present a pitch detection algorithm that is extremely robust for both high quality and telephone speech. The kernel method for this algorithm is the \"NCCF or Normalized Cross Correlation\" reported by David Talkin [1]. Major innovations include: processing of the original acoustic signal and a nonlinearly processed version of the signal to partially restore very weak F0 components; intelligent peak picking to select multiple F0 candidates and assign merit factors; and, incorporation of highly rohust pitch contours obtained from smoothed versions of low frequency portions of spectrograms. Dynamic programming is used to find the \"best\" pitch track among all the candidates, using both local and transition costs. We evaluated our algorithm using the Keele pitch extraction reference database as \"ground truth\" for both \"high quality\" and \"telephone\" speech. For both types of speech, the error rates obtained are lower than the lowest reported in the literature.",
      "doi": "https://doi.org/10.1109/icassp.2002.5743729",
      "openalex_id": "https://openalex.org/W2115098197",
      "arxiv_id": "",
      "publication_date": "2002-05-01",
      "published": "2002-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on a pitch tracking algorithm and evaluation, with no discussion of discrete audio token representations or tokenization suitable for language-model style processing, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on a pitch tracking algorithm and evaluation, with no discussion of discrete audio token representations or tokenization suitable for language-model style processing, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a pitch detection algorithm for speech signals and does not discuss any discrete audio tokenization methods, vector quantization, or their use in audio language modeling or sequence modeling. It also lacks detail on token generation processes and does not meet the inclusion criteria centered on discrete audio token design and use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a pitch detection algorithm for speech signals and does not discuss any discrete audio tokenization methods, vector quantization, or their use in audio language modeling or sequence modeling. It also lacks detail on token generation processes and does not meet the inclusion criteria centered on discrete audio token design and use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion",
    "abstract": "Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.",
    "metadata": {
      "title": "Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion",
      "summary": "Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.",
      "abstract": "Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683865",
      "openalex_id": "https://openalex.org/W2938833595",
      "arxiv_id": "",
      "publication_date": "2019-04-17",
      "published": "2019-04-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio tokens or token-based representations; it focuses on seq2seq F0 transformation for speech emotion conversion rather than token design.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio tokens or token-based representations; it focuses on seq2seq F0 transformation for speech emotion conversion rather than token design.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sequence-to-sequence modeling of F0 for speech emotion conversion, which addresses transforming intonation of human voice for emotional expressivity. However, there is no mention of discrete audio token generation, vector quantization, or other quantization methods to produce discrete tokens for subsequent use in language or sequence models. The abstract lacks description about token generation process, codebook design, quantization levels, or usage of discrete tokens for audio language modeling or downstream tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sequence-to-sequence modeling of F0 for speech emotion conversion, which addresses transforming intonation of human voice for emotional expressivity. However, there is no mention of discrete audio token generation, vector quantization, or other quantization methods to produce discrete tokens for subsequent use in language or sequence models. The abstract lacks description about token generation process, codebook design, quantization levels, or usage of discrete tokens for audio language modeling or downstream tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning",
    "abstract": "Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",
    "metadata": {
      "title": "An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning",
      "summary": "Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",
      "abstract": "Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",
      "doi": "https://doi.org/10.1109/taslp.2020.3038524",
      "openalex_id": "https://openalex.org/W3098557217",
      "arxiv_id": "",
      "publication_date": "2020-11-17",
      "published": "2020-11-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article provides an overview of voice conversion from statistical modeling to deep learning and does not discuss discrete audio tokens or token-based representations needed by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article provides an overview of voice conversion from statistical modeling to deep learning and does not discuss discrete audio tokens or token-based representations needed by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article provides a comprehensive overview of voice conversion techniques, including statistical modeling and deep learning, focusing on modifying speaker identity while preserving linguistic content. However, it does not mention discrete audio tokens, vector quantization, or the process of generating or using such tokens for audio language modeling or related tasks, which are essential inclusion criteria. It appears more focused on voice conversion technologies rather than the design or application of discrete audio tokens as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article provides a comprehensive overview of voice conversion techniques, including statistical modeling and deep learning, focusing on modifying speaker identity while preserving linguistic content. However, it does not mention discrete audio tokens, vector quantization, or the process of generating or using such tokens for audio language modeling or related tasks, which are essential inclusion criteria. It appears more focused on voice conversion technologies rather than the design or application of discrete audio tokens as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications",
    "abstract": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.",
    "metadata": {
      "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications",
      "summary": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.",
      "abstract": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.",
      "doi": "https://doi.org/10.1587/transinf.2015edp7457",
      "openalex_id": "https://openalex.org/W2471520273",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The WORLD paper presents a vocoder-based speech synthesis system aimed at real-time quality and speed and does not discuss discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The WORLD paper presents a vocoder-based speech synthesis system aimed at real-time quality and speed and does not discuss discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a vocoder-based speech synthesis system focused on improving sound quality and real-time processing speed. However, it does not discuss generating discrete audio tokens via vector quantization or related discrete representation methods aimed at sequence modeling or language modeling, nor does it describe any token formation, quantization, or semantic disentanglement processes. It is a speech synthesis system emphasizing real-time performance rather than discrete token design or utilization for downstream generative or understanding tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a vocoder-based speech synthesis system focused on improving sound quality and real-time processing speed. However, it does not discuss generating discrete audio tokens via vector quantization or related discrete representation methods aimed at sequence modeling or language modeling, nor does it describe any token formation, quantization, or semantic disentanglement processes. It is a speech synthesis system emphasizing real-time performance rather than discrete token design or utilization for downstream generative or understanding tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Expressive Text-to-Speech using Style Tag",
    "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.",
    "metadata": {
      "title": "Expressive Text-to-Speech using Style Tag",
      "summary": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.",
      "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.",
      "doi": "https://doi.org/10.48550/arxiv.2104.00436",
      "openalex_id": "https://openalex.org/W3144988954",
      "arxiv_id": "",
      "publication_date": "2021-04-01",
      "published": "2021-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes expressive TTS using natural-language style tags and a non-autoregressive architecture, but it does not address discrete audio tokens, token generation/quantization processes, or token-based sequence modeling, so it fails the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes expressive TTS using natural-language style tags and a non-autoregressive architecture, but it does not address discrete audio tokens, token generation/quantization processes, or token-based sequence modeling, so it fails the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an expressive text-to-speech system that uses style tags in natural language to control speaking style, without mentioning any discrete audio token representations, vector quantization, or similar mechanisms that convert audio into discrete token sequences for language model processing, which is central to the inclusion criteria. It primarily targets expressive TTS architecture and style control rather than discrete token generation or audio compression, and does not describe token generation flows or quantization techniques. Therefore, it does not meet the core inclusion requirements related to discrete audio tokens and their usage for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an expressive text-to-speech system that uses style tags in natural language to control speaking style, without mentioning any discrete audio token representations, vector quantization, or similar mechanisms that convert audio into discrete token sequences for language model processing, which is central to the inclusion criteria. It primarily targets expressive TTS architecture and style control rather than discrete token generation or audio compression, and does not describe token generation flows or quantization techniques. Therefore, it does not meet the core inclusion requirements related to discrete audio tokens and their usage for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating Language Impact in Bilingual Approaches for Computational\\n Language Documentation",
    "abstract": "For endangered languages, data collection campaigns have to accommodate the\\nchallenge that many of them are from oral tradition, and producing\\ntranscriptions is costly. Therefore, it is fundamental to translate them into a\\nwidely spoken language to ensure interpretability of the recordings. In this\\npaper we investigate how the choice of translation language affects the\\nposterior documentation work and potential automatic approaches which will work\\non top of the produced bilingual corpus. For answering this question, we use\\nthe MaSS multilingual speech corpus (Boito et al., 2020) for creating 56\\nbilingual pairs that we apply to the task of low-resource unsupervised word\\nsegmentation and alignment. Our results highlight that the choice of language\\nfor translation influences the word segmentation performance, and that\\ndifferent lexicons are learned by using different aligned translations. Lastly,\\nthis paper proposes a hybrid approach for bilingual word segmentation,\\ncombining boundary clues extracted from a non-parametric Bayesian model\\n(Goldwater et al., 2009a) with the attentional word segmentation neural model\\nfrom Godard et al. (2018). Our results suggest that incorporating these clues\\ninto the neural models' input representation increases their translation and\\nalignment quality, specially for challenging language pairs.\\n",
    "metadata": {
      "title": "Investigating Language Impact in Bilingual Approaches for Computational\\n Language Documentation",
      "summary": "For endangered languages, data collection campaigns have to accommodate the\\nchallenge that many of them are from oral tradition, and producing\\ntranscriptions is costly. Therefore, it is fundamental to translate them into a\\nwidely spoken language to ensure interpretability of the recordings. In this\\npaper we investigate how the choice of translation language affects the\\nposterior documentation work and potential automatic approaches which will work\\non top of the produced bilingual corpus. For answering this question, we use\\nthe MaSS multilingual speech corpus (Boito et al., 2020) for creating 56\\nbilingual pairs that we apply to the task of low-resource unsupervised word\\nsegmentation and alignment. Our results highlight that the choice of language\\nfor translation influences the word segmentation performance, and that\\ndifferent lexicons are learned by using different aligned translations. Lastly,\\nthis paper proposes a hybrid approach for bilingual word segmentation,\\ncombining boundary clues extracted from a non-parametric Bayesian model\\n(Goldwater et al., 2009a) with the attentional word segmentation neural model\\nfrom Godard et al. (2018). Our results suggest that incorporating these clues\\ninto the neural models' input representation increases their translation and\\nalignment quality, specially for challenging language pairs.\\n",
      "abstract": "For endangered languages, data collection campaigns have to accommodate the\\nchallenge that many of them are from oral tradition, and producing\\ntranscriptions is costly. Therefore, it is fundamental to translate them into a\\nwidely spoken language to ensure interpretability of the recordings. In this\\npaper we investigate how the choice of translation language affects the\\nposterior documentation work and potential automatic approaches which will work\\non top of the produced bilingual corpus. For answering this question, we use\\nthe MaSS multilingual speech corpus (Boito et al., 2020) for creating 56\\nbilingual pairs that we apply to the task of low-resource unsupervised word\\nsegmentation and alignment. Our results highlight that the choice of language\\nfor translation influences the word segmentation performance, and that\\ndifferent lexicons are learned by using different aligned translations. Lastly,\\nthis paper proposes a hybrid approach for bilingual word segmentation,\\ncombining boundary clues extracted from a non-parametric Bayesian model\\n(Goldwater et al., 2009a) with the attentional word segmentation neural model\\nfrom Godard et al. (2018). Our results suggest that incorporating these clues\\ninto the neural models' input representation increases their translation and\\nalignment quality, specially for challenging language pairs.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2003.13325",
      "openalex_id": "https://openalex.org/W3029422373",
      "arxiv_id": "",
      "publication_date": "2020-03-30",
      "published": "2020-03-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses bilingual word segmentation and translation impact in computational language documentation, with no discussion of discrete audio token representations (e.g., VQ/RVQ, SSL-derived discrete tokens) or audio token design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses bilingual word segmentation and translation impact in computational language documentation, with no discussion of discrete audio token representations (e.g., VQ/RVQ, SSL-derived discrete tokens) or audio token design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the impact of translation language choice on bilingual corpus creation for endangered languages and on methods for word segmentation and alignment, but it does not address the generation or utilization of discrete audio tokens through vector quantization, discrete representations, or similar tokenization techniques for audio language modeling or sequence modeling. Therefore, it does not meet the inclusion criteria, which require discrete audio token design and usage as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the impact of translation language choice on bilingual corpus creation for endangered languages and on methods for word segmentation and alignment, but it does not address the generation or utilization of discrete audio tokens through vector quantization, discrete representations, or similar tokenization techniques for audio language modeling or sequence modeling. Therefore, it does not meet the inclusion criteria, which require discrete audio token design and usage as a core contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Building Speech Recognition Systems for Language Documentation: The CoEDL Endangered Language Pipeline and Inference System (ELPIS)",
    "abstract": "Machine learning has revolutionized speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of ELPIS, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. ELPIS puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies.",
    "metadata": {
      "title": "Building Speech Recognition Systems for Language Documentation: The CoEDL Endangered Language Pipeline and Inference System (ELPIS)",
      "summary": "Machine learning has revolutionized speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of ELPIS, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. ELPIS puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies.",
      "abstract": "Machine learning has revolutionized speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of ELPIS, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. ELPIS puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies.",
      "doi": "https://doi.org/10.21437/sltu.2018-43",
      "openalex_id": "https://openalex.org/W2895097770",
      "arxiv_id": "",
      "publication_date": "2018-08-29",
      "published": "2018-08-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on a practical ASR pipeline for endangered languages and does not mention discrete audio tokens, tokenization, or quantization methods required by the inclusion criteria, so it does not meet the core criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a practical ASR pipeline for endangered languages and does not mention discrete audio tokens, tokenization, or quantization methods required by the inclusion criteria, so it does not meet the core criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a speech recognition pipeline for endangered languages but does not provide any indication of discrete audio token design or quantization mechanisms such as vector quantization that produce discrete tokens as core contributions; it focuses on applying machine learning speech technologies rather than creating or utilizing discrete audio token representations as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a speech recognition pipeline for endangered languages but does not provide any indication of discrete audio token design or quantization mechanisms such as vector quantization that produce discrete tokens as core contributions; it focuses on applying machine learning speech technologies rather than creating or utilizing discrete audio token representations as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DEVELOPMENTS OF SWAHILI RESOURCES FOR AN AUTOMATIC SPEECH RECOGNITION SYSTEM",
    "abstract": "This article describes our efforts to provide ASR resources for Swahili, a Bantu language spoken in a wide area of East Africa. We start with an introduction on the language situation, both at linguistic and digital level. Then, we report the selected strategies to develop a text corpus, a pronunciation dictionary and a speech corpus for this under-resourced language. We explore methodologies as crowdsourcing or collaborative transcription process. Besides, we take advantage of some linguistic characteristics of the language such as rich morphology or shared vocabulary with English to improve performance of our baseline Swahili ASR system in a broadcast speech transcription task. Index Terms — Swahili, under-resourced languages, automatic speech recognition, speech resources",
    "metadata": {
      "title": "DEVELOPMENTS OF SWAHILI RESOURCES FOR AN AUTOMATIC SPEECH RECOGNITION SYSTEM",
      "summary": "This article describes our efforts to provide ASR resources for Swahili, a Bantu language spoken in a wide area of East Africa. We start with an introduction on the language situation, both at linguistic and digital level. Then, we report the selected strategies to develop a text corpus, a pronunciation dictionary and a speech corpus for this under-resourced language. We explore methodologies as crowdsourcing or collaborative transcription process. Besides, we take advantage of some linguistic characteristics of the language such as rich morphology or shared vocabulary with English to improve performance of our baseline Swahili ASR system in a broadcast speech transcription task. Index Terms — Swahili, under-resourced languages, automatic speech recognition, speech resources",
      "abstract": "This article describes our efforts to provide ASR resources for Swahili, a Bantu language spoken in a wide area of East Africa. We start with an introduction on the language situation, both at linguistic and digital level. Then, we report the selected strategies to develop a text corpus, a pronunciation dictionary and a speech corpus for this under-resourced language. We explore methodologies as crowdsourcing or collaborative transcription process. Besides, we take advantage of some linguistic characteristics of the language such as rich morphology or shared vocabulary with English to improve performance of our baseline Swahili ASR system in a broadcast speech transcription task. Index Terms — Swahili, under-resourced languages, automatic speech recognition, speech resources",
      "doi": "",
      "openalex_id": "https://openalex.org/W2401271873",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses Swahili ASR resources and data collection rather than discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses Swahili ASR resources and data collection rather than discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on developing ASR resources such as text corpora, pronunciation dictionaries, and speech corpora for Swahili, but does not mention any process of converting continuous audio signals into discrete audio tokens through vector quantization or related methods, nor discusses discrete token generation, usage in sequence models, or semantic disentanglement, which are critical for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on developing ASR resources such as text corpora, pronunciation dictionaries, and speech corpora for Swahili, but does not mention any process of converting continuous audio signals into discrete audio tokens through vector quantization or related methods, nor discusses discrete token generation, usage in sequence models, or semantic disentanglement, which are critical for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An evaluation of graph clustering methods for unsupervised term discovery",
    "abstract": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
    "metadata": {
      "title": "An evaluation of graph clustering methods for unsupervised term discovery",
      "summary": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
      "abstract": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
      "doi": "https://doi.org/10.21437/interspeech.2015-646",
      "openalex_id": "https://openalex.org/W2407614114",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦自監督語音詞語的發現與圖聚類的評估，並未以離散音訊 token 的設計、產生流程或可用於序列模型的 token 表徵為核心貢獻，因此不符合離散音訊 token 的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦自監督語音詞語的發現與圖聚類的評估，並未以離散音訊 token 的設計、產生流程或可用於序列模型的 token 表徵為核心貢獻，因此不符合離散音訊 token 的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on evaluating graph clustering methods for unsupervised term discovery (UTD) in speech audio, which involves identifying repeated words and phrases without language-specific resources. However, it does not describe or focus on the design or generation of discrete audio tokens derived via vector quantization or similar discrete representation methods as defined in the inclusion criteria. The study is not centered on discrete audio token formation or their use in language or sequence modeling, so it does not meet the inclusion requirements. Additionally, it does not fall under exclusion criteria related to lack of methodological details or codec-only focus, but the key subject does not align with discrete audio tokens as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on evaluating graph clustering methods for unsupervised term discovery (UTD) in speech audio, which involves identifying repeated words and phrases without language-specific resources. However, it does not describe or focus on the design or generation of discrete audio tokens derived via vector quantization or similar discrete representation methods as defined in the inclusion criteria. The study is not centered on discrete audio token formation or their use in language or sequence modeling, so it does not meet the inclusion requirements. Additionally, it does not fall under exclusion criteria related to lack of methodological details or codec-only focus, but the key subject does not align with discrete audio tokens as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Nonparametric bayesian models of lexical acquisition",
    "abstract": "The child learning language is faced with a daunting task: to learn to extract meaning from an apparently meaningless stream of sound. This thesis rests on the assumption that the kinds of generalizations the learner may make are constrained by the interaction of many different types of stochastic information, including innate learning biases. I use computational modeling to investigate how the generalizations made by unsupervised learners are affected by the sources of information available to them. I adopt a Bayesian perspective, where both internal representations of language and any learning biases are made explicit. I begin by presenting a generic framework for language modeling based on nonparametric Bayesian statistics, where model complexity grows with the amount of input data. This framework divides the work of modeling between a generator, which generates lexical items, and an adaptor, which generates frequencies for those items. Separating the two tasks in this way makes the framework flexible, allowing individual components to be easily modified. Standard sampling methods, such as Gibbs or Metropolis-Hastings sampling, may be used for inference. Using this framework, I develop several specific models to investigate questions related to morphological acquisition (identifying stems and suffixes) and word segmentation (identifying word boundaries in phonemically transcribed speech). I apply these models to English corpora of newspaper text and phonemically transcribed child-directed speech. With regard to morphology, my experiments provide evidence that morphological information is learned better from word types than from word tokens. With regard to word segmentation, my results indicate that assuming independence between words (as many previous models have done) leads to undersegmentation of the data. Accounting for local context improves segmentation markedly and yields better results than previous models. I conclude by describing briefly how the models presented here can be extended in order to account for a wider range of linguistic phenomena, including phonetic variability and the relationship between morphology and syntactic class.",
    "metadata": {
      "title": "Nonparametric bayesian models of lexical acquisition",
      "summary": "The child learning language is faced with a daunting task: to learn to extract meaning from an apparently meaningless stream of sound. This thesis rests on the assumption that the kinds of generalizations the learner may make are constrained by the interaction of many different types of stochastic information, including innate learning biases. I use computational modeling to investigate how the generalizations made by unsupervised learners are affected by the sources of information available to them. I adopt a Bayesian perspective, where both internal representations of language and any learning biases are made explicit. \r\nI begin by presenting a generic framework for language modeling based on nonparametric Bayesian statistics, where model complexity grows with the amount of input data. This framework divides the work of modeling between a generator, which generates lexical items, and an adaptor, which generates frequencies for those items. Separating the two tasks in this way makes the framework flexible, allowing individual components to be easily modified. Standard sampling methods, such as Gibbs or Metropolis-Hastings sampling, may be used for inference. \r\nUsing this framework, I develop several specific models to investigate questions related to morphological acquisition (identifying stems and suffixes) and word segmentation (identifying word boundaries in phonemically transcribed speech). I apply these models to English corpora of newspaper text and phonemically transcribed child-directed speech. With regard to morphology, my experiments provide evidence that morphological information is learned better from word types than from word tokens. With regard to word segmentation, my results indicate that assuming independence between words (as many previous models have done) leads to undersegmentation of the data. Accounting for local context improves segmentation markedly and yields better results than previous models. \r\nI conclude by describing briefly how the models presented here can be extended in order to account for a wider range of linguistic phenomena, including phonetic variability and the relationship between morphology and syntactic class.",
      "abstract": "The child learning language is faced with a daunting task: to learn to extract meaning from an apparently meaningless stream of sound. This thesis rests on the assumption that the kinds of generalizations the learner may make are constrained by the interaction of many different types of stochastic information, including innate learning biases. I use computational modeling to investigate how the generalizations made by unsupervised learners are affected by the sources of information available to them. I adopt a Bayesian perspective, where both internal representations of language and any learning biases are made explicit. \r\nI begin by presenting a generic framework for language modeling based on nonparametric Bayesian statistics, where model complexity grows with the amount of input data. This framework divides the work of modeling between a generator, which generates lexical items, and an adaptor, which generates frequencies for those items. Separating the two tasks in this way makes the framework flexible, allowing individual components to be easily modified. Standard sampling methods, such as Gibbs or Metropolis-Hastings sampling, may be used for inference. \r\nUsing this framework, I develop several specific models to investigate questions related to morphological acquisition (identifying stems and suffixes) and word segmentation (identifying word boundaries in phonemically transcribed speech). I apply these models to English corpora of newspaper text and phonemically transcribed child-directed speech. With regard to morphology, my experiments provide evidence that morphological information is learned better from word types than from word tokens. With regard to word segmentation, my results indicate that assuming independence between words (as many previous models have done) leads to undersegmentation of the data. Accounting for local context improves segmentation markedly and yields better results than previous models. \r\nI conclude by describing briefly how the models presented here can be extended in order to account for a wider range of linguistic phenomena, including phonetic variability and the relationship between morphology and syntactic class.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2111668269",
      "arxiv_id": "",
      "publication_date": "2007-01-01",
      "published": "2007-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents nonparametric Bayesian models of lexical acquisition and segmentation, but does not propose or analyze discrete audio token representations or tokenization methods needed for the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents nonparametric Bayesian models of lexical acquisition and segmentation, but does not propose or analyze discrete audio token representations or tokenization methods needed for the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on nonparametric Bayesian models applied to lexical acquisition, morphology, and word segmentation from textual and phonemically transcribed speech data. They do not describe any method for discretizing continuous audio signals into discrete audio tokens, nor do they discuss vector quantization or other token generation processes suitable for language or sequence models processing audio representations. The work centers on computational modeling and Bayesian inference of linguistic structures rather than audio tokenization or audio language modeling as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on nonparametric Bayesian models applied to lexical acquisition, morphology, and word segmentation from textual and phonemically transcribed speech data. They do not describe any method for discretizing continuous audio signals into discrete audio tokens, nor do they discuss vector quantization or other token generation processes suitable for language or sequence models processing audio representations. The work centers on computational modeling and Bayesian inference of linguistic structures rather than audio tokenization or audio language modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Amharic speech corpus for large vocabulary continuous speech recognition",
    "abstract": "• has rich morphology -> many word forms. Phonetics Amharic has a set of 38 phones, seven vowels and thirty-one consonants. Consonants Manner Voicing Place of Articulation of Art/n Lab Dent Pal Vel Glo Stops Voiceless p[p] t[t] m[t∫ ] k[k] [?] Voiced b[b] d[d] ¥[d ] g[g] GlottalizedI[p‘] μ[t‘] 1⁄2[t∫ ‘]q[q] Rounded [kw], [gw], [qw] Fricatives Voiceless f[f] s[s] ][∫ ] h[h] Voiced z[z] [ ] Glottalized O[s‘] Rounded [hw] Nasals Voiced m[m]n[n] }[ ] Liquids Voiced l[l], r[r] Semi vowelsVoiced w[w] y[j]",
    "metadata": {
      "title": "An Amharic speech corpus for large vocabulary continuous speech recognition",
      "summary": "• has rich morphology -> many word forms. Phonetics Amharic has a set of 38 phones, seven vowels and thirty-one consonants. Consonants Manner Voicing Place of Articulation of Art/n Lab Dent Pal Vel Glo Stops Voiceless p[p] t[t] m[t∫ ] k[k] [?] Voiced b[b] d[d] ¥[d ] g[g] GlottalizedI[p‘] μ[t‘] 1⁄2[t∫ ‘]q[q] Rounded [kw], [gw], [qw] Fricatives Voiceless f[f] s[s] ][∫ ] h[h] Voiced z[z] [ ] Glottalized O[s‘] Rounded [hw] Nasals Voiced m[m]n[n] }[ ] Liquids Voiced l[l], r[r] Semi vowelsVoiced w[w] y[j]",
      "abstract": "• has rich morphology -> many word forms. Phonetics Amharic has a set of 38 phones, seven vowels and thirty-one consonants. Consonants Manner Voicing Place of Articulation of Art/n Lab Dent Pal Vel Glo Stops Voiceless p[p] t[t] m[t∫ ] k[k] [?] Voiced b[b] d[d] ¥[d ] g[g] GlottalizedI[p‘] μ[t‘] 1⁄2[t∫ ‘]q[q] Rounded [kw], [gw], [qw] Fricatives Voiceless f[f] s[s] ][∫ ] h[h] Voiced z[z] [ ] Glottalized O[s‘] Rounded [hw] Nasals Voiced m[m]n[n] }[ ] Liquids Voiced l[l], r[r] Semi vowelsVoiced w[w] y[j]",
      "doi": "https://doi.org/10.21437/interspeech.2005-467",
      "openalex_id": "https://openalex.org/W2195354",
      "arxiv_id": "",
      "publication_date": "2005-09-04",
      "published": "2005-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on an Amharic LVCSR corpus and phonetic inventory, with no discussion of discrete audio tokens, token generation, or methods to obtain token representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on an Amharic LVCSR corpus and phonetic inventory, with no discussion of discrete audio tokens, token generation, or methods to obtain token representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an Amharic speech corpus and phonetic details, but there is no indication that the study involves the creation or use of discrete audio tokens, vector quantization, or any form of learned discrete token representation for language/sequence modeling; it mainly focuses on corpus and phonetic characteristics, lacking methods or tokenization techniques related to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an Amharic speech corpus and phonetic details, but there is no indication that the study involves the creation or use of discrete audio tokens, vector quantization, or any form of learned discrete token representation for language/sequence modeling; it mainly focuses on corpus and phonetic characteristics, lacking methods or tokenization techniques related to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models",
    "abstract": "Word discovery is the task of extracting words from un-segmented text. In this paper we examine to what extent neu-ral networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.",
    "metadata": {
      "title": "Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models",
      "summary": "Word discovery is the task of extracting words from un-segmented text. In this paper we examine to what extent neu-ral networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.",
      "abstract": "Word discovery is the task of extracting words from un-segmented text. In this paper we examine to what extent neu-ral networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2963819008",
      "arxiv_id": "",
      "publication_date": "2017-12-16",
      "published": "2017-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article concerns word discovery in unwritten languages using encoder-decoder neural MT; it does not discuss discrete audio tokens or tokenization strategies for audio, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article concerns word discovery in unwritten languages using encoder-decoder neural MT; it does not discuss discrete audio tokens or tokenization strategies for audio, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on word discovery from un-segmented text in an unwritten language scenario using encoder-decoder neural models for translation, without involving any discrete audio tokens, vector quantization, or audio signal processing. It does not discuss generating discrete audio token representations or their use in audio language modeling, unimodal or cross-modal generation, nor address the token generation mechanisms or quantization processes required by the inclusion criteria. Therefore, it does not meet the core research contribution related to discrete audio tokens, and so should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on word discovery from un-segmented text in an unwritten language scenario using encoder-decoder neural models for translation, without involving any discrete audio tokens, vector quantization, or audio signal processing. It does not discuss generating discrete audio token representations or their use in audio language modeling, unimodal or cross-modal generation, nor address the token generation mechanisms or quantization processes required by the inclusion criteria. Therefore, it does not meet the core research contribution related to discrete audio tokens, and so should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised word discovery for computational language documentation",
    "abstract": "Language diversity is under considerable pressure: half of the world’s languages could disappear by the end of this century. This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation. Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited. Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools. The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists. This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting. This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available. Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods. We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist. We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure. To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.",
    "metadata": {
      "title": "Unsupervised word discovery for computational language documentation",
      "summary": "Language diversity is under considerable pressure: half of the world’s languages could disappear by the end of this century. This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation. Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited. Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools. The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists. This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting. This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available. Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods. We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist. We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure. To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.",
      "abstract": "Language diversity is under considerable pressure: half of the world’s languages could disappear by the end of this century. This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation. Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited. Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools. The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists. This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting. This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available. Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods. We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist. We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure. To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2985777044",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on unsupervised word discovery/segmentation in very-low-resource speech rather than on designing/discussing discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) for language modeling. It emphasizes linguistic segmentation techniques (grammar-based, Bayesian, attention-based) rather than tokenized audio tokens for downstream models, so it does not meet the inclusion criteria for discrete audio token design.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on unsupervised word discovery/segmentation in very-low-resource speech rather than on designing/discussing discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) for language modeling. It emphasizes linguistic segmentation techniques (grammar-based, Bayesian, attention-based) rather than tokenized audio tokens for downstream models, so it does not meet the inclusion criteria for discrete audio token design.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on unsupervised word discovery for computational language documentation, specifically segmentation methods in low-resource language scenarios, but does not discuss discrete audio tokens generated via vector quantization or related quantization methods, nor the design or use of discrete audio token representations for sequence modeling or generation, which are core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on unsupervised word discovery for computational language documentation, specifically segmentation methods in low-resource language scenarios, but does not discuss discrete audio tokens generated via vector quantization or related quantization methods, nor the design or use of discrete audio token representations for sequence modeling or generation, which are core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS",
    "abstract": "Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler et al., 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.",
    "metadata": {
      "title": "Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS",
      "summary": "Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler et al., 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.",
      "abstract": "Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler et al., 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2173413395",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses forced alignment for language documentation using WebMAUS, not discrete audio token representations or token generation/quantization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses forced alignment for language documentation using WebMAUS, not discrete audio token representations or token generation/quantization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on untrained forced alignment of transcriptions and audio for language documentation, which involves aligning speech and transcription at the word or phoneme level. However, it does not propose nor describe a method to generate discrete audio tokens via vector quantization or similar discretization techniques essential to the inclusion criteria. The work does not discuss the creation or use of discrete token representations for sequence modeling or audio language modeling, and lacks details on token generation processes or evaluation of such tokens, which are core to the inclusion requirements. Therefore, it does not meet the key inclusion criteria of focusing on discrete audio token design or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on untrained forced alignment of transcriptions and audio for language documentation, which involves aligning speech and transcription at the word or phoneme level. However, it does not propose nor describe a method to generate discrete audio tokens via vector quantization or similar discretization techniques essential to the inclusion criteria. The work does not discuss the creation or use of discrete token representations for sequence modeling or audio language modeling, and lacks details on token generation processes or evaluation of such tokens, which are core to the inclusion requirements. Therefore, it does not meet the key inclusion criteria of focusing on discrete audio token design or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit",
    "abstract": "Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using Persephone, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",
    "metadata": {
      "title": "Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit",
      "summary": "Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using Persephone, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",
      "abstract": "Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using Persephone, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2883972335",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on integrating automatic transcription into language documentation workflow and does not address discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on integrating automatic transcription into language documentation workflow and does not address discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on integrating automatic transcription for language documentation using the Persephone toolkit and does not mention the use, generation, or design of discrete audio tokens through vector quantization or related methods; it mainly regards a practical application of speech recognition rather than research on discrete audio token representations or their quantization details, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on integrating automatic transcription for language documentation using the Persephone toolkit and does not mention the use, generation, or design of discrete audio tokens through vector quantization or related methods; it mainly regards a practical application of speech recognition rather than research on discrete audio token representations or their quantization details, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TOWARDS SPEECH TRANSLATION OF NON WRITTEN LANGUAGES",
    "abstract": "A large amount of languages in the world do not have an acknowledged written form. However, for a task like speech to speech translation, the written form of a language may be considered as secondary and it might be possible, under certain conditions, to bypass it. This paper is our first attempt to show that such an approach is possible. We propose a phone-based speech translation approach where translation models are learned on a parallel corpus made of foreign phone sequences and their corresponding English translation. Our experiments show that using our so-called phone-based approach leads almost to the same performance as the baseline approach, while being theoretically applicable to any non written language.",
    "metadata": {
      "title": "TOWARDS SPEECH TRANSLATION OF NON WRITTEN LANGUAGES",
      "summary": "A large amount of languages in the world do not have an acknowledged written form. However, for a task like speech to speech translation, the written form of a language may be considered as secondary and it might be possible, under certain conditions, to bypass it. This paper is our first attempt to show that such an approach is possible. We propose a phone-based speech translation approach where translation models are learned on a parallel corpus made of foreign phone sequences and their corresponding English translation. Our experiments show that using our so-called phone-based approach leads almost to the same performance as the baseline approach, while being theoretically applicable to any non written language.",
      "abstract": "A large amount of languages in the world do not have an acknowledged written form. However, for a task like speech to speech translation, the written form of a language may be considered as secondary and it might be possible, under certain conditions, to bypass it. This paper is our first attempt to show that such an approach is possible. We propose a phone-based speech translation approach where translation models are learned on a parallel corpus made of foreign phone sequences and their corresponding English translation. Our experiments show that using our so-called phone-based approach leads almost to the same performance as the baseline approach, while being theoretically applicable to any non written language.",
      "doi": "https://doi.org/10.1109/slt.2006.326795",
      "openalex_id": "https://openalex.org/W2101281673",
      "arxiv_id": "",
      "publication_date": "2006-01-01",
      "published": "2006-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on a phone-based speech translation approach for non-written languages and does not present discrete audio token design or tokenization methods, which is the core inclusion criterion. It also lacks a clear publication/update date, triggering an additional exclusion criterion. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on a phone-based speech translation approach for non-written languages and does not present discrete audio token design or tokenization methods, which is the core inclusion criterion. It also lacks a clear publication/update date, triggering an additional exclusion criterion. Therefore, it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a phone-based speech translation approach using phone sequences and English translations, but it does not describe producing discrete audio tokens through quantization or neural codecs for use in sequence or language models, nor does it clarify token generation processes or semantic disentanglement as required by the inclusion criteria; thus, it does not meet the core requirements related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a phone-based speech translation approach using phone sequences and English translations, but it does not describe producing discrete audio tokens through quantization or neural codecs for use in sequence or language models, nor does it clarify token generation processes or semantic disentanglement as required by the inclusion criteria; thus, it does not meet the core requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transcription bottleneck of speech corpus exploitation",
    "abstract": "While written corpora can be exploited without any linguistic annotations, speech corpora need at least a basic transcription to be of any use for linguistic research. The basic annotation of speech data usually consists of time-aligned orthographic transcriptions. To answer phonetic or phonological research questions, phonetic transcriptions are needed as well. However, manual annotation is very time-consuming and requires considerable skill and near-native competence. Therefore it can take years of speech corpus compilation and annotation before any analyses can be carried out. In this paper, approaches that address the transcription bottleneck of speech corpus exploitation are presented and discussed, including crowdsourcing the orthographic transcription, automatic phonetic alignment, and query-driven annotation. Currently, query-driven annotation and automatic phonetic alignment are being combined and applied in two speech research projects at the Institut fur Deutsche Sprache (IDS), whereas crowdsourcing the orthographic transcription still awaits implementation.",
    "metadata": {
      "title": "Transcription bottleneck of speech corpus exploitation",
      "summary": "While written corpora can be exploited without any linguistic annotations, speech corpora need at least a basic transcription to be of any use for linguistic research. The basic annotation of speech data usually consists of time-aligned orthographic transcriptions. To answer phonetic or phonological research questions, phonetic transcriptions are needed as well. However, manual annotation is very time-consuming and requires considerable skill and near-native competence. Therefore it can take years of speech corpus compilation and annotation before any analyses can be carried out. In this paper, approaches that address the transcription bottleneck of speech corpus exploitation are presented and discussed, including crowdsourcing the orthographic transcription, automatic phonetic alignment, and query-driven annotation. Currently, query-driven annotation and automatic phonetic alignment are being combined and applied in two speech research projects at the Institut fur Deutsche Sprache (IDS), whereas crowdsourcing the orthographic transcription still awaits implementation.",
      "abstract": "While written corpora can be exploited without any linguistic annotations, speech corpora need at least a basic transcription to be of any use for linguistic research. The basic annotation of speech data usually consists of time-aligned orthographic transcriptions. To answer phonetic or phonological research questions, phonetic transcriptions are needed as well. However, manual annotation is very time-consuming and requires considerable skill and near-native competence. Therefore it can take years of speech corpus compilation and annotation before any analyses can be carried out. In this paper, approaches that address the transcription bottleneck of speech corpus exploitation are presented and discussed, including crowdsourcing the orthographic transcription, automatic phonetic alignment, and query-driven annotation. Currently, query-driven annotation and automatic phonetic alignment are being combined and applied in two speech research projects at the Institut fur Deutsche Sprache (IDS), whereas crowdsourcing the orthographic transcription still awaits implementation.",
      "doi": "",
      "openalex_id": "https://openalex.org/W175497273",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses transcription bottlenecks and annotation approaches for speech corpora (orthographic/phonetic transcriptions, crowdsourcing, and alignment) rather than designing or evaluating discrete audio tokens or token-based representations for language-modeling; it does not meet the inclusion criteria focused on discrete audio tokenization and related methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses transcription bottlenecks and annotation approaches for speech corpora (orthographic/phonetic transcriptions, crowdsourcing, and alignment) rather than designing or evaluating discrete audio tokens or token-based representations for language-modeling; it does not meet the inclusion criteria focused on discrete audio tokenization and related methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses approaches to overcome transcription bottlenecks in speech corpus annotation such as crowdsourcing orthographic transcriptions, automatic phonetic alignment, and query-driven annotation. However, it does not focus on discrete audio token design, vector quantization, or any form of neural audio codec producing discrete tokens as defined by the inclusion criteria. It lacks mention of token generation processes, quantization methods, or use of such tokens in audio language modeling or sequence modeling. Therefore, it does not meet the core inclusion requirements and is more about corpus annotation methodologies rather than discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses approaches to overcome transcription bottlenecks in speech corpus annotation such as crowdsourcing orthographic transcriptions, automatic phonetic alignment, and query-driven annotation. However, it does not focus on discrete audio token design, vector quantization, or any form of neural audio codec producing discrete tokens as defined by the inclusion criteria. It lacks mention of token generation processes, quantization methods, or use of such tokens in audio language modeling or sequence modeling. Therefore, it does not meet the core inclusion requirements and is more about corpus annotation methodologies rather than discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning a Latent Space of Multitrack Measures",
    "abstract": "Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical \"style\". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).",
    "metadata": {
      "title": "Learning a Latent Space of Multitrack Measures",
      "summary": "Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical \"style\". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).",
      "abstract": "Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical \"style\". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).",
      "doi": "https://doi.org/10.48550/arxiv.1806.00195",
      "openalex_id": "https://openalex.org/W2805697608",
      "arxiv_id": "",
      "publication_date": "2018-06-01",
      "published": "2018-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on learning a latent space for multitrack music with chord conditioning, and does not involve discrete audio tokens or tokenization mechanisms, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on learning a latent space for multitrack music with chord conditioning, and does not involve discrete audio tokens or tokenization mechanisms, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning a latent space representation of multitrack music measures to enable music generation and manipulation; however, it does not explicitly mention the use or design of discrete audio tokens, vector quantization, or any form of tokenization process required by the inclusion criteria. There is no indication of producing discrete tokens for sequence modeling or language-model-like processing of audio. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning a latent space representation of multitrack music measures to enable music generation and manipulation; however, it does not explicitly mention the use or design of discrete audio tokens, vector quantization, or any form of tokenization process required by the inclusion criteria. There is no indication of producing discrete tokens for sequence modeling or language-model-like processing of audio. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning to Groove with Inverse Sequence Transformations",
    "abstract": "We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using Seq2Seq and recurrent Variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and models for learning to invert them have real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, including demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).",
    "metadata": {
      "title": "Learning to Groove with Inverse Sequence Transformations",
      "summary": "We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using Seq2Seq and recurrent Variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and models for learning to invert them have real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, including demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).",
      "abstract": "We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using Seq2Seq and recurrent Variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and models for learning to invert them have real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, including demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).",
      "doi": "https://doi.org/10.48550/arxiv.1905.06118",
      "openalex_id": "https://openalex.org/W2946521317",
      "arxiv_id": "",
      "publication_date": "2019-05-14",
      "published": "2019-05-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on Seq2Seq models and inverse transformations for drum performances and dataset creation, with no emphasis on discretized audio token representations or token-based modeling, thus it fails the 'Discrete Audio Tokens' inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on Seq2Seq models and inverse transformations for drum performances and dataset creation, with no emphasis on discretized audio token representations or token-based modeling, thus it fails the 'Discrete Audio Tokens' inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on models translating musical scores into expressive performances using sequence-to-sequence and generative approaches but does not describe the use or generation of discrete audio tokens through vector quantization or similar methods central to the inclusion criteria; it lacks details on tokenization processes essential for audio language modeling or discrete token representation, so it does not meet the core aspect of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on models translating musical scores into expressive performances using sequence-to-sequence and generative approaches but does not describe the use or generation of discrete audio tokens through vector quantization or similar methods central to the inclusion criteria; it lacks details on tokenization processes essential for audio language modeling or discrete token representation, so it does not meet the core aspect of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching",
    "abstract": "Sequences of feature vectors are a natural way of representing temporal data. Given a database of sequences, a fundamental task is to find the database entry which is the most similar to a query. In this thesis, we present learning-based methods for efficiently and accurately comparing sequences in order to facilitate large-scale sequence search. Throughout, we will focus on the problem of matching MIDI files (a digital score format) to a large collection of audio recordings of music. The combination of our proposed approaches enables us to create the largest corpus of paired MIDI files and audio recordings ever assembled. Dynamic time warping (DTW) has proven to be an extremely effective method for both aligning and matching sequences. However, its performance is heavily affected by factors such as the feature representation used and its adjustable parameters. We therefore investigate automatically optimizing DTW-based alignment and matching of MIDI and audio data. Our approach uses Bayesian optimization to tune system design and parameters over a synthetically-created dataset of audio and MIDI pairs. We then perform an exhaustive search over DTW score normalization techniques to find the optimal method for reporting a reliable alignment confidence score, as required in matching tasks. This results in a DTW-based system which is conceptually simple and highly accurate at both alignment and matching. We also verify that this system achieves high performance in a large-scale qualitative evaluation of real-world alignments. Unfortunately, DTW can be far too inefficient for large-scale search when sequences are very long and consist of high-dimensional feature vectors. We therefore propose a method for mapping sequences of continuously-valued feature vectors to downsampled sequences of binary vectors. Our approach involves training a pair of convolutional networks to map paired groups of subsequent feature vectors to a Hamming space where similarity is preserved. Evaluated on the task of matching MIDI files to a large database of audio recordings, we show that this technique enables 99.99\\% of the database to be discarded with a modest false reject rate while only requiring 0.2\\% of the time to compute. Even when sped-up with a more efficient representation, the quadratic complexity of DTW greatly hinders its feasibility for very large-scale search. This cost can be avoided by mapping entire sequences to fixed-length vectors in an embedded space where sequence similarity is approximated by Euclidean distance. To achieve this embedding, we propose a feed-forward attention-based neural network model which can integrate arbitrarily long sequences. We show that this approach can extremely efficiently prune 90\\% of our audio recording database with high confidence. After developing these approaches, we applied them together to the practical task of matching 178,561 unique MIDI files to the Million Song Dataset. The resulting ``Lakh MIDI Dataset'' provides a potential bounty of ground truth information for audio content-based music information retrieval. This can include transcription, meter, lyrics, and high-level musicological features. The reliability of the resulting annotations depends both on the quality of the transcription and the accuracy of the score-to-audio alignment. We therefore establish a baseline of reliability for score-derived information for different content-based MIR tasks. Finally, we discuss potential future uses of our dataset and the learning-based sequence comparison methods we developed.",
    "metadata": {
      "title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching",
      "summary": "Sequences of feature vectors are a natural way of representing temporal data. Given a database of sequences, a fundamental task is to find the database entry which is the most similar to a query. In this thesis, we present learning-based methods for efficiently and accurately comparing sequences in order to facilitate large-scale sequence search. Throughout, we will focus on the problem of matching MIDI files (a digital score format) to a large collection of audio recordings of music. The combination of our proposed approaches enables us to create the largest corpus of paired MIDI files and audio recordings ever assembled. Dynamic time warping (DTW) has proven to be an extremely effective method for both aligning and matching sequences. However, its performance is heavily affected by factors such as the feature representation used and its adjustable parameters. We therefore investigate automatically optimizing DTW-based alignment and matching of MIDI and audio data. Our approach uses Bayesian optimization to tune system design and parameters over a synthetically-created dataset of audio and MIDI pairs. We then perform an exhaustive search over DTW score normalization techniques to find the optimal method for reporting a reliable alignment confidence score, as required in matching tasks. This results in a DTW-based system which is conceptually simple and highly accurate at both alignment and matching. We also verify that this system achieves high performance in a large-scale qualitative evaluation of real-world alignments. Unfortunately, DTW can be far too inefficient for large-scale search when sequences are very long and consist of high-dimensional feature vectors. We therefore propose a method for mapping sequences of continuously-valued feature vectors to downsampled sequences of binary vectors. Our approach involves training a pair of convolutional networks to map paired groups of subsequent feature vectors to a Hamming space where similarity is preserved. Evaluated on the task of matching MIDI files to a large database of audio recordings, we show that this technique enables 99.99\\% of the database to be discarded with a modest false reject rate while only requiring 0.2\\% of the time to compute. Even when sped-up with a more efficient representation, the quadratic complexity of DTW greatly hinders its feasibility for very large-scale search. This cost can be avoided by mapping entire sequences to fixed-length vectors in an embedded space where sequence similarity is approximated by Euclidean distance. To achieve this embedding, we propose a feed-forward attention-based neural network model which can integrate arbitrarily long sequences. We show that this approach can extremely efficiently prune 90\\% of our audio recording database with high confidence. After developing these approaches, we applied them together to the practical task of matching 178,561 unique MIDI files to the Million Song Dataset. The resulting ``Lakh MIDI Dataset'' provides a potential bounty of ground truth information for audio content-based music information retrieval. This can include transcription, meter, lyrics, and high-level musicological features. The reliability of the resulting annotations depends both on the quality of the transcription and the accuracy of the score-to-audio alignment. We therefore establish a baseline of reliability for score-derived information for different content-based MIR tasks. Finally, we discuss potential future uses of our dataset and the learning-based sequence comparison methods we developed.",
      "abstract": "Sequences of feature vectors are a natural way of representing temporal data. Given a database of sequences, a fundamental task is to find the database entry which is the most similar to a query. In this thesis, we present learning-based methods for efficiently and accurately comparing sequences in order to facilitate large-scale sequence search. Throughout, we will focus on the problem of matching MIDI files (a digital score format) to a large collection of audio recordings of music. The combination of our proposed approaches enables us to create the largest corpus of paired MIDI files and audio recordings ever assembled. Dynamic time warping (DTW) has proven to be an extremely effective method for both aligning and matching sequences. However, its performance is heavily affected by factors such as the feature representation used and its adjustable parameters. We therefore investigate automatically optimizing DTW-based alignment and matching of MIDI and audio data. Our approach uses Bayesian optimization to tune system design and parameters over a synthetically-created dataset of audio and MIDI pairs. We then perform an exhaustive search over DTW score normalization techniques to find the optimal method for reporting a reliable alignment confidence score, as required in matching tasks. This results in a DTW-based system which is conceptually simple and highly accurate at both alignment and matching. We also verify that this system achieves high performance in a large-scale qualitative evaluation of real-world alignments. Unfortunately, DTW can be far too inefficient for large-scale search when sequences are very long and consist of high-dimensional feature vectors. We therefore propose a method for mapping sequences of continuously-valued feature vectors to downsampled sequences of binary vectors. Our approach involves training a pair of convolutional networks to map paired groups of subsequent feature vectors to a Hamming space where similarity is preserved. Evaluated on the task of matching MIDI files to a large database of audio recordings, we show that this technique enables 99.99\\% of the database to be discarded with a modest false reject rate while only requiring 0.2\\% of the time to compute. Even when sped-up with a more efficient representation, the quadratic complexity of DTW greatly hinders its feasibility for very large-scale search. This cost can be avoided by mapping entire sequences to fixed-length vectors in an embedded space where sequence similarity is approximated by Euclidean distance. To achieve this embedding, we propose a feed-forward attention-based neural network model which can integrate arbitrarily long sequences. We show that this approach can extremely efficiently prune 90\\% of our audio recording database with high confidence. After developing these approaches, we applied them together to the practical task of matching 178,561 unique MIDI files to the Million Song Dataset. The resulting ``Lakh MIDI Dataset'' provides a potential bounty of ground truth information for audio content-based music information retrieval. This can include transcription, meter, lyrics, and high-level musicological features. The reliability of the resulting annotations depends both on the quality of the transcription and the accuracy of the score-to-audio alignment. We therefore establish a baseline of reliability for score-derived information for different content-based MIR tasks. Finally, we discuss potential future uses of our dataset and the learning-based sequence comparison methods we developed.",
      "doi": "https://doi.org/10.7916/d8n58mhv",
      "openalex_id": "https://openalex.org/W2475687244",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes mapping continuous audio/midi sequences to discrete representations (downsampled binary vectors in Hamming space and discrete embeddings) for efficient sequence comparison, which aligns with the discrete audio token paradigm and provides sufficient methodological detail beyond demos.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes mapping continuous audio/midi sequences to discrete representations (downsampled binary vectors in Hamming space and discrete embeddings) for efficient sequence comparison, which aligns with the discrete audio token paradigm and provides sufficient methodological detail beyond demos.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning-based methods for comparing sequences like audio and MIDI alignment, emphasizing dynamic time warping and neural network embeddings, but it does not discuss the generation of discrete audio tokens via vector quantization or similar discrete tokenization mechanisms, nor does it focus on token design for audio language modeling or sequence modeling as defined in the inclusion criteria. Hence, it does not meet the core requirement of discrete audio token representation and usage for language models or sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning-based methods for comparing sequences like audio and MIDI alignment, emphasizing dynamic time warping and neural network embeddings, but it does not discuss the generation of discrete audio tokens via vector quantization or similar discrete tokenization mechanisms, nor does it focus on token design for audio language modeling or sequence modeling as defined in the inclusion criteria. Hence, it does not meet the core requirement of discrete audio token representation and usage for language models or sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano argued inclusion because the paper maps continuous feature sequences to downsampled binary vectors (Hamming space) and fixed-length embeddings, which they interpreted as discrete representations; JuniorMini countered that the work does not describe vector-quantization/codebook-style token generation nor target use of tokens in language/sequence models. After review, I agree with JuniorMini: although the thesis uses discrete-like binary hashing and embeddings for retrieval/alignment, it does not present a VQ/RVQ/hierarchical or semantic quantization pipeline nor frame the outputs as discrete audio tokens for language-model-style sequence modeling, which are required by the inclusion criteria. Therefore it should be excluded. 1-sentence explanation: The work focuses on retrieval/alignment and binary hashing/embeddings rather than producing VQ-like discrete audio tokens intended for audio language modeling or sequence-model use, so it fails the core inclusion requirement.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorNano argued inclusion because the paper maps continuous feature sequences to downsampled binary vectors (Hamming space) and fixed-length embeddings, which they interpreted as discrete representations; JuniorMini countered that the work does not describe vector-quantization/codebook-style token generation nor target use of tokens in language/sequence models. After review, I agree with JuniorMini: although the thesis uses discrete-like binary hashing and embeddings for retrieval/alignment, it does not present a VQ/RVQ/hierarchical or semantic quantization pipeline nor frame the outputs as discrete audio tokens for language-model-style sequence modeling, which are required by the inclusion criteria. Therefore it should be excluded. 1-sentence explanation: The work focuses on retrieval/alignment and binary hashing/embeddings rather than producing VQ-like discrete audio tokens intended for audio language modeling or sequence-model use, so it fails the core inclusion requirement.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
    "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
    "metadata": {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "summary": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
      "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
      "doi": "https://doi.org/10.1109/iccv.2015.123",
      "openalex_id": "https://openalex.org/W1677182931",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses rectified activation units (PReLU) and initialization for deep CNNs on ImageNet; it does not address discrete audio tokens, VQ/RVQ, or audio-language modeling. Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses rectified activation units (PReLU) and initialization for deep CNNs on ImageNet; it does not address discrete audio tokens, VQ/RVQ, or audio-language modeling. Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on advancements in rectifier activation units for image classification on ImageNet, a computer vision task, with no mention of audio signals, discrete audio tokens, vector quantization, or any related audio processing techniques; thus, they do not meet the inclusion criteria related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on advancements in rectifier activation units for image classification on ImageNet, a computer vision task, with no mention of audio signals, discrete audio tokens, vector quantization, or any related audio processing techniques; thus, they do not meet the inclusion criteria related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Learning for Multi-label Classification",
    "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature",
    "metadata": {
      "title": "Deep Learning for Multi-label Classification",
      "summary": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature",
      "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature",
      "doi": "https://doi.org/10.48550/arxiv.1502.05988",
      "openalex_id": "https://openalex.org/W1884029234",
      "arxiv_id": "",
      "publication_date": "2014-12-17",
      "published": "2014-12-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses multi-label classification with deep learning and restricted Boltzmann machines, not discrete audio tokens or tokenization methods; it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses multi-label classification with deep learning and restricted Boltzmann machines, not discrete audio tokens or tokenization methods; it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on deep learning for multi-label classification without any discussion or indication of discrete audio tokens, vector quantization, or audio-related sequence modeling. There is no mention of audio signal processing, token generation, or any related discrete token design, which are central to the inclusion criteria. Thus, the study does not meet the inclusion criteria related to discrete audio token representation and does not cover the core theme required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on deep learning for multi-label classification without any discussion or indication of discrete audio tokens, vector quantization, or audio-related sequence modeling. There is no mention of audio signal processing, token generation, or any related discrete token design, which are central to the inclusion criteria. Thus, the study does not meet the inclusion criteria related to discrete audio token representation and does not cover the core theme required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  }
]