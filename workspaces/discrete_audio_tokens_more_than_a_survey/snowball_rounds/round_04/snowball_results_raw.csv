openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4415039223,https://doi.org/10.1007/s41314-025-00078-1,Enhancing Hindi–English Direct Speech-to-Speech Translation with Clustering-Aided Cross-Contrastive Self-Supervised Speech Representation Learning,,"['https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W4404035483', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W4287072252', 'https://openalex.org/W4287854499', 'https://openalex.org/W4296070387', 'https://openalex.org/W4319862416', 'https://openalex.org/W4280601369', 'https://openalex.org/W4372349107', 'https://openalex.org/W4404781583', 'https://openalex.org/W4392884616', 'https://openalex.org/W4281492411', 'https://openalex.org/W3034999214', 'https://openalex.org/W3001434439', 'https://openalex.org/W4385569956', 'https://openalex.org/W2963532001', 'https://openalex.org/W2798685342']",2025-10-10
https://openalex.org/W4281492411,https://doi.org/10.1109/jstsp.2022.3207050,Self-Supervised Speech Representation Learning: A Review,"Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n","['https://openalex.org/W2919115771', 'https://openalex.org/W2160815625', 'https://openalex.org/W811578723', 'https://openalex.org/W1555037511', 'https://openalex.org/W1975113979', 'https://openalex.org/W2110073835', 'https://openalex.org/W2124537004', 'https://openalex.org/W6683825394', 'https://openalex.org/W2163922914', 'https://openalex.org/W1901616594', 'https://openalex.org/W4244017338', 'https://openalex.org/W4239390603', 'https://openalex.org/W6675401909', 'https://openalex.org/W1902027874', 'https://openalex.org/W2100495367', 'https://openalex.org/W6800751262', 'https://openalex.org/W3207924272', 'https://openalex.org/W3035725276', 'https://openalex.org/W6773996589', 'https://openalex.org/W3185341429', 'https://openalex.org/W6784023748', 'https://openalex.org/W3011574394', 'https://openalex.org/W3023371261', 'https://openalex.org/W6811170316', 'https://openalex.org/W6772230580', 'https://openalex.org/W1703050006', 'https://openalex.org/W2048648518', 'https://openalex.org/W2100969003', 'https://openalex.org/W1979447841', 'https://openalex.org/W1877570817', 'https://openalex.org/W6680522077', 'https://openalex.org/W1487784522', 'https://openalex.org/W2155230809', 'https://openalex.org/W2150769028', 'https://openalex.org/W2408021097', 'https://openalex.org/W6681096077', 'https://openalex.org/W6680106237', 'https://openalex.org/W2145889472', 'https://openalex.org/W2113606819', 'https://openalex.org/W2067474491', 'https://openalex.org/W6736430770', 'https://openalex.org/W2116064496', 'https://openalex.org/W2923014074', 'https://openalex.org/W3197580070', 'https://openalex.org/W2326925005', 'https://openalex.org/W2883725317', 'https://openalex.org/W343636949', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818', 'https://openalex.org/W3217536461', 'https://openalex.org/W2962739339', 'https://openalex.org/W6767997687', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W6844194202', 'https://openalex.org/W3035524453', 'https://openalex.org/W6774670964', 'https://openalex.org/W6779997284', 'https://openalex.org/W2962907457', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W6811088048', 'https://openalex.org/W2913340405', 'https://openalex.org/W2291975472', 'https://openalex.org/W3112702554', 'https://openalex.org/W4226380987', 'https://openalex.org/W2973157397', 'https://openalex.org/W6617744952', 'https://openalex.org/W6712395597', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W6757193177', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198217962', 'https://openalex.org/W6790356757', 'https://openalex.org/W6690026940', 'https://openalex.org/W2097012520', 'https://openalex.org/W1945356021', 'https://openalex.org/W3100270690', 'https://openalex.org/W6729448088', 'https://openalex.org/W2972867623', 'https://openalex.org/W2035424729', 'https://openalex.org/W2020607164', 'https://openalex.org/W2396043527', 'https://openalex.org/W1545920196', 'https://openalex.org/W1796128977', 'https://openalex.org/W2932675979', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W2020883660', 'https://openalex.org/W2146444479', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769238691', 'https://openalex.org/W6778265221', 'https://openalex.org/W3003875258', 'https://openalex.org/W3160345865', 'https://openalex.org/W3196919915', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W3096485810', 'https://openalex.org/W3196798358', 'https://openalex.org/W6674330103', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963425185', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W2982039329', 'https://openalex.org/W2963571336', 'https://openalex.org/W3148040514', 'https://openalex.org/W2963317665', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197411683', 'https://openalex.org/W4226033575', 'https://openalex.org/W3198608154', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6810673746', 'https://openalex.org/W6677884823', 'https://openalex.org/W6682948231', 'https://openalex.org/W2124509324', 'https://openalex.org/W6762931180', 'https://openalex.org/W3159481202', 'https://openalex.org/W1536680647', 'https://openalex.org/W6766978945', 'https://openalex.org/W6600971220', 'https://openalex.org/W2096391593', 'https://openalex.org/W1974783905', 'https://openalex.org/W2091863306', 'https://openalex.org/W4237938692', 'https://openalex.org/W142945732', 'https://openalex.org/W2594690981', 'https://openalex.org/W6810168380', 'https://openalex.org/W2016538560', 'https://openalex.org/W4396724964', 'https://openalex.org/W2296654356', 'https://openalex.org/W6686207219', 'https://openalex.org/W900447646', 'https://openalex.org/W6606244218', 'https://openalex.org/W4237723258', 'https://openalex.org/W6631216910', 'https://openalex.org/W6639103823', 'https://openalex.org/W6728094556', 'https://openalex.org/W6693697572', 'https://openalex.org/W1484147505', 'https://openalex.org/W2130055251', 'https://openalex.org/W2049252044', 'https://openalex.org/W6678885645', 'https://openalex.org/W1531883353', 'https://openalex.org/W6633682082', 'https://openalex.org/W2136189984', 'https://openalex.org/W4297841641', 'https://openalex.org/W3157861865', 'https://openalex.org/W6864391120', 'https://openalex.org/W6729977899', 'https://openalex.org/W2962862718', 'https://openalex.org/W2971709506', 'https://openalex.org/W3197828817', 'https://openalex.org/W3200287550', 'https://openalex.org/W2988907666', 'https://openalex.org/W3196698946', 'https://openalex.org/W3205715971', 'https://openalex.org/W6809593508', 'https://openalex.org/W6770596778', 'https://openalex.org/W2586148577', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963902314', 'https://openalex.org/W4224875474', 'https://openalex.org/W3095293218', 'https://openalex.org/W2963330681', 'https://openalex.org/W2920166246', 'https://openalex.org/W2895651543', 'https://openalex.org/W2973135958', 'https://openalex.org/W6803092890', 'https://openalex.org/W1577418252', 'https://openalex.org/W1496120315', 'https://openalex.org/W2962980711', 'https://openalex.org/W2964169922', 'https://openalex.org/W6720204814', 'https://openalex.org/W2296681920', 'https://openalex.org/W2059652594', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963720603', 'https://openalex.org/W6786885278', 'https://openalex.org/W2407151108', 'https://openalex.org/W2190506272', 'https://openalex.org/W3037530970', 'https://openalex.org/W3201254286', 'https://openalex.org/W3150635893', 'https://openalex.org/W2995181338', 'https://openalex.org/W2593116425', 'https://openalex.org/W4289665794', 'https://openalex.org/W6603931906', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W6771467084', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W6696449567', 'https://openalex.org/W3213029956', 'https://openalex.org/W3206252155', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6691509046', 'https://openalex.org/W2166637769', 'https://openalex.org/W3139878283', 'https://openalex.org/W1526236009', 'https://openalex.org/W2963242190', 'https://openalex.org/W2963127222', 'https://openalex.org/W2884797218', 'https://openalex.org/W6712941328', 'https://openalex.org/W2883409523', 'https://openalex.org/W6936113694', 'https://openalex.org/W2726515241', 'https://openalex.org/W2972584841', 'https://openalex.org/W1567520911', 'https://openalex.org/W6748215858', 'https://openalex.org/W3196509775', 'https://openalex.org/W2094544353', 'https://openalex.org/W6727418883', 'https://openalex.org/W6712757354', 'https://openalex.org/W6731521493', 'https://openalex.org/W6688816777', 'https://openalex.org/W2883595988', 'https://openalex.org/W6750665317', 'https://openalex.org/W2775794021', 'https://openalex.org/W6736723571', 'https://openalex.org/W2972894903', 'https://openalex.org/W3033038061', 'https://openalex.org/W942963634', 'https://openalex.org/W3197223534', 'https://openalex.org/W6784614252', 'https://openalex.org/W6753575415', 'https://openalex.org/W3160554450', 'https://openalex.org/W6803378298', 'https://openalex.org/W3189296823', 'https://openalex.org/W3093096176', 'https://openalex.org/W6809947431', 'https://openalex.org/W3006926732', 'https://openalex.org/W4225713393', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2251253014', 'https://openalex.org/W6795952400', 'https://openalex.org/W2970820321', 'https://openalex.org/W6801828775', 'https://openalex.org/W3198815374', 'https://openalex.org/W3024182269', 'https://openalex.org/W4224934179', 'https://openalex.org/W3096017728', 'https://openalex.org/W3162133897', 'https://openalex.org/W6784614126', 'https://openalex.org/W2786608204', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198771897', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962799225', 'https://openalex.org/W2802557066', 'https://openalex.org/W6735913928', 'https://openalex.org/W6751433836', 'https://openalex.org/W6744957266', 'https://openalex.org/W2899134946', 'https://openalex.org/W6750365303', 'https://openalex.org/W6757699909', 'https://openalex.org/W2962799131', 'https://openalex.org/W6738077056', 'https://openalex.org/W6760519848', 'https://openalex.org/W3214697273', 'https://openalex.org/W6678282225', 'https://openalex.org/W4319862670', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W3204917342', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3015280134', 'https://openalex.org/W2963796886', 'https://openalex.org/W2963581463', 'https://openalex.org/W2972889948', 'https://openalex.org/W2119717200', 'https://openalex.org/W2046932483', 'https://openalex.org/W6640059789', 'https://openalex.org/W2577366047', 'https://openalex.org/W2964243274', 'https://openalex.org/W6756385397', 'https://openalex.org/W2940200615', 'https://openalex.org/W3008480565', 'https://openalex.org/W3024464021', 'https://openalex.org/W3016008406', 'https://openalex.org/W2883586237', 'https://openalex.org/W2964012862', 'https://openalex.org/W3097632072', 'https://openalex.org/W2963216553', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6786696081', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W2972374322', 'https://openalex.org/W3197381195', 'https://openalex.org/W6803066952', 'https://openalex.org/W4307680525', 'https://openalex.org/W4221146627', 'https://openalex.org/W6805530253', 'https://openalex.org/W6777028661', 'https://openalex.org/W4287854499', 'https://openalex.org/W6759579507', 'https://openalex.org/W3176828726', 'https://openalex.org/W6787411158', 'https://openalex.org/W4225274946', 'https://openalex.org/W4226162428', 'https://openalex.org/W6796551075', 'https://openalex.org/W3203140070', 'https://openalex.org/W6685943813', 'https://openalex.org/W3137147200', 'https://openalex.org/W3085139254', 'https://openalex.org/W3198039885', 'https://openalex.org/W6839738141', 'https://openalex.org/W6803547063', 'https://openalex.org/W4297841871', 'https://openalex.org/W3214576767', 'https://openalex.org/W4296068785', 'https://openalex.org/W3209376089', 'https://openalex.org/W4221140371', 'https://openalex.org/W4292825791', 'https://openalex.org/W3101648800', 'https://openalex.org/W4200635400', 'https://openalex.org/W3088409176', 'https://openalex.org/W4286918540', 'https://openalex.org/W2102409316', 'https://openalex.org/W3211224152', 'https://openalex.org/W4288348042', 'https://openalex.org/W2981991061', 'https://openalex.org/W2998249245', 'https://openalex.org/W3009561768', 'https://openalex.org/W2242818861', 'https://openalex.org/W2997574889', 'https://openalex.org/W1515020792', 'https://openalex.org/W2965373594', 'https://openalex.org/W2530846021', 'https://openalex.org/W4297808394', 'https://openalex.org/W1915251500', 'https://openalex.org/W4295116917', 'https://openalex.org/W2964303773', 'https://openalex.org/W3026842484', 'https://openalex.org/W2125290066', 'https://openalex.org/W2219249508', 'https://openalex.org/W2095705004', 'https://openalex.org/W22517275', 'https://openalex.org/W4221145109', 'https://openalex.org/W4287591426', 'https://openalex.org/W2898727538', 'https://openalex.org/W2797583228', 'https://openalex.org/W3195577433', 'https://openalex.org/W3099142230', 'https://openalex.org/W3107298252', 'https://openalex.org/W3213873715', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289750118', 'https://openalex.org/W4394671563', 'https://openalex.org/W3207222250', 'https://openalex.org/W3024605872', 'https://openalex.org/W3161411634', 'https://openalex.org/W2973727699', 'https://openalex.org/W4320013820', 'https://openalex.org/W1508165687', 'https://openalex.org/W2138204974']",2022-09-15
https://openalex.org/W4287854499,https://doi.org/10.18653/v1/2022.naacl-main.63,Textless Speech-to-Speech Translation on Real Data,"Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3092424727', 'https://openalex.org/W2514828952', 'https://openalex.org/W1494198834', 'https://openalex.org/W3142316150', 'https://openalex.org/W3175301726', 'https://openalex.org/W2972495969', 'https://openalex.org/W2963979492', 'https://openalex.org/W3015698636', 'https://openalex.org/W4301980136', 'https://openalex.org/W4287079508', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963609956', 'https://openalex.org/W620750443', 'https://openalex.org/W2946200149', 'https://openalex.org/W3213873715', 'https://openalex.org/W2903739847', 'https://openalex.org/W3140429000', 'https://openalex.org/W3134644026', 'https://openalex.org/W2808631503', 'https://openalex.org/W3016160783', 'https://openalex.org/W3186843219', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963799213', 'https://openalex.org/W3213018012', 'https://openalex.org/W3118578889', 'https://openalex.org/W2972466499', 'https://openalex.org/W2964243274', 'https://openalex.org/W3033411150', 'https://openalex.org/W2972802841', 'https://openalex.org/W4286984129', 'https://openalex.org/W3043665049', 'https://openalex.org/W2949328740', 'https://openalex.org/W2046056978', 'https://openalex.org/W3092028330', 'https://openalex.org/W3119308075', 'https://openalex.org/W3175871055', 'https://openalex.org/W3169320628', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963532001', 'https://openalex.org/W3201257124', 'https://openalex.org/W2136545725', 'https://openalex.org/W2619368999', 'https://openalex.org/W4385245566', 'https://openalex.org/W3030437843', 'https://openalex.org/W4394671563']",2022-01-01
https://openalex.org/W4221147462,https://doi.org/10.1109/taffc.2022.3175578,Emotion Intensity and its Control for Emotional Voice Conversion,"Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n","['https://openalex.org/W4205742757', 'https://openalex.org/W388732865', 'https://openalex.org/W2190260761', 'https://openalex.org/W3092628071', 'https://openalex.org/W3098557217', 'https://openalex.org/W2493477844', 'https://openalex.org/W2123003832', 'https://openalex.org/W2111090013', 'https://openalex.org/W2478838513', 'https://openalex.org/W1963640919', 'https://openalex.org/W6639399909', 'https://openalex.org/W2161736993', 'https://openalex.org/W2007541064', 'https://openalex.org/W4251603968', 'https://openalex.org/W4254718357', 'https://openalex.org/W3095701549', 'https://openalex.org/W3193748859', 'https://openalex.org/W3015841875', 'https://openalex.org/W3136699727', 'https://openalex.org/W2077801020', 'https://openalex.org/W1588037970', 'https://openalex.org/W2040587156', 'https://openalex.org/W2148846882', 'https://openalex.org/W2793479148', 'https://openalex.org/W2748654097', 'https://openalex.org/W2517513811', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W3015241559', 'https://openalex.org/W3096939667', 'https://openalex.org/W3142644187', 'https://openalex.org/W2899361462', 'https://openalex.org/W3097962967', 'https://openalex.org/W3015805741', 'https://openalex.org/W6756197946', 'https://openalex.org/W2963609956', 'https://openalex.org/W2897353073', 'https://openalex.org/W2899877258', 'https://openalex.org/W3113687514', 'https://openalex.org/W6803066952', 'https://openalex.org/W2938833595', 'https://openalex.org/W3015719316', 'https://openalex.org/W2043301392', 'https://openalex.org/W1966797434', 'https://openalex.org/W2005885879', 'https://openalex.org/W2885005742', 'https://openalex.org/W3163573274', 'https://openalex.org/W3168542456', 'https://openalex.org/W2105482032', 'https://openalex.org/W3034420534', 'https://openalex.org/W3162517041', 'https://openalex.org/W6793624898', 'https://openalex.org/W3118753411', 'https://openalex.org/W3083423753', 'https://openalex.org/W2996414377', 'https://openalex.org/W2901254300', 'https://openalex.org/W2150556990', 'https://openalex.org/W3197993066', 'https://openalex.org/W1969386661', 'https://openalex.org/W1978136968', 'https://openalex.org/W2759925408', 'https://openalex.org/W1492383498', 'https://openalex.org/W2745595539', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W2964138190', 'https://openalex.org/W2973158936', 'https://openalex.org/W3197704090', 'https://openalex.org/W3152136404', 'https://openalex.org/W6640963894', 'https://openalex.org/W2904459034', 'https://openalex.org/W6762643587', 'https://openalex.org/W1973378890', 'https://openalex.org/W2330979245', 'https://openalex.org/W2800822639', 'https://openalex.org/W3008691130', 'https://openalex.org/W3146550708', 'https://openalex.org/W2990881710', 'https://openalex.org/W2342475039', 'https://openalex.org/W2146334809', 'https://openalex.org/W2149628368', 'https://openalex.org/W2937154351', 'https://openalex.org/W3014201970', 'https://openalex.org/W3199964822', 'https://openalex.org/W3096830101', 'https://openalex.org/W2803098682', 'https://openalex.org/W3198791321', 'https://openalex.org/W3135644023', 'https://openalex.org/W3160329778', 'https://openalex.org/W2002875620', 'https://openalex.org/W6678800043', 'https://openalex.org/W6683338658', 'https://openalex.org/W6783596713', 'https://openalex.org/W2294130536', 'https://openalex.org/W2033365921', 'https://openalex.org/W2042328763', 'https://openalex.org/W2157336702', 'https://openalex.org/W2147898188', 'https://openalex.org/W3095669918', 'https://openalex.org/W6603838645', 'https://openalex.org/W6917585676', 'https://openalex.org/W6843673214', 'https://openalex.org/W2108323654', 'https://openalex.org/W4231336072', 'https://openalex.org/W4246198815', 'https://openalex.org/W2113586398', 'https://openalex.org/W6680970901', 'https://openalex.org/W2966387353', 'https://openalex.org/W2471520273', 'https://openalex.org/W3015338123', 'https://openalex.org/W3046998876', 'https://openalex.org/W2085662862', 'https://openalex.org/W1501669607', 'https://openalex.org/W2785960144', 'https://openalex.org/W3015249983', 'https://openalex.org/W3126625480', 'https://openalex.org/W2032532974', 'https://openalex.org/W2107860279', 'https://openalex.org/W1975163393', 'https://openalex.org/W2963047186', 'https://openalex.org/W2066629760', 'https://openalex.org/W4388323056', 'https://openalex.org/W2007962718', 'https://openalex.org/W2747070883', 'https://openalex.org/W4240592325', 'https://openalex.org/W1976725440', 'https://openalex.org/W2137639365', 'https://openalex.org/W1488156371', 'https://openalex.org/W1959608418', 'https://openalex.org/W567437002', 'https://openalex.org/W1540664512', 'https://openalex.org/W2952269766', 'https://openalex.org/W4287236468', 'https://openalex.org/W2125560515', 'https://openalex.org/W2187089797', 'https://openalex.org/W3099078140', 'https://openalex.org/W2901997113', 'https://openalex.org/W2319660501', 'https://openalex.org/W4232693607', 'https://openalex.org/W3165478005', 'https://openalex.org/W4288097605', 'https://openalex.org/W3194143312', 'https://openalex.org/W3091905774', 'https://openalex.org/W2133564696', 'https://openalex.org/W4244075810', 'https://openalex.org/W4287241477', 'https://openalex.org/W3104132365', 'https://openalex.org/W2608207374', 'https://openalex.org/W3006108364', 'https://openalex.org/W3101689408', 'https://openalex.org/W4294619240', 'https://openalex.org/W2128070398', 'https://openalex.org/W3102905810', 'https://openalex.org/W4232132981', 'https://openalex.org/W95152782', 'https://openalex.org/W3156592906', 'https://openalex.org/W2794490148', 'https://openalex.org/W4213469706', 'https://openalex.org/W4287250434', 'https://openalex.org/W2138615112', 'https://openalex.org/W4289299319', 'https://openalex.org/W1673075472', 'https://openalex.org/W3213873715', 'https://openalex.org/W1881869418', 'https://openalex.org/W4250182547', 'https://openalex.org/W3144792678']",2022-05-19
https://openalex.org/W4307680525,https://doi.org/10.1162/tacl_a_00545,Generative Spoken Dialogue Language Modeling,"Abstract We introduce dGSLM, the first “textless” model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter, and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn taking compared to a text-based cascaded model.1,2","['https://openalex.org/W6772715161', 'https://openalex.org/W6630821413', 'https://openalex.org/W6780218876', 'https://openalex.org/W6811170316', 'https://openalex.org/W6843330092', 'https://openalex.org/W2102722370', 'https://openalex.org/W3015783745', 'https://openalex.org/W3016011332', 'https://openalex.org/W6603931906', 'https://openalex.org/W6791299810', 'https://openalex.org/W1964725106', 'https://openalex.org/W3094393093', 'https://openalex.org/W6805110637', 'https://openalex.org/W3197934793', 'https://openalex.org/W2008741806', 'https://openalex.org/W2001292406', 'https://openalex.org/W6796554684', 'https://openalex.org/W3198771897', 'https://openalex.org/W2995181338', 'https://openalex.org/W3198217962', 'https://openalex.org/W6631190155', 'https://openalex.org/W6798616804', 'https://openalex.org/W6783867762', 'https://openalex.org/W6803066952', 'https://openalex.org/W6767671539', 'https://openalex.org/W6790356757', 'https://openalex.org/W1608166496', 'https://openalex.org/W3034999214', 'https://openalex.org/W6777615688', 'https://openalex.org/W6640842352', 'https://openalex.org/W2982223350', 'https://openalex.org/W2889231094', 'https://openalex.org/W2087762538', 'https://openalex.org/W4220690942', 'https://openalex.org/W6794898703', 'https://openalex.org/W6704752648', 'https://openalex.org/W2933138175', 'https://openalex.org/W3140429000', 'https://openalex.org/W2067097374', 'https://openalex.org/W2160473997', 'https://openalex.org/W3148101939', 'https://openalex.org/W2963747517', 'https://openalex.org/W6776750061', 'https://openalex.org/W4248634141', 'https://openalex.org/W6628172081', 'https://openalex.org/W2161345458', 'https://openalex.org/W2057563799', 'https://openalex.org/W2962784628', 'https://openalex.org/W6729983166', 'https://openalex.org/W4385573862', 'https://openalex.org/W2786387151', 'https://openalex.org/W3112188842', 'https://openalex.org/W2162634167', 'https://openalex.org/W1986450839', 'https://openalex.org/W792814583', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W6635590879', 'https://openalex.org/W2921495256', 'https://openalex.org/W3186804217', 'https://openalex.org/W3197580070', 'https://openalex.org/W6641025798', 'https://openalex.org/W6769690604', 'https://openalex.org/W3134881075', 'https://openalex.org/W3213873715', 'https://openalex.org/W3155584966', 'https://openalex.org/W4394671563', 'https://openalex.org/W2988937804', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287900772', 'https://openalex.org/W3102393842', 'https://openalex.org/W4297809080', 'https://openalex.org/W2963206148', 'https://openalex.org/W4237772098', 'https://openalex.org/W4381786045', 'https://openalex.org/W3161207330', 'https://openalex.org/W4297808394', 'https://openalex.org/W1219741494', 'https://openalex.org/W3186138538', 'https://openalex.org/W4226199158', 'https://openalex.org/W1591706642', 'https://openalex.org/W3027879771', 'https://openalex.org/W2974231335', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4200113972']",2023-01-01
https://openalex.org/W4221166168,https://doi.org/10.21437/interspeech.2022-225,Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus,"Training a text-to-speech (TTS) model requires a large scale text labeled\nspeech corpus, which is troublesome to collect. In this paper, we propose a\ntransfer learning framework for TTS that utilizes a large amount of unlabeled\nspeech dataset for pre-training. By leveraging wav2vec2.0 representation,\nunlabeled speech can highly improve performance, especially in the lack of\nlabeled speech. We also extend the proposed method to zero-shot multi-speaker\nTTS (ZS-TTS). The experimental results verify the effectiveness of the proposed\nmethod in terms of naturalness, intelligibility, and speaker generalization. We\nhighlight that the single speaker TTS model fine-tuned on the only 10 minutes\nof labeled dataset outperforms the other baselines, and the ZS-TTS model\nfine-tuned on the only 30 minutes of single speaker dataset can generate the\nvoice of the arbitrary speaker, by pre-training on unlabeled multi-speaker\nspeech corpus.\n","['https://openalex.org/W2964243274', 'https://openalex.org/W3026874504', 'https://openalex.org/W2194775991', 'https://openalex.org/W2965373594', 'https://openalex.org/W2955425717', 'https://openalex.org/W2896457183', 'https://openalex.org/W3167533889', 'https://openalex.org/W3094502228', 'https://openalex.org/W3196584150', 'https://openalex.org/W4239072543', 'https://openalex.org/W3024869864', 'https://openalex.org/W3033411150', 'https://openalex.org/W2788357188', 'https://openalex.org/W4301371414', 'https://openalex.org/W3092028330', 'https://openalex.org/W1686810756', 'https://openalex.org/W2970597249', 'https://openalex.org/W2060277733', 'https://openalex.org/W4287248095', 'https://openalex.org/W2940703732', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287121924', 'https://openalex.org/W4287173589', 'https://openalex.org/W3213873715', 'https://openalex.org/W3150572638', 'https://openalex.org/W3015826515', 'https://openalex.org/W3169320628', 'https://openalex.org/W3095883095', 'https://openalex.org/W3168527213', 'https://openalex.org/W2946200149', 'https://openalex.org/W3090254849', 'https://openalex.org/W3036601975', 'https://openalex.org/W3198213150', 'https://openalex.org/W3140429000']",2022-09-16
https://openalex.org/W4287887366,https://doi.org/10.18653/v1/2022.naacl-demo.1,textless-lib: a Library for Textless Spoken Language Processing,"Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.","['https://openalex.org/W2964243274', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015338123', 'https://openalex.org/W3148101939', 'https://openalex.org/W3209984917', 'https://openalex.org/W3093096176', 'https://openalex.org/W3033411150', 'https://openalex.org/W2250539671', 'https://openalex.org/W2965373594', 'https://openalex.org/W4226033575', 'https://openalex.org/W4287591426', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4287854499', 'https://openalex.org/W3140429000', 'https://openalex.org/W4286984129', 'https://openalex.org/W2893425640', 'https://openalex.org/W3092028330', 'https://openalex.org/W2752177116', 'https://openalex.org/W2516090925', 'https://openalex.org/W3180374548', 'https://openalex.org/W4292779060', 'https://openalex.org/W4200635400', 'https://openalex.org/W4394671563', 'https://openalex.org/W3197974236', 'https://openalex.org/W2973026522', 'https://openalex.org/W2950018712', 'https://openalex.org/W3141523618', 'https://openalex.org/W4307680525', 'https://openalex.org/W3144810982', 'https://openalex.org/W2963821905', 'https://openalex.org/W3198815374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2799124508', 'https://openalex.org/W2995181338', 'https://openalex.org/W2970006822', 'https://openalex.org/W2896457183', 'https://openalex.org/W3161348170', 'https://openalex.org/W4308349017', 'https://openalex.org/W4286899907', 'https://openalex.org/W4287374065', 'https://openalex.org/W3213873715', 'https://openalex.org/W2962866891', 'https://openalex.org/W2973049979', 'https://openalex.org/W2515741950', 'https://openalex.org/W2963300588']",2022-01-01
https://openalex.org/W4221146627,https://doi.org/10.21437/interspeech.2022-612,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,"Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users.Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts.Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly.Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task.Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult.This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and finetuned by the SQA downstream task.The time intervals of spoken answers can be directly predicted from spoken documents.We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios.We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data.Our code and model will be open-sourced 1 .","['https://openalex.org/W3167533889', 'https://openalex.org/W3153592532', 'https://openalex.org/W3096109555', 'https://openalex.org/W2963446094', 'https://openalex.org/W1494198834', 'https://openalex.org/W2885485938', 'https://openalex.org/W3213234281', 'https://openalex.org/W4226089239', 'https://openalex.org/W3198802556', 'https://openalex.org/W3163162786', 'https://openalex.org/W4394671563', 'https://openalex.org/W3016191377', 'https://openalex.org/W3191850102', 'https://openalex.org/W4287854499', 'https://openalex.org/W3148001440', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963748441', 'https://openalex.org/W3134307371', 'https://openalex.org/W2962904995', 'https://openalex.org/W3140429000', 'https://openalex.org/W3213873715', 'https://openalex.org/W2747874407', 'https://openalex.org/W3104570641', 'https://openalex.org/W2962854302', 'https://openalex.org/W3015468748', 'https://openalex.org/W3197580070', 'https://openalex.org/W3092945658', 'https://openalex.org/W3169320628', 'https://openalex.org/W3180374548', 'https://openalex.org/W3136270197', 'https://openalex.org/W2896457183', 'https://openalex.org/W4286984129']",2022-09-16
https://openalex.org/W4385569716,https://doi.org/10.18653/v1/2023.acl-long.251,Back Translation for Speech-to-text Translation Without Transcripts,"The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the remarkable success of back translation in MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo ST data from monolingual target data. To ease the challenges posed by short-to-long generation and one-to-many mapping, we introduce self-supervised discrete units and achieve back translation by cascading a target-to-unit model and a unit-to-speech model. With our synthetic ST data, we achieve an average boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments show that our method is especially effective in low-resource scenarios.","['https://openalex.org/W4221155340', 'https://openalex.org/W4309129001', 'https://openalex.org/W2933138175', 'https://openalex.org/W3213873715', 'https://openalex.org/W3102811925', 'https://openalex.org/W4385570154', 'https://openalex.org/W3173666333', 'https://openalex.org/W4312056676', 'https://openalex.org/W3196292088', 'https://openalex.org/W222053410', 'https://openalex.org/W2949328740', 'https://openalex.org/W2970279348', 'https://openalex.org/W3128910262', 'https://openalex.org/W3113908264', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963216553', 'https://openalex.org/W3120929527', 'https://openalex.org/W2508809683', 'https://openalex.org/W4385245566', 'https://openalex.org/W3202201199', 'https://openalex.org/W3034571331', 'https://openalex.org/W4287890956', 'https://openalex.org/W4297677272', 'https://openalex.org/W4320085355', 'https://openalex.org/W4312052802', 'https://openalex.org/W2964172053', 'https://openalex.org/W3176711365', 'https://openalex.org/W3176382501', 'https://openalex.org/W3176455679', 'https://openalex.org/W4385570550', 'https://openalex.org/W3180374548', 'https://openalex.org/W3173767661', 'https://openalex.org/W4226120743', 'https://openalex.org/W3103169714', 'https://openalex.org/W4226092197', 'https://openalex.org/W4285158119', 'https://openalex.org/W3205644108', 'https://openalex.org/W4287854499', 'https://openalex.org/W3140429000', 'https://openalex.org/W2997436923', 'https://openalex.org/W4221163209', 'https://openalex.org/W4385571004', 'https://openalex.org/W3209059054', 'https://openalex.org/W3033411150', 'https://openalex.org/W2886095922', 'https://openalex.org/W3113676066', 'https://openalex.org/W1522301498', 'https://openalex.org/W2962788625', 'https://openalex.org/W4285215858', 'https://openalex.org/W2970015022', 'https://openalex.org/W4206028909', 'https://openalex.org/W4372260139', 'https://openalex.org/W3034474651', 'https://openalex.org/W2466918907', 'https://openalex.org/W4223622550', 'https://openalex.org/W3162471442', 'https://openalex.org/W3207222250', 'https://openalex.org/W2964161387', 'https://openalex.org/W3210177631', 'https://openalex.org/W2963532001', 'https://openalex.org/W4385573012', 'https://openalex.org/W3105825505', 'https://openalex.org/W4287854398', 'https://openalex.org/W4281982771', 'https://openalex.org/W3036601975', 'https://openalex.org/W3172698324', 'https://openalex.org/W2889326796', 'https://openalex.org/W2945700568', 'https://openalex.org/W4385571229', 'https://openalex.org/W3006988520', 'https://openalex.org/W3186200218', 'https://openalex.org/W3092028330', 'https://openalex.org/W4311000453', 'https://openalex.org/W3198217962', 'https://openalex.org/W3162037819']",2023-01-01
https://openalex.org/W4389524060,https://doi.org/10.18653/v1/2023.findings-emnlp.541,Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units,"We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.","['https://openalex.org/W2963539064', 'https://openalex.org/W4311000453', 'https://openalex.org/W4287854499', 'https://openalex.org/W2774848319', 'https://openalex.org/W4381786045', 'https://openalex.org/W4394671563', 'https://openalex.org/W4307680525', 'https://openalex.org/W3092028330', 'https://openalex.org/W2589664214', 'https://openalex.org/W4206711328', 'https://openalex.org/W3214606349', 'https://openalex.org/W3141523618', 'https://openalex.org/W4300980117', 'https://openalex.org/W4287887366', 'https://openalex.org/W2294351487', 'https://openalex.org/W2747874407', 'https://openalex.org/W3169739675', 'https://openalex.org/W4225892118', 'https://openalex.org/W4283659485', 'https://openalex.org/W4372266960', 'https://openalex.org/W4303494982', 'https://openalex.org/W4300980462', 'https://openalex.org/W4297412183', 'https://openalex.org/W3126283728', 'https://openalex.org/W2972659941', 'https://openalex.org/W4296070387', 'https://openalex.org/W3169320628', 'https://openalex.org/W2156142001', 'https://openalex.org/W3161695192', 'https://openalex.org/W3163475957', 'https://openalex.org/W2123003832', 'https://openalex.org/W4297841435', 'https://openalex.org/W3034794073', 'https://openalex.org/W3180374548', 'https://openalex.org/W4387247604', 'https://openalex.org/W1494198834', 'https://openalex.org/W3213873715', 'https://openalex.org/W2527729766', 'https://openalex.org/W4378105483', 'https://openalex.org/W3198217962', 'https://openalex.org/W2125101937', 'https://openalex.org/W3140429000', 'https://openalex.org/W3167533889', 'https://openalex.org/W2945478979', 'https://openalex.org/W4308161937', 'https://openalex.org/W4306169273', 'https://openalex.org/W3150271208', 'https://openalex.org/W3024869864']",2023-01-01
https://openalex.org/W4385571911,https://doi.org/10.18653/v1/2023.iwslt-1.46,Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling,International audience,"['https://openalex.org/W2939710050', 'https://openalex.org/W2973049979', 'https://openalex.org/W2972943112', 'https://openalex.org/W4283659485', 'https://openalex.org/W2982223350', 'https://openalex.org/W2395899413', 'https://openalex.org/W3160408143', 'https://openalex.org/W2963532001', 'https://openalex.org/W3209984917', 'https://openalex.org/W3169320628', 'https://openalex.org/W4287079508', 'https://openalex.org/W3209141406', 'https://openalex.org/W3140429000', 'https://openalex.org/W1522301498', 'https://openalex.org/W1647671624', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963300588', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964243274', 'https://openalex.org/W4381786045', 'https://openalex.org/W3213873715', 'https://openalex.org/W3015698636', 'https://openalex.org/W4286984129', 'https://openalex.org/W4287591426', 'https://openalex.org/W3144810982', 'https://openalex.org/W2593116425', 'https://openalex.org/W4297808394', 'https://openalex.org/W3030102521', 'https://openalex.org/W4394671563', 'https://openalex.org/W175951530', 'https://openalex.org/W2786608204', 'https://openalex.org/W3096216486', 'https://openalex.org/W2763188033', 'https://openalex.org/W4287887366', 'https://openalex.org/W3041561163', 'https://openalex.org/W3197580070', 'https://openalex.org/W3036601975', 'https://openalex.org/W4296070387', 'https://openalex.org/W4307680525', 'https://openalex.org/W4287854499']",2023-01-01
https://openalex.org/W4389524018,https://doi.org/10.18653/v1/2023.emnlp-main.182,Generative Spoken Language Model based on continuous word-sized audio tokens,"In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.","['https://openalex.org/W2979476256', 'https://openalex.org/W4221164184', 'https://openalex.org/W4299585995', 'https://openalex.org/W2121227244', 'https://openalex.org/W4385245566', 'https://openalex.org/W4292825791', 'https://openalex.org/W2145410271', 'https://openalex.org/W2963456134', 'https://openalex.org/W4286984129', 'https://openalex.org/W2519091744', 'https://openalex.org/W3092028330', 'https://openalex.org/W2079656678', 'https://openalex.org/W2970006822', 'https://openalex.org/W3148101939', 'https://openalex.org/W4295308567', 'https://openalex.org/W2972374322', 'https://openalex.org/W1614298861', 'https://openalex.org/W3096656254', 'https://openalex.org/W4313182775', 'https://openalex.org/W101045393', 'https://openalex.org/W2346964103', 'https://openalex.org/W2785896739', 'https://openalex.org/W4224875474', 'https://openalex.org/W4381786045', 'https://openalex.org/W2766812927', 'https://openalex.org/W4224308101', 'https://openalex.org/W2913062184', 'https://openalex.org/W4372260156', 'https://openalex.org/W3133702157', 'https://openalex.org/W2777302760', 'https://openalex.org/W4303649106', 'https://openalex.org/W2768381684', 'https://openalex.org/W3140429000', 'https://openalex.org/W2131738223', 'https://openalex.org/W2947445680', 'https://openalex.org/W4214633470', 'https://openalex.org/W3146777637', 'https://openalex.org/W3036601975', 'https://openalex.org/W2126377586', 'https://openalex.org/W4286902103', 'https://openalex.org/W3110761489', 'https://openalex.org/W3197580070', 'https://openalex.org/W1810943226', 'https://openalex.org/W1494198834', 'https://openalex.org/W3215615641', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W3097692357', 'https://openalex.org/W46679369', 'https://openalex.org/W4223486244', 'https://openalex.org/W3213873715', 'https://openalex.org/W4303519914', 'https://openalex.org/W2160473997', 'https://openalex.org/W2152790380', 'https://openalex.org/W2950414763', 'https://openalex.org/W4287887366', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963979492']",2023-01-01
https://openalex.org/W3204049785,https://doi.org/10.1016/j.jrtpm.2021.100265,Multi-class railway complaints categorization using Neural Networks: RailNeural,,"['https://openalex.org/W6768157946', 'https://openalex.org/W3112819431', 'https://openalex.org/W3039988603', 'https://openalex.org/W2800318991', 'https://openalex.org/W6795894136', 'https://openalex.org/W3007551217', 'https://openalex.org/W2397632331', 'https://openalex.org/W6749462855', 'https://openalex.org/W2929376427', 'https://openalex.org/W6789575259', 'https://openalex.org/W624253300', 'https://openalex.org/W6757415957', 'https://openalex.org/W6781423736', 'https://openalex.org/W2969322340', 'https://openalex.org/W2914767245', 'https://openalex.org/W6769307432', 'https://openalex.org/W2975191766', 'https://openalex.org/W6682625732', 'https://openalex.org/W2913579387', 'https://openalex.org/W2891768540', 'https://openalex.org/W6779292933', 'https://openalex.org/W6760192154', 'https://openalex.org/W6794314333', 'https://openalex.org/W6782676586', 'https://openalex.org/W2540382670', 'https://openalex.org/W99444093', 'https://openalex.org/W3166839062', 'https://openalex.org/W4254084718', 'https://openalex.org/W2599674900', 'https://openalex.org/W2913428326', 'https://openalex.org/W3124228036', 'https://openalex.org/W4233731269', 'https://openalex.org/W2618360053', 'https://openalex.org/W3048675253', 'https://openalex.org/W3152642467', 'https://openalex.org/W4246597566', 'https://openalex.org/W4246199710', 'https://openalex.org/W4254894594', 'https://openalex.org/W2790634708', 'https://openalex.org/W2894382158', 'https://openalex.org/W2981052417', 'https://openalex.org/W4285069606', 'https://openalex.org/W2486185411', 'https://openalex.org/W2921841429', 'https://openalex.org/W1516297070', 'https://openalex.org/W2587721921', 'https://openalex.org/W2998924201', 'https://openalex.org/W3031277321', 'https://openalex.org/W3034834827', 'https://openalex.org/W4252991110', 'https://openalex.org/W4299679770', 'https://openalex.org/W3085804837', 'https://openalex.org/W3096301022', 'https://openalex.org/W2904450029', 'https://openalex.org/W2152525996', 'https://openalex.org/W2975195699', 'https://openalex.org/W3014180268', 'https://openalex.org/W4251050432', 'https://openalex.org/W4207055505']",2021-09-24
https://openalex.org/W4386100600,https://doi.org/10.1038/s41586-023-06443-4,A high-performance neuroprosthesis for speech decoding and avatar control,,"['https://openalex.org/W3180220247', 'https://openalex.org/W2091540943', 'https://openalex.org/W4308523455', 'https://openalex.org/W2127141656', 'https://openalex.org/W2766901645', 'https://openalex.org/W2549587034', 'https://openalex.org/W2589952473', 'https://openalex.org/W3161343553', 'https://openalex.org/W2952609044', 'https://openalex.org/W2940585064', 'https://openalex.org/W3209059054', 'https://openalex.org/W4372348980', 'https://openalex.org/W3129009457', 'https://openalex.org/W2963300588', 'https://openalex.org/W2165143604', 'https://openalex.org/W2887348729', 'https://openalex.org/W2014621385', 'https://openalex.org/W2804300206', 'https://openalex.org/W2138672527', 'https://openalex.org/W2610838041', 'https://openalex.org/W2893673238', 'https://openalex.org/W2166970146', 'https://openalex.org/W3034583536', 'https://openalex.org/W3036586088', 'https://openalex.org/W1501600119', 'https://openalex.org/W2960235886', 'https://openalex.org/W2091788365', 'https://openalex.org/W2153180505', 'https://openalex.org/W3012321363', 'https://openalex.org/W2725329339', 'https://openalex.org/W1554341681', 'https://openalex.org/W2230201647', 'https://openalex.org/W2072428342', 'https://openalex.org/W2765847708', 'https://openalex.org/W2791481012', 'https://openalex.org/W1966537851', 'https://openalex.org/W3015415357', 'https://openalex.org/W2160783467', 'https://openalex.org/W2792612906', 'https://openalex.org/W4245267204', 'https://openalex.org/W3003257820', 'https://openalex.org/W2143612262', 'https://openalex.org/W1922655562', 'https://openalex.org/W2970971581', 'https://openalex.org/W2520160253', 'https://openalex.org/W3208743843', 'https://openalex.org/W1934041838', 'https://openalex.org/W1494198834', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W3180374548', 'https://openalex.org/W4200631896', 'https://openalex.org/W4297841848', 'https://openalex.org/W2107860279', 'https://openalex.org/W1588539311', 'https://openalex.org/W2169918686', 'https://openalex.org/W6675354045', 'https://openalex.org/W3150635270', 'https://openalex.org/W2181523240', 'https://openalex.org/W2292251807', 'https://openalex.org/W2765117211', 'https://openalex.org/W4244739505', 'https://openalex.org/W3103145119', 'https://openalex.org/W2571331207']",2023-08-23
https://openalex.org/W4393955733,https://doi.org/10.2139/ssrn.4737265,"Theory Is All You Need: AI, Human Cognition, and Decision Making",,"['https://openalex.org/W2896556344', 'https://openalex.org/W4388846828', 'https://openalex.org/W4385804359', 'https://openalex.org/W2099697766', 'https://openalex.org/W4392165544', 'https://openalex.org/W4310522492', 'https://openalex.org/W4413288418', 'https://openalex.org/W2915136920', 'https://openalex.org/W4388962306', 'https://openalex.org/W4297359407', 'https://openalex.org/W2108767715', 'https://openalex.org/W4366487778', 'https://openalex.org/W2494559574', 'https://openalex.org/W3133702157', 'https://openalex.org/W3176707157', 'https://openalex.org/W6847488925', 'https://openalex.org/W4379933116', 'https://openalex.org/W2913896553', 'https://openalex.org/W3107957832', 'https://openalex.org/W2130457181', 'https://openalex.org/W4389765664', 'https://openalex.org/W2767996707', 'https://openalex.org/W2105748003', 'https://openalex.org/W4391136507', 'https://openalex.org/W2655825240', 'https://openalex.org/W6857372633', 'https://openalex.org/W2760328466', 'https://openalex.org/W3096595992', 'https://openalex.org/W4389861121', 'https://openalex.org/W2129902464', 'https://openalex.org/W2531845968', 'https://openalex.org/W3165496502', 'https://openalex.org/W3013065462', 'https://openalex.org/W2062769132', 'https://openalex.org/W4206921982', 'https://openalex.org/W4403819497', 'https://openalex.org/W1997767101', 'https://openalex.org/W4323528558', 'https://openalex.org/W4225769511', 'https://openalex.org/W2769121085', 'https://openalex.org/W6782354139', 'https://openalex.org/W2555652387', 'https://openalex.org/W6862062360', 'https://openalex.org/W2318645212', 'https://openalex.org/W3127505281', 'https://openalex.org/W4251091592', 'https://openalex.org/W2048231652', 'https://openalex.org/W2096375888', 'https://openalex.org/W6858993230', 'https://openalex.org/W4312636567', 'https://openalex.org/W3124729337', 'https://openalex.org/W2113812196', 'https://openalex.org/W1892018222', 'https://openalex.org/W4395050990', 'https://openalex.org/W2605959375', 'https://openalex.org/W4254499959', 'https://openalex.org/W3108981297', 'https://openalex.org/W3010455538', 'https://openalex.org/W2164646550', 'https://openalex.org/W2167956961', 'https://openalex.org/W220355551', 'https://openalex.org/W2895994941', 'https://openalex.org/W3004619146', 'https://openalex.org/W1982846861', 'https://openalex.org/W1984710451', 'https://openalex.org/W4247883378', 'https://openalex.org/W3010115675', 'https://openalex.org/W2993834601', 'https://openalex.org/W4361019934', 'https://openalex.org/W3177828909', 'https://openalex.org/W2043787485', 'https://openalex.org/W2771080039', 'https://openalex.org/W1986230825', 'https://openalex.org/W4367173429', 'https://openalex.org/W4389396390', 'https://openalex.org/W2132003317', 'https://openalex.org/W2883445328', 'https://openalex.org/W2905500226', 'https://openalex.org/W2163605009', 'https://openalex.org/W3015786209', 'https://openalex.org/W2149893809', 'https://openalex.org/W2335957875', 'https://openalex.org/W2779578326', 'https://openalex.org/W2180809782', 'https://openalex.org/W4387949708', 'https://openalex.org/W3129009457', 'https://openalex.org/W2963513020', 'https://openalex.org/W2919115771', 'https://openalex.org/W2101493843', 'https://openalex.org/W3027879771', 'https://openalex.org/W4392935324', 'https://openalex.org/W4394973191', 'https://openalex.org/W4213417814', 'https://openalex.org/W2138162238', 'https://openalex.org/W2073257493', 'https://openalex.org/W4245748981', 'https://openalex.org/W3213407400', 'https://openalex.org/W4403122979', 'https://openalex.org/W1995341919', 'https://openalex.org/W3010218250', 'https://openalex.org/W2765383588', 'https://openalex.org/W3036555122', 'https://openalex.org/W4390562274', 'https://openalex.org/W4391019441', 'https://openalex.org/W30980520', 'https://openalex.org/W3006166680', 'https://openalex.org/W2008236332', 'https://openalex.org/W4392735105', 'https://openalex.org/W4210444768', 'https://openalex.org/W2040870580', 'https://openalex.org/W2945976633', 'https://openalex.org/W1498436455', 'https://openalex.org/W4285726648', 'https://openalex.org/W2811289501', 'https://openalex.org/W4400949264', 'https://openalex.org/W2148962857', 'https://openalex.org/W2002170215', 'https://openalex.org/W2108360848', 'https://openalex.org/W2178752668', 'https://openalex.org/W2167081754', 'https://openalex.org/W2053555487', 'https://openalex.org/W1129363535', 'https://openalex.org/W2286353276', 'https://openalex.org/W4403173903', 'https://openalex.org/W2001771035', 'https://openalex.org/W6739901393', 'https://openalex.org/W4384023346', 'https://openalex.org/W3082188664', 'https://openalex.org/W3029977317', 'https://openalex.org/W4387966945', 'https://openalex.org/W4220662376', 'https://openalex.org/W4380319195', 'https://openalex.org/W4200634402', 'https://openalex.org/W4392353722', 'https://openalex.org/W4390430249', 'https://openalex.org/W4391766565', 'https://openalex.org/W4212775911', 'https://openalex.org/W4390605317', 'https://openalex.org/W4388481599', 'https://openalex.org/W4384389802', 'https://openalex.org/W4385245566', 'https://openalex.org/W4401924583', 'https://openalex.org/W4388067858', 'https://openalex.org/W2323385789', 'https://openalex.org/W2962681511', 'https://openalex.org/W4404826238', 'https://openalex.org/W4393124223', 'https://openalex.org/W4386910589', 'https://openalex.org/W2949733122', 'https://openalex.org/W2002773046', 'https://openalex.org/W2056245254', 'https://openalex.org/W2093585241', 'https://openalex.org/W4390721566', 'https://openalex.org/W4367692219', 'https://openalex.org/W4233045210', 'https://openalex.org/W4237288401', 'https://openalex.org/W4301914006', 'https://openalex.org/W4394671563', 'https://openalex.org/W1589878313', 'https://openalex.org/W3128519047', 'https://openalex.org/W4396239104', 'https://openalex.org/W3215367316', 'https://openalex.org/W4293718192', 'https://openalex.org/W1664309691', 'https://openalex.org/W4226323416', 'https://openalex.org/W4408929481', 'https://openalex.org/W2994602346', 'https://openalex.org/W4386113651', 'https://openalex.org/W1923807516', 'https://openalex.org/W4403558626', 'https://openalex.org/W4390412899', 'https://openalex.org/W4388000092', 'https://openalex.org/W3036706159', 'https://openalex.org/W4366660864', 'https://openalex.org/W4280614857', 'https://openalex.org/W4387299529', 'https://openalex.org/W2504757423', 'https://openalex.org/W2972314773', 'https://openalex.org/W1546700616', 'https://openalex.org/W317793146', 'https://openalex.org/W4385181053', 'https://openalex.org/W4385447813', 'https://openalex.org/W3174696866', 'https://openalex.org/W2786990444', 'https://openalex.org/W2938704169', 'https://openalex.org/W2326588846', 'https://openalex.org/W1531190209']",2024-01-01
https://openalex.org/W4393160744,https://doi.org/10.1609/aaai.v38i18.29991,SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge,"Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast ""world knowledge"". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.","['https://openalex.org/W6678231568', 'https://openalex.org/W2897513296', 'https://openalex.org/W2896457183', 'https://openalex.org/W4378767246', 'https://openalex.org/W6786039469', 'https://openalex.org/W4221152848', 'https://openalex.org/W2168359464', 'https://openalex.org/W6849548236', 'https://openalex.org/W3129009457', 'https://openalex.org/W2948589675', 'https://openalex.org/W4360819401', 'https://openalex.org/W2799002257', 'https://openalex.org/W2981852735', 'https://openalex.org/W4297161808', 'https://openalex.org/W3198685994', 'https://openalex.org/W3096099141', 'https://openalex.org/W4286336838', 'https://openalex.org/W4388660746', 'https://openalex.org/W4224912544', 'https://openalex.org/W4394671563', 'https://openalex.org/W4389520747', 'https://openalex.org/W3101355526', 'https://openalex.org/W4307934016', 'https://openalex.org/W4378509449', 'https://openalex.org/W4385473486', 'https://openalex.org/W4386251907', 'https://openalex.org/W4322718191', 'https://openalex.org/W4366999541', 'https://openalex.org/W4383108457', 'https://openalex.org/W4285428875', 'https://openalex.org/W4307079201', 'https://openalex.org/W4322825501', 'https://openalex.org/W4312045619', 'https://openalex.org/W4377130677', 'https://openalex.org/W4383097638', 'https://openalex.org/W4320559489', 'https://openalex.org/W4221158689', 'https://openalex.org/W4288089799', 'https://openalex.org/W4321011818', 'https://openalex.org/W2122054842', 'https://openalex.org/W4297808394', 'https://openalex.org/W4323066451', 'https://openalex.org/W4292119927', 'https://openalex.org/W2973379954', 'https://openalex.org/W3210940825']",2024-03-24
https://openalex.org/W4382202703,https://doi.org/10.1609/aaai.v37i11.26488,A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.","['https://openalex.org/W6778883912', 'https://openalex.org/W3169688220', 'https://openalex.org/W6747158283', 'https://openalex.org/W4226132755', 'https://openalex.org/W3111551570', 'https://openalex.org/W6750305986', 'https://openalex.org/W4298277274', 'https://openalex.org/W2982055294', 'https://openalex.org/W6802659129', 'https://openalex.org/W2963981733', 'https://openalex.org/W6761551260', 'https://openalex.org/W1574170747', 'https://openalex.org/W3026874504', 'https://openalex.org/W6631190155', 'https://openalex.org/W3092028330', 'https://openalex.org/W3129009457', 'https://openalex.org/W6757079273', 'https://openalex.org/W3023126978', 'https://openalex.org/W2742542661', 'https://openalex.org/W2981087920', 'https://openalex.org/W3196318247', 'https://openalex.org/W2946200149', 'https://openalex.org/W3091928890', 'https://openalex.org/W2777302760', 'https://openalex.org/W4221145199', 'https://openalex.org/W2794490148', 'https://openalex.org/W6750226204', 'https://openalex.org/W6749954789', 'https://openalex.org/W2998572311', 'https://openalex.org/W6798096123', 'https://openalex.org/W6793472422', 'https://openalex.org/W3116834994', 'https://openalex.org/W3163906773', 'https://openalex.org/W1522301498', 'https://openalex.org/W3180355996', 'https://openalex.org/W4301206121', 'https://openalex.org/W3016160783', 'https://openalex.org/W3178839419', 'https://openalex.org/W3206375275', 'https://openalex.org/W2979476256', 'https://openalex.org/W4287019748', 'https://openalex.org/W4287121924', 'https://openalex.org/W4287120025', 'https://openalex.org/W3151269043', 'https://openalex.org/W2962780374', 'https://openalex.org/W2964243274', 'https://openalex.org/W4292779060', 'https://openalex.org/W2963096510', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972702018', 'https://openalex.org/W4294619417', 'https://openalex.org/W4361994820', 'https://openalex.org/W4394671563', 'https://openalex.org/W2938704169']",2023-06-26
https://openalex.org/W4311481261,https://doi.org/10.31234/osf.io/rx94d,Can statistical learning bootstrap early language acquisition? A modeling investigation,"Before they even produce their first word, infants start developing a language-specific perception, recognize the auditory form of frequent words, and develop a rudimentary knowledge of grammatical categories. A major question in language development is: what mechanisms are responsible for the effortless learning infants demonstrate? In-laboratory experiments have shown that young infants are exquisitely sensitive to fine-grained statistical regularities of their speech input. This has led researchers to propose statistical learning as the cornerstone mechanism of early language acquisition. While the statistical learning account has been influential, the extent to which it can explain early language acquisition is still controversial. Recent computational studies provide evidence in favour of the statistical learning hypothesis for sound learning, but can this result be extended to higher level linguistic categories? Here, we introduce STELA, a developmental and psycholinguistic-inspired computational model that simulates how infants might learn at multiple linguistic levels simultaneously based on statistical analysis of raw audio signals. Our algorithm uses only the raw input without any human annotation, and it is trained to predict future segments of speech based on past ones. It reproduces the pattern of parallel learning across sound and word levels reported in infants: it learns to discriminate sounds, recognizes the auditory form of words, and organizes sounds and words along linguistic dimensions. This suggests that statistical learning from raw speech is sufficient to bootstrap early language acquisition at the sound and word levels.","['https://openalex.org/W2759573091', 'https://openalex.org/W2099164611', 'https://openalex.org/W2995929068', 'https://openalex.org/W3193068792', 'https://openalex.org/W2063303346', 'https://openalex.org/W6679093046', 'https://openalex.org/W122673323', 'https://openalex.org/W6790730029', 'https://openalex.org/W2164274485', 'https://openalex.org/W2032442824', 'https://openalex.org/W4225726571', 'https://openalex.org/W2483390977', 'https://openalex.org/W2037662195', 'https://openalex.org/W2040344088', 'https://openalex.org/W2396361046', 'https://openalex.org/W2126377586', 'https://openalex.org/W4295309037', 'https://openalex.org/W1989914796', 'https://openalex.org/W2149784807', 'https://openalex.org/W2038056950', 'https://openalex.org/W6659778137', 'https://openalex.org/W6635894473', 'https://openalex.org/W6653853028', 'https://openalex.org/W3198217962', 'https://openalex.org/W2103091632', 'https://openalex.org/W2063525438', 'https://openalex.org/W3129009457', 'https://openalex.org/W4220770602', 'https://openalex.org/W2741692265', 'https://openalex.org/W2114156501', 'https://openalex.org/W2074488330', 'https://openalex.org/W2136653392', 'https://openalex.org/W2104752510', 'https://openalex.org/W414516981', 'https://openalex.org/W2120713167', 'https://openalex.org/W6631362777', 'https://openalex.org/W2010188467', 'https://openalex.org/W3005511757', 'https://openalex.org/W1993984071', 'https://openalex.org/W6645640999', 'https://openalex.org/W6738652494', 'https://openalex.org/W2029008609', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W2064135025', 'https://openalex.org/W6683656786', 'https://openalex.org/W2119165475', 'https://openalex.org/W2160464066', 'https://openalex.org/W2108582985', 'https://openalex.org/W2003341094', 'https://openalex.org/W6640227979', 'https://openalex.org/W2153767712', 'https://openalex.org/W6678493852', 'https://openalex.org/W3146245645', 'https://openalex.org/W3144810982', 'https://openalex.org/W2615444509', 'https://openalex.org/W2163097816', 'https://openalex.org/W4254816979', 'https://openalex.org/W4297808394', 'https://openalex.org/W4221038855', 'https://openalex.org/W3030437843', 'https://openalex.org/W4362220304', 'https://openalex.org/W3016181583', 'https://openalex.org/W4235199064', 'https://openalex.org/W4301785137', 'https://openalex.org/W1631260214', 'https://openalex.org/W4286984129', 'https://openalex.org/W3202070718', 'https://openalex.org/W4251221781', 'https://openalex.org/W1597121597', 'https://openalex.org/W2125566341', 'https://openalex.org/W4307680525', 'https://openalex.org/W2134795773', 'https://openalex.org/W2127523122', 'https://openalex.org/W3199093330', 'https://openalex.org/W3095410713', 'https://openalex.org/W4381786045', 'https://openalex.org/W1524333225', 'https://openalex.org/W593365102', 'https://openalex.org/W1980862600', 'https://openalex.org/W2037752261', 'https://openalex.org/W2809193001', 'https://openalex.org/W2933138175', 'https://openalex.org/W4394671563', 'https://openalex.org/W4287591426', 'https://openalex.org/W2402146185', 'https://openalex.org/W2519091744', 'https://openalex.org/W1925965306']",2022-12-13
https://openalex.org/W4403780722,https://doi.org/10.1145/3664647.3681697,Generative Expressive Conversational Speech Synthesis,,"['https://openalex.org/W4392384650', 'https://openalex.org/W2963349408', 'https://openalex.org/W4382202703', 'https://openalex.org/W4308852802', 'https://openalex.org/W4389983267', 'https://openalex.org/W4387969386', 'https://openalex.org/W4392908891', 'https://openalex.org/W3095319910', 'https://openalex.org/W3151309757', 'https://openalex.org/W3209059054', 'https://openalex.org/W4320813209', 'https://openalex.org/W4384615685', 'https://openalex.org/W4391709903', 'https://openalex.org/W3129009457', 'https://openalex.org/W4372260474', 'https://openalex.org/W4225300652', 'https://openalex.org/W4304013787', 'https://openalex.org/W4390306229', 'https://openalex.org/W4392931281', 'https://openalex.org/W4393160807', 'https://openalex.org/W4387968649', 'https://openalex.org/W4226092197', 'https://openalex.org/W4283067649', 'https://openalex.org/W3195592874', 'https://openalex.org/W4378446074', 'https://openalex.org/W3155340931', 'https://openalex.org/W4313679638', 'https://openalex.org/W4387595589', 'https://openalex.org/W4391021666', 'https://openalex.org/W4390528829', 'https://openalex.org/W4375868859', 'https://openalex.org/W4389524500', 'https://openalex.org/W4281397735', 'https://openalex.org/W4394671563']",2024-10-26
https://openalex.org/W4406120141,https://doi.org/10.1038/s41746-024-01417-w,Multimodal deep ensemble classification system with wearable vibration sensor for detecting throat-related events,"Dysphagia, a swallowing disorder, requires continuous monitoring of throat-related events to obtain comprehensive insights into the patient's pharyngeal and laryngeal functions. However, conventional assessments were performed by medical professionals in clinical settings, limiting persistent monitoring. We demonstrate feasibility of a ubiquitous monitoring system for autonomously detecting throat-related events utilizing a soft skin-attachable throat vibration sensor (STVS). The STVS accurately records throat vibrations without interference from surrounding noise, enabling measurement of subtle sounds such as swallowing. Out of the continuous data stream, we automatically classify events of interest using an ensemble-based deep learning model. The proposed model integrates multiple deep neural networks based on multi-modal acoustic features of throat-related events to enhance robustness and accuracy of classification. The performance of our model outperforms previous studies with a classification accuracy of 95.96%. These results show the potential of wearable solutions for improving dysphagia management and patient outcomes outside of clinical environments.","['https://openalex.org/W1967662809', 'https://openalex.org/W2971784612', 'https://openalex.org/W4385889130', 'https://openalex.org/W2188190481', 'https://openalex.org/W2100387032', 'https://openalex.org/W2162800226', 'https://openalex.org/W4385871822', 'https://openalex.org/W2981047412', 'https://openalex.org/W4390012088', 'https://openalex.org/W4296462521', 'https://openalex.org/W4327546431', 'https://openalex.org/W4389035980', 'https://openalex.org/W2990691150', 'https://openalex.org/W2805927228', 'https://openalex.org/W4386835103', 'https://openalex.org/W3161975008', 'https://openalex.org/W2769435864', 'https://openalex.org/W3185940864', 'https://openalex.org/W2891053383', 'https://openalex.org/W2990016708', 'https://openalex.org/W4316022315', 'https://openalex.org/W4400446910', 'https://openalex.org/W3155723493', 'https://openalex.org/W3186065429', 'https://openalex.org/W3161949382', 'https://openalex.org/W4399728170', 'https://openalex.org/W4200482780', 'https://openalex.org/W4200581517', 'https://openalex.org/W4280547844', 'https://openalex.org/W2507380695', 'https://openalex.org/W2151703098', 'https://openalex.org/W3149839747', 'https://openalex.org/W4385823180', 'https://openalex.org/W3129009457', 'https://openalex.org/W2943858543', 'https://openalex.org/W2004412381', 'https://openalex.org/W2791688502', 'https://openalex.org/W2148594799', 'https://openalex.org/W4310172534', 'https://openalex.org/W2194775991', 'https://openalex.org/W4309028162', 'https://openalex.org/W2962858109', 'https://openalex.org/W3166100196', 'https://openalex.org/W28412257', 'https://openalex.org/W2088794999', 'https://openalex.org/W2911964244', 'https://openalex.org/W2295598076', 'https://openalex.org/W2768348081', 'https://openalex.org/W2056132907', 'https://openalex.org/W1968969471', 'https://openalex.org/W2008056655', 'https://openalex.org/W1601133879', 'https://openalex.org/W3088067841', 'https://openalex.org/W4295513942', 'https://openalex.org/W2954996726', 'https://openalex.org/W4226323522', 'https://openalex.org/W3175301885', 'https://openalex.org/W3176923149']",2025-01-07
https://openalex.org/W4362707416,https://doi.org/10.1145/3585088.3589353,"What Do Children and Parents Want and Perceive in Conversational Agents? Towards Transparent, Trustworthy, Democratized Agents","Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of ""ideal future agents"" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.","['https://openalex.org/W2141766660', 'https://openalex.org/W3163442280', 'https://openalex.org/W2886572631', 'https://openalex.org/W6778883912', 'https://openalex.org/W2135363691', 'https://openalex.org/W4288359812', 'https://openalex.org/W3126382704', 'https://openalex.org/W4225082850', 'https://openalex.org/W3130937151', 'https://openalex.org/W3025165719', 'https://openalex.org/W2089358714', 'https://openalex.org/W2898089993', 'https://openalex.org/W2963026768', 'https://openalex.org/W3012624518', 'https://openalex.org/W2008021971', 'https://openalex.org/W3129009457', 'https://openalex.org/W1119960297', 'https://openalex.org/W2948116819', 'https://openalex.org/W1972753335', 'https://openalex.org/W2123534889', 'https://openalex.org/W2084235337', 'https://openalex.org/W2889495041', 'https://openalex.org/W2967206385', 'https://openalex.org/W3185000777', 'https://openalex.org/W1982451263', 'https://openalex.org/W2084944215', 'https://openalex.org/W2955826451', 'https://openalex.org/W3187653187', 'https://openalex.org/W2160779131', 'https://openalex.org/W6964593435', 'https://openalex.org/W3174968969', 'https://openalex.org/W3177181326', 'https://openalex.org/W4382318433', 'https://openalex.org/W2092969723', 'https://openalex.org/W3134428422', 'https://openalex.org/W3162051685', 'https://openalex.org/W4251560328', 'https://openalex.org/W3162165749', 'https://openalex.org/W1628789162', 'https://openalex.org/W4394671563', 'https://openalex.org/W3205692592']",2023-06-14
https://openalex.org/W4363676804,https://doi.org/10.1080/20551940.2023.2195057,"‘All possible sounds’: speech, music, and the emergence of machine listening","""Machine listening"" is one common term for a fast-growing interdisciplinary field of science and engineering that ""uses signal processing and machine learning to extract useful information from sound"". This article contributes to the critical literature on machine listening by presenting some of its history as a field. From the 1940s to the 1990s, work on artificial intelligence and audio developed along two streams. There was work on speech recognition/understanding, and work in computer music. In the early 1990s, another stream began to emerge. At institutions such as MIT Media Lab and Stanford's CCRMA, researchers started turning towards ""more fundamental problems of audition"". Propelled by work being done by and alongside musicians, speech and music would increasingly be understood by computer scientists as particular sounds within a broader ""auditory scene"". Researchers began to develop machine listening systems for a more diverse range of sounds and classification tasks: often in the service of speech recognition, but also increasingly for their own sake. The soundscape itself was becoming an object of computational concern. Today, the ambition is ""to cover all possible sounds"". That is the aspiration with which we must now contend politically, and which this article sets out to historicise and understand.","['https://openalex.org/W4413288418', 'https://openalex.org/W3045995288', 'https://openalex.org/W4235452281', 'https://openalex.org/W4238227321', 'https://openalex.org/W4236038202', 'https://openalex.org/W4240510634', 'https://openalex.org/W4256399001', 'https://openalex.org/W2068298977', 'https://openalex.org/W2026030136', 'https://openalex.org/W2044222806', 'https://openalex.org/W1977510622', 'https://openalex.org/W2082758173', 'https://openalex.org/W1991139021', 'https://openalex.org/W2889315866', 'https://openalex.org/W3005901320', 'https://openalex.org/W2020259652', 'https://openalex.org/W1001112348', 'https://openalex.org/W2097095326', 'https://openalex.org/W1965419851', 'https://openalex.org/W2159284348', 'https://openalex.org/W4253075065', 'https://openalex.org/W3116838706', 'https://openalex.org/W4399962391', 'https://openalex.org/W2603037960', 'https://openalex.org/W3210970177', 'https://openalex.org/W3185261023', 'https://openalex.org/W6749080293', 'https://openalex.org/W2555769234', 'https://openalex.org/W4236515469', 'https://openalex.org/W2593116425', 'https://openalex.org/W3103677873', 'https://openalex.org/W2153586312', 'https://openalex.org/W1486531642', 'https://openalex.org/W2048914355', 'https://openalex.org/W2103140519', 'https://openalex.org/W4243369708', 'https://openalex.org/W2122979854', 'https://openalex.org/W2505701133', 'https://openalex.org/W4313816055', 'https://openalex.org/W4311226163', 'https://openalex.org/W3129009457', 'https://openalex.org/W6814850485', 'https://openalex.org/W2144037622', 'https://openalex.org/W2070177582', 'https://openalex.org/W2789439097', 'https://openalex.org/W2788049062', 'https://openalex.org/W2141373701', 'https://openalex.org/W2913360400', 'https://openalex.org/W2046972719', 'https://openalex.org/W2612464443', 'https://openalex.org/W2331920492', 'https://openalex.org/W2493882299', 'https://openalex.org/W3023656861', 'https://openalex.org/W2736758126', 'https://openalex.org/W2066629230', 'https://openalex.org/W2566935005', 'https://openalex.org/W2151359505', 'https://openalex.org/W2063789736', 'https://openalex.org/W2643239522', 'https://openalex.org/W2123023349', 'https://openalex.org/W6678067405', 'https://openalex.org/W1977609277', 'https://openalex.org/W1992982563', 'https://openalex.org/W4250902796', 'https://openalex.org/W2943964375', 'https://openalex.org/W4249856834', 'https://openalex.org/W3212209585', 'https://openalex.org/W2002767771', 'https://openalex.org/W4235806191', 'https://openalex.org/W2075979668', 'https://openalex.org/W2062913357', 'https://openalex.org/W4238553276', 'https://openalex.org/W6712155849', 'https://openalex.org/W3140531618', 'https://openalex.org/W2318948281', 'https://openalex.org/W4255453945', 'https://openalex.org/W2955412730', 'https://openalex.org/W1599105370', 'https://openalex.org/W376085715', 'https://openalex.org/W6850607388', 'https://openalex.org/W3147911274', 'https://openalex.org/W4281676013', 'https://openalex.org/W2086384421', 'https://openalex.org/W1505221106', 'https://openalex.org/W2166894917', 'https://openalex.org/W6605752046', 'https://openalex.org/W2037761428', 'https://openalex.org/W122994913', 'https://openalex.org/W4233392025', 'https://openalex.org/W4247784448', 'https://openalex.org/W3213151880', 'https://openalex.org/W2074188409', 'https://openalex.org/W2486063739', 'https://openalex.org/W2492473494', 'https://openalex.org/W2796212141', 'https://openalex.org/W1661086764', 'https://openalex.org/W2944297474', 'https://openalex.org/W1500230234', 'https://openalex.org/W1994675634', 'https://openalex.org/W4233545208', 'https://openalex.org/W2504585274', 'https://openalex.org/W610324613', 'https://openalex.org/W1970185999', 'https://openalex.org/W4360988722', 'https://openalex.org/W2032498967', 'https://openalex.org/W1541647209', 'https://openalex.org/W2805618020', 'https://openalex.org/W2588217777', 'https://openalex.org/W1510737763', 'https://openalex.org/W1585961924', 'https://openalex.org/W2801777706', 'https://openalex.org/W2396542925', 'https://openalex.org/W4242644378', 'https://openalex.org/W2039407780']",2023-04-10
https://openalex.org/W4391021530,https://doi.org/10.1109/asru57964.2023.10389731,Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model,"Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3129009457', 'https://openalex.org/W3198217962', 'https://openalex.org/W4377865046', 'https://openalex.org/W4372270126', 'https://openalex.org/W3185341429', 'https://openalex.org/W6759579507', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W4372270069', 'https://openalex.org/W4319862642', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226278833', 'https://openalex.org/W4381786045', 'https://openalex.org/W4281672148', 'https://openalex.org/W2970476646', 'https://openalex.org/W3098267758', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W6852859116', 'https://openalex.org/W6752946794', 'https://openalex.org/W6809645183', 'https://openalex.org/W4372346241', 'https://openalex.org/W4372260195', 'https://openalex.org/W6738045163', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3172698324', 'https://openalex.org/W6846600677', 'https://openalex.org/W6750665317', 'https://openalex.org/W2972584841', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3161223924', 'https://openalex.org/W3095410713', 'https://openalex.org/W6802744804', 'https://openalex.org/W2963211188', 'https://openalex.org/W4297808394', 'https://openalex.org/W4298312696', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W4379539302', 'https://openalex.org/W4385245566', 'https://openalex.org/W3168867926', 'https://openalex.org/W4307783813', 'https://openalex.org/W4322825254', 'https://openalex.org/W2797583228', 'https://openalex.org/W4221155122']",2023-12-16
https://openalex.org/W4392791724,https://doi.org/10.1101/2024.03.11.584533,Subject-Agnostic Transformer-Based Neural Speech Decoding from Surface and Depth Electrode Signals,"Abstract Objective This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e., Electrocorticographic or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface (ECoG) and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements and the trained model should perform well on participants unseen during training. Approach We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-patient models exploiting data from multiple participants. Main Results The subject-specific models using only low-density 8×8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC=0.817), over N=43 participants, outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N=39) led to further improvement (PCC=0.838). For participants with only sEEG electrodes (N=9), subject-specific models still enjoy comparable performance with an average PCC=0.798. The multi-subject models achieved high performance on unseen participants, with an average PCC=0.765 in leave-one-out cross-validation. Significance The proposed SwinTW decoder enables future speech neuropros-theses to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. Importantly, the generalizability of the multi-patient models suggests that such a model can be applied to new patients that do not have paired acoustic and neural data, providing an advance in neuroprostheses for people with speech disability, where acoustic-neural training data is not feasible.","['https://openalex.org/W2952609044', 'https://openalex.org/W4225307836', 'https://openalex.org/W3198923947', 'https://openalex.org/W2940585064', 'https://openalex.org/W2076676508', 'https://openalex.org/W2032275524', 'https://openalex.org/W4386815321', 'https://openalex.org/W2991804118', 'https://openalex.org/W4387378260', 'https://openalex.org/W2995233853', 'https://openalex.org/W3130062067', 'https://openalex.org/W2194775991', 'https://openalex.org/W2991284028', 'https://openalex.org/W3008011111', 'https://openalex.org/W3209059054', 'https://openalex.org/W2673183736', 'https://openalex.org/W3206311643', 'https://openalex.org/W4295350540', 'https://openalex.org/W1522301498', 'https://openalex.org/W3210188778', 'https://openalex.org/W4224926216', 'https://openalex.org/W2396366106', 'https://openalex.org/W3129009457', 'https://openalex.org/W4312349930', 'https://openalex.org/W3138516171', 'https://openalex.org/W4312560592', 'https://openalex.org/W4210261408', 'https://openalex.org/W3013691153', 'https://openalex.org/W4386100600', 'https://openalex.org/W2964982538', 'https://openalex.org/W3180220247', 'https://openalex.org/W2145817563', 'https://openalex.org/W2761532371', 'https://openalex.org/W2150823344', 'https://openalex.org/W2768153200', 'https://openalex.org/W4388842084', 'https://openalex.org/W4372342291', 'https://openalex.org/W3048859947', 'https://openalex.org/W3095432487', 'https://openalex.org/W2918972315', 'https://openalex.org/W2095516539', 'https://openalex.org/W4286462232', 'https://openalex.org/W3028375381', 'https://openalex.org/W4387526256', 'https://openalex.org/W4399722542', 'https://openalex.org/W4404955916']",2024-03-14
https://openalex.org/W3187244867,https://doi.org/10.21437/interspeech.2021-1755,The Zero Resource Speech Challenge 2021: Spoken Language Modelling,"We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.","['https://openalex.org/W2142625445', 'https://openalex.org/W1854884267', 'https://openalex.org/W2982223350', 'https://openalex.org/W2251012068', 'https://openalex.org/W2132631284', 'https://openalex.org/W2963341956', 'https://openalex.org/W3093096176', 'https://openalex.org/W2026487812', 'https://openalex.org/W3036601975', 'https://openalex.org/W2510413766', 'https://openalex.org/W2176085882', 'https://openalex.org/W2170682101', 'https://openalex.org/W3005511757', 'https://openalex.org/W2940544976', 'https://openalex.org/W3129009457', 'https://openalex.org/W3099782249', 'https://openalex.org/W2996383576', 'https://openalex.org/W2014307400', 'https://openalex.org/W2794753807', 'https://openalex.org/W2933138175', 'https://openalex.org/W2080100102', 'https://openalex.org/W2137735870', 'https://openalex.org/W2842511635', 'https://openalex.org/W2593779438', 'https://openalex.org/W2963583956', 'https://openalex.org/W1494198834', 'https://openalex.org/W2990241049', 'https://openalex.org/W2103318667', 'https://openalex.org/W2965373594', 'https://openalex.org/W3110458199', 'https://openalex.org/W2995181338', 'https://openalex.org/W3129289122']",2021-08-27
https://openalex.org/W3143247654,https://doi.org/10.21437/interspeech.2021-1783,Configurable Privacy-Preserving Automatic Speech Recognition,"Voice assistive technologies have given rise to far-reaching privacy and security concerns. In this paper we investigate whether modular automatic speech recognition (ASR) can improve privacy in voice assistive systems by combining independently trained separation, recognition, and discretization modules to design configurable privacy-preserving ASR systems. We evaluate privacy concerns and the effects of applying various state-of-the-art techniques at each stage of the system, and report results using task-specific metrics (i.e. WER, ABX, and accuracy). We show that overlapping speech inputs to ASR systems present further privacy concerns, and how these may be mitigated using speech separation and optimization techniques. Our discretization module is shown to minimize paralinguistics privacy leakage from ASR acoustic models to levels commensurate with random guessing. We show that voice privacy can be configurable, and argue this presents new opportunities for privacy-preserving applications incorporating ASR.","['https://openalex.org/W3034010994', 'https://openalex.org/W3005862564', 'https://openalex.org/W2972541922', 'https://openalex.org/W3134286091', 'https://openalex.org/W3082482205', 'https://openalex.org/W2898564584', 'https://openalex.org/W3082522567', 'https://openalex.org/W2982574661', 'https://openalex.org/W3036601975', 'https://openalex.org/W3129009457', 'https://openalex.org/W2962760690', 'https://openalex.org/W3130248090', 'https://openalex.org/W3024768724', 'https://openalex.org/W2951082691', 'https://openalex.org/W3163652268', 'https://openalex.org/W3097320994', 'https://openalex.org/W2726515241', 'https://openalex.org/W3042776162', 'https://openalex.org/W3098439673', 'https://openalex.org/W2995929068', 'https://openalex.org/W2995181338', 'https://openalex.org/W3110761489', 'https://openalex.org/W2981507750', 'https://openalex.org/W2030931454', 'https://openalex.org/W3016232124', 'https://openalex.org/W3099782249', 'https://openalex.org/W3024085360', 'https://openalex.org/W3021040286', 'https://openalex.org/W3027008958', 'https://openalex.org/W3110458199']",2021-08-27
https://openalex.org/W4378086438,https://doi.org/10.1007/978-3-031-23190-2_7,"Foundation Models for Speech, Images, Videos, and Control","Abstract Foundation Models are able to model not only tokens of natural language but also token elements of arbitrary sequences. For images, square image patches can be represented as tokens; for videos, we can define tubelets that span an image patch across multiple frames. Subsequently, the proven self-attention algorithms can be applied to these tokens. Most importantly, several modalities like text and images can be processed in the same sequence allowing, for instance, the generation of images from text and text descriptions from video. In addition, the models are scalable to very large networks and huge datasets. The following multimedia types are covered in the subsequent sections. Speech recognition and text-to-speech models describe the translation of spoken language into text and vice versa. Image processing has the task to interpret images, describe them by captions, and generate new images according to textual descriptions. Video interpretation aims at recognizing action in videos and describing them through text. Furthermore, new videos can be created according to a textual description. Dynamical system trajectories characterize sequential decision problems, which can be simulated and controlled. DNA and protein sequences can be analyzed with Foundation Models to predict the structure and properties of the corresponding molecules.","['https://openalex.org/W2891205112', 'https://openalex.org/W3154596443', 'https://openalex.org/W4225323055', 'https://openalex.org/W3170905880', 'https://openalex.org/W4288421316', 'https://openalex.org/W3180059462', 'https://openalex.org/W3186179742', 'https://openalex.org/W4221145109', 'https://openalex.org/W3036601975', 'https://openalex.org/W6796761347', 'https://openalex.org/W2150468603', 'https://openalex.org/W4225013417', 'https://openalex.org/W3128633047', 'https://openalex.org/W3208581766', 'https://openalex.org/W3106784008', 'https://openalex.org/W4313069509', 'https://openalex.org/W3139918052', 'https://openalex.org/W3176196997', 'https://openalex.org/W3169291081', 'https://openalex.org/W2962892438', 'https://openalex.org/W6785310428', 'https://openalex.org/W1889081078', 'https://openalex.org/W6761628794', 'https://openalex.org/W3104152799', 'https://openalex.org/W4300861364', 'https://openalex.org/W3096109555', 'https://openalex.org/W4226033575', 'https://openalex.org/W3207758636', 'https://openalex.org/W3198076979', 'https://openalex.org/W6776218486', 'https://openalex.org/W3162926177', 'https://openalex.org/W3207290297', 'https://openalex.org/W3179891906', 'https://openalex.org/W3165647589', 'https://openalex.org/W2259643685', 'https://openalex.org/W3094502228', 'https://openalex.org/W4225683910', 'https://openalex.org/W2765349170', 'https://openalex.org/W3180355996', 'https://openalex.org/W3210084825', 'https://openalex.org/W1915786104', 'https://openalex.org/W3118781290', 'https://openalex.org/W3035029089', 'https://openalex.org/W3094751268', 'https://openalex.org/W4312372834', 'https://openalex.org/W2625366777', 'https://openalex.org/W2560730294', 'https://openalex.org/W2127141656', 'https://openalex.org/W3209374680', 'https://openalex.org/W3097777922', 'https://openalex.org/W3196936439', 'https://openalex.org/W3202536355', 'https://openalex.org/W3170405112', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963981733', 'https://openalex.org/W6779823529', 'https://openalex.org/W4303440777', 'https://openalex.org/W2896348597', 'https://openalex.org/W3209059054', 'https://openalex.org/W3091177855', 'https://openalex.org/W3174476431', 'https://openalex.org/W2919624000', 'https://openalex.org/W3206384369', 'https://openalex.org/W3133511908', 'https://openalex.org/W6729448088', 'https://openalex.org/W3127238141', 'https://openalex.org/W6804316701', 'https://openalex.org/W3195809285', 'https://openalex.org/W3142316150', 'https://openalex.org/W3035574324', 'https://openalex.org/W2619947201', 'https://openalex.org/W4206706211', 'https://openalex.org/W3126942607', 'https://openalex.org/W3033324992', 'https://openalex.org/W6790749177', 'https://openalex.org/W3129009457', 'https://openalex.org/W3201555865', 'https://openalex.org/W4200500394', 'https://openalex.org/W4225096589', 'https://openalex.org/W4312407537', 'https://openalex.org/W2903739847', 'https://openalex.org/W3091588028', 'https://openalex.org/W3207918547', 'https://openalex.org/W3041561163', 'https://openalex.org/W3138516171', 'https://openalex.org/W2966715458', 'https://openalex.org/W3100732527', 'https://openalex.org/W3217023900', 'https://openalex.org/W3099551617', 'https://openalex.org/W2984008963', 'https://openalex.org/W3207661553', 'https://openalex.org/W2962711930', 'https://openalex.org/W6795475546', 'https://openalex.org/W4226125322', 'https://openalex.org/W3122887982', 'https://openalex.org/W3144701084', 'https://openalex.org/W3139408490', 'https://openalex.org/W2752796333', 'https://openalex.org/W1494198834', 'https://openalex.org/W3213191779', 'https://openalex.org/W2936774411', 'https://openalex.org/W3026041220', 'https://openalex.org/W2962974533', 'https://openalex.org/W3144192372', 'https://openalex.org/W2963300588', 'https://openalex.org/W3135367836', 'https://openalex.org/W3195830874', 'https://openalex.org/W6746023985', 'https://openalex.org/W4224035735', 'https://openalex.org/W4221142845', 'https://openalex.org/W639708223', 'https://openalex.org/W6778823374', 'https://openalex.org/W3148101939', 'https://openalex.org/W4312933868', 'https://openalex.org/W1901129140', 'https://openalex.org/W4210352519', 'https://openalex.org/W3175343838', 'https://openalex.org/W3212516020', 'https://openalex.org/W4281485151', 'https://openalex.org/W6718379498', 'https://openalex.org/W4297841719', 'https://openalex.org/W2886641317', 'https://openalex.org/W2964243274', 'https://openalex.org/W3213549365', 'https://openalex.org/W2129069237', 'https://openalex.org/W6798308576', 'https://openalex.org/W2981851019', 'https://openalex.org/W2962843773', 'https://openalex.org/W2183341477', 'https://openalex.org/W6796730497', 'https://openalex.org/W3167366812', 'https://openalex.org/W3048484056', 'https://openalex.org/W3177174258', 'https://openalex.org/W2902437806', 'https://openalex.org/W1956340063', 'https://openalex.org/W4281633595', 'https://openalex.org/W3193402170', 'https://openalex.org/W6763239785', 'https://openalex.org/W4226283246', 'https://openalex.org/W3215495615', 'https://openalex.org/W3157199281', 'https://openalex.org/W3173241699', 'https://openalex.org/W2912371042', 'https://openalex.org/W2549139847', 'https://openalex.org/W2773514261', 'https://openalex.org/W2916979304', 'https://openalex.org/W2425121537', 'https://openalex.org/W4282963182', 'https://openalex.org/W3160525311', 'https://openalex.org/W4312658081', 'https://openalex.org/W3161109662', 'https://openalex.org/W2489434015', 'https://openalex.org/W3168154341', 'https://openalex.org/W3172942063', 'https://openalex.org/W4225755514', 'https://openalex.org/W4312358791', 'https://openalex.org/W3174525637', 'https://openalex.org/W3173220247', 'https://openalex.org/W2962785568', 'https://openalex.org/W3204696009', 'https://openalex.org/W3093579165', 'https://openalex.org/W3203949114', 'https://openalex.org/W3173290664', 'https://openalex.org/W3216237230', 'https://openalex.org/W4312044727', 'https://openalex.org/W3125488228', 'https://openalex.org/W3207212285']",2023-01-01
https://openalex.org/W4391026406,https://doi.org/10.26634/javr.1.1.19412,Synthetic audio and video generation for language translation using GANs,"Language barriers create a digital divide that prevents people from benefiting from the vast amount of content produced worldwide. In addition, content creators face challenges in producing content in multiple languages to reach a wider audience. To address this problem, this study proposed a solution through a survey that utilized Generative Adversarial Networks (GAN), Natural Language Processing (NPL), and Computer Vision. A Generative Adversarial Network (GAN) is a Machine Learning (ML) model in which two neural networks compete with each other by using deep learning methods to obtain more accurate predictions. The solution provided in this study can generate synthesized videos that are close to reality, ultimately bridging the language barrier and providing access to content.","['https://openalex.org/W3036601975', 'https://openalex.org/W2951523806', 'https://openalex.org/W3129009457', 'https://openalex.org/W4297772798', 'https://openalex.org/W2963420272', 'https://openalex.org/W3081492798', 'https://openalex.org/W2963684088', 'https://openalex.org/W6713645886', 'https://openalex.org/W2963373786', 'https://openalex.org/W6726983635', 'https://openalex.org/W2546066744', 'https://openalex.org/W2521028896', 'https://openalex.org/W2474531669', 'https://openalex.org/W2519536754', 'https://openalex.org/W2962793481', 'https://openalex.org/W3101631197']",2023-01-01
https://openalex.org/W4398252084,https://doi.org/10.2139/ssrn.4838997,Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network,,"['https://openalex.org/W4253928870', 'https://openalex.org/W2150543128', 'https://openalex.org/W4221150601', 'https://openalex.org/W4372347086', 'https://openalex.org/W2062164080', 'https://openalex.org/W3206531472', 'https://openalex.org/W2901368553', 'https://openalex.org/W3129009457', 'https://openalex.org/W3214314684', 'https://openalex.org/W2809290718', 'https://openalex.org/W3204647170', 'https://openalex.org/W4379659733', 'https://openalex.org/W3092739317', 'https://openalex.org/W2940256401', 'https://openalex.org/W2753789229', 'https://openalex.org/W1742512077', 'https://openalex.org/W2127851351', 'https://openalex.org/W6853513598', 'https://openalex.org/W3197321511', 'https://openalex.org/W3024869864', 'https://openalex.org/W3169320628', 'https://openalex.org/W6810856829', 'https://openalex.org/W4384080510', 'https://openalex.org/W2141998673', 'https://openalex.org/W4225302959', 'https://openalex.org/W4379984888', 'https://openalex.org/W2972425344', 'https://openalex.org/W4297915175', 'https://openalex.org/W4387323811', 'https://openalex.org/W4385822330', 'https://openalex.org/W3040479127', 'https://openalex.org/W4225298533', 'https://openalex.org/W4226390724', 'https://openalex.org/W3163464523', 'https://openalex.org/W4244387609', 'https://openalex.org/W2964058413', 'https://openalex.org/W3197813307', 'https://openalex.org/W2981757109', 'https://openalex.org/W4386185396', 'https://openalex.org/W4300980246', 'https://openalex.org/W4297841603', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385002383', 'https://openalex.org/W2972852081', 'https://openalex.org/W4226029295', 'https://openalex.org/W4394671563', 'https://openalex.org/W2950199911']",2024-01-01
https://openalex.org/W4403780768,https://doi.org/10.1145/3664647.3681221,Efficient Training for Multilingual Visual Speech Recognition: Pre-training with Discretized Visual Speech Representation,,"['https://openalex.org/W2963431393', 'https://openalex.org/W3015830103', 'https://openalex.org/W4385822683', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385823403', 'https://openalex.org/W2808631503', 'https://openalex.org/W3034552680', 'https://openalex.org/W3024869864', 'https://openalex.org/W4289665794', 'https://openalex.org/W3163793923', 'https://openalex.org/W2194775991', 'https://openalex.org/W4386071467', 'https://openalex.org/W3209059054', 'https://openalex.org/W3008549139', 'https://openalex.org/W3205193540', 'https://openalex.org/W4390874021', 'https://openalex.org/W4224319127', 'https://openalex.org/W3129009457', 'https://openalex.org/W4287854499', 'https://openalex.org/W4372346152', 'https://openalex.org/W3162707322', 'https://openalex.org/W3162293946', 'https://openalex.org/W4307286264', 'https://openalex.org/W4225299282', 'https://openalex.org/W4307680525', 'https://openalex.org/W4390871839', 'https://openalex.org/W2963192365', 'https://openalex.org/W2404704342', 'https://openalex.org/W2963654155', 'https://openalex.org/W2963785710', 'https://openalex.org/W3140429000', 'https://openalex.org/W4296070387', 'https://openalex.org/W4312638101', 'https://openalex.org/W3095410713', 'https://openalex.org/W3167917117', 'https://openalex.org/W4375868953', 'https://openalex.org/W2963528589', 'https://openalex.org/W2964309797', 'https://openalex.org/W2766219058', 'https://openalex.org/W4225746985', 'https://openalex.org/W4390691978', 'https://openalex.org/W2996970093', 'https://openalex.org/W4376481237', 'https://openalex.org/W2042718506', 'https://openalex.org/W4394671563']",2024-10-26
https://openalex.org/W3190032417,https://doi.org/10.21437/interspeech.2021-1182,Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing,"Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.","['https://openalex.org/W3158802984', 'https://openalex.org/W3093096176', 'https://openalex.org/W2963620343', 'https://openalex.org/W3099782249', 'https://openalex.org/W3125709657', 'https://openalex.org/W3003875258', 'https://openalex.org/W2973026522', 'https://openalex.org/W2347098582', 'https://openalex.org/W2138615112', 'https://openalex.org/W2962689740', 'https://openalex.org/W2996728628', 'https://openalex.org/W2766049740', 'https://openalex.org/W3110458199', 'https://openalex.org/W2113054345', 'https://openalex.org/W2395899413', 'https://openalex.org/W2889326414', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W3129009457', 'https://openalex.org/W2117041980', 'https://openalex.org/W1494198834', 'https://openalex.org/W2787447541', 'https://openalex.org/W3197381195', 'https://openalex.org/W2996383576', 'https://openalex.org/W3197349023', 'https://openalex.org/W2025482506', 'https://openalex.org/W2014307400', 'https://openalex.org/W3095361818', 'https://openalex.org/W2747874407', 'https://openalex.org/W2100768664']",2021-08-27
https://openalex.org/W3166440012,https://doi.org/10.48550/arxiv.2106.05933,"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition","Self-supervised speech representation learning (speech SSL) has demonstrated the benefit of scale in learning rich representations for Automatic Speech Recognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate the existence of sparse subnetworks in pre-trained speech SSL models that achieve even better low-resource ASR results. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, we show that the discovered subnetworks yield minimal performance gain compared to the original dense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and finetunes subnetworks for much better performance, while only requiring a single downstream ASR finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks need merely a slight adjustment to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource ASR verify (1) sparse subnetworks exist in mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. In particular, on the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We further demonstrate the effectiveness of PARP via: cross-lingual pruning without any phone recognition degradation, the discovery of a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks.","['https://openalex.org/W3163842642', 'https://openalex.org/W3143035657', 'https://openalex.org/W3160399536', 'https://openalex.org/W3148001440', 'https://openalex.org/W2802201485', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963340922', 'https://openalex.org/W3025165719', 'https://openalex.org/W2971237698', 'https://openalex.org/W2294543795', 'https://openalex.org/W3093579165', 'https://openalex.org/W3098680936', 'https://openalex.org/W3099782249', 'https://openalex.org/W3162665866', 'https://openalex.org/W3178203035', 'https://openalex.org/W3198217962', 'https://openalex.org/W3160405885', 'https://openalex.org/W3035615218', 'https://openalex.org/W1974205368', 'https://openalex.org/W3205032693', 'https://openalex.org/W3112034174', 'https://openalex.org/W2996383576', 'https://openalex.org/W2962965870', 'https://openalex.org/W2963813662', 'https://openalex.org/W3162391496', 'https://openalex.org/W2768501777', 'https://openalex.org/W3163600291', 'https://openalex.org/W3207300132', 'https://openalex.org/W2948130861', 'https://openalex.org/W2608554408', 'https://openalex.org/W2730658205', 'https://openalex.org/W3152519008', 'https://openalex.org/W3021469861', 'https://openalex.org/W3178584664', 'https://openalex.org/W3202278141', 'https://openalex.org/W3147962056', 'https://openalex.org/W2930682606', 'https://openalex.org/W3161223924', 'https://openalex.org/W3157923770', 'https://openalex.org/W3204123830', 'https://openalex.org/W3205533980', 'https://openalex.org/W3104263050', 'https://openalex.org/W2963310665', 'https://openalex.org/W3104136798', 'https://openalex.org/W2975044525', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963981420', 'https://openalex.org/W2972808286', 'https://openalex.org/W2114766824', 'https://openalex.org/W3129009457', 'https://openalex.org/W2972943112', 'https://openalex.org/W3206559778', 'https://openalex.org/W2963674932', 'https://openalex.org/W3040454670', 'https://openalex.org/W3034487470', 'https://openalex.org/W2043422002', 'https://openalex.org/W2888867175', 'https://openalex.org/W3204696009', 'https://openalex.org/W2084910356', 'https://openalex.org/W2125389748', 'https://openalex.org/W3111265704', 'https://openalex.org/W2965862774', 'https://openalex.org/W3167207712', 'https://openalex.org/W3037057938', 'https://openalex.org/W2407115099', 'https://openalex.org/W3197974236', 'https://openalex.org/W3081179222', 'https://openalex.org/W3003875258', 'https://openalex.org/W3166035876', 'https://openalex.org/W3035081900', 'https://openalex.org/W104222852', 'https://openalex.org/W3165666670', 'https://openalex.org/W2963247446', 'https://openalex.org/W3206252155', 'https://openalex.org/W2951569836', 'https://openalex.org/W3144173820', 'https://openalex.org/W2894835365', 'https://openalex.org/W3094550259', 'https://openalex.org/W3162309234', 'https://openalex.org/W3204224625', 'https://openalex.org/W1515156256', 'https://openalex.org/W3104350794', 'https://openalex.org/W3095292526', 'https://openalex.org/W2936481169', 'https://openalex.org/W3193521535', 'https://openalex.org/W3207558756', 'https://openalex.org/W3162649911', 'https://openalex.org/W2963400886', 'https://openalex.org/W3173970713', 'https://openalex.org/W3169320628', 'https://openalex.org/W2988736778', 'https://openalex.org/W3197278374', 'https://openalex.org/W2112984492', 'https://openalex.org/W3140429000', 'https://openalex.org/W3147414526', 'https://openalex.org/W3152884768', 'https://openalex.org/W3038041907', 'https://openalex.org/W3111921445', 'https://openalex.org/W3128478537', 'https://openalex.org/W3160525311', 'https://openalex.org/W3157697407', 'https://openalex.org/W2962813140', 'https://openalex.org/W3041561163', 'https://openalex.org/W3205644108', 'https://openalex.org/W2963503967', 'https://openalex.org/W3001899777', 'https://openalex.org/W2915589364', 'https://openalex.org/W3034234149', 'https://openalex.org/W3022969335', 'https://openalex.org/W2842511635', 'https://openalex.org/W2156700117', 'https://openalex.org/W2291975472', 'https://openalex.org/W3139918052', 'https://openalex.org/W3205710300', 'https://openalex.org/W3087835661', 'https://openalex.org/W2995816250', 'https://openalex.org/W3162868000', 'https://openalex.org/W3024171804', 'https://openalex.org/W3162133897', 'https://openalex.org/W2970120757', 'https://openalex.org/W3096587983', 'https://openalex.org/W3016181583', 'https://openalex.org/W2292087804', 'https://openalex.org/W3093346109']",2021-06-10
https://openalex.org/W3134881075,https://doi.org/10.48550/arxiv.2103.06089,Variable-rate discrete representation learning,"Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.","['https://openalex.org/W2593493485', 'https://openalex.org/W2963522047', 'https://openalex.org/W2995181338', 'https://openalex.org/W2635692202', 'https://openalex.org/W1959608418', 'https://openalex.org/W2961839838', 'https://openalex.org/W2063668051', 'https://openalex.org/W2562731582', 'https://openalex.org/W2011301426', 'https://openalex.org/W2971278153', 'https://openalex.org/W3175871055', 'https://openalex.org/W2906007967', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963321993', 'https://openalex.org/W2922386270', 'https://openalex.org/W3129009457', 'https://openalex.org/W2242818861', 'https://openalex.org/W2325237720', 'https://openalex.org/W2885380450', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963532523', 'https://openalex.org/W2962784628', 'https://openalex.org/W2086161653', 'https://openalex.org/W2885185669', 'https://openalex.org/W2964182247', 'https://openalex.org/W2913870675', 'https://openalex.org/W648786980', 'https://openalex.org/W2963711785', 'https://openalex.org/W2963223306', 'https://openalex.org/W3135058862', 'https://openalex.org/W2971074500', 'https://openalex.org/W3024605872', 'https://openalex.org/W2963408210', 'https://openalex.org/W2990500698', 'https://openalex.org/W3035358681', 'https://openalex.org/W2271232908', 'https://openalex.org/W2191779130', 'https://openalex.org/W2911448865', 'https://openalex.org/W3129831491', 'https://openalex.org/W1994211684', 'https://openalex.org/W2140262144', 'https://openalex.org/W2761228892', 'https://openalex.org/W3102696055', 'https://openalex.org/W2963951231', 'https://openalex.org/W2146444479', 'https://openalex.org/W2963403868', 'https://openalex.org/W2122825543', 'https://openalex.org/W2972359262', 'https://openalex.org/W2949382160', 'https://openalex.org/W2141863509', 'https://openalex.org/W2560512785', 'https://openalex.org/W2996287690', 'https://openalex.org/W2842511635', 'https://openalex.org/W2891364477', 'https://openalex.org/W2242464395', 'https://openalex.org/W2963494889', 'https://openalex.org/W2160660594', 'https://openalex.org/W3035435378', 'https://openalex.org/W3033210410', 'https://openalex.org/W2919112510', 'https://openalex.org/W2963799213', 'https://openalex.org/W3122023385', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963370182', 'https://openalex.org/W2931316642', 'https://openalex.org/W2127141656', 'https://openalex.org/W3030163527', 'https://openalex.org/W3034119625', 'https://openalex.org/W2902630600', 'https://openalex.org/W2963137467', 'https://openalex.org/W2128002512', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962788496', 'https://openalex.org/W3125709657', 'https://openalex.org/W3048217718', 'https://openalex.org/W3008499099', 'https://openalex.org/W2257979135', 'https://openalex.org/W2548228487', 'https://openalex.org/W3130248090', 'https://openalex.org/W3132432992', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963263347', 'https://openalex.org/W2806153517', 'https://openalex.org/W4226051885', 'https://openalex.org/W2962942158', 'https://openalex.org/W3018535504', 'https://openalex.org/W1745681373', 'https://openalex.org/W2103559027', 'https://openalex.org/W2547418827']",2021-03-10
https://openalex.org/W3201728305,https://doi.org/10.48550/arxiv.2107.13349,Self-Supervised Inference in State-Space Models,"We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.","['https://openalex.org/W2094227853', 'https://openalex.org/W3023201122', 'https://openalex.org/W1981364618', 'https://openalex.org/W2970375342', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963074410', 'https://openalex.org/W2120615054', 'https://openalex.org/W2105934661', 'https://openalex.org/W2889928394', 'https://openalex.org/W2101675075', 'https://openalex.org/W2271522117', 'https://openalex.org/W2993076031', 'https://openalex.org/W2912752824', 'https://openalex.org/W2070047061', 'https://openalex.org/W2963166838', 'https://openalex.org/W2470207857', 'https://openalex.org/W2797583228', 'https://openalex.org/W3129009457', 'https://openalex.org/W2962986780', 'https://openalex.org/W2963714390', 'https://openalex.org/W2962695963', 'https://openalex.org/W2914204228', 'https://openalex.org/W2064675550', 'https://openalex.org/W2143612262', 'https://openalex.org/W2176035349', 'https://openalex.org/W2964232608', 'https://openalex.org/W2964204553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2902857081', 'https://openalex.org/W2971082124', 'https://openalex.org/W2964334616', 'https://openalex.org/W3093010610', 'https://openalex.org/W2154368244']",2021-07-28
https://openalex.org/W3204516855,https://doi.org/10.48550/arxiv.2110.02782,How BPE Affects Memorization in Transformers,"Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.","['https://openalex.org/W2972433021', 'https://openalex.org/W3102659883', 'https://openalex.org/W2996094825', 'https://openalex.org/W2101105183', 'https://openalex.org/W2571532437', 'https://openalex.org/W3214210379', 'https://openalex.org/W2971869958', 'https://openalex.org/W3087835661', 'https://openalex.org/W2962784628', 'https://openalex.org/W3129009457', 'https://openalex.org/W3017374003', 'https://openalex.org/W3126425262', 'https://openalex.org/W3030163527', 'https://openalex.org/W2950220847', 'https://openalex.org/W3000779003', 'https://openalex.org/W2981852735', 'https://openalex.org/W2951528897', 'https://openalex.org/W2890077576', 'https://openalex.org/W2963430224', 'https://openalex.org/W3119000810', 'https://openalex.org/W3192500523', 'https://openalex.org/W3047026265', 'https://openalex.org/W2970971581', 'https://openalex.org/W2997731766', 'https://openalex.org/W2986378306', 'https://openalex.org/W3169369929', 'https://openalex.org/W2009525093', 'https://openalex.org/W3035168240', 'https://openalex.org/W2773466258', 'https://openalex.org/W3132730484', 'https://openalex.org/W2795435272', 'https://openalex.org/W2963250244', 'https://openalex.org/W2535690855', 'https://openalex.org/W3152698349', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945767825', 'https://openalex.org/W2995154514', 'https://openalex.org/W3198320319', 'https://openalex.org/W3198217962', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963005248', 'https://openalex.org/W2947029787', 'https://openalex.org/W2965373594', 'https://openalex.org/W3082928416', 'https://openalex.org/W1553508775', 'https://openalex.org/W1840435438', 'https://openalex.org/W2972324944', 'https://openalex.org/W2079656678', 'https://openalex.org/W2972498556', 'https://openalex.org/W3177804148', 'https://openalex.org/W3038040931', 'https://openalex.org/W2963267799', 'https://openalex.org/W3170672407', 'https://openalex.org/W1991762558', 'https://openalex.org/W2952744660', 'https://openalex.org/W2963341956', 'https://openalex.org/W3104739822', 'https://openalex.org/W3035207248', 'https://openalex.org/W2144862731', 'https://openalex.org/W3190860428', 'https://openalex.org/W2798663534', 'https://openalex.org/W2626778328']",2021-10-06
https://openalex.org/W3210088932,https://doi.org/10.48550/arxiv.2111.00610,Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units,"Language models (LMs) for text data have been studied extensively for their usefulness in language generation and other downstream tasks. However, language modelling purely in the speech domain is still a relatively unexplored topic, with traditional speech LMs often depending on auxiliary text LMs for learning distributional aspects of the language. For the English language, these LMs treat words as atomic units, which presents inherent challenges to language modelling in the speech domain. In this paper, we propose a novel LSTM-based generative speech LM that is inspired by the CBOW model and built on linguistic units including syllables and phonemes. This offers better acoustic consistency across utterances in the dataset, as opposed to single melspectrogram frames, or whole words. With a limited dataset, orders of magnitude smaller than that required by contemporary generative models, our model closely approximates babbling speech. We show the effect of training with auxiliary text LMs, multitask learning objectives, and auxiliary articulatory features. Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality. Our experiments provide an early indication that while validation loss and Mel Cepstral Distortion (MCD) are not strongly correlated with generated speech quality, traditional text language modelling metrics like perplexity and next-token-prediction accuracy might be.","['https://openalex.org/W2982223350', 'https://openalex.org/W2963609956', 'https://openalex.org/W2965373594', 'https://openalex.org/W3044438666', 'https://openalex.org/W2996286887', 'https://openalex.org/W179875071', 'https://openalex.org/W3199958362', 'https://openalex.org/W2963341956', 'https://openalex.org/W2973049979', 'https://openalex.org/W3030163527', 'https://openalex.org/W1579838312', 'https://openalex.org/W3095361947', 'https://openalex.org/W3129009457', 'https://openalex.org/W3001279689', 'https://openalex.org/W3134881075', 'https://openalex.org/W2572670101', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962778134', 'https://openalex.org/W8140679', 'https://openalex.org/W1614298861', 'https://openalex.org/W2777448780']",2021-10-31
https://openalex.org/W4387877468,https://doi.org/10.31234/osf.io/5p8ge,Artificial neural networks to analyze and simulate language acquisition in children,"Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.","['https://openalex.org/W2885156775', 'https://openalex.org/W4407276585', 'https://openalex.org/W2888800758', 'https://openalex.org/W2989863749', 'https://openalex.org/W2406262283', 'https://openalex.org/W6632323398', 'https://openalex.org/W2919849250', 'https://openalex.org/W2887814324', 'https://openalex.org/W2187280882', 'https://openalex.org/W2561127898', 'https://openalex.org/W2048632248', 'https://openalex.org/W3004481867', 'https://openalex.org/W2131168675', 'https://openalex.org/W3045733287', 'https://openalex.org/W1269382222', 'https://openalex.org/W2070218750', 'https://openalex.org/W2010604004', 'https://openalex.org/W4394142399', 'https://openalex.org/W2115857751', 'https://openalex.org/W2750259098', 'https://openalex.org/W6688816777', 'https://openalex.org/W2544860310', 'https://openalex.org/W2985913104', 'https://openalex.org/W1499999342', 'https://openalex.org/W6769833506', 'https://openalex.org/W1982599741', 'https://openalex.org/W6786127183', 'https://openalex.org/W2611943505', 'https://openalex.org/W2696967604', 'https://openalex.org/W2768416973', 'https://openalex.org/W4319862721', 'https://openalex.org/W2296434735', 'https://openalex.org/W2805466703', 'https://openalex.org/W2982059757', 'https://openalex.org/W3099610051', 'https://openalex.org/W2410879554', 'https://openalex.org/W3015235644', 'https://openalex.org/W4311000453', 'https://openalex.org/W1494198834', 'https://openalex.org/W2593116425', 'https://openalex.org/W2555915854', 'https://openalex.org/W3031133340', 'https://openalex.org/W3161374022', 'https://openalex.org/W2901243971', 'https://openalex.org/W3038871978', 'https://openalex.org/W2895356663', 'https://openalex.org/W2516342150', 'https://openalex.org/W2063525438', 'https://openalex.org/W4220770602', 'https://openalex.org/W2103091632', 'https://openalex.org/W122673323', 'https://openalex.org/W6600387335', 'https://openalex.org/W3129009457', 'https://openalex.org/W2809193001', 'https://openalex.org/W2759573091', 'https://openalex.org/W1592295210', 'https://openalex.org/W6789725041', 'https://openalex.org/W2063303346', 'https://openalex.org/W6665016202', 'https://openalex.org/W6790730029', 'https://openalex.org/W2944539396', 'https://openalex.org/W2586148577', 'https://openalex.org/W4213306813', 'https://openalex.org/W3135377987', 'https://openalex.org/W2483390977', 'https://openalex.org/W2774051897', 'https://openalex.org/W3084297320', 'https://openalex.org/W2991557631', 'https://openalex.org/W2112883467', 'https://openalex.org/W2071591642', 'https://openalex.org/W2889102505', 'https://openalex.org/W1485633403', 'https://openalex.org/W6641916425', 'https://openalex.org/W2085478996', 'https://openalex.org/W2972476505', 'https://openalex.org/W3025683731', 'https://openalex.org/W3125043549', 'https://openalex.org/W6729411815', 'https://openalex.org/W3125087428', 'https://openalex.org/W2141994663', 'https://openalex.org/W2135563147', 'https://openalex.org/W3192452456', 'https://openalex.org/W2610616322', 'https://openalex.org/W3128683352', 'https://openalex.org/W2012125774', 'https://openalex.org/W6945130725', 'https://openalex.org/W2022042240', 'https://openalex.org/W7054993619', 'https://openalex.org/W3129048159', 'https://openalex.org/W1981826156', 'https://openalex.org/W3034612657', 'https://openalex.org/W2095458199', 'https://openalex.org/W1988747931', 'https://openalex.org/W2101048305', 'https://openalex.org/W2340774970', 'https://openalex.org/W2046338384', 'https://openalex.org/W2890174796', 'https://openalex.org/W4295309037', 'https://openalex.org/W6812385817', 'https://openalex.org/W1990351858', 'https://openalex.org/W6655910177', 'https://openalex.org/W1968703923', 'https://openalex.org/W2912762364', 'https://openalex.org/W2104752510', 'https://openalex.org/W4206953441', 'https://openalex.org/W6808618455', 'https://openalex.org/W2776941264', 'https://openalex.org/W6678463487', 'https://openalex.org/W2062956221', 'https://openalex.org/W2138930657', 'https://openalex.org/W2051676521', 'https://openalex.org/W3136507086', 'https://openalex.org/W2125766564', 'https://openalex.org/W6600721412', 'https://openalex.org/W2090314151', 'https://openalex.org/W4303629135', 'https://openalex.org/W2802302202', 'https://openalex.org/W6640227979', 'https://openalex.org/W2153767712', 'https://openalex.org/W2066213611', 'https://openalex.org/W6778883912', 'https://openalex.org/W3110458199', 'https://openalex.org/W3198217962', 'https://openalex.org/W4300021539', 'https://openalex.org/W3213014097', 'https://openalex.org/W2765364385', 'https://openalex.org/W3215376969', 'https://openalex.org/W4318621130', 'https://openalex.org/W2155042697', 'https://openalex.org/W2995181338', 'https://openalex.org/W4281492411', 'https://openalex.org/W2741692265', 'https://openalex.org/W305655093', 'https://openalex.org/W4394150413', 'https://openalex.org/W2014307400', 'https://openalex.org/W4200300291', 'https://openalex.org/W4390854704', 'https://openalex.org/W6810610419', 'https://openalex.org/W3135131435', 'https://openalex.org/W4225079082', 'https://openalex.org/W6718447644', 'https://openalex.org/W2137181162', 'https://openalex.org/W2980577029', 'https://openalex.org/W2767826640', 'https://openalex.org/W2169471344', 'https://openalex.org/W2118611915', 'https://openalex.org/W3011583491', 'https://openalex.org/W2995929068', 'https://openalex.org/W2098192529', 'https://openalex.org/W241613093', 'https://openalex.org/W2747060779', 'https://openalex.org/W2508684615', 'https://openalex.org/W2935067899', 'https://openalex.org/W2091746061', 'https://openalex.org/W4291746033', 'https://openalex.org/W3112495382', 'https://openalex.org/W1984586950', 'https://openalex.org/W3046659789', 'https://openalex.org/W6684012063', 'https://openalex.org/W2805407818', 'https://openalex.org/W2088377333', 'https://openalex.org/W2997253105', 'https://openalex.org/W6781919819', 'https://openalex.org/W4322766928', 'https://openalex.org/W2162693531', 'https://openalex.org/W6811685372', 'https://openalex.org/W7014979702', 'https://openalex.org/W4213430915', 'https://openalex.org/W3157861865', 'https://openalex.org/W2553608650', 'https://openalex.org/W153534061', 'https://openalex.org/W4281621399', 'https://openalex.org/W3016398799', 'https://openalex.org/W2767354245', 'https://openalex.org/W3005165546', 'https://openalex.org/W3036367741', 'https://openalex.org/W1530250655', 'https://openalex.org/W4378619943', 'https://openalex.org/W4321854743', 'https://openalex.org/W3090329631', 'https://openalex.org/W3187244867', 'https://openalex.org/W2026484633', 'https://openalex.org/W6676391964', 'https://openalex.org/W2107959623', 'https://openalex.org/W2762841950', 'https://openalex.org/W52412328', 'https://openalex.org/W4404957577', 'https://openalex.org/W2605959375', 'https://openalex.org/W6663865628', 'https://openalex.org/W2176118107', 'https://openalex.org/W2129705892', 'https://openalex.org/W2019922577', 'https://openalex.org/W122498424', 'https://openalex.org/W1740027839', 'https://openalex.org/W4318686658', 'https://openalex.org/W6983383401', 'https://openalex.org/W2118599126', 'https://openalex.org/W2160815625', 'https://openalex.org/W6661090959', 'https://openalex.org/W6644252350', 'https://openalex.org/W2999130843', 'https://openalex.org/W6779451415', 'https://openalex.org/W3162021500', 'https://openalex.org/W2133189729', 'https://openalex.org/W1989914796', 'https://openalex.org/W2149784807', 'https://openalex.org/W2742950837', 'https://openalex.org/W2038056950', 'https://openalex.org/W2045343524', 'https://openalex.org/W2068116204', 'https://openalex.org/W6659778137', 'https://openalex.org/W2059824090', 'https://openalex.org/W1975418600', 'https://openalex.org/W2898208057', 'https://openalex.org/W2780264041', 'https://openalex.org/W2132573777', 'https://openalex.org/W2488199425', 'https://openalex.org/W2005311247', 'https://openalex.org/W2140661818', 'https://openalex.org/W2989321827', 'https://openalex.org/W4311481261', 'https://openalex.org/W4379474370', 'https://openalex.org/W3211278025', 'https://openalex.org/W3008675085', 'https://openalex.org/W2114156501', 'https://openalex.org/W6673731931', 'https://openalex.org/W6610742289', 'https://openalex.org/W2074488330', 'https://openalex.org/W6664716616', 'https://openalex.org/W2136653392', 'https://openalex.org/W2165627680', 'https://openalex.org/W6643209160', 'https://openalex.org/W4304465621', 'https://openalex.org/W3093121832', 'https://openalex.org/W2978027383', 'https://openalex.org/W2102870983', 'https://openalex.org/W281094599', 'https://openalex.org/W7043347943', 'https://openalex.org/W4292997377', 'https://openalex.org/W2167834252', 'https://openalex.org/W3092318106', 'https://openalex.org/W2132566726', 'https://openalex.org/W2089720224', 'https://openalex.org/W4288360843', 'https://openalex.org/W6670256387', 'https://openalex.org/W2398490608', 'https://openalex.org/W2768381684', 'https://openalex.org/W2951943862', 'https://openalex.org/W2969145661', 'https://openalex.org/W3082004699', 'https://openalex.org/W4224979832', 'https://openalex.org/W6610556715', 'https://openalex.org/W2010980346', 'https://openalex.org/W6616763113', 'https://openalex.org/W6632820619', 'https://openalex.org/W2136412480', 'https://openalex.org/W6645640999', 'https://openalex.org/W6738652494', 'https://openalex.org/W2395225233', 'https://openalex.org/W2110638188', 'https://openalex.org/W2064135025', 'https://openalex.org/W2963962561', 'https://openalex.org/W4377220931', 'https://openalex.org/W6834868713', 'https://openalex.org/W2160527964', 'https://openalex.org/W2059168261', 'https://openalex.org/W2154600605', 'https://openalex.org/W3163700796', 'https://openalex.org/W6681399687', 'https://openalex.org/W2971977259', 'https://openalex.org/W3084747096', 'https://openalex.org/W6677458792', 'https://openalex.org/W6677158074', 'https://openalex.org/W7064937209', 'https://openalex.org/W6641355812', 'https://openalex.org/W2032476212', 'https://openalex.org/W6601893370', 'https://openalex.org/W2805234167', 'https://openalex.org/W6681346506', 'https://openalex.org/W3190032417', 'https://openalex.org/W6635139342', 'https://openalex.org/W3119308075', 'https://openalex.org/W2089452757', 'https://openalex.org/W2990241049', 'https://openalex.org/W2147740811', 'https://openalex.org/W2037751943', 'https://openalex.org/W2101509422', 'https://openalex.org/W2398830367', 'https://openalex.org/W2058616551', 'https://openalex.org/W6634417500', 'https://openalex.org/W2169868772', 'https://openalex.org/W2917235195', 'https://openalex.org/W1532494781', 'https://openalex.org/W3100385063', 'https://openalex.org/W4200598898', 'https://openalex.org/W6800751262', 'https://openalex.org/W4297677272', 'https://openalex.org/W140261823', 'https://openalex.org/W2162505970', 'https://openalex.org/W2060204180', 'https://openalex.org/W2095223181', 'https://openalex.org/W4221038855', 'https://openalex.org/W4225726571', 'https://openalex.org/W3197259906', 'https://openalex.org/W833103999', 'https://openalex.org/W1705150430', 'https://openalex.org/W2118373646', 'https://openalex.org/W2141038596', 'https://openalex.org/W2595479191', 'https://openalex.org/W3005081886', 'https://openalex.org/W2163999179', 'https://openalex.org/W6642493903', 'https://openalex.org/W6672662947', 'https://openalex.org/W7071849196', 'https://openalex.org/W135984148', 'https://openalex.org/W1540073772', 'https://openalex.org/W6635894473', 'https://openalex.org/W6637818730', 'https://openalex.org/W2784570041', 'https://openalex.org/W2937297214', 'https://openalex.org/W3166292821', 'https://openalex.org/W3134108900', 'https://openalex.org/W6676636377', 'https://openalex.org/W1968225092', 'https://openalex.org/W3023172065', 'https://openalex.org/W2913939165', 'https://openalex.org/W2016292361', 'https://openalex.org/W414516981', 'https://openalex.org/W2149932965', 'https://openalex.org/W2005592929', 'https://openalex.org/W1540332606', 'https://openalex.org/W6655970245', 'https://openalex.org/W1993984071', 'https://openalex.org/W2107917162', 'https://openalex.org/W2395899413', 'https://openalex.org/W4313484307', 'https://openalex.org/W2972337257', 'https://openalex.org/W1995403064', 'https://openalex.org/W2585893098', 'https://openalex.org/W6676456322', 'https://openalex.org/W6623095062', 'https://openalex.org/W3171477941', 'https://openalex.org/W4246559809', 'https://openalex.org/W2125566341', 'https://openalex.org/W2000977437', 'https://openalex.org/W2303872361', 'https://openalex.org/W4320013820', 'https://openalex.org/W4242257761', 'https://openalex.org/W3196595845', 'https://openalex.org/W4243223559', 'https://openalex.org/W2032442824', 'https://openalex.org/W2059146861', 'https://openalex.org/W2995609441', 'https://openalex.org/W2119165475', 'https://openalex.org/W4313255477', 'https://openalex.org/W1597121597', 'https://openalex.org/W2398326651', 'https://openalex.org/W4254816979', 'https://openalex.org/W4288279357', 'https://openalex.org/W1543397219', 'https://openalex.org/W2618602706', 'https://openalex.org/W4297798492', 'https://openalex.org/W821509450', 'https://openalex.org/W4307680525', 'https://openalex.org/W4243618065', 'https://openalex.org/W2120713167', 'https://openalex.org/W4394671563', 'https://openalex.org/W4246867089', 'https://openalex.org/W2131070395', 'https://openalex.org/W4249920809', 'https://openalex.org/W2160997109', 'https://openalex.org/W4297938313', 'https://openalex.org/W4287365906', 'https://openalex.org/W4308231222', 'https://openalex.org/W4237938692', 'https://openalex.org/W4247178956', 'https://openalex.org/W4298742451', 'https://openalex.org/W3036601975', 'https://openalex.org/W2252657604', 'https://openalex.org/W2099164611', 'https://openalex.org/W1559022555', 'https://openalex.org/W2143296986', 'https://openalex.org/W4300721020', 'https://openalex.org/W2973180715', 'https://openalex.org/W3151534266', 'https://openalex.org/W3034905892', 'https://openalex.org/W2163097816', 'https://openalex.org/W4255020641', 'https://openalex.org/W2435103813', 'https://openalex.org/W1981253069', 'https://openalex.org/W4240474221', 'https://openalex.org/W2799770360', 'https://openalex.org/W4210307751', 'https://openalex.org/W2963648280', 'https://openalex.org/W2163028944', 'https://openalex.org/W2996728628', 'https://openalex.org/W3202070718', 'https://openalex.org/W2010188467', 'https://openalex.org/W4319862635', 'https://openalex.org/W3162231828', 'https://openalex.org/W4294955557', 'https://openalex.org/W2533523411', 'https://openalex.org/W3080620940', 'https://openalex.org/W2933138175', 'https://openalex.org/W2973049979', 'https://openalex.org/W3199093330', 'https://openalex.org/W2959443032', 'https://openalex.org/W4292779060', 'https://openalex.org/W4249366912', 'https://openalex.org/W2219249508', 'https://openalex.org/W2041394569', 'https://openalex.org/W4251221781', 'https://openalex.org/W3130041921', 'https://openalex.org/W4225815933', 'https://openalex.org/W2801193150', 'https://openalex.org/W2333023345', 'https://openalex.org/W3126722376', 'https://openalex.org/W3198815374', 'https://openalex.org/W4362220304', 'https://openalex.org/W3133848337', 'https://openalex.org/W4385822336', 'https://openalex.org/W4253001367', 'https://openalex.org/W4245984049', 'https://openalex.org/W4376140088', 'https://openalex.org/W2029008609', 'https://openalex.org/W3212943633', 'https://openalex.org/W4255249122', 'https://openalex.org/W1524333225', 'https://openalex.org/W1925965306', 'https://openalex.org/W3144810982', 'https://openalex.org/W2621934507', 'https://openalex.org/W4245117732', 'https://openalex.org/W2037662195', 'https://openalex.org/W4256695500', 'https://openalex.org/W2182291882', 'https://openalex.org/W4375868953', 'https://openalex.org/W4307320512', 'https://openalex.org/W2164274485', 'https://openalex.org/W2032543155', 'https://openalex.org/W4309419356', 'https://openalex.org/W2803055582', 'https://openalex.org/W4387865047', 'https://openalex.org/W4236000557', 'https://openalex.org/W3036063182', 'https://openalex.org/W2995680346', 'https://openalex.org/W2040344088', 'https://openalex.org/W2963321191', 'https://openalex.org/W1984717236', 'https://openalex.org/W577928986', 'https://openalex.org/W4381786045', 'https://openalex.org/W4243341362', 'https://openalex.org/W2888777493', 'https://openalex.org/W2980877534', 'https://openalex.org/W4385822676', 'https://openalex.org/W3097945073', 'https://openalex.org/W4253926802', 'https://openalex.org/W4292825791', 'https://openalex.org/W1969005071', 'https://openalex.org/W4242738990', 'https://openalex.org/W2972449503', 'https://openalex.org/W166170480', 'https://openalex.org/W2127523122', 'https://openalex.org/W4251435902', 'https://openalex.org/W2058878924', 'https://openalex.org/W4238964169', 'https://openalex.org/W1577624656', 'https://openalex.org/W2065159495', 'https://openalex.org/W4253947715', 'https://openalex.org/W2296607128', 'https://openalex.org/W3197479040', 'https://openalex.org/W2546861836', 'https://openalex.org/W4237154886', 'https://openalex.org/W4288072840', 'https://openalex.org/W4301785137', 'https://openalex.org/W4231114907', 'https://openalex.org/W2519091744', 'https://openalex.org/W1719717336', 'https://openalex.org/W2044321477', 'https://openalex.org/W1972102750', 'https://openalex.org/W4297808394', 'https://openalex.org/W2020944885', 'https://openalex.org/W4323066695', 'https://openalex.org/W1558150890', 'https://openalex.org/W4283332789', 'https://openalex.org/W2323385789', 'https://openalex.org/W2115099665', 'https://openalex.org/W2123599888', 'https://openalex.org/W4230806239', 'https://openalex.org/W2160464066', 'https://openalex.org/W4232589384', 'https://openalex.org/W2964052309', 'https://openalex.org/W4230804341', 'https://openalex.org/W1533561824', 'https://openalex.org/W4230637005', 'https://openalex.org/W2187824139', 'https://openalex.org/W2108582985', 'https://openalex.org/W4252366034', 'https://openalex.org/W2058354688', 'https://openalex.org/W4366994024', 'https://openalex.org/W4385822936', 'https://openalex.org/W3015944949', 'https://openalex.org/W2070653320', 'https://openalex.org/W2003341094', 'https://openalex.org/W2963604492', 'https://openalex.org/W4385823426', 'https://openalex.org/W4235338493', 'https://openalex.org/W2126377586', 'https://openalex.org/W4283762111', 'https://openalex.org/W3146245645', 'https://openalex.org/W2060238187', 'https://openalex.org/W4250832892', 'https://openalex.org/W4246103655', 'https://openalex.org/W4242334097', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015877095', 'https://openalex.org/W2015075592', 'https://openalex.org/W4251965084', 'https://openalex.org/W2110221456', 'https://openalex.org/W4307979480', 'https://openalex.org/W3095410713', 'https://openalex.org/W1964575515', 'https://openalex.org/W2618478924', 'https://openalex.org/W2115975321', 'https://openalex.org/W3195577433', 'https://openalex.org/W2037752261', 'https://openalex.org/W2343593471', 'https://openalex.org/W3177829661', 'https://openalex.org/W2571532437', 'https://openalex.org/W4290673677', 'https://openalex.org/W2169403152', 'https://openalex.org/W3016181583', 'https://openalex.org/W4286984129', 'https://openalex.org/W1967834254', 'https://openalex.org/W2964054038', 'https://openalex.org/W2950416202', 'https://openalex.org/W2615444509', 'https://openalex.org/W1980862600', 'https://openalex.org/W3047246203', 'https://openalex.org/W4253971549', 'https://openalex.org/W4287591426', 'https://openalex.org/W2054948443', 'https://openalex.org/W3015783745', 'https://openalex.org/W2728435982']",2023-10-22
https://openalex.org/W4403780681,https://doi.org/10.1145/3664647.3681695,VoiceTuner: Self-Supervised Pre-training and Efficient Fine-tuning For Voice Generation,,"['https://openalex.org/W3209984917', 'https://openalex.org/W4375869257', 'https://openalex.org/W3209059054', 'https://openalex.org/W6840200333', 'https://openalex.org/W4285345683', 'https://openalex.org/W3206191467', 'https://openalex.org/W2995181338', 'https://openalex.org/W3129009457', 'https://openalex.org/W3158762648', 'https://openalex.org/W2981852735', 'https://openalex.org/W2131738223', 'https://openalex.org/W3215615641', 'https://openalex.org/W3203407300', 'https://openalex.org/W4285483774', 'https://openalex.org/W4288089799', 'https://openalex.org/W4394671563']",2024-10-26
https://openalex.org/W4405138899,https://doi.org/10.1007/978-981-96-0917-8_8,EmoTalker: Audio Driven Emotion Aware Talking Head Generation,,"['https://openalex.org/W4303448003', 'https://openalex.org/W3107666850', 'https://openalex.org/W2963081548', 'https://openalex.org/W2944294033', 'https://openalex.org/W4310379947', 'https://openalex.org/W2604379605', 'https://openalex.org/W2964449965', 'https://openalex.org/W3180355996', 'https://openalex.org/W4200630629', 'https://openalex.org/W3211147706', 'https://openalex.org/W4390872428', 'https://openalex.org/W3209059054', 'https://openalex.org/W4281730245', 'https://openalex.org/W3174763799', 'https://openalex.org/W4385574033', 'https://openalex.org/W3129009457', 'https://openalex.org/W4312444931', 'https://openalex.org/W6602670149', 'https://openalex.org/W4382469130', 'https://openalex.org/W4312309978', 'https://openalex.org/W2107037917', 'https://openalex.org/W3140429000', 'https://openalex.org/W3081492798', 'https://openalex.org/W1981276685', 'https://openalex.org/W4312933868', 'https://openalex.org/W2997336602', 'https://openalex.org/W2752796333', 'https://openalex.org/W2979894294', 'https://openalex.org/W3099284785', 'https://openalex.org/W3187364420', 'https://openalex.org/W2884460600', 'https://openalex.org/W3207849023', 'https://openalex.org/W4386076250', 'https://openalex.org/W3197580070', 'https://openalex.org/W6604204020', 'https://openalex.org/W4386076288', 'https://openalex.org/W4386072021', 'https://openalex.org/W2963290645', 'https://openalex.org/W3186090335', 'https://openalex.org/W3208601549', 'https://openalex.org/W3101631197']",2024-12-07
https://openalex.org/W4405796378,https://doi.org/10.1007/978-981-96-1045-7_19,Investigation on Training Strategy for Cross-Modal Large Language Models with Speech and Text,,"['https://openalex.org/W4381786045', 'https://openalex.org/W4389518827', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963571336', 'https://openalex.org/W6601899773', 'https://openalex.org/W6852781825', 'https://openalex.org/W3209059054', 'https://openalex.org/W2995181338', 'https://openalex.org/W3129009457', 'https://openalex.org/W4392904805', 'https://openalex.org/W2692059227', 'https://openalex.org/W2973049979', 'https://openalex.org/W2752796333', 'https://openalex.org/W2996728628', 'https://openalex.org/W3215615641', 'https://openalex.org/W4389524500']",2024-12-26
https://openalex.org/W4406132022,https://doi.org/10.1111/desc.13606,Simulating Early Phonetic and Word Learning Without Linguistic Categories,"ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.","['https://openalex.org/W2529194139', 'https://openalex.org/W4283453659', 'https://openalex.org/W4296710617', 'https://openalex.org/W2099164611', 'https://openalex.org/W2995929068', 'https://openalex.org/W3193068792', 'https://openalex.org/W3202070718', 'https://openalex.org/W2063303346', 'https://openalex.org/W122673323', 'https://openalex.org/W3130041921', 'https://openalex.org/W4229781645', 'https://openalex.org/W6811685372', 'https://openalex.org/W2032442824', 'https://openalex.org/W4221038855', 'https://openalex.org/W4225815933', 'https://openalex.org/W1532704504', 'https://openalex.org/W4256675988', 'https://openalex.org/W4225726571', 'https://openalex.org/W4295308567', 'https://openalex.org/W2483390977', 'https://openalex.org/W2037662195', 'https://openalex.org/W2110485445', 'https://openalex.org/W6676391964', 'https://openalex.org/W2107038463', 'https://openalex.org/W3199093330', 'https://openalex.org/W6800775014', 'https://openalex.org/W2396361046', 'https://openalex.org/W2148764920', 'https://openalex.org/W6682122378', 'https://openalex.org/W4401117643', 'https://openalex.org/W2126377586', 'https://openalex.org/W2160727679', 'https://openalex.org/W2037525070', 'https://openalex.org/W2991557631', 'https://openalex.org/W1995422333', 'https://openalex.org/W1579929927', 'https://openalex.org/W3140196993', 'https://openalex.org/W1989914796', 'https://openalex.org/W2149784807', 'https://openalex.org/W2038056950', 'https://openalex.org/W2037752261', 'https://openalex.org/W4206039057', 'https://openalex.org/W1597121597', 'https://openalex.org/W6635894473', 'https://openalex.org/W2014307400', 'https://openalex.org/W6653853028', 'https://openalex.org/W2103091632', 'https://openalex.org/W1984717236', 'https://openalex.org/W7037316554', 'https://openalex.org/W2063525438', 'https://openalex.org/W3129009457', 'https://openalex.org/W4366994024', 'https://openalex.org/W4391669171', 'https://openalex.org/W4379474370', 'https://openalex.org/W4385822936', 'https://openalex.org/W2741692265', 'https://openalex.org/W4232693094', 'https://openalex.org/W6651343493', 'https://openalex.org/W2068247585', 'https://openalex.org/W6667887468', 'https://openalex.org/W2026992087', 'https://openalex.org/W2251025892', 'https://openalex.org/W2074488330', 'https://openalex.org/W2136653392', 'https://openalex.org/W4313255477', 'https://openalex.org/W6808618455', 'https://openalex.org/W2776941264', 'https://openalex.org/W2167834252', 'https://openalex.org/W3110458199', 'https://openalex.org/W4221140961', 'https://openalex.org/W4292825791', 'https://openalex.org/W2842511635', 'https://openalex.org/W4297808394', 'https://openalex.org/W2113332115', 'https://openalex.org/W2078828996', 'https://openalex.org/W6670256387', 'https://openalex.org/W4395036961', 'https://openalex.org/W6631362777', 'https://openalex.org/W2119885245', 'https://openalex.org/W3005511757', 'https://openalex.org/W3016181583', 'https://openalex.org/W1993984071', 'https://openalex.org/W1980862600', 'https://openalex.org/W6645640999', 'https://openalex.org/W2029008609', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W1944583518', 'https://openalex.org/W6640915429', 'https://openalex.org/W2163097816', 'https://openalex.org/W2119165475', 'https://openalex.org/W6632622110', 'https://openalex.org/W2108582985', 'https://openalex.org/W2003341094', 'https://openalex.org/W1925965306', 'https://openalex.org/W6640227979', 'https://openalex.org/W4391428296', 'https://openalex.org/W2101509422', 'https://openalex.org/W3146245645', 'https://openalex.org/W4235199064', 'https://openalex.org/W1974052916', 'https://openalex.org/W2082256905', 'https://openalex.org/W593365102', 'https://openalex.org/W4394671563', 'https://openalex.org/W2468383091']",2025-01-06
https://openalex.org/W4409167067,https://doi.org/10.1007/978-3-031-88717-8_2,CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval,,"['https://openalex.org/W4225323055', 'https://openalex.org/W2995929068', 'https://openalex.org/W3135057108', 'https://openalex.org/W3198528147', 'https://openalex.org/W3092085609', 'https://openalex.org/W4385573236', 'https://openalex.org/W4226120743', 'https://openalex.org/W3096109555', 'https://openalex.org/W3035390927', 'https://openalex.org/W4319862635', 'https://openalex.org/W4241900798', 'https://openalex.org/W3039695075', 'https://openalex.org/W3100801259', 'https://openalex.org/W2064675550', 'https://openalex.org/W3209059054', 'https://openalex.org/W3171927989', 'https://openalex.org/W6602558577', 'https://openalex.org/W4286359908', 'https://openalex.org/W3129009457', 'https://openalex.org/W4311137818', 'https://openalex.org/W4214825215', 'https://openalex.org/W3096471021', 'https://openalex.org/W4281492411', 'https://openalex.org/W4385573385', 'https://openalex.org/W2742097923', 'https://openalex.org/W4375869174', 'https://openalex.org/W3135367836', 'https://openalex.org/W3035269921', 'https://openalex.org/W2003154243', 'https://openalex.org/W2964243274', 'https://openalex.org/W4319862477', 'https://openalex.org/W3209371554', 'https://openalex.org/W4379929801', 'https://openalex.org/W4291920479', 'https://openalex.org/W4385571103', 'https://openalex.org/W4287890956', 'https://openalex.org/W4385573012']",2025-01-01
https://openalex.org/W4411727068,https://doi.org/10.1109/icitiit64777.2025.11041138,Linguify: An AI-Powered System for Real-Time Audio Translation Across Language,,"['https://openalex.org/W4385245566', 'https://openalex.org/W6727690538', 'https://openalex.org/W2964243274', 'https://openalex.org/W3213029956', 'https://openalex.org/W3033411150', 'https://openalex.org/W2806311723', 'https://openalex.org/W4225905768', 'https://openalex.org/W3180374548', 'https://openalex.org/W3129009457', 'https://openalex.org/W6780218876', 'https://openalex.org/W6870059087', 'https://openalex.org/W4402683093', 'https://openalex.org/W6874544534', 'https://openalex.org/W6852381208', 'https://openalex.org/W4402684152']",2025-02-21
https://openalex.org/W4413343866,https://doi.org/10.2139/ssrn.5398167,Is Self-Supervised Learning Enough to Fill in the Gap? A Study on Speech Inpainting,,"['https://openalex.org/W4405269872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297677272', 'https://openalex.org/W4200631896', 'https://openalex.org/W4322766928', 'https://openalex.org/W3209984917', 'https://openalex.org/W4406417959', 'https://openalex.org/W4401597646', 'https://openalex.org/W4401597532', 'https://openalex.org/W4380714544', 'https://openalex.org/W2131686285', 'https://openalex.org/W2096375888', 'https://openalex.org/W2145103350', 'https://openalex.org/W6677390740', 'https://openalex.org/W3105728642', 'https://openalex.org/W3169320628', 'https://openalex.org/W4386076005', 'https://openalex.org/W2999130843', 'https://openalex.org/W4308505129', 'https://openalex.org/W2581073931', 'https://openalex.org/W2995181338', 'https://openalex.org/W6631190155', 'https://openalex.org/W4323322997', 'https://openalex.org/W2106748815', 'https://openalex.org/W3092028330', 'https://openalex.org/W1561829009', 'https://openalex.org/W3129009457', 'https://openalex.org/W2243752967', 'https://openalex.org/W4391307345', 'https://openalex.org/W3160450938', 'https://openalex.org/W2143954638', 'https://openalex.org/W4245919820', 'https://openalex.org/W2799789537', 'https://openalex.org/W6764473457', 'https://openalex.org/W4281492411', 'https://openalex.org/W6784121926', 'https://openalex.org/W4308831579', 'https://openalex.org/W4308669694', 'https://openalex.org/W3178638614', 'https://openalex.org/W2137539246', 'https://openalex.org/W4404160901', 'https://openalex.org/W2556910157', 'https://openalex.org/W4311000453', 'https://openalex.org/W1552314771', 'https://openalex.org/W2132036212', 'https://openalex.org/W3011573174', 'https://openalex.org/W4200483526', 'https://openalex.org/W2067295501', 'https://openalex.org/W932459478', 'https://openalex.org/W6678441402', 'https://openalex.org/W2168510624', 'https://openalex.org/W2962866211', 'https://openalex.org/W3207100935', 'https://openalex.org/W4322716825', 'https://openalex.org/W4391021786', 'https://openalex.org/W4408697618', 'https://openalex.org/W4386764852', 'https://openalex.org/W3043119899', 'https://openalex.org/W2163324046', 'https://openalex.org/W4402111969', 'https://openalex.org/W4391307838', 'https://openalex.org/W4221140119', 'https://openalex.org/W4310609216', 'https://openalex.org/W4372260323', 'https://openalex.org/W4296068407', 'https://openalex.org/W4297841545', 'https://openalex.org/W4296068992', 'https://openalex.org/W4394671563', 'https://openalex.org/W2998572311', 'https://openalex.org/W151350810', 'https://openalex.org/W4320013936', 'https://openalex.org/W3161904430', 'https://openalex.org/W4225680573', 'https://openalex.org/W3024200358', 'https://openalex.org/W4406137536', 'https://openalex.org/W4381786045', 'https://openalex.org/W4372266927', 'https://openalex.org/W4385823130', 'https://openalex.org/W4375869259', 'https://openalex.org/W4297841420', 'https://openalex.org/W2973049979', 'https://openalex.org/W4205169642', 'https://openalex.org/W4221161734', 'https://openalex.org/W4385822421', 'https://openalex.org/W2122901349', 'https://openalex.org/W2114925438', 'https://openalex.org/W2981283774', 'https://openalex.org/W4386764866', 'https://openalex.org/W3209059054', 'https://openalex.org/W1522301498', 'https://openalex.org/W4375868986', 'https://openalex.org/W3203491020', 'https://openalex.org/W2998498479', 'https://openalex.org/W3140429000', 'https://openalex.org/W4296069296']",2025-01-01
https://openalex.org/W4366123788,https://doi.org/10.1007/s10462-023-10488-2,"A comprehensive survey on image captioning: from handcrafted to deep learning-based techniques, a taxonomy and open research issues",,"['https://openalex.org/W3045086295', 'https://openalex.org/W3112404983', 'https://openalex.org/W2506483933', 'https://openalex.org/W2963686907', 'https://openalex.org/W2803259101', 'https://openalex.org/W3135254306', 'https://openalex.org/W2282219577', 'https://openalex.org/W4283737877', 'https://openalex.org/W4281760686', 'https://openalex.org/W3134272453', 'https://openalex.org/W1895989618', 'https://openalex.org/W3140792177', 'https://openalex.org/W3034655362', 'https://openalex.org/W2962968835', 'https://openalex.org/W2105103432', 'https://openalex.org/W2977970283', 'https://openalex.org/W3155217823', 'https://openalex.org/W2250379249', 'https://openalex.org/W6630506854', 'https://openalex.org/W1931639407', 'https://openalex.org/W4313131769', 'https://openalex.org/W1897761818', 'https://openalex.org/W2168356304', 'https://openalex.org/W2558834163', 'https://openalex.org/W2965846473', 'https://openalex.org/W1734113335', 'https://openalex.org/W1969616664', 'https://openalex.org/W92662927', 'https://openalex.org/W2740118378', 'https://openalex.org/W3035463379', 'https://openalex.org/W3035160838', 'https://openalex.org/W3217070527', 'https://openalex.org/W68733909', 'https://openalex.org/W2896348597', 'https://openalex.org/W3158091094', 'https://openalex.org/W3128640753', 'https://openalex.org/W3047058320', 'https://openalex.org/W3015625772', 'https://openalex.org/W2986670728', 'https://openalex.org/W2508429489', 'https://openalex.org/W2220981600', 'https://openalex.org/W3137069976', 'https://openalex.org/W3045147787', 'https://openalex.org/W3137679908', 'https://openalex.org/W2981165461', 'https://openalex.org/W2481240925', 'https://openalex.org/W3216475702', 'https://openalex.org/W2984138079', 'https://openalex.org/W2296385829', 'https://openalex.org/W2749708282', 'https://openalex.org/W2963527096', 'https://openalex.org/W2914306086', 'https://openalex.org/W2968660381', 'https://openalex.org/W3046260628', 'https://openalex.org/W3192652975', 'https://openalex.org/W2990818246', 'https://openalex.org/W2418300416', 'https://openalex.org/W2317752800', 'https://openalex.org/W1861492603', 'https://openalex.org/W3106925514', 'https://openalex.org/W2805954242', 'https://openalex.org/W3125154076', 'https://openalex.org/W2963630207', 'https://openalex.org/W2965359408', 'https://openalex.org/W2949376505', 'https://openalex.org/W2575842049', 'https://openalex.org/W2795151422', 'https://openalex.org/W3046675509', 'https://openalex.org/W2131179926', 'https://openalex.org/W3193528885', 'https://openalex.org/W3153200530', 'https://openalex.org/W2509490957', 'https://openalex.org/W3035284526', 'https://openalex.org/W2101105183', 'https://openalex.org/W3208647617', 'https://openalex.org/W2607579284', 'https://openalex.org/W2032699694', 'https://openalex.org/W2560645892', 'https://openalex.org/W2965697393', 'https://openalex.org/W2277195237', 'https://openalex.org/W2985745757', 'https://openalex.org/W2607151106', 'https://openalex.org/W3034316193', 'https://openalex.org/W2604178507', 'https://openalex.org/W2149557440', 'https://openalex.org/W3038038411', 'https://openalex.org/W2753232960', 'https://openalex.org/W6604922321', 'https://openalex.org/W2197223256', 'https://openalex.org/W2964248669', 'https://openalex.org/W1956340063', 'https://openalex.org/W2044626341', 'https://openalex.org/W1895577753', 'https://openalex.org/W3117344638', 'https://openalex.org/W3006487741', 'https://openalex.org/W3199287825', 'https://openalex.org/W2801271919', 'https://openalex.org/W2989377923', 'https://openalex.org/W2607768201', 'https://openalex.org/W3018388102', 'https://openalex.org/W2963101956', 'https://openalex.org/W2990069284', 'https://openalex.org/W2890531016', 'https://openalex.org/W2983141445', 'https://openalex.org/W2251583212', 'https://openalex.org/W2887272576', 'https://openalex.org/W2963778889', 'https://openalex.org/W2997056851', 'https://openalex.org/W3188057041', 'https://openalex.org/W3009270862', 'https://openalex.org/W3167939936', 'https://openalex.org/W3110761028', 'https://openalex.org/W3127255516', 'https://openalex.org/W3035323998', 'https://openalex.org/W3217347476', 'https://openalex.org/W3103022576', 'https://openalex.org/W3102566412', 'https://openalex.org/W3097754216']",2023-04-17
https://openalex.org/W4396510248,https://doi.org/10.3390/electronics13091726,"Recent Advances in Synthesis and Interaction of Speech, Text, and Vision","In recent years, there has been increasing interest in the conversion of images into audio descriptions. This is a field that lies at the intersection of Computer Vision (CV) and Natural Language Processing (NLP), and it involves various tasks, including creating textual descriptions of images and converting them directly into auditory representations. Another aspect of this field is the synthesis of natural speech from text. This has significant potential to improve accessibility, user experience, and the applications of Artificial Intelligence (AI). In this article, we reviewed a wide range of image-to-audio conversion techniques. Various aspects of image captioning, speech synthesis, and direct image-to-speech conversion have been explored, from fundamental encoder–decoder architectures to more advanced methods such as transformers and adversarial learning. Although the focus of this review is on synthesizing audio descriptions from visual data, the reverse task of creating visual content from natural language descriptions is also covered. This study provides a comprehensive overview of the techniques and methodologies used in these fields and highlights the strengths and weaknesses of each approach. The study emphasizes the importance of various datasets, such as MS COCO, LibriTTS, and VizWiz Captions, which play a critical role in training models, evaluating them, promoting inclusivity, and solving real-world problems. The implications for the future suggest the potential of generating more natural and contextualized audio descriptions, whereas direct image-to-speech tasks provide opportunities for intuitive auditory representations of visual content.","['https://openalex.org/W4299541593', 'https://openalex.org/W2112955368', 'https://openalex.org/W2885959306', 'https://openalex.org/W6765342010', 'https://openalex.org/W3198196812', 'https://openalex.org/W4230559685', 'https://openalex.org/W2996011278', 'https://openalex.org/W2169153920', 'https://openalex.org/W2001538905', 'https://openalex.org/W4292455922', 'https://openalex.org/W2463955103', 'https://openalex.org/W3160100917', 'https://openalex.org/W3165571294', 'https://openalex.org/W3122720459', 'https://openalex.org/W1514535095', 'https://openalex.org/W6739901393', 'https://openalex.org/W6770800577', 'https://openalex.org/W2607151106', 'https://openalex.org/W2963084599', 'https://openalex.org/W2902440562', 'https://openalex.org/W2962968835', 'https://openalex.org/W2604178507', 'https://openalex.org/W3018385214', 'https://openalex.org/W2901988662', 'https://openalex.org/W6640200569', 'https://openalex.org/W1570629387', 'https://openalex.org/W2056191164', 'https://openalex.org/W2150658333', 'https://openalex.org/W133559434', 'https://openalex.org/W1600722501', 'https://openalex.org/W2154920538', 'https://openalex.org/W200094172', 'https://openalex.org/W2145575463', 'https://openalex.org/W175280642', 'https://openalex.org/W344150399', 'https://openalex.org/W3142087749', 'https://openalex.org/W2129142580', 'https://openalex.org/W2102003408', 'https://openalex.org/W2134973740', 'https://openalex.org/W2294797155', 'https://openalex.org/W2806733747', 'https://openalex.org/W1576227399', 'https://openalex.org/W2515943672', 'https://openalex.org/W2963609956', 'https://openalex.org/W3197407562', 'https://openalex.org/W3095990227', 'https://openalex.org/W6748588790', 'https://openalex.org/W3174311593', 'https://openalex.org/W2997208288', 'https://openalex.org/W2969302028', 'https://openalex.org/W2113899393', 'https://openalex.org/W4206865574', 'https://openalex.org/W3155217823', 'https://openalex.org/W3206387123', 'https://openalex.org/W3095909497', 'https://openalex.org/W4389959945', 'https://openalex.org/W3007780968', 'https://openalex.org/W3203245760', 'https://openalex.org/W3199400376', 'https://openalex.org/W4289600032', 'https://openalex.org/W6676497082', 'https://openalex.org/W68733909', 'https://openalex.org/W2185175083', 'https://openalex.org/W1861492603', 'https://openalex.org/W1895989618', 'https://openalex.org/W2963062932', 'https://openalex.org/W2886641317', 'https://openalex.org/W3110019360', 'https://openalex.org/W3095670406', 'https://openalex.org/W3106859150', 'https://openalex.org/W6917585676', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972359262', 'https://openalex.org/W3197824033', 'https://openalex.org/W4297570641', 'https://openalex.org/W3198533616', 'https://openalex.org/W3204379680', 'https://openalex.org/W3155727213', 'https://openalex.org/W2963902314', 'https://openalex.org/W3197064454', 'https://openalex.org/W2962862718', 'https://openalex.org/W4385245566', 'https://openalex.org/W3008618223', 'https://openalex.org/W4320013936', 'https://openalex.org/W2788357188', 'https://openalex.org/W2109586012']",2024-04-30
https://openalex.org/W3204420730,https://doi.org/10.1109/icassp43922.2022.9746421,Visualtts: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over,"In this paper, we formulate a novel task to synthesize speech in sync with a silent pre-recorded video, denoted as automatic voice over (AVO). Unlike traditional speech synthesis, AVO seeks to generate not only human-sounding speech, but also perfect lip-speech synchronization. A natural solution to AVO is to condition the speech rendering on the temporal progression of lip sequence in the video. We propose a novel text-to-speech model that is conditioned on visual input, named VisualTTS, for accurate lip-speech synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1) textual-visual attention, and 2) visual fusion strategy during acoustic decoding, which both contribute to forming accurate alignment between the input text content and lip motion in input lip sequence. Experimental results show that VisualTTS achieves accurate lip-speech synchronization and outperforms all baseline systems.","['https://openalex.org/W3098557217', 'https://openalex.org/W6735927292', 'https://openalex.org/W3081492798', 'https://openalex.org/W6748409065', 'https://openalex.org/W6754392867', 'https://openalex.org/W4289665794', 'https://openalex.org/W6739901393', 'https://openalex.org/W3163287738', 'https://openalex.org/W2015143272', 'https://openalex.org/W6787100281', 'https://openalex.org/W3155217823', 'https://openalex.org/W3035626590', 'https://openalex.org/W6794378236', 'https://openalex.org/W6754420807', 'https://openalex.org/W2903739847', 'https://openalex.org/W2046056978', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6754473786', 'https://openalex.org/W6763832098', 'https://openalex.org/W3015826515', 'https://openalex.org/W2963945466', 'https://openalex.org/W2963609956', 'https://openalex.org/W3168542456', 'https://openalex.org/W3084396630', 'https://openalex.org/W3168662520', 'https://openalex.org/W3095379519', 'https://openalex.org/W2963522141', 'https://openalex.org/W6732872814', 'https://openalex.org/W2964171275', 'https://openalex.org/W3008400075', 'https://openalex.org/W3130016944', 'https://openalex.org/W3157840621', 'https://openalex.org/W2578229578', 'https://openalex.org/W3114436296', 'https://openalex.org/W2889048668', 'https://openalex.org/W2949382160', 'https://openalex.org/W2970730223', 'https://openalex.org/W3101631197', 'https://openalex.org/W2527729766', 'https://openalex.org/W2890952074', 'https://openalex.org/W2891205112', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963403868', 'https://openalex.org/W3123318516', 'https://openalex.org/W2964307104', 'https://openalex.org/W2519091744', 'https://openalex.org/W2946200149', 'https://openalex.org/W3033411150', 'https://openalex.org/W3174311593', 'https://openalex.org/W2604379605', 'https://openalex.org/W4385245566', 'https://openalex.org/W4298112588']",2022-04-27
https://openalex.org/W4394933221,https://doi.org/10.1007/s13735-024-00328-6,Domain-specific image captioning: a comprehensive review,,"['https://openalex.org/W3045086295', 'https://openalex.org/W3112404983', 'https://openalex.org/W2506483933', 'https://openalex.org/W2803259101', 'https://openalex.org/W4296139711', 'https://openalex.org/W1528802670', 'https://openalex.org/W2282219577', 'https://openalex.org/W3134272453', 'https://openalex.org/W2550553598', 'https://openalex.org/W4390872240', 'https://openalex.org/W3140792177', 'https://openalex.org/W2962968835', 'https://openalex.org/W4321080401', 'https://openalex.org/W4213207626', 'https://openalex.org/W2152772232', 'https://openalex.org/W4387777447', 'https://openalex.org/W3114308765', 'https://openalex.org/W3155217823', 'https://openalex.org/W4381094892', 'https://openalex.org/W2250379249', 'https://openalex.org/W1897761818', 'https://openalex.org/W2156830856', 'https://openalex.org/W4283360854', 'https://openalex.org/W92662927', 'https://openalex.org/W146900863', 'https://openalex.org/W68733909', 'https://openalex.org/W2896348597', 'https://openalex.org/W3128640753', 'https://openalex.org/W3047058320', 'https://openalex.org/W3015625772', 'https://openalex.org/W3003478339', 'https://openalex.org/W2220981600', 'https://openalex.org/W4285265382', 'https://openalex.org/W2770165365', 'https://openalex.org/W2995225687', 'https://openalex.org/W1905882502', 'https://openalex.org/W3216475702', 'https://openalex.org/W1969616664', 'https://openalex.org/W2767780846', 'https://openalex.org/W3008515501', 'https://openalex.org/W3192652975', 'https://openalex.org/W3046260628', 'https://openalex.org/W1861492603', 'https://openalex.org/W3036848992', 'https://openalex.org/W2949376505', 'https://openalex.org/W2805954242', 'https://openalex.org/W2995904231', 'https://openalex.org/W2779054585', 'https://openalex.org/W3046675509', 'https://openalex.org/W3005983418', 'https://openalex.org/W4319300501', 'https://openalex.org/W2963109634', 'https://openalex.org/W3193528885', 'https://openalex.org/W3138298063', 'https://openalex.org/W2101105183', 'https://openalex.org/W2607579284', 'https://openalex.org/W3208647617', 'https://openalex.org/W2032699694', 'https://openalex.org/W2510520237', 'https://openalex.org/W4324129116', 'https://openalex.org/W2883105896', 'https://openalex.org/W4366123788', 'https://openalex.org/W4387193636', 'https://openalex.org/W2886641317', 'https://openalex.org/W3042724941', 'https://openalex.org/W2982260276', 'https://openalex.org/W2149557440', 'https://openalex.org/W4285197428', 'https://openalex.org/W6730747955', 'https://openalex.org/W3038038411', 'https://openalex.org/W2197223256', 'https://openalex.org/W1956340063', 'https://openalex.org/W2044626341', 'https://openalex.org/W1895577753', 'https://openalex.org/W3006487741', 'https://openalex.org/W2339652278', 'https://openalex.org/W2989377923', 'https://openalex.org/W3117344638', 'https://openalex.org/W4200272616', 'https://openalex.org/W2560920409', 'https://openalex.org/W3093225449', 'https://openalex.org/W2890718122', 'https://openalex.org/W4214587440', 'https://openalex.org/W2251583212', 'https://openalex.org/W2997056851', 'https://openalex.org/W3188057041', 'https://openalex.org/W3048500491', 'https://openalex.org/W2973586224', 'https://openalex.org/W2963409068', 'https://openalex.org/W3213119051', 'https://openalex.org/W3110761028', 'https://openalex.org/W4327573095', 'https://openalex.org/W2962938439', 'https://openalex.org/W3217347476', 'https://openalex.org/W3103022576', 'https://openalex.org/W3100245404', 'https://openalex.org/W3097754216', 'https://openalex.org/W3098325931']",2024-04-18
https://openalex.org/W3206387123,https://doi.org/10.1109/taslp.2021.3120644,Synthesizing Spoken Descriptions of Images,"Image captioning technology has great potential in many scenarios. However, current text-based image captioning methods cannot be applied to approximately half of the world's languages due to these languages’ lack of a written form. To solve this problem, recently the image-to-speech task was proposed, which generates spoken descriptions of images bypassing any text via an intermediate representation consisting of phonemes (image-to-phoneme). Here, we present a comprehensive study on the image-to-speech task in which, 1) several representative image-to-text generation methods are implemented for the image-to-phoneme task, 2) objective metrics are sought to evaluate the image-to-phoneme task, and 3) an end-to-end image-to-speech model that is able to synthesize spoken descriptions of images bypassing both text and phonemes is proposed. Extensive experiments are conducted on the public benchmark database Flickr8k. Results of our experiments demonstrate that 1) State-of-the-art image-to-text models can perform well on the image-to-phoneme task, and 2) several evaluation metrics, including BLEU3, BLEU4, BLEU5, and ROUGE-L can be used to evaluate image-to-phoneme performance. Finally, 3) end-to-end image-to-speech bypassing text and phonemes is feasible.","['https://openalex.org/W3123798147', 'https://openalex.org/W3095030004', 'https://openalex.org/W3005578234', 'https://openalex.org/W2950133079', 'https://openalex.org/W2988907666', 'https://openalex.org/W2971709506', 'https://openalex.org/W6795412237', 'https://openalex.org/W3027851323', 'https://openalex.org/W2796315435', 'https://openalex.org/W6794343169', 'https://openalex.org/W6917585676', 'https://openalex.org/W6791006397', 'https://openalex.org/W6775036401', 'https://openalex.org/W6763832098', 'https://openalex.org/W2963902314', 'https://openalex.org/W6778823374', 'https://openalex.org/W6763643401', 'https://openalex.org/W3097538987', 'https://openalex.org/W2185175083', 'https://openalex.org/W6729977899', 'https://openalex.org/W2603567530', 'https://openalex.org/W2907262790', 'https://openalex.org/W2895420168', 'https://openalex.org/W6630875275', 'https://openalex.org/W1895577753', 'https://openalex.org/W2575842049', 'https://openalex.org/W1905882502', 'https://openalex.org/W2965846473', 'https://openalex.org/W2803259101', 'https://openalex.org/W6739901393', 'https://openalex.org/W2745461083', 'https://openalex.org/W2108598243', 'https://openalex.org/W2277195237', 'https://openalex.org/W6621543089', 'https://openalex.org/W1494198834', 'https://openalex.org/W2888867175', 'https://openalex.org/W6631362777', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964243274', 'https://openalex.org/W2997591391', 'https://openalex.org/W4206865574', 'https://openalex.org/W6749352598', 'https://openalex.org/W6787100281', 'https://openalex.org/W6770596778', 'https://openalex.org/W3155217823', 'https://openalex.org/W3096249149', 'https://openalex.org/W68733909', 'https://openalex.org/W2131179926', 'https://openalex.org/W2062955551', 'https://openalex.org/W1987835821', 'https://openalex.org/W2986670728', 'https://openalex.org/W2990818246', 'https://openalex.org/W2963084599', 'https://openalex.org/W3034655362', 'https://openalex.org/W6751792830', 'https://openalex.org/W2963992143', 'https://openalex.org/W6637373629', 'https://openalex.org/W3035284526', 'https://openalex.org/W1956340063', 'https://openalex.org/W6682631176', 'https://openalex.org/W6802821423', 'https://openalex.org/W2962862718', 'https://openalex.org/W2795151422', 'https://openalex.org/W6620707391', 'https://openalex.org/W6678262379', 'https://openalex.org/W6898505805', 'https://openalex.org/W1686810756', 'https://openalex.org/W2613718673', 'https://openalex.org/W2123301721', 'https://openalex.org/W3204007566', 'https://openalex.org/W3158565912', 'https://openalex.org/W648786980', 'https://openalex.org/W2971310675', 'https://openalex.org/W3102219307', 'https://openalex.org/W4288329833', 'https://openalex.org/W3132706255', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970730223', 'https://openalex.org/W2101105183', 'https://openalex.org/W3174311593', 'https://openalex.org/W3035160838', 'https://openalex.org/W1514535095', 'https://openalex.org/W2556930864', 'https://openalex.org/W854541894', 'https://openalex.org/W2963260927', 'https://openalex.org/W2949382160', 'https://openalex.org/W3033411150', 'https://openalex.org/W2995680346', 'https://openalex.org/W4385245566', 'https://openalex.org/W2519091744', 'https://openalex.org/W3130016944', 'https://openalex.org/W2154652894', 'https://openalex.org/W3114436296', 'https://openalex.org/W2968831808', 'https://openalex.org/W639708223', 'https://openalex.org/W3161204797', 'https://openalex.org/W2946200149', 'https://openalex.org/W4287324741', 'https://openalex.org/W1524333225']",2021-01-01
https://openalex.org/W4220653738,https://doi.org/10.1007/s10489-022-03227-7,Speech synthesis with face embeddings,,"['https://openalex.org/W2573123177', 'https://openalex.org/W3035574064', 'https://openalex.org/W2808713080', 'https://openalex.org/W2898284445', 'https://openalex.org/W1536822605', 'https://openalex.org/W2166563626', 'https://openalex.org/W1981276685', 'https://openalex.org/W1975077471', 'https://openalex.org/W2897024124', 'https://openalex.org/W2901418702', 'https://openalex.org/W3111065871', 'https://openalex.org/W3193498964', 'https://openalex.org/W3198062544', 'https://openalex.org/W3011997138', 'https://openalex.org/W3086218926', 'https://openalex.org/W3166492115', 'https://openalex.org/W3131852626', 'https://openalex.org/W2980008268', 'https://openalex.org/W2889455726', 'https://openalex.org/W2972745026', 'https://openalex.org/W2979157532', 'https://openalex.org/W2922538097', 'https://openalex.org/W2726515241', 'https://openalex.org/W2551572271', 'https://openalex.org/W2972563022', 'https://openalex.org/W2964352155', 'https://openalex.org/W3155217823', 'https://openalex.org/W4240324299', 'https://openalex.org/W2962788625', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963912679', 'https://openalex.org/W2964243274', 'https://openalex.org/W3024040651', 'https://openalex.org/W3096303254', 'https://openalex.org/W2759925408', 'https://openalex.org/W2901611695', 'https://openalex.org/W2767052532', 'https://openalex.org/W2808631503', 'https://openalex.org/W2963839617', 'https://openalex.org/W2999905431']",2022-03-18
https://openalex.org/W4392904292,https://doi.org/10.1109/icassp48485.2024.10446888,Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-Training and Multi-Modal Tokens,"In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ.","['https://openalex.org/W6807079506', 'https://openalex.org/W6630875275', 'https://openalex.org/W2752796333', 'https://openalex.org/W4375868850', 'https://openalex.org/W3206387123', 'https://openalex.org/W6790356757', 'https://openalex.org/W4385570550', 'https://openalex.org/W4296070387', 'https://openalex.org/W6855650468', 'https://openalex.org/W4375868953', 'https://openalex.org/W4307680525', 'https://openalex.org/W6777028661', 'https://openalex.org/W4385823403', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385822683', 'https://openalex.org/W4390874021', 'https://openalex.org/W2586148577', 'https://openalex.org/W4319862477', 'https://openalex.org/W4386071467', 'https://openalex.org/W6791353385', 'https://openalex.org/W6850204008', 'https://openalex.org/W3180355996', 'https://openalex.org/W6802517614', 'https://openalex.org/W1861492603', 'https://openalex.org/W68733909', 'https://openalex.org/W2972394484', 'https://openalex.org/W4372260534', 'https://openalex.org/W3174311593', 'https://openalex.org/W3155217823', 'https://openalex.org/W3140429000', 'https://openalex.org/W6783867762', 'https://openalex.org/W3094502228', 'https://openalex.org/W4213019189', 'https://openalex.org/W6677929280', 'https://openalex.org/W4390871839', 'https://openalex.org/W2962862718', 'https://openalex.org/W1905882502', 'https://openalex.org/W6780218876', 'https://openalex.org/W6898505805', 'https://openalex.org/W2133459682', 'https://openalex.org/W6682631176', 'https://openalex.org/W1956340063', 'https://openalex.org/W2506483933', 'https://openalex.org/W6796464841', 'https://openalex.org/W4385245566', 'https://openalex.org/W2886641317', 'https://openalex.org/W6676497082', 'https://openalex.org/W4287854499', 'https://openalex.org/W3037465386', 'https://openalex.org/W4206865574', 'https://openalex.org/W3024605872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4385970143', 'https://openalex.org/W2154652894', 'https://openalex.org/W2109586012', 'https://openalex.org/W4394671563', 'https://openalex.org/W4320458302', 'https://openalex.org/W3099142230']",2024-03-18
https://openalex.org/W4395004920,https://doi.org/10.1007/s11042-024-19259-9,Neuraltalk+: neural image captioning with visual assistance capabilities,,"['https://openalex.org/W3140792177', 'https://openalex.org/W4312361860', 'https://openalex.org/W3155217823', 'https://openalex.org/W1931639407', 'https://openalex.org/W4313131769', 'https://openalex.org/W1897761818', 'https://openalex.org/W92662927', 'https://openalex.org/W3035160838', 'https://openalex.org/W3217070527', 'https://openalex.org/W2194775991', 'https://openalex.org/W2896348597', 'https://openalex.org/W3158091094', 'https://openalex.org/W3137069976', 'https://openalex.org/W6603036350', 'https://openalex.org/W2481240925', 'https://openalex.org/W1969616664', 'https://openalex.org/W2968660381', 'https://openalex.org/W3192652975', 'https://openalex.org/W1861492603', 'https://openalex.org/W2101105183', 'https://openalex.org/W4318953563', 'https://openalex.org/W4366123788', 'https://openalex.org/W1956340063', 'https://openalex.org/W1895577753', 'https://openalex.org/W3201187808', 'https://openalex.org/W2339652278', 'https://openalex.org/W4280622456', 'https://openalex.org/W2981165461', 'https://openalex.org/W4304148174', 'https://openalex.org/W3110761028']",2024-04-22
https://openalex.org/W4319997228,https://doi.org/10.1016/j.procs.2022.12.195,Common brain activity features discretization for predicting perceived speech quality,"The synthesized speech quality evaluation is one of the important steps to ensure the generated speech audio sounds good to humans. There are two main approaches to perform the evaluation; subjective and objective. Subjective approaches use human as the assessor, which is the most natural approach. However, it is time-consuming and expensive. Hence, it has generally been replaced by the quicker and cheaper objective approaches. Nevertheless, since objective approaches only analyze the audio features, the predicted quality might not correlated to what humans would perceive. Recent studies shows that brain activity contains some information that can be useful to enhance the prediction performance. This work proposed a method to extract the common features among participants' brain activity to predict the perceived speech audio quality. The result shows that the proposed approach significantly reduces the prediction error.","['https://openalex.org/W1974520683', 'https://openalex.org/W2043429285', 'https://openalex.org/W6761921182', 'https://openalex.org/W3047992192', 'https://openalex.org/W2573696275', 'https://openalex.org/W2916612990', 'https://openalex.org/W2807741448', 'https://openalex.org/W6767743360', 'https://openalex.org/W6784611469', 'https://openalex.org/W1525254066', 'https://openalex.org/W2599251041', 'https://openalex.org/W6773231842', 'https://openalex.org/W6947865715', 'https://openalex.org/W2954952848', 'https://openalex.org/W2090530235', 'https://openalex.org/W2982655240', 'https://openalex.org/W6777523379', 'https://openalex.org/W3155217823', 'https://openalex.org/W6685507350', 'https://openalex.org/W2128495200', 'https://openalex.org/W6724887576', 'https://openalex.org/W6748381668', 'https://openalex.org/W2889326414', 'https://openalex.org/W2507937192', 'https://openalex.org/W2182253985', 'https://openalex.org/W2559463885', 'https://openalex.org/W4231734965', 'https://openalex.org/W3000978293', 'https://openalex.org/W402218045']",2023-01-01
https://openalex.org/W4312097514,https://doi.org/10.23919/apsipaasc55919.2022.9979940,Direct speech-reply generation from text-dialogue context,"Natural speech-dialogue generation has been achieved with cascade systems combining automatic speech recog-nition, text-dialogue, and text-to-speech models. However, it is still challenging to generate expressive speech-replies depending on context because text-replies could lead to information loss in estimating appropriate expressions for speech generation. One promising approach is generating speech without requiring text. Direct speech generation from a dialogue context has never been achieved because it is difficult to learn the semantically one- to-many relationship between context and reply. This paper proposes a direct speech-reply generation model from the text-dialogue context in the same manner as the text-dialogue model. We focus on two challenges: an insufficient number of training dialogue pairs of text-context and speech-reply, and the difference between continuous speech signals and discrete text sequences. For the former, we applied text- to-speech to a text-dialogue dataset to acquire huge-scale training pairs. For the latter, we introduced the vector quantization on acoustic features to convert them into discrete sequences. The results indicate that the proposed model can successfully generate speech-reply directly from text-dialogue contexts, although a quality gap still exists with the text-dialogue model.","['https://openalex.org/W3161053832', 'https://openalex.org/W3206387123', 'https://openalex.org/W6798080464', 'https://openalex.org/W2972495969', 'https://openalex.org/W3142316150', 'https://openalex.org/W2963250244', 'https://openalex.org/W2134383396', 'https://openalex.org/W6801812709', 'https://openalex.org/W2903739847', 'https://openalex.org/W6783867762', 'https://openalex.org/W2933971837', 'https://openalex.org/W3035451444', 'https://openalex.org/W6755805197', 'https://openalex.org/W2901492641', 'https://openalex.org/W6778823374', 'https://openalex.org/W2964243274', 'https://openalex.org/W3155584966', 'https://openalex.org/W6772715161', 'https://openalex.org/W3155217823', 'https://openalex.org/W3163793923', 'https://openalex.org/W4226399820', 'https://openalex.org/W2963903950', 'https://openalex.org/W3140429000', 'https://openalex.org/W2899891766', 'https://openalex.org/W4287900772', 'https://openalex.org/W3201566090', 'https://openalex.org/W3033411150', 'https://openalex.org/W3180374548', 'https://openalex.org/W3092028330', 'https://openalex.org/W3104205004']",2022-11-07
https://openalex.org/W4393389449,https://doi.org/10.24036/voteteknika.v11i4.125218,Penerapan Optical Character Recognition (OCR) Dengan Text-To-Speech (TTS) dalam Konversi Gambar ke Suara,"Aksesibilitas informasi menjadi perhatian utama untuk memastikan bahwa semua individu dapat mengakses dan memahami konten secara maksimal Gangguan penglihatan menjadi salah satu disabilitas atau kekurangan yang cukup banyak dialami oleh orang Indonesia yang dalam perkembangannya menimbulkan berbagai masalah sebagai akibat dari kekurangan yang dimiliki salah satunya adalah aksebilitas informasi. Penelitian ini secara tidak langsung output yang dihasilkan merupakan hasil pengabungan dari menggunakan Optical Character Recognition dengan konversi representasi Vector Quantized Variational Autoencoder dengan pengubah suara Text-to-Speech dari google (gTTS) yang dilakukan sebagai upaya untuk menghasilkan kualitas suara yang lebih baik dan alami serta mempertahankan informasi asli. Hasil pengujian dalam penelitian diperoleh akurasi konversi dan pengubahan sebanyak 83,33% dengan 10 data uji dapat dikonversi dan diubah dengan baik dan cukup efektif dalam mempertahankan informasi asli dan menghasilkan suara natural. Kata kunci : Akses Informasi; Gangguan Penglihatan; OCR; VQ-VAE; gTTS; Machine Learning Accessibility to information is a major concern to ensure that all individuals can access and understand content to the fullest. Impaired vision is one of the disabilities or deficiencies experienced by quite a lot of Indonesians, which in its development creates various problems as a result of the deficiencies they have, one of which is information accessibility. This research indirectly produces the output that is the result of a combination of using Optical Character Recognition with the conversion of the Vector Quantized Variational Autoencoder representation with the Text-to-Speech voice modifier from Google (gTTS) which is carried out as an effort to produce better and more natural voice quality and retain original information. The test results in this study obtained an accuracy of conversion and conversion of 83.33% with 10 test data that can be converted and changed properly and are quite effective in retaining original information and producing natural sound. Keywords: Information Access; Visual Impairment; OCR; VQ-VAE; gTTS; Machine Learning","['https://openalex.org/W3205782414', 'https://openalex.org/W4308695120', 'https://openalex.org/W2885527204', 'https://openalex.org/W3004048800', 'https://openalex.org/W3155217823', 'https://openalex.org/W3175270391', 'https://openalex.org/W3114332015', 'https://openalex.org/W4309734222', 'https://openalex.org/W3174311593', 'https://openalex.org/W3128424678', 'https://openalex.org/W3082851515', 'https://openalex.org/W2947591107', 'https://openalex.org/W2963799213', 'https://openalex.org/W3008260390']",2023-12-02
https://openalex.org/W4408353895,https://doi.org/10.1109/icassp49660.2025.10890285,From Pixels to Voice: A Simple and Efficient End-to-End Spoken Image Description Approach via Vision Codec Language Models,,"['https://openalex.org/W4385245566', 'https://openalex.org/W4307323391', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6853611000', 'https://openalex.org/W6857968694', 'https://openalex.org/W6811013733', 'https://openalex.org/W6790019176', 'https://openalex.org/W6610006008', 'https://openalex.org/W3174311593', 'https://openalex.org/W4392904292', 'https://openalex.org/W3155217823', 'https://openalex.org/W6770596778', 'https://openalex.org/W2752796333', 'https://openalex.org/W3096216486', 'https://openalex.org/W3161053832', 'https://openalex.org/W2964243274', 'https://openalex.org/W639708223', 'https://openalex.org/W2962862718', 'https://openalex.org/W6853096648', 'https://openalex.org/W6757817989', 'https://openalex.org/W2101105183', 'https://openalex.org/W6678262379', 'https://openalex.org/W2154652894', 'https://openalex.org/W1956340063']",2025-03-12
https://openalex.org/W4391021561,https://doi.org/10.1109/asru57964.2023.10389725,Pseudo-Label Based Supervised Contrastive Loss for Robust Speech Representations,"The self supervised learning (SSL) of speech, with discrete tokenization (pseudo-labels), while illustrating performance improvements in low-resource speech recognition, has faced challenges in achieving context invariant and noise robust representations. In this paper, we propose a self-supervised framework based on contrastive loss of the pseudo-labels, obtained from an offline k-means quantizer (tokenizer). We refer to the proposed setting as pseudo-con. The pseudo-con loss, within a batch of training, allows the model to cluster the instances of the same pseudo-label while separating the instances of a different pseudo-label. The proposed pseudo-con loss can also be combined with the cross entropy loss, commonly used in self-supervised learning schemes. We demonstrate the effectiveness of the pseudo-con loss applied for various SSL techniques, like hidden unit bidirectional encoder representations from transformers (HuBERT), best random quantizer (BEST-RQ) and hidden unit clustering (HUC). Our evaluations using the proposed pseudo-con framework achieves state of art results on various sub-tasks of ZeroSpeech 2021 challenge as well as on the context invariance benchmarks. Further, we show significant performance improvements over existing SSL approaches on the TIMIT phoneme recognition task as well as the Librispeech (100h) ASR experiments.","['https://openalex.org/W6755207826', 'https://openalex.org/W3159481202', 'https://openalex.org/W3209059054', 'https://openalex.org/W6838461927', 'https://openalex.org/W2973157397', 'https://openalex.org/W3016011332', 'https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W4292825791', 'https://openalex.org/W6810673746', 'https://openalex.org/W6786696081', 'https://openalex.org/W4281492411', 'https://openalex.org/W4385822676', 'https://openalex.org/W3006926732', 'https://openalex.org/W3163596720', 'https://openalex.org/W6780483730', 'https://openalex.org/W4319862416', 'https://openalex.org/W6846365670', 'https://openalex.org/W6678975374', 'https://openalex.org/W6839738141', 'https://openalex.org/W6776700526', 'https://openalex.org/W3156636935', 'https://openalex.org/W3113328489', 'https://openalex.org/W4312766345', 'https://openalex.org/W3200253633', 'https://openalex.org/W4389317789', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W6769196770', 'https://openalex.org/W3198608154', 'https://openalex.org/W3209984917', 'https://openalex.org/W3144810982', 'https://openalex.org/W6717772578', 'https://openalex.org/W108866686', 'https://openalex.org/W6774314701', 'https://openalex.org/W1494198834', 'https://openalex.org/W3197349023', 'https://openalex.org/W2395899413', 'https://openalex.org/W2741692265', 'https://openalex.org/W2996728628', 'https://openalex.org/W1635512741', 'https://openalex.org/W1647671624', 'https://openalex.org/W6688816777', 'https://openalex.org/W2085487226', 'https://openalex.org/W2726515241', 'https://openalex.org/W2030931454', 'https://openalex.org/W4306672449', 'https://openalex.org/W4283026156', 'https://openalex.org/W4297808394', 'https://openalex.org/W2129068307', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287591426', 'https://openalex.org/W4287812705', 'https://openalex.org/W2896457183', 'https://openalex.org/W2431080869', 'https://openalex.org/W2219249508', 'https://openalex.org/W2979476256']",2023-12-16
https://openalex.org/W4403334492,https://doi.org/10.1145/3654777.3676416,Towards Music-Aware Virtual Assistants,,"['https://openalex.org/W2943686508', 'https://openalex.org/W2962966940', 'https://openalex.org/W4384261732', 'https://openalex.org/W2025949779', 'https://openalex.org/W3137883189', 'https://openalex.org/W1838578708', 'https://openalex.org/W2340080546', 'https://openalex.org/W2751379218', 'https://openalex.org/W1975255955', 'https://openalex.org/W2795720774', 'https://openalex.org/W3037149862', 'https://openalex.org/W2103475180', 'https://openalex.org/W3205523375', 'https://openalex.org/W4390075359', 'https://openalex.org/W4382603054', 'https://openalex.org/W3206801991', 'https://openalex.org/W3158762648', 'https://openalex.org/W2128519044', 'https://openalex.org/W4294619240', 'https://openalex.org/W2964243274', 'https://openalex.org/W2908350269', 'https://openalex.org/W4366593983', 'https://openalex.org/W2968379763', 'https://openalex.org/W3207272747', 'https://openalex.org/W47863897', 'https://openalex.org/W624465380']",2024-10-11
https://openalex.org/W4280633309,https://doi.org/10.1016/j.isci.2022.104393,Toward understanding the communication in sperm whales,,"['https://openalex.org/W3087638917', 'https://openalex.org/W2013579586', 'https://openalex.org/W2107101015', 'https://openalex.org/W3006696976', 'https://openalex.org/W6757870374', 'https://openalex.org/W6716358881', 'https://openalex.org/W2048139184', 'https://openalex.org/W6745740328', 'https://openalex.org/W6769196770', 'https://openalex.org/W2004775819', 'https://openalex.org/W2164079735', 'https://openalex.org/W3033010920', 'https://openalex.org/W2970226357', 'https://openalex.org/W2016661916', 'https://openalex.org/W2066908170', 'https://openalex.org/W6778883912', 'https://openalex.org/W6662691273', 'https://openalex.org/W2032408259', 'https://openalex.org/W2965751848', 'https://openalex.org/W2136479563', 'https://openalex.org/W6784767866', 'https://openalex.org/W2124479173', 'https://openalex.org/W6790168579', 'https://openalex.org/W6750129843', 'https://openalex.org/W6777659283', 'https://openalex.org/W6703366769', 'https://openalex.org/W2013832123', 'https://openalex.org/W4253316789', 'https://openalex.org/W2739497626', 'https://openalex.org/W2066404906', 'https://openalex.org/W6780968998', 'https://openalex.org/W3215285253', 'https://openalex.org/W2028008134', 'https://openalex.org/W2102224657', 'https://openalex.org/W2011238950', 'https://openalex.org/W1593341158', 'https://openalex.org/W6761557580', 'https://openalex.org/W6676025551', 'https://openalex.org/W2017677081', 'https://openalex.org/W1828958803', 'https://openalex.org/W6780249428', 'https://openalex.org/W1876949284', 'https://openalex.org/W2149374875', 'https://openalex.org/W779394199', 'https://openalex.org/W2965465245', 'https://openalex.org/W4205671217', 'https://openalex.org/W2104137851', 'https://openalex.org/W6676000232', 'https://openalex.org/W2071196664', 'https://openalex.org/W2413716136', 'https://openalex.org/W2300894760', 'https://openalex.org/W6631924159', 'https://openalex.org/W2082793319', 'https://openalex.org/W2897984329', 'https://openalex.org/W2808427125', 'https://openalex.org/W6809715509', 'https://openalex.org/W6693199996', 'https://openalex.org/W2768572194', 'https://openalex.org/W4280593913', 'https://openalex.org/W2796315435', 'https://openalex.org/W2147908439', 'https://openalex.org/W2165545766', 'https://openalex.org/W1963933074', 'https://openalex.org/W2159906077', 'https://openalex.org/W3005458772', 'https://openalex.org/W2063140472', 'https://openalex.org/W2074452946', 'https://openalex.org/W2103505394', 'https://openalex.org/W17120685', 'https://openalex.org/W2884032882', 'https://openalex.org/W2105879912', 'https://openalex.org/W2035110320', 'https://openalex.org/W2062248806', 'https://openalex.org/W3177828909', 'https://openalex.org/W6649703416', 'https://openalex.org/W2793128170', 'https://openalex.org/W6772056094', 'https://openalex.org/W6763797061', 'https://openalex.org/W2271388808', 'https://openalex.org/W1976252333', 'https://openalex.org/W6669139235', 'https://openalex.org/W2161394327', 'https://openalex.org/W6750739041', 'https://openalex.org/W6719236976', 'https://openalex.org/W2065116459', 'https://openalex.org/W2919115771', 'https://openalex.org/W6766904570', 'https://openalex.org/W1608215728', 'https://openalex.org/W2023314508', 'https://openalex.org/W1598934683', 'https://openalex.org/W2061280432', 'https://openalex.org/W6803902625', 'https://openalex.org/W2048911549', 'https://openalex.org/W6600959991', 'https://openalex.org/W2089908537', 'https://openalex.org/W2058142728', 'https://openalex.org/W6639664838', 'https://openalex.org/W2408060352', 'https://openalex.org/W2086648308', 'https://openalex.org/W2050688900', 'https://openalex.org/W6783125911', 'https://openalex.org/W4210893176', 'https://openalex.org/W2056263517', 'https://openalex.org/W2074719795', 'https://openalex.org/W1994717818', 'https://openalex.org/W2032914003', 'https://openalex.org/W1993533333', 'https://openalex.org/W6684558577', 'https://openalex.org/W2069138632', 'https://openalex.org/W1974388862', 'https://openalex.org/W2086193061', 'https://openalex.org/W2213467510', 'https://openalex.org/W6785355775', 'https://openalex.org/W2103593412', 'https://openalex.org/W4248634141', 'https://openalex.org/W2900428170', 'https://openalex.org/W2079214866', 'https://openalex.org/W2131462529', 'https://openalex.org/W2473499653', 'https://openalex.org/W2108112408', 'https://openalex.org/W2152036362', 'https://openalex.org/W2106318382', 'https://openalex.org/W2037460931', 'https://openalex.org/W2079145130', 'https://openalex.org/W6763525482', 'https://openalex.org/W2999187172', 'https://openalex.org/W2127419931', 'https://openalex.org/W2149557440', 'https://openalex.org/W2164527661', 'https://openalex.org/W3120387374', 'https://openalex.org/W2517592094', 'https://openalex.org/W1984891176', 'https://openalex.org/W2168201256', 'https://openalex.org/W2418992898', 'https://openalex.org/W3007309629', 'https://openalex.org/W2035396734', 'https://openalex.org/W6690047748', 'https://openalex.org/W3047188629', 'https://openalex.org/W2034217745', 'https://openalex.org/W6770582336', 'https://openalex.org/W3027324582', 'https://openalex.org/W2465328155', 'https://openalex.org/W2755569859', 'https://openalex.org/W2094555011', 'https://openalex.org/W2061596145', 'https://openalex.org/W2059539229', 'https://openalex.org/W2066974403', 'https://openalex.org/W2027409010', 'https://openalex.org/W2518136372', 'https://openalex.org/W2088152161', 'https://openalex.org/W2074874566', 'https://openalex.org/W2764033918', 'https://openalex.org/W3012651379', 'https://openalex.org/W2119286107', 'https://openalex.org/W4236965008', 'https://openalex.org/W2526529994', 'https://openalex.org/W2607303097', 'https://openalex.org/W569478347', 'https://openalex.org/W607174839', 'https://openalex.org/W2963571336', 'https://openalex.org/W2603150165', 'https://openalex.org/W3141239769', 'https://openalex.org/W4250568089', 'https://openalex.org/W2091109208', 'https://openalex.org/W4206413123', 'https://openalex.org/W4221167446', 'https://openalex.org/W1700952868', 'https://openalex.org/W4292779060', 'https://openalex.org/W566575418', 'https://openalex.org/W2949399644', 'https://openalex.org/W4205807230', 'https://openalex.org/W2166715144', 'https://openalex.org/W2514723185', 'https://openalex.org/W2107191910', 'https://openalex.org/W3097286738', 'https://openalex.org/W2464664036', 'https://openalex.org/W2610930722', 'https://openalex.org/W2074361287', 'https://openalex.org/W4212807739', 'https://openalex.org/W572086733', 'https://openalex.org/W2966715458', 'https://openalex.org/W2521581510', 'https://openalex.org/W1881636570', 'https://openalex.org/W2048570735', 'https://openalex.org/W2949579048', 'https://openalex.org/W4255452305']",2022-05-13
https://openalex.org/W4389668679,https://doi.org/10.1371/journal.pbio.3002366,Many but not all deep neural network audio models capture brain responses and exhibit correspondence between model stages and brain regions,"Models that predict brain responses to stimuli provide one measure of understanding of a sensory system and have many potential applications in science and engineering. Deep artificial neural networks have emerged as the leading such predictive models of the visual system but are less explored in audition. Prior work provided examples of audio-trained neural networks that produced good predictions of auditory cortical fMRI responses and exhibited correspondence between model stages and brain regions, but left it unclear whether these results generalize to other neural network models and, thus, how to further improve models in this domain. We evaluated model-brain correspondence for publicly available audio neural network models along with in-house models trained on 4 different tasks. Most tested models outpredicted standard spectromporal filter-bank models of auditory cortex and exhibited systematic model-brain correspondence: Middle stages best predicted primary auditory cortex, while deep stages best predicted non-primary cortex. However, some state-of-the-art models produced substantially worse brain predictions. Models trained to recognize speech in background noise produced better brain predictions than models trained to recognize speech in quiet, potentially because hearing in noise imposes constraints on biological auditory representations. The training task influenced the prediction quality for specific cortical tuning properties, with best overall predictions resulting from models trained on multiple tasks. The results generally support the promise of deep neural networks as models of audition, though they also indicate that current models do not explain auditory cortical responses in their entirety.","['https://openalex.org/W2082560699', 'https://openalex.org/W2088744610', 'https://openalex.org/W2604020748', 'https://openalex.org/W2978368159', 'https://openalex.org/W2922023527', 'https://openalex.org/W3086360455', 'https://openalex.org/W3100035092', 'https://openalex.org/W2404596811', 'https://openalex.org/W2795373741', 'https://openalex.org/W2952760759', 'https://openalex.org/W4200443526', 'https://openalex.org/W2058616551', 'https://openalex.org/W1715013381', 'https://openalex.org/W2412480261', 'https://openalex.org/W2537084945', 'https://openalex.org/W6637162671', 'https://openalex.org/W1932198206', 'https://openalex.org/W2751592082', 'https://openalex.org/W2807007689', 'https://openalex.org/W2888339491', 'https://openalex.org/W2913314773', 'https://openalex.org/W2970540112', 'https://openalex.org/W2898726413', 'https://openalex.org/W4387666878', 'https://openalex.org/W3137888317', 'https://openalex.org/W2902617128', 'https://openalex.org/W4223622179', 'https://openalex.org/W2800311957', 'https://openalex.org/W4226070978', 'https://openalex.org/W4226064907', 'https://openalex.org/W4224103177', 'https://openalex.org/W4283332789', 'https://openalex.org/W2945679118', 'https://openalex.org/W3036927415', 'https://openalex.org/W3016185014', 'https://openalex.org/W3134212080', 'https://openalex.org/W4304465621', 'https://openalex.org/W4281765823', 'https://openalex.org/W4221102486', 'https://openalex.org/W4327569188', 'https://openalex.org/W2054139811', 'https://openalex.org/W2063951486', 'https://openalex.org/W2160654481', 'https://openalex.org/W2153633111', 'https://openalex.org/W2951877485', 'https://openalex.org/W2210407171', 'https://openalex.org/W3129880834', 'https://openalex.org/W2811245934', 'https://openalex.org/W2970104209', 'https://openalex.org/W6687483927', 'https://openalex.org/W88719000', 'https://openalex.org/W2593116425', 'https://openalex.org/W1985466488', 'https://openalex.org/W2891276475', 'https://openalex.org/W2344975321', 'https://openalex.org/W2622627557', 'https://openalex.org/W2782213998', 'https://openalex.org/W2078483536', 'https://openalex.org/W6697847759', 'https://openalex.org/W2040036684', 'https://openalex.org/W3179888904', 'https://openalex.org/W2883992483', 'https://openalex.org/W3146620971', 'https://openalex.org/W2499800833', 'https://openalex.org/W2091766358', 'https://openalex.org/W4416539', 'https://openalex.org/W2088984420', 'https://openalex.org/W1971204243', 'https://openalex.org/W2089624763', 'https://openalex.org/W2057898689', 'https://openalex.org/W2970087077', 'https://openalex.org/W2948163068', 'https://openalex.org/W4321764341', 'https://openalex.org/W2017399882', 'https://openalex.org/W2037783071', 'https://openalex.org/W2582082042', 'https://openalex.org/W2067098744', 'https://openalex.org/W2109023847', 'https://openalex.org/W2130688825', 'https://openalex.org/W6683156416', 'https://openalex.org/W2107911922', 'https://openalex.org/W2154649497', 'https://openalex.org/W2082091957', 'https://openalex.org/W2009319803', 'https://openalex.org/W1515995260', 'https://openalex.org/W2197518923', 'https://openalex.org/W4214537939', 'https://openalex.org/W4210997545', 'https://openalex.org/W3193706553', 'https://openalex.org/W2076923487', 'https://openalex.org/W1983248752', 'https://openalex.org/W2125229781', 'https://openalex.org/W2091009634', 'https://openalex.org/W2143125861', 'https://openalex.org/W2213214752', 'https://openalex.org/W2183255833', 'https://openalex.org/W2077880866', 'https://openalex.org/W2046331056', 'https://openalex.org/W2062663442', 'https://openalex.org/W3210923133', 'https://openalex.org/W4366981350', 'https://openalex.org/W2106664807', 'https://openalex.org/W3146685237', 'https://openalex.org/W3201124393', 'https://openalex.org/W3209091531', 'https://openalex.org/W4285083025', 'https://openalex.org/W4366590873', 'https://openalex.org/W4200489976', 'https://openalex.org/W1965248225', 'https://openalex.org/W2156194815', 'https://openalex.org/W1977419921', 'https://openalex.org/W2151217816', 'https://openalex.org/W1965749376', 'https://openalex.org/W2000677804', 'https://openalex.org/W2807663970', 'https://openalex.org/W3092973197', 'https://openalex.org/W2108598243', 'https://openalex.org/W3004895274', 'https://openalex.org/W3119948327', 'https://openalex.org/W4312259544', 'https://openalex.org/W3036601975', 'https://openalex.org/W2898929289', 'https://openalex.org/W2943353555', 'https://openalex.org/W4366003941', 'https://openalex.org/W3038121656', 'https://openalex.org/W3035965352', 'https://openalex.org/W3003257820', 'https://openalex.org/W6675354045', 'https://openalex.org/W3013369344', 'https://openalex.org/W2148726987', 'https://openalex.org/W2020044743', 'https://openalex.org/W2151721316', 'https://openalex.org/W2154024859', 'https://openalex.org/W3196974791', 'https://openalex.org/W2726051499', 'https://openalex.org/W3015591594', 'https://openalex.org/W1494198834', 'https://openalex.org/W2094721231', 'https://openalex.org/W6784050962', 'https://openalex.org/W2981541003', 'https://openalex.org/W3027324582', 'https://openalex.org/W2940544976', 'https://openalex.org/W6633499030', 'https://openalex.org/W3095292526', 'https://openalex.org/W6739901393', 'https://openalex.org/W2979826702', 'https://openalex.org/W2885185669', 'https://openalex.org/W1686810756', 'https://openalex.org/W3093096176', 'https://openalex.org/W7027429494', 'https://openalex.org/W2050758723', 'https://openalex.org/W6656414902', 'https://openalex.org/W2964213897', 'https://openalex.org/W3198858531', 'https://openalex.org/W3099878876', 'https://openalex.org/W2996904338', 'https://openalex.org/W2919816947', 'https://openalex.org/W2963250244', 'https://openalex.org/W4225497281', 'https://openalex.org/W4320482536', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092424727', 'https://openalex.org/W1673923490', 'https://openalex.org/W4286892233', 'https://openalex.org/W2154369653', 'https://openalex.org/W2298475536', 'https://openalex.org/W2404126548', 'https://openalex.org/W2475730566', 'https://openalex.org/W4310572212', 'https://openalex.org/W2972831213', 'https://openalex.org/W2951065015', 'https://openalex.org/W4293030666', 'https://openalex.org/W3163652268', 'https://openalex.org/W2101234009', 'https://openalex.org/W2973026522', 'https://openalex.org/W4308831279', 'https://openalex.org/W4289330434', 'https://openalex.org/W3095361818', 'https://openalex.org/W3103145119', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963102712', 'https://openalex.org/W4390692365', 'https://openalex.org/W4388034436', 'https://openalex.org/W3015191643', 'https://openalex.org/W2951940613', 'https://openalex.org/W4390571036', 'https://openalex.org/W3118608800']",2023-12-13
https://openalex.org/W3157861865,https://doi.org/10.1613/jair.1.12967,"Visually Grounded Models of Spoken Language: A Survey of Datasets, Architectures and Evaluation Techniques","This survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. Such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. Several fields have made important contributions to this approach to modeling or mimicking the process of learning language: Machine Learning, Natural Language and Speech Processing, Computer Vision and Cognitive Science. The current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. We discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. We then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.","['https://openalex.org/W2524365899', 'https://openalex.org/W3105148948', 'https://openalex.org/W3177829661', 'https://openalex.org/W6747045456', 'https://openalex.org/W2230076941', 'https://openalex.org/W2957089051', 'https://openalex.org/W2752168051', 'https://openalex.org/W2282219577', 'https://openalex.org/W6766153511', 'https://openalex.org/W2006969979', 'https://openalex.org/W2906407728', 'https://openalex.org/W2586148577', 'https://openalex.org/W4393717817', 'https://openalex.org/W3017025049', 'https://openalex.org/W1924770834', 'https://openalex.org/W2553608650', 'https://openalex.org/W6676297131', 'https://openalex.org/W2531381952', 'https://openalex.org/W2102605133', 'https://openalex.org/W2796156786', 'https://openalex.org/W2137010615', 'https://openalex.org/W2580178245', 'https://openalex.org/W2927673779', 'https://openalex.org/W2991557631', 'https://openalex.org/W2796315435', 'https://openalex.org/W2556930864', 'https://openalex.org/W2736876693', 'https://openalex.org/W4288076474', 'https://openalex.org/W4393840460', 'https://openalex.org/W2920166246', 'https://openalex.org/W2974048280', 'https://openalex.org/W6687483927', 'https://openalex.org/W3092512595', 'https://openalex.org/W3161348170', 'https://openalex.org/W2960271609', 'https://openalex.org/W3114436296', 'https://openalex.org/W2974393448', 'https://openalex.org/W2938991416', 'https://openalex.org/W2808286951', 'https://openalex.org/W2601713192', 'https://openalex.org/W2903320905', 'https://openalex.org/W1905882502', 'https://openalex.org/W2112912048', 'https://openalex.org/W3164946614', 'https://openalex.org/W2160654481', 'https://openalex.org/W3093241733', 'https://openalex.org/W6756021500', 'https://openalex.org/W6639102338', 'https://openalex.org/W2250790822', 'https://openalex.org/W6767668184', 'https://openalex.org/W2948859046', 'https://openalex.org/W1614298861', 'https://openalex.org/W3160014322', 'https://openalex.org/W1574972348', 'https://openalex.org/W2533598788', 'https://openalex.org/W3095881291', 'https://openalex.org/W3015300171', 'https://openalex.org/W3111013239', 'https://openalex.org/W2941492599', 'https://openalex.org/W6656414902', 'https://openalex.org/W6801926475', 'https://openalex.org/W2992526251', 'https://openalex.org/W3034875620', 'https://openalex.org/W2132921748', 'https://openalex.org/W2080320702', 'https://openalex.org/W2107917162', 'https://openalex.org/W3143035657', 'https://openalex.org/W6773790731', 'https://openalex.org/W2593779438', 'https://openalex.org/W3029315861', 'https://openalex.org/W1686810756', 'https://openalex.org/W6864391120', 'https://openalex.org/W2752796333', 'https://openalex.org/W3027324582', 'https://openalex.org/W6739901393', 'https://openalex.org/W3158565912', 'https://openalex.org/W2611064105', 'https://openalex.org/W2029096624', 'https://openalex.org/W6678493852', 'https://openalex.org/W2766091292', 'https://openalex.org/W2134670479', 'https://openalex.org/W2507296351', 'https://openalex.org/W2784025607', 'https://openalex.org/W6720905350', 'https://openalex.org/W2989358187', 'https://openalex.org/W2952132648', 'https://openalex.org/W2971709506', 'https://openalex.org/W385555557', 'https://openalex.org/W4288595436', 'https://openalex.org/W2965147078', 'https://openalex.org/W2984008963', 'https://openalex.org/W4287240590', 'https://openalex.org/W4393617183', 'https://openalex.org/W2973135958', 'https://openalex.org/W1861492603', 'https://openalex.org/W3159476814', 'https://openalex.org/W3200287550', 'https://openalex.org/W2972892814', 'https://openalex.org/W3121480429', 'https://openalex.org/W3174311593', 'https://openalex.org/W2995680346', 'https://openalex.org/W2988907666', 'https://openalex.org/W2963983719', 'https://openalex.org/W2962862718', 'https://openalex.org/W1797268635', 'https://openalex.org/W2964001192', 'https://openalex.org/W2963799213', 'https://openalex.org/W2108598243', 'https://openalex.org/W3196698946', 'https://openalex.org/W2963778889', 'https://openalex.org/W2972808286', 'https://openalex.org/W2123815913', 'https://openalex.org/W2963403868', 'https://openalex.org/W3100813302', 'https://openalex.org/W2119775030', 'https://openalex.org/W2964099072', 'https://openalex.org/W3213502289', 'https://openalex.org/W2194775991', 'https://openalex.org/W2095897464', 'https://openalex.org/W2125566341', 'https://openalex.org/W4297606427', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963902314', 'https://openalex.org/W2963525826', 'https://openalex.org/W4237938692', 'https://openalex.org/W2963163163', 'https://openalex.org/W2963330681', 'https://openalex.org/W2950133079', 'https://openalex.org/W3100923070', 'https://openalex.org/W2024490156', 'https://openalex.org/W2963115079', 'https://openalex.org/W3095670406', 'https://openalex.org/W2962753610', 'https://openalex.org/W4394453761', 'https://openalex.org/W4294555862', 'https://openalex.org/W4230640548', 'https://openalex.org/W3095361818', 'https://openalex.org/W2962813140', 'https://openalex.org/W3217290931', 'https://openalex.org/W4286973758', 'https://openalex.org/W4297826211', 'https://openalex.org/W3197828817', 'https://openalex.org/W3005578234', 'https://openalex.org/W3035750922', 'https://openalex.org/W4300047444', 'https://openalex.org/W3170972077', 'https://openalex.org/W2940544976']",2022-02-18
https://openalex.org/W3033010920,https://doi.org/10.1016/j.neunet.2021.03.017,CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks,"How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN - InfoGAN (Chen et al., 2016) - and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start, even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework.","['https://openalex.org/W6785782189', 'https://openalex.org/W6741832134', 'https://openalex.org/W2606425707', 'https://openalex.org/W2907162900', 'https://openalex.org/W6769196770', 'https://openalex.org/W6785455557', 'https://openalex.org/W3031894486', 'https://openalex.org/W2007251877', 'https://openalex.org/W6718140377', 'https://openalex.org/W6782496585', 'https://openalex.org/W3100270690', 'https://openalex.org/W6776959109', 'https://openalex.org/W6777659283', 'https://openalex.org/W6703366769', 'https://openalex.org/W2152134037', 'https://openalex.org/W6755257315', 'https://openalex.org/W6761568071', 'https://openalex.org/W6697293080', 'https://openalex.org/W6784039545', 'https://openalex.org/W6761557580', 'https://openalex.org/W6676028815', 'https://openalex.org/W6648638054', 'https://openalex.org/W2151794869', 'https://openalex.org/W2126377586', 'https://openalex.org/W6735913928', 'https://openalex.org/W2033413759', 'https://openalex.org/W6713421067', 'https://openalex.org/W6779984486', 'https://openalex.org/W6756098772', 'https://openalex.org/W2468716020', 'https://openalex.org/W6649703416', 'https://openalex.org/W2155721440', 'https://openalex.org/W6675022971', 'https://openalex.org/W1778492285', 'https://openalex.org/W6665204316', 'https://openalex.org/W2079207700', 'https://openalex.org/W3027324582', 'https://openalex.org/W2604683892', 'https://openalex.org/W2010188467', 'https://openalex.org/W3046603443', 'https://openalex.org/W2398490608', 'https://openalex.org/W2621649611', 'https://openalex.org/W1980862600', 'https://openalex.org/W6640902190', 'https://openalex.org/W6762378949', 'https://openalex.org/W2962879692', 'https://openalex.org/W2996556191', 'https://openalex.org/W2612690371', 'https://openalex.org/W52412328', 'https://openalex.org/W2100768664', 'https://openalex.org/W3100385063', 'https://openalex.org/W3209383001', 'https://openalex.org/W1516184288', 'https://openalex.org/W2173520492', 'https://openalex.org/W3095706145', 'https://openalex.org/W2789720136', 'https://openalex.org/W2122364000', 'https://openalex.org/W3093427098', 'https://openalex.org/W2963226019', 'https://openalex.org/W3095361818', 'https://openalex.org/W2623769798', 'https://openalex.org/W2107038463', 'https://openalex.org/W3097485645', 'https://openalex.org/W2401235434', 'https://openalex.org/W2945769669', 'https://openalex.org/W2888991776', 'https://openalex.org/W3105148948', 'https://openalex.org/W1513618424', 'https://openalex.org/W2963620343', 'https://openalex.org/W3097692357', 'https://openalex.org/W2559655401', 'https://openalex.org/W1583837637', 'https://openalex.org/W4320013936', 'https://openalex.org/W3098643042', 'https://openalex.org/W1700952868', 'https://openalex.org/W3106255532', 'https://openalex.org/W3022428196', 'https://openalex.org/W3093096176', 'https://openalex.org/W2151518054', 'https://openalex.org/W2468383091', 'https://openalex.org/W2963720603', 'https://openalex.org/W4300047444', 'https://openalex.org/W2099471712', 'https://openalex.org/W2059652594', 'https://openalex.org/W1997505733', 'https://openalex.org/W2963684088', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2739748921', 'https://openalex.org/W4255113413', 'https://openalex.org/W2894295011', 'https://openalex.org/W2973026522', 'https://openalex.org/W4295521014', 'https://openalex.org/W2972867623', 'https://openalex.org/W1993755070', 'https://openalex.org/W2963571336', 'https://openalex.org/W3085331098', 'https://openalex.org/W4297817572', 'https://openalex.org/W2582743722', 'https://openalex.org/W3097286738', 'https://openalex.org/W2940544976', 'https://openalex.org/W2996383576', 'https://openalex.org/W1949377791', 'https://openalex.org/W4376537811', 'https://openalex.org/W3212451506', 'https://openalex.org/W3127686677']",2021-03-19
https://openalex.org/W3204915839,https://doi.org/10.1109/taslp.2022.3180684,Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding,"Typically, unsupervised segmentation of speech into the phone- and word-like units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of the threshold on boundary detector performance, and our results suggest that automatically learning the boundary threshold can be as effective as manually tuning that threshold. We discover that phone class impacts the boundary detection performance, and the boundaries between successive vowels or semivowels are the most difficult. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We can lower the feature extraction rate from the typical 100 Hz to as low as 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.","['https://openalex.org/W6780218876', 'https://openalex.org/W6770506093', 'https://openalex.org/W2981857663', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W2010188467', 'https://openalex.org/W2057007397', 'https://openalex.org/W2020607164', 'https://openalex.org/W2117041980', 'https://openalex.org/W2170659185', 'https://openalex.org/W6675022971', 'https://openalex.org/W2078769636', 'https://openalex.org/W2468716020', 'https://openalex.org/W2747192917', 'https://openalex.org/W2964169922', 'https://openalex.org/W2890718354', 'https://openalex.org/W2160815625', 'https://openalex.org/W2150769028', 'https://openalex.org/W4301204483', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W3096656254', 'https://openalex.org/W6755207826', 'https://openalex.org/W6766673545', 'https://openalex.org/W6774314701', 'https://openalex.org/W6682948231', 'https://openalex.org/W3198782837', 'https://openalex.org/W2882319491', 'https://openalex.org/W2115867364', 'https://openalex.org/W2126377586', 'https://openalex.org/W2115197214', 'https://openalex.org/W6789826613', 'https://openalex.org/W2145410271', 'https://openalex.org/W2963137467', 'https://openalex.org/W2962799131', 'https://openalex.org/W3198134274', 'https://openalex.org/W3100270690', 'https://openalex.org/W6769196770', 'https://openalex.org/W2117126688', 'https://openalex.org/W2972794572', 'https://openalex.org/W1778492285', 'https://openalex.org/W6631526040', 'https://openalex.org/W2111732304', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W2146444479', 'https://openalex.org/W6774456908', 'https://openalex.org/W2752796333', 'https://openalex.org/W6690026940', 'https://openalex.org/W130754613', 'https://openalex.org/W3095361818', 'https://openalex.org/W2890704021', 'https://openalex.org/W6795952400', 'https://openalex.org/W2719865699', 'https://openalex.org/W6790356757', 'https://openalex.org/W6786696081', 'https://openalex.org/W2949382160', 'https://openalex.org/W2152790380', 'https://openalex.org/W3169072315', 'https://openalex.org/W2100768664', 'https://openalex.org/W3027324582', 'https://openalex.org/W3034978746', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287173589', 'https://openalex.org/W2991213871', 'https://openalex.org/W3008499099', 'https://openalex.org/W2242818861', 'https://openalex.org/W1525748279', 'https://openalex.org/W3125709657', 'https://openalex.org/W4394671563', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996383576', 'https://openalex.org/W2973026522', 'https://openalex.org/W3099782249', 'https://openalex.org/W2965373594', 'https://openalex.org/W2601836666', 'https://openalex.org/W3110458199', 'https://openalex.org/W1828163288', 'https://openalex.org/W1994458317', 'https://openalex.org/W3098643042', 'https://openalex.org/W2842511635', 'https://openalex.org/W3112613336', 'https://openalex.org/W3127686677', 'https://openalex.org/W2478415332']",2022-01-01
https://openalex.org/W4224926225,https://doi.org/10.1109/icassp43922.2022.9747427,VCVTS: Multi-Speaker Video-to-Speech Synthesis Via Cross-Modal Knowledge Transfer from Voice Conversion,"Though significant progress has been made for speaker-dependent Video-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker VTS that can map silent video to speech, while allowing flexible control of speaker identity, all in a single system. This paper proposes a novel multi-speaker VTS system based on cross-modal knowledge transfer from voice conversion (VC), where vector quantization with contrastive predictive coding (VQCPC) is used for the content encoder of VC to derive discrete phoneme-like acoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to infer the index sequence of acoustic units. The Lip2Ind network can then substitute the content encoder of VC to form a multi-speaker VTS system to convert silent video to acoustic units for reconstructing accurate spoken content. The VTS system also inherits the advantages of VC by using a speaker encoder to produce speaker representations to effectively control the speaker identity of generated speech. Extensive evaluations verify the effectiveness of proposed approach, which can be applied in both constrained vocabulary and open vocabulary conditions, achieving state-of-the-art performance in generating high-quality speech with high naturalness, intelligibility and speaker similarity. Our demo page is released here <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W2516001803', 'https://openalex.org/W2120847449', 'https://openalex.org/W6745136726', 'https://openalex.org/W2194775991', 'https://openalex.org/W3016011581', 'https://openalex.org/W2067295501', 'https://openalex.org/W1552314771', 'https://openalex.org/W2972359262', 'https://openalex.org/W3015338123', 'https://openalex.org/W2964352155', 'https://openalex.org/W3096650361', 'https://openalex.org/W2963936489', 'https://openalex.org/W2972563022', 'https://openalex.org/W6794378236', 'https://openalex.org/W3160305627', 'https://openalex.org/W3035626590', 'https://openalex.org/W4206204999', 'https://openalex.org/W2015143272', 'https://openalex.org/W6769196770', 'https://openalex.org/W3097777922', 'https://openalex.org/W2739009240', 'https://openalex.org/W6677618333', 'https://openalex.org/W2585824449', 'https://openalex.org/W6631190155', 'https://openalex.org/W2625027024', 'https://openalex.org/W2963019222', 'https://openalex.org/W2068056889', 'https://openalex.org/W2887437849', 'https://openalex.org/W7042829694', 'https://openalex.org/W6734491695', 'https://openalex.org/W3027324582', 'https://openalex.org/W3197659778', 'https://openalex.org/W2146444479', 'https://openalex.org/W2842511635', 'https://openalex.org/W6779459370', 'https://openalex.org/W1862426464', 'https://openalex.org/W2115252128', 'https://openalex.org/W2765407302', 'https://openalex.org/W4297808394', 'https://openalex.org/W1522301498', 'https://openalex.org/W2187089797', 'https://openalex.org/W2293856338', 'https://openalex.org/W3095361818', 'https://openalex.org/W2979476256', 'https://openalex.org/W3157840621', 'https://openalex.org/W3036928441', 'https://openalex.org/W3101481642']",2022-04-27
https://openalex.org/W3097320994,https://doi.org/10.1145/3570161,Paralinguistic Privacy Protection at the Edge,"Voice user interfaces and digital assistants are rapidly entering our lives and becoming singular touch points spanning our devices. These always-on services capture and transmit our audio data to powerful cloud services for further processing and subsequent actions. Our voices and raw audio signals collected through these devices contain a host of sensitive paralinguistic information that is transmitted to service providers regardless of deliberate or false triggers. As our emotional patterns and sensitive attributes like our identity, gender, and well-being are easily inferred using deep acoustic models, we encounter a new generation of privacy risks by using these services. One approach to mitigate the risk of paralinguistic-based privacy breaches is to exploit a combination of cloud-based processing with privacy-preserving, on-device paralinguistic information learning and filtering before transmitting voice data. In this article we introduce EDGY , a configurable, lightweight, disentangled representation learning framework that transforms and filters high-dimensional voice data to identify and contain sensitive attributes at the edge prior to offloading to the cloud. We evaluate EDGY’s on-device performance and explore optimization techniques, including model quantization and knowledge distillation, to enable private, accurate, and efficient representation learning on resource-constrained devices. Our results show that EDGY runs in tens of milliseconds with 0.2% relative improvement in “zero-shot” ABX score or minimal performance penalties of approximately 5.95% word error rate (WER) in learning linguistic representations from raw voice signals, using a CPU and a single-core ARM processor without specialized hardware.","['https://openalex.org/W3047841241', 'https://openalex.org/W3045886598', 'https://openalex.org/W2995929068', 'https://openalex.org/W2979476256', 'https://openalex.org/W3036601975', 'https://openalex.org/W2242818861', 'https://openalex.org/W3165040079', 'https://openalex.org/W2030931454', 'https://openalex.org/W2327501763', 'https://openalex.org/W2972659941', 'https://openalex.org/W2963830550', 'https://openalex.org/W2962824709', 'https://openalex.org/W3100270690', 'https://openalex.org/W2885806496', 'https://openalex.org/W3025747610', 'https://openalex.org/W2593116425', 'https://openalex.org/W2887059375', 'https://openalex.org/W3047081942', 'https://openalex.org/W2048520715', 'https://openalex.org/W3100378519', 'https://openalex.org/W2982574661', 'https://openalex.org/W4287887366', 'https://openalex.org/W3130248090', 'https://openalex.org/W3007566156', 'https://openalex.org/W3098361150', 'https://openalex.org/W3012640291', 'https://openalex.org/W2998249245', 'https://openalex.org/W3112034174', 'https://openalex.org/W2898186212', 'https://openalex.org/W2986437614', 'https://openalex.org/W3101314853', 'https://openalex.org/W2972951327', 'https://openalex.org/W6736780897', 'https://openalex.org/W2951082691', 'https://openalex.org/W3110458199', 'https://openalex.org/W4297808394', 'https://openalex.org/W2596378825', 'https://openalex.org/W1494198834', 'https://openalex.org/W2935938411', 'https://openalex.org/W2898564584', 'https://openalex.org/W3020570669', 'https://openalex.org/W2906993533', 'https://openalex.org/W2057563799', 'https://openalex.org/W3006926732', 'https://openalex.org/W2953900859', 'https://openalex.org/W3024768724', 'https://openalex.org/W2948954216', 'https://openalex.org/W6776763299', 'https://openalex.org/W3027324582', 'https://openalex.org/W3150635893', 'https://openalex.org/W2964539095', 'https://openalex.org/W2962814013', 'https://openalex.org/W3122866338', 'https://openalex.org/W2795435272', 'https://openalex.org/W2999160446', 'https://openalex.org/W2971229607', 'https://openalex.org/W4287865702', 'https://openalex.org/W2964299589', 'https://openalex.org/W3082482205', 'https://openalex.org/W4206821167', 'https://openalex.org/W2612690371', 'https://openalex.org/W3099785009', 'https://openalex.org/W2963799213', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963104724', 'https://openalex.org/W2949382160', 'https://openalex.org/W2730845691', 'https://openalex.org/W3009186400', 'https://openalex.org/W3024962219', 'https://openalex.org/W3016021263', 'https://openalex.org/W2965272715', 'https://openalex.org/W4287326402', 'https://openalex.org/W2995129880', 'https://openalex.org/W3082522567', 'https://openalex.org/W2964307104', 'https://openalex.org/W2962760690', 'https://openalex.org/W3111682954', 'https://openalex.org/W2608554408', 'https://openalex.org/W2915661750', 'https://openalex.org/W4287691744', 'https://openalex.org/W2904459034', 'https://openalex.org/W2953737218', 'https://openalex.org/W2981507750', 'https://openalex.org/W2515385951', 'https://openalex.org/W3104686647', 'https://openalex.org/W2995525544', 'https://openalex.org/W2916104401', 'https://openalex.org/W4212774754', 'https://openalex.org/W4288088095', 'https://openalex.org/W2963122961', 'https://openalex.org/W2294370754', 'https://openalex.org/W2726515241', 'https://openalex.org/W1493267010', 'https://openalex.org/W3042776162', 'https://openalex.org/W2963618559', 'https://openalex.org/W2763421725', 'https://openalex.org/W3016181583', 'https://openalex.org/W2937343983', 'https://openalex.org/W3021040286', 'https://openalex.org/W3103272945', 'https://openalex.org/W4297689207', 'https://openalex.org/W3098486933', 'https://openalex.org/W2912512634', 'https://openalex.org/W3005862564', 'https://openalex.org/W3125709657', 'https://openalex.org/W3030437843', 'https://openalex.org/W3034794073', 'https://openalex.org/W4310650179', 'https://openalex.org/W3098439673', 'https://openalex.org/W1821462560', 'https://openalex.org/W2996383576', 'https://openalex.org/W4287692175', 'https://openalex.org/W4236026035', 'https://openalex.org/W2913668833', 'https://openalex.org/W2954386831', 'https://openalex.org/W3099782249', 'https://openalex.org/W3210177631', 'https://openalex.org/W2928560789', 'https://openalex.org/W2795409001', 'https://openalex.org/W3043999252', 'https://openalex.org/W3015141382', 'https://openalex.org/W2914052719', 'https://openalex.org/W2787685498', 'https://openalex.org/W2803193013', 'https://openalex.org/W3015212100', 'https://openalex.org/W2519091744', 'https://openalex.org/W4287591426', 'https://openalex.org/W3095361818', 'https://openalex.org/W4246571396', 'https://openalex.org/W3029286132', 'https://openalex.org/W2842511635', 'https://openalex.org/W4234552385']",2022-11-03
https://openalex.org/W3199367817,https://doi.org/10.21437/vccbc.2020-20,Non-parallel Voice Conversion based on Hierarchical Latent Embedding Vector Quantized Variational Autoencoder,,"['https://openalex.org/W2120605154', 'https://openalex.org/W2889329491', 'https://openalex.org/W3092368332', 'https://openalex.org/W2518172956', 'https://openalex.org/W2796495654', 'https://openalex.org/W2608338293', 'https://openalex.org/W2774848319', 'https://openalex.org/W2963539064', 'https://openalex.org/W2605762339', 'https://openalex.org/W2972388953', 'https://openalex.org/W2905266595', 'https://openalex.org/W2752796333', 'https://openalex.org/W2982602185', 'https://openalex.org/W3027324582', 'https://openalex.org/W2972849140', 'https://openalex.org/W6762931180', 'https://openalex.org/W3011028252', 'https://openalex.org/W6936113694', 'https://openalex.org/W2981728663', 'https://openalex.org/W1522301498', 'https://openalex.org/W2293049663', 'https://openalex.org/W4288079962', 'https://openalex.org/W2994715919', 'https://openalex.org/W2963048608', 'https://openalex.org/W2946809691', 'https://openalex.org/W2937343983', 'https://openalex.org/W1967858847', 'https://openalex.org/W2963035245', 'https://openalex.org/W2964121744', 'https://openalex.org/W2968131493', 'https://openalex.org/W3015338123', 'https://openalex.org/W2972659941', 'https://openalex.org/W2972544500', 'https://openalex.org/W2963799213']",2020-10-30
https://openalex.org/W3161348170,https://doi.org/10.18653/v1/2021.blackboxnlp-1.11,Discrete representations in neural models of spoken language,"The distributed and continuous representations used by neural networks are at odds with representations employed in linguistics, which are typically symbolic. Vector quantization has been proposed as a way to induce discrete neural representations that are closer in nature to their linguistic counterparts. However, it is not clear which metrics are the best-suited to analyze such discrete representations. We compare the merits of four commonly used metrics in the context of weakly supervised models of spoken language. We compare the results they show when applied to two different models, while systematically studying the effect of the placement and size of the discretization layer. We find that different evaluation regimes can give inconsistent results. While we can attribute them to the properties of the different metrics in most cases, one point of concern remains: the use of minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit inventories, unlike metrics applied to complete utterances. Furthermore, while in general vector quantization induces representations that correlate with units posited in linguistics, the strength of this correlation is only moderate.","['https://openalex.org/W2024081693', 'https://openalex.org/W2888912391', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2972867623', 'https://openalex.org/W3100813302', 'https://openalex.org/W2962862718', 'https://openalex.org/W1524333225', 'https://openalex.org/W2936295285', 'https://openalex.org/W3025429027', 'https://openalex.org/W2947591107', 'https://openalex.org/W3105148948', 'https://openalex.org/W2964204621', 'https://openalex.org/W3044967013', 'https://openalex.org/W2963430224', 'https://openalex.org/W3027324582', 'https://openalex.org/W2593779438', 'https://openalex.org/W2396043527', 'https://openalex.org/W2119775030', 'https://openalex.org/W3157861865', 'https://openalex.org/W2796315435', 'https://openalex.org/W2963620343', 'https://openalex.org/W2515741950', 'https://openalex.org/W2972374322', 'https://openalex.org/W2138615112', 'https://openalex.org/W2984673553', 'https://openalex.org/W2964054038', 'https://openalex.org/W2973047874', 'https://openalex.org/W2963799213', 'https://openalex.org/W2786608204', 'https://openalex.org/W2242818861', 'https://openalex.org/W2940544976', 'https://openalex.org/W2962776659', 'https://openalex.org/W2894164357', 'https://openalex.org/W2137010615', 'https://openalex.org/W2160654481', 'https://openalex.org/W4288107125', 'https://openalex.org/W3035750922', 'https://openalex.org/W3035305735', 'https://openalex.org/W2964121744', 'https://openalex.org/W3098952151', 'https://openalex.org/W1522301498', 'https://openalex.org/W2949079242', 'https://openalex.org/W3093096176', 'https://openalex.org/W2946296745', 'https://openalex.org/W3097286738', 'https://openalex.org/W3171345413', 'https://openalex.org/W2971709506', 'https://openalex.org/W2194775991', 'https://openalex.org/W3100270690', 'https://openalex.org/W3087357110', 'https://openalex.org/W2995680346', 'https://openalex.org/W2963902314', 'https://openalex.org/W3125709657']",2021-01-01
https://openalex.org/W3169072315,https://doi.org/10.21437/interspeech.2021-1874,Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation,"Automatic detection of phoneme or word-like units is one of the core objectives in zero-resource speech processing. Recent attempts employ self-supervised training methods, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework that can model the signal structure at a higher level e.g. at the phoneme level. In this framework, a convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Typically, phoneme and word segmentation are treated as separate tasks. We unify them and experimentally show that our single model outperforms existing phoneme and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and when is the right time to include the segmental loss in the learning process.","['https://openalex.org/W130754613', 'https://openalex.org/W2078769636', 'https://openalex.org/W2747192917', 'https://openalex.org/W2170659185', 'https://openalex.org/W3127686677', 'https://openalex.org/W2117126688', 'https://openalex.org/W2126377586', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963137467', 'https://openalex.org/W2842511635', 'https://openalex.org/W2890718354', 'https://openalex.org/W2242818861', 'https://openalex.org/W2145410271', 'https://openalex.org/W2973049979', 'https://openalex.org/W2940544976', 'https://openalex.org/W2152790380', 'https://openalex.org/W2100768664', 'https://openalex.org/W2601836666', 'https://openalex.org/W3036601975', 'https://openalex.org/W2964169922', 'https://openalex.org/W3096656254', 'https://openalex.org/W2478415332', 'https://openalex.org/W3112613336', 'https://openalex.org/W2057007397', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972794572', 'https://openalex.org/W3027324582', 'https://openalex.org/W3125709657', 'https://openalex.org/W3099782249', 'https://openalex.org/W2468716020', 'https://openalex.org/W2020607164']",2021-08-27
https://openalex.org/W3136576491,https://doi.org/10.48550/arxiv.2103.08199,Double Articulation Analyzer with Prosody for Unsupervised Word and Phoneme Discovery,"Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.","['https://openalex.org/W2256409625', 'https://openalex.org/W2607257024', 'https://openalex.org/W2099626219', 'https://openalex.org/W2153767712', 'https://openalex.org/W1502984613', 'https://openalex.org/W3027324582', 'https://openalex.org/W1980862600', 'https://openalex.org/W2073685522', 'https://openalex.org/W2017449852', 'https://openalex.org/W1796128977', 'https://openalex.org/W2908005395', 'https://openalex.org/W1778492285', 'https://openalex.org/W2767009175', 'https://openalex.org/W2165470078', 'https://openalex.org/W2157149948', 'https://openalex.org/W2100768664', 'https://openalex.org/W2220867547', 'https://openalex.org/W2121137195', 'https://openalex.org/W3004441278', 'https://openalex.org/W2963720603', 'https://openalex.org/W2022058071', 'https://openalex.org/W2752552296', 'https://openalex.org/W2772857214', 'https://openalex.org/W2055226328', 'https://openalex.org/W2066006005', 'https://openalex.org/W2900800742', 'https://openalex.org/W2951004968', 'https://openalex.org/W2962206103', 'https://openalex.org/W3102667484', 'https://openalex.org/W2115867364', 'https://openalex.org/W2468716020', 'https://openalex.org/W2335112305', 'https://openalex.org/W2170353620', 'https://openalex.org/W2124973255', 'https://openalex.org/W2041258965', 'https://openalex.org/W2795648103', 'https://openalex.org/W2024079589', 'https://openalex.org/W1993755070', 'https://openalex.org/W2110614779', 'https://openalex.org/W2142111485', 'https://openalex.org/W2078769636', 'https://openalex.org/W2912021853', 'https://openalex.org/W2059824090', 'https://openalex.org/W2465308763', 'https://openalex.org/W2296607128', 'https://openalex.org/W2294206829']",2021-03-15
https://openalex.org/W3217035088,https://doi.org/10.1109/lsp.2022.3161115,Non-Intrusive Binaural Speech Intelligibility Prediction From Discrete Latent Representations,"Non-intrusive speech intelligibility (SI) prediction from binaural signals is useful in many applications. However, most existing signal-based measures are designed to be applied to single-channel signals. Measures specifically designed to take into account the binaural properties of the signal are often intrusive - characterised by requiring access to a clean speech signal - and typically rely on combining both channels into a single-channel signal before making predictions. This paper proposes a non-intrusive SI measure that computes features from a binaural input signal using a combination of vector quantization (VQ) and contrastive predictive coding (CPC) methods. VQ-CPC feature extraction does not rely on any model of the auditory system and is instead trained to maximise the mutual information between the input signal and output features. The computed VQ-CPC features are input to a predicting function parameterized by a neural network. Two predicting functions are considered in this paper. Both feature extractor and predicting functions are trained on simulated binaural signals with isotropic noise. They are tested on simulated signals with isotropic and real noise. For all signals, the ground truth scores are the (intrusive) deterministic binaural STOI. Results are presented in terms of correlations and MSE and demonstrate that VQ-CPC features are able to capture information relevant to modelling SI and outperform all the considered benchmarks - even when evaluating on data comprising of different noise field types.","['https://openalex.org/W2605449545', 'https://openalex.org/W6789284391', 'https://openalex.org/W1985029311', 'https://openalex.org/W1987831012', 'https://openalex.org/W2141998673', 'https://openalex.org/W2034593061', 'https://openalex.org/W2602577565', 'https://openalex.org/W2809058357', 'https://openalex.org/W2765278963', 'https://openalex.org/W3086428351', 'https://openalex.org/W6785906987', 'https://openalex.org/W2147101913', 'https://openalex.org/W1986177448', 'https://openalex.org/W1996646214', 'https://openalex.org/W2465704452', 'https://openalex.org/W3163472112', 'https://openalex.org/W3111796160', 'https://openalex.org/W2048741136', 'https://openalex.org/W2342810974', 'https://openalex.org/W2809874909', 'https://openalex.org/W6844194202', 'https://openalex.org/W3095361818', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2752796333', 'https://openalex.org/W6795062860', 'https://openalex.org/W1494198834', 'https://openalex.org/W2136682440', 'https://openalex.org/W6688816777', 'https://openalex.org/W2089586791', 'https://openalex.org/W3197832224', 'https://openalex.org/W4213196139', 'https://openalex.org/W2009649672', 'https://openalex.org/W6674330103', 'https://openalex.org/W6638667902', 'https://openalex.org/W6640212811', 'https://openalex.org/W6766978945', 'https://openalex.org/W2219249508', 'https://openalex.org/W2939710050', 'https://openalex.org/W2095705004', 'https://openalex.org/W2970971581', 'https://openalex.org/W3027324582', 'https://openalex.org/W2949117887', 'https://openalex.org/W3124666641', 'https://openalex.org/W2996383576', 'https://openalex.org/W1924770834', 'https://openalex.org/W2842511635', 'https://openalex.org/W3105857760', 'https://openalex.org/W2963799213', 'https://openalex.org/W3159778524']",2022-01-01
https://openalex.org/W3138466558,,Whole brain Probabilistic Generative Model toward Realizing Cognitive Architecture for Developmental Robots,"Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence, is one of the goals in artificial intelligence and developmental robotics. Furthermore, a computational model that enables an artificial cognitive system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes the development of a cognitive architecture using probabilistic generative models (PGMs) to fully mirror the human cognitive system. The integrative model is called a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In this paper, the process of building the WB-PGM and learning from the human brain to build cognitive architectures is described.","['https://openalex.org/W2152722155', 'https://openalex.org/W2799151646', 'https://openalex.org/W1970792572', 'https://openalex.org/W2082627290', 'https://openalex.org/W2122050160', 'https://openalex.org/W1978549608', 'https://openalex.org/W1592304224', 'https://openalex.org/W2148300948', 'https://openalex.org/W2094342172', 'https://openalex.org/W2025091021', 'https://openalex.org/W2919115771', 'https://openalex.org/W2159162180', 'https://openalex.org/W3016960123', 'https://openalex.org/W2480949139', 'https://openalex.org/W1572811707', 'https://openalex.org/W2530876040', 'https://openalex.org/W1490667783', 'https://openalex.org/W2954626430', 'https://openalex.org/W2017687738', 'https://openalex.org/W2154943827', 'https://openalex.org/W615893297', 'https://openalex.org/W2130195834', 'https://openalex.org/W2117726420', 'https://openalex.org/W2889231692', 'https://openalex.org/W2964846271', 'https://openalex.org/W2400660831', 'https://openalex.org/W2220867547', 'https://openalex.org/W2607257024', 'https://openalex.org/W1928882148', 'https://openalex.org/W2781726626', 'https://openalex.org/W2753738274', 'https://openalex.org/W2403726878', 'https://openalex.org/W2558297939', 'https://openalex.org/W2951841654', 'https://openalex.org/W3111064853', 'https://openalex.org/W1993665893', 'https://openalex.org/W3011755845', 'https://openalex.org/W2040382032', 'https://openalex.org/W2021544751', 'https://openalex.org/W2952484912', 'https://openalex.org/W1959608418', 'https://openalex.org/W2091789872', 'https://openalex.org/W3131413056', 'https://openalex.org/W2023209753', 'https://openalex.org/W2346811982', 'https://openalex.org/W419646410', 'https://openalex.org/W2103617625', 'https://openalex.org/W72347498', 'https://openalex.org/W2538140147', 'https://openalex.org/W2055226328', 'https://openalex.org/W2148820580', 'https://openalex.org/W1912390567', 'https://openalex.org/W2117217843', 'https://openalex.org/W1652173018', 'https://openalex.org/W3131140269', 'https://openalex.org/W2121863487', 'https://openalex.org/W2060723376', 'https://openalex.org/W2275064919', 'https://openalex.org/W2059320470', 'https://openalex.org/W2055523610', 'https://openalex.org/W2256409625', 'https://openalex.org/W1981221473', 'https://openalex.org/W2336416123', 'https://openalex.org/W3030163527', 'https://openalex.org/W2032471499', 'https://openalex.org/W2788388592', 'https://openalex.org/W2056653303', 'https://openalex.org/W2127958135', 'https://openalex.org/W2919112510', 'https://openalex.org/W2989869785', 'https://openalex.org/W3056727383', 'https://openalex.org/W3033314805', 'https://openalex.org/W2019401192', 'https://openalex.org/W2794177543', 'https://openalex.org/W2970364795', 'https://openalex.org/W2996815917', 'https://openalex.org/W2785874930', 'https://openalex.org/W2887972576', 'https://openalex.org/W2168856390', 'https://openalex.org/W2955277818', 'https://openalex.org/W2979569224', 'https://openalex.org/W2147008239', 'https://openalex.org/W2556013083', 'https://openalex.org/W2000214310', 'https://openalex.org/W2153562927', 'https://openalex.org/W2168202614', 'https://openalex.org/W2149885167', 'https://openalex.org/W2068247585', 'https://openalex.org/W2123039986', 'https://openalex.org/W2911448865', 'https://openalex.org/W2525658408', 'https://openalex.org/W2522169164', 'https://openalex.org/W2130422193', 'https://openalex.org/W1663973292', 'https://openalex.org/W2963305465', 'https://openalex.org/W2121780146', 'https://openalex.org/W2963341956', 'https://openalex.org/W2992977009', 'https://openalex.org/W2595771591', 'https://openalex.org/W2119885245', 'https://openalex.org/W2267408097', 'https://openalex.org/W3098767311', 'https://openalex.org/W3008097900', 'https://openalex.org/W3024138969', 'https://openalex.org/W2101524054', 'https://openalex.org/W2265661972', 'https://openalex.org/W2056163665', 'https://openalex.org/W2771915570', 'https://openalex.org/W2952334031', 'https://openalex.org/W2043543877', 'https://openalex.org/W2488240401', 'https://openalex.org/W1983023238', 'https://openalex.org/W2621274416', 'https://openalex.org/W1710476689', 'https://openalex.org/W2986752388', 'https://openalex.org/W2327562811', 'https://openalex.org/W2012592267', 'https://openalex.org/W3122690883', 'https://openalex.org/W2517275106', 'https://openalex.org/W2105563024', 'https://openalex.org/W1973788414', 'https://openalex.org/W580906298', 'https://openalex.org/W2060052647', 'https://openalex.org/W2097356275', 'https://openalex.org/W379488513', 'https://openalex.org/W2016708835', 'https://openalex.org/W2103692957', 'https://openalex.org/W3102667484', 'https://openalex.org/W3101212901', 'https://openalex.org/W2155772159', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964321317', 'https://openalex.org/W2900152462', 'https://openalex.org/W2124150121', 'https://openalex.org/W3201002944', 'https://openalex.org/W3036730253', 'https://openalex.org/W2162381107', 'https://openalex.org/W2996361813', 'https://openalex.org/W2958484796', 'https://openalex.org/W3027324582', 'https://openalex.org/W2041848110', 'https://openalex.org/W1880262756', 'https://openalex.org/W2574963492']",2021-03-15
https://openalex.org/W3134286091,https://doi.org/10.48550/arxiv.2103.02858,crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder,"In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.","['https://openalex.org/W3082130377', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962896155', 'https://openalex.org/W3015338123', 'https://openalex.org/W3092246895', 'https://openalex.org/W2946555236', 'https://openalex.org/W2518172956', 'https://openalex.org/W3027324582', 'https://openalex.org/W3199367817', 'https://openalex.org/W2963035245', 'https://openalex.org/W2510867321', 'https://openalex.org/W2937579788', 'https://openalex.org/W2052871313', 'https://openalex.org/W2994715919', 'https://openalex.org/W2963539064', 'https://openalex.org/W2972544500', 'https://openalex.org/W3101689408', 'https://openalex.org/W2937993361', 'https://openalex.org/W2242818861', 'https://openalex.org/W2972849140', 'https://openalex.org/W2532494225', 'https://openalex.org/W2964069186', 'https://openalex.org/W2963799213', 'https://openalex.org/W2156142001', 'https://openalex.org/W2598638573', 'https://openalex.org/W2120847449', 'https://openalex.org/W2971074500', 'https://openalex.org/W3100378519', 'https://openalex.org/W2548275288', 'https://openalex.org/W2012086895', 'https://openalex.org/W3093058615', 'https://openalex.org/W1524333225', 'https://openalex.org/W2800289214', 'https://openalex.org/W1509691205']",2021-03-04
https://openalex.org/W3092674239,https://doi.org/10.48550/arxiv.2010.05959,Towards Induction of Structured Phoneme Inventories,"This extended abstract surveying the work on phonological typology was prepared for ""SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"" to be held at EMNLP 2020.","['https://openalex.org/W2746555167', 'https://openalex.org/W2572670101', 'https://openalex.org/W2972708855', 'https://openalex.org/W3017568157', 'https://openalex.org/W2973034126', 'https://openalex.org/W3037580942', 'https://openalex.org/W3032617810', 'https://openalex.org/W3015877095', 'https://openalex.org/W2978709484', 'https://openalex.org/W2739967986', 'https://openalex.org/W2945774221', 'https://openalex.org/W3035490255', 'https://openalex.org/W2398528382', 'https://openalex.org/W3024040651', 'https://openalex.org/W2895651543', 'https://openalex.org/W2936123380', 'https://openalex.org/W2127846433', 'https://openalex.org/W3027324582', 'https://openalex.org/W1778492285', 'https://openalex.org/W2668277942', 'https://openalex.org/W2935756752', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963069394', 'https://openalex.org/W2402366697']",2020-10-12
https://openalex.org/W3095018479,https://doi.org/10.48550/arxiv.2010.14230,A Comparison of Discrete Latent Variable Models for Speech Representation Learning,Neural latent variable models enable the discovery of interesting structure in speech audio data. This paper presents a comparison of two different approaches which are broadly based on predicting future time-steps or auto-encoding the input signal. Our study compares the representations learned by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance. Results show that future time-step prediction with vq-wav2vec achieves better performance. The best system achieves an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge,"['https://openalex.org/W2124509324', 'https://openalex.org/W2963341956', 'https://openalex.org/W3127686677', 'https://openalex.org/W2930682606', 'https://openalex.org/W2842511635', 'https://openalex.org/W2933138175', 'https://openalex.org/W3099782249', 'https://openalex.org/W3125709657', 'https://openalex.org/W2970119519', 'https://openalex.org/W1494198834', 'https://openalex.org/W2593814746', 'https://openalex.org/W2973049979', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963799213', 'https://openalex.org/W2118154032', 'https://openalex.org/W2789543585', 'https://openalex.org/W2953190524', 'https://openalex.org/W2518108298', 'https://openalex.org/W2996383576', 'https://openalex.org/W3015419784', 'https://openalex.org/W3027324582', 'https://openalex.org/W2949382160']",2020-10-24
https://openalex.org/W3207272747,https://doi.org/10.1109/icassp43922.2022.9746112,Phone-to-Audio Alignment without Text: A Semi-Supervised Approach,"The task of phone-to-audio alignment has many applications in speech research. Here we introduce two Wav2Vec2-based models for both text-dependent and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a semi-supervised model, directly learns phone-to-audio alignment through contrastive learning and a forward sum loss, and can be coupled with a pretrained phone recognizer to achieve text-independent alignment. The other model, Wav2Vec2-FC, is a frame classification model trained on forced aligned labels that can both perform forced alignment and text-independent segmentation. Evaluation results suggest that both proposed methods, even when transcriptions are not available, generate highly close results to existing forced alignment tools. Our work presents a neural pipeline of fully automated phone-to-audio alignment. Code and pretrained models are available at https://github.com/lingjzhu/charsiu.","['https://openalex.org/W3197876970', 'https://openalex.org/W3043783436', 'https://openalex.org/W2808743435', 'https://openalex.org/W3016185664', 'https://openalex.org/W3196294373', 'https://openalex.org/W3096656254', 'https://openalex.org/W6787178341', 'https://openalex.org/W3198782837', 'https://openalex.org/W6800393981', 'https://openalex.org/W6800389019', 'https://openalex.org/W6755207826', 'https://openalex.org/W2972584841', 'https://openalex.org/W6609836551', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W6771467084', 'https://openalex.org/W6780218876', 'https://openalex.org/W6603616073', 'https://openalex.org/W6633249632', 'https://openalex.org/W2127141656', 'https://openalex.org/W2747874407', 'https://openalex.org/W1566289585', 'https://openalex.org/W2748816379', 'https://openalex.org/W2936774411', 'https://openalex.org/W6778823374', 'https://openalex.org/W6745317255', 'https://openalex.org/W3026041220', 'https://openalex.org/W2296073425', 'https://openalex.org/W3169072315', 'https://openalex.org/W3112613336', 'https://openalex.org/W1556470778', 'https://openalex.org/W3033411150', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963341956', 'https://openalex.org/W3196001064', 'https://openalex.org/W2767052532', 'https://openalex.org/W2896457183', 'https://openalex.org/W1524333225', 'https://openalex.org/W88081813', 'https://openalex.org/W3194000401', 'https://openalex.org/W1635512741', 'https://openalex.org/W3130016944', 'https://openalex.org/W3036601975', 'https://openalex.org/W267812377', 'https://openalex.org/W3099782249', 'https://openalex.org/W3198134274']",2022-04-27
https://openalex.org/W3209993061,https://doi.org/10.1109/icassp43922.2022.9746102,Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words,"We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.","['https://openalex.org/W130754613', 'https://openalex.org/W2064675550', 'https://openalex.org/W6739901393', 'https://openalex.org/W6674330103', 'https://openalex.org/W6786696081', 'https://openalex.org/W2842511635', 'https://openalex.org/W6787178341', 'https://openalex.org/W2127141656', 'https://openalex.org/W3197974236', 'https://openalex.org/W6917638038', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198782837', 'https://openalex.org/W3096656254', 'https://openalex.org/W2145410271', 'https://openalex.org/W4297808394', 'https://openalex.org/W3127686677', 'https://openalex.org/W3198134274', 'https://openalex.org/W3110458199', 'https://openalex.org/W4287591426', 'https://openalex.org/W2963403868', 'https://openalex.org/W3112613336', 'https://openalex.org/W4385245566', 'https://openalex.org/W2095705004']",2022-04-27
https://openalex.org/W4226177016,https://doi.org/10.1109/taslp.2022.3171975,Non-Parametric Bayesian Subspace Models for Acoustic Unit Discovery,"This work investigates subspace non-parametric models for the task of learning a set of acoustic units from unlabeled speech recordings. We constrain the base-measure of a Dirichlet-Process mixture with a phonetic subspace---estimated from other source languages---to build an \emph{educated prior}, thereby forcing the learned acoustic units to resemble phones of known source languages. Two types of models are proposed: (i) the Subspace HMM (SHMM) which assumes that the phonetic subspace is the same for every language, (ii) the Hierarchical-Subspace HMM (H-SHMM) which relaxes this assumption and allows to have a language-specific subspace estimated on the unlabeled target data. These models are applied on 3 languages: English, Yoruba and Mboshi and they are compared with various competitive acoustic units discovery baselines. Experimental results show that both subspace models outperform other systems in terms of clustering quality and segmentation accuracy. Moreover, we observe that the H-SHMM provides results superior to the SHMM supporting the idea that language-specific priors are preferable to language-agnostic priors for acoustic unit discovery.","['https://openalex.org/W2928408492', 'https://openalex.org/W2962907457', 'https://openalex.org/W2193413348', 'https://openalex.org/W6687566353', 'https://openalex.org/W2545177271', 'https://openalex.org/W2483390977', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2940544976', 'https://openalex.org/W2100768664', 'https://openalex.org/W6675022971', 'https://openalex.org/W2399576818', 'https://openalex.org/W2586754519', 'https://openalex.org/W2347098582', 'https://openalex.org/W6704752648', 'https://openalex.org/W3100270690', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W6769196770', 'https://openalex.org/W2991557631', 'https://openalex.org/W6770596778', 'https://openalex.org/W2750248772', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W3100202343', 'https://openalex.org/W3161215977', 'https://openalex.org/W2109176692', 'https://openalex.org/W6676755231', 'https://openalex.org/W1503207992', 'https://openalex.org/W1990005915', 'https://openalex.org/W2612661529', 'https://openalex.org/W4241943628', 'https://openalex.org/W4212863985', 'https://openalex.org/W2077804127', 'https://openalex.org/W4394867155', 'https://openalex.org/W6865393803', 'https://openalex.org/W2107638917', 'https://openalex.org/W4237840503', 'https://openalex.org/W6645634967', 'https://openalex.org/W7048738093', 'https://openalex.org/W2408460974', 'https://openalex.org/W2762715843', 'https://openalex.org/W1516111018', 'https://openalex.org/W6784355959', 'https://openalex.org/W6744702808', 'https://openalex.org/W2195354', 'https://openalex.org/W3092791109', 'https://openalex.org/W2401271873', 'https://openalex.org/W2084534958', 'https://openalex.org/W2572097499', 'https://openalex.org/W2401396251', 'https://openalex.org/W6600087822', 'https://openalex.org/W6712757354', 'https://openalex.org/W2752796333', 'https://openalex.org/W6731521493', 'https://openalex.org/W2972374322', 'https://openalex.org/W6712648922', 'https://openalex.org/W2099111195', 'https://openalex.org/W3093096176', 'https://openalex.org/W3112613336', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125838338', 'https://openalex.org/W2115979064', 'https://openalex.org/W3198134274', 'https://openalex.org/W2072169887', 'https://openalex.org/W2805607737', 'https://openalex.org/W3209059054', 'https://openalex.org/W6804030475', 'https://openalex.org/W3198429080', 'https://openalex.org/W6677308353', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818']",2022-01-01
https://openalex.org/W3158953319,https://doi.org/10.21437/interspeech.2021-1544,Aligned Contrastive Predictive Coding,"We investigate the possibility of forcing a self-supervised model trained using a contrastive predictive loss to extract slowly varying latent representations. Rather than producing individual predictions for each of the future representations, the model emits a sequence of predictions shorter than that of the upcoming representations to which they will be aligned. In this way, the prediction network solves a simpler task of predicting the next symbols, but not their exact timing, while the encoding network is trained to produce piece-wise constant latent codes. We evaluate the model on a speech coding task and demonstrate that the proposed Aligned Contrastive Predictive Coding (ACPC) leads to higher linear phone prediction accuracy and lower ABX error rates, while being slightly faster to train due to the reduced number of prediction heads.","['https://openalex.org/W2127141656', 'https://openalex.org/W2064675550', 'https://openalex.org/W2842511635', 'https://openalex.org/W2125838338', 'https://openalex.org/W3033038061', 'https://openalex.org/W854541894', 'https://openalex.org/W3134881075', 'https://openalex.org/W2964308564', 'https://openalex.org/W2535388113', 'https://openalex.org/W2963799213', 'https://openalex.org/W3095361818', 'https://openalex.org/W3016181583', 'https://openalex.org/W3110458199', 'https://openalex.org/W2963403868', 'https://openalex.org/W3125709657', 'https://openalex.org/W2095705004', 'https://openalex.org/W3093788532', 'https://openalex.org/W3008499099', 'https://openalex.org/W2146444479', 'https://openalex.org/W3112613336', 'https://openalex.org/W2078993594', 'https://openalex.org/W1828163288']",2021-08-27
https://openalex.org/W3170928308,https://doi.org/10.48550/arxiv.2106.04298,Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings,"Documenting languages helps to prevent the extinction of endangered dialects, many of which are otherwise expected to disappear by the end of the century. When documenting oral languages, unsupervised word segmentation (UWS) from speech is a useful, yet challenging, task. It consists in producing time-stamps for slicing utterances into smaller segments corresponding to words, being performed from phonetic transcriptions, or in the absence of these, from the output of unsupervised speech discretization models. These discretization models are trained using raw speech only, producing discrete speech units that can be applied for downstream (text-based) tasks. In this paper we compare five of these models: three Bayesian and two neural approaches, with regards to the exploitability of the produced units for UWS. For the UWS task, we experiment with two models, using as our target language the Mboshi (Bantu C25), an unwritten language from Congo-Brazzaville. Additionally, we report results for Finnish, Hungarian, Romanian and Russian in equally low-resource settings, using only 4 hours of speech. Our results suggest that neural models for speech discretization are difficult to exploit in our setting, and that it might be necessary to adapt them to limit sequence length. We obtain our best UWS results by using Bayesian models that produce high quality, yet compressed, discrete representations of the input speech signal.","['https://openalex.org/W3029422373', 'https://openalex.org/W3100202343', 'https://openalex.org/W3097485645', 'https://openalex.org/W2466918907', 'https://openalex.org/W2895097770', 'https://openalex.org/W2347098582', 'https://openalex.org/W2808682925', 'https://openalex.org/W2401271873', 'https://openalex.org/W2407614114', 'https://openalex.org/W2996383576', 'https://openalex.org/W2111668269', 'https://openalex.org/W2972574141', 'https://openalex.org/W2973026522', 'https://openalex.org/W2401396251', 'https://openalex.org/W2195354', 'https://openalex.org/W1563026167', 'https://openalex.org/W2515167330', 'https://openalex.org/W2963378435', 'https://openalex.org/W2572097499', 'https://openalex.org/W2055408826', 'https://openalex.org/W2963819008', 'https://openalex.org/W2985777044', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963799213', 'https://openalex.org/W2547875792', 'https://openalex.org/W2173413395', 'https://openalex.org/W2242818861', 'https://openalex.org/W2346964103', 'https://openalex.org/W2483390977', 'https://openalex.org/W2126377586', 'https://openalex.org/W3125709657', 'https://openalex.org/W2962784628', 'https://openalex.org/W2586232309', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963620343', 'https://openalex.org/W2883972335', 'https://openalex.org/W3112613336', 'https://openalex.org/W2025482506', 'https://openalex.org/W2101281673', 'https://openalex.org/W2963678298', 'https://openalex.org/W2126449874', 'https://openalex.org/W1778492285', 'https://openalex.org/W175497273', 'https://openalex.org/W3093096176', 'https://openalex.org/W2972704741', 'https://openalex.org/W2084534958', 'https://openalex.org/W2582956876', 'https://openalex.org/W3032598645']",2021-06-08
https://openalex.org/W3200461789,https://doi.org/10.36227/techrxiv.16618135.v1,Non-Parametric Bayesian Subspace Models for Acoustic Unit Discovery,"This work investigates subspace non-parametric models for the task of learning a set of acoustic units from unlabeled speech recordings. We constrain the base-measure of a Dirichlet-Process mixture with a phonetic subspace---estimated from other source languages---to build an \emph{educated prior}, thereby forcing the learned acoustic units to resemble phones of known source languages. Two types of models are proposed: (i) the Subspace HMM (SHMM) which assumes that the phonetic subspace is the same for every language, (ii) the Hierarchical-Subspace HMM (H-SHMM) which relaxes this assumption and allows to have a language-specific subspace estimated on the unlabeled target data. These models are applied on 3 languages: English, Yoruba and Mboshi and they are compared with various competitive acoustic units discovery baselines. Experimental results show that both subspace models outperform other systems in terms of clustering quality and segmentation accuracy. Moreover, we observe that the H-SHMM provides results superior to the SHMM supporting the idea that language-specific priors are preferable to language-agnostic priors for acoustic unit discovery.","['https://openalex.org/W2928408492', 'https://openalex.org/W2193413348', 'https://openalex.org/W2545177271', 'https://openalex.org/W2483390977', 'https://openalex.org/W6973666849', 'https://openalex.org/W6697293080', 'https://openalex.org/W2940544976', 'https://openalex.org/W2100768664', 'https://openalex.org/W2399576818', 'https://openalex.org/W2586754519', 'https://openalex.org/W6704752648', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2991557631', 'https://openalex.org/W2750248772', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W3100202343', 'https://openalex.org/W6676755231', 'https://openalex.org/W1503207992', 'https://openalex.org/W2077804127', 'https://openalex.org/W4394867155', 'https://openalex.org/W4237840503', 'https://openalex.org/W7048738093', 'https://openalex.org/W2762715843', 'https://openalex.org/W6784355959', 'https://openalex.org/W2195354', 'https://openalex.org/W2401271873', 'https://openalex.org/W2572097499', 'https://openalex.org/W2401396251', 'https://openalex.org/W2752796333', 'https://openalex.org/W2972374322', 'https://openalex.org/W3093096176', 'https://openalex.org/W3112613336', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125838338', 'https://openalex.org/W2115979064', 'https://openalex.org/W2072169887']",2021-09-17
https://openalex.org/W4223988985,https://doi.org/10.1186/s13636-022-00240-z,Paralinguistic singing attribute recognition using supervised machine learning for describing the classical tenor solo singing voice in vocal pedagogy,"Abstract Humans can recognize someone’s identity through their voice and describe the timbral phenomena of voices. Likewise, the singing voice also has timbral phenomena. In vocal pedagogy, vocal teachers listen and then describe the timbral phenomena of their student’s singing voice. In this study, in order to enable machines to describe the singing voice from the vocal pedagogy point of view, we perform a task called paralinguistic singing attribute recognition. To achieve this goal, we first construct and publish an open source dataset named Singing Voice Quality and Technique Database (SVQTD) for supervised learning. All the audio clips in SVQTD are downloaded from YouTube and processed by music source separation and silence detection. For annotation, seven paralinguistic singing attributes commonly used in vocal pedagogy are adopted as the labels. Furthermore, to explore the different supervised machine learning algorithm for classifying each paralinguistic singing attribute, we adopt three main frameworks, namely openSMILE features with support vector machine (SF-SVM), end-to-end deep learning (E2EDL), and deep embedding with support vector machine (DE-SVM). Our methods are based on existing frameworks commonly employed in other paralinguistic speech attribute recognition tasks. In SF-SVM, we separately use the feature set of the INTERSPEECH 2009 Challenge and that of the INTERSPEECH 2016 Challenge as the SVM classifier’s input. In E2EDL, the end-to-end framework separately utilizes the ResNet and transformer encoder as feature extractors. In particular, to handle two-dimensional spectrogram input for a transformer, we adopt a sliced multi-head self-attention (SMSA) mechanism. In the DE-SVM, we use the representation extracted from the E2EDL model as the input of the SVM classifier. Experimental results on SVQTD show no absolute winner between E2EDL and the DE-SVM, which means that the back-end SVM classifier with the representation learned by E2E as input does not necessarily improve the performance. However, the DE-SVM that utilizes the ResNet as the feature extractor achieves the best average UAR, with an average 16% improvement over that of the SF-SVM with INTERSPEECH’s hand-crafted feature set.","['https://openalex.org/W1971525508', 'https://openalex.org/W2169295472', 'https://openalex.org/W2189397211', 'https://openalex.org/W2111926505', 'https://openalex.org/W2020866611', 'https://openalex.org/W2094085880', 'https://openalex.org/W2115002300', 'https://openalex.org/W2051580501', 'https://openalex.org/W2054287732', 'https://openalex.org/W2028895660', 'https://openalex.org/W3213967396', 'https://openalex.org/W2321391359', 'https://openalex.org/W1943497805', 'https://openalex.org/W2059970763', 'https://openalex.org/W6930575966', 'https://openalex.org/W2058392145', 'https://openalex.org/W2085662862', 'https://openalex.org/W2513507089', 'https://openalex.org/W2119417805', 'https://openalex.org/W2034345148', 'https://openalex.org/W786031425', 'https://openalex.org/W2002299114', 'https://openalex.org/W2089364955', 'https://openalex.org/W2137371678', 'https://openalex.org/W2120256304', 'https://openalex.org/W2170938075', 'https://openalex.org/W1995663108', 'https://openalex.org/W2399733683', 'https://openalex.org/W3095513995', 'https://openalex.org/W2746419079', 'https://openalex.org/W2973025177', 'https://openalex.org/W2889467522', 'https://openalex.org/W2972411915', 'https://openalex.org/W3037149862', 'https://openalex.org/W3000819039', 'https://openalex.org/W3033704032', 'https://openalex.org/W2009955221', 'https://openalex.org/W2000860788', 'https://openalex.org/W2037239148', 'https://openalex.org/W2174219101', 'https://openalex.org/W1998038926', 'https://openalex.org/W2067002903', 'https://openalex.org/W2033012815', 'https://openalex.org/W1570824959', 'https://openalex.org/W109946471', 'https://openalex.org/W2128980425', 'https://openalex.org/W2074788634', 'https://openalex.org/W1501669607', 'https://openalex.org/W2082927806', 'https://openalex.org/W6828894009', 'https://openalex.org/W2194775991', 'https://openalex.org/W2405274704', 'https://openalex.org/W2963351448', 'https://openalex.org/W576379397', 'https://openalex.org/W4299308749', 'https://openalex.org/W2612690371', 'https://openalex.org/W4212774754', 'https://openalex.org/W2535255545', 'https://openalex.org/W2742451886', 'https://openalex.org/W1521262965', 'https://openalex.org/W3136889378']",2022-04-15
https://openalex.org/W4312953326,https://doi.org/10.1007/978-3-031-10960-7_8,Latent Spaces: A Creative Approach,,"['https://openalex.org/W3183052014', 'https://openalex.org/W2321639984', 'https://openalex.org/W2322262468', 'https://openalex.org/W2982753834', 'https://openalex.org/W2012574051', 'https://openalex.org/W3213967396', 'https://openalex.org/W2108598243', 'https://openalex.org/W1522754984', 'https://openalex.org/W3180355996', 'https://openalex.org/W2103498773', 'https://openalex.org/W2965664547', 'https://openalex.org/W2962770929', 'https://openalex.org/W2919115771', 'https://openalex.org/W2191779130', 'https://openalex.org/W3122300013', 'https://openalex.org/W2078003816']",2022-01-01
https://openalex.org/W4391350578,https://doi.org/10.11647/obp.0353.18,18. Artificial Intelligence and the Symphony Orchestra,"This chapter documents a process of practice-based research concerning the relationship between artificial intelligence and classical music. I argue that classical music (as an industry) is well placed o answer salient questions that the age of artificial intelligence demands we consider. The relationship is explored through three themes, which are: 1. The relationship between the future and the past 2. The idea of “authenticity” 3. The notion of music as an abstract artform that can or cannot be reduced to data alone Using these thematic areas as a bedrock, the case study will discuss the three- movement work Silicon, written for the BBC Philharmonic Orchestra. Drawing from my own practice and views the case study will explore how this new technology affects, and will affect, the way an orchestra interacts with a composer, and how orchestral music can be used to explore technology that has an increasingly profound effect on all aspects of our day-to-day lives. Alongside theoretical and aesthetic ties between classical music and artificial intelligence, practical methodologies for utilising artificial intelligence as part of the creative process will be explored. This includes the benefits and limitations of using artificial intelligence to create or develop musical material, long-term structures, or novel synthesized instruments, and also some compositional methodologies I have developed to magnify or mitigate the effect of artificial intelligence on a large-scale work.","['https://openalex.org/W2325249702', 'https://openalex.org/W2789439097', 'https://openalex.org/W3158826226', 'https://openalex.org/W4248428759', 'https://openalex.org/W6770076868', 'https://openalex.org/W3207559276', 'https://openalex.org/W3003668765', 'https://openalex.org/W4220820301', 'https://openalex.org/W3000389243', 'https://openalex.org/W6688413453', 'https://openalex.org/W2560316200', 'https://openalex.org/W2952428868', 'https://openalex.org/W2800068420', 'https://openalex.org/W2794157322', 'https://openalex.org/W3206525235', 'https://openalex.org/W3093199223', 'https://openalex.org/W6749133656', 'https://openalex.org/W2988750365', 'https://openalex.org/W2004353254', 'https://openalex.org/W2792048062', 'https://openalex.org/W2799040008', 'https://openalex.org/W3213967396', 'https://openalex.org/W3037480493', 'https://openalex.org/W597620852', 'https://openalex.org/W2210543184', 'https://openalex.org/W2963575853', 'https://openalex.org/W2930789748']",2024-01-30
https://openalex.org/W3214536476,https://doi.org/10.48550/arxiv.2111.08839,Zero-shot Singing Technique Conversion,"In this paper we propose modifications to the neural network framework, AutoVC for the task of singing technique conversion. This includes utilising a pretrained singing technique encoder which extracts technique information, upon which a decoder is conditioned during training. By swapping out a source singer's technique information for that of the target's during conversion, the input spectrogram is reconstructed with the target's technique. We document the beneficial effects of omitting the latent loss, the importance of sequential training, and our process for fine-tuning the bottleneck. We also conducted a listening study where participants rate the specificity of technique-converted voices as well as their naturalness. From this we are able to conclude how effective the technique conversions are and how different conditions affect them, while assessing the model's ability to reconstruct its input data.","['https://openalex.org/W2972812066', 'https://openalex.org/W3035014310', 'https://openalex.org/W3015434413', 'https://openalex.org/W2296724634', 'https://openalex.org/W3160569418', 'https://openalex.org/W2963568578', 'https://openalex.org/W2963451564', 'https://openalex.org/W1901129140', 'https://openalex.org/W2962788625', 'https://openalex.org/W2527729766', 'https://openalex.org/W2201092681', 'https://openalex.org/W2767757834', 'https://openalex.org/W3015805741', 'https://openalex.org/W3016250102', 'https://openalex.org/W3109050852', 'https://openalex.org/W2973046048', 'https://openalex.org/W3034794073', 'https://openalex.org/W2994584361', 'https://openalex.org/W2949281321', 'https://openalex.org/W3015268063', 'https://openalex.org/W2903032817', 'https://openalex.org/W2887264325', 'https://openalex.org/W3020570669', 'https://openalex.org/W2808706139', 'https://openalex.org/W3213967396']",2021-11-16
https://openalex.org/W4388824457,https://doi.org/10.1007/978-3-031-48044-7_1,The Pendular Graph: Visualising Hierarchical Repetitive Structure in Point-Set Representations of the POP909 Music Dataset,,"['https://openalex.org/W2906214917', 'https://openalex.org/W6813315704', 'https://openalex.org/W2921128774', 'https://openalex.org/W2111066885', 'https://openalex.org/W3213967396', 'https://openalex.org/W2460900141', 'https://openalex.org/W2090095712', 'https://openalex.org/W2142177998', 'https://openalex.org/W2002661161', 'https://openalex.org/W4249969361', 'https://openalex.org/W3040638667', 'https://openalex.org/W3187543148', 'https://openalex.org/W6781910838', 'https://openalex.org/W2167983404', 'https://openalex.org/W2023723978']",2023-01-01
https://openalex.org/W3119308075,https://doi.org/10.18653/v1/2021.acl-long.80,"VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",International audience,"['https://openalex.org/W2985913104', 'https://openalex.org/W2933138175', 'https://openalex.org/W2939710050', 'https://openalex.org/W3092424727', 'https://openalex.org/W2111316763', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963250244', 'https://openalex.org/W3015698636', 'https://openalex.org/W3093533780', 'https://openalex.org/W3118578889', 'https://openalex.org/W4300047444', 'https://openalex.org/W2972630480', 'https://openalex.org/W3037057938', 'https://openalex.org/W2842511635', 'https://openalex.org/W3134644026', 'https://openalex.org/W2395899413', 'https://openalex.org/W2251542837', 'https://openalex.org/W2134800885', 'https://openalex.org/W3049256661', 'https://openalex.org/W2966095117', 'https://openalex.org/W2520160253', 'https://openalex.org/W3016181583', 'https://openalex.org/W2995181338', 'https://openalex.org/W3007068036', 'https://openalex.org/W3144810982', 'https://openalex.org/W3160235762', 'https://openalex.org/W3160525311', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W1494198834', 'https://openalex.org/W2937197076', 'https://openalex.org/W2972495969', 'https://openalex.org/W2398171323', 'https://openalex.org/W3015522062', 'https://openalex.org/W3092085609', 'https://openalex.org/W3175871055', 'https://openalex.org/W3095410713', 'https://openalex.org/W3099782249', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015783745', 'https://openalex.org/W3094637009', 'https://openalex.org/W4301980136', 'https://openalex.org/W3093579165', 'https://openalex.org/W2945700568', 'https://openalex.org/W22168010', 'https://openalex.org/W620750443', 'https://openalex.org/W2963620343', 'https://openalex.org/W4385245566', 'https://openalex.org/W2461231802', 'https://openalex.org/W2953190524', 'https://openalex.org/W3198429080', 'https://openalex.org/W3054645415', 'https://openalex.org/W3097787369', 'https://openalex.org/W3148101939', 'https://openalex.org/W2973088264']",2021-01-01
https://openalex.org/W3203407300,https://doi.org/10.1109/icassp43922.2022.9746682,WENETSPEECH: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition,"In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation – Dev for cross-validation purpose in training, Test_Net, collected from Internet for matched test, and Test_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.","['https://openalex.org/W2739883972', 'https://openalex.org/W6739901393', 'https://openalex.org/W2514741789', 'https://openalex.org/W2963211739', 'https://openalex.org/W2194187530', 'https://openalex.org/W3163793923', 'https://openalex.org/W2936774411', 'https://openalex.org/W2802023636', 'https://openalex.org/W2888867175', 'https://openalex.org/W2327501763', 'https://openalex.org/W6780815891', 'https://openalex.org/W6728030952', 'https://openalex.org/W2892009249', 'https://openalex.org/W3097777922', 'https://openalex.org/W6631362777', 'https://openalex.org/W2158791054', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962780374', 'https://openalex.org/W3197478142', 'https://openalex.org/W2973049979', 'https://openalex.org/W3198429080', 'https://openalex.org/W6687566353', 'https://openalex.org/W6754473786', 'https://openalex.org/W2127141656', 'https://openalex.org/W2143612262', 'https://openalex.org/W2519818067', 'https://openalex.org/W6638749077', 'https://openalex.org/W6796463219', 'https://openalex.org/W3151526698', 'https://openalex.org/W2147768505', 'https://openalex.org/W6623517193', 'https://openalex.org/W2160815625', 'https://openalex.org/W6780218876', 'https://openalex.org/W6795952400', 'https://openalex.org/W3160799772', 'https://openalex.org/W2963242190', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W3095410713', 'https://openalex.org/W1828163288', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099782249', 'https://openalex.org/W2526425061', 'https://openalex.org/W2184045248', 'https://openalex.org/W3128442956', 'https://openalex.org/W3042170933', 'https://openalex.org/W3036601975', 'https://openalex.org/W3168612151', 'https://openalex.org/W3165666670', 'https://openalex.org/W3037057938', 'https://openalex.org/W2889048668', 'https://openalex.org/W3169688220', 'https://openalex.org/W1524333225', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197917733', 'https://openalex.org/W854541894', 'https://openalex.org/W4287173589']",2022-04-27
https://openalex.org/W3173767661,https://doi.org/10.18653/v1/2021.acl-long.68,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,"Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2933138175', 'https://openalex.org/W3092424727', 'https://openalex.org/W3082274269', 'https://openalex.org/W3102342027', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035202887', 'https://openalex.org/W2963779652', 'https://openalex.org/W3118578889', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971274815', 'https://openalex.org/W3016181583', 'https://openalex.org/W3046368065', 'https://openalex.org/W3099782249', 'https://openalex.org/W2983040767', 'https://openalex.org/W2973049979', 'https://openalex.org/W2964303773', 'https://openalex.org/W2966715458', 'https://openalex.org/W3037542581', 'https://openalex.org/W2989195139', 'https://openalex.org/W2972448360', 'https://openalex.org/W4287694131', 'https://openalex.org/W3034571331', 'https://openalex.org/W3037057938', 'https://openalex.org/W2963460174', 'https://openalex.org/W2981851019', 'https://openalex.org/W4288089799', 'https://openalex.org/W3036601975', 'https://openalex.org/W2970854433', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015698636', 'https://openalex.org/W4288026527', 'https://openalex.org/W3015633994', 'https://openalex.org/W2945260553', 'https://openalex.org/W3198429080', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970608575', 'https://openalex.org/W3023622314', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964161387', 'https://openalex.org/W3008125272', 'https://openalex.org/W2914120296', 'https://openalex.org/W3176711365', 'https://openalex.org/W2997436923', 'https://openalex.org/W2969876226', 'https://openalex.org/W4287824654', 'https://openalex.org/W3153805297', 'https://openalex.org/W2970120757', 'https://openalex.org/W2964172053', 'https://openalex.org/W3107826490', 'https://openalex.org/W3101498587', 'https://openalex.org/W2593011301', 'https://openalex.org/W3037465386', 'https://openalex.org/W3035390927', 'https://openalex.org/W2963532001', 'https://openalex.org/W2936848022', 'https://openalex.org/W3054645415', 'https://openalex.org/W2949328740', 'https://openalex.org/W2958953787', 'https://openalex.org/W3162037819']",2021-01-01
https://openalex.org/W3112702554,https://doi.org/10.1109/ojsp.2020.3045349,Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview,"We present a structured overview of adaptation algorithms for neural\nnetwork-based speech recognition, considering both hybrid hidden Markov model /\nneural network systems and end-to-end neural network systems, with a focus on\nspeaker adaptation, domain adaptation, and accent adaptation. The overview\ncharacterizes adaptation algorithms as based on embeddings, model parameter\nadaptation, or data augmentation. We present a meta-analysis of the performance\nof speech recognition adaptation algorithms, based on relative error rate\nreductions as reported in the literature.\n","['https://openalex.org/W2889152503', 'https://openalex.org/W2905678190', 'https://openalex.org/W2005874308', 'https://openalex.org/W2890144771', 'https://openalex.org/W6680532216', 'https://openalex.org/W6607455302', 'https://openalex.org/W6712868603', 'https://openalex.org/W6740674931', 'https://openalex.org/W2171928131', 'https://openalex.org/W2963381607', 'https://openalex.org/W2514740276', 'https://openalex.org/W2972322676', 'https://openalex.org/W2963596039', 'https://openalex.org/W6696912008', 'https://openalex.org/W2973213659', 'https://openalex.org/W2963288281', 'https://openalex.org/W2056738732', 'https://openalex.org/W2508162385', 'https://openalex.org/W2150884987', 'https://openalex.org/W2253429366', 'https://openalex.org/W2117539524', 'https://openalex.org/W2972389417', 'https://openalex.org/W2936078256', 'https://openalex.org/W2892009249', 'https://openalex.org/W6739901393', 'https://openalex.org/W6602065484', 'https://openalex.org/W2108598243', 'https://openalex.org/W2127602683', 'https://openalex.org/W6784418856', 'https://openalex.org/W6631552591', 'https://openalex.org/W3016010032', 'https://openalex.org/W6769806307', 'https://openalex.org/W2595767284', 'https://openalex.org/W3016234571', 'https://openalex.org/W3008037978', 'https://openalex.org/W2799800213', 'https://openalex.org/W2963240019', 'https://openalex.org/W2939164678', 'https://openalex.org/W1710082047', 'https://openalex.org/W2972799770', 'https://openalex.org/W6640059789', 'https://openalex.org/W6744707562', 'https://openalex.org/W6727099177', 'https://openalex.org/W2056445351', 'https://openalex.org/W2939173691', 'https://openalex.org/W6639822467', 'https://openalex.org/W2696967604', 'https://openalex.org/W2117678320', 'https://openalex.org/W6713403589', 'https://openalex.org/W2913340405', 'https://openalex.org/W4253573210', 'https://openalex.org/W6638749077', 'https://openalex.org/W3006713268', 'https://openalex.org/W3095311338', 'https://openalex.org/W6640708543', 'https://openalex.org/W2963827914', 'https://openalex.org/W2963414781', 'https://openalex.org/W3007227084', 'https://openalex.org/W2962760690', 'https://openalex.org/W2513257850', 'https://openalex.org/W2769205094', 'https://openalex.org/W6731780175', 'https://openalex.org/W2127141656', 'https://openalex.org/W2316560407', 'https://openalex.org/W6610566761', 'https://openalex.org/W6770847983', 'https://openalex.org/W2889488531', 'https://openalex.org/W2407299475', 'https://openalex.org/W2288402171', 'https://openalex.org/W2015633636', 'https://openalex.org/W2083751884', 'https://openalex.org/W6754473786', 'https://openalex.org/W6712756720', 'https://openalex.org/W2080005694', 'https://openalex.org/W1513862252', 'https://openalex.org/W6712730384', 'https://openalex.org/W2912331391', 'https://openalex.org/W3015412845', 'https://openalex.org/W2400607496', 'https://openalex.org/W2973036346', 'https://openalex.org/W6615969787', 'https://openalex.org/W2045317385', 'https://openalex.org/W2900200882', 'https://openalex.org/W2046056978', 'https://openalex.org/W2889267317', 'https://openalex.org/W2398776621', 'https://openalex.org/W6713701104', 'https://openalex.org/W1985371235', 'https://openalex.org/W2079623482', 'https://openalex.org/W2586950923', 'https://openalex.org/W2947196194', 'https://openalex.org/W2963300588', 'https://openalex.org/W6753612206', 'https://openalex.org/W2804935296', 'https://openalex.org/W2292517305', 'https://openalex.org/W2087006792', 'https://openalex.org/W6632212032', 'https://openalex.org/W2695252763', 'https://openalex.org/W2394882406', 'https://openalex.org/W6713250295', 'https://openalex.org/W3008391164', 'https://openalex.org/W2641129314', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963266252', 'https://openalex.org/W2327501763', 'https://openalex.org/W2100772283', 'https://openalex.org/W2070725064', 'https://openalex.org/W1993409002', 'https://openalex.org/W1517386993', 'https://openalex.org/W2998284473', 'https://openalex.org/W82936479', 'https://openalex.org/W6676923526', 'https://openalex.org/W6683161958', 'https://openalex.org/W2979509742', 'https://openalex.org/W6759579507', 'https://openalex.org/W6735236233', 'https://openalex.org/W6743661861', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962739339', 'https://openalex.org/W2891177506', 'https://openalex.org/W6769627184', 'https://openalex.org/W2036351241', 'https://openalex.org/W2403731734', 'https://openalex.org/W6738395172', 'https://openalex.org/W2058552721', 'https://openalex.org/W2963364041', 'https://openalex.org/W2589099502', 'https://openalex.org/W6605298800', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125107268', 'https://openalex.org/W6632550522', 'https://openalex.org/W2048526313', 'https://openalex.org/W2024490156', 'https://openalex.org/W2239847623', 'https://openalex.org/W6789826613', 'https://openalex.org/W6691770337', 'https://openalex.org/W6691509046', 'https://openalex.org/W2802248956', 'https://openalex.org/W6686282164', 'https://openalex.org/W6764584974', 'https://openalex.org/W2106051978', 'https://openalex.org/W2166637769', 'https://openalex.org/W2258198666', 'https://openalex.org/W3015501067', 'https://openalex.org/W3012043827', 'https://openalex.org/W6779469704', 'https://openalex.org/W6639518841', 'https://openalex.org/W2559260703', 'https://openalex.org/W1592672653', 'https://openalex.org/W1526236009', 'https://openalex.org/W2081074144', 'https://openalex.org/W6602944679', 'https://openalex.org/W2069631319', 'https://openalex.org/W6629448772', 'https://openalex.org/W2276408190', 'https://openalex.org/W6603970283', 'https://openalex.org/W6713009045', 'https://openalex.org/W2117620049', 'https://openalex.org/W2033256038', 'https://openalex.org/W2288371874', 'https://openalex.org/W1509793305', 'https://openalex.org/W2511419867', 'https://openalex.org/W6752888775', 'https://openalex.org/W6713762819', 'https://openalex.org/W6775382371', 'https://openalex.org/W2156142001', 'https://openalex.org/W6607040217', 'https://openalex.org/W2106119541', 'https://openalex.org/W2617258110', 'https://openalex.org/W2140567543', 'https://openalex.org/W1992475611', 'https://openalex.org/W6675409298', 'https://openalex.org/W2156886787', 'https://openalex.org/W2093886330', 'https://openalex.org/W2151262064', 'https://openalex.org/W2118850452', 'https://openalex.org/W6603743288', 'https://openalex.org/W2123867168', 'https://openalex.org/W2146871184', 'https://openalex.org/W2092186611', 'https://openalex.org/W6601083597', 'https://openalex.org/W2100969003', 'https://openalex.org/W6773205534', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W2398054607', 'https://openalex.org/W1658791872', 'https://openalex.org/W1961429348', 'https://openalex.org/W2964309797', 'https://openalex.org/W2511277277', 'https://openalex.org/W2963431393', 'https://openalex.org/W6780788142', 'https://openalex.org/W2971840980', 'https://openalex.org/W4205130185', 'https://openalex.org/W6779919476', 'https://openalex.org/W4205286048', 'https://openalex.org/W3096032230', 'https://openalex.org/W2990706771', 'https://openalex.org/W2914282508', 'https://openalex.org/W1497807607', 'https://openalex.org/W1945532488', 'https://openalex.org/W6600211438', 'https://openalex.org/W2963499843', 'https://openalex.org/W3007095894', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963486098', 'https://openalex.org/W6767670687', 'https://openalex.org/W6605885551', 'https://openalex.org/W2514741789', 'https://openalex.org/W2756127416', 'https://openalex.org/W6639627149', 'https://openalex.org/W6680187304', 'https://openalex.org/W1967452917', 'https://openalex.org/W6696757691', 'https://openalex.org/W6713969595', 'https://openalex.org/W1994606281', 'https://openalex.org/W1978660892', 'https://openalex.org/W2025198378', 'https://openalex.org/W2579851902', 'https://openalex.org/W6781709318', 'https://openalex.org/W2587080466', 'https://openalex.org/W2889494795', 'https://openalex.org/W2964099675', 'https://openalex.org/W2889785444', 'https://openalex.org/W2938374794', 'https://openalex.org/W2913244614', 'https://openalex.org/W2962893195', 'https://openalex.org/W2962684181', 'https://openalex.org/W2973094925', 'https://openalex.org/W2962940707', 'https://openalex.org/W3094667432', 'https://openalex.org/W3006752097', 'https://openalex.org/W3097794466', 'https://openalex.org/W2903250132', 'https://openalex.org/W2791636785', 'https://openalex.org/W2889374926', 'https://openalex.org/W3007327651', 'https://openalex.org/W6714125293', 'https://openalex.org/W3015915933', 'https://openalex.org/W6712100661', 'https://openalex.org/W2406264770', 'https://openalex.org/W3095468613', 'https://openalex.org/W2584515623', 'https://openalex.org/W2972539100', 'https://openalex.org/W2972815379', 'https://openalex.org/W6712847557', 'https://openalex.org/W2921194440', 'https://openalex.org/W2889423969', 'https://openalex.org/W2750527898', 'https://openalex.org/W2889074736', 'https://openalex.org/W6777176401', 'https://openalex.org/W6697090674', 'https://openalex.org/W2160306971', 'https://openalex.org/W6712892205', 'https://openalex.org/W2515525878', 'https://openalex.org/W7075705205', 'https://openalex.org/W2045644151', 'https://openalex.org/W3006835112', 'https://openalex.org/W2164240571', 'https://openalex.org/W2112582577', 'https://openalex.org/W6711962127', 'https://openalex.org/W4234330420', 'https://openalex.org/W2160815625', 'https://openalex.org/W6686491854', 'https://openalex.org/W2963523217', 'https://openalex.org/W2910715461', 'https://openalex.org/W2680270903', 'https://openalex.org/W2936252403', 'https://openalex.org/W6638523607', 'https://openalex.org/W2911629330', 'https://openalex.org/W2507699225', 'https://openalex.org/W2711861986', 'https://openalex.org/W2962894366', 'https://openalex.org/W3008008574', 'https://openalex.org/W2767005120', 'https://openalex.org/W2128408412', 'https://openalex.org/W2165108269', 'https://openalex.org/W1599512239', 'https://openalex.org/W6605201677', 'https://openalex.org/W2002342963', 'https://openalex.org/W2145691584', 'https://openalex.org/W2055226387', 'https://openalex.org/W2890964092', 'https://openalex.org/W2094147890', 'https://openalex.org/W2150769028', 'https://openalex.org/W6639480849', 'https://openalex.org/W2587088898', 'https://openalex.org/W2795867901', 'https://openalex.org/W2787133659', 'https://openalex.org/W2769025471', 'https://openalex.org/W6753379557', 'https://openalex.org/W2888858245', 'https://openalex.org/W2963057973', 'https://openalex.org/W1665921526', 'https://openalex.org/W2117671523', 'https://openalex.org/W2020073413', 'https://openalex.org/W6712930963', 'https://openalex.org/W2112796928', 'https://openalex.org/W1995562189', 'https://openalex.org/W2064675550', 'https://openalex.org/W2143612262', 'https://openalex.org/W2131774270', 'https://openalex.org/W2972621414', 'https://openalex.org/W2005708641', 'https://openalex.org/W6623517193', 'https://openalex.org/W2963211739', 'https://openalex.org/W2973051376', 'https://openalex.org/W1989549063', 'https://openalex.org/W2010362084', 'https://openalex.org/W3016008406', 'https://openalex.org/W6713823255', 'https://openalex.org/W3007550212', 'https://openalex.org/W3097904285', 'https://openalex.org/W2886319145', 'https://openalex.org/W6674367285', 'https://openalex.org/W6629552394', 'https://openalex.org/W2127836646', 'https://openalex.org/W1481113913', 'https://openalex.org/W2138309071', 'https://openalex.org/W6633028107', 'https://openalex.org/W2739427748', 'https://openalex.org/W3102342027', 'https://openalex.org/W173010698', 'https://openalex.org/W3037057938', 'https://openalex.org/W3127686677', 'https://openalex.org/W2957823618', 'https://openalex.org/W2099621636', 'https://openalex.org/W1526361935', 'https://openalex.org/W4385245566', 'https://openalex.org/W3041846892', 'https://openalex.org/W92539696', 'https://openalex.org/W2137354628', 'https://openalex.org/W1553387275', 'https://openalex.org/W2184135559', 'https://openalex.org/W4288301759', 'https://openalex.org/W2963571818', 'https://openalex.org/W2156692643', 'https://openalex.org/W4285719527', 'https://openalex.org/W2884636860', 'https://openalex.org/W2402146185', 'https://openalex.org/W3042359118', 'https://openalex.org/W3013079409', 'https://openalex.org/W2601450892', 'https://openalex.org/W2399550240', 'https://openalex.org/W2963938518', 'https://openalex.org/W2975901202', 'https://openalex.org/W2142416747', 'https://openalex.org/W1915251500', 'https://openalex.org/W2776923314', 'https://openalex.org/W2889048668', 'https://openalex.org/W2402040300', 'https://openalex.org/W2144792281', 'https://openalex.org/W2013996527', 'https://openalex.org/W1588593315', 'https://openalex.org/W3023256384', 'https://openalex.org/W2772270478', 'https://openalex.org/W2407080277', 'https://openalex.org/W2808706139', 'https://openalex.org/W70088150', 'https://openalex.org/W4302557958', 'https://openalex.org/W3015216127', 'https://openalex.org/W4308252168', 'https://openalex.org/W4288337064', 'https://openalex.org/W2403307129', 'https://openalex.org/W2963376890', 'https://openalex.org/W1979651826', 'https://openalex.org/W2095848329', 'https://openalex.org/W26405188', 'https://openalex.org/W567546468', 'https://openalex.org/W2294543795', 'https://openalex.org/W2757047188', 'https://openalex.org/W50249030', 'https://openalex.org/W1828163288', 'https://openalex.org/W3082274269', 'https://openalex.org/W2183016404', 'https://openalex.org/W4288102383', 'https://openalex.org/W2404620314', 'https://openalex.org/W2963403868', 'https://openalex.org/W854541894', 'https://openalex.org/W3002741552', 'https://openalex.org/W3104245447', 'https://openalex.org/W4288089799', 'https://openalex.org/W2963432880', 'https://openalex.org/W2952137015', 'https://openalex.org/W2963341956', 'https://openalex.org/W3156786336', 'https://openalex.org/W182840523', 'https://openalex.org/W2407793339', 'https://openalex.org/W2407320368', 'https://openalex.org/W1501429302', 'https://openalex.org/W2401277329', 'https://openalex.org/W2964182776', 'https://openalex.org/W3198429080', 'https://openalex.org/W2889158616', 'https://openalex.org/W2753160622', 'https://openalex.org/W2398754323', 'https://openalex.org/W1893339448', 'https://openalex.org/W2964303773', 'https://openalex.org/W3034729383', 'https://openalex.org/W2992035660', 'https://openalex.org/W72302491', 'https://openalex.org/W2737615485', 'https://openalex.org/W1892978018', 'https://openalex.org/W3125118953', 'https://openalex.org/W2998704965', 'https://openalex.org/W2394932179', 'https://openalex.org/W1942035323', 'https://openalex.org/W3010079431', 'https://openalex.org/W4292198620', 'https://openalex.org/W1736701665', 'https://openalex.org/W2951672049', 'https://openalex.org/W2115353073', 'https://openalex.org/W1882958252', 'https://openalex.org/W130324013', 'https://openalex.org/W5524598', 'https://openalex.org/W3105315310', 'https://openalex.org/W2747616140', 'https://openalex.org/W2982413405', 'https://openalex.org/W2250357346', 'https://openalex.org/W2125838338', 'https://openalex.org/W97192492', 'https://openalex.org/W2251321385', 'https://openalex.org/W2976324903', 'https://openalex.org/W2760103357', 'https://openalex.org/W4288088457', 'https://openalex.org/W2039057510', 'https://openalex.org/W1537275613', 'https://openalex.org/W2295119550', 'https://openalex.org/W1540779870', 'https://openalex.org/W4298422451', 'https://openalex.org/W2294108103', 'https://openalex.org/W3106526813', 'https://openalex.org/W143647774', 'https://openalex.org/W1492293509', 'https://openalex.org/W2970351109', 'https://openalex.org/W2953937146', 'https://openalex.org/W1821462560', 'https://openalex.org/W2963826681', 'https://openalex.org/W2896457183', 'https://openalex.org/W127709214', 'https://openalex.org/W3161873870', 'https://openalex.org/W2401396087', 'https://openalex.org/W2405866807', 'https://openalex.org/W2571859396', 'https://openalex.org/W2617565145', 'https://openalex.org/W2399832792', 'https://openalex.org/W1492775261', 'https://openalex.org/W2963494889', 'https://openalex.org/W4302176825', 'https://openalex.org/W2963090522', 'https://openalex.org/W3150807214', 'https://openalex.org/W2401812832', 'https://openalex.org/W2396230943']",2020-12-17
https://openalex.org/W3217767527,https://doi.org/10.1109/icassp43922.2022.9747674,ESPnet-SLU: Advancing Spoken Language Understanding Through ESPnet,"As Automatic Speech Processing (ASR) systems are getting better, there is an increasing interest of using the ASR output to do downstream Natural Language Processing (NLP) tasks. However, there are few open source toolkits that can be used to generate reproducible results on different Spoken Language Understanding (SLU) benchmarks. Hence, there is a need to build an open source standard that can be used to have a faster start into SLU research. We present ESPnet-SLU, which is designed for quick development of spoken language understanding in a single framework. ESPnet-SLU is a project inside end-to-end speech processing toolkit, ESPnet, which is a widely used open-source standard for various speech processing tasks like ASR, Text to Speech (TTS) and Speech Translation (ST). We enhance the toolkit to provide implementations for various SLU benchmarks that enable researchers to seamlessly mix-and-match different ASR and NLU models. We also provide pretrained models with intensively tuned hyper-parameters that can match or even outperform the current state-of-the-art performances. The toolkit is publicly available at https://github.com/espnet/espnet.","['https://openalex.org/W2972430654', 'https://openalex.org/W6741704050', 'https://openalex.org/W3151851237', 'https://openalex.org/W3161223924', 'https://openalex.org/W3110740243', 'https://openalex.org/W3092368332', 'https://openalex.org/W6752049284', 'https://openalex.org/W3198555173', 'https://openalex.org/W2951910991', 'https://openalex.org/W6751425476', 'https://openalex.org/W2971351151', 'https://openalex.org/W6780218876', 'https://openalex.org/W3097286738', 'https://openalex.org/W3041561163', 'https://openalex.org/W6776148200', 'https://openalex.org/W6755207826', 'https://openalex.org/W3167533889', 'https://openalex.org/W3198361190', 'https://openalex.org/W2933138175', 'https://openalex.org/W2979826702', 'https://openalex.org/W2559260703', 'https://openalex.org/W2972541922', 'https://openalex.org/W2962803631', 'https://openalex.org/W2964331270', 'https://openalex.org/W2917937435', 'https://openalex.org/W6779919476', 'https://openalex.org/W3197697656', 'https://openalex.org/W3197314576', 'https://openalex.org/W3198594919', 'https://openalex.org/W1989271290', 'https://openalex.org/W6755871579', 'https://openalex.org/W2962780374', 'https://openalex.org/W3016160783', 'https://openalex.org/W3037217258', 'https://openalex.org/W2936774411', 'https://openalex.org/W3097777922', 'https://openalex.org/W3209059054', 'https://openalex.org/W3100460087', 'https://openalex.org/W2146334809', 'https://openalex.org/W3197674197', 'https://openalex.org/W2128970689', 'https://openalex.org/W2963968160', 'https://openalex.org/W2972818416', 'https://openalex.org/W6785224828', 'https://openalex.org/W2166637769', 'https://openalex.org/W2766219058', 'https://openalex.org/W3197580070', 'https://openalex.org/W3096251052', 'https://openalex.org/W6784579990', 'https://openalex.org/W6750665317', 'https://openalex.org/W2981269614', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962756109', 'https://openalex.org/W3143995484', 'https://openalex.org/W2739936944', 'https://openalex.org/W3105163367', 'https://openalex.org/W2803609229', 'https://openalex.org/W3095384101', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W4287596060', 'https://openalex.org/W2896457183', 'https://openalex.org/W3176896803', 'https://openalex.org/W4297683418', 'https://openalex.org/W3098169801', 'https://openalex.org/W2171144711', 'https://openalex.org/W3098824823', 'https://openalex.org/W2963341956', 'https://openalex.org/W3017650141', 'https://openalex.org/W2805479865', 'https://openalex.org/W3169320628', 'https://openalex.org/W3036601975', 'https://openalex.org/W3007328579', 'https://openalex.org/W3025165719', 'https://openalex.org/W2797583228', 'https://openalex.org/W2962745521', 'https://openalex.org/W3016473712']",2022-04-27
https://openalex.org/W3212799896,https://doi.org/10.1109/icassp43922.2022.9746038,Joint Unsupervised and Supervised Training for Multilingual ASR,"Self-supervised training has shown promising gains in pretraining models and facilitating the downstream finetuning for speech recognition, like multilingual ASR. Most existing methods adopt a 2-stage scheme where the self-supervised loss is optimized in the first pretraining stage, and the standard supervised finetuning resumes in the second stage. In this paper, we propose an end-to-end (E2E) Joint Unsupervised and Supervised Training (JUST) method to combine the supervised RNN-T loss and the self-supervised contrastive and masked language modeling (MLM) losses. We validate its performance on the public dataset Multilingual LibriSpeech (MLS), which includes 8 languages and is extremely imbalanced. On MLS, we explore (1) JUST trained from scratch, and (2) JUST finetuned from a pretrained checkpoint. Experiments show that JUST can consistently outperform other existing state-of-the-art methods, and beat the monolingual baseline by a significant margin, demonstrating JUST's capability of handling low-resource languages in multilingual ASR. Our average WER of all languages outperforms average monolingual baseline by 33.3%, and the state-of-the-art 2-stage XLSR by 32%. On low-resource languages like Polish, our WER is less than half of the monolingual baseline and even beats the supervised transfer learning method which uses external supervision.","['https://openalex.org/W3104215796', 'https://openalex.org/W6799245484', 'https://openalex.org/W3095410713', 'https://openalex.org/W4210463634', 'https://openalex.org/W6767737316', 'https://openalex.org/W3016181583', 'https://openalex.org/W6779919476', 'https://openalex.org/W6638749077', 'https://openalex.org/W6790807788', 'https://openalex.org/W6631190155', 'https://openalex.org/W3003875258', 'https://openalex.org/W3097777922', 'https://openalex.org/W4210690962', 'https://openalex.org/W6784532283', 'https://openalex.org/W6784614252', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973157397', 'https://openalex.org/W2127141656', 'https://openalex.org/W6784807880', 'https://openalex.org/W6788335241', 'https://openalex.org/W6729448088', 'https://openalex.org/W6801625237', 'https://openalex.org/W2970049541', 'https://openalex.org/W2935542736', 'https://openalex.org/W3197074955', 'https://openalex.org/W3094637009', 'https://openalex.org/W3160235762', 'https://openalex.org/W4297808394', 'https://openalex.org/W3167207712', 'https://openalex.org/W3037057938', 'https://openalex.org/W3099782249', 'https://openalex.org/W3193521535', 'https://openalex.org/W3093579165', 'https://openalex.org/W1828163288', 'https://openalex.org/W3198429080', 'https://openalex.org/W2547875792', 'https://openalex.org/W3202037040', 'https://openalex.org/W3036601975', 'https://openalex.org/W2914120296', 'https://openalex.org/W3169320628', 'https://openalex.org/W3017961061', 'https://openalex.org/W3128768055', 'https://openalex.org/W3186596101', 'https://openalex.org/W3166278947', 'https://openalex.org/W3025165719', 'https://openalex.org/W3131692382', 'https://openalex.org/W2963341956', 'https://openalex.org/W3157697407', 'https://openalex.org/W2896457183', 'https://openalex.org/W1522301498']",2022-04-27
https://openalex.org/W4403558103,https://doi.org/10.1016/j.aej.2024.10.065,Transforming English language learning: Advanced speech recognition with MLP-LSTM for personalized education,"Speaking of speech recognition within the English language, it is the process of recognizing oral speech and transcribing it into writing using exclusive algorithms. For the perishable skill of English language learning, use of innovative speech recognition technology using Advanced Speech Recognition Technologies MLP-LSTM is proposed in this paper to advance the existing online learning platforms. Previous research addresses the importance of NLP in English language learning but notes the challenges in effectively extracting and segmenting features from multimodal data. In order to overcome these problems, this paper incorporate the proposed MLP for feature extraction and LSTM for sequence learning. The utilization of MLP-LSTM provides not only a brilliant improvement of the capacity to transform spoken language and perceive it but also minimizes the Word Error Rate (WER) to 0.075. With this low WER, along with the total accuracy rate of 98.25 %, this paper focus on underlining how this system is more effective than traditional language learning tools. This paper has been implemented through Python Software. The given MLP-LSTM based speech recognition model lays the foundation for a highly complex yet accurate paced English language learning platform that will cater to the needs of the learners in the global scenario.","['https://openalex.org/W4307286264', 'https://openalex.org/W3132830522', 'https://openalex.org/W6795952400', 'https://openalex.org/W6793802969', 'https://openalex.org/W4385260920', 'https://openalex.org/W3010748916', 'https://openalex.org/W3002741552', 'https://openalex.org/W2981858587', 'https://openalex.org/W6782880199', 'https://openalex.org/W4221161761', 'https://openalex.org/W6785154326', 'https://openalex.org/W3037057938', 'https://openalex.org/W3207558756', 'https://openalex.org/W4311000453', 'https://openalex.org/W4225274946', 'https://openalex.org/W2998386507', 'https://openalex.org/W3011516543', 'https://openalex.org/W3017022801', 'https://openalex.org/W3118883493', 'https://openalex.org/W2971766686', 'https://openalex.org/W3176616792', 'https://openalex.org/W3006156918', 'https://openalex.org/W4319302797', 'https://openalex.org/W3081285175', 'https://openalex.org/W2964732384', 'https://openalex.org/W6646543388', 'https://openalex.org/W4385692946', 'https://openalex.org/W3156503941', 'https://openalex.org/W3122499880']",2024-10-19
https://openalex.org/W3213618310,https://doi.org/10.1609/aaai.v36i10.21327,Towards Building ASR Systems for the Next Billion Users,"Recent methods in speech and language technology pretrain very large models which are fine-tuned for specific tasks. However, the benefits of such large models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vectors of similar sounding phonemes are shared across languages, representations across layers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent.","['https://openalex.org/W2958953787', 'https://openalex.org/W2889169007', 'https://openalex.org/W2983040767', 'https://openalex.org/W6767737316', 'https://openalex.org/W2896457183', 'https://openalex.org/W6699243985', 'https://openalex.org/W2889427787', 'https://openalex.org/W7027429494', 'https://openalex.org/W3177999760', 'https://openalex.org/W2134800885', 'https://openalex.org/W2124509324', 'https://openalex.org/W3099919888', 'https://openalex.org/W1574170747', 'https://openalex.org/W2895676041', 'https://openalex.org/W2402146185', 'https://openalex.org/W2953190524', 'https://openalex.org/W6753742708', 'https://openalex.org/W3097106378', 'https://openalex.org/W3142385198', 'https://openalex.org/W3160186953', 'https://openalex.org/W2895103375', 'https://openalex.org/W572280432', 'https://openalex.org/W3046368065', 'https://openalex.org/W3121299949', 'https://openalex.org/W1494198834', 'https://openalex.org/W3147984406', 'https://openalex.org/W2781384251', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963341956', 'https://openalex.org/W2933138175', 'https://openalex.org/W4287694131', 'https://openalex.org/W4287374065', 'https://openalex.org/W2936774411', 'https://openalex.org/W3035390927', 'https://openalex.org/W4394642489', 'https://openalex.org/W3198429080', 'https://openalex.org/W3099782249', 'https://openalex.org/W4297818305', 'https://openalex.org/W4287075881', 'https://openalex.org/W2127141656', 'https://openalex.org/W2889275521', 'https://openalex.org/W2952809536', 'https://openalex.org/W3037057938', 'https://openalex.org/W3173217749', 'https://openalex.org/W2914120296', 'https://openalex.org/W3012541776', 'https://openalex.org/W3167207712', 'https://openalex.org/W2941814890', 'https://openalex.org/W2991213871', 'https://openalex.org/W3195874849', 'https://openalex.org/W2316579313', 'https://openalex.org/W2988736778', 'https://openalex.org/W3027083471', 'https://openalex.org/W2908336025']",2022-06-28
https://openalex.org/W3180180466,https://doi.org/10.1109/icassp43922.2022.9747667,Improved Language Identification Through Cross-Lingual Self-Supervised Learning,"Language identification greatly impacts the success of downstream tasks such as automatic speech recognition. Recently, self-supervised speech representations learned by wav2vec 2.0 have been shown to be very effective for a range of speech tasks. We extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple languages and not just on English. We show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled data to perform well. Results on a 26 languages setup show that with only 10 minutes of labeled data per language, a cross-lingually pre-trained model can achieve over 89.2% accuracy.","['https://openalex.org/W6772381481', 'https://openalex.org/W6755207826', 'https://openalex.org/W6735377749', 'https://openalex.org/W3198771897', 'https://openalex.org/W2748488820', 'https://openalex.org/W6631190155', 'https://openalex.org/W2130414229', 'https://openalex.org/W1978660892', 'https://openalex.org/W2014869452', 'https://openalex.org/W2894835365', 'https://openalex.org/W2964309797', 'https://openalex.org/W2971840980', 'https://openalex.org/W2964002616', 'https://openalex.org/W3096032230', 'https://openalex.org/W2172287020', 'https://openalex.org/W2056119007', 'https://openalex.org/W6769196770', 'https://openalex.org/W6770506093', 'https://openalex.org/W2124509324', 'https://openalex.org/W2981857663', 'https://openalex.org/W3026041220', 'https://openalex.org/W6729448088', 'https://openalex.org/W3096338464', 'https://openalex.org/W6780218876', 'https://openalex.org/W2963425185', 'https://openalex.org/W3097777922', 'https://openalex.org/W2123798005', 'https://openalex.org/W2892009249', 'https://openalex.org/W2842511635', 'https://openalex.org/W6787335539', 'https://openalex.org/W2973049979', 'https://openalex.org/W6784614252', 'https://openalex.org/W6779919476', 'https://openalex.org/W6780226713', 'https://openalex.org/W6739901393', 'https://openalex.org/W2963403868', 'https://openalex.org/W4297808394', 'https://openalex.org/W3198275944', 'https://openalex.org/W2963386218', 'https://openalex.org/W3036601975', 'https://openalex.org/W1522301498', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963341956', 'https://openalex.org/W3034772996', 'https://openalex.org/W2997347790', 'https://openalex.org/W3037057938', 'https://openalex.org/W2597655663', 'https://openalex.org/W2979476256', 'https://openalex.org/W2547875792', 'https://openalex.org/W3144173820', 'https://openalex.org/W3093579165', 'https://openalex.org/W3025165719', 'https://openalex.org/W3112616666', 'https://openalex.org/W2991213871', 'https://openalex.org/W3198429080', 'https://openalex.org/W3099782249', 'https://openalex.org/W2896457183', 'https://openalex.org/W4385245566']",2022-04-27
https://openalex.org/W3166631396,https://doi.org/10.1613/jair.1.13083,Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems,"In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent with the aim of completing a concrete task. Although this technology represents one of the central objectives of AI and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an extensive overview of existing methods and resources in multilingual ToD as an entry point to this exciting and emerging field. We find that the most critical factor preventing the creation of truly multilingual ToD systems is the lack of datasets in most languages for both training and evaluation. In fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English), either by means of (i) machine translation or (ii) multilingual representations. These approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. On the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). To overcome this limitation, we draw parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios. Finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current ToD systems.","['https://openalex.org/W6777875527', 'https://openalex.org/W6758715935', 'https://openalex.org/W2949568611', 'https://openalex.org/W2797012101', 'https://openalex.org/W3214578205', 'https://openalex.org/W2958953787', 'https://openalex.org/W3154145628', 'https://openalex.org/W2798908575', 'https://openalex.org/W3016252650', 'https://openalex.org/W6795952400', 'https://openalex.org/W2081795963', 'https://openalex.org/W2910795780', 'https://openalex.org/W3100460087', 'https://openalex.org/W2054177078', 'https://openalex.org/W6766308965', 'https://openalex.org/W6776048684', 'https://openalex.org/W3097696666', 'https://openalex.org/W2127838323', 'https://openalex.org/W6691180772', 'https://openalex.org/W21503661', 'https://openalex.org/W2956901422', 'https://openalex.org/W2891732163', 'https://openalex.org/W3017465475', 'https://openalex.org/W2971737394', 'https://openalex.org/W2995118574', 'https://openalex.org/W3023592562', 'https://openalex.org/W2969955062', 'https://openalex.org/W3012396850', 'https://openalex.org/W2902203763', 'https://openalex.org/W2123640007', 'https://openalex.org/W2953126480', 'https://openalex.org/W2885519967', 'https://openalex.org/W3094437659', 'https://openalex.org/W6992685905', 'https://openalex.org/W2974236805', 'https://openalex.org/W3011663485', 'https://openalex.org/W3037057938', 'https://openalex.org/W2983040767', 'https://openalex.org/W2891555348', 'https://openalex.org/W3035547806', 'https://openalex.org/W3025915678', 'https://openalex.org/W2803609229', 'https://openalex.org/W2294065713', 'https://openalex.org/W2889511806', 'https://openalex.org/W3094176124', 'https://openalex.org/W6731973159', 'https://openalex.org/W6792388242', 'https://openalex.org/W2252216246', 'https://openalex.org/W2558809543', 'https://openalex.org/W2988926962', 'https://openalex.org/W6786130232', 'https://openalex.org/W2896457183', 'https://openalex.org/W2913443447', 'https://openalex.org/W3207583316', 'https://openalex.org/W3122836184', 'https://openalex.org/W3090196146', 'https://openalex.org/W2949925034', 'https://openalex.org/W6684309063', 'https://openalex.org/W2979917395', 'https://openalex.org/W2148708890', 'https://openalex.org/W6754111937', 'https://openalex.org/W2467764055', 'https://openalex.org/W1597744629', 'https://openalex.org/W2345720230', 'https://openalex.org/W2616122292', 'https://openalex.org/W3093871477', 'https://openalex.org/W6787882028', 'https://openalex.org/W2115384955', 'https://openalex.org/W2604763608', 'https://openalex.org/W4224310727', 'https://openalex.org/W2946068894', 'https://openalex.org/W6754313673', 'https://openalex.org/W3088631993', 'https://openalex.org/W2903509201', 'https://openalex.org/W3014153848', 'https://openalex.org/W2604799547', 'https://openalex.org/W6987196544', 'https://openalex.org/W3127861905', 'https://openalex.org/W2738919465', 'https://openalex.org/W2890225082', 'https://openalex.org/W6649207683', 'https://openalex.org/W2914073025', 'https://openalex.org/W2798962680', 'https://openalex.org/W3034402523', 'https://openalex.org/W2904800998', 'https://openalex.org/W2803392141', 'https://openalex.org/W4226155512', 'https://openalex.org/W3158572634', 'https://openalex.org/W1971034924', 'https://openalex.org/W2956612537', 'https://openalex.org/W3017961061', 'https://openalex.org/W3013192639', 'https://openalex.org/W2473329891', 'https://openalex.org/W3034533785', 'https://openalex.org/W2146393546', 'https://openalex.org/W3006485553', 'https://openalex.org/W2026445738', 'https://openalex.org/W3092010805', 'https://openalex.org/W3094140582', 'https://openalex.org/W2988299267', 'https://openalex.org/W6691368330', 'https://openalex.org/W2250297846', 'https://openalex.org/W3093755178', 'https://openalex.org/W2971316210', 'https://openalex.org/W3041108231', 'https://openalex.org/W4226211262', 'https://openalex.org/W6638523607', 'https://openalex.org/W2891717274', 'https://openalex.org/W3109862485', 'https://openalex.org/W3035579820', 'https://openalex.org/W4281482120', 'https://openalex.org/W2970849641', 'https://openalex.org/W3017311573', 'https://openalex.org/W2891896107', 'https://openalex.org/W6633111279', 'https://openalex.org/W2573626026', 'https://openalex.org/W3019003205', 'https://openalex.org/W3125833936', 'https://openalex.org/W2250976127', 'https://openalex.org/W2034384404', 'https://openalex.org/W6836674828', 'https://openalex.org/W2987442863', 'https://openalex.org/W2962910139', 'https://openalex.org/W3018564137', 'https://openalex.org/W6631190155', 'https://openalex.org/W3108950052', 'https://openalex.org/W2803751389', 'https://openalex.org/W3012624518', 'https://openalex.org/W6682058439', 'https://openalex.org/W3136045925', 'https://openalex.org/W3089820221', 'https://openalex.org/W2948798646', 'https://openalex.org/W2250859223', 'https://openalex.org/W2510887268', 'https://openalex.org/W6604798298', 'https://openalex.org/W2843010082', 'https://openalex.org/W2971418622', 'https://openalex.org/W3100198908', 'https://openalex.org/W2141820854', 'https://openalex.org/W3195500343', 'https://openalex.org/W3214585123', 'https://openalex.org/W2164628107', 'https://openalex.org/W3080427942', 'https://openalex.org/W1958706068', 'https://openalex.org/W6714618025', 'https://openalex.org/W6717339413', 'https://openalex.org/W3102483398', 'https://openalex.org/W2947992883', 'https://openalex.org/W3092327118', 'https://openalex.org/W3170632963', 'https://openalex.org/W3022333975', 'https://openalex.org/W2739967986', 'https://openalex.org/W2797760463', 'https://openalex.org/W2328886022', 'https://openalex.org/W3001434439', 'https://openalex.org/W3010930612', 'https://openalex.org/W2971160427', 'https://openalex.org/W2991225581', 'https://openalex.org/W3095391368', 'https://openalex.org/W3115820425', 'https://openalex.org/W2741333084', 'https://openalex.org/W2593751037', 'https://openalex.org/W2606901057', 'https://openalex.org/W2798779216', 'https://openalex.org/W4221144898', 'https://openalex.org/W2974169385', 'https://openalex.org/W6731419696', 'https://openalex.org/W2106087324', 'https://openalex.org/W2137871902', 'https://openalex.org/W3044324512', 'https://openalex.org/W2432549722', 'https://openalex.org/W2798902093', 'https://openalex.org/W2620558438', 'https://openalex.org/W3097392354', 'https://openalex.org/W3046670326', 'https://openalex.org/W2798737723', 'https://openalex.org/W2734693922', 'https://openalex.org/W3009586901', 'https://openalex.org/W2902864835', 'https://openalex.org/W2945878859', 'https://openalex.org/W3033400291', 'https://openalex.org/W2798463315', 'https://openalex.org/W2796932047', 'https://openalex.org/W2950797315', 'https://openalex.org/W3007894275', 'https://openalex.org/W2556427613', 'https://openalex.org/W2985891764', 'https://openalex.org/W4280512622', 'https://openalex.org/W3023911605', 'https://openalex.org/W3128424785', 'https://openalex.org/W6763687114', 'https://openalex.org/W1988085234', 'https://openalex.org/W3022671220', 'https://openalex.org/W2810095012', 'https://openalex.org/W2799266483', 'https://openalex.org/W3105096579', 'https://openalex.org/W2970453266', 'https://openalex.org/W3040454670', 'https://openalex.org/W3095410713', 'https://openalex.org/W3034724424', 'https://openalex.org/W3020510661', 'https://openalex.org/W3092860793', 'https://openalex.org/W6744410009', 'https://openalex.org/W6718053083', 'https://openalex.org/W3031882196', 'https://openalex.org/W178897730', 'https://openalex.org/W2405622356', 'https://openalex.org/W4285161860', 'https://openalex.org/W3017604292', 'https://openalex.org/W1985610876', 'https://openalex.org/W6636936614', 'https://openalex.org/W3022089060', 'https://openalex.org/W6754699398', 'https://openalex.org/W2979736636', 'https://openalex.org/W2799016112', 'https://openalex.org/W3090808901', 'https://openalex.org/W3152609875', 'https://openalex.org/W6786438351', 'https://openalex.org/W4283321842', 'https://openalex.org/W3120253119', 'https://openalex.org/W2892131163', 'https://openalex.org/W3167335398', 'https://openalex.org/W3037763555', 'https://openalex.org/W2898856000', 'https://openalex.org/W2915128308', 'https://openalex.org/W2962883855', 'https://openalex.org/W2304545146', 'https://openalex.org/W3032944943', 'https://openalex.org/W2960803587', 'https://openalex.org/W2805523732', 'https://openalex.org/W3023986361', 'https://openalex.org/W2971411841', 'https://openalex.org/W3011555699', 'https://openalex.org/W4221151028', 'https://openalex.org/W2594021297', 'https://openalex.org/W3175060421', 'https://openalex.org/W2786983967', 'https://openalex.org/W1518951372', 'https://openalex.org/W6751912812', 'https://openalex.org/W2892354565', 'https://openalex.org/W2739503347', 'https://openalex.org/W2791169651', 'https://openalex.org/W3021534198', 'https://openalex.org/W4221167491', 'https://openalex.org/W3162711154', 'https://openalex.org/W3123596112', 'https://openalex.org/W2728656723', 'https://openalex.org/W2833021259', 'https://openalex.org/W2970365965', 'https://openalex.org/W6739901393', 'https://openalex.org/W2250711464', 'https://openalex.org/W6652244895', 'https://openalex.org/W2800895358', 'https://openalex.org/W2620812234', 'https://openalex.org/W2129683077', 'https://openalex.org/W2913129712', 'https://openalex.org/W2911588830', 'https://openalex.org/W2998151577', 'https://openalex.org/W2340944142', 'https://openalex.org/W2885421725', 'https://openalex.org/W2607892599', 'https://openalex.org/W6691246603', 'https://openalex.org/W3100110884', 'https://openalex.org/W3019975998', 'https://openalex.org/W2939455697', 'https://openalex.org/W6778125352', 'https://openalex.org/W2891416139', 'https://openalex.org/W2962369866', 'https://openalex.org/W2094472029', 'https://openalex.org/W3022054129', 'https://openalex.org/W2937632168', 'https://openalex.org/W2798392716', 'https://openalex.org/W6634587104', 'https://openalex.org/W1975244201', 'https://openalex.org/W6781533629', 'https://openalex.org/W3048379951', 'https://openalex.org/W2784400615', 'https://openalex.org/W2890969459', 'https://openalex.org/W2988937804', 'https://openalex.org/W2957543415', 'https://openalex.org/W2990149993', 'https://openalex.org/W2822830299', 'https://openalex.org/W3119983867', 'https://openalex.org/W2798367796', 'https://openalex.org/W3015364215', 'https://openalex.org/W2970444947', 'https://openalex.org/W3006892896', 'https://openalex.org/W3040853550', 'https://openalex.org/W2889720764', 'https://openalex.org/W2337363174', 'https://openalex.org/W4225651262', 'https://openalex.org/W4317897852', 'https://openalex.org/W2495448072', 'https://openalex.org/W3099771192', 'https://openalex.org/W4285155368', 'https://openalex.org/W4287900772', 'https://openalex.org/W3091355780', 'https://openalex.org/W3214173179', 'https://openalex.org/W3035276082', 'https://openalex.org/W3176173434', 'https://openalex.org/W3104820280', 'https://openalex.org/W3095012670', 'https://openalex.org/W3035148359', 'https://openalex.org/W2963909453', 'https://openalex.org/W3093517588', 'https://openalex.org/W3035390927', 'https://openalex.org/W2963047838', 'https://openalex.org/W3169483174', 'https://openalex.org/W3100544532', 'https://openalex.org/W2963641152', 'https://openalex.org/W2986193249', 'https://openalex.org/W2963496089', 'https://openalex.org/W2165256480', 'https://openalex.org/W2994963504', 'https://openalex.org/W3156414406', 'https://openalex.org/W4404781009', 'https://openalex.org/W3102854726', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963903950', 'https://openalex.org/W3106321930', 'https://openalex.org/W118215316', 'https://openalex.org/W2983537304', 'https://openalex.org/W2091671846', 'https://openalex.org/W2964101860', 'https://openalex.org/W2963206148', 'https://openalex.org/W2945087694', 'https://openalex.org/W4285260111', 'https://openalex.org/W1645937837', 'https://openalex.org/W2978300271', 'https://openalex.org/W2098880560', 'https://openalex.org/W3045703328', 'https://openalex.org/W3107826490', 'https://openalex.org/W2963797754', 'https://openalex.org/W3100806282', 'https://openalex.org/W2006832571', 'https://openalex.org/W2889326796', 'https://openalex.org/W2953173959', 'https://openalex.org/W2250459435', 'https://openalex.org/W3139634031', 'https://openalex.org/W2963088995', 'https://openalex.org/W2954108589', 'https://openalex.org/W3099744315', 'https://openalex.org/W2760620168', 'https://openalex.org/W3166514234', 'https://openalex.org/W3118093735', 'https://openalex.org/W2963167310', 'https://openalex.org/W4296216888', 'https://openalex.org/W1917215959', 'https://openalex.org/W3034238904', 'https://openalex.org/W3096303254', 'https://openalex.org/W3120168417', 'https://openalex.org/W1994606281', 'https://openalex.org/W2963789888', 'https://openalex.org/W1534317862', 'https://openalex.org/W3034861927', 'https://openalex.org/W3103147437', 'https://openalex.org/W2251058040', 'https://openalex.org/W2139079654', 'https://openalex.org/W3034573951', 'https://openalex.org/W4285188834', 'https://openalex.org/W4287173589', 'https://openalex.org/W3165666670', 'https://openalex.org/W3173954987', 'https://openalex.org/W2962775474', 'https://openalex.org/W2950733326', 'https://openalex.org/W2998653650', 'https://openalex.org/W3104723404', 'https://openalex.org/W2985067290', 'https://openalex.org/W3156038928', 'https://openalex.org/W3088382025', 'https://openalex.org/W4328052582', 'https://openalex.org/W3160818482', 'https://openalex.org/W2949446780', 'https://openalex.org/W3034739704', 'https://openalex.org/W2962847367', 'https://openalex.org/W3102425047', 'https://openalex.org/W3165977309', 'https://openalex.org/W2963341956', 'https://openalex.org/W3038047279', 'https://openalex.org/W2972473628', 'https://openalex.org/W4287116904', 'https://openalex.org/W3186655327', 'https://openalex.org/W3035301094', 'https://openalex.org/W2964266061', 'https://openalex.org/W4287373797', 'https://openalex.org/W3168656614', 'https://openalex.org/W1821462560', 'https://openalex.org/W2126725946', 'https://openalex.org/W2952402849', 'https://openalex.org/W3035097673', 'https://openalex.org/W2998653236', 'https://openalex.org/W2914204778', 'https://openalex.org/W3106031848', 'https://openalex.org/W3104405162', 'https://openalex.org/W3168921237', 'https://openalex.org/W3096032230', 'https://openalex.org/W2807023564', 'https://openalex.org/W4287890953', 'https://openalex.org/W2962738716', 'https://openalex.org/W3035633461', 'https://openalex.org/W2986896820', 'https://openalex.org/W2573176814', 'https://openalex.org/W2963527209', 'https://openalex.org/W2988647680', 'https://openalex.org/W4287815000', 'https://openalex.org/W2418300416', 'https://openalex.org/W2952638691', 'https://openalex.org/W4385572438', 'https://openalex.org/W3015468748', 'https://openalex.org/W2963825865', 'https://openalex.org/W3036362489', 'https://openalex.org/W3034773667', 'https://openalex.org/W3099219382', 'https://openalex.org/W2960744756', 'https://openalex.org/W2148365102', 'https://openalex.org/W2785885124', 'https://openalex.org/W3115908473', 'https://openalex.org/W2964213933', 'https://openalex.org/W4287599851', 'https://openalex.org/W2917128112', 'https://openalex.org/W3008618223', 'https://openalex.org/W4289548744', 'https://openalex.org/W3098101716', 'https://openalex.org/W3034708897', 'https://openalex.org/W2963527228', 'https://openalex.org/W3105604018', 'https://openalex.org/W3104423855', 'https://openalex.org/W3114610051', 'https://openalex.org/W2970854433', 'https://openalex.org/W3103065189', 'https://openalex.org/W3088059392', 'https://openalex.org/W2251235149', 'https://openalex.org/W2971296908', 'https://openalex.org/W2043147031', 'https://openalex.org/W2952190837', 'https://openalex.org/W3012291345', 'https://openalex.org/W4288288848', 'https://openalex.org/W4254836149', 'https://openalex.org/W2952267213', 'https://openalex.org/W3046251064', 'https://openalex.org/W4287704453', 'https://openalex.org/W3045462440', 'https://openalex.org/W3198429080', 'https://openalex.org/W3172697791', 'https://openalex.org/W2963736442', 'https://openalex.org/W4287887270', 'https://openalex.org/W4230579319', 'https://openalex.org/W2963748441', 'https://openalex.org/W3082017874', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963846996', 'https://openalex.org/W3034284720', 'https://openalex.org/W2971071849', 'https://openalex.org/W2464790259', 'https://openalex.org/W2964278185', 'https://openalex.org/W2980656092', 'https://openalex.org/W2964236999', 'https://openalex.org/W2963901637', 'https://openalex.org/W4297683418', 'https://openalex.org/W2964270525', 'https://openalex.org/W2963567240', 'https://openalex.org/W2964248669', 'https://openalex.org/W2565814481', 'https://openalex.org/W2997214274', 'https://openalex.org/W3035052826', 'https://openalex.org/W3174366011', 'https://openalex.org/W2962682659', 'https://openalex.org/W3000779003', 'https://openalex.org/W3045492832', 'https://openalex.org/W3104078590', 'https://openalex.org/W2964028591', 'https://openalex.org/W3121541553', 'https://openalex.org/W2964006684', 'https://openalex.org/W3102021588', 'https://openalex.org/W3098826124', 'https://openalex.org/W2963165489', 'https://openalex.org/W3105421296', 'https://openalex.org/W2951883832', 'https://openalex.org/W2921312604', 'https://openalex.org/W2970037872', 'https://openalex.org/W2963545917', 'https://openalex.org/W4287827771', 'https://openalex.org/W4288624561', 'https://openalex.org/W2963491014', 'https://openalex.org/W4206535058', 'https://openalex.org/W3098199128', 'https://openalex.org/W3035032094', 'https://openalex.org/W3198310612', 'https://openalex.org/W4385245566', 'https://openalex.org/W2080550848', 'https://openalex.org/W3030754432', 'https://openalex.org/W2963272610', 'https://openalex.org/W1577202350', 'https://openalex.org/W2884814595', 'https://openalex.org/W3101498587']",2022-07-13
https://openalex.org/W3161695192,https://doi.org/10.1109/icassp39728.2021.9415079,Any-to-One Sequence-to-Sequence Voice Conversion Using Self-Supervised Discrete Speech Representations,"We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.","['https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W2982223350', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6754299077', 'https://openalex.org/W6729448088', 'https://openalex.org/W6603838645', 'https://openalex.org/W6629717138', 'https://openalex.org/W3034420534', 'https://openalex.org/W3025844872', 'https://openalex.org/W2899877258', 'https://openalex.org/W2996414377', 'https://openalex.org/W6779919476', 'https://openalex.org/W2897353073', 'https://openalex.org/W3096567388', 'https://openalex.org/W2972970915', 'https://openalex.org/W6679436768', 'https://openalex.org/W3100270690', 'https://openalex.org/W2156142001', 'https://openalex.org/W3016160783', 'https://openalex.org/W2933138175', 'https://openalex.org/W2903739847', 'https://openalex.org/W2962780374', 'https://openalex.org/W3015338123', 'https://openalex.org/W3118753411', 'https://openalex.org/W2892009249', 'https://openalex.org/W2130942839', 'https://openalex.org/W4297808394', 'https://openalex.org/W3099078140', 'https://openalex.org/W3125709657', 'https://openalex.org/W2996383576', 'https://openalex.org/W2842511635', 'https://openalex.org/W3037057938', 'https://openalex.org/W3101689408', 'https://openalex.org/W2979476256', 'https://openalex.org/W95152782', 'https://openalex.org/W2547875792', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198429080']",2021-05-13
https://openalex.org/W3200129129,https://doi.org/10.1109/icassp43922.2022.9747432,Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition,"This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition (ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance and its efficiency. Putting together all our observations, we introduce SEW-D (Squeezed and Efficient Wav2vec with Disentangled Attention), a pre-trained model architecture with significant improvements along both performance and efficiency dimensions across a variety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW-D achieves a 1.9x inference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference time, SEW reduces word error rate by 25–50% across different model sizes.","['https://openalex.org/W6755862295', 'https://openalex.org/W6727336983', 'https://openalex.org/W6779326418', 'https://openalex.org/W6780218876', 'https://openalex.org/W6779919476', 'https://openalex.org/W3198771897', 'https://openalex.org/W6795346631', 'https://openalex.org/W6629717138', 'https://openalex.org/W2799473636', 'https://openalex.org/W3119308075', 'https://openalex.org/W2842511635', 'https://openalex.org/W2127141656', 'https://openalex.org/W3197642003', 'https://openalex.org/W2933138175', 'https://openalex.org/W6784637704', 'https://openalex.org/W6766673545', 'https://openalex.org/W1991133427', 'https://openalex.org/W6755207826', 'https://openalex.org/W6770717842', 'https://openalex.org/W6779068807', 'https://openalex.org/W3198299542', 'https://openalex.org/W6774314701', 'https://openalex.org/W6784614252', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W6697501834', 'https://openalex.org/W6729448088', 'https://openalex.org/W2963925437', 'https://openalex.org/W6638667902', 'https://openalex.org/W10548402', 'https://openalex.org/W2964121744', 'https://openalex.org/W2896457183', 'https://openalex.org/W3135828102', 'https://openalex.org/W3027083471', 'https://openalex.org/W1836465849', 'https://openalex.org/W2988736778', 'https://openalex.org/W3021469861', 'https://openalex.org/W2987283559', 'https://openalex.org/W2626778328', 'https://openalex.org/W3005680577', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W3126565544', 'https://openalex.org/W2804648901', 'https://openalex.org/W3122890974', 'https://openalex.org/W2520160253', 'https://openalex.org/W2981363336', 'https://openalex.org/W3026408381', 'https://openalex.org/W2795935804', 'https://openalex.org/W2975381464', 'https://openalex.org/W1828163288', 'https://openalex.org/W2622263826', 'https://openalex.org/W3153287399', 'https://openalex.org/W3093533780', 'https://openalex.org/W4297808394', 'https://openalex.org/W1494198834', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2898700502', 'https://openalex.org/W3035524453', 'https://openalex.org/W3004728855', 'https://openalex.org/W3025165719', 'https://openalex.org/W1524333225', 'https://openalex.org/W3162391496', 'https://openalex.org/W2973049979', 'https://openalex.org/W3033187248', 'https://openalex.org/W2155273149', 'https://openalex.org/W2331143823', 'https://openalex.org/W2952509486', 'https://openalex.org/W2345968833', 'https://openalex.org/W4322614701', 'https://openalex.org/W2963970792', 'https://openalex.org/W3144173820', 'https://openalex.org/W2143612262', 'https://openalex.org/W2965373594', 'https://openalex.org/W2547875792', 'https://openalex.org/W3157916917', 'https://openalex.org/W3093579165', 'https://openalex.org/W3046727238', 'https://openalex.org/W3144073776', 'https://openalex.org/W179875071', 'https://openalex.org/W3127866760', 'https://openalex.org/W3160799772', 'https://openalex.org/W3198429080', 'https://openalex.org/W3160525311', 'https://openalex.org/W2327501763', 'https://openalex.org/W3164279099', 'https://openalex.org/W2982413405', 'https://openalex.org/W2892009249', 'https://openalex.org/W3001279689', 'https://openalex.org/W2949117887', 'https://openalex.org/W3035060554', 'https://openalex.org/W3037057938', 'https://openalex.org/W2160815625', 'https://openalex.org/W2979476256', 'https://openalex.org/W2193413348', 'https://openalex.org/W3101648800', 'https://openalex.org/W2946948417', 'https://openalex.org/W2970971581', 'https://openalex.org/W2296701362', 'https://openalex.org/W2995181338', 'https://openalex.org/W3134206242']",2022-04-27
https://openalex.org/W3206440620,https://doi.org/10.1016/j.wocn.2022.101137,Neural representations for modeling variation in speech,"Variation in speech is often quantified by comparing phonetic transcriptions of the same utterance. However, manually transcribing speech is time-consuming and error prone. As an alternative, therefore, we investigate the extraction of acoustic embeddings from several self-supervised neural models. We use these representations to compute word-based pronunciation differences between non-native and native speakers of English, and between Norwegian dialect speakers. For comparison with several earlier studies, we evaluate how well these differences match human perception by comparing them with available human judgements of similarity. We show that speech representations extracted from a specific type of neural model (i.e. Transformers) lead to a better match with human perception than two earlier approaches on the basis of phonetic transcriptions and MFCC-based acoustic features. We furthermore find that features from the neural models can generally best be extracted from one of the middle hidden layers than from the final layer. We also demonstrate that neural speech representations not only capture segmental differences, but also intonational and durational differences that cannot adequately be represented by a set of discrete symbols used in phonetic transcriptions.","['https://openalex.org/W6771467084', 'https://openalex.org/W2080236136', 'https://openalex.org/W3213029956', 'https://openalex.org/W6780218876', 'https://openalex.org/W3030929541', 'https://openalex.org/W6725192793', 'https://openalex.org/W2146204822', 'https://openalex.org/W2484174190', 'https://openalex.org/W6635456232', 'https://openalex.org/W3198429080', 'https://openalex.org/W2159306398', 'https://openalex.org/W2132133133', 'https://openalex.org/W6755207826', 'https://openalex.org/W1971918007', 'https://openalex.org/W2103523269', 'https://openalex.org/W2054246464', 'https://openalex.org/W6696449567', 'https://openalex.org/W2107092366', 'https://openalex.org/W2161217167', 'https://openalex.org/W1986094003', 'https://openalex.org/W7027429494', 'https://openalex.org/W2161482971', 'https://openalex.org/W2003573652', 'https://openalex.org/W2100379591', 'https://openalex.org/W6713954808', 'https://openalex.org/W6712213709', 'https://openalex.org/W2773621007', 'https://openalex.org/W2995181338', 'https://openalex.org/W3012624518', 'https://openalex.org/W2058100611', 'https://openalex.org/W2099565417', 'https://openalex.org/W6636915900', 'https://openalex.org/W6848220499', 'https://openalex.org/W6775396121', 'https://openalex.org/W1783786615', 'https://openalex.org/W6784343036', 'https://openalex.org/W2032117164', 'https://openalex.org/W6948063453', 'https://openalex.org/W6768798221', 'https://openalex.org/W6682691769', 'https://openalex.org/W2150684916', 'https://openalex.org/W2046382036', 'https://openalex.org/W2168943479', 'https://openalex.org/W6697306262', 'https://openalex.org/W6729981485', 'https://openalex.org/W6677378386', 'https://openalex.org/W7070833347', 'https://openalex.org/W6782458300', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W6629717138', 'https://openalex.org/W2024490156', 'https://openalex.org/W6748634344', 'https://openalex.org/W6784642771', 'https://openalex.org/W3200887081', 'https://openalex.org/W2124882279', 'https://openalex.org/W6761176036', 'https://openalex.org/W2097137621', 'https://openalex.org/W6780393137', 'https://openalex.org/W6762537594', 'https://openalex.org/W6739901393', 'https://openalex.org/W6767231498', 'https://openalex.org/W3100308117', 'https://openalex.org/W6624578213', 'https://openalex.org/W6639225818', 'https://openalex.org/W2151466612', 'https://openalex.org/W6726114760', 'https://openalex.org/W2047069719', 'https://openalex.org/W2109052424', 'https://openalex.org/W2158884543', 'https://openalex.org/W2029996593', 'https://openalex.org/W6652165249', 'https://openalex.org/W3025286576', 'https://openalex.org/W3037057938', 'https://openalex.org/W34858320', 'https://openalex.org/W2547875792', 'https://openalex.org/W1989528006', 'https://openalex.org/W2153579005', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015265920', 'https://openalex.org/W3149836819', 'https://openalex.org/W3036601975', 'https://openalex.org/W2962739339', 'https://openalex.org/W4238634189', 'https://openalex.org/W612402774', 'https://openalex.org/W2912512851', 'https://openalex.org/W2694849690', 'https://openalex.org/W2963403868', 'https://openalex.org/W745779314', 'https://openalex.org/W2292087804', 'https://openalex.org/W3152218910', 'https://openalex.org/W2115915304', 'https://openalex.org/W1954463725', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2494980014', 'https://openalex.org/W2973049979', 'https://openalex.org/W3081796673', 'https://openalex.org/W2515735122', 'https://openalex.org/W2406395231', 'https://openalex.org/W942963634', 'https://openalex.org/W1647671624', 'https://openalex.org/W4285719527', 'https://openalex.org/W2963799213', 'https://openalex.org/W2127141656', 'https://openalex.org/W3200501660', 'https://openalex.org/W2294933947', 'https://openalex.org/W3095410713', 'https://openalex.org/W2137807925', 'https://openalex.org/W1518308693', 'https://openalex.org/W3040987109', 'https://openalex.org/W4385245566', 'https://openalex.org/W2042515517', 'https://openalex.org/W3095732712', 'https://openalex.org/W2308426595', 'https://openalex.org/W4285074441', 'https://openalex.org/W2980286501', 'https://openalex.org/W1502957213', 'https://openalex.org/W2548759296', 'https://openalex.org/W2964235839', 'https://openalex.org/W2395655916', 'https://openalex.org/W2946417913', 'https://openalex.org/W2608702473', 'https://openalex.org/W2613000335', 'https://openalex.org/W4294170691', 'https://openalex.org/W602842521', 'https://openalex.org/W2973094925', 'https://openalex.org/W2606429533', 'https://openalex.org/W3163596720', 'https://openalex.org/W1593045043']",2022-03-05
https://openalex.org/W3210574093,https://doi.org/10.1109/icassp43922.2022.9747908,Temporal Knowledge Distillation for on-device Audio Classification,"Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.","['https://openalex.org/W6739901393', 'https://openalex.org/W6730179637', 'https://openalex.org/W3095385545', 'https://openalex.org/W6779919476', 'https://openalex.org/W2593116425', 'https://openalex.org/W6755207826', 'https://openalex.org/W6761660874', 'https://openalex.org/W2953219395', 'https://openalex.org/W2888641632', 'https://openalex.org/W3096297644', 'https://openalex.org/W6783462664', 'https://openalex.org/W6638523607', 'https://openalex.org/W2963864497', 'https://openalex.org/W2963681135', 'https://openalex.org/W3198035615', 'https://openalex.org/W3025581723', 'https://openalex.org/W6750665317', 'https://openalex.org/W3198715852', 'https://openalex.org/W2470673105', 'https://openalex.org/W2936774411', 'https://openalex.org/W3035422918', 'https://openalex.org/W4205689591', 'https://openalex.org/W2973226577', 'https://openalex.org/W2963341956', 'https://openalex.org/W2561238782', 'https://openalex.org/W1821462560', 'https://openalex.org/W3198429080', 'https://openalex.org/W2896457183', 'https://openalex.org/W3090388844', 'https://openalex.org/W4385245566', 'https://openalex.org/W3037057938', 'https://openalex.org/W3170968215', 'https://openalex.org/W2930364467', 'https://openalex.org/W2963403868', 'https://openalex.org/W4285719527', 'https://openalex.org/W2797583228', 'https://openalex.org/W2602634800']",2022-04-27
https://openalex.org/W3144173820,https://doi.org/10.21437/interspeech.2021-236,Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training,"Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. On a large-scale competitive setup, we show that pre-training on unlabeled in-domain data reduces the gap between models trained on in-domain and out-of-domain labeled data by 66%-73%. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. Code and models will be made available at https://github.com/pytorch/fairseq.","['https://openalex.org/W2547875792', 'https://openalex.org/W1494198834', 'https://openalex.org/W2758785877', 'https://openalex.org/W2062164080', 'https://openalex.org/W2995929068', 'https://openalex.org/W2617258110', 'https://openalex.org/W2996383576', 'https://openalex.org/W3093502935', 'https://openalex.org/W1992272902', 'https://openalex.org/W2962946733', 'https://openalex.org/W3037057938', 'https://openalex.org/W2964245029', 'https://openalex.org/W1571339265', 'https://openalex.org/W3099782249', 'https://openalex.org/W3015726069', 'https://openalex.org/W2973157397', 'https://openalex.org/W3025165719', 'https://openalex.org/W2134800885', 'https://openalex.org/W3102342027', 'https://openalex.org/W2996159613', 'https://openalex.org/W2972943112', 'https://openalex.org/W2896457183', 'https://openalex.org/W2995181338', 'https://openalex.org/W2842511635', 'https://openalex.org/W2953190524', 'https://openalex.org/W2124509324', 'https://openalex.org/W2973049979', 'https://openalex.org/W3005511757', 'https://openalex.org/W2963403868', 'https://openalex.org/W3119308075', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015349902', 'https://openalex.org/W2988736778', 'https://openalex.org/W3101648800', 'https://openalex.org/W3163464943']",2021-08-27
https://openalex.org/W3200601846,https://doi.org/10.18653/v1/2021.findings-emnlp.236,Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition,"Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the transfer from speech to text. However, how to solve the representation discrepancy of speech and text is unexplored, which hinders the utilization of acoustic and linguistic information. Moreover, previous works simply replace the embedding layer of the pre-trained language model with the acoustic features, which may cause the catastrophic forgetting problem. In this work, we introduce Wav-BERT, a cooperative acoustic and linguistic representation learning method to fuse and utilize the contextual information of speech and text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a language model (BERT) into an end-to-end trainable framework. A Representation Aggregation Module is designed to aggregate acoustic and linguistic representation, and an Embedding Attention Module is introduced to incorporate acoustic information into BERT, which can effectively facilitate the cooperation of two pre-trained models and thus boost the representation learning. Extensive experiments show that our Wav-BERT significantly outperforms the existing approaches and achieves state-of-the-art performance on low-resource speech recognition.","['https://openalex.org/W2933138175', 'https://openalex.org/W2939710050', 'https://openalex.org/W2990391581', 'https://openalex.org/W3094002217', 'https://openalex.org/W3016011332', 'https://openalex.org/W3025165719', 'https://openalex.org/W3097580812', 'https://openalex.org/W2134800885', 'https://openalex.org/W2604763608', 'https://openalex.org/W2292087804', 'https://openalex.org/W2787560479', 'https://openalex.org/W2976556660', 'https://openalex.org/W2973049979', 'https://openalex.org/W3036601975', 'https://openalex.org/W2127141656', 'https://openalex.org/W3097777922', 'https://openalex.org/W2940322076', 'https://openalex.org/W3100311862', 'https://openalex.org/W2939111082', 'https://openalex.org/W3093345276', 'https://openalex.org/W2964309797', 'https://openalex.org/W3155427814', 'https://openalex.org/W3129868806', 'https://openalex.org/W3037057938', 'https://openalex.org/W2842511635', 'https://openalex.org/W2795900505', 'https://openalex.org/W3033017368', 'https://openalex.org/W3099782249', 'https://openalex.org/W2972818416', 'https://openalex.org/W2963303951', 'https://openalex.org/W2964067969', 'https://openalex.org/W2937808806', 'https://openalex.org/W3015522062', 'https://openalex.org/W2327501763', 'https://openalex.org/W3095358777', 'https://openalex.org/W4294646197', 'https://openalex.org/W3111562797', 'https://openalex.org/W2894835365', 'https://openalex.org/W2766219058', 'https://openalex.org/W4297808394', 'https://openalex.org/W3197845195', 'https://openalex.org/W2963242190', 'https://openalex.org/W3014413043', 'https://openalex.org/W3163132306', 'https://openalex.org/W2913129712', 'https://openalex.org/W2962739339', 'https://openalex.org/W2991509857', 'https://openalex.org/W2988975212', 'https://openalex.org/W2113839990', 'https://openalex.org/W3015585292', 'https://openalex.org/W2193413348', 'https://openalex.org/W3048208370', 'https://openalex.org/W2896457183', 'https://openalex.org/W3156902660', 'https://openalex.org/W3198429080', 'https://openalex.org/W3173563729', 'https://openalex.org/W3141961557', 'https://openalex.org/W2963341956', 'https://openalex.org/W3160622492', 'https://openalex.org/W2786835190', 'https://openalex.org/W2963292011', 'https://openalex.org/W3110524561']",2021-01-01
https://openalex.org/W3199531394,https://doi.org/10.1109/ijcnn52387.2021.9533587,Transfer Ability of Monolingual Wav2vec2.0 for Low-resource Speech Recognition,"Recently, there are several domains that have their own feature extractors, such as ResNet, BERT, and GPT-x, which are widely used for various down-stream tasks. These models are pre-trained on large amounts of unlabeled data by self-supervision. In the speech domain, wav2vec2.0 starts to show its powerful representation ability and feasibility for ultra-low resource speech recognition tasks. This speech feature extractor is pre-trained on the monolingual audiobook corpus, whereas it has not been thoroughly examined in real spoken scenarios and other languages. In this work, we endeavor to transfer the knowledge from the pre-trained monolingual wav2vec2.0 to cross-lingual spoken ASR tasks with less than 20 hours of labeled data. We achieve more than 20% relative improvements in all the six languages compared with previous methods, establishing a strong benchmark on CALLHOME datasets. Compared with supervised pre-training, self-supervision training used in wav2vec2.0 has a better transfer ability. We also find that using coarse-grained modeling units, such as subword or character, usually achieves better results than fine-grained modeling units, such as phone or letter.","['https://openalex.org/W6755207826', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769238691', 'https://openalex.org/W3024182269', 'https://openalex.org/W3015265920', 'https://openalex.org/W6769593479', 'https://openalex.org/W2936078256', 'https://openalex.org/W3016167541', 'https://openalex.org/W6780226713', 'https://openalex.org/W6755977528', 'https://openalex.org/W2951974815', 'https://openalex.org/W6752194381', 'https://openalex.org/W2972943112', 'https://openalex.org/W3100270690', 'https://openalex.org/W6780218876', 'https://openalex.org/W6638749077', 'https://openalex.org/W6759455113', 'https://openalex.org/W2953190524', 'https://openalex.org/W6772253010', 'https://openalex.org/W2973034126', 'https://openalex.org/W6757304564', 'https://openalex.org/W6779919476', 'https://openalex.org/W1846073453', 'https://openalex.org/W2141820854', 'https://openalex.org/W6743477263', 'https://openalex.org/W2971840980', 'https://openalex.org/W2891616026', 'https://openalex.org/W2894835365', 'https://openalex.org/W6676215431', 'https://openalex.org/W6752630080', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2962824709', 'https://openalex.org/W6728841359', 'https://openalex.org/W3095311338', 'https://openalex.org/W2025198378', 'https://openalex.org/W2921194440', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973180718', 'https://openalex.org/W2962784628', 'https://openalex.org/W6679434410', 'https://openalex.org/W2746192915', 'https://openalex.org/W1526236009', 'https://openalex.org/W2750545698', 'https://openalex.org/W6771812881', 'https://openalex.org/W6629717138', 'https://openalex.org/W3125709657', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099782249', 'https://openalex.org/W1828163288', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995549860', 'https://openalex.org/W2981991061', 'https://openalex.org/W4300191749', 'https://openalex.org/W2133564696', 'https://openalex.org/W3198429080', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963403868', 'https://openalex.org/W2842511635', 'https://openalex.org/W2982223350', 'https://openalex.org/W2107298017', 'https://openalex.org/W2805394970', 'https://openalex.org/W2533523411', 'https://openalex.org/W2963897095', 'https://openalex.org/W2888779557', 'https://openalex.org/W2899663614', 'https://openalex.org/W1494198834', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963341956', 'https://openalex.org/W273093436', 'https://openalex.org/W2907380995', 'https://openalex.org/W2896457183', 'https://openalex.org/W2808640845', 'https://openalex.org/W3037057938', 'https://openalex.org/W4385245566', 'https://openalex.org/W4297808394', 'https://openalex.org/W2914120296', 'https://openalex.org/W2995181338']",2021-07-18
https://openalex.org/W3209976096,https://doi.org/10.1109/icassp43922.2022.9746832,Pseudo-Labeling for Massively Multilingual Speech Recognition,"Semi-supervised learning through pseudo-labeling has become a staple of state-of-the-art monolingual speech recognition systems. In this work, we extend pseudo-labeling to massively multilingual speech recognition with 60 languages. We propose a simple pseudo-labeling recipe that works well even with low-resource languages: train a supervised multilingual model, fine-tune it with semi-supervised learning on a target language, generate pseudo-labels for that language, and train a final model using pseudo-labels for all languages, either from scratch or by fine-tuning. Experiments on the labeled Common Voice and unlabeled VoxPopuli datasets show that our recipe can yield a model with better performance for many languages that also transfers well to LibriSpeech.","['https://openalex.org/W6797264745', 'https://openalex.org/W2964309797', 'https://openalex.org/W2936774411', 'https://openalex.org/W3198270883', 'https://openalex.org/W3149629662', 'https://openalex.org/W3015522062', 'https://openalex.org/W3026041220', 'https://openalex.org/W6771467084', 'https://openalex.org/W3119308075', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127141656', 'https://openalex.org/W6809739816', 'https://openalex.org/W2769639994', 'https://openalex.org/W2891816510', 'https://openalex.org/W6779919476', 'https://openalex.org/W2962760690', 'https://openalex.org/W6770506093', 'https://openalex.org/W6638749077', 'https://openalex.org/W3160525311', 'https://openalex.org/W6780218876', 'https://openalex.org/W6784614252', 'https://openalex.org/W3198098585', 'https://openalex.org/W3096338464', 'https://openalex.org/W3096032230', 'https://openalex.org/W3197223534', 'https://openalex.org/W6601939441', 'https://openalex.org/W2963431393', 'https://openalex.org/W2894835365', 'https://openalex.org/W2971840980', 'https://openalex.org/W3096215352', 'https://openalex.org/W6756000814', 'https://openalex.org/W2327501763', 'https://openalex.org/W6623517193', 'https://openalex.org/W4221149170', 'https://openalex.org/W3214457259', 'https://openalex.org/W3099782249', 'https://openalex.org/W2900819823', 'https://openalex.org/W3173814445', 'https://openalex.org/W3204397973', 'https://openalex.org/W4287124724', 'https://openalex.org/W3198429080', 'https://openalex.org/W2963925437', 'https://openalex.org/W3030437843', 'https://openalex.org/W47568227', 'https://openalex.org/W2951816288', 'https://openalex.org/W3093579165', 'https://openalex.org/W3037057938', 'https://openalex.org/W1828163288', 'https://openalex.org/W854541894', 'https://openalex.org/W2750499125', 'https://openalex.org/W3036601975', 'https://openalex.org/W2964002616', 'https://openalex.org/W2991213871']",2022-04-27
https://openalex.org/W3110524561,https://doi.org/10.21437/interspeech.2021-1390,Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition,"One crucial challenge of real-world multilingual speech recognition is the long-tailed distribution problem, where some resource-rich languages like English have abundant training data, but a long tail of low-resource languages have varying amounts of limited training data. To overcome the long-tail problem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based multi-task learning framework for end-to-end multilingual speech recognition. The A2 framework overcomes the long-tail problem via three techniques: (1) exploiting a pretrained multilingual language model (mBERT) to improve the performance of low-resource languages; (2) proposing dual adapters consisting of both language-specific and language-agnostic adaptation with minimal additional parameters; and (3) overcoming the class imbalance, either by imposing class priors in the loss during training or adjusting the logits of the softmax output during inference. Extensive experiments on the CommonVoice corpus show that A2 significantly outperforms conventional approaches.","['https://openalex.org/W2461918431', 'https://openalex.org/W2397987315', 'https://openalex.org/W3042667808', 'https://openalex.org/W3096032230', 'https://openalex.org/W2971840980', 'https://openalex.org/W2148143831', 'https://openalex.org/W2962893195', 'https://openalex.org/W2963691377', 'https://openalex.org/W2892009249', 'https://openalex.org/W2970206392', 'https://openalex.org/W2995197345', 'https://openalex.org/W2127141656', 'https://openalex.org/W2964303773', 'https://openalex.org/W2963403868', 'https://openalex.org/W3037057938', 'https://openalex.org/W2795935804', 'https://openalex.org/W2104539041', 'https://openalex.org/W2972389417', 'https://openalex.org/W2964309797', 'https://openalex.org/W2952470929', 'https://openalex.org/W2978017171', 'https://openalex.org/W3023911605', 'https://openalex.org/W3030437843', 'https://openalex.org/W1986174057', 'https://openalex.org/W3015697211', 'https://openalex.org/W2143612262', 'https://openalex.org/W2963211188', 'https://openalex.org/W2131427446', 'https://openalex.org/W1855892484', 'https://openalex.org/W3042496707', 'https://openalex.org/W3035552357', 'https://openalex.org/W2950304420', 'https://openalex.org/W2119191234', 'https://openalex.org/W1965555277', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963979492', 'https://openalex.org/W3035390927', 'https://openalex.org/W2970941190', 'https://openalex.org/W2148226542', 'https://openalex.org/W85350352']",2021-08-27
https://openalex.org/W4391436317,https://doi.org/10.1002/alz.13668,"Validity, feasibility, and effectiveness of a voice‐recognition based digital cognitive screener for dementia and mild cognitive impairment in community‐dwelling older Chinese adults: A large‐scale implementation study","Abstract INTRODUCTION We investigated the validity, feasibility, and effectiveness of a voice recognition‐based digital cognitive screener (DCS), for detecting dementia and mild cognitive impairment (MCI) in a large‐scale community of elderly participants. METHODS Eligible participants completed demographic, cognitive, functional assessments and the DCS. Neuropsychological tests were used to assess domain‐specific and global cognition, while the diagnosis of MCI and dementia relied on the Clinical Dementia Rating Scale. RESULTS Among the 11,186 participants, the DCS showed high completion rates (97.5%) and a short administration time (5.9 min) across gender, age, and education groups. The DCS demonstrated areas under the receiver operating characteristics curve (AUCs) of 0.95 and 0.83 for dementia and MCI detection, respectively, among 328 participants in the validation phase. Furthermore, the DCS resulted in time savings of 16.2% to 36.0% compared to the Mini‐Mental State Examination (MMSE) and Montral Cognitive Assessment (MoCA). DISCUSSION This study suggests that the DCS is an effective and efficient tool for dementia and MCI case‐finding in large‐scale cognitive screening. Highlights To our best knowledge, this is the first cognitive screening tool based on voice recognition and utilizing conversational AI that has been assessed in a large population of Chinese community‐dwelling elderly. With the upgrading of a new multimodal understanding model, the DCS can accurately assess participants' responses, including different Chinese dialects, and provide automatic scores. The DCS not only exhibited good discriminant ability in detecting dementia and MCI cases, it also demonstrated a high completion rate and efficient administration regardless of gender, age, and education differences. The DCS is economically efficient, scalable, and had a better screening efficacy compared to the MMSE or MoCA, for wider implementation.","['https://openalex.org/W2754967293', 'https://openalex.org/W2971598158', 'https://openalex.org/W4313812373', 'https://openalex.org/W4210462706', 'https://openalex.org/W3008391208', 'https://openalex.org/W3106725626', 'https://openalex.org/W4308426417', 'https://openalex.org/W3046275966', 'https://openalex.org/W4212957127', 'https://openalex.org/W3014561849', 'https://openalex.org/W2907272110', 'https://openalex.org/W3209426885', 'https://openalex.org/W3184269374', 'https://openalex.org/W4286697593', 'https://openalex.org/W4284667718', 'https://openalex.org/W2150246838', 'https://openalex.org/W2041981133', 'https://openalex.org/W4308766716', 'https://openalex.org/W4287890956', 'https://openalex.org/W4282048378', 'https://openalex.org/W3037057938', 'https://openalex.org/W2995590269', 'https://openalex.org/W2113082716', 'https://openalex.org/W2102150307', 'https://openalex.org/W3041185083', 'https://openalex.org/W3193821230', 'https://openalex.org/W2271953872', 'https://openalex.org/W2165758561', 'https://openalex.org/W1847168837', 'https://openalex.org/W2136250824', 'https://openalex.org/W2903296995', 'https://openalex.org/W2896841832', 'https://openalex.org/W4220730951', 'https://openalex.org/W2772976454', 'https://openalex.org/W2971452237', 'https://openalex.org/W2161769056', 'https://openalex.org/W3127826753', 'https://openalex.org/W3204082461', 'https://openalex.org/W2699929765', 'https://openalex.org/W2913295464', 'https://openalex.org/W2955844975', 'https://openalex.org/W2008854521', 'https://openalex.org/W4385574358', 'https://openalex.org/W4391436317', 'https://openalex.org/W3198429080', 'https://openalex.org/W2167311298', 'https://openalex.org/W2990959992', 'https://openalex.org/W2148080316', 'https://openalex.org/W2058161128']",2024-02-01
https://openalex.org/W3179803166,https://doi.org/10.1109/asru51503.2021.9688093,Layer-Wise Analysis of a Self-Supervised Speech Representation Model,"Recently proposed self-supervised learning approaches have been successful for pre-training speech representation models. The utility of these learned representations has been observed empirically, but not much has been studied about the type or extent of information encoded in the pre-trained representations themselves. Developing such insights can help understand the capabilities and limits of these models and enable the research community to more efficiently develop their usage for downstream applications. In this work, we begin to fill this gap by examining one recent and successful pre-trained model (wav2vec 2.0), via its intermediate representation vectors, using a suite of analysis tools. We use the metrics of canonical correlation, mutual information, and performance on simple downstream tasks with non-parametric probes, in order to (i) query for acoustic and linguistic information content, (ii) characterize the evolution of information across model layers, and (iii) understand how fine-tuning the model for automatic speech recognition (ASR) affects these observations. Our findings motivate modifying the fine-tuning protocol for ASR, which produces improved word error rates in a low-resource setting.","['https://openalex.org/W2251253014', 'https://openalex.org/W3146777637', 'https://openalex.org/W2250539671', 'https://openalex.org/W2932675979', 'https://openalex.org/W6752726010', 'https://openalex.org/W6761472960', 'https://openalex.org/W3044967013', 'https://openalex.org/W3095706145', 'https://openalex.org/W1545920196', 'https://openalex.org/W2407151108', 'https://openalex.org/W4237723258', 'https://openalex.org/W2970820321', 'https://openalex.org/W6745682157', 'https://openalex.org/W6755207826', 'https://openalex.org/W343636949', 'https://openalex.org/W3034273309', 'https://openalex.org/W6788328058', 'https://openalex.org/W3163596720', 'https://openalex.org/W6786696081', 'https://openalex.org/W3162133897', 'https://openalex.org/W2946417913', 'https://openalex.org/W2586148577', 'https://openalex.org/W3160799772', 'https://openalex.org/W1494198834', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015265920', 'https://openalex.org/W3197580070', 'https://openalex.org/W3198771897', 'https://openalex.org/W6779919476', 'https://openalex.org/W6795952400', 'https://openalex.org/W3198299542', 'https://openalex.org/W2906152891', 'https://openalex.org/W2963259843', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W3003875258', 'https://openalex.org/W3167533889', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W2153579005', 'https://openalex.org/W2963425185', 'https://openalex.org/W2988217457', 'https://openalex.org/W3034709122', 'https://openalex.org/W2127141656', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972584841', 'https://openalex.org/W6743149223', 'https://openalex.org/W3165666670', 'https://openalex.org/W2767204723', 'https://openalex.org/W3099782249', 'https://openalex.org/W2942810103', 'https://openalex.org/W3005680577', 'https://openalex.org/W2025341678', 'https://openalex.org/W2747874407', 'https://openalex.org/W3121914243', 'https://openalex.org/W3110458199', 'https://openalex.org/W3126074026', 'https://openalex.org/W3144173820', 'https://openalex.org/W3096196861', 'https://openalex.org/W3153287399', 'https://openalex.org/W2963759780', 'https://openalex.org/W2963341956', 'https://openalex.org/W3037057938']",2021-12-13
https://openalex.org/W3203098807,https://doi.org/10.1109/icassp43922.2022.9746276,Magic Dust for Cross-Lingual Adaptation of Monolingual Wav2vec-2.0,"We propose a simple and effective cross-lingual transfer learning method to\nadapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in\nresource-scarce languages. We show that a monolingual wav2vec-2.0 is a good\nfew-shot ASR learner in several languages. We improve its performance further\nvia several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by\nusing a moderate-sized unlabeled speech dataset in the target language. A key\nfinding of this work is that the adapted monolingual wav2vec-2.0 achieves\nsimilar performance as the topline multilingual XLSR model, which is trained on\nfifty-three languages, on the target language ASR task.\n","['https://openalex.org/W2111316763', 'https://openalex.org/W3015522062', 'https://openalex.org/W3096338464', 'https://openalex.org/W6768222176', 'https://openalex.org/W6636915900', 'https://openalex.org/W6766960374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3198771897', 'https://openalex.org/W2933138175', 'https://openalex.org/W2113896236', 'https://openalex.org/W6796418813', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963522845', 'https://openalex.org/W6779919476', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755207826', 'https://openalex.org/W3163464943', 'https://openalex.org/W6774314701', 'https://openalex.org/W6727048030', 'https://openalex.org/W6787587704', 'https://openalex.org/W6638749077', 'https://openalex.org/W3101498587', 'https://openalex.org/W3016181583', 'https://openalex.org/W6799245484', 'https://openalex.org/W2971840980', 'https://openalex.org/W3144173820', 'https://openalex.org/W2278264165', 'https://openalex.org/W3196321886', 'https://openalex.org/W3005680577', 'https://openalex.org/W2952509486', 'https://openalex.org/W1828163288', 'https://openalex.org/W3037057938', 'https://openalex.org/W3198429080', 'https://openalex.org/W3114727685', 'https://openalex.org/W3027083471', 'https://openalex.org/W2963077089', 'https://openalex.org/W2976223659', 'https://openalex.org/W4302764113', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W3023911605', 'https://openalex.org/W3005511757', 'https://openalex.org/W2972981541', 'https://openalex.org/W2522137090', 'https://openalex.org/W3186596101', 'https://openalex.org/W1647671624', 'https://openalex.org/W2963341956', 'https://openalex.org/W3165404421', 'https://openalex.org/W4297808394', 'https://openalex.org/W3099782249']",2022-04-27
https://openalex.org/W4388068584,https://doi.org/10.21437/sigul.2023-9,The Applicability of Wav2Vec2 and Whisper for Low-Resource Maltese ASR,"Maltese is a low-resource language with limited digital tools, including automatic speech recognition. With very limited datasets of Maltese speech available, a recent project, MASRI, developed further speech datasets and produced an initial prototype trained using the Jasper architecture. The best system achieved 55.05% WER on the MASRI test set. Our work builds upon this, producing a further two-and-a half-hour annotated speech corpus from a domain in which no data was previously available (Parliament of Malta). Moreover, we experiment with existing pre-trained self-supervised models (Wav2Vec2.0 and Whisper) and further fine-tune these models on Maltese annotated data. A total of 30 Maltese ASR models are trained and evaluated using the WER and the CER. The results indicate that the performance of the models scales with the quantity of data, although not linearly. The best model achieves state-of-the-art results of 8.53% WER and 1.93% CER on a test set extracted from the CommonVoice project and 24.98% WER and 8.37% CER on the MASRI test set.","['https://openalex.org/W2120209245', 'https://openalex.org/W1993660824', 'https://openalex.org/W2939710050', 'https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W4311000453', 'https://openalex.org/W3031570736', 'https://openalex.org/W6761449031', 'https://openalex.org/W3213247366', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015356564', 'https://openalex.org/W1494198834', 'https://openalex.org/W3119308075', 'https://openalex.org/W2995929068', 'https://openalex.org/W2991262856', 'https://openalex.org/W3198429080', 'https://openalex.org/W2973049979', 'https://openalex.org/W3030437843', 'https://openalex.org/W4287689669', 'https://openalex.org/W2973215447']",2023-08-18
https://openalex.org/W3210685431,https://doi.org/10.21437/interspeech.2022-112,Voice Conversion Can Improve ASR in Very Low-Resource Settings,"Voice conversion (VC) could be used to improve speech recognition systems in low-resource languages by using it to augment limited training data.However, VC has not been widely used for this purpose because of practical issues such as compute speed and limitations when converting to and from unseen speakers.Moreover, it is still unclear whether a VC model trained on one well-resourced language can be applied to speech from another low-resource language for the aim of data augmentation.In this work we assess whether a VC system can be used cross-lingually to improve low-resource speech recognition.We combine several recent techniques to design and train a practical VC system in English, and then use this system to augment data for training speech recognition models in several low-resource languages.When using a sensible amount of VC augmented data, speech recognition performance is improved in all four low-resource languages considered.We also show that VC-based augmentation is superior to SpecAugment (a widely used signal processing augmentation method) in the low-resource languages considered.","['https://openalex.org/W1494198834', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287591426', 'https://openalex.org/W3198429080', 'https://openalex.org/W3198815374', 'https://openalex.org/W3007067948', 'https://openalex.org/W3093579165', 'https://openalex.org/W3113738713', 'https://openalex.org/W2964243274', 'https://openalex.org/W2121681768', 'https://openalex.org/W2166891329', 'https://openalex.org/W3092028330', 'https://openalex.org/W2945478979', 'https://openalex.org/W3037057938', 'https://openalex.org/W3140429000', 'https://openalex.org/W2127141656', 'https://openalex.org/W2064675550', 'https://openalex.org/W2949281321', 'https://openalex.org/W2576309025', 'https://openalex.org/W2091746061', 'https://openalex.org/W4288079962', 'https://openalex.org/W2099621636', 'https://openalex.org/W2995181338', 'https://openalex.org/W3097017391', 'https://openalex.org/W2962788625', 'https://openalex.org/W2964121744', 'https://openalex.org/W4394671563', 'https://openalex.org/W3107298252', 'https://openalex.org/W2936774411', 'https://openalex.org/W2749581528', 'https://openalex.org/W3110458199', 'https://openalex.org/W3097032879', 'https://openalex.org/W3099782249', 'https://openalex.org/W3210177631', 'https://openalex.org/W2947007461', 'https://openalex.org/W3196702840', 'https://openalex.org/W1979651826', 'https://openalex.org/W3161492781', 'https://openalex.org/W3098403858', 'https://openalex.org/W1522301498', 'https://openalex.org/W1531318651', 'https://openalex.org/W3169320628', 'https://openalex.org/W2750167318', 'https://openalex.org/W2167191127', 'https://openalex.org/W3168656614', 'https://openalex.org/W2963587345', 'https://openalex.org/W2933138175']",2022-09-16
https://openalex.org/W3136717460,https://doi.org/10.3390/s21093063,Dynamic Acoustic Unit Augmentation with BPE-Dropout for Low-Resource End-to-End Speech Recognition,"With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assistants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system.","['https://openalex.org/W2996769030', 'https://openalex.org/W3015194534', 'https://openalex.org/W3096686110', 'https://openalex.org/W2889511491', 'https://openalex.org/W2160815625', 'https://openalex.org/W2127141656', 'https://openalex.org/W2327501763', 'https://openalex.org/W1604697534', 'https://openalex.org/W3088860656', 'https://openalex.org/W2737429942', 'https://openalex.org/W2903732155', 'https://openalex.org/W2714176837', 'https://openalex.org/W3097794466', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492', 'https://openalex.org/W2117621558', 'https://openalex.org/W3009065282', 'https://openalex.org/W3035207248', 'https://openalex.org/W2938348542', 'https://openalex.org/W3095229326', 'https://openalex.org/W3100684858', 'https://openalex.org/W3120549770', 'https://openalex.org/W3118510458', 'https://openalex.org/W3034789084', 'https://openalex.org/W3126537755', 'https://openalex.org/W3015537910', 'https://openalex.org/W1828163288', 'https://openalex.org/W3094667432', 'https://openalex.org/W3018441253', 'https://openalex.org/W2526425061', 'https://openalex.org/W6739901393', 'https://openalex.org/W3035531585', 'https://openalex.org/W3024464021', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W3038736627', 'https://openalex.org/W6631190155', 'https://openalex.org/W6743178762', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962780374', 'https://openalex.org/W3196321886', 'https://openalex.org/W2407080277', 'https://openalex.org/W6631362777', 'https://openalex.org/W2122931774', 'https://openalex.org/W2963812242', 'https://openalex.org/W2894835365', 'https://openalex.org/W3198429080', 'https://openalex.org/W2671812860', 'https://openalex.org/W4246402807', 'https://openalex.org/W1539673959', 'https://openalex.org/W2749581528', 'https://openalex.org/W3094800360', 'https://openalex.org/W1524333225', 'https://openalex.org/W3037057938', 'https://openalex.org/W3031412205', 'https://openalex.org/W2963587345', 'https://openalex.org/W3104502794', 'https://openalex.org/W2751936342', 'https://openalex.org/W2963403868', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964121744', 'https://openalex.org/W3114727685', 'https://openalex.org/W4235669915', 'https://openalex.org/W2496616370', 'https://openalex.org/W3012900830', 'https://openalex.org/W3013246586', 'https://openalex.org/W4385245566', 'https://openalex.org/W3107298252']",2021-04-28
https://openalex.org/W3200887081,https://doi.org/10.1109/asru51503.2021.9688301,Leveraging Pre-Trained Representations to Improve Access to Untranscribed Speech from Endangered Languages,"Pre-trained speech representations like wav2vec 2.0 are a powerful tool for automatic speech recognition (ASR). Yet many endangered languages lack sufficient data for pre-training such models, or are predominantly oral vernaculars without a standardised writing system, precluding fine-tuning. Query-by-example spoken term detection (QbE-STD) offers an alternative for iteratively indexing untranscribed speech corpora by locating spoken query terms. Using data from 7 Australian Aboriginal languages and a regional variety of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can be improved by leveraging representations developed for ASR (wav2vec 2.0: the English monolingual model and XLSR53 multilingual model). Surprisingly, the English model outperformed the multilingual model on 4 Australian language datasets, raising questions around how to optimally leverage self-supervised speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0 representations (either English or XLSR53) offer large improvements (56-86% relative) over state-of-the-art approaches on our endangered language datasets.","['https://openalex.org/W3117408830', 'https://openalex.org/W2888951652', 'https://openalex.org/W3019546258', 'https://openalex.org/W6756326128', 'https://openalex.org/W3015522062', 'https://openalex.org/W2946417913', 'https://openalex.org/W6786071721', 'https://openalex.org/W6794685259', 'https://openalex.org/W6780218876', 'https://openalex.org/W4239270640', 'https://openalex.org/W2088615611', 'https://openalex.org/W2137044559', 'https://openalex.org/W6718561954', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W4211190719', 'https://openalex.org/W2805607737', 'https://openalex.org/W2294799344', 'https://openalex.org/W2076307659', 'https://openalex.org/W3174056976', 'https://openalex.org/W2899377381', 'https://openalex.org/W2972764223', 'https://openalex.org/W2996383576', 'https://openalex.org/W2979476256', 'https://openalex.org/W1594560709', 'https://openalex.org/W3037057938', 'https://openalex.org/W2442329935', 'https://openalex.org/W3107720207', 'https://openalex.org/W2957905185', 'https://openalex.org/W3036601975', 'https://openalex.org/W3198429080', 'https://openalex.org/W3099782249']",2021-12-13
https://openalex.org/W3207547029,https://doi.org/10.21437/interspeech.2022-547,K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables,"Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work on the topic has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean which uses the Hangul having a unique writing system. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean automatic speech recognition by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the problem of words existing outside of the vocabulary. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method yields the best performance on both Korean ASR datasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a call-based dialog corpus). Further pre-training is also effective in language adaptation, leading to large improvements without additional data.","['https://openalex.org/W2964172053', 'https://openalex.org/W3096899423', 'https://openalex.org/W2884975363', 'https://openalex.org/W3091427154', 'https://openalex.org/W2193413348', 'https://openalex.org/W3163702140', 'https://openalex.org/W3005680577', 'https://openalex.org/W3037057938', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962780374', 'https://openalex.org/W2739883972', 'https://openalex.org/W2884254529', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127141656', 'https://openalex.org/W2890197052', 'https://openalex.org/W3124030384', 'https://openalex.org/W2963341956', 'https://openalex.org/W2992095114', 'https://openalex.org/W3000083466', 'https://openalex.org/W3099782249', 'https://openalex.org/W2787663903', 'https://openalex.org/W2327501763', 'https://openalex.org/W2769205094', 'https://openalex.org/W3083353586', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W2965373594']",2022-09-16
https://openalex.org/W3211110040,https://doi.org/10.1016/j.eswa.2022.117232,"Scanflow: A multi-graph framework for Machine Learning workflow management, supervision, and debugging","Machine Learning (ML) is more than just training models, the whole workflow must be considered. Once deployed, a ML model needs to be watched and constantly supervised and debugged to guarantee its validity and robustness in unexpected situations. Debugging in ML aims to identify (and address) the model weaknesses in not trivial contexts. Several techniques have been proposed to identify different types of model weaknesses, such as bias in classification, model decay, adversarial attacks, etc., yet there is not a generic framework that allows them to work in a collaborative, modular, portable, iterative way and, more importantly, flexible enough to allow both human- and machine-driven techniques. In this paper, we propose a novel containerized directed graph framework to support and accelerate end-to-end ML workflow management, supervision, and debugging. The framework allows defining and deploying ML workflows in containers, tracking their metadata, checking their behavior in production, and improving the models by using both learned and human-provided knowledge. We demonstrate these capabilities by integrating in the framework two hybrid systems to detect data drift distribution which identify the samples that are far from the latent space of the original distribution, ask for human intervention, and whether retrain the model or wrap it with a filter to remove the noise of corrupted data at inference time. We test these systems on MNIST-C, CIFAR-10-C, and FashionMNIST-C datasets, obtaining promising accuracy results with the help of human involvement.","['https://openalex.org/W6752771859', 'https://openalex.org/W2163922914', 'https://openalex.org/W6600428322', 'https://openalex.org/W2017257315', 'https://openalex.org/W6725811047', 'https://openalex.org/W6676297131', 'https://openalex.org/W2099419573', 'https://openalex.org/W6754561974', 'https://openalex.org/W2962772482', 'https://openalex.org/W2613574453', 'https://openalex.org/W6735804486', 'https://openalex.org/W6761839128', 'https://openalex.org/W2618530766', 'https://openalex.org/W6650119170', 'https://openalex.org/W6632100814', 'https://openalex.org/W6639478124', 'https://openalex.org/W6697144307', 'https://openalex.org/W6766978945', 'https://openalex.org/W6628973269', 'https://openalex.org/W6682610290', 'https://openalex.org/W2117539524', 'https://openalex.org/W6687241523', 'https://openalex.org/W2103104224', 'https://openalex.org/W6637162671', 'https://openalex.org/W6657028776', 'https://openalex.org/W2244109919', 'https://openalex.org/W1502957213', 'https://openalex.org/W2901122692', 'https://openalex.org/W3030163527', 'https://openalex.org/W2913556077', 'https://openalex.org/W4288404646', 'https://openalex.org/W2025768430', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962872506', 'https://openalex.org/W2964308564', 'https://openalex.org/W1981276685', 'https://openalex.org/W2913668833', 'https://openalex.org/W2888580379', 'https://openalex.org/W2900812411', 'https://openalex.org/W2133564696', 'https://openalex.org/W2296335794', 'https://openalex.org/W1652505363', 'https://openalex.org/W2271840356', 'https://openalex.org/W2989818084', 'https://openalex.org/W3005742798', 'https://openalex.org/W2775461895', 'https://openalex.org/W3004223849', 'https://openalex.org/W2002011878', 'https://openalex.org/W2124776405', 'https://openalex.org/W4292779060', 'https://openalex.org/W4200329523', 'https://openalex.org/W3037057938', 'https://openalex.org/W2963076808', 'https://openalex.org/W2915480215', 'https://openalex.org/W2982767129', 'https://openalex.org/W3118608800', 'https://openalex.org/W4234552385', 'https://openalex.org/W2953388933', 'https://openalex.org/W4300511536', 'https://openalex.org/W2750384547', 'https://openalex.org/W1970092758', 'https://openalex.org/W2512971201', 'https://openalex.org/W9657784', 'https://openalex.org/W4301599248', 'https://openalex.org/W4287824528', 'https://openalex.org/W2963446712', 'https://openalex.org/W3198429080', 'https://openalex.org/W4235916601', 'https://openalex.org/W2911495555', 'https://openalex.org/W4365393081', 'https://openalex.org/W2912283112', 'https://openalex.org/W2963060032']",2022-04-19
https://openalex.org/W3183140049,https://doi.org/10.18653/v1/2021.iwslt-1.20,ON-TRAC’ systems for the IWSLT 2021 low-resource speech translation and multilingual speech translation shared tasks,International audience,"['https://openalex.org/W2169200297', 'https://openalex.org/W2402146185', 'https://openalex.org/W1522301498', 'https://openalex.org/W2973049979', 'https://openalex.org/W3127012371', 'https://openalex.org/W2964121744', 'https://openalex.org/W2101105183', 'https://openalex.org/W3036601975', 'https://openalex.org/W3105425516', 'https://openalex.org/W3118578889', 'https://openalex.org/W1524333225', 'https://openalex.org/W2936774411', 'https://openalex.org/W3049256661', 'https://openalex.org/W3008549139', 'https://openalex.org/W3032001786', 'https://openalex.org/W2963250244', 'https://openalex.org/W3037057938', 'https://openalex.org/W2514741789', 'https://openalex.org/W3198429080', 'https://openalex.org/W3189296823', 'https://openalex.org/W2888867175', 'https://openalex.org/W2064675550', 'https://openalex.org/W3008125272', 'https://openalex.org/W3099782249', 'https://openalex.org/W3102811925', 'https://openalex.org/W1902237438', 'https://openalex.org/W3037337508', 'https://openalex.org/W2401271873', 'https://openalex.org/W3197771105', 'https://openalex.org/W4285719527']",2021-01-01
https://openalex.org/W4377089543,https://doi.org/10.1109/ner52421.2023.10123751,Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,"Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based ""AI Listener"" that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.","['https://openalex.org/W2140360678', 'https://openalex.org/W4200300291', 'https://openalex.org/W180052447', 'https://openalex.org/W1494198834', 'https://openalex.org/W3109832790', 'https://openalex.org/W3161343553', 'https://openalex.org/W6719401913', 'https://openalex.org/W2981852735', 'https://openalex.org/W1922655562', 'https://openalex.org/W3204224625', 'https://openalex.org/W2991284028', 'https://openalex.org/W2940585064', 'https://openalex.org/W3037057938', 'https://openalex.org/W1524333225', 'https://openalex.org/W4322714819', 'https://openalex.org/W4288089799', 'https://openalex.org/W2015441235', 'https://openalex.org/W2462780241', 'https://openalex.org/W3198429080']",2023-04-24
https://openalex.org/W3111682954,https://doi.org/10.1109/slt48900.2021.9383625,A Comparison of Self-Supervised Speech Representations As Input Features For Unsupervised Acoustic Word Embeddings,"Many speech processing tasks involve measuring the acoustic similarity between speech segments. Acoustic word embeddings (AWE) allow for efficient comparisons by mapping speech segments of arbitrary duration to fixed-dimensional vectors. For zero-resource speech processing, where unlabelled speech is the only available resource, some of the best AWE approaches rely on weak top-down constraints in the form of automatically discovered word-like segments. Rather than learning embeddings at the segment level, another line of zero-resource research has looked at representation learning at the short-time frame level. Recent approaches include self-supervised predictive coding and correspondence autoencoder (CAE) models. In this paper we consider whether these frame-level features are beneficial when used as inputs for training to an unsupervised AWE model. We compare frame-level features from contrastive predictive coding (CPC), autoregressive predictive coding and a CAE to conventional MFCCs. These are used as inputs to a recurrent CAE-based AWE model. In a word discrimination task on English and Xitsonga data, all three representation learning approaches outperform MFCCs, with CPC consistently showing the biggest improvement. In cross-lingual experiments we find that CPC features trained on English can also be transferred to Xitsonga.","['https://openalex.org/W3095361818', 'https://openalex.org/W6682948231', 'https://openalex.org/W6844194202', 'https://openalex.org/W3006358483', 'https://openalex.org/W2962850179', 'https://openalex.org/W3040997499', 'https://openalex.org/W2962824366', 'https://openalex.org/W1545920196', 'https://openalex.org/W2972943112', 'https://openalex.org/W2780786457', 'https://openalex.org/W6712202099', 'https://openalex.org/W2826003142', 'https://openalex.org/W2114347655', 'https://openalex.org/W2962980711', 'https://openalex.org/W6697293080', 'https://openalex.org/W2010188467', 'https://openalex.org/W2940544976', 'https://openalex.org/W6781819162', 'https://openalex.org/W2758697525', 'https://openalex.org/W3044967013', 'https://openalex.org/W3011209123', 'https://openalex.org/W6779919476', 'https://openalex.org/W3016181583', 'https://openalex.org/W6780218876', 'https://openalex.org/W2963425185', 'https://openalex.org/W2889313720', 'https://openalex.org/W3035202887', 'https://openalex.org/W1577418252', 'https://openalex.org/W6756497944', 'https://openalex.org/W2963879199', 'https://openalex.org/W2057007397', 'https://openalex.org/W2927191280', 'https://openalex.org/W3097485645', 'https://openalex.org/W2025482506', 'https://openalex.org/W6704726871', 'https://openalex.org/W2059652594', 'https://openalex.org/W2072054026', 'https://openalex.org/W2963571336', 'https://openalex.org/W6697456849', 'https://openalex.org/W2963720603', 'https://openalex.org/W2190506272', 'https://openalex.org/W2791647162', 'https://openalex.org/W2064675550', 'https://openalex.org/W2044138293', 'https://openalex.org/W2407151108', 'https://openalex.org/W6631190155', 'https://openalex.org/W6679436768', 'https://openalex.org/W2888329843', 'https://openalex.org/W2145410271', 'https://openalex.org/W2157331557', 'https://openalex.org/W3037057938', 'https://openalex.org/W3036601975', 'https://openalex.org/W2842511635', 'https://openalex.org/W2130942839', 'https://openalex.org/W2899518769', 'https://openalex.org/W2346964103', 'https://openalex.org/W3098361150', 'https://openalex.org/W3099782249', 'https://openalex.org/W2972574141', 'https://openalex.org/W2964121744', 'https://openalex.org/W3134136786', 'https://openalex.org/W2963620343', 'https://openalex.org/W2296681920', 'https://openalex.org/W2152790380', 'https://openalex.org/W2973026522', 'https://openalex.org/W2396043527', 'https://openalex.org/W3096196861']",2021-01-19
https://openalex.org/W3186761682,https://doi.org/10.18653/v1/2021.iwslt-1.14,FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task,"In this paper, we describe our end-to-end multilingual speech translation system submitted to the IWSLT 2021 evaluation campaign on the Multilingual Speech Translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin. In some translation directions, our speech translation results evaluated on the public Multilingual TEDx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input.","['https://openalex.org/W3162037819', 'https://openalex.org/W3037057938', 'https://openalex.org/W3015698636', 'https://openalex.org/W3036601975', 'https://openalex.org/W3035390927', 'https://openalex.org/W3095410713', 'https://openalex.org/W3032433061', 'https://openalex.org/W3176711365', 'https://openalex.org/W3107826490', 'https://openalex.org/W3032816972', 'https://openalex.org/W2963250244', 'https://openalex.org/W3198429080', 'https://openalex.org/W3008549139', 'https://openalex.org/W2963672008', 'https://openalex.org/W3119308075', 'https://openalex.org/W3099782249', 'https://openalex.org/W3173767661', 'https://openalex.org/W3112092703', 'https://openalex.org/W2962735107', 'https://openalex.org/W3127012371', 'https://openalex.org/W630532510', 'https://openalex.org/W3030437843', 'https://openalex.org/W2989539713', 'https://openalex.org/W3197771105']",2021-01-01
https://openalex.org/W4327858845,https://doi.org/10.21203/rs.3.rs-2708355/v1,Multilingual Speech Recognition Initiative for African Languages,"Abstract This paper summarizes a speech recognition initiative for African languages. More precisely, we propose innovative approaches that address the low-resource property of these languages. For both monolingual and multilingual systems, our methods rely on self-supervised pre-trained models for multiple languages. We tested our method on seven African languages and dialects: Amharic, Darija, Fongbe, Sudanese, Swahili, Wolof, and Yoruba. We first trained monolingual models that were used as baselines, and then proposed proof-of-concepts for systems that handle multiple languages. Our multilingual systems were based on three scenarios: (a) we trained a single model by concate-nating the multilingual corpora; (b) we discussed this first model by testing another joint model that predicts the spoken language using language-specific tokens before the text transcription; and (c) we fed a one-hot encoder vector to the latent feature extractions before training the single model and for inference. For this purpose, a language identification model is required. We also investigated the impact of lexical ambiguity by removing diacritics from text in some languages.","['https://openalex.org/W2110911455', 'https://openalex.org/W2769180089', 'https://openalex.org/W2770604845', 'https://openalex.org/W2572097499', 'https://openalex.org/W2897369970', 'https://openalex.org/W2524544624', 'https://openalex.org/W2127141656', 'https://openalex.org/W3213029956', 'https://openalex.org/W2786672974', 'https://openalex.org/W4285265369', 'https://openalex.org/W1922655562', 'https://openalex.org/W2193413348', 'https://openalex.org/W2896457183', 'https://openalex.org/W3021268743', 'https://openalex.org/W4200416755', 'https://openalex.org/W2939710050', 'https://openalex.org/W3037057938', 'https://openalex.org/W3158394340', 'https://openalex.org/W2108978495', 'https://openalex.org/W3215667245', 'https://openalex.org/W3216017440', 'https://openalex.org/W2094544353', 'https://openalex.org/W3092791109', 'https://openalex.org/W6662183122', 'https://openalex.org/W2919115771', 'https://openalex.org/W2163605009', 'https://openalex.org/W6739901393', 'https://openalex.org/W3158058158', 'https://openalex.org/W2403246322', 'https://openalex.org/W1637570796', 'https://openalex.org/W3167533889', 'https://openalex.org/W6746214216', 'https://openalex.org/W2578746142', 'https://openalex.org/W3207041304', 'https://openalex.org/W2971840980', 'https://openalex.org/W4385245566', 'https://openalex.org/W2973049979', 'https://openalex.org/W3174056976', 'https://openalex.org/W3098466758', 'https://openalex.org/W2618530766', 'https://openalex.org/W2160802179', 'https://openalex.org/W4281492411', 'https://openalex.org/W3198429080', 'https://openalex.org/W2963590452', 'https://openalex.org/W2106445473']",2023-03-20
https://openalex.org/W3128311069,https://doi.org/10.1109/icassp39728.2021.9413711,CDPAM: Contrastive Learning for Perceptual Audio Similarity,"Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.","['https://openalex.org/W6772622344', 'https://openalex.org/W2105921478', 'https://openalex.org/W3096159803', 'https://openalex.org/W2940385941', 'https://openalex.org/W6631190155', 'https://openalex.org/W2757519008', 'https://openalex.org/W6917585676', 'https://openalex.org/W2140828385', 'https://openalex.org/W3097945073', 'https://openalex.org/W2946896833', 'https://openalex.org/W6756824971', 'https://openalex.org/W2892129657', 'https://openalex.org/W6779919476', 'https://openalex.org/W2964287480', 'https://openalex.org/W2163922914', 'https://openalex.org/W3015829441', 'https://openalex.org/W6756205873', 'https://openalex.org/W2890983311', 'https://openalex.org/W2972394484', 'https://openalex.org/W2737697117', 'https://openalex.org/W1728888090', 'https://openalex.org/W2940290905', 'https://openalex.org/W6729924827', 'https://openalex.org/W3090098535', 'https://openalex.org/W6762114000', 'https://openalex.org/W6767111847', 'https://openalex.org/W3097934054', 'https://openalex.org/W2963830550', 'https://openalex.org/W6844194202', 'https://openalex.org/W2951974815', 'https://openalex.org/W2973049979', 'https://openalex.org/W6777232839', 'https://openalex.org/W6774314701', 'https://openalex.org/W3096656254', 'https://openalex.org/W3037057938', 'https://openalex.org/W2557915412', 'https://openalex.org/W2899775901', 'https://openalex.org/W2943895317', 'https://openalex.org/W3002439978', 'https://openalex.org/W2964121744', 'https://openalex.org/W2842511635', 'https://openalex.org/W3034978746', 'https://openalex.org/W2970006822', 'https://openalex.org/W2473388484', 'https://openalex.org/W2796495654', 'https://openalex.org/W2902476877', 'https://openalex.org/W3025035610', 'https://openalex.org/W2795409001', 'https://openalex.org/W2086381917', 'https://openalex.org/W2971127577']",2021-05-13
https://openalex.org/W3198555173,https://doi.org/10.21437/interspeech.2021-818,End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining,,"['https://openalex.org/W3048707928', 'https://openalex.org/W2927258315', 'https://openalex.org/W2891229414', 'https://openalex.org/W2963288440', 'https://openalex.org/W2981269614', 'https://openalex.org/W2889048668', 'https://openalex.org/W3037057938', 'https://openalex.org/W2786839803', 'https://openalex.org/W3099782249', 'https://openalex.org/W2803609229', 'https://openalex.org/W2933138175', 'https://openalex.org/W3095552229', 'https://openalex.org/W2889173507', 'https://openalex.org/W2963403868', 'https://openalex.org/W3015885816', 'https://openalex.org/W2478796159', 'https://openalex.org/W2972584841', 'https://openalex.org/W2963242190', 'https://openalex.org/W3094550259']",2021-08-27
https://openalex.org/W4388110922,https://doi.org/10.32604/cmc.2023.041332,Using Speaker-Specific Emotion Representations in Wav2vec 2.0-Based Modules for Speech Emotion Recognition,"Speech emotion recognition is essential for frictionless human-machine interaction, where machines respond to human instructions with context-aware actions. The properties of individuals’ voices vary with culture, language, gender, and personality. These variations in speaker-specific properties may hamper the performance of standard representations in downstream tasks such as speech emotion recognition (SER). This study demonstrates the significance of speaker-specific speech characteristics and how considering them can be leveraged to improve the performance of SER models. In the proposed approach, two wav2vec-based modules (a speaker-identification network and an emotion classification network) are trained with the Arcface loss. The speaker-identification network has a single attention block to encode an input audio waveform into a speaker-specific representation. The emotion classification network uses a wav2vec 2.0-backbone as well as four attention blocks to encode the same input audio waveform into an emotion representation. These two representations are then fused into a single vector representation containing emotion and speaker-specific information. Experimental results showed that the use of speaker-specific characteristics improves SER performance. Additionally, combining these with an angular marginal loss such as the Arcface loss improves intra-class compactness while increasing inter-class separability, as demonstrated by the plots of t-distributed stochastic neighbor embeddings (t-SNE). The proposed approach outperforms previous methods using similar training strategies, with a weighted accuracy (WA) of 72.14% and unweighted accuracy (UA) of 72.97% on the Interactive Emotional Dynamic Motion Capture (IEMOCAP) dataset. This demonstrates its effectiveness and potential to enhance human-machine interaction through more accurate emotion recognition in speech.","['https://openalex.org/W2193413348', 'https://openalex.org/W2777302760', 'https://openalex.org/W4285106979', 'https://openalex.org/W3154221883', 'https://openalex.org/W3036601975', 'https://openalex.org/W2146334809', 'https://openalex.org/W2726515241', 'https://openalex.org/W2784874046', 'https://openalex.org/W2110052520', 'https://openalex.org/W2885005742', 'https://openalex.org/W4223460604', 'https://openalex.org/W4377009115', 'https://openalex.org/W2767554854', 'https://openalex.org/W2478006605', 'https://openalex.org/W3124414150', 'https://openalex.org/W6811336702', 'https://openalex.org/W3015949486', 'https://openalex.org/W1614298861', 'https://openalex.org/W2982223350', 'https://openalex.org/W3135828102', 'https://openalex.org/W3169320628', 'https://openalex.org/W2896457183', 'https://openalex.org/W3093475354', 'https://openalex.org/W2939710050', 'https://openalex.org/W2979476256', 'https://openalex.org/W3144173820', 'https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W3157923770', 'https://openalex.org/W3211224152', 'https://openalex.org/W3144073776', 'https://openalex.org/W3206192140', 'https://openalex.org/W2594407953', 'https://openalex.org/W3099206234', 'https://openalex.org/W2520774990', 'https://openalex.org/W2609296554', 'https://openalex.org/W2786817236', 'https://openalex.org/W4286436501', 'https://openalex.org/W4205383840', 'https://openalex.org/W1494198834', 'https://openalex.org/W6761164592', 'https://openalex.org/W1522301498', 'https://openalex.org/W4372266212', 'https://openalex.org/W4312209192', 'https://openalex.org/W3202667537', 'https://openalex.org/W3197642003', 'https://openalex.org/W2963656735', 'https://openalex.org/W2969985801', 'https://openalex.org/W2962898354', 'https://openalex.org/W3197580070', 'https://openalex.org/W3198771897', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963881567', 'https://openalex.org/W4311137818', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963466847', 'https://openalex.org/W2933138175', 'https://openalex.org/W3198429080', 'https://openalex.org/W3198668286', 'https://openalex.org/W4287372095', 'https://openalex.org/W2962901777', 'https://openalex.org/W4375869379', 'https://openalex.org/W3201143670', 'https://openalex.org/W4239447739', 'https://openalex.org/W2110384472']",2023-01-01
https://openalex.org/W3120335524,,The CogALex shared task on monolingual and multilingual identification of semantic relations,"The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.","['https://openalex.org/W2135964261', 'https://openalex.org/W2772346733', 'https://openalex.org/W2250533418', 'https://openalex.org/W2250539671', 'https://openalex.org/W2155870214', 'https://openalex.org/W205765513', 'https://openalex.org/W2133656631', 'https://openalex.org/W1662133657', 'https://openalex.org/W2962684341', 'https://openalex.org/W2964134541', 'https://openalex.org/W2251981022', 'https://openalex.org/W3118816581', 'https://openalex.org/W2128823172', 'https://openalex.org/W2962972001', 'https://openalex.org/W2068737686', 'https://openalex.org/W2963768750', 'https://openalex.org/W1516501661', 'https://openalex.org/W2884156253', 'https://openalex.org/W3119478981', 'https://openalex.org/W3037057938', 'https://openalex.org/W3034588514', 'https://openalex.org/W2100258351', 'https://openalex.org/W141602984', 'https://openalex.org/W2799237891', 'https://openalex.org/W2251283399', 'https://openalex.org/W3038979953', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963639153', 'https://openalex.org/W2136930489', 'https://openalex.org/W3119088571', 'https://openalex.org/W2970017773', 'https://openalex.org/W2142086811', 'https://openalex.org/W2160587453', 'https://openalex.org/W2950577311', 'https://openalex.org/W2979082816', 'https://openalex.org/W2109617188', 'https://openalex.org/W2951160051', 'https://openalex.org/W2964350953', 'https://openalex.org/W2912144401', 'https://openalex.org/W2608204223', 'https://openalex.org/W2163953154', 'https://openalex.org/W2614785613', 'https://openalex.org/W2101941521']",2020-12-01
https://openalex.org/W3136219906,https://doi.org/10.48550/arxiv.2103.07762,OkwuGbé: End-to-End Speech Recognition for Fon and Igbo,"Language is inherent and compulsory for human communication. Whether expressed in a written or spoken way, it ensures understanding between people of the same and different regions. With the growing awareness and effort to include more low-resourced languages in NLP research, African languages have recently been a major subject of research in machine translation, and other text-based areas of NLP. However, there is still very little comparable research in speech recognition for African languages. Interestingly, some of the unique properties of African languages affecting NLP, like their diacritical and tonal complexities, have a major root in their speech, suggesting that careful speech interpretation could provide more intuition on how to deal with the linguistic complexities of African languages for text-based NLP. OkwuGbé is a step towards building speech recognition systems for African low-resourced languages. Using Fon and Igbo as our case study, we conduct a comprehensive linguistic analysis of each language and describe the creation of end-to-end, deep neural network-based speech recognition models for both languages. We present a state-of-art ASR model for Fon, as well as benchmark ASR model results for Igbo. Our linguistic analyses (for Fon and Igbo) provide valuable insights and guidance into the creation of speech recognition models for other African low-resourced languages, as well as guide future NLP research for Fon and Igbo. The Fon and Igbo models source code have been made publicly available.","['https://openalex.org/W1861537833', 'https://openalex.org/W3044684492', 'https://openalex.org/W3092791109', 'https://openalex.org/W2949117887', 'https://openalex.org/W3134606166', 'https://openalex.org/W2127141656', 'https://openalex.org/W1922655562', 'https://openalex.org/W2949640717', 'https://openalex.org/W2750167318', 'https://openalex.org/W2061272101', 'https://openalex.org/W2041113956', 'https://openalex.org/W2143612262', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015419784', 'https://openalex.org/W3013122861', 'https://openalex.org/W2901616036', 'https://openalex.org/W2795877110', 'https://openalex.org/W2095705004', 'https://openalex.org/W3035032094', 'https://openalex.org/W2988736778', 'https://openalex.org/W2936774411', 'https://openalex.org/W3149448910', 'https://openalex.org/W2745596852', 'https://openalex.org/W3081416955', 'https://openalex.org/W2970971581', 'https://openalex.org/W2949650786', 'https://openalex.org/W571987451', 'https://openalex.org/W2016095526', 'https://openalex.org/W3177035667', 'https://openalex.org/W3037057938', 'https://openalex.org/W2489997055', 'https://openalex.org/W2327501763', 'https://openalex.org/W2903739847', 'https://openalex.org/W2899663614', 'https://openalex.org/W2734531544', 'https://openalex.org/W3009096638', 'https://openalex.org/W2969945254', 'https://openalex.org/W2103869314', 'https://openalex.org/W2131774270', 'https://openalex.org/W2964308564', 'https://openalex.org/W570440754', 'https://openalex.org/W2908510526', 'https://openalex.org/W1586532344', 'https://openalex.org/W2302255633', 'https://openalex.org/W2963739817', 'https://openalex.org/W3034854768', 'https://openalex.org/W2963500086', 'https://openalex.org/W2125610452', 'https://openalex.org/W1647671624', 'https://openalex.org/W2945078028', 'https://openalex.org/W2524544624']",2021-03-13
https://openalex.org/W3204316016,https://doi.org/10.1109/asru51503.2021.9688061,Comparison of Self-Supervised Speech Pre-Training Methods on Flemish Dutch,"Recent research in speech processing exhibits a growing interest in unsupervised and self-supervised representation learning from unlabelled data to alleviate the need for large amounts of annotated data. We investigate several popular pre-training methods and apply them to Flemish Dutch. We compare off-the-shelf English pre-trained models to models trained on an increasing amount of Flemish data. We find that the most important factors for positive transfer to downstream speech recognition tasks include a substantial amount of data and a matching pre-training domain. Ideally, we also finetune on an annotated subset in the target language. All pre-trained models improve linear phone separability in Flemish, but not all methods improve Automatic Speech Recognition. We experience superior performance with wav2vec 2.0 and we obtain a 30% WER improvement by finetuning the multilingually pre-trained XLSR-53 model on Flemish Dutch, after integration into an HMM-DNN acoustic model.","['https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W3163596720', 'https://openalex.org/W2888867175', 'https://openalex.org/W6601192135', 'https://openalex.org/W6628009067', 'https://openalex.org/W2933138175', 'https://openalex.org/W3097286738', 'https://openalex.org/W3095410713', 'https://openalex.org/W3035202887', 'https://openalex.org/W2982223350', 'https://openalex.org/W3148040514', 'https://openalex.org/W2973049979', 'https://openalex.org/W3102342027', 'https://openalex.org/W2158081297', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016181583', 'https://openalex.org/W3198771897', 'https://openalex.org/W2972943112', 'https://openalex.org/W3162133897', 'https://openalex.org/W3100270690', 'https://openalex.org/W3033038061', 'https://openalex.org/W6696449567', 'https://openalex.org/W3015213852', 'https://openalex.org/W3119308075', 'https://openalex.org/W2973157397', 'https://openalex.org/W2626778328', 'https://openalex.org/W2896457183', 'https://openalex.org/W2842511635', 'https://openalex.org/W3095292526', 'https://openalex.org/W4287173589', 'https://openalex.org/W1524333225', 'https://openalex.org/W2926827382', 'https://openalex.org/W2939710050', 'https://openalex.org/W3125709657', 'https://openalex.org/W2292087804', 'https://openalex.org/W3002741552', 'https://openalex.org/W2996383576', 'https://openalex.org/W3160525311', 'https://openalex.org/W3198608154', 'https://openalex.org/W3198858531', 'https://openalex.org/W2979476256', 'https://openalex.org/W3016011332', 'https://openalex.org/W3030437843', 'https://openalex.org/W1269315465', 'https://openalex.org/W4297808394', 'https://openalex.org/W3198429080', 'https://openalex.org/W3041561163', 'https://openalex.org/W3144173820', 'https://openalex.org/W3099782249', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963341956', 'https://openalex.org/W2988736778', 'https://openalex.org/W3093579165', 'https://openalex.org/W29952999', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W3093533780', 'https://openalex.org/W3165666670', 'https://openalex.org/W3096587983']",2021-12-13
https://openalex.org/W4385695089,https://doi.org/10.1109/icaisc58445.2023.10200222,Transformer Based Speech to Text Translation for Indic Languages,"In this study, we are looking into additional ways that could help us enhance our output from Speech recog-nition models. We're specifically interested in improving the language model (LM) to improve the current accuracy. Rare words continue to be a challenge in developing high-quality speech recognition systems because words based on names, proper nouns, or localities, often called tail words are crucial to the decoded transcript's meaning. They are difficult to handle correctly since they do not appear frequently in the audio-text pairs that make up the training set. Using the transformer architecture, utilizing better datasets and finetuning can help us achieve a more sustainable model.","['https://openalex.org/W6811382313', 'https://openalex.org/W6779919476', 'https://openalex.org/W6847144370', 'https://openalex.org/W2118549742', 'https://openalex.org/W6851537007', 'https://openalex.org/W2948915926', 'https://openalex.org/W2805365459', 'https://openalex.org/W4226443359', 'https://openalex.org/W3037057938', 'https://openalex.org/W3204950970', 'https://openalex.org/W4365802508', 'https://openalex.org/W3198429080', 'https://openalex.org/W4312120502']",2023-06-16
https://openalex.org/W3145189018,https://doi.org/10.21437/interspeech.2021-1683,Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model,"In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.","['https://openalex.org/W3008912312', 'https://openalex.org/W2729379037', 'https://openalex.org/W2808939837', 'https://openalex.org/W3160641957', 'https://openalex.org/W2933138175', 'https://openalex.org/W2292087804', 'https://openalex.org/W2134800885', 'https://openalex.org/W2973229104', 'https://openalex.org/W2953190524', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W2842511635', 'https://openalex.org/W2127141656', 'https://openalex.org/W3092634798', 'https://openalex.org/W3041561163', 'https://openalex.org/W1524333225', 'https://openalex.org/W1631260214', 'https://openalex.org/W2166637769', 'https://openalex.org/W3003875258', 'https://openalex.org/W2805745502', 'https://openalex.org/W2972943112', 'https://openalex.org/W2970971581', 'https://openalex.org/W1494198834', 'https://openalex.org/W2973049979', 'https://openalex.org/W1779452081']",2021-08-27
https://openalex.org/W4307861474,https://doi.org/10.20944/preprints202210.0480.v1,Bilingual Asr Model With Language Identification for Brazilian Portuguese and South-American Spanish,"This paper documents the development of a special case of multilingual Automatic Speech Recognition model, specifically tailored to attend two languages spoken by the majority of Latin America, Portuguese and Spanish. The bilingual model combines Language Identification and Speech Recognition developed with the Wav2Vec2.0 architecture and trained on several open and private speech datasets. In this model, the feature encoder is trained jointly for all tasks and different context encoders are trained for each task. The model is evaluated separately on two tasks: language identification and speech recognition. The results indicate that this model achieves good performance on speech recognition and average performance on language identification, training on a low quantity of speech material. The average accuracy of the language identification module on the MLS dataset is 66.75%. The average Word Error Rate in the same scenario is 13.89%, which is better than average 22.58% achieved by the commercial speech recognizer developed by Google.","['https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W2091746061', 'https://openalex.org/W2995929068', 'https://openalex.org/W3095410713', 'https://openalex.org/W2992847832', 'https://openalex.org/W2888922844', 'https://openalex.org/W2805087519', 'https://openalex.org/W2894835365', 'https://openalex.org/W3040454670', 'https://openalex.org/W3031533975', 'https://openalex.org/W3042007685', 'https://openalex.org/W3036601975', 'https://openalex.org/W2139851371', 'https://openalex.org/W2154740140', 'https://openalex.org/W3028785944', 'https://openalex.org/W3106807794', 'https://openalex.org/W3180180466', 'https://openalex.org/W3030437843', 'https://openalex.org/W3096032230', 'https://openalex.org/W2997787503', 'https://openalex.org/W3198429080', 'https://openalex.org/W4287726212', 'https://openalex.org/W2316579313', 'https://openalex.org/W3139878283', 'https://openalex.org/W1526974435']",2022-10-31
https://openalex.org/W4404462754,https://doi.org/10.1121/10.0034430,Direct articulatory observation reveals phoneme recognition performance characteristics of a self-supervised speech model,"Variability in speech pronunciation is widely observed across different linguistic backgrounds, which impacts modern automatic speech recognition performance. Here, we evaluate the performance of a self-supervised speech model in phoneme recognition using direct articulatory evidence. Findings indicate significant differences in phoneme recognition, especially in front vowels, between American English and Indian English speakers. To gain a deeper understanding of these differences, we conduct real-time MRI-based articulatory analysis, revealing distinct velar region patterns during the production of specific front vowels. This underscores the need to deepen the scientific understanding of self-supervised speech model variances to advance robust and inclusive speech technology.","['https://openalex.org/W4238812956', 'https://openalex.org/W3036601975', 'https://openalex.org/W2158203944', 'https://openalex.org/W2018315802', 'https://openalex.org/W4307466192', 'https://openalex.org/W3037057938', 'https://openalex.org/W3036367741', 'https://openalex.org/W2140300418', 'https://openalex.org/W2182927573', 'https://openalex.org/W2184285154', 'https://openalex.org/W4307928434', 'https://openalex.org/W3130860000', 'https://openalex.org/W2747874407', 'https://openalex.org/W4385822606', 'https://openalex.org/W2996769030', 'https://openalex.org/W117109648', 'https://openalex.org/W3144073776', 'https://openalex.org/W1475549456', 'https://openalex.org/W2137037226', 'https://openalex.org/W4295992775', 'https://openalex.org/W3204224625', 'https://openalex.org/W3183290900']",2024-11-01
https://openalex.org/W4406087185,https://doi.org/10.59490/ejtir.2024.24.4.7531,Minimum effort adaptation of automatic speech recognition system in air traffic management,"Advancements in Automatic Speech Recognition (ASR) technology is exemplified by ubiquitous voice assistants such as Siri and Alexa. Researchers have been exploring the application of ASR for Air Traffic Management (ATM) systems. Initial prototypes utilized ASR to pre-fill aircraft radar labels and achieved a technological readiness level before industrialization (TRL6). However, accurately recognizing infrequently used but highly informative domain-specific vocabulary is still an issue. This includes waypoint names specific to each airspace region and unique airline designators, e.g., “dexon” or “pobeda”. Traditionally, open-source ASR toolkits or large pre-trained models require substantial domain-specific transcribed speech data to adapt to specialized vocabularies. However, typically, a “universal” ASR engine capable of reliably recognizing a core dictionary of several hundreds of frequently used words suffices for ATM applications. The challenge lies in dynamically integrating the additional region-specific words used less frequently. These uncommon words are crucial for maintaining clear communication within the ATM environment. This paper proposes a novel approach that facilitates the dynamic integration of these new and specific word entities into the existing universal ASR system. This paves the way for “plug-and-play” customization with minimal expert intervention and eliminates the need for extensive fine-tuning of the universal ASR model. The proposed approach demonstrably improves the accuracy of these region-specific words by a factor of ≈7 (from 10% F1-score to 70%) for all rare words and ≈5 (from 13% F1-score to 64%) for waypoints.","['https://openalex.org/W4379231824', 'https://openalex.org/W4388561595', 'https://openalex.org/W4392911130', 'https://openalex.org/W4319862232', 'https://openalex.org/W4391012982', 'https://openalex.org/W1540624770', 'https://openalex.org/W3161539047', 'https://openalex.org/W4372295708', 'https://openalex.org/W7014773875', 'https://openalex.org/W4366399696', 'https://openalex.org/W2046932483', 'https://openalex.org/W1895481600', 'https://openalex.org/W2131342762', 'https://openalex.org/W2726599793', 'https://openalex.org/W2903601559', 'https://openalex.org/W3148326220', 'https://openalex.org/W4308670437', 'https://openalex.org/W3109258989', 'https://openalex.org/W2402146185', 'https://openalex.org/W3145189018', 'https://openalex.org/W3037057938', 'https://openalex.org/W3036601975', 'https://openalex.org/W4400434794', 'https://openalex.org/W6631362777', 'https://openalex.org/W6636811518', 'https://openalex.org/W3196895794', 'https://openalex.org/W4382603057', 'https://openalex.org/W6958958984']",2025-01-06
https://openalex.org/W4411824970,https://doi.org/10.3989/loquens.2025.e116,Evaluation of German Automatic Speech Recognition solutions in the context of speech and language therapy support of people with aphasia,"Those who suffer from aphasia benefit from digital speech and language therapy solutions, and automatic speech recognition (ASR) has been already used for giving feedback on the correctness of the answers in naming exercises. AphaDIGITAL application is to provide German-speaking users with detailed feedback on phonemic/phonetic and semantic errors, based on automatic speech and language processing. For this purpose, open-source ASR solutions for German were evaluated on different corpora of atypical speech, including two small datasets with aphasic speech samples. Character error rate, the number of precisely recognized items and empty outputs served as evaluation metrics. The four selected models are generally robust to the deteriorated condition of speech and audio quality and consistently outperform commercial models in atypical speech recognition. Applying error acceptance threshold, additional use of phonemic error rate, and other valuable insights for ASR implementation in aphaDIGITAL are discussed.","['https://openalex.org/W1985579076', 'https://openalex.org/W4376225888', 'https://openalex.org/W4309810303', 'https://openalex.org/W3213029956', 'https://openalex.org/W2961302496', 'https://openalex.org/W3127557305', 'https://openalex.org/W6650732429', 'https://openalex.org/W6639688857', 'https://openalex.org/W4411824970', 'https://openalex.org/W2188736233', 'https://openalex.org/W3133364914', 'https://openalex.org/W2019657476', 'https://openalex.org/W4226136526', 'https://openalex.org/W3037057938', 'https://openalex.org/W6767005581', 'https://openalex.org/W2737674149', 'https://openalex.org/W2251658484', 'https://openalex.org/W3197712601', 'https://openalex.org/W2995795209', 'https://openalex.org/W6603690868', 'https://openalex.org/W2756314874', 'https://openalex.org/W4284900157', 'https://openalex.org/W2909246629', 'https://openalex.org/W2586602577', 'https://openalex.org/W2065506310', 'https://openalex.org/W2974231335', 'https://openalex.org/W2502847729', 'https://openalex.org/W2748199382', 'https://openalex.org/W2508777706', 'https://openalex.org/W6650287573', 'https://openalex.org/W2058518981', 'https://openalex.org/W6685245795', 'https://openalex.org/W2889294685', 'https://openalex.org/W2990699120', 'https://openalex.org/W2902747799', 'https://openalex.org/W2582743722', 'https://openalex.org/W4311000453', 'https://openalex.org/W2767305917', 'https://openalex.org/W4407075413', 'https://openalex.org/W7001921137', 'https://openalex.org/W6903849114', 'https://openalex.org/W6603343362', 'https://openalex.org/W3203147359', 'https://openalex.org/W3095259585', 'https://openalex.org/W1574317991', 'https://openalex.org/W964345155', 'https://openalex.org/W6810506511', 'https://openalex.org/W6790415189']",2025-06-30
https://openalex.org/W3153532013,https://doi.org/10.18653/v1/2021.emnlp-main.591,Multilingual and Cross-Lingual Intent Detection from Spoken Data,"We present a systematic study on multilingual and cross-lingual intent detection from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the intent detection task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) can yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., zero-shot versus few-shot learning, translation direction, and impact of speech recognition. We see this work as an important step towards more inclusive development and evaluation of multilingual intent detectors from spoken data, in a much wider spectrum of languages compared to prior work.","['https://openalex.org/W3168656614', 'https://openalex.org/W3045492832', 'https://openalex.org/W2970283086', 'https://openalex.org/W1665214252', 'https://openalex.org/W2810095012', 'https://openalex.org/W2891177506', 'https://openalex.org/W3096032230', 'https://openalex.org/W3103425602', 'https://openalex.org/W3038033387', 'https://openalex.org/W2963854351', 'https://openalex.org/W2898700502', 'https://openalex.org/W88081813', 'https://openalex.org/W2963341956', 'https://openalex.org/W2948110372', 'https://openalex.org/W3037057938', 'https://openalex.org/W2970618241', 'https://openalex.org/W2966087730', 'https://openalex.org/W2953958347', 'https://openalex.org/W2077302143', 'https://openalex.org/W2095705004', 'https://openalex.org/W2997771882', 'https://openalex.org/W3144596436', 'https://openalex.org/W2898856000', 'https://openalex.org/W2969574947', 'https://openalex.org/W3104078590', 'https://openalex.org/W2097550833', 'https://openalex.org/W2964121744']",2021-01-01
https://openalex.org/W3158793689,https://doi.org/10.21437/interspeech.2021-556,LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,"Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This questions the objective comparison of SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose LeBenchmark: a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also focus on speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact. LeBenchmark is shared with the scientific community for reproducible research in SSL from speech.","['https://openalex.org/W2971155163', 'https://openalex.org/W3016181583', 'https://openalex.org/W3092424727', 'https://openalex.org/W3034978746', 'https://openalex.org/W2810556878', 'https://openalex.org/W3035202887', 'https://openalex.org/W3049256661', 'https://openalex.org/W2576530755', 'https://openalex.org/W2962739339', 'https://openalex.org/W3015867372', 'https://openalex.org/W2972327934', 'https://openalex.org/W1524333225', 'https://openalex.org/W2972943112', 'https://openalex.org/W157724941', 'https://openalex.org/W3119308075', 'https://openalex.org/W2988736778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2514741789', 'https://openalex.org/W3021934733', 'https://openalex.org/W3099782249', 'https://openalex.org/W2045528981', 'https://openalex.org/W3016011332', 'https://openalex.org/W2249819665', 'https://openalex.org/W3030437843', 'https://openalex.org/W2933138175', 'https://openalex.org/W3095410713', 'https://openalex.org/W2313339984', 'https://openalex.org/W3127012371', 'https://openalex.org/W2133564696', 'https://openalex.org/W2402146185', 'https://openalex.org/W2327501763', 'https://openalex.org/W2329093554', 'https://openalex.org/W3026640763', 'https://openalex.org/W3036601975', 'https://openalex.org/W2973049979', 'https://openalex.org/W2888867175', 'https://openalex.org/W3037057938', 'https://openalex.org/W3102342027', 'https://openalex.org/W3099944122', 'https://openalex.org/W2963341956', 'https://openalex.org/W3054645415', 'https://openalex.org/W72302491', 'https://openalex.org/W2399733683']",2021-08-27
https://openalex.org/W3176656282,https://doi.org/10.18653/v1/2021.findings-acl.162,“Does it Matter When I Think You Are Lying?” Improving Deception Detection by Integrating Interlocutor’s Judgements in Conversations,"It is well known that human is not good at deception detection because of a natural inclination of truth-bias.However, during a conversation, when an interlocutor (interrogator) is being asked explicitly to assess whether his/her interacting partner (deceiver) is lying, this perceptual judgment depends highly on how the interrogator interprets the context of the conversation.While the deceptive behaviors can be difficult to model due to their heterogeneous manifestation, we hypothesize that this contextual information, i.e., whether the interlocutor trusts or distrusts what his/her partner is saying, provides an important condition in which the deceiver's deceptive behaviors are more consistently distinct.In this work, we propose a Judgmental-Enhanced Automatic Deception Detection Network (JEADDN) that explicitly considers interrogator's perceived truths-deceptions with three types of speechlanguage features (acoustic-prosodic, linguistic, and conversational temporal dynamics features) extracted during a conversation.We evaluate our framework on a large Mandarin Chinese Deception Dialog Database.The results show that the method significantly outperforms the current state-of-the-art approach without conditioning on the judgements of interrogators on this database.We further demonstrate that the behaviors of interrogators are important in detecting deception when the interrogators distrust the deceivers.Finally, with the late fusion of audio, text, and turntaking dynamics (TTD) features, we obtain promising results of 87.27% and 94.18% accuracy under the conditions that the interrogators trust and distrust the deceivers in deception detection which improves 7.27% and 13.57% than the model without considering the judgements of interlocutor respectively.","['https://openalex.org/W2088228840', 'https://openalex.org/W2963341956', 'https://openalex.org/W98324485', 'https://openalex.org/W2963597768', 'https://openalex.org/W2620968327', 'https://openalex.org/W3102725307', 'https://openalex.org/W2116828782', 'https://openalex.org/W2951146693', 'https://openalex.org/W2056714750', 'https://openalex.org/W1566563533', 'https://openalex.org/W3036601975', 'https://openalex.org/W3041988699', 'https://openalex.org/W2900531829', 'https://openalex.org/W3129648813', 'https://openalex.org/W2997900951', 'https://openalex.org/W2514266867', 'https://openalex.org/W2096809665', 'https://openalex.org/W3016658527', 'https://openalex.org/W2163833659', 'https://openalex.org/W3090549409', 'https://openalex.org/W2804621770', 'https://openalex.org/W2284721260', 'https://openalex.org/W2770400880', 'https://openalex.org/W3011511209', 'https://openalex.org/W2090721600', 'https://openalex.org/W2604452528', 'https://openalex.org/W3153018243', 'https://openalex.org/W2073427023', 'https://openalex.org/W2793571048', 'https://openalex.org/W4214916497', 'https://openalex.org/W4233336302', 'https://openalex.org/W3099782249', 'https://openalex.org/W3198429080', 'https://openalex.org/W3037057938', 'https://openalex.org/W2125556865', 'https://openalex.org/W2973121879', 'https://openalex.org/W3037198256', 'https://openalex.org/W3120923022', 'https://openalex.org/W3007419529', 'https://openalex.org/W2888942552', 'https://openalex.org/W2410465342', 'https://openalex.org/W2076064927', 'https://openalex.org/W2163265397', 'https://openalex.org/W2976364114', 'https://openalex.org/W4295312788', 'https://openalex.org/W1947418959', 'https://openalex.org/W1676969001', 'https://openalex.org/W2970971581', 'https://openalex.org/W2085662862', 'https://openalex.org/W3015630747', 'https://openalex.org/W2915177913', 'https://openalex.org/W2096006703', 'https://openalex.org/W2747192447', 'https://openalex.org/W2940935336']",2021-01-01
https://openalex.org/W3204844985,https://doi.org/10.1109/asru51503.2021.9688123,On Architectures and Training for Raw Waveform Feature Extraction in ASR,"With the success of neural network based modeling in automatic speech recognition (ASR), many studies investigated acoustic modeling and learning of feature extractors directly based on the raw waveform. Recently, one line of research has focused on unsupervised pre-training of feature extractors on audio-only data to improve downstream ASR performance. In this work, we investigate the usefulness of one of these front-end frameworks, namely wav2vec, in a setting without additional untranscribed data for hybrid ASR systems. We compare this framework both to the manually defined standard Gammatone feature set, as well as to features extracted as part of the acoustic model of an ASR system trained supervised. We study the benefits of using the pre-trained feature extractor and explore how to additionally exploit an existing acoustic model trained with different features. Finally, we systematically examine combinations of the described features in order to further advance the performance.","['https://openalex.org/W6750824539', 'https://openalex.org/W6723481938', 'https://openalex.org/W2024200390', 'https://openalex.org/W6757717501', 'https://openalex.org/W6777337586', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W6779919476', 'https://openalex.org/W3102342027', 'https://openalex.org/W6638749077', 'https://openalex.org/W6785097957', 'https://openalex.org/W3016234571', 'https://openalex.org/W2939690918', 'https://openalex.org/W6765658108', 'https://openalex.org/W2408093180', 'https://openalex.org/W3132613748', 'https://openalex.org/W3015411292', 'https://openalex.org/W2401869809', 'https://openalex.org/W2943845043', 'https://openalex.org/W6684821101', 'https://openalex.org/W2799958557', 'https://openalex.org/W2398826216', 'https://openalex.org/W3015726069', 'https://openalex.org/W2964052309', 'https://openalex.org/W2327501763', 'https://openalex.org/W4238620777', 'https://openalex.org/W2037976268', 'https://openalex.org/W2009150118', 'https://openalex.org/W2916979304', 'https://openalex.org/W6764260666', 'https://openalex.org/W2944255943', 'https://openalex.org/W2900440209', 'https://openalex.org/W1573166544', 'https://openalex.org/W3003382064', 'https://openalex.org/W3097558625', 'https://openalex.org/W3099782249', 'https://openalex.org/W2905489173', 'https://openalex.org/W2168249605', 'https://openalex.org/W2962728618', 'https://openalex.org/W3037057938', 'https://openalex.org/W2799923439', 'https://openalex.org/W2972692349', 'https://openalex.org/W3025165719', 'https://openalex.org/W1828163288', 'https://openalex.org/W2996383576', 'https://openalex.org/W3103005696', 'https://openalex.org/W3160551958', 'https://openalex.org/W3036601975']",2021-12-13
https://openalex.org/W4298397801,https://doi.org/10.1145/3552466.3556527,Deep Spectro-temporal Artifacts for Detecting Synthesized Speech,"The Audio Deep Synthesis Detection (ADD) Challenge has been held to detect\ngenerated human-like speech. With our submitted system, this paper provides an\noverall assessment of track 1 (Low-quality Fake Audio Detection) and track 2\n(Partially Fake Audio Detection). In this paper, spectro-temporal artifacts\nwere detected using raw temporal signals, spectral features, as well as deep\nembedding features. To address track 1, low-quality data augmentation, domain\nadaptation via finetuning, and various complementary feature information fusion\nwere aggregated in our system. Furthermore, we analyzed the clustering\ncharacteristics of subsystems with different features by visualization method\nand explained the effectiveness of our proposed greedy fusion strategy. As for\ntrack 2, frame transition and smoothing were detected using self-supervised\nlearning structure to capture the manipulation of PF attacks in the time\ndomain. We ranked 4th and 5th in track 1 and track 2, respectively.\n","['https://openalex.org/W2123299109', 'https://openalex.org/W2176804518', 'https://openalex.org/W2745896134', 'https://openalex.org/W3128666957', 'https://openalex.org/W3197358873', 'https://openalex.org/W4221138880', 'https://openalex.org/W2079735306', 'https://openalex.org/W3094002217', 'https://openalex.org/W3143148229', 'https://openalex.org/W3196774886', 'https://openalex.org/W6784846362', 'https://openalex.org/W6753968785', 'https://openalex.org/W3094678546', 'https://openalex.org/W2103075368', 'https://openalex.org/W3089365019', 'https://openalex.org/W3212117663', 'https://openalex.org/W3209984917', 'https://openalex.org/W4225527248', 'https://openalex.org/W4224932929', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W3157923770', 'https://openalex.org/W3140934869', 'https://openalex.org/W3183763412', 'https://openalex.org/W6801331058', 'https://openalex.org/W3200319900', 'https://openalex.org/W3201397251', 'https://openalex.org/W3199956586', 'https://openalex.org/W3200636257', 'https://openalex.org/W3196368020', 'https://openalex.org/W3095498208', 'https://openalex.org/W2972594541', 'https://openalex.org/W2954526560', 'https://openalex.org/W3154748208', 'https://openalex.org/W6743731764', 'https://openalex.org/W6761176859', 'https://openalex.org/W6631190155', 'https://openalex.org/W2157331557', 'https://openalex.org/W2792764867', 'https://openalex.org/W3163596559', 'https://openalex.org/W4226126880', 'https://openalex.org/W3198429080', 'https://openalex.org/W1522301498', 'https://openalex.org/W2187089797', 'https://openalex.org/W2928165649', 'https://openalex.org/W3197580070', 'https://openalex.org/W2972884023', 'https://openalex.org/W3199131409', 'https://openalex.org/W3197457172', 'https://openalex.org/W4309845474', 'https://openalex.org/W3199161700', 'https://openalex.org/W3131786367', 'https://openalex.org/W4385245566', 'https://openalex.org/W2887814324', 'https://openalex.org/W4300167667', 'https://openalex.org/W3181162804', 'https://openalex.org/W3198837656', 'https://openalex.org/W4287235833']",2022-10-01
https://openalex.org/W4328111785,https://doi.org/10.58921/ijaims.v1i2.36,An Approach for Identification of Speaker using Deep Learning,"The audio data is getting increased on daily basis across the world with the increase of telephonic conversations, video conferences, podcasts and voice notes. This study presents a mechanism for identification of a speaker in an audio file, which is based on the biometric features of human voice such as frequency, amplitude, and pitch. We proposed an unsupervised learning model which uses wav2vec 2.0 where the model learns speech representation with the dataset provided. We used Librispeech dataset in our research and we achieved our results at an error rate which is 1.8.&#x0D;","['https://openalex.org/W3027083471', 'https://openalex.org/W3037057938', 'https://openalex.org/W3125709657', 'https://openalex.org/W3026041220', 'https://openalex.org/W3036601975', 'https://openalex.org/W3198429080', 'https://openalex.org/W2979476256', 'https://openalex.org/W4287173589', 'https://openalex.org/W2981991061', 'https://openalex.org/W3096338464', 'https://openalex.org/W3002741552']",2023-01-31
https://openalex.org/W3096565276,https://doi.org/10.48550/arxiv.2011.01403,Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning,"State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.","['https://openalex.org/W2963656735', 'https://openalex.org/W2138204974', 'https://openalex.org/W3034978746', 'https://openalex.org/W3102631365', 'https://openalex.org/W2944828972', 'https://openalex.org/W3035524453', 'https://openalex.org/W2913881544', 'https://openalex.org/W2117539524', 'https://openalex.org/W2973562770', 'https://openalex.org/W2887997457', 'https://openalex.org/W3043462782', 'https://openalex.org/W2963759070', 'https://openalex.org/W1544827683', 'https://openalex.org/W2965373594', 'https://openalex.org/W3035160371', 'https://openalex.org/W2171849160', 'https://openalex.org/W2183341477', 'https://openalex.org/W2970206392', 'https://openalex.org/W2963012544', 'https://openalex.org/W3099782249', 'https://openalex.org/W1821462560', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035204084', 'https://openalex.org/W2194775991', 'https://openalex.org/W3100345210', 'https://openalex.org/W2293778248', 'https://openalex.org/W2943152387', 'https://openalex.org/W3026732421', 'https://openalex.org/W2963399829', 'https://openalex.org/W3034781633', 'https://openalex.org/W3108655343', 'https://openalex.org/W2963060032', 'https://openalex.org/W2971155163', 'https://openalex.org/W2917551568', 'https://openalex.org/W2964013229', 'https://openalex.org/W2962788840', 'https://openalex.org/W3034709122', 'https://openalex.org/W2788496822', 'https://openalex.org/W2798991696', 'https://openalex.org/W2889326796', 'https://openalex.org/W2842511635', 'https://openalex.org/W3010874390', 'https://openalex.org/W1840435438', 'https://openalex.org/W2163614729', 'https://openalex.org/W2097732278', 'https://openalex.org/W2962369866', 'https://openalex.org/W2963026768', 'https://openalex.org/W3118608800', 'https://openalex.org/W2970941190', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949736877', 'https://openalex.org/W3005700362', 'https://openalex.org/W3034850762', 'https://openalex.org/W2555897561', 'https://openalex.org/W3100859887', 'https://openalex.org/W2913939497', 'https://openalex.org/W2992308087', 'https://openalex.org/W3099206234', 'https://openalex.org/W3037057938', 'https://openalex.org/W2963310665']",2020-11-03
https://openalex.org/W3207222250,https://doi.org/10.48550/arxiv.2110.10329,SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training,"Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST~2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.","['https://openalex.org/W2995181338', 'https://openalex.org/W2963341956', 'https://openalex.org/W3122317902', 'https://openalex.org/W2970597249', 'https://openalex.org/W3030163527', 'https://openalex.org/W11314411', 'https://openalex.org/W2336585117', 'https://openalex.org/W3204696009', 'https://openalex.org/W3126261584', 'https://openalex.org/W2963846996', 'https://openalex.org/W3035579820', 'https://openalex.org/W2251939518', 'https://openalex.org/W3093579165', 'https://openalex.org/W2965373594', 'https://openalex.org/W3152609875', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250357346', 'https://openalex.org/W3119866685', 'https://openalex.org/W3034999214', 'https://openalex.org/W2787560479', 'https://openalex.org/W2125336414', 'https://openalex.org/W3144173820', 'https://openalex.org/W3082274269', 'https://openalex.org/W1494198834', 'https://openalex.org/W2936295285', 'https://openalex.org/W2963250244', 'https://openalex.org/W2546744831', 'https://openalex.org/W3104681546', 'https://openalex.org/W2995929068', 'https://openalex.org/W2138204974', 'https://openalex.org/W2270070752', 'https://openalex.org/W2943552823', 'https://openalex.org/W2547875792', 'https://openalex.org/W2132339004', 'https://openalex.org/W3165666670', 'https://openalex.org/W2842511635', 'https://openalex.org/W3154596443', 'https://openalex.org/W2963799213', 'https://openalex.org/W3193521535', 'https://openalex.org/W2958953787', 'https://openalex.org/W2950797609', 'https://openalex.org/W2973049979', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963748441', 'https://openalex.org/W2983040767', 'https://openalex.org/W2963310665', 'https://openalex.org/W2970049541', 'https://openalex.org/W3054645415', 'https://openalex.org/W2996383576', 'https://openalex.org/W3169483174', 'https://openalex.org/W2626778328', 'https://openalex.org/W3093502935', 'https://openalex.org/W3037057938', 'https://openalex.org/W3032816972', 'https://openalex.org/W3153287399', 'https://openalex.org/W3197324626', 'https://openalex.org/W3211483028', 'https://openalex.org/W3011411500', 'https://openalex.org/W3097777922', 'https://openalex.org/W3139918052']",2021-10-20
https://openalex.org/W3165666670,https://doi.org/10.48550/arxiv.2105.11084,Unsupervised Speech Recognition,"Despite rapid progress in the recent past, current speech recognition systems still require labeled training data which limits this technology to a small fraction of the languages spoken around the globe. This paper describes wav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition models without any labeled data. We leverage self-supervised speech representations to segment unlabeled audio and learn a mapping from these representations to phonemes via adversarial training. The right representations are key to the success of our method. Compared to the best previous unsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark, wav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the best published systems trained on 960 hours of labeled data from only two years ago. We also experiment on nine other languages, including low-resource languages such as Kyrgyz, Swahili and Tatar.","['https://openalex.org/W2970049541', 'https://openalex.org/W3026041220', 'https://openalex.org/W2988736778', 'https://openalex.org/W3030437843', 'https://openalex.org/W2546938941', 'https://openalex.org/W3095410713', 'https://openalex.org/W3099782249', 'https://openalex.org/W2996383576', 'https://openalex.org/W2842511635', 'https://openalex.org/W2964121744', 'https://openalex.org/W2890536590', 'https://openalex.org/W2964169922', 'https://openalex.org/W2995680346', 'https://openalex.org/W2794209590', 'https://openalex.org/W2963340922', 'https://openalex.org/W2963216553', 'https://openalex.org/W1877570817', 'https://openalex.org/W2168171912', 'https://openalex.org/W2889326796', 'https://openalex.org/W2995181338', 'https://openalex.org/W2963414781', 'https://openalex.org/W3016181583', 'https://openalex.org/W2972706021', 'https://openalex.org/W3097558625', 'https://openalex.org/W211509693', 'https://openalex.org/W2401271873', 'https://openalex.org/W2126203737', 'https://openalex.org/W3095361818', 'https://openalex.org/W3096656254', 'https://openalex.org/W3127686677', 'https://openalex.org/W3160799772', 'https://openalex.org/W2130942839', 'https://openalex.org/W2126725946', 'https://openalex.org/W2347098582', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963118869', 'https://openalex.org/W2095705004', 'https://openalex.org/W1966812932', 'https://openalex.org/W2962879692', 'https://openalex.org/W3097777922', 'https://openalex.org/W2748816379', 'https://openalex.org/W2936774411', 'https://openalex.org/W2134800885', 'https://openalex.org/W2059824090', 'https://openalex.org/W1828163288', 'https://openalex.org/W2802248956', 'https://openalex.org/W2157149948', 'https://openalex.org/W3095173472', 'https://openalex.org/W2954930777', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963425185', 'https://openalex.org/W2401396251', 'https://openalex.org/W2100768664', 'https://openalex.org/W2964079874', 'https://openalex.org/W3096338464', 'https://openalex.org/W2577366047', 'https://openalex.org/W3037057938', 'https://openalex.org/W1710476689', 'https://openalex.org/W854541894', 'https://openalex.org/W3144173820', 'https://openalex.org/W2094544353', 'https://openalex.org/W2964227577', 'https://openalex.org/W2883586237', 'https://openalex.org/W2998702515', 'https://openalex.org/W2127141656', 'https://openalex.org/W1524333225', 'https://openalex.org/W3015522062', 'https://openalex.org/W2953190524', 'https://openalex.org/W3198275944', 'https://openalex.org/W2972943112', 'https://openalex.org/W3015419784', 'https://openalex.org/W2041394569', 'https://openalex.org/W2748795451', 'https://openalex.org/W1980044217', 'https://openalex.org/W2164579587', 'https://openalex.org/W2153579005', 'https://openalex.org/W2117041980', 'https://openalex.org/W1778492285', 'https://openalex.org/W3102342027', 'https://openalex.org/W2998532468', 'https://openalex.org/W2981991061', 'https://openalex.org/W2155273149', 'https://openalex.org/W2892009249', 'https://openalex.org/W2547875792', 'https://openalex.org/W2933138175', 'https://openalex.org/W2802422770', 'https://openalex.org/W2962824887', 'https://openalex.org/W2046932483', 'https://openalex.org/W2398490608', 'https://openalex.org/W2963341956', 'https://openalex.org/W2101509422', 'https://openalex.org/W2160815625', 'https://openalex.org/W51277926', 'https://openalex.org/W2195354', 'https://openalex.org/W3160525311', 'https://openalex.org/W2991213871', 'https://openalex.org/W2193413348', 'https://openalex.org/W2741602058', 'https://openalex.org/W2963602293', 'https://openalex.org/W2124509324', 'https://openalex.org/W2962799225', 'https://openalex.org/W2468716020', 'https://openalex.org/W2927673779', 'https://openalex.org/W2962942158', 'https://openalex.org/W2948947170', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963631907', 'https://openalex.org/W2125529971', 'https://openalex.org/W3093579165', 'https://openalex.org/W3089828951', 'https://openalex.org/W2973049979']",2021-05-24
https://openalex.org/W3121299949,https://doi.org/10.48550/arxiv.2101.07597,UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data,"In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.","['https://openalex.org/W2842511635', 'https://openalex.org/W2547875792', 'https://openalex.org/W2995929068', 'https://openalex.org/W2286443923', 'https://openalex.org/W3032859754', 'https://openalex.org/W3095184753', 'https://openalex.org/W3197411683', 'https://openalex.org/W2973157397', 'https://openalex.org/W3099782249', 'https://openalex.org/W2127141656', 'https://openalex.org/W2025401819', 'https://openalex.org/W3024182269', 'https://openalex.org/W3005511757', 'https://openalex.org/W3015213852', 'https://openalex.org/W2952509486', 'https://openalex.org/W2533523411', 'https://openalex.org/W1994606281', 'https://openalex.org/W2996383576', 'https://openalex.org/W1828163288', 'https://openalex.org/W2951974815', 'https://openalex.org/W2963341956', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963303951', 'https://openalex.org/W2626778328', 'https://openalex.org/W2982223350', 'https://openalex.org/W3037057938', 'https://openalex.org/W1494198834', 'https://openalex.org/W2327501763', 'https://openalex.org/W2025198378', 'https://openalex.org/W3102342027', 'https://openalex.org/W1992475611', 'https://openalex.org/W2294108103', 'https://openalex.org/W2972943112', 'https://openalex.org/W3015265920', 'https://openalex.org/W2962760690', 'https://openalex.org/W2795935804', 'https://openalex.org/W3101648800', 'https://openalex.org/W3094963205', 'https://openalex.org/W3094637009', 'https://openalex.org/W2055884980', 'https://openalex.org/W2973049979']",2021-01-19
https://openalex.org/W3186596101,,Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition.,"We present a method for continual learning of speech representations for multiple languages using self-supervised learning (SSL) and applying these for automatic speech recognition. There is an abundance of unannotated speech, so creating self-supervised representations from raw audio and finetuning on a small annotated datasets is a promising direction to build speech recognition systems. Wav2vec models perform SSL on raw audio in a pretraining phase and then finetune on a small fraction of annotated data. SSL models have produced state of the art results for ASR. However, these models are very expensive to pretrain with self-supervision. We tackle the problem of learning new language representations continually from audio without forgetting a previous language representation. We use ideas from continual learning to transfer knowledge from a previous task to speed up pretraining a new language task. Our continual-wav2vec2 model can decrease pretraining times by 32% when learning a new language task, and learn this new audio-language representation without forgetting previous language representation.","['https://openalex.org/W2941814890', 'https://openalex.org/W3170823761', 'https://openalex.org/W2996383576', 'https://openalex.org/W3037057938', 'https://openalex.org/W3025165719', 'https://openalex.org/W2583761661', 'https://openalex.org/W2963559848', 'https://openalex.org/W3198094329', 'https://openalex.org/W3099782249', 'https://openalex.org/W2113839990', 'https://openalex.org/W3015356564', 'https://openalex.org/W3016181583', 'https://openalex.org/W3093579165', 'https://openalex.org/W2426267443', 'https://openalex.org/W2060277733', 'https://openalex.org/W3101498587', 'https://openalex.org/W3102342027', 'https://openalex.org/W2952165242', 'https://openalex.org/W2970586779', 'https://openalex.org/W2462831000', 'https://openalex.org/W3134307371', 'https://openalex.org/W2963540014', 'https://openalex.org/W3116551962', 'https://openalex.org/W3097968133', 'https://openalex.org/W3030437843', 'https://openalex.org/W2765101016', 'https://openalex.org/W3110524561', 'https://openalex.org/W1494198834', 'https://openalex.org/W2737492962', 'https://openalex.org/W2973049979', 'https://openalex.org/W2547875792', 'https://openalex.org/W3153675281', 'https://openalex.org/W2473930607', 'https://openalex.org/W3002177189', 'https://openalex.org/W3008658065', 'https://openalex.org/W2963850662', 'https://openalex.org/W2933138175', 'https://openalex.org/W2964088867', 'https://openalex.org/W2187089797', 'https://openalex.org/W3160525311', 'https://openalex.org/W2902625698', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963211188', 'https://openalex.org/W2842511635', 'https://openalex.org/W2962724315']",2021-07-26
https://openalex.org/W3157697407,https://doi.org/10.48550/arxiv.2104.14830,Scaling End-to-End Models for Large-Scale Multilingual ASR,"Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.6K to 53.5K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model already outperforms monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth works better than width and large encoders do better than large decoders; (4) with continuous training, they can be adapted to new languages and domains.","['https://openalex.org/W3160766462', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971840980', 'https://openalex.org/W2808640845', 'https://openalex.org/W3163300396', 'https://openalex.org/W2962893195', 'https://openalex.org/W1549321558', 'https://openalex.org/W3092189037', 'https://openalex.org/W3096215352', 'https://openalex.org/W2962760690', 'https://openalex.org/W3037057938', 'https://openalex.org/W3045485643', 'https://openalex.org/W3032859754', 'https://openalex.org/W2933976917', 'https://openalex.org/W2293634267', 'https://openalex.org/W2958953787', 'https://openalex.org/W2271840356', 'https://openalex.org/W3043585682', 'https://openalex.org/W3016545553', 'https://openalex.org/W3040573126', 'https://openalex.org/W3025165719', 'https://openalex.org/W1855892484', 'https://openalex.org/W3095410713', 'https://openalex.org/W3001279689', 'https://openalex.org/W2964002616', 'https://openalex.org/W2963341956', 'https://openalex.org/W2526425061', 'https://openalex.org/W3093579165', 'https://openalex.org/W3030163527', 'https://openalex.org/W36434594', 'https://openalex.org/W3096032230', 'https://openalex.org/W2913340405', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962911098', 'https://openalex.org/W2160815625', 'https://openalex.org/W3095173472', 'https://openalex.org/W2928941594', 'https://openalex.org/W2585945212', 'https://openalex.org/W3016010032', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963403868', 'https://openalex.org/W3136989387']",2021-04-30
https://openalex.org/W3210188917,https://doi.org/10.48550/arxiv.2110.14513,Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations,"We present a neural analysis and synthesis (NANSY) framework that can manipulate voice, pitch, and speed of an arbitrary speech signal. Most of the previous works have focused on using information bottleneck to disentangle analysis features for controllable synthesis, which usually results in poor reconstruction quality. We address this issue by proposing a novel training strategy based on information perturbation. The idea is to perturb information in the original input signal (e.g., formant, pitch, and frequency response), thereby letting synthesis networks selectively take essential attributes to reconstruct the input signal. Because NANSY does not need any bottleneck structures, it enjoys both high reconstruction quality and controllability. Furthermore, NANSY does not require any labels associated with speech data such as text and speaker information, but rather uses a new set of analysis features, i.e., wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. Taking advantage of fully self-supervised training, NANSY can be easily extended to a multilingual setting by simply training it with a multilingual dataset. The experiments show that NANSY can achieve significant improvement in performance in several applications such as zero-shot voice conversion, pitch shift, and time-scale modification.","['https://openalex.org/W2471520273', 'https://openalex.org/W3099782249', 'https://openalex.org/W2962754210', 'https://openalex.org/W2518172956', 'https://openalex.org/W3020975377', 'https://openalex.org/W2502312327', 'https://openalex.org/W2164764235', 'https://openalex.org/W2972802841', 'https://openalex.org/W2972359262', 'https://openalex.org/W2592691248', 'https://openalex.org/W1571477232', 'https://openalex.org/W2963432880', 'https://openalex.org/W3112616666', 'https://openalex.org/W2099076569', 'https://openalex.org/W2187089797', 'https://openalex.org/W2842511635', 'https://openalex.org/W3128910262', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973157397', 'https://openalex.org/W2515028311', 'https://openalex.org/W3024869864', 'https://openalex.org/W3157576687', 'https://openalex.org/W2603777577', 'https://openalex.org/W2995233853', 'https://openalex.org/W2949281321', 'https://openalex.org/W3198095523', 'https://openalex.org/W2097645910', 'https://openalex.org/W2963175743', 'https://openalex.org/W2094721231', 'https://openalex.org/W3083362357', 'https://openalex.org/W3093077017', 'https://openalex.org/W3098403858', 'https://openalex.org/W2973046048', 'https://openalex.org/W2995521531', 'https://openalex.org/W3096524539', 'https://openalex.org/W3015213852', 'https://openalex.org/W2091425152', 'https://openalex.org/W2788372077', 'https://openalex.org/W2962369866', 'https://openalex.org/W2963091184', 'https://openalex.org/W3015587431', 'https://openalex.org/W3120229981', 'https://openalex.org/W2972659941', 'https://openalex.org/W2755000466', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963970792', 'https://openalex.org/W3121914243', 'https://openalex.org/W2884225676', 'https://openalex.org/W3147454823', 'https://openalex.org/W2428180336', 'https://openalex.org/W3037057938', 'https://openalex.org/W2929299742']",2021-10-27
https://openalex.org/W3049397270,,Adaptation Algorithms for Speech Recognition: An Overview.,"We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.","['https://openalex.org/W2963826681', 'https://openalex.org/W2128408412', 'https://openalex.org/W2156886787', 'https://openalex.org/W2288371874', 'https://openalex.org/W3105315310', 'https://openalex.org/W3015412845', 'https://openalex.org/W2889374926', 'https://openalex.org/W1967452917', 'https://openalex.org/W2399832792', 'https://openalex.org/W2087006792', 'https://openalex.org/W1658791872', 'https://openalex.org/W2045317385', 'https://openalex.org/W2394882406', 'https://openalex.org/W2069631319', 'https://openalex.org/W2998284473', 'https://openalex.org/W2394932179', 'https://openalex.org/W2508162385', 'https://openalex.org/W2099621636', 'https://openalex.org/W1497807607', 'https://openalex.org/W2750527898', 'https://openalex.org/W2936252403', 'https://openalex.org/W2962824709', 'https://openalex.org/W2756127416', 'https://openalex.org/W2127836646', 'https://openalex.org/W1592672653', 'https://openalex.org/W2617565145', 'https://openalex.org/W2791636785', 'https://openalex.org/W1985371235', 'https://openalex.org/W2962760690', 'https://openalex.org/W2953937146', 'https://openalex.org/W3007327651', 'https://openalex.org/W2963240019', 'https://openalex.org/W2795867901', 'https://openalex.org/W2020073413', 'https://openalex.org/W2251321385', 'https://openalex.org/W1899458394', 'https://openalex.org/W2680270903', 'https://openalex.org/W2776923314', 'https://openalex.org/W2595767284', 'https://openalex.org/W2972799770', 'https://openalex.org/W2903250132', 'https://openalex.org/W2953022181', 'https://openalex.org/W2921194440', 'https://openalex.org/W2046056978', 'https://openalex.org/W2398054607', 'https://openalex.org/W2117671523', 'https://openalex.org/W2559260703', 'https://openalex.org/W2525332836', 'https://openalex.org/W2095848329', 'https://openalex.org/W82936479', 'https://openalex.org/W2973157397', 'https://openalex.org/W2005708641', 'https://openalex.org/W1892978018', 'https://openalex.org/W3010079431', 'https://openalex.org/W2403307129', 'https://openalex.org/W2975901202', 'https://openalex.org/W2971840980', 'https://openalex.org/W130324013', 'https://openalex.org/W1995562189', 'https://openalex.org/W2093886330', 'https://openalex.org/W2005874308', 'https://openalex.org/W2982413405', 'https://openalex.org/W2160815625', 'https://openalex.org/W3044481399', 'https://openalex.org/W2963938518', 'https://openalex.org/W2253429366', 'https://openalex.org/W2115353073', 'https://openalex.org/W1509793305', 'https://openalex.org/W3104245447', 'https://openalex.org/W2515525878', 'https://openalex.org/W2889074736', 'https://openalex.org/W2938374794', 'https://openalex.org/W2890964092', 'https://openalex.org/W1915251500', 'https://openalex.org/W2402146185', 'https://openalex.org/W1526361935', 'https://openalex.org/W26405188', 'https://openalex.org/W1513862252', 'https://openalex.org/W97192492', 'https://openalex.org/W5524598', 'https://openalex.org/W1942035323', 'https://openalex.org/W2605287558', 'https://openalex.org/W2972389417', 'https://openalex.org/W2406264770', 'https://openalex.org/W1517386993', 'https://openalex.org/W2184135559', 'https://openalex.org/W2963432880', 'https://openalex.org/W1993409002', 'https://openalex.org/W2150884987', 'https://openalex.org/W2398776621', 'https://openalex.org/W2125838338', 'https://openalex.org/W2889158616', 'https://openalex.org/W2696967604', 'https://openalex.org/W2045644151', 'https://openalex.org/W2002342963', 'https://openalex.org/W2056738732', 'https://openalex.org/W2889785444', 'https://openalex.org/W2589099502', 'https://openalex.org/W3007550212', 'https://openalex.org/W2626778328', 'https://openalex.org/W2292517305', 'https://openalex.org/W2587088898', 'https://openalex.org/W3023256384', 'https://openalex.org/W1994606281', 'https://openalex.org/W3093620947', 'https://openalex.org/W1979651826', 'https://openalex.org/W2963364041', 'https://openalex.org/W2131774270', 'https://openalex.org/W2891177506', 'https://openalex.org/W2963211739', 'https://openalex.org/W1492293509', 'https://openalex.org/W1961429348', 'https://openalex.org/W1599512239', 'https://openalex.org/W2939173691', 'https://openalex.org/W2950924994', 'https://openalex.org/W2979509742', 'https://openalex.org/W2143612262', 'https://openalex.org/W2398754323', 'https://openalex.org/W3012043827', 'https://openalex.org/W2963486098', 'https://openalex.org/W3016008406', 'https://openalex.org/W2964309797', 'https://openalex.org/W3015216127', 'https://openalex.org/W2982211334', 'https://openalex.org/W50249030', 'https://openalex.org/W2081074144', 'https://openalex.org/W2033256038', 'https://openalex.org/W2972815379', 'https://openalex.org/W2905678190', 'https://openalex.org/W2112796928', 'https://openalex.org/W2100772283', 'https://openalex.org/W2973094925', 'https://openalex.org/W2952935684', 'https://openalex.org/W126474493', 'https://openalex.org/W2799800213', 'https://openalex.org/W2970351109', 'https://openalex.org/W1588593315', 'https://openalex.org/W2117620049', 'https://openalex.org/W2407793339', 'https://openalex.org/W2753160622', 'https://openalex.org/W2973213659', 'https://openalex.org/W2892009249', 'https://openalex.org/W2914282508', 'https://openalex.org/W2514741789', 'https://openalex.org/W2407299475', 'https://openalex.org/W1665921526', 'https://openalex.org/W2936078256', 'https://openalex.org/W2889494795', 'https://openalex.org/W2769025471', 'https://openalex.org/W2787133659', 'https://openalex.org/W2401396087', 'https://openalex.org/W2064675550', 'https://openalex.org/W2039057510', 'https://openalex.org/W2407320368', 'https://openalex.org/W2962940707', 'https://openalex.org/W2056445351', 'https://openalex.org/W173010698', 'https://openalex.org/W2276408190', 'https://openalex.org/W1710082047', 'https://openalex.org/W3007227084', 'https://openalex.org/W2024490156', 'https://openalex.org/W1481113913', 'https://openalex.org/W2747616140', 'https://openalex.org/W1821462560', 'https://openalex.org/W2507699225', 'https://openalex.org/W2963414781', 'https://openalex.org/W1978660892', 'https://openalex.org/W2601450892', 'https://openalex.org/W3022318834', 'https://openalex.org/W2990706771', 'https://openalex.org/W2403731734', 'https://openalex.org/W2160306971', 'https://openalex.org/W2888858245', 'https://openalex.org/W2100969003', 'https://openalex.org/W72302491', 'https://openalex.org/W2094147890', 'https://openalex.org/W1989549063', 'https://openalex.org/W2080005694', 'https://openalex.org/W3042359118', 'https://openalex.org/W2165108269', 'https://openalex.org/W2399550240', 'https://openalex.org/W2166637769', 'https://openalex.org/W2156692643', 'https://openalex.org/W2513257850', 'https://openalex.org/W2963341956', 'https://openalex.org/W182840523', 'https://openalex.org/W2963827914', 'https://openalex.org/W127709214', 'https://openalex.org/W2132339004', 'https://openalex.org/W2992035660', 'https://openalex.org/W2025198378', 'https://openalex.org/W2586950923', 'https://openalex.org/W2015633636', 'https://openalex.org/W2239847623', 'https://openalex.org/W2401277329', 'https://openalex.org/W2695252763', 'https://openalex.org/W2055226387', 'https://openalex.org/W2125107268', 'https://openalex.org/W3003382064', 'https://openalex.org/W2013996527', 'https://openalex.org/W1526236009', 'https://openalex.org/W3016010032', 'https://openalex.org/W2604132379', 'https://openalex.org/W2142416747', 'https://openalex.org/W2401812832', 'https://openalex.org/W2400607496', 'https://openalex.org/W2171928131', 'https://openalex.org/W2951672049', 'https://openalex.org/W2079623482', 'https://openalex.org/W1893339448', 'https://openalex.org/W3037057938', 'https://openalex.org/W2402040300', 'https://openalex.org/W1492775261', 'https://openalex.org/W143647774', 'https://openalex.org/W2889152503', 'https://openalex.org/W2295119550', 'https://openalex.org/W2250357346', 'https://openalex.org/W2641129314', 'https://openalex.org/W2889423969', 'https://openalex.org/W2579851902', 'https://openalex.org/W2117539524', 'https://openalex.org/W2145691584', 'https://openalex.org/W2889048668', 'https://openalex.org/W2913244614', 'https://openalex.org/W1494198834', 'https://openalex.org/W2123867168', 'https://openalex.org/W3032859754', 'https://openalex.org/W2138309071', 'https://openalex.org/W2802248956', 'https://openalex.org/W2749784707', 'https://openalex.org/W2396230943', 'https://openalex.org/W2151262064', 'https://openalex.org/W1828163288', 'https://openalex.org/W2156142001', 'https://openalex.org/W2407080277', 'https://openalex.org/W2787228318', 'https://openalex.org/W2140567543', 'https://openalex.org/W2083751884', 'https://openalex.org/W2514740276', 'https://openalex.org/W2737615485', 'https://openalex.org/W2146871184', 'https://openalex.org/W2972539100', 'https://openalex.org/W2294543795', 'https://openalex.org/W567546468', 'https://openalex.org/W2889488531', 'https://openalex.org/W2058552721', 'https://openalex.org/W3034729383', 'https://openalex.org/W2962893195', 'https://openalex.org/W2405866807', 'https://openalex.org/W2802497966', 'https://openalex.org/W3032901281', 'https://openalex.org/W3045726014', 'https://openalex.org/W2511277277', 'https://openalex.org/W2288402171', 'https://openalex.org/W3002741552', 'https://openalex.org/W2048526313', 'https://openalex.org/W2511419867', 'https://openalex.org/W1945532488', 'https://openalex.org/W2036351241', 'https://openalex.org/W2070725064', 'https://openalex.org/W3008037978', 'https://openalex.org/W2327501763', 'https://openalex.org/W3006835112', 'https://openalex.org/W2711861986', 'https://openalex.org/W2127141656', 'https://openalex.org/W2996545246', 'https://openalex.org/W1992475611', 'https://openalex.org/W2010362084', 'https://openalex.org/W2617258110', 'https://openalex.org/W92539696', 'https://openalex.org/W2939164678', 'https://openalex.org/W2910715461', 'https://openalex.org/W1553387275']",2020-08-14
https://openalex.org/W3204224625,https://doi.org/10.48550/arxiv.2109.11680,Simple and Effective Zero-shot Cross-lingual Phoneme Recognition,"Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech recognition systems without any labeled data. However, in many cases there is labeled data available for related languages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by mapping phonemes of the training languages to the target language using articulatory features. Experiments show that this simple method significantly outperforms prior work which introduced task-specific architectures and used only part of a monolingually pretrained model.","['https://openalex.org/W2963341956', 'https://openalex.org/W3096032230', 'https://openalex.org/W2292087804', 'https://openalex.org/W3093788532', 'https://openalex.org/W3160525311', 'https://openalex.org/W2972706021', 'https://openalex.org/W2963292011', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995929068', 'https://openalex.org/W3026041220', 'https://openalex.org/W2991213871', 'https://openalex.org/W3099782249', 'https://openalex.org/W2972943112', 'https://openalex.org/W3015877095', 'https://openalex.org/W3174617982', 'https://openalex.org/W2963403868', 'https://openalex.org/W3169320628', 'https://openalex.org/W2124509324', 'https://openalex.org/W3165666670', 'https://openalex.org/W2572670101', 'https://openalex.org/W3095410713', 'https://openalex.org/W2547875792', 'https://openalex.org/W3088761213', 'https://openalex.org/W3036601975', 'https://openalex.org/W3110524561', 'https://openalex.org/W2952509486', 'https://openalex.org/W2127141656', 'https://openalex.org/W3184789469', 'https://openalex.org/W2962799225', 'https://openalex.org/W66627554', 'https://openalex.org/W3037057938', 'https://openalex.org/W2998284473', 'https://openalex.org/W2963425185', 'https://openalex.org/W3027083471', 'https://openalex.org/W2996383576']",2021-09-23
https://openalex.org/W3178203035,https://doi.org/10.48550/arxiv.2107.05233,UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset,"Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.","['https://openalex.org/W3024182269', 'https://openalex.org/W2963925437', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963523217', 'https://openalex.org/W3099782249', 'https://openalex.org/W3125709657', 'https://openalex.org/W2127141656', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W3015194534', 'https://openalex.org/W3121299949', 'https://openalex.org/W3093579165', 'https://openalex.org/W2842511635', 'https://openalex.org/W2508418541', 'https://openalex.org/W2963631907', 'https://openalex.org/W3094667432', 'https://openalex.org/W2997347790', 'https://openalex.org/W2963376890', 'https://openalex.org/W2996383576', 'https://openalex.org/W3161873870', 'https://openalex.org/W2951974815', 'https://openalex.org/W3160525311', 'https://openalex.org/W1828163288', 'https://openalex.org/W2982413405', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016010032', 'https://openalex.org/W2626778328', 'https://openalex.org/W3037057938', 'https://openalex.org/W2160815625', 'https://openalex.org/W2982223350', 'https://openalex.org/W3139918052', 'https://openalex.org/W2973049979', 'https://openalex.org/W3165666670']",2021-07-12
https://openalex.org/W3202072801,https://doi.org/10.48550/arxiv.2110.01493,Exploiting Pre-Trained ASR Models for Alzheimer's Disease Recognition Through Spontaneous Speech,"Alzheimer's disease (AD) is a progressive neurodegenerative disease and recently attracts extensive attention worldwide. Speech technology is considered a promising solution for the early diagnosis of AD and has been enthusiastically studied. Most recent works concentrate on the use of advanced BERT-like classifiers for AD detection. Input to these classifiers are speech transcripts produced by automatic speech recognition (ASR) models. The major challenge is that the quality of transcription could degrade significantly under complex acoustic conditions in the real world. The detection performance, in consequence, is largely limited. This paper tackles the problem via tailoring and adapting pre-trained neural-network based ASR model for the downstream AD recognition task. Only bottom layers of the ASR model are retained. A simple fully-connected neural network is added on top of the tailored ASR model for classification. The heavy BERT classifier is discarded. The resulting model is light-weight and can be fine-tuned in an end-to-end manner for AD recognition. Our proposed approach takes only raw speech as input, and no extra transcription process is required. The linguistic information of speech is implicitly encoded in the tailored ASR model and contributes to boosting the performance. Experiments show that our proposed approach outperforms the best manual transcript-based RoBERTa by an absolute margin of 4.6% in terms of accuracy. Our best-performing models achieve the accuracy of 83.2% and 78.0% in the long-audio and short-audio competition tracks of the 2021 NCMMSC Alzheimer's Disease Recognition Challenge, respectively.","['https://openalex.org/W3098824823', 'https://openalex.org/W2963341956', 'https://openalex.org/W3037057938', 'https://openalex.org/W1965740658', 'https://openalex.org/W3163826605', 'https://openalex.org/W2891147667', 'https://openalex.org/W3097288651', 'https://openalex.org/W3099782249', 'https://openalex.org/W3036601975', 'https://openalex.org/W3096912371', 'https://openalex.org/W2808631503', 'https://openalex.org/W2889048668', 'https://openalex.org/W2965373594', 'https://openalex.org/W2991435809', 'https://openalex.org/W1853705225', 'https://openalex.org/W3094848124', 'https://openalex.org/W3097109903', 'https://openalex.org/W3143075381', 'https://openalex.org/W3015598461', 'https://openalex.org/W2889320648', 'https://openalex.org/W3024869864', 'https://openalex.org/W3198521429', 'https://openalex.org/W3161873458', 'https://openalex.org/W2901101348', 'https://openalex.org/W3097533615', 'https://openalex.org/W2085662862', 'https://openalex.org/W3198116506', 'https://openalex.org/W2239141610', 'https://openalex.org/W2597672453', 'https://openalex.org/W2407894931', 'https://openalex.org/W2995929068', 'https://openalex.org/W3210120707', 'https://openalex.org/W2963242190', 'https://openalex.org/W147964346', 'https://openalex.org/W3016010032', 'https://openalex.org/W3198786495', 'https://openalex.org/W2979979079', 'https://openalex.org/W2158516192']",2021-10-04
https://openalex.org/W3137720654,https://doi.org/10.48550/arxiv.2103.08207,XLST: Cross-lingual Self-training to Learn Multilingual Representation for Low Resource Speech Recognition,"In this paper, we propose a weakly supervised multilingual representation learning framework, called cross-lingual self-training (XLST). XLST is able to utilize a small amount of annotated data from high-resource languages to improve the representation learning on multilingual un-annotated data. Specifically, XLST uses a supervised trained model to produce initial representations and another model to learn from them, by maximizing the similarity between output embeddings of these two models. Furthermore, the moving average mechanism and multi-view data augmentation are employed, which are experimentally shown to be crucial to XLST. Comprehensive experiments have been conducted on the CommonVoice corpus to evaluate the effectiveness of XLST. Results on 5 downstream low-resource ASR tasks shows that our multilingual pretrained model achieves relatively 18.6% PER reduction over the state-of-the-art self-supervised method, with leveraging additional 100 hours of annotated English data.","['https://openalex.org/W1970890968', 'https://openalex.org/W2127141656', 'https://openalex.org/W2894835365', 'https://openalex.org/W1993660824', 'https://openalex.org/W2025198378', 'https://openalex.org/W3041561163', 'https://openalex.org/W2106440210', 'https://openalex.org/W2091746061', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W3125709657', 'https://openalex.org/W3107668149', 'https://openalex.org/W2982223350', 'https://openalex.org/W3095311338', 'https://openalex.org/W2891816510', 'https://openalex.org/W2131042651', 'https://openalex.org/W1994606281', 'https://openalex.org/W3034978746', 'https://openalex.org/W2941814890', 'https://openalex.org/W2402040300', 'https://openalex.org/W1796128977', 'https://openalex.org/W3121299949', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972943112', 'https://openalex.org/W3026041220', 'https://openalex.org/W2921087533', 'https://openalex.org/W2963292011', 'https://openalex.org/W2079508481', 'https://openalex.org/W2127982613', 'https://openalex.org/W2936774411', 'https://openalex.org/W3112034174', 'https://openalex.org/W3016181583', 'https://openalex.org/W2407897255', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015522062', 'https://openalex.org/W2972347614', 'https://openalex.org/W3101821705', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W2765407302', 'https://openalex.org/W2198724430', 'https://openalex.org/W2981857663']",2021-03-15
https://openalex.org/W3153287399,https://doi.org/10.48550/arxiv.2104.06678,Large-Scale Self- and Semi-Supervised Learning for Speech Translation,"In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.","['https://openalex.org/W2995181338', 'https://openalex.org/W2963341956', 'https://openalex.org/W3096490862', 'https://openalex.org/W2117278770', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015698636', 'https://openalex.org/W3037057938', 'https://openalex.org/W3049256661', 'https://openalex.org/W3006988520', 'https://openalex.org/W2997436923', 'https://openalex.org/W3027083471', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964161387', 'https://openalex.org/W3054645415', 'https://openalex.org/W3032433061', 'https://openalex.org/W2995746049', 'https://openalex.org/W3026041220', 'https://openalex.org/W3015522062', 'https://openalex.org/W2973049979', 'https://openalex.org/W2991213871', 'https://openalex.org/W1915251500', 'https://openalex.org/W3162037819', 'https://openalex.org/W3032816972', 'https://openalex.org/W3035390927', 'https://openalex.org/W3093579165', 'https://openalex.org/W3160525311', 'https://openalex.org/W3090196146', 'https://openalex.org/W2842511635', 'https://openalex.org/W3112092703', 'https://openalex.org/W3008549139', 'https://openalex.org/W2964172053', 'https://openalex.org/W3093788532', 'https://openalex.org/W3035524453', 'https://openalex.org/W2605131327', 'https://openalex.org/W2141440284', 'https://openalex.org/W3097787369', 'https://openalex.org/W3005680577', 'https://openalex.org/W2982129078', 'https://openalex.org/W3099782249', 'https://openalex.org/W854541894', 'https://openalex.org/W2964104866', 'https://openalex.org/W2963631907', 'https://openalex.org/W3008125272']",2021-04-14
https://openalex.org/W3098598562,https://doi.org/10.48550/arxiv.2011.06195,Towards Semi-Supervised Semantics Understanding from Speech,"Much recent work on Spoken Language Understanding (SLU) falls short in at least one of three ways: models were trained on oracle text input and neglected the Automatics Speech Recognition (ASR) outputs, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. We proposed a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed speech to address these. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU corpus. In parallel, we identified two inadequate settings under which SLU models have been tested: noise-robustness and E2E semantics evaluation. We tested the proposed framework under realistic environmental noises and with a new metric, the slots edit F1 score, on two public SLU corpora. Experiments show that our SLU framework with speech as input can perform on par with those with oracle text as input in semantics understanding, while environmental noises are present, and a limited amount of labeled semantics data is available.","['https://openalex.org/W2077302143', 'https://openalex.org/W3015267417', 'https://openalex.org/W3001770030', 'https://openalex.org/W3027083471', 'https://openalex.org/W3041561163', 'https://openalex.org/W3093312917', 'https://openalex.org/W2219249508', 'https://openalex.org/W2930682606', 'https://openalex.org/W3035202887', 'https://openalex.org/W2803609229', 'https://openalex.org/W2963446094', 'https://openalex.org/W2963340922', 'https://openalex.org/W2327501763', 'https://openalex.org/W2806429264', 'https://openalex.org/W2973049979', 'https://openalex.org/W1494198834', 'https://openalex.org/W3125709657', 'https://openalex.org/W3026041220', 'https://openalex.org/W2556930864', 'https://openalex.org/W3099782249', 'https://openalex.org/W2917128112', 'https://openalex.org/W2982223350', 'https://openalex.org/W2097550833', 'https://openalex.org/W2991557631', 'https://openalex.org/W3036601975', 'https://openalex.org/W2981458636', 'https://openalex.org/W3037057938', 'https://openalex.org/W2926827382', 'https://openalex.org/W2936774411', 'https://openalex.org/W3026408381', 'https://openalex.org/W2988736778', 'https://openalex.org/W3080993257', 'https://openalex.org/W2894164357', 'https://openalex.org/W2766219058', 'https://openalex.org/W3092630929', 'https://openalex.org/W2963288440', 'https://openalex.org/W2974831423', 'https://openalex.org/W2963403868', 'https://openalex.org/W3039910566', 'https://openalex.org/W2890964092', 'https://openalex.org/W3025035610', 'https://openalex.org/W2998616931', 'https://openalex.org/W2795935804', 'https://openalex.org/W2980282514', 'https://openalex.org/W3016011332', 'https://openalex.org/W2153501885', 'https://openalex.org/W2964117975', 'https://openalex.org/W3032892481', 'https://openalex.org/W3025429027', 'https://openalex.org/W2963720603', 'https://openalex.org/W3034875620', 'https://openalex.org/W2842511635', 'https://openalex.org/W2019116789', 'https://openalex.org/W2981991061', 'https://openalex.org/W3025324327', 'https://openalex.org/W2963341956', 'https://openalex.org/W2998532468', 'https://openalex.org/W2935542736', 'https://openalex.org/W3049038774', 'https://openalex.org/W3015412890', 'https://openalex.org/W2885185669', 'https://openalex.org/W2972818416', 'https://openalex.org/W2093973850', 'https://openalex.org/W2889201969', 'https://openalex.org/W2928075308', 'https://openalex.org/W2979722627', 'https://openalex.org/W2979476256', 'https://openalex.org/W3015522062', 'https://openalex.org/W1936920915']",2020-11-11
https://openalex.org/W3140964896,,Leveraging neural representations for facilitating access to untranscribed speech from endangered languages.,"For languages with insufficient resources to train speech recognition systems, query-by-example spoken term detection (QbE-STD) offers a way of accessing an untranscribed speech corpus by helping identify regions where spoken query terms occur. Yet retrieval performance can be poor when the query and corpus are spoken by different speakers and produced in different recording conditions. Using data selected from a variety of speakers and recording conditions from 7 Australian Aboriginal languages and a regional variety of Dutch, all of which are endangered or vulnerable, we evaluated whether QbE-STD performance on these languages could be improved by leveraging representations extracted from the pre-trained English wav2vec 2.0 model. Compared to the use of Mel-frequency cepstral coefficients and bottleneck features, we find that representations from the middle layers of the wav2vec 2.0 Transformer offer large gains in task performance (between 56% and 86%). While features extracted using the pre-trained English model yielded improved detection on all the evaluation languages, better detection performance was associated with the evaluation language's phonological similarity to English.","['https://openalex.org/W2979476256', 'https://openalex.org/W2957905185', 'https://openalex.org/W2442329935', 'https://openalex.org/W2088615611', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W3158394340', 'https://openalex.org/W2899377381']",2021-03-26
https://openalex.org/W3207041304,https://doi.org/10.48550/arxiv.2110.07909,Multilingual Speech Recognition using Knowledge Transfer across Learning Processes,"Multilingual end-to-end(E2E) models have shown a great potential in the expansion of the language coverage in the realm of automatic speech recognition(ASR). In this paper, we aim to enhance the multilingual ASR performance in two ways, 1)studying the impact of feeding a one-hot vector identifying the language, 2)formulating the task with a meta-learning objective combined with self-supervised learning (SSL). We associate every language with a distinct task manifold and attempt to improve the performance by transferring knowledge across learning processes itself as compared to transferring through final model parameters. We employ this strategy on a dataset comprising of 6 languages for an in-domain ASR task, by minimizing an objective related to expected gradient path length. Experimental results reveal the best pre-training strategy resulting in 3.55% relative reduction in overall WER. A combination of LEAP and SSL yields 3.51% relative reduction in overall WER when using language ID.","['https://openalex.org/W2025401819', 'https://openalex.org/W2716988359', 'https://openalex.org/W3008434450', 'https://openalex.org/W3156323585', 'https://openalex.org/W2902253437', 'https://openalex.org/W3096215352', 'https://openalex.org/W3095311338', 'https://openalex.org/W2964309797', 'https://openalex.org/W2964112702', 'https://openalex.org/W1978660892', 'https://openalex.org/W2964002616', 'https://openalex.org/W2963649564', 'https://openalex.org/W3167207712', 'https://openalex.org/W2147768505', 'https://openalex.org/W3016010032', 'https://openalex.org/W2971840980', 'https://openalex.org/W3037057938', 'https://openalex.org/W2276408190', 'https://openalex.org/W2786835190', 'https://openalex.org/W3178647810', 'https://openalex.org/W2291975472', 'https://openalex.org/W2601450892', 'https://openalex.org/W3204696009', 'https://openalex.org/W3093579165', 'https://openalex.org/W2962893195']",2021-10-15
https://openalex.org/W3109527089,https://doi.org/10.48550/arxiv.2012.00876,Automatically Identifying Language Family from Acoustic Examples in Low Resource Scenarios,"Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.","['https://openalex.org/W2939167810', 'https://openalex.org/W2937197076', 'https://openalex.org/W2270364989', 'https://openalex.org/W3037057938', 'https://openalex.org/W1994606281', 'https://openalex.org/W2971840980', 'https://openalex.org/W3030437843', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963217356', 'https://openalex.org/W2295297373', 'https://openalex.org/W2963431393', 'https://openalex.org/W2396312253', 'https://openalex.org/W1978660892', 'https://openalex.org/W2604173493']",2020-12-01
https://openalex.org/W3093848788,https://doi.org/10.48550/arxiv.2010.12231,Any-to-One Sequence-to-Sequence Voice Conversion using Self-Supervised Discrete Speech Representations,"We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.","['https://openalex.org/W2156142001', 'https://openalex.org/W95152782', 'https://openalex.org/W3048330082', 'https://openalex.org/W3015338123', 'https://openalex.org/W2962780374', 'https://openalex.org/W2547875792', 'https://openalex.org/W3016160783', 'https://openalex.org/W2842511635', 'https://openalex.org/W2933138175', 'https://openalex.org/W2996383576', 'https://openalex.org/W1494198834', 'https://openalex.org/W3101689408', 'https://openalex.org/W2899877258', 'https://openalex.org/W2982223350', 'https://openalex.org/W3016011332', 'https://openalex.org/W3037057938', 'https://openalex.org/W2973049979', 'https://openalex.org/W2994715919', 'https://openalex.org/W2903739847', 'https://openalex.org/W3034420534', 'https://openalex.org/W3025844872', 'https://openalex.org/W3035202887', 'https://openalex.org/W2130942839', 'https://openalex.org/W3125709657', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972970915']",2020-10-23
https://openalex.org/W3165619625,https://doi.org/10.24406/publica-380,Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition,"Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often contain incorrect phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classification task to distinguish Anglicisms from native German words. With this approach, the model learns to generate different pronunciations depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries to be added to an existing German speech recognition model. Tested on a special Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by a relative 1 % and the Anglicism error rate by a relative 3 %. With our experiment, we show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.","['https://openalex.org/W1614862348', 'https://openalex.org/W2968767118', 'https://openalex.org/W2250896178', 'https://openalex.org/W2090755665', 'https://openalex.org/W3037057938', 'https://openalex.org/W2963299674', 'https://openalex.org/W3099782249', 'https://openalex.org/W3098824823', 'https://openalex.org/W2964934899', 'https://openalex.org/W2130942839', 'https://openalex.org/W2748157908', 'https://openalex.org/W2064675550', 'https://openalex.org/W1895132741']",2022-01-01
https://openalex.org/W3177999760,https://doi.org/10.48550/arxiv.2107.07402,CLSRIL-23: Cross Lingual Speech Representations for Indic Languages,"We present a CLSRIL-23, a self supervised learning based audio pre-trained model which learns cross lingual speech representations from raw audio across 23 Indic languages. It is built on top of wav2vec 2.0 which is solved by training a contrastive task over masked latent speech representations and jointly learns the quantization of latents shared across all languages. We compare the language wise loss during pretraining to compare effects of monolingual and multilingual pretraining. Performance on some downstream fine-tuning tasks for speech recognition is also compared and our experiments show that multilingual pretraining outperforms monolingual training, in terms of learning speech representations which encodes phonetic similarity of languages and also in terms of performance on down stream tasks. A decrease of 5% is observed in WER and 9.5% in CER when a multilingual pretrained model is used for finetuning in Hindi. All the code models are also open sourced. CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio data to facilitate research in speech recognition for Indic languages. We hope that new state of the art systems will be created using the self supervised approach, especially for low resources Indic languages.","['https://openalex.org/W3037057938', 'https://openalex.org/W3016181583', 'https://openalex.org/W2995680346', 'https://openalex.org/W1574170747', 'https://openalex.org/W3099782249', 'https://openalex.org/W2091746061', 'https://openalex.org/W2547875792', 'https://openalex.org/W2127141656', 'https://openalex.org/W2160815625', 'https://openalex.org/W2134800885', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963425185', 'https://openalex.org/W2962788625', 'https://openalex.org/W2787560479', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963341956']",2021-07-15
https://openalex.org/W3196174113,https://doi.org/10.48550/arxiv.2108.10791,Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19,"Natural language processing (NLP) plays a significant role in tools for the COVID-19 pandemic response, from detecting misinformation on social media to helping to provide accurate clinical information or summarizing scientific research. However, the approaches developed thus far have not benefited all populations, regions or languages equally. We discuss ways in which current and future NLP approaches can be made more inclusive by covering low-resource languages, including alternative modalities, leveraging out-of-the-box tools and forming meaningful partnerships. We suggest several future directions for researchers interested in maximizing the positive societal impacts of NLP.","['https://openalex.org/W3098341425', 'https://openalex.org/W3037057938', 'https://openalex.org/W156690507', 'https://openalex.org/W2980704391', 'https://openalex.org/W2994898777', 'https://openalex.org/W1849719402', 'https://openalex.org/W2770664662', 'https://openalex.org/W255593144', 'https://openalex.org/W3024530634', 'https://openalex.org/W2550821151', 'https://openalex.org/W3000603264', 'https://openalex.org/W3097301690', 'https://openalex.org/W3109495579', 'https://openalex.org/W3027890277', 'https://openalex.org/W3033632083', 'https://openalex.org/W3037960575', 'https://openalex.org/W3016888066', 'https://openalex.org/W273093436', 'https://openalex.org/W3099781644', 'https://openalex.org/W1976185248', 'https://openalex.org/W2117332520', 'https://openalex.org/W2137910577', 'https://openalex.org/W2770259901', 'https://openalex.org/W3030648110', 'https://openalex.org/W3109672743', 'https://openalex.org/W3013122861', 'https://openalex.org/W2171890804', 'https://openalex.org/W2129094221', 'https://openalex.org/W3034748831', 'https://openalex.org/W2948869406', 'https://openalex.org/W2607147673', 'https://openalex.org/W2734823615', 'https://openalex.org/W2072514260', 'https://openalex.org/W3048070932']",2021-08-11
https://openalex.org/W3205328248,,Incremental Speech Synthesis For Speech-To-Speech Translation.,"In a speech-to-speech translation (S2ST) pipeline, the text-to-speech (TTS) module is an important component for delivering the translated speech to users. To enable incremental S2ST, the TTS module must be capable of synthesizing and playing utterances while its input text is still streaming in. In this work, we focus on improving the incremental synthesis performance of TTS models. With a simple data augmentation strategy based on prefixes, we are able to improve the incremental TTS quality to approach offline performance. Furthermore, we bring our incremental TTS system to the practical scenario in combination with an upstream simultaneous speech translation system, and show the gains also carry over to this use-case. In addition, we propose latency metrics tailored to S2ST applications, and investigate methods for latency reduction in this context.","['https://openalex.org/W3099782249', 'https://openalex.org/W3211696375', 'https://openalex.org/W3037057938', 'https://openalex.org/W3180374548', 'https://openalex.org/W2972895078', 'https://openalex.org/W2972495969', 'https://openalex.org/W3162000275', 'https://openalex.org/W3012492057', 'https://openalex.org/W3130016944', 'https://openalex.org/W2973048981', 'https://openalex.org/W2936969148', 'https://openalex.org/W3037793211', 'https://openalex.org/W2889095150', 'https://openalex.org/W3118578889', 'https://openalex.org/W3098403858', 'https://openalex.org/W2517566275', 'https://openalex.org/W3100608856', 'https://openalex.org/W3174758275', 'https://openalex.org/W1964771471', 'https://openalex.org/W3048023795', 'https://openalex.org/W3092316169', 'https://openalex.org/W3197407562', 'https://openalex.org/W3037465386', 'https://openalex.org/W3115075512', 'https://openalex.org/W2747874407', 'https://openalex.org/W3007068036', 'https://openalex.org/W2986976179', 'https://openalex.org/W2972802841', 'https://openalex.org/W3169320628', 'https://openalex.org/W3186843219', 'https://openalex.org/W44352932', 'https://openalex.org/W3186200218', 'https://openalex.org/W2056890381', 'https://openalex.org/W3104081910']",2021-10-15
https://openalex.org/W3206083773,https://doi.org/10.48550/arxiv.2110.08583,ASR4REAL: An extended benchmark for speech models,"Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models","['https://openalex.org/W2788481061', 'https://openalex.org/W2101045344', 'https://openalex.org/W2680270903', 'https://openalex.org/W2786459654', 'https://openalex.org/W3037057938', 'https://openalex.org/W3167533889', 'https://openalex.org/W2955148358', 'https://openalex.org/W3158977306', 'https://openalex.org/W3041048110', 'https://openalex.org/W3093502935', 'https://openalex.org/W1494198834', 'https://openalex.org/W3009634068', 'https://openalex.org/W3012624518', 'https://openalex.org/W3119308075', 'https://openalex.org/W2900091092', 'https://openalex.org/W3184326107', 'https://openalex.org/W3099782249']",2021-10-16
https://openalex.org/W4382934849,https://doi.org/10.32657/10356/168498,Enhancing spoken language identification and diarization for multilingual speech,"Spoken language identification (LID) refers to the automatic process of determining the identity of the language spoken in a speech signal. It has been widely employed as preprocessing in multilingual speech signal processing systems. While existing approaches have shown high performance for general LID, it is still challenging to perform well on speech of various durations. In addition, general LID methods only employ a single type of language cue. Since language cues depict language information from different perspectives, the incorporation of language cues is expected to exhibit higher performance compared to using a single language cue. Therefore, in this thesis, an x-vector self-attention LID (XSA-LID) model is proposed to achieve robustness to speech duration. Two approaches are next introduced to improve and incorporate the language cues, respectively. Finally, LID is performed in a more complex scenario—language diarization via an end-to-end LD model.&#13;\nTo achieve robustness against performance degradation due to varying duration, a dual-mode framework on the XSA-LID model with knowledge distillation (KD) is proposed. The dual-mode XSA-LID model is trained by jointly optimizing both the full and short modes with their respective inputs being the full-length speech and its short clip extracted by a specific Boolean mask before KD is applied to further boost the performance on short utterances. In addition, the impact of clip-wise linguistic variability and lexical integrity for LID is investigated by analyzing the variation of LID performance in terms of the lengths and positions of the mimicked speech clips.&#13;\nTo enhance LID from the perspective of language cues, two methods are next introduced through which language cues can be utilized efficiently and effectively. The first method investigated efficient methods to compute reliable representations and discard redundant information for LID using a pre-trained multilingual wav2vec 2.0 model. To determine an optimal basic system, the performance of the wav2vec features extracted from the different inner layers of the context network are compared. For this approach, the XSA-LID model forms the backbone used to discriminate between distinct languages. Two mechanisms are then employed to reduce the irrelevant information of the representations in LID—the first being the attentive squeeze-and-excitation (SE) block for dimension-wise scaling and the second being the linear bottleneck (LBN) block that reduces the irrelevant information by nonlinear dimension reduction. These two methods are incorporated within the XSA-LID model, named AttSE-XSA and LBN-XSA respectively.&#13;\nIn the second approach, a novel LID model is proposed to hierarchically incorporate phoneme and phonotactic information without requiring phoneme annotations for training. In this model, named PHO-LID, a self-supervised phoneme segmentation task and a LID task share a convolutional neural network (CNN) module, which encodes both language identity and sequential phonemic information in the input speech to generate an intermediate sequence of “phonotactic” embeddings. These embeddings are then fed into transformer encoder layers for utterance-level LID. This architecture is called CNN-Trans.&#13;\nFinally, LID is extended for a code-switching scenario language diarization. In this work, two end-to-end neural configurations are proposed for language diarization on bilingual code-switching speech. The first, a BLSTM-E2E architecture, includes a set of stacked bidirectional LSTMs to compute embeddings and incorporates the deep clustering loss to enforce grouping of languages belonging to the same class. The second, an XSA-E2E architecture, is based on an x-vector model followed by a self-attention encoder. The former encodes frame-level features into segment-level embeddings while the latter considers all those embeddings to generate a sequence of segment-level language labels.&#13;\nAll proposed approaches are evaluated on standard datasets including NIST LRE 2017, OLR, SEAME, and WSTCSMC 2020. Compared with the baseline systems, the proposed approaches exhibit significant performance improvement on their corresponding language identification and diarization tasks.","['https://openalex.org/W6675575806', 'https://openalex.org/W3008434450', 'https://openalex.org/W3207041304', 'https://openalex.org/W2189248916', 'https://openalex.org/W6752688541', 'https://openalex.org/W6682600690', 'https://openalex.org/W2748488820', 'https://openalex.org/W2890964092', 'https://openalex.org/W2888924726', 'https://openalex.org/W2805603126', 'https://openalex.org/W2972433488', 'https://openalex.org/W2032036568', 'https://openalex.org/W1969579838', 'https://openalex.org/W1983072039', 'https://openalex.org/W2172287020', 'https://openalex.org/W2148154194', 'https://openalex.org/W2915073473', 'https://openalex.org/W80991132', 'https://openalex.org/W2939710050', 'https://openalex.org/W2049032416', 'https://openalex.org/W6638523607', 'https://openalex.org/W3197380209', 'https://openalex.org/W6739901393', 'https://openalex.org/W3036601975', 'https://openalex.org/W2796898542', 'https://openalex.org/W2395750323', 'https://openalex.org/W2039057510', 'https://openalex.org/W2078169166', 'https://openalex.org/W6725739302', 'https://openalex.org/W2938358845', 'https://openalex.org/W2972392008', 'https://openalex.org/W6768721803', 'https://openalex.org/W6743731764', 'https://openalex.org/W3024869864', 'https://openalex.org/W2408175559', 'https://openalex.org/W2515090196', 'https://openalex.org/W2795465146', 'https://openalex.org/W2612520659', 'https://openalex.org/W3010983946', 'https://openalex.org/W6785366589', 'https://openalex.org/W156922528', 'https://openalex.org/W2118180989', 'https://openalex.org/W1816313093', 'https://openalex.org/W2134815611', 'https://openalex.org/W2045709117', 'https://openalex.org/W50201798', 'https://openalex.org/W2147500075', 'https://openalex.org/W6675526594', 'https://openalex.org/W3112616666', 'https://openalex.org/W3037057938', 'https://openalex.org/W3180180466', 'https://openalex.org/W2153181479', 'https://openalex.org/W2185814970', 'https://openalex.org/W3169320628', 'https://openalex.org/W87424008', 'https://openalex.org/W2895419391', 'https://openalex.org/W2638067502', 'https://openalex.org/W6744765397', 'https://openalex.org/W6755054534', 'https://openalex.org/W2221409856', 'https://openalex.org/W97072897', 'https://openalex.org/W2166637769', 'https://openalex.org/W6759267758', 'https://openalex.org/W6740384931', 'https://openalex.org/W6751871280', 'https://openalex.org/W197277712', 'https://openalex.org/W2898943523', 'https://openalex.org/W151003953', 'https://openalex.org/W2747270953', 'https://openalex.org/W2889341949', 'https://openalex.org/W2936791331', 'https://openalex.org/W3024583412', 'https://openalex.org/W6756319913', 'https://openalex.org/W3125815078', 'https://openalex.org/W3163361649', 'https://openalex.org/W6780226713', 'https://openalex.org/W6631362777', 'https://openalex.org/W3025165719', 'https://openalex.org/W3198614662', 'https://openalex.org/W3157923770', 'https://openalex.org/W6795952400', 'https://openalex.org/W6687483927', 'https://openalex.org/W2796438033', 'https://openalex.org/W3213029956', 'https://openalex.org/W2794506738', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W2292087804', 'https://openalex.org/W6687152286', 'https://openalex.org/W2979826702', 'https://openalex.org/W1836465849', 'https://openalex.org/W3209984917', 'https://openalex.org/W3043897409', 'https://openalex.org/W2152790380', 'https://openalex.org/W2842511635', 'https://openalex.org/W2038101708', 'https://openalex.org/W1965819578', 'https://openalex.org/W4214556932', 'https://openalex.org/W6754192381', 'https://openalex.org/W2972956699', 'https://openalex.org/W2900022717', 'https://openalex.org/W2973072991', 'https://openalex.org/W2972808922', 'https://openalex.org/W6756707011', 'https://openalex.org/W2079735306', 'https://openalex.org/W1522301498', 'https://openalex.org/W2193413348', 'https://openalex.org/W3123196343', 'https://openalex.org/W3036313961', 'https://openalex.org/W1828163288', 'https://openalex.org/W2766219058', 'https://openalex.org/W1821462560', 'https://openalex.org/W2194775991', 'https://openalex.org/W2978904488', 'https://openalex.org/W2403547305', 'https://openalex.org/W4385245566', 'https://openalex.org/W2896538040', 'https://openalex.org/W2104457544', 'https://openalex.org/W4287173589', 'https://openalex.org/W3197580070', 'https://openalex.org/W2150769028', 'https://openalex.org/W2889381673', 'https://openalex.org/W48568691', 'https://openalex.org/W2408021097', 'https://openalex.org/W3010814938', 'https://openalex.org/W2963307329', 'https://openalex.org/W2498338956', 'https://openalex.org/W4287553982', 'https://openalex.org/W2963063081', 'https://openalex.org/W2962788262', 'https://openalex.org/W4225325590', 'https://openalex.org/W1524333225', 'https://openalex.org/W2191779130', 'https://openalex.org/W3097306574', 'https://openalex.org/W2963077989', 'https://openalex.org/W2963446712', 'https://openalex.org/W2102356979', 'https://openalex.org/W2962760690', 'https://openalex.org/W3198429080', 'https://openalex.org/W2973049979', 'https://openalex.org/W4361745846', 'https://openalex.org/W3008357631', 'https://openalex.org/W2962784628', 'https://openalex.org/W112217986', 'https://openalex.org/W2963371159', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963470929', 'https://openalex.org/W4297808394', 'https://openalex.org/W2525778437', 'https://openalex.org/W4309845474', 'https://openalex.org/W3096656254', 'https://openalex.org/W3095402318', 'https://openalex.org/W2902094805', 'https://openalex.org/W4234330420', 'https://openalex.org/W4394666973', 'https://openalex.org/W2963032538', 'https://openalex.org/W2807627734', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963163009']",2023-01-01
https://openalex.org/W3126050678,,Fusing Wav2vec2.0 and BERT into End-to-end Model for Low-resource Speech Recognition,"Self-supervised acoustic pre-training has achieved impressive results on low-resource speech recognition tasks. It indicates that the pretrain-and-finetune paradigm is a promising direction. In this work, we propose an end-to-end model for the low-resource speech recognition, which fuses a pre-trained audio encoder (wav2vec2.0) and a pre-trained text decoder (BERT). The two modules are connected by a linear attention mechanism without parameters. A fully connected layer is introduced for hidden mapping between speech and language modalities. Besides, we design an effective fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained decoder. Armed with this strategy, our model exhibits distinct faster convergence and better performance. Our model achieves approaching recognition performance in CALLHOME corpus (15h) as the SOTA pipeline modeling.","['https://openalex.org/W1526236009', 'https://openalex.org/W3025417467', 'https://openalex.org/W3099782249', 'https://openalex.org/W2973180718', 'https://openalex.org/W3028382961', 'https://openalex.org/W2808640845', 'https://openalex.org/W3001434439', 'https://openalex.org/W2962925243', 'https://openalex.org/W2936078256', 'https://openalex.org/W2102113734', 'https://openalex.org/W2996428491', 'https://openalex.org/W2125113755', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2803399609', 'https://openalex.org/W2767206889', 'https://openalex.org/W2750545698', 'https://openalex.org/W3113594615', 'https://openalex.org/W2988736778', 'https://openalex.org/W2888779557', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963341956', 'https://openalex.org/W3036601975', 'https://openalex.org/W3112092703', 'https://openalex.org/W3014413043', 'https://openalex.org/W2963400424', 'https://openalex.org/W3037057938', 'https://openalex.org/W2127141656', 'https://openalex.org/W3016167541', 'https://openalex.org/W2953190524', 'https://openalex.org/W2952509486', 'https://openalex.org/W2939111082']",2021-01-17
https://openalex.org/W3136649840,https://doi.org/10.48550/arxiv.2103.11011,Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of Cardiac Signals,"Cardiac signals, such as the electrocardiogram, convey a significant amount of information about the health status of a patient which is typically summarized by a clinician in the form of a clinical report, a cumbersome process that is prone to errors. To streamline this routine process, we propose a deep neural network capable of captioning cardiac signals; it receives a cardiac signal as input and generates a clinical report as output. We extend this further to generate multilingual reports. To that end, we create and make publicly available a multilingual clinical report dataset. In the absence of sufficient labelled data, deep neural networks can benefit from a warm-start, or pre-training, procedure in which parameters are first learned in an arbitrary task. We propose such a task in the form of discriminative multilingual pre-training where tokens from clinical reports are randomly replaced with those from other languages and the network is tasked with predicting the language of all tokens. We show that our method performs on par with state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE, while simultaneously generating diverse and plausible clinical reports. We also demonstrate that multilingual models can outperform their monolingual counterparts, informally terming this beneficial phenomenon as the blessing of multilinguality.","['https://openalex.org/W2971310675', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970049541', 'https://openalex.org/W2932592714', 'https://openalex.org/W3013838212', 'https://openalex.org/W2963456134', 'https://openalex.org/W2963341956', 'https://openalex.org/W2101105183', 'https://openalex.org/W2017086716', 'https://openalex.org/W2983040767', 'https://openalex.org/W3021394814', 'https://openalex.org/W3037854022', 'https://openalex.org/W67279148', 'https://openalex.org/W3016672431', 'https://openalex.org/W2929309437', 'https://openalex.org/W2963967185', 'https://openalex.org/W2970971581', 'https://openalex.org/W3091546937', 'https://openalex.org/W3017454464', 'https://openalex.org/W2937845937', 'https://openalex.org/W2123301721', 'https://openalex.org/W2560730294', 'https://openalex.org/W2911489562', 'https://openalex.org/W1514535095', 'https://openalex.org/W3040454670', 'https://openalex.org/W3013571468', 'https://openalex.org/W2154652894', 'https://openalex.org/W2971863715', 'https://openalex.org/W3027572331', 'https://openalex.org/W3095349973', 'https://openalex.org/W2885851583', 'https://openalex.org/W3034727271', 'https://openalex.org/W2984008963', 'https://openalex.org/W2914397182', 'https://openalex.org/W3015962271', 'https://openalex.org/W3107826490', 'https://openalex.org/W3016252650', 'https://openalex.org/W2970608575', 'https://openalex.org/W2953109491', 'https://openalex.org/W2981851019', 'https://openalex.org/W2958953787', 'https://openalex.org/W2075784151', 'https://openalex.org/W2745461083', 'https://openalex.org/W2041806027', 'https://openalex.org/W2942105358', 'https://openalex.org/W3037057938']",2021-03-19
https://openalex.org/W3176481516,https://doi.org/10.1109/jstsp.2022.3195430,Pretext Tasks selection for multitask self-supervised speech\n representation learning,"Through solving pretext tasks, self-supervised learning leverages unlabeled\ndata to extract useful latent representations replacing traditional input\nfeatures in the downstream task. In audio/speech signal processing, a wide\nrange of features where engineered through decades of research efforts. As it\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\nto be a particularly relevant pretext task, leading to useful self-supervised\nrepresentations which prove to be effective for downstream tasks. However,\nmethods and common practices for combining such pretext tasks for better\nperformance on the downstream task have not been explored and understood\nproperly. In fact, the process relies almost exclusively on a computationally\nheavy experimental procedure, which becomes intractable with the increase of\nthe number of pretext tasks. This paper introduces a method to select a group\nof pretext tasks among a set of candidates. The method we propose estimates\ncalibrated weights for the partial losses corresponding to the considered\npretext tasks during the self-supervised training process. The experiments\nconducted on automatic speech recognition, speaker and emotion recognition\nvalidate our approach, as the groups selected and weighted with our method\nperform better than classic baselines, thus facilitating the selection and\ncombination of relevant pseudo-labels for self-supervised representation\nlearning.\n","['https://openalex.org/W2758739367', 'https://openalex.org/W2963123301', 'https://openalex.org/W3023371261', 'https://openalex.org/W2890964092', 'https://openalex.org/W3096485810', 'https://openalex.org/W3042860357', 'https://openalex.org/W3103005696', 'https://openalex.org/W3157923770', 'https://openalex.org/W2755891984', 'https://openalex.org/W3034978746', 'https://openalex.org/W3040260790', 'https://openalex.org/W2990704537', 'https://openalex.org/W3189296823', 'https://openalex.org/W3035060554', 'https://openalex.org/W3093579165', 'https://openalex.org/W2242632974', 'https://openalex.org/W2407151108', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096587983', 'https://openalex.org/W2973157397', 'https://openalex.org/W2926827382', 'https://openalex.org/W3046882683', 'https://openalex.org/W3099782249', 'https://openalex.org/W1589137271', 'https://openalex.org/W2290689761', 'https://openalex.org/W2146334809', 'https://openalex.org/W3120044914', 'https://openalex.org/W3037057938', 'https://openalex.org/W2138019504', 'https://openalex.org/W2994633389', 'https://openalex.org/W2982223350', 'https://openalex.org/W3213380193', 'https://openalex.org/W2321533354', 'https://openalex.org/W3160799772', 'https://openalex.org/W3131755153', 'https://openalex.org/W2889313720', 'https://openalex.org/W1796128977', 'https://openalex.org/W2940256401', 'https://openalex.org/W2143426320', 'https://openalex.org/W2982303846', 'https://openalex.org/W2996905456', 'https://openalex.org/W2971155163', 'https://openalex.org/W2957946919', 'https://openalex.org/W2110709236', 'https://openalex.org/W343636949', 'https://openalex.org/W3156691089', 'https://openalex.org/W3015213852', 'https://openalex.org/W2112552549', 'https://openalex.org/W2085662862', 'https://openalex.org/W2842511635', 'https://openalex.org/W2090042521', 'https://openalex.org/W3210177631', 'https://openalex.org/W2787672622', 'https://openalex.org/W3090196146', 'https://openalex.org/W2962835968', 'https://openalex.org/W2750912449', 'https://openalex.org/W3093563057', 'https://openalex.org/W3096196861', 'https://openalex.org/W3036982689', 'https://openalex.org/W2100643000', 'https://openalex.org/W3036601975', 'https://openalex.org/W3198275944', 'https://openalex.org/W2402146185', 'https://openalex.org/W2995480165', 'https://openalex.org/W108866686', 'https://openalex.org/W3030437843', 'https://openalex.org/W2996428491', 'https://openalex.org/W3197411683', 'https://openalex.org/W2951585248', 'https://openalex.org/W2899260162', 'https://openalex.org/W2798512429', 'https://openalex.org/W2747874407', 'https://openalex.org/W3096715616', 'https://openalex.org/W2395899413', 'https://openalex.org/W2154462399', 'https://openalex.org/W3033038061', 'https://openalex.org/W1542637018', 'https://openalex.org/W2963115079', 'https://openalex.org/W3160719641', 'https://openalex.org/W2150772522', 'https://openalex.org/W3106428938', 'https://openalex.org/W2154053567', 'https://openalex.org/W3034781633', 'https://openalex.org/W2962742544']",2021-07-01
https://openalex.org/W3177457454,https://doi.org/10.18653/v1/2021.iwslt-1.21,IMS’ Systems for the IWSLT 2021 Low-Resource Speech Translation Task,"This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9.","['https://openalex.org/W2933138175', 'https://openalex.org/W3101860695', 'https://openalex.org/W3099782249', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963250244', 'https://openalex.org/W3025165719', 'https://openalex.org/W2960374072', 'https://openalex.org/W2936774411', 'https://openalex.org/W3100806282', 'https://openalex.org/W630532510', 'https://openalex.org/W3105425516', 'https://openalex.org/W3119308075', 'https://openalex.org/W1494198834', 'https://openalex.org/W3092614733', 'https://openalex.org/W2949303037', 'https://openalex.org/W3143186397', 'https://openalex.org/W2401271873', 'https://openalex.org/W3094800360', 'https://openalex.org/W2963403868', 'https://openalex.org/W3037057938', 'https://openalex.org/W2095705004', 'https://openalex.org/W2127141656', 'https://openalex.org/W2526425061', 'https://openalex.org/W2962784628']",2021-01-01
https://openalex.org/W3179278524,https://doi.org/10.48550/arxiv.2107.06959,FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task,"In this paper, we describe our end-to-end multilingual speech translation system submitted to the IWSLT 2021 evaluation campaign on the Multilingual Speech Translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin. In some translation directions, our speech translation results evaluated on the public Multilingual TEDx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input.","['https://openalex.org/W3112092703', 'https://openalex.org/W3099782249', 'https://openalex.org/W3093871477', 'https://openalex.org/W3162037819', 'https://openalex.org/W3107826490', 'https://openalex.org/W3037057938', 'https://openalex.org/W2962735107', 'https://openalex.org/W3015698636', 'https://openalex.org/W3119308075', 'https://openalex.org/W3008549139', 'https://openalex.org/W3127012371', 'https://openalex.org/W630532510', 'https://openalex.org/W3032433061', 'https://openalex.org/W3176711365', 'https://openalex.org/W3032816972', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963250244']",2021-07-14
https://openalex.org/W3204917385,https://doi.org/10.48550/arxiv.2109.12306,Topic Model Robustness to Automatic Speech Recognition Errors in Podcast Transcripts,"For a multilingual podcast streaming service, it is critical to be able to deliver relevant content to all users independent of language. Podcast content relevance is conventionally determined using various metadata sources. However, with the increasing quality of speech recognition in many languages, utilizing automatic transcriptions to provide better content recommendations becomes possible. In this work, we explore the robustness of a Latent Dirichlet Allocation topic model when applied to transcripts created by an automatic speech recognition engine. Specifically, we explore how increasing transcription noise influences topics obtained from transcriptions in Danish; a low resource language. First, we observe a baseline of cosine similarity scores between topic embeddings from automatic transcriptions and the descriptions of the podcasts written by the podcast creators. We then observe how the cosine similarities decrease as transcription noise increases and conclude that even when automatic speech recognition transcripts are erroneous, it is still possible to obtain high-quality topic embeddings from the transcriptions.","['https://openalex.org/W2404506086', 'https://openalex.org/W3136258522', 'https://openalex.org/W3093579165', 'https://openalex.org/W1494198834', 'https://openalex.org/W2147946282', 'https://openalex.org/W2137779536', 'https://openalex.org/W168564468', 'https://openalex.org/W2995181338', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W2161152810', 'https://openalex.org/W2949464201', 'https://openalex.org/W1612003148', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2171343266', 'https://openalex.org/W1902027874', 'https://openalex.org/W1880262756', 'https://openalex.org/W2964228074', 'https://openalex.org/W2160949192', 'https://openalex.org/W2106480521', 'https://openalex.org/W3026041220', 'https://openalex.org/W2995929068', 'https://openalex.org/W2144100511', 'https://openalex.org/W2147152072', 'https://openalex.org/W2767899794', 'https://openalex.org/W2112050062', 'https://openalex.org/W1982474113', 'https://openalex.org/W3169061850', 'https://openalex.org/W2906777204', 'https://openalex.org/W2127141656', 'https://openalex.org/W1710082047', 'https://openalex.org/W3036601975', 'https://openalex.org/W2130339025']",2021-09-25
https://openalex.org/W3211231337,https://doi.org/10.48550/arxiv.2111.01326,Cross-lingual Transfer for Speech Processing using Acoustic Language Similarity,"Speech processing systems currently do not support the vast majority of languages, in part due to the lack of data in low-resource languages. Cross-lingual transfer offers a compelling way to help bridge this digital divide by incorporating high-resource data into low-resource systems. Current cross-lingual algorithms have shown success in text-based tasks and speech-related tasks over some low-resource languages. However, scaling up speech systems to support hundreds of low-resource languages remains unsolved. To help bridge this gap, we propose a language similarity approach that can efficiently identify acoustic cross-lingual transfer pairs across hundreds of languages. We demonstrate the effectiveness of our approach in language family classification, speech recognition, and speech synthesis tasks.","['https://openalex.org/W2963609956', 'https://openalex.org/W3153849864', 'https://openalex.org/W3025165719', 'https://openalex.org/W3037057938', 'https://openalex.org/W2524541983', 'https://openalex.org/W3096032230', 'https://openalex.org/W2952289666', 'https://openalex.org/W2903739847', 'https://openalex.org/W2964309797', 'https://openalex.org/W3198694222', 'https://openalex.org/W3016160783', 'https://openalex.org/W2396466159', 'https://openalex.org/W2972818416', 'https://openalex.org/W2998284473', 'https://openalex.org/W3167335398', 'https://openalex.org/W2058094241', 'https://openalex.org/W202879582', 'https://openalex.org/W3030437843', 'https://openalex.org/W3035390927', 'https://openalex.org/W256536324', 'https://openalex.org/W803770162', 'https://openalex.org/W3142087749', 'https://openalex.org/W2408021097', 'https://openalex.org/W2963403868', 'https://openalex.org/W2396312253', 'https://openalex.org/W3109527089', 'https://openalex.org/W3206808041', 'https://openalex.org/W2786835190', 'https://openalex.org/W2972581290', 'https://openalex.org/W61749939', 'https://openalex.org/W2107860279', 'https://openalex.org/W2895676041', 'https://openalex.org/W3096215352', 'https://openalex.org/W3101000907', 'https://openalex.org/W2962780374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2936774411', 'https://openalex.org/W2937197076', 'https://openalex.org/W2273653065', 'https://openalex.org/W273093436', 'https://openalex.org/W3198958373', 'https://openalex.org/W2716988359', 'https://openalex.org/W3095410713', 'https://openalex.org/W2726515241', 'https://openalex.org/W2998653236', 'https://openalex.org/W1566883001', 'https://openalex.org/W2963292011', 'https://openalex.org/W2963431393', 'https://openalex.org/W2739967986', 'https://openalex.org/W2807627734', 'https://openalex.org/W3100345210', 'https://openalex.org/W2972608701']",2021-11-02
https://openalex.org/W3211624279,https://doi.org/10.48550/arxiv.2111.04823,Cascaded Multilingual Audio-Visual Learning from Videos,"In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.","['https://openalex.org/W3161204797', 'https://openalex.org/W2953114965', 'https://openalex.org/W3143035657', 'https://openalex.org/W2988907666', 'https://openalex.org/W2127982613', 'https://openalex.org/W2973132572', 'https://openalex.org/W2586148577', 'https://openalex.org/W385555557', 'https://openalex.org/W3037057938', 'https://openalex.org/W2920166246', 'https://openalex.org/W3095881291', 'https://openalex.org/W3100177202', 'https://openalex.org/W2194775991', 'https://openalex.org/W2972073579', 'https://openalex.org/W3157861865', 'https://openalex.org/W2134670479', 'https://openalex.org/W2025198378', 'https://openalex.org/W2962978519', 'https://openalex.org/W2962862718', 'https://openalex.org/W2799316004', 'https://openalex.org/W3015300171', 'https://openalex.org/W2962753610', 'https://openalex.org/W2965147078', 'https://openalex.org/W2556930864', 'https://openalex.org/W2984008963', 'https://openalex.org/W3158986867', 'https://openalex.org/W1994606281', 'https://openalex.org/W2963330681', 'https://openalex.org/W2963902314', 'https://openalex.org/W2968848930', 'https://openalex.org/W3034875620', 'https://openalex.org/W2964099072', 'https://openalex.org/W3096372900', 'https://openalex.org/W2894835365', 'https://openalex.org/W3161945002', 'https://openalex.org/W2972345028', 'https://openalex.org/W3042657922', 'https://openalex.org/W2800448574', 'https://openalex.org/W2784025607', 'https://openalex.org/W2995680346', 'https://openalex.org/W2962934715', 'https://openalex.org/W2971709506', 'https://openalex.org/W3035276082', 'https://openalex.org/W3170972077']",2021-11-08
https://openalex.org/W3213175848,https://doi.org/10.48550/arxiv.2109.09161,Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition,"Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the transfer from speech to text. However, how to solve the representation discrepancy of speech and text is unexplored, which hinders the utilization of acoustic and linguistic information. Moreover, previous works simply replace the embedding layer of the pre-trained language model with the acoustic features, which may cause the catastrophic forgetting problem. In this work, we introduce Wav-BERT, a cooperative acoustic and linguistic representation learning method to fuse and utilize the contextual information of speech and text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a language model (BERT) into an end-to-end trainable framework. A Representation Aggregation Module is designed to aggregate acoustic and linguistic representation, and an Embedding Attention Module is introduced to incorporate acoustic information into BERT, which can effectively facilitate the cooperation of two pre-trained models and thus boost the representation learning. Extensive experiments show that our Wav-BERT significantly outperforms the existing approaches and achieves state-of-the-art performance on low-resource speech recognition.","['https://openalex.org/W3155427814', 'https://openalex.org/W2193413348', 'https://openalex.org/W2962739339', 'https://openalex.org/W2842511635', 'https://openalex.org/W3099782249', 'https://openalex.org/W3094002217', 'https://openalex.org/W2964309797', 'https://openalex.org/W3037057938', 'https://openalex.org/W2940322076', 'https://openalex.org/W2939111082', 'https://openalex.org/W3088059392', 'https://openalex.org/W2766219058', 'https://openalex.org/W3025165719', 'https://openalex.org/W3141961557', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990391581', 'https://openalex.org/W3173563729', 'https://openalex.org/W3015585292', 'https://openalex.org/W2113839990', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963341956', 'https://openalex.org/W2786835190', 'https://openalex.org/W2604763608', 'https://openalex.org/W2134800885', 'https://openalex.org/W3110524561', 'https://openalex.org/W3014413043', 'https://openalex.org/W2963303951', 'https://openalex.org/W2327501763', 'https://openalex.org/W2795900505', 'https://openalex.org/W3100311862', 'https://openalex.org/W2976556660', 'https://openalex.org/W2933138175', 'https://openalex.org/W2991509857', 'https://openalex.org/W2972818416', 'https://openalex.org/W2963242190', 'https://openalex.org/W2292087804', 'https://openalex.org/W3163132306', 'https://openalex.org/W2973049979']",2021-09-19
https://openalex.org/W4313888176,https://doi.org/10.36227/techrxiv.21792920,Sample-Efficient Unsupervised Domain Adaptation of Speech Recognition Systems: A case study for Modern Greek,"&lt;p&gt;Modern speech recognition systems exhibits rapid performance degradation under domain shift. This issue is especially prevalent in data-scarce settings, such as low-resource languages, where diversity of training data is limited.&lt;/p&gt; &lt;p&gt;In this work we propose M2DS2, a simple and sample-efficient finetuning strategy for large pretrained speech models, based on mixed source and target domain self-supervision. We find that including source domain self-supervision stabilizes training and avoids mode collapse of the latent representations. For evaluation, we collect HParl, a 120 hour speech corpus for Greek, consisting of plenary sessions in the Greek Parliament. We merge HParl with two popular Greek corpora to create GREC-MD, a test-bed for multi-domain evaluation of Greek ASR systems. In our experiments we find that, while other Unsupervised Domain Adaptation baselines fail in this resource-constrained environment, M2DS2 yields significant improvements for cross-domain adaptation, even when a only a few hours of in-domain audio are available. When we relax the problem in a weakly supervised setting, we find that independent adaptation for audio using M2DS2 and language using simple LM augmentation techniques is particularly effective, yielding word error rates comparable to the fully supervised baselines.&lt;/p&gt;","['https://openalex.org/W2159291411', 'https://openalex.org/W1731081199', 'https://openalex.org/W3112702554', 'https://openalex.org/W2749784707', 'https://openalex.org/W2911629330', 'https://openalex.org/W2510867321', 'https://openalex.org/W2796339975', 'https://openalex.org/W2802497966', 'https://openalex.org/W3201468377', 'https://openalex.org/W3115169127', 'https://openalex.org/W4310348293', 'https://openalex.org/W2123867168', 'https://openalex.org/W567546468', 'https://openalex.org/W2404620314', 'https://openalex.org/W2015633636', 'https://openalex.org/W2151484683', 'https://openalex.org/W2398723458', 'https://openalex.org/W2327501763', 'https://openalex.org/W1828163288', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W2884636860', 'https://openalex.org/W3202037040', 'https://openalex.org/W3144173820', 'https://openalex.org/W3109234242', 'https://openalex.org/W3099701846', 'https://openalex.org/W3025165719', 'https://openalex.org/W2293634267', 'https://openalex.org/W2680270903', 'https://openalex.org/W2288645994', 'https://openalex.org/W3196855257', 'https://openalex.org/W2901837390', 'https://openalex.org/W2584667682', 'https://openalex.org/W2587088898', 'https://openalex.org/W2402146185', 'https://openalex.org/W2899135636', 'https://openalex.org/W854541894', 'https://openalex.org/W3111374309', 'https://openalex.org/W2804758356', 'https://openalex.org/W4283313740', 'https://openalex.org/W3207547029', 'https://openalex.org/W4283827269', 'https://openalex.org/W3155145134', 'https://openalex.org/W2111316763', 'https://openalex.org/W2101210369', 'https://openalex.org/W2088622183', 'https://openalex.org/W4306176218', 'https://openalex.org/W3015522062', 'https://openalex.org/W3026041220', 'https://openalex.org/W3093579165', 'https://openalex.org/W2936774411', 'https://openalex.org/W1882958252', 'https://openalex.org/W2024490156', 'https://openalex.org/W47839664', 'https://openalex.org/W2896457183', 'https://openalex.org/W2124509324', 'https://openalex.org/W1494198834', 'https://openalex.org/W2306118375', 'https://openalex.org/W2073101704', 'https://openalex.org/W2786234940', 'https://openalex.org/W22934434', 'https://openalex.org/W2087064593', 'https://openalex.org/W2995929068', 'https://openalex.org/W2908510526', 'https://openalex.org/W2989539713', 'https://openalex.org/W2293498086', 'https://openalex.org/W2134800885', 'https://openalex.org/W3035299099', 'https://openalex.org/W2962684181', 'https://openalex.org/W3097777922', 'https://openalex.org/W3198771897', 'https://openalex.org/W3162061711', 'https://openalex.org/W3198429080', 'https://openalex.org/W2953127297', 'https://openalex.org/W3030437843', 'https://openalex.org/W2404126548', 'https://openalex.org/W3162649911', 'https://openalex.org/W4226246059', 'https://openalex.org/W4375850708', 'https://openalex.org/W3163464943', 'https://openalex.org/W3166720688', 'https://openalex.org/W2187089797', 'https://openalex.org/W2962894366', 'https://openalex.org/W4290711009', 'https://openalex.org/W4302283057', 'https://openalex.org/W2962995362', 'https://openalex.org/W1524333225', 'https://openalex.org/W3034238904', 'https://openalex.org/W2962760690']",2023-01-09
https://openalex.org/W4377088036,https://doi.org/10.36227/techrxiv.22821992,Speech Emotion Recognition in Italian Using Wav2Vec 2.0 and the Novel Crowdsourced Emotional Speech Corpus Emozionalmente,"&lt;p&gt;Speech emotion recognition (SER) relies on speech corpora to collect emotional voices for analysis. However, emo- tions may vary by culture and language, and resources in Italian are scarce. To address this gap, we launched a crowdsourcing campaign and obtained Emozionalmente, a corpus of 6902 samples produced by 431 non-professional Italian speakers verbalizing 18 sentences expressing the Big Six emotions and neutrality. We conducted a subjective validation of Emozionalmente by asking 829 humans to guess the emotion expressed in the audio clips, achieving an overall accuracy of 66%. Additionally, we fine- tuned the deep learning wav2vec 2.0 model on Emozionalmente and achieved good performance, with an accuracy of around 81- 83%. In this paper, we describe the design choices, a descriptive analysis of the corpus, and the methodology and results of the behavioral and computational studies conducted on the dataset. Our work provides an alternative and extensive resource for linguistic and speech-processing research on the Italian language.&lt;/p&gt;","['https://openalex.org/W2803098682', 'https://openalex.org/W6641983419', 'https://openalex.org/W2074788634', 'https://openalex.org/W2066391335', 'https://openalex.org/W2048765566', 'https://openalex.org/W1966210085', 'https://openalex.org/W6674598386', 'https://openalex.org/W6685070725', 'https://openalex.org/W2159190230', 'https://openalex.org/W589698015', 'https://openalex.org/W2997399314', 'https://openalex.org/W2102953093', 'https://openalex.org/W6602559115', 'https://openalex.org/W6607193717', 'https://openalex.org/W2401942008', 'https://openalex.org/W126844829', 'https://openalex.org/W4390926572', 'https://openalex.org/W2146334809', 'https://openalex.org/W1825415099', 'https://openalex.org/W6712950603', 'https://openalex.org/W2113396990', 'https://openalex.org/W2525246137', 'https://openalex.org/W6682250703', 'https://openalex.org/W2128653836', 'https://openalex.org/W3019556205', 'https://openalex.org/W2345972626', 'https://openalex.org/W2972640480', 'https://openalex.org/W3006475563', 'https://openalex.org/W2978310905', 'https://openalex.org/W2509065397', 'https://openalex.org/W2648194195', 'https://openalex.org/W2119534679', 'https://openalex.org/W59795698', 'https://openalex.org/W2098507061', 'https://openalex.org/W2120945046', 'https://openalex.org/W2111926505', 'https://openalex.org/W6750280428', 'https://openalex.org/W6658280115', 'https://openalex.org/W1973378890', 'https://openalex.org/W1940107713', 'https://openalex.org/W2118789253', 'https://openalex.org/W1988111866', 'https://openalex.org/W6657213212', 'https://openalex.org/W2239141610', 'https://openalex.org/W2160815625', 'https://openalex.org/W2107789863', 'https://openalex.org/W1819710477', 'https://openalex.org/W3082977586', 'https://openalex.org/W6800809764', 'https://openalex.org/W3144073776', 'https://openalex.org/W3206192140', 'https://openalex.org/W3035688366', 'https://openalex.org/W3162343586', 'https://openalex.org/W2114212188', 'https://openalex.org/W2753178813', 'https://openalex.org/W2043152858', 'https://openalex.org/W2169295472', 'https://openalex.org/W2165113952', 'https://openalex.org/W2756005953', 'https://openalex.org/W2123119128', 'https://openalex.org/W2023937851', 'https://openalex.org/W6635996524', 'https://openalex.org/W49331665', 'https://openalex.org/W2625297138', 'https://openalex.org/W2542587589', 'https://openalex.org/W2092489135', 'https://openalex.org/W283543405', 'https://openalex.org/W2110052520', 'https://openalex.org/W2119417805', 'https://openalex.org/W2144264893', 'https://openalex.org/W1548561233', 'https://openalex.org/W2123864036', 'https://openalex.org/W2140801466', 'https://openalex.org/W2024186865', 'https://openalex.org/W3037057938', 'https://openalex.org/W3112616666', 'https://openalex.org/W3198712976', 'https://openalex.org/W1976725440', 'https://openalex.org/W4239447739', 'https://openalex.org/W4297997662', 'https://openalex.org/W62174847', 'https://openalex.org/W4245627386', 'https://openalex.org/W175750906', 'https://openalex.org/W4212951177', 'https://openalex.org/W1495679096', 'https://openalex.org/W2082445899', 'https://openalex.org/W1598253959', 'https://openalex.org/W2796430037', 'https://openalex.org/W2027545148', 'https://openalex.org/W3197642003', 'https://openalex.org/W4245744384', 'https://openalex.org/W1506682350', 'https://openalex.org/W3122100772', 'https://openalex.org/W3197156295', 'https://openalex.org/W2099441831', 'https://openalex.org/W2724582867', 'https://openalex.org/W3015707499', 'https://openalex.org/W2168730540', 'https://openalex.org/W4375869379', 'https://openalex.org/W2032254851', 'https://openalex.org/W3198429080', 'https://openalex.org/W4213192516', 'https://openalex.org/W3003735678', 'https://openalex.org/W2400135701', 'https://openalex.org/W2149535104', 'https://openalex.org/W3098357269', 'https://openalex.org/W4301204483', 'https://openalex.org/W2183892644', 'https://openalex.org/W4287553982', 'https://openalex.org/W1966797434']",2023-05-19
https://openalex.org/W4386712571,https://doi.org/10.21437/spsc.2023-5,Federated Representation Learning for Automatic Speech Recognition,"Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data.Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations.In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints.We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD.We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training.We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.","['https://openalex.org/W2541884796', 'https://openalex.org/W6771536673', 'https://openalex.org/W2842511635', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W3209984917', 'https://openalex.org/W4281492411', 'https://openalex.org/W2995181338', 'https://openalex.org/W1828163288', 'https://openalex.org/W3097714942', 'https://openalex.org/W3158424777', 'https://openalex.org/W4286224990', 'https://openalex.org/W4372347050', 'https://openalex.org/W6755691404', 'https://openalex.org/W4307937515', 'https://openalex.org/W3101275766', 'https://openalex.org/W4221161839', 'https://openalex.org/W3047081942', 'https://openalex.org/W4225317525', 'https://openalex.org/W4226041925', 'https://openalex.org/W2995929068', 'https://openalex.org/W6802287288', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W2885185669', 'https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W3203098807', 'https://openalex.org/W2152790380', 'https://openalex.org/W2963250244', 'https://openalex.org/W3025567392', 'https://openalex.org/W3095866088', 'https://openalex.org/W4318619660', 'https://openalex.org/W4303495611', 'https://openalex.org/W4297808394', 'https://openalex.org/W3198429080', 'https://openalex.org/W3030437843', 'https://openalex.org/W4296070200', 'https://openalex.org/W2896422817', 'https://openalex.org/W4294106961', 'https://openalex.org/W4372268380', 'https://openalex.org/W3203625365']",2023-08-19
https://openalex.org/W4387942868,https://doi.org/10.14209/sbrt.2023.1570916069,Bilingual ASR model with language identification for Brazilian Portuguese and South-American Spanish,"Creating accurate and reliable low-resource automatic speech recognition (ASR) models remains challenging due to limited curated data.This work proposes a bilingual ASR model for Brazilian Portuguese and Latin-American Spanish implemented with the Wav2Vec2.0architecture and trained on multiple speech datasets.It combines Language Identification and Speech Recognition, employing a joint feature encoder and task-specific context encoders.Evaluation in the Multilingual Librispeech dataset demonstrates promising results, with an average accuracy of 75.98% for language identification and a competitive Word Error Rate of 30.45% in a bilingual setting, comparable to the Whisper model.","['https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W2091746061', 'https://openalex.org/W2995929068', 'https://openalex.org/W3095410713', 'https://openalex.org/W2992847832', 'https://openalex.org/W2997787503', 'https://openalex.org/W2888922844', 'https://openalex.org/W2805087519', 'https://openalex.org/W2894835365', 'https://openalex.org/W3040454670', 'https://openalex.org/W3031533975', 'https://openalex.org/W3042007685', 'https://openalex.org/W2988736778', 'https://openalex.org/W3036601975', 'https://openalex.org/W3112616666', 'https://openalex.org/W4221161148', 'https://openalex.org/W3144073776', 'https://openalex.org/W2139851371', 'https://openalex.org/W2154740140', 'https://openalex.org/W3028785944', 'https://openalex.org/W3106807794', 'https://openalex.org/W38194800', 'https://openalex.org/W4311000453', 'https://openalex.org/W4287726212', 'https://openalex.org/W2316579313', 'https://openalex.org/W3198429080', 'https://openalex.org/W3030437843', 'https://openalex.org/W2315995013', 'https://openalex.org/W3139878283', 'https://openalex.org/W3096032230', 'https://openalex.org/W3197642003', 'https://openalex.org/W4287553982']",2023-01-01
https://openalex.org/W4392465474,https://doi.org/10.18002/10612/18582,Deep learning applied to speech processing: development of novel models and techniques,"This thesis proposes and evaluates new machine learning techniques and models for different tasks in the field of speech processing.It mainly addresses the identification of speakers, languages, and accents using several descriptor proposals based on different sound representations.In addition, it presents a new transfer learning technique based on a new descriptor, and two new architectures for deep learning models based on complementary audio representations.The new transfer learning technique is based on a descriptor we call Grad-Transfer, which is based on the model interpretability method Gradient-weighted Class Activation Mapping (Grad-CAM).Grad-CAM generates a heat map of the most relevant zones in the input data according to their influence on a given model prediction.For the development of Grad-Transfer, we experimentally demonstrate, using Birch and k-means clustering algorithms, that the heat maps generated by the Grad-CAM method are able to store part of the knowledge acquired by a deep learning speech processing model fed by spectrograms during its training process.We exploited this capability of Grad-CAM to formulate a new technique that transfers knowledge from a pre-trained model to an untrained one, through the Grad-Transfer descriptor, which is responsible for summarizing and reusing such knowledge.Several Grad-Transfer-based models were evaluated for the accent identification task using the Voice Cloning Toolkit dataset.These models include Gaussian Naive Bayes, Support Vector Machines, and Passive Aggressive classifiers.Experimental results show an increase in performance of up to 23.58% in models fed by Grad-Transfer descriptors and spectrograms compared to models fed by spectrograms alone.This demonstrates the ability of Grad-Transfer to improve the performance of speech processing models and opens the door to new implementations for similar tasks.On the other hand, new transfer learning approaches based on embedding generation models were evaluated.Embeddings are generated by machine learning models trained for a specific task on large datasets.By exploiting the knowledge already acquired, these models can be reused for new tasks where the amount of available data is small.This thesis proposes a new architecture for deep learning models, called Mel and Wave Embeddings for Human Voice Tasks (MeWEHV), capable of generating robust embeddings for speech processing.MeWEHV combines embeddings generated by a pre-En conjunto, esta tesis presenta varios avances en las áreas de identificación de hablantes, idiomas y acentos, y propone nuevas técnicas y modelos que utilizan el aprendizaje por transferencia para mejorar el rendimiento de los modelos del estado del arte evaluados.","['https://openalex.org/W4385151928', 'https://openalex.org/W4385453314', 'https://openalex.org/W2912740705', 'https://openalex.org/W2057731362', 'https://openalex.org/W2981068089', 'https://openalex.org/W2133803203', 'https://openalex.org/W2995929068', 'https://openalex.org/W2747060779', 'https://openalex.org/W2040357394', 'https://openalex.org/W2966095117', 'https://openalex.org/W6678809451', 'https://openalex.org/W3169688220', 'https://openalex.org/W3209984917', 'https://openalex.org/W3037057938', 'https://openalex.org/W1974205368', 'https://openalex.org/W3197561413', 'https://openalex.org/W3143315506', 'https://openalex.org/W6688112758', 'https://openalex.org/W6986997663', 'https://openalex.org/W2884535146', 'https://openalex.org/W3007113923', 'https://openalex.org/W3111336239', 'https://openalex.org/W3127686677', 'https://openalex.org/W2593116425', 'https://openalex.org/W3190634208', 'https://openalex.org/W2002499281', 'https://openalex.org/W179777611', 'https://openalex.org/W3049079173', 'https://openalex.org/W3169320628', 'https://openalex.org/W6743731764', 'https://openalex.org/W2101066392', 'https://openalex.org/W6605133283', 'https://openalex.org/W3133187441', 'https://openalex.org/W2762294195', 'https://openalex.org/W2515121265', 'https://openalex.org/W2995181338', 'https://openalex.org/W3164115648', 'https://openalex.org/W4200362612', 'https://openalex.org/W2108501770', 'https://openalex.org/W3205878676', 'https://openalex.org/W2994728585', 'https://openalex.org/W3012371808', 'https://openalex.org/W2102893423', 'https://openalex.org/W2889766475', 'https://openalex.org/W6779260352', 'https://openalex.org/W2803193013', 'https://openalex.org/W2928075308', 'https://openalex.org/W1579838312', 'https://openalex.org/W6670074220', 'https://openalex.org/W3145157056', 'https://openalex.org/W2883935097', 'https://openalex.org/W3211384831', 'https://openalex.org/W2919849250', 'https://openalex.org/W2726515241', 'https://openalex.org/W3033326607', 'https://openalex.org/W4206711979', 'https://openalex.org/W3128793294', 'https://openalex.org/W4226192228', 'https://openalex.org/W3191417718', 'https://openalex.org/W1494198834', 'https://openalex.org/W4283321136', 'https://openalex.org/W4398958419', 'https://openalex.org/W6713502316', 'https://openalex.org/W3095410713', 'https://openalex.org/W3199481609', 'https://openalex.org/W2117539524', 'https://openalex.org/W2347141333', 'https://openalex.org/W2980077696', 'https://openalex.org/W2962858109', 'https://openalex.org/W3099372643', 'https://openalex.org/W3130041558', 'https://openalex.org/W3006926732', 'https://openalex.org/W6759825495', 'https://openalex.org/W3014279133', 'https://openalex.org/W1980850109', 'https://openalex.org/W2290689761', 'https://openalex.org/W2890964092', 'https://openalex.org/W6846884369', 'https://openalex.org/W3094581495', 'https://openalex.org/W1600470703', 'https://openalex.org/W2133824856', 'https://openalex.org/W1966511183', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198689362', 'https://openalex.org/W6695553435', 'https://openalex.org/W4221141648', 'https://openalex.org/W3049278490', 'https://openalex.org/W2797583228', 'https://openalex.org/W2972798094', 'https://openalex.org/W3036173644', 'https://openalex.org/W3198370097', 'https://openalex.org/W2003268946', 'https://openalex.org/W3157923770', 'https://openalex.org/W4210933516', 'https://openalex.org/W2777468850', 'https://openalex.org/W3205156060', 'https://openalex.org/W3132764260', 'https://openalex.org/W6785624432', 'https://openalex.org/W6770432743', 'https://openalex.org/W2098859361', 'https://openalex.org/W1922658220', 'https://openalex.org/W4287120025', 'https://openalex.org/W4295723153', 'https://openalex.org/W3198429080', 'https://openalex.org/W3211552069', 'https://openalex.org/W2210153255', 'https://openalex.org/W2284628133', 'https://openalex.org/W2916510398', 'https://openalex.org/W3030437843', 'https://openalex.org/W3162508345', 'https://openalex.org/W4302033981', 'https://openalex.org/W2972584841', 'https://openalex.org/W2991572650', 'https://openalex.org/W124356247', 'https://openalex.org/W3012374325', 'https://openalex.org/W3094550259', 'https://openalex.org/W2404169761', 'https://openalex.org/W3197580070', 'https://openalex.org/W2078350191', 'https://openalex.org/W2963420686', 'https://openalex.org/W3036601975', 'https://openalex.org/W4319779707', 'https://openalex.org/W4319862462', 'https://openalex.org/W3041133507', 'https://openalex.org/W4379549064', 'https://openalex.org/W2963433413', 'https://openalex.org/W3100040045', 'https://openalex.org/W4301980136', 'https://openalex.org/W2934177128', 'https://openalex.org/W2963604492', 'https://openalex.org/W4297797918', 'https://openalex.org/W2125336414', 'https://openalex.org/W3160475509']",2023-11-11
https://openalex.org/W4393157347,https://doi.org/10.1609/aaai.v38i21.30366,Towards Building a Language-Independent Speech Scoring Assessment,"Automatic speech scoring is crucial in language learning, providing targeted feedback to language learners by assessing pronunciation, fluency, and other speech qualities. However, the scarcity of human-labeled data for languages beyond English poses a significant challenge in developing such systems. In this work, we propose a Language-Independent scoring approach to evaluate speech without relying on labeled data in the target language. We introduce a multilingual speech scoring system that leverages representations from the wav2vec 2.0 XLSR model and a force-alignment technique based on CTC-Segmentation to construct speech features. These features are used to train a machine learning model to predict pronunciation and fluency scores. We demonstrate the potential of our method by predicting expert ratings on a speech dataset spanning five languages - English, French, Spanish, German and Portuguese, and comparing its performance against Language-Specific models trained individually on each language, as well as a jointly-trained model on all languages. Results indicate that our approach shows promise as an initial step towards a universal language independent speech scoring.","['https://openalex.org/W2130689396', 'https://openalex.org/W3092539250', 'https://openalex.org/W2995929068', 'https://openalex.org/W4200300291', 'https://openalex.org/W3037057938', 'https://openalex.org/W1988687075', 'https://openalex.org/W2408752745', 'https://openalex.org/W2164810574', 'https://openalex.org/W103312509', 'https://openalex.org/W2786790428', 'https://openalex.org/W2091856355', 'https://openalex.org/W3043783436', 'https://openalex.org/W4283728719', 'https://openalex.org/W1931766939', 'https://openalex.org/W6659418573', 'https://openalex.org/W3011567515', 'https://openalex.org/W2972347929', 'https://openalex.org/W2295209184', 'https://openalex.org/W3204224625', 'https://openalex.org/W2979575266', 'https://openalex.org/W1979364015', 'https://openalex.org/W3198712976', 'https://openalex.org/W3030437843', 'https://openalex.org/W4299785905', 'https://openalex.org/W4322714819', 'https://openalex.org/W2963993537', 'https://openalex.org/W3198429080']",2024-03-24
https://openalex.org/W4412638664,https://doi.org/10.22214/ijraset.2025.73288,Multilingual Translation for Speech and Text using Whisper AI: A Deep Learning Approach,"In an increasingly interconnected world, the ability to accurately translate between multiple languages, both written and spoken, is essential for global communication. Traditional machine translation and speech recognition systems often operate as separate pipelines, leading to increased complexity and reduced efficiency, especially when dealing with low-resource languages or noisy audio environments. This research presents a comprehensive study of Whisper AI, a multilingual, multitask model developed by OpenAI for speech recognition and translation. Leveraging a transformer-based encoder-decoder architecture, Whisper has been trained on 680,000 hours of supervised multilingual and multitask audio data, making it one of the most robust open-source models for end-to-end speech processing tasks. In this paper, we analyze Whisper’s performance on a variety of multilingual datasets covering both high-resource (e.g., English, Spanish, French) and low-resource languages (e.g., Hindi, Tamil, Swahili). We evaluate the model’s capabilities in automatic speech recognition (ASR), speech-to-text translation, and text- to-text translation tasks. Performance metrics such as BLEU score, Word Error Rate (WER), and inference latency are used to assess translation accuracy and efficiency. Our experimental results demonstrate that Whisper AI achieves competitive, and in many cases state-of-the-art, results across multiple language pairs and modalities. Additionally, Whisper exhibits robust zero- shot learning capabilities, enabling effective translation even for unseen language combinations. The paper also discusses Whisper’s strengths, such as its robustness to accents and background noise, as well as its limitations, including computational demands and occasional mistranslations in rare languages. Finally, we highlight real-world applications and propose directions for future research, including domain-specific fine-tuning and speech-to-speech translation. Our findings support Whisper’s potential to drive advancements in multilingual natural language processing and democratize access to AI-powered translation tools.","['https://openalex.org/W4311000453', 'https://openalex.org/W4323066695', 'https://openalex.org/W3112092703', 'https://openalex.org/W2972818416', 'https://openalex.org/W6768544969', 'https://openalex.org/W2987395887', 'https://openalex.org/W4392355216', 'https://openalex.org/W6767231498', 'https://openalex.org/W2019125599', 'https://openalex.org/W3010079431', 'https://openalex.org/W2519224033', 'https://openalex.org/W6739901393', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W3025165719', 'https://openalex.org/W2936774411', 'https://openalex.org/W2767906378', 'https://openalex.org/W2157331557']",2025-07-24
https://openalex.org/W4311137818,https://doi.org/10.1016/j.patter.2022.100616,Audio self-supervised learning: A survey,,"['https://openalex.org/W2084342166', 'https://openalex.org/W238467844', 'https://openalex.org/W4245602249', 'https://openalex.org/W2515031274', 'https://openalex.org/W6676156825', 'https://openalex.org/W3023371261', 'https://openalex.org/W2163922914', 'https://openalex.org/W6797132756', 'https://openalex.org/W6784811639', 'https://openalex.org/W6791753951', 'https://openalex.org/W6781476637', 'https://openalex.org/W6774314701', 'https://openalex.org/W6787584414', 'https://openalex.org/W3096655658', 'https://openalex.org/W2917551568', 'https://openalex.org/W6785511501', 'https://openalex.org/W6782305238', 'https://openalex.org/W3011574394', 'https://openalex.org/W6802366264', 'https://openalex.org/W6731293529', 'https://openalex.org/W3168903682', 'https://openalex.org/W6765696844', 'https://openalex.org/W6783797576', 'https://openalex.org/W6685777803', 'https://openalex.org/W2919115771', 'https://openalex.org/W6770717842', 'https://openalex.org/W7015890786', 'https://openalex.org/W6735738663', 'https://openalex.org/W6747899497', 'https://openalex.org/W6611801654', 'https://openalex.org/W6700872662', 'https://openalex.org/W6771137614', 'https://openalex.org/W3138202130', 'https://openalex.org/W6780697849', 'https://openalex.org/W6683390034', 'https://openalex.org/W6785269020', 'https://openalex.org/W6730323794', 'https://openalex.org/W6844194202', 'https://openalex.org/W6778102432', 'https://openalex.org/W3208254055', 'https://openalex.org/W6746516505', 'https://openalex.org/W6754278344', 'https://openalex.org/W6766489549', 'https://openalex.org/W6785703896', 'https://openalex.org/W6779326418', 'https://openalex.org/W6786614245', 'https://openalex.org/W6790850890', 'https://openalex.org/W6744513255', 'https://openalex.org/W6638667902', 'https://openalex.org/W6784531446', 'https://openalex.org/W6791742336', 'https://openalex.org/W6753000030', 'https://openalex.org/W6750305973', 'https://openalex.org/W6761306096', 'https://openalex.org/W6779997284', 'https://openalex.org/W6759370263', 'https://openalex.org/W6740057804', 'https://openalex.org/W6754217449', 'https://openalex.org/W6636510571', 'https://openalex.org/W6682691769', 'https://openalex.org/W6760822226', 'https://openalex.org/W6755207826', 'https://openalex.org/W6803870738', 'https://openalex.org/W6784776607', 'https://openalex.org/W6674387193', 'https://openalex.org/W6762573206', 'https://openalex.org/W6780218876', 'https://openalex.org/W6756349349', 'https://openalex.org/W6784077883', 'https://openalex.org/W6786091203', 'https://openalex.org/W6774092362', 'https://openalex.org/W6761176036', 'https://openalex.org/W6769196770', 'https://openalex.org/W6769455919', 'https://openalex.org/W6769313972', 'https://openalex.org/W6999638445', 'https://openalex.org/W6791429434', 'https://openalex.org/W6792227261', 'https://openalex.org/W6722416126', 'https://openalex.org/W6761050228', 'https://openalex.org/W6753968785', 'https://openalex.org/W6773243159', 'https://openalex.org/W6764398373', 'https://openalex.org/W3015949486', 'https://openalex.org/W6750129843', 'https://openalex.org/W6775396121', 'https://openalex.org/W3167347862', 'https://openalex.org/W6780483730', 'https://openalex.org/W6776076330', 'https://openalex.org/W6769593479', 'https://openalex.org/W3041561163', 'https://openalex.org/W6779471845', 'https://openalex.org/W6777232839', 'https://openalex.org/W6784532283', 'https://openalex.org/W6629717138', 'https://openalex.org/W6792927658', 'https://openalex.org/W6773553514', 'https://openalex.org/W6773205534', 'https://openalex.org/W6803140071', 'https://openalex.org/W6777337586', 'https://openalex.org/W6791599762', 'https://openalex.org/W2752796333', 'https://openalex.org/W6810007534', 'https://openalex.org/W6796554684', 'https://openalex.org/W6803164887', 'https://openalex.org/W6777659283', 'https://openalex.org/W2124509324', 'https://openalex.org/W7027429494', 'https://openalex.org/W6810046013', 'https://openalex.org/W2962866211', 'https://openalex.org/W6757632829', 'https://openalex.org/W6779216093', 'https://openalex.org/W6786367813', 'https://openalex.org/W6800868458', 'https://openalex.org/W6796988199', 'https://openalex.org/W6784499681', 'https://openalex.org/W6718827390', 'https://openalex.org/W6794812780', 'https://openalex.org/W6793904692', 'https://openalex.org/W6738607494', 'https://openalex.org/W6747045456', 'https://openalex.org/W6770340598', 'https://openalex.org/W6750591037', 'https://openalex.org/W6750736135', 'https://openalex.org/W6781706164', 'https://openalex.org/W6797160411', 'https://openalex.org/W6754048563', 'https://openalex.org/W6773964871', 'https://openalex.org/W6773803412', 'https://openalex.org/W6754610156', 'https://openalex.org/W6729977899', 'https://openalex.org/W6750651883', 'https://openalex.org/W6770596778', 'https://openalex.org/W6787100281', 'https://openalex.org/W6630875275', 'https://openalex.org/W6746700228', 'https://openalex.org/W2962969419', 'https://openalex.org/W6770805772', 'https://openalex.org/W6776441977', 'https://openalex.org/W6792796076', 'https://openalex.org/W6785011006', 'https://openalex.org/W6781303990', 'https://openalex.org/W6750599028', 'https://openalex.org/W6780294235', 'https://openalex.org/W6761222284', 'https://openalex.org/W6784429643', 'https://openalex.org/W6782004394', 'https://openalex.org/W6780637495', 'https://openalex.org/W6773436985', 'https://openalex.org/W3098994423', 'https://openalex.org/W6793790799', 'https://openalex.org/W6795952400', 'https://openalex.org/W6751433836', 'https://openalex.org/W6779191341', 'https://openalex.org/W6759578584', 'https://openalex.org/W6779199192', 'https://openalex.org/W6767853649', 'https://openalex.org/W6793736971', 'https://openalex.org/W6792919013', 'https://openalex.org/W6769238691', 'https://openalex.org/W6761559202', 'https://openalex.org/W6785591002', 'https://openalex.org/W6782134137', 'https://openalex.org/W6782066904', 'https://openalex.org/W6784488536', 'https://openalex.org/W3110458199', 'https://openalex.org/W2346964103', 'https://openalex.org/W6771812881', 'https://openalex.org/W6804302685', 'https://openalex.org/W6734260513', 'https://openalex.org/W6729831399', 'https://openalex.org/W6738587995', 'https://openalex.org/W6740167877', 'https://openalex.org/W6752581720', 'https://openalex.org/W6734491695', 'https://openalex.org/W6801281349', 'https://openalex.org/W3086505308', 'https://openalex.org/W6750782655', 'https://openalex.org/W6771917389', 'https://openalex.org/W6788335241', 'https://openalex.org/W6779977557', 'https://openalex.org/W6774670964', 'https://openalex.org/W2951974815', 'https://openalex.org/W2975357369', 'https://openalex.org/W3114632476', 'https://openalex.org/W3088409176', 'https://openalex.org/W3035837245', 'https://openalex.org/W4221145109', 'https://openalex.org/W3204453541', 'https://openalex.org/W4287282076', 'https://openalex.org/W3173151551', 'https://openalex.org/W3138296617', 'https://openalex.org/W3032892481', 'https://openalex.org/W3097286738', 'https://openalex.org/W3009561768', 'https://openalex.org/W2100738903', 'https://openalex.org/W2757910899', 'https://openalex.org/W3094123278', 'https://openalex.org/W2981991061', 'https://openalex.org/W2954540134', 'https://openalex.org/W3197411683', 'https://openalex.org/W2157364932', 'https://openalex.org/W2804648901', 'https://openalex.org/W38657318', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4297808394']",2022-12-01
https://openalex.org/W3210530853,https://doi.org/10.1109/icassp43922.2022.9746484,A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion,"The goal of voice conversion is to transform source speech into a target\nvoice, keeping the content unchanged. In this paper, we focus on\nself-supervised representation learning for voice conversion. Specifically, we\ncompare discrete and soft speech units as input features. We find that discrete\nrepresentations effectively remove speaker information but discard some\nlinguistic content - leading to mispronunciations. As a solution, we propose\nsoft speech units. To learn soft units, we predict a distribution over discrete\nspeech units. By modeling uncertainty, soft units capture more content\ninformation, improving the intelligibility and naturalness of converted speech.\nSamples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code\navailable at https://github.com/bshall/soft-vc/.\n","['https://openalex.org/W2395718496', 'https://openalex.org/W2890964092', 'https://openalex.org/W3140429000', 'https://openalex.org/W3095361818', 'https://openalex.org/W3161695192', 'https://openalex.org/W3209059054', 'https://openalex.org/W2155490028', 'https://openalex.org/W2842511635', 'https://openalex.org/W6786696081', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197580070', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015877095', 'https://openalex.org/W2899877258', 'https://openalex.org/W2995181338', 'https://openalex.org/W2120605154', 'https://openalex.org/W3092368332', 'https://openalex.org/W6752124048', 'https://openalex.org/W2518172956', 'https://openalex.org/W6762533536', 'https://openalex.org/W2963539064', 'https://openalex.org/W6840412704', 'https://openalex.org/W3188160682', 'https://openalex.org/W6783867762', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W2750167318', 'https://openalex.org/W2972802841', 'https://openalex.org/W4288079962', 'https://openalex.org/W4287591426', 'https://openalex.org/W2949382160', 'https://openalex.org/W2949281321', 'https://openalex.org/W3082130377', 'https://openalex.org/W4288079605', 'https://openalex.org/W4297808394', 'https://openalex.org/W3110458199', 'https://openalex.org/W3169320628', 'https://openalex.org/W2945478979', 'https://openalex.org/W2519091744', 'https://openalex.org/W3098403858', 'https://openalex.org/W4289299319', 'https://openalex.org/W2805993470', 'https://openalex.org/W3092028330']",2022-04-27
https://openalex.org/W3200287550,https://doi.org/10.1109/icassp43922.2022.9747103,Fast-Slow Transformer for Visually Grounding Speech,"We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS. FaST-VGS is a Transformer-based model for learning the associations between raw speech waveforms and visual images. The model unifies dual-encoder and cross-attention architectures into a single model, reaping the superior retrieval speed of the former along with the accuracy of the latter. FaST-VGS achieves state-of-the-art speech-image retrieval accuracy on benchmark datasets, and its learned representations exhibit strong performance on the ZeroSpeech 2021 phonetic and semantic tasks.","['https://openalex.org/W3155230099', 'https://openalex.org/W6755207826', 'https://openalex.org/W68733909', 'https://openalex.org/W6639102338', 'https://openalex.org/W6679792166', 'https://openalex.org/W3171668871', 'https://openalex.org/W6755977528', 'https://openalex.org/W6620707391', 'https://openalex.org/W6780218876', 'https://openalex.org/W2481240925', 'https://openalex.org/W6770596778', 'https://openalex.org/W3100813302', 'https://openalex.org/W6750651883', 'https://openalex.org/W3161204797', 'https://openalex.org/W3197467690', 'https://openalex.org/W2927673779', 'https://openalex.org/W3174311593', 'https://openalex.org/W6750194919', 'https://openalex.org/W2964099072', 'https://openalex.org/W6779753737', 'https://openalex.org/W3015300171', 'https://openalex.org/W6797832685', 'https://openalex.org/W2972892814', 'https://openalex.org/W6786696081', 'https://openalex.org/W6750570362', 'https://openalex.org/W2971709506', 'https://openalex.org/W2970231061', 'https://openalex.org/W2964001192', 'https://openalex.org/W3198411039', 'https://openalex.org/W3095293218', 'https://openalex.org/W2586850765', 'https://openalex.org/W2586148577', 'https://openalex.org/W3157861865', 'https://openalex.org/W2906407728', 'https://openalex.org/W2988907666', 'https://openalex.org/W3096372900', 'https://openalex.org/W3198815374', 'https://openalex.org/W6739901393', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962862718', 'https://openalex.org/W3197349023', 'https://openalex.org/W6729977899', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995680346', 'https://openalex.org/W3114436296', 'https://openalex.org/W2962866381', 'https://openalex.org/W3175947832', 'https://openalex.org/W3110458199', 'https://openalex.org/W1861492603', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963902314', 'https://openalex.org/W3196698946', 'https://openalex.org/W2899663614', 'https://openalex.org/W3099782249', 'https://openalex.org/W3177829661', 'https://openalex.org/W2963525826', 'https://openalex.org/W639708223', 'https://openalex.org/W2134670479', 'https://openalex.org/W2796315435', 'https://openalex.org/W2556930864', 'https://openalex.org/W2295158492', 'https://openalex.org/W4287591426', 'https://openalex.org/W4385245566', 'https://openalex.org/W2277195237', 'https://openalex.org/W4287757663', 'https://openalex.org/W2963330681']",2022-04-27
https://openalex.org/W4394773771,https://doi.org/10.1162/tacl_a_00656,What Do Self-Supervised Speech Models Know About Words?,"Abstract Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1","['https://openalex.org/W4385823003', 'https://openalex.org/W6754420807', 'https://openalex.org/W4296710617', 'https://openalex.org/W3044967013', 'https://openalex.org/W4385823338', 'https://openalex.org/W6795952400', 'https://openalex.org/W6810007534', 'https://openalex.org/W6780218876', 'https://openalex.org/W4319862401', 'https://openalex.org/W3202070718', 'https://openalex.org/W2906152891', 'https://openalex.org/W3198782837', 'https://openalex.org/W2407151108', 'https://openalex.org/W3203140070', 'https://openalex.org/W3198694222', 'https://openalex.org/W3209984917', 'https://openalex.org/W6803547063', 'https://openalex.org/W6748452836', 'https://openalex.org/W3209993061', 'https://openalex.org/W6755207826', 'https://openalex.org/W3093096176', 'https://openalex.org/W6787335539', 'https://openalex.org/W2251253014', 'https://openalex.org/W2963419157', 'https://openalex.org/W6839512648', 'https://openalex.org/W4372346125', 'https://openalex.org/W2166637769', 'https://openalex.org/W3097777922', 'https://openalex.org/W2962753610', 'https://openalex.org/W6731763572', 'https://openalex.org/W2970862333', 'https://openalex.org/W2025341678', 'https://openalex.org/W3174311593', 'https://openalex.org/W6792927658', 'https://openalex.org/W3160799772', 'https://openalex.org/W3095706145', 'https://openalex.org/W4225529283', 'https://openalex.org/W1967924372', 'https://openalex.org/W6839364956', 'https://openalex.org/W2995181338', 'https://openalex.org/W4313182775', 'https://openalex.org/W2190506272', 'https://openalex.org/W4223651314', 'https://openalex.org/W6761472960', 'https://openalex.org/W3096656254', 'https://openalex.org/W6790356757', 'https://openalex.org/W2059652594', 'https://openalex.org/W4319862479', 'https://openalex.org/W4391021793', 'https://openalex.org/W4385823426', 'https://openalex.org/W2972584841', 'https://openalex.org/W3163596720', 'https://openalex.org/W1632114991', 'https://openalex.org/W2747874407', 'https://openalex.org/W3198266945', 'https://openalex.org/W4306317873', 'https://openalex.org/W2065157922', 'https://openalex.org/W4281492411', 'https://openalex.org/W6752726010', 'https://openalex.org/W4303649106', 'https://openalex.org/W3110458199', 'https://openalex.org/W4307680525', 'https://openalex.org/W2963259843', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226380987', 'https://openalex.org/W4375869259', 'https://openalex.org/W4287887773', 'https://openalex.org/W4224875474', 'https://openalex.org/W6786885278', 'https://openalex.org/W2250539671', 'https://openalex.org/W2145410271', 'https://openalex.org/W3034273309', 'https://openalex.org/W6745682157', 'https://openalex.org/W1558402681', 'https://openalex.org/W3155744586', 'https://openalex.org/W4206075291', 'https://openalex.org/W4375869060', 'https://openalex.org/W6948152991', 'https://openalex.org/W2932675979', 'https://openalex.org/W2962736743', 'https://openalex.org/W6788328058', 'https://openalex.org/W4385823328', 'https://openalex.org/W6810168380', 'https://openalex.org/W3150750326', 'https://openalex.org/W4385571440', 'https://openalex.org/W4226103796', 'https://openalex.org/W1606268232', 'https://openalex.org/W2946417913', 'https://openalex.org/W4285250921', 'https://openalex.org/W2963482440', 'https://openalex.org/W2251066368', 'https://openalex.org/W3198815374', 'https://openalex.org/W3150635893', 'https://openalex.org/W6739901393', 'https://openalex.org/W3008003211', 'https://openalex.org/W2970820321', 'https://openalex.org/W3119308075', 'https://openalex.org/W4372270126', 'https://openalex.org/W4385574560', 'https://openalex.org/W4385822254', 'https://openalex.org/W6853627120', 'https://openalex.org/W3197580070', 'https://openalex.org/W4385484924', 'https://openalex.org/W4386273179', 'https://openalex.org/W4385573456', 'https://openalex.org/W398859631', 'https://openalex.org/W569478347', 'https://openalex.org/W3096196861', 'https://openalex.org/W4283694096', 'https://openalex.org/W2593390416', 'https://openalex.org/W4394671563', 'https://openalex.org/W4385245566', 'https://openalex.org/W4319779871', 'https://openalex.org/W2891205112', 'https://openalex.org/W4280638376', 'https://openalex.org/W2602024037', 'https://openalex.org/W4310895557']",2024-01-01
https://openalex.org/W3171477941,https://doi.org/10.1016/j.cognition.2021.104779,SCALa: A blueprint for computational models of language acquisition in social context,,"['https://openalex.org/W2980577029', 'https://openalex.org/W2059100041', 'https://openalex.org/W2885156775', 'https://openalex.org/W6748031072', 'https://openalex.org/W6743108112', 'https://openalex.org/W6781919819', 'https://openalex.org/W2488227055', 'https://openalex.org/W6791910184', 'https://openalex.org/W2766298282', 'https://openalex.org/W2483390977', 'https://openalex.org/W2011238950', 'https://openalex.org/W2159190230', 'https://openalex.org/W3005081886', 'https://openalex.org/W4234313254', 'https://openalex.org/W1964174279', 'https://openalex.org/W2052262800', 'https://openalex.org/W1613027418', 'https://openalex.org/W2132730112', 'https://openalex.org/W6789659809', 'https://openalex.org/W2004393017', 'https://openalex.org/W6732850833', 'https://openalex.org/W2165545766', 'https://openalex.org/W2132951686', 'https://openalex.org/W2165345255', 'https://openalex.org/W2140025977', 'https://openalex.org/W6758354414', 'https://openalex.org/W2110958718', 'https://openalex.org/W6795317780', 'https://openalex.org/W2888800758', 'https://openalex.org/W2117164941', 'https://openalex.org/W2026764577', 'https://openalex.org/W2045031658', 'https://openalex.org/W6786696081', 'https://openalex.org/W6630560557', 'https://openalex.org/W2905623858', 'https://openalex.org/W6630228087', 'https://openalex.org/W2160487945', 'https://openalex.org/W2158286954', 'https://openalex.org/W3125955384', 'https://openalex.org/W2123998717', 'https://openalex.org/W1965192788', 'https://openalex.org/W6789817408', 'https://openalex.org/W2030512997', 'https://openalex.org/W2059168261', 'https://openalex.org/W6761197245', 'https://openalex.org/W6682685583', 'https://openalex.org/W1897242287', 'https://openalex.org/W2419654175', 'https://openalex.org/W2996728628', 'https://openalex.org/W2089883580', 'https://openalex.org/W2033177442', 'https://openalex.org/W803770162', 'https://openalex.org/W4246559809', 'https://openalex.org/W2073475009', 'https://openalex.org/W2328091329', 'https://openalex.org/W3114436296', 'https://openalex.org/W2154521990', 'https://openalex.org/W1980491396', 'https://openalex.org/W1992119553', 'https://openalex.org/W2557283755', 'https://openalex.org/W3161374022', 'https://openalex.org/W1576632330', 'https://openalex.org/W3193504637', 'https://openalex.org/W1971771135', 'https://openalex.org/W2999905431', 'https://openalex.org/W1562911371', 'https://openalex.org/W3082004699', 'https://openalex.org/W2333916262', 'https://openalex.org/W3023371261', 'https://openalex.org/W2094249282', 'https://openalex.org/W1580585489', 'https://openalex.org/W2931316642', 'https://openalex.org/W3125087428', 'https://openalex.org/W1989044351', 'https://openalex.org/W3123354371', 'https://openalex.org/W4249366912', 'https://openalex.org/W1500567887', 'https://openalex.org/W1903073458', 'https://openalex.org/W1508753152', 'https://openalex.org/W2962753610', 'https://openalex.org/W4287591426', 'https://openalex.org/W2747789846', 'https://openalex.org/W2804964047', 'https://openalex.org/W2130526565', 'https://openalex.org/W3110458199', 'https://openalex.org/W2003867956', 'https://openalex.org/W2129264276', 'https://openalex.org/W1507558854', 'https://openalex.org/W2915722758', 'https://openalex.org/W4404108541', 'https://openalex.org/W2599276130', 'https://openalex.org/W2580178245', 'https://openalex.org/W2955647675', 'https://openalex.org/W4212995409', 'https://openalex.org/W3080620940', 'https://openalex.org/W3174311593', 'https://openalex.org/W2785533964', 'https://openalex.org/W1607278777', 'https://openalex.org/W2135943618', 'https://openalex.org/W4237960366', 'https://openalex.org/W4235553294', 'https://openalex.org/W1515851193', 'https://openalex.org/W4301409532', 'https://openalex.org/W562707371']",2021-06-03
https://openalex.org/W4247178956,https://doi.org/10.31234/osf.io/pt9xq,Reverse engineering language acquisition with child-centered long-form recordings,"Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.","['https://openalex.org/W2759573091', 'https://openalex.org/W3177829661', 'https://openalex.org/W1592295210', 'https://openalex.org/W1532494781', 'https://openalex.org/W6789725041', 'https://openalex.org/W2885156775', 'https://openalex.org/W2063303346', 'https://openalex.org/W2772732614', 'https://openalex.org/W3112495382', 'https://openalex.org/W6665016202', 'https://openalex.org/W1984586950', 'https://openalex.org/W3046659789', 'https://openalex.org/W6790730029', 'https://openalex.org/W6781919819', 'https://openalex.org/W2997253105', 'https://openalex.org/W2944539396', 'https://openalex.org/W2586148577', 'https://openalex.org/W4213306813', 'https://openalex.org/W3123651923', 'https://openalex.org/W3005165546', 'https://openalex.org/W2015075592', 'https://openalex.org/W3135377987', 'https://openalex.org/W2483390977', 'https://openalex.org/W2160997109', 'https://openalex.org/W2774051897', 'https://openalex.org/W6685399832', 'https://openalex.org/W2991557631', 'https://openalex.org/W2112883467', 'https://openalex.org/W6633336849', 'https://openalex.org/W2071591642', 'https://openalex.org/W2889102505', 'https://openalex.org/W6781576031', 'https://openalex.org/W6629325461', 'https://openalex.org/W6641916425', 'https://openalex.org/W2085478996', 'https://openalex.org/W3110458199', 'https://openalex.org/W2618478924', 'https://openalex.org/W2132566726', 'https://openalex.org/W2972476505', 'https://openalex.org/W3025683731', 'https://openalex.org/W3125043549', 'https://openalex.org/W6729411815', 'https://openalex.org/W3129957462', 'https://openalex.org/W3125087428', 'https://openalex.org/W2801193150', 'https://openalex.org/W2141994663', 'https://openalex.org/W6658483803', 'https://openalex.org/W2799770360', 'https://openalex.org/W2805234167', 'https://openalex.org/W6681346506', 'https://openalex.org/W4407276585', 'https://openalex.org/W2065159495', 'https://openalex.org/W2621934507', 'https://openalex.org/W2252657604', 'https://openalex.org/W2101509422', 'https://openalex.org/W2803055582', 'https://openalex.org/W2058616551', 'https://openalex.org/W2135563147', 'https://openalex.org/W2060238187', 'https://openalex.org/W4253971549', 'https://openalex.org/W4246103655', 'https://openalex.org/W2058354688', 'https://openalex.org/W2343593471', 'https://openalex.org/W4255020641', 'https://openalex.org/W4251221781', 'https://openalex.org/W1559022555', 'https://openalex.org/W4232589384', 'https://openalex.org/W1558150890', 'https://openalex.org/W4297612016', 'https://openalex.org/W4245117732', 'https://openalex.org/W2546861836', 'https://openalex.org/W4253947715', 'https://openalex.org/W2032543155', 'https://openalex.org/W4287591426', 'https://openalex.org/W3080620940', 'https://openalex.org/W1967834254', 'https://openalex.org/W1485633403', 'https://openalex.org/W4242334097', 'https://openalex.org/W4236000557', 'https://openalex.org/W3047246203', 'https://openalex.org/W4251435902', 'https://openalex.org/W2995680346', 'https://openalex.org/W3126722376', 'https://openalex.org/W4252366034']",2021-03-31
https://openalex.org/W4296710617,https://doi.org/10.1162/tacl_a_00505,DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon,"Abstract Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1","['https://openalex.org/W4223486244', 'https://openalex.org/W3044967013', 'https://openalex.org/W6780218876', 'https://openalex.org/W6640135064', 'https://openalex.org/W3097159218', 'https://openalex.org/W3198782837', 'https://openalex.org/W3015783745', 'https://openalex.org/W2407151108', 'https://openalex.org/W2963425185', 'https://openalex.org/W3209993061', 'https://openalex.org/W6755207826', 'https://openalex.org/W2963620343', 'https://openalex.org/W3093096176', 'https://openalex.org/W6731922460', 'https://openalex.org/W4235505822', 'https://openalex.org/W2946322623', 'https://openalex.org/W2126377586', 'https://openalex.org/W2122228338', 'https://openalex.org/W6682948231', 'https://openalex.org/W6796554684', 'https://openalex.org/W3108506107', 'https://openalex.org/W3037580942', 'https://openalex.org/W3146777637', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117126688', 'https://openalex.org/W6756098772', 'https://openalex.org/W6810047917', 'https://openalex.org/W2468716020', 'https://openalex.org/W1997505733', 'https://openalex.org/W2964169922', 'https://openalex.org/W3198134274', 'https://openalex.org/W2952125979', 'https://openalex.org/W6790356757', 'https://openalex.org/W6675022971', 'https://openalex.org/W1778492285', 'https://openalex.org/W6793306531', 'https://openalex.org/W6691362072', 'https://openalex.org/W6636510571', 'https://openalex.org/W2140991203', 'https://openalex.org/W3110458199', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W2114347655', 'https://openalex.org/W2118020555', 'https://openalex.org/W2962850179', 'https://openalex.org/W2398490608', 'https://openalex.org/W2395899413', 'https://openalex.org/W2962736743', 'https://openalex.org/W2913062184', 'https://openalex.org/W2346964103', 'https://openalex.org/W1700952868', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W4313182775', 'https://openalex.org/W101045393', 'https://openalex.org/W2771976988', 'https://openalex.org/W4394671563', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963720603', 'https://openalex.org/W4287591426', 'https://openalex.org/W2613000335', 'https://openalex.org/W4297808394', 'https://openalex.org/W3095706145', 'https://openalex.org/W3096196861', 'https://openalex.org/W1614298861', 'https://openalex.org/W2916009164', 'https://openalex.org/W3151510603', 'https://openalex.org/W3209059054', 'https://openalex.org/W3098643042', 'https://openalex.org/W4240908132']",2022-01-01
https://openalex.org/W4319862218,https://doi.org/10.1109/slt54892.2023.10022791,Maestro-U: Leveraging Joint Speech-Text Representation Learning for Zero Supervised Speech ASR,"Training state-of-the-art Automated Speech Recognition (ASR) models typically requires a substantial amount of transcribed speech. In this work, we demonstrate that a modality-matched joint speech and text model introduced in [1] can be leveraged to train a massively multilingual ASR model without any supervised (manually transcribed) speech for some languages. This paper explores the use of jointly learnt speech and text representations in a massively multilingual, zero supervised speech, real-world setting to expand the set of languages covered by ASR with only unlabeled speech and text in the target languages. Using the FLEURS dataset, we define the task to cover 102 languages, where transcribed speech is available in 52 of these languages and can be used to improve end-to-end ASR quality on the remaining 50. First, we show that by combining speech representations with byte-level text representations and use of language embeddings, we can dramatically reduce the Character Error Rate (CER) on languages with no supervised speech from 64.8% to 30.8%, a relative reduction of 53%. Second, using a subset of South Asian languages we show that Maestro-U can promote knowledge transfer from languages with supervised speech even when there is limited to no graphemic overlap. Overall, Maestro-U closes the gap to oracle performance by 68.5% relative and reduces the CER of 19 languages below 15%.","['https://openalex.org/W3198840231', 'https://openalex.org/W1978660892', 'https://openalex.org/W2894835365', 'https://openalex.org/W3095410713', 'https://openalex.org/W4225295099', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963425185', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W2962826786', 'https://openalex.org/W2963240019', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W2962799225', 'https://openalex.org/W6757699909', 'https://openalex.org/W6760519848', 'https://openalex.org/W6795952400', 'https://openalex.org/W6849880362', 'https://openalex.org/W3110458199', 'https://openalex.org/W6838929754', 'https://openalex.org/W3119308075', 'https://openalex.org/W6771467084', 'https://openalex.org/W6696449567', 'https://openalex.org/W6784577980', 'https://openalex.org/W3137010024', 'https://openalex.org/W3115778530', 'https://openalex.org/W6765544429', 'https://openalex.org/W4226033575', 'https://openalex.org/W3161296985', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015564377', 'https://openalex.org/W2971840980', 'https://openalex.org/W2914699162', 'https://openalex.org/W4210690962', 'https://openalex.org/W6757585730', 'https://openalex.org/W2964002616', 'https://openalex.org/W6784614252', 'https://openalex.org/W4225272718', 'https://openalex.org/W3202037040', 'https://openalex.org/W3212799896', 'https://openalex.org/W4288282820', 'https://openalex.org/W2934852845', 'https://openalex.org/W4319862670', 'https://openalex.org/W4297808394', 'https://openalex.org/W2964079874', 'https://openalex.org/W4287173589', 'https://openalex.org/W3030437843', 'https://openalex.org/W4221155340', 'https://openalex.org/W4319862635', 'https://openalex.org/W3207222250', 'https://openalex.org/W3169483174', 'https://openalex.org/W2952809536', 'https://openalex.org/W3093579165', 'https://openalex.org/W3036601975']",2023-01-09
https://openalex.org/W4392903468,https://doi.org/10.1109/icassp48485.2024.10446737,Zero Resource Code-Switched Speech Benchmark Using Speech Utterance Pairs for Multiple Spoken Languages,"We introduce a new zero resource code-switched speech bench-mark designed to assess the code-switching capabilities of self-supervised speech encoders directly. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc., on three tracks of different code-switched language pairs: Spanish-English, French-English, and Chinese-English. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.","['https://openalex.org/W4375869049', 'https://openalex.org/W6846997368', 'https://openalex.org/W4285158365', 'https://openalex.org/W6845064686', 'https://openalex.org/W3097032879', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4287854499', 'https://openalex.org/W4296068785', 'https://openalex.org/W4375869224', 'https://openalex.org/W4319862641', 'https://openalex.org/W3198429080', 'https://openalex.org/W3213029956', 'https://openalex.org/W4389518325', 'https://openalex.org/W4389519244', 'https://openalex.org/W3173110011', 'https://openalex.org/W2996728628', 'https://openalex.org/W3110458199', 'https://openalex.org/W4246076598', 'https://openalex.org/W6771467084', 'https://openalex.org/W3180374548', 'https://openalex.org/W2933138175', 'https://openalex.org/W3035390927', 'https://openalex.org/W4385565879', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W6769196770', 'https://openalex.org/W4385822439', 'https://openalex.org/W3197259906', 'https://openalex.org/W4302306287', 'https://openalex.org/W2914304175']",2024-03-18
https://openalex.org/W4378619943,https://doi.org/10.1017/s0305000923000272,Realistic and broad-scope learning simulations: first results and challenges,"Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.","['https://openalex.org/W2913939165', 'https://openalex.org/W4232516125', 'https://openalex.org/W2064135025', 'https://openalex.org/W2059168261', 'https://openalex.org/W3134108900', 'https://openalex.org/W1532494781', 'https://openalex.org/W2973180715', 'https://openalex.org/W6825992397', 'https://openalex.org/W3161374022', 'https://openalex.org/W2089883580', 'https://openalex.org/W2303872361', 'https://openalex.org/W4206039057', 'https://openalex.org/W2896342372', 'https://openalex.org/W2141038596', 'https://openalex.org/W2917235195', 'https://openalex.org/W1969005071', 'https://openalex.org/W2585893098', 'https://openalex.org/W2483390977', 'https://openalex.org/W4225726571', 'https://openalex.org/W2937297214', 'https://openalex.org/W2125566341', 'https://openalex.org/W833103999', 'https://openalex.org/W2765364385', 'https://openalex.org/W2153897911', 'https://openalex.org/W1968225092', 'https://openalex.org/W2078828996', 'https://openalex.org/W2595479191', 'https://openalex.org/W6800751262', 'https://openalex.org/W1530250655', 'https://openalex.org/W3199093330', 'https://openalex.org/W2060204180', 'https://openalex.org/W135984148', 'https://openalex.org/W6712444837', 'https://openalex.org/W2162505970', 'https://openalex.org/W4242738990', 'https://openalex.org/W2784570041', 'https://openalex.org/W1984586950', 'https://openalex.org/W2759573091', 'https://openalex.org/W4200598898', 'https://openalex.org/W4220770602', 'https://openalex.org/W6809594290', 'https://openalex.org/W3110458199', 'https://openalex.org/W2615444509', 'https://openalex.org/W4292825791', 'https://openalex.org/W4297677272', 'https://openalex.org/W4375868953', 'https://openalex.org/W2435103813', 'https://openalex.org/W2996556191', 'https://openalex.org/W6790356757', 'https://openalex.org/W1997015862', 'https://openalex.org/W4309419356', 'https://openalex.org/W2110221456', 'https://openalex.org/W2163999179', 'https://openalex.org/W2005592929', 'https://openalex.org/W4205897400', 'https://openalex.org/W6648830005', 'https://openalex.org/W2110485445', 'https://openalex.org/W2149932965', 'https://openalex.org/W3197259906', 'https://openalex.org/W3166292821', 'https://openalex.org/W6676025551', 'https://openalex.org/W1540332606', 'https://openalex.org/W1995403064', 'https://openalex.org/W1597121597', 'https://openalex.org/W3023172065', 'https://openalex.org/W2016292361', 'https://openalex.org/W2000977437', 'https://openalex.org/W4311481261', 'https://openalex.org/W2138641981', 'https://openalex.org/W3125087428', 'https://openalex.org/W4253682481', 'https://openalex.org/W2935067899', 'https://openalex.org/W4237938692', 'https://openalex.org/W3005081886', 'https://openalex.org/W2153767712', 'https://openalex.org/W4230725619', 'https://openalex.org/W2103091632', 'https://openalex.org/W2001771035', 'https://openalex.org/W2118373646', 'https://openalex.org/W3171477941', 'https://openalex.org/W2005311247', 'https://openalex.org/W4250883074', 'https://openalex.org/W2343593471', 'https://openalex.org/W4300721020', 'https://openalex.org/W6778883912', 'https://openalex.org/W1980862600', 'https://openalex.org/W3198815374', 'https://openalex.org/W2089358714', 'https://openalex.org/W4394671563', 'https://openalex.org/W2980877534', 'https://openalex.org/W2333023345', 'https://openalex.org/W4210307751', 'https://openalex.org/W593365102', 'https://openalex.org/W2950416202', 'https://openalex.org/W4221038855', 'https://openalex.org/W2107959623', 'https://openalex.org/W2395899413', 'https://openalex.org/W4298742451', 'https://openalex.org/W4292779060', 'https://openalex.org/W2132387635', 'https://openalex.org/W2612271891']",2023-05-29
https://openalex.org/W4382918397,https://doi.org/10.1111/cogs.13307,Introducing Meta‐analysis in the Evaluation of Computational Models of Infant Language Development,"Abstract Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large‐scale cumulative empirical data from infants, as quantified by meta‐analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta‐analytic evaluation of computational models. We exemplify the meta‐analytic model evaluation approach with two modeling experiments on infant‐directed speech preference and native/non‐native vowel discrimination.","['https://openalex.org/W2137295896', 'https://openalex.org/W3177829661', 'https://openalex.org/W4213058750', 'https://openalex.org/W4283258029', 'https://openalex.org/W2772732614', 'https://openalex.org/W4212863985', 'https://openalex.org/W6629510986', 'https://openalex.org/W2054289822', 'https://openalex.org/W3130490927', 'https://openalex.org/W3165112351', 'https://openalex.org/W2067191956', 'https://openalex.org/W6667443654', 'https://openalex.org/W3017025049', 'https://openalex.org/W3035750922', 'https://openalex.org/W2926827382', 'https://openalex.org/W2972943112', 'https://openalex.org/W4244103449', 'https://openalex.org/W6652776558', 'https://openalex.org/W2253063626', 'https://openalex.org/W3082376609', 'https://openalex.org/W3023889965', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2483390977', 'https://openalex.org/W2130890537', 'https://openalex.org/W4299555621', 'https://openalex.org/W2157823046', 'https://openalex.org/W3145811386', 'https://openalex.org/W3196459653', 'https://openalex.org/W2079408923', 'https://openalex.org/W6670098071', 'https://openalex.org/W2123292690', 'https://openalex.org/W2782905495', 'https://openalex.org/W2595479191', 'https://openalex.org/W2141038596', 'https://openalex.org/W3119754616', 'https://openalex.org/W2148764920', 'https://openalex.org/W3084297320', 'https://openalex.org/W4253971549', 'https://openalex.org/W2191828469', 'https://openalex.org/W3119802905', 'https://openalex.org/W3123354371', 'https://openalex.org/W7071764571', 'https://openalex.org/W2070696251', 'https://openalex.org/W4285726357', 'https://openalex.org/W4295309037', 'https://openalex.org/W3114436296', 'https://openalex.org/W3174311593', 'https://openalex.org/W2466796873', 'https://openalex.org/W2144981148', 'https://openalex.org/W135984148', 'https://openalex.org/W4206039057', 'https://openalex.org/W3164946614', 'https://openalex.org/W4230640548', 'https://openalex.org/W2057256606', 'https://openalex.org/W4225079082', 'https://openalex.org/W1990351858', 'https://openalex.org/W2103091632', 'https://openalex.org/W2031445901', 'https://openalex.org/W4253566955', 'https://openalex.org/W3161374022', 'https://openalex.org/W2785183465', 'https://openalex.org/W6618980683', 'https://openalex.org/W3081675162', 'https://openalex.org/W2031880432', 'https://openalex.org/W6658466740', 'https://openalex.org/W1576931943', 'https://openalex.org/W2002572909', 'https://openalex.org/W3023172065', 'https://openalex.org/W1965305729', 'https://openalex.org/W6641383474', 'https://openalex.org/W2024579455', 'https://openalex.org/W2091143423', 'https://openalex.org/W2615578810', 'https://openalex.org/W3110458199', 'https://openalex.org/W3163184902', 'https://openalex.org/W1897139626', 'https://openalex.org/W2610253745', 'https://openalex.org/W1494198834', 'https://openalex.org/W2268773773', 'https://openalex.org/W2140386792', 'https://openalex.org/W6680931182', 'https://openalex.org/W3033724742', 'https://openalex.org/W2119885245', 'https://openalex.org/W2010188467', 'https://openalex.org/W4376140200', 'https://openalex.org/W2415378728', 'https://openalex.org/W3023533951', 'https://openalex.org/W3035507081', 'https://openalex.org/W3129957462', 'https://openalex.org/W2768381684', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W3102519966', 'https://openalex.org/W4303629135', 'https://openalex.org/W2154600605', 'https://openalex.org/W3084747096', 'https://openalex.org/W2092929651', 'https://openalex.org/W2041173449', 'https://openalex.org/W2077382402', 'https://openalex.org/W6669981445', 'https://openalex.org/W2032476212', 'https://openalex.org/W2114831903', 'https://openalex.org/W7073679870', 'https://openalex.org/W1925965306', 'https://openalex.org/W6640227979', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2089883580', 'https://openalex.org/W3157923770', 'https://openalex.org/W3197580070', 'https://openalex.org/W3117111924', 'https://openalex.org/W4211209158', 'https://openalex.org/W2284729062', 'https://openalex.org/W2129168188', 'https://openalex.org/W2973026522', 'https://openalex.org/W612372977', 'https://openalex.org/W2482374272']",2023-07-01
https://openalex.org/W3196938511,https://doi.org/10.1145/3477087.3478382,EDGY,"Voice user interfaces and assistants are rapidly entering our lives and becoming singular touchpoints spanning our devices. Raw audio signals collected through these devices contain a host of sensitive paralinguistic information (e.g., emotional patterns) that is transmitted to service providers regardless of deliberate or false triggers. We thus encounter a new generation of privacy risks by using these services. To tackle this issue, we have developed EDGY; a configurable, lightweight, disentangled representation learning framework that transforms and filters high-dimensional voice data to identify and selectively filter sensitive attributes at the edge prior to offloading to the cloud. Our results show that EDGY runs in tens of milliseconds with 0.2% relative improvement in ABX score and minimal performance penalties in learning linguistic representations from raw signals on a CPU and single-core ARM processor with no specialized hardware.","['https://openalex.org/W3143247654', 'https://openalex.org/W3007566156', 'https://openalex.org/W3110458199', 'https://openalex.org/W3095361818', 'https://openalex.org/W2057563799', 'https://openalex.org/W3098439673', 'https://openalex.org/W3097320994', 'https://openalex.org/W3104686647', 'https://openalex.org/W2915661750']",2021-08-27
https://openalex.org/W3178584664,https://doi.org/10.21437/interspeech.2021-1503,Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021,"We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.","['https://openalex.org/W2996383576', 'https://openalex.org/W3096338464', 'https://openalex.org/W3015737168', 'https://openalex.org/W1836465849', 'https://openalex.org/W2964074409', 'https://openalex.org/W2963403868', 'https://openalex.org/W3026041220', 'https://openalex.org/W3016011332', 'https://openalex.org/W3025165719', 'https://openalex.org/W3095783102', 'https://openalex.org/W2973157397', 'https://openalex.org/W2395899413', 'https://openalex.org/W2883725317', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W3094800360', 'https://openalex.org/W2064675550', 'https://openalex.org/W3110458199', 'https://openalex.org/W2765741717', 'https://openalex.org/W2982223350', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996728628', 'https://openalex.org/W2965373594', 'https://openalex.org/W2152790380', 'https://openalex.org/W1538131130', 'https://openalex.org/W3093579165', 'https://openalex.org/W3099782249', 'https://openalex.org/W2095705004']",2021-08-27
https://openalex.org/W4395036961,https://doi.org/10.31234/osf.io/gc5kp,Modeling the initial state of early phonetic learning in infants,"What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.","['https://openalex.org/W2214604124', 'https://openalex.org/W2027219560', 'https://openalex.org/W4232516125', 'https://openalex.org/W2548930763', 'https://openalex.org/W2001365213', 'https://openalex.org/W2786031913', 'https://openalex.org/W3015783745', 'https://openalex.org/W4229781645', 'https://openalex.org/W4225815933', 'https://openalex.org/W2765364385', 'https://openalex.org/W4382918397', 'https://openalex.org/W4225726571', 'https://openalex.org/W4378619943', 'https://openalex.org/W3197259906', 'https://openalex.org/W2483390977', 'https://openalex.org/W2050517152', 'https://openalex.org/W2011238950', 'https://openalex.org/W2595479191', 'https://openalex.org/W2593116425', 'https://openalex.org/W2605959375', 'https://openalex.org/W2165545766', 'https://openalex.org/W4230449744', 'https://openalex.org/W2999130843', 'https://openalex.org/W2133189729', 'https://openalex.org/W2995181338', 'https://openalex.org/W1597121597', 'https://openalex.org/W3144810982', 'https://openalex.org/W2020944885', 'https://openalex.org/W2103091632', 'https://openalex.org/W2063525438', 'https://openalex.org/W2005311247', 'https://openalex.org/W4311481261', 'https://openalex.org/W4391669171', 'https://openalex.org/W4385822936', 'https://openalex.org/W6791720001', 'https://openalex.org/W3034729383', 'https://openalex.org/W4384626726', 'https://openalex.org/W2104752510', 'https://openalex.org/W2776941264', 'https://openalex.org/W2062956221', 'https://openalex.org/W281094599', 'https://openalex.org/W3110458199', 'https://openalex.org/W2050864369', 'https://openalex.org/W6656414902', 'https://openalex.org/W2113332115', 'https://openalex.org/W2145410271', 'https://openalex.org/W1970688873', 'https://openalex.org/W2010188467', 'https://openalex.org/W2621649611', 'https://openalex.org/W3016181583', 'https://openalex.org/W577928986', 'https://openalex.org/W2615444509', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W66627554', 'https://openalex.org/W4303629135', 'https://openalex.org/W1604716266', 'https://openalex.org/W2160464066', 'https://openalex.org/W2003341094', 'https://openalex.org/W1925965306', 'https://openalex.org/W292738443', 'https://openalex.org/W2153767712', 'https://openalex.org/W4238846128', 'https://openalex.org/W4389521008', 'https://openalex.org/W4246559809', 'https://openalex.org/W2037751943', 'https://openalex.org/W2088724121', 'https://openalex.org/W4389519222', 'https://openalex.org/W2024490156', 'https://openalex.org/W598767079', 'https://openalex.org/W4396127524', 'https://openalex.org/W3119308075', 'https://openalex.org/W4297808394', 'https://openalex.org/W3133848337', 'https://openalex.org/W4287591426']",2024-04-23
https://openalex.org/W3175947832,https://doi.org/10.21437/interspeech.2021-1465,Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw,"We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.","['https://openalex.org/W2965373594', 'https://openalex.org/W2142625445', 'https://openalex.org/W3110458199', 'https://openalex.org/W2251803266', 'https://openalex.org/W2950577311', 'https://openalex.org/W2128160875', 'https://openalex.org/W2626778328', 'https://openalex.org/W2786608204', 'https://openalex.org/W2252211741', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963979492', 'https://openalex.org/W2995181338', 'https://openalex.org/W2842511635', 'https://openalex.org/W2996728628', 'https://openalex.org/W2053921957', 'https://openalex.org/W2014307400', 'https://openalex.org/W2963751529', 'https://openalex.org/W2963250244']",2021-08-27
https://openalex.org/W3157923770,https://doi.org/10.48550/arxiv.2105.01051,SUPERB: Speech processing Universal PERformance Benchmark,"Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.","['https://openalex.org/W2107092366', 'https://openalex.org/W3016181583', 'https://openalex.org/W3041561163', 'https://openalex.org/W3112616666', 'https://openalex.org/W2883725317', 'https://openalex.org/W3006926732', 'https://openalex.org/W2294718077', 'https://openalex.org/W2962739339', 'https://openalex.org/W3095953768', 'https://openalex.org/W2981087920', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015075030', 'https://openalex.org/W3034238904', 'https://openalex.org/W1567520911', 'https://openalex.org/W2972949456', 'https://openalex.org/W3169320628', 'https://openalex.org/W2953190524', 'https://openalex.org/W2146334809', 'https://openalex.org/W3099944122', 'https://openalex.org/W2973157397', 'https://openalex.org/W2842511635', 'https://openalex.org/W2923014074', 'https://openalex.org/W3015213852', 'https://openalex.org/W2973049979', 'https://openalex.org/W2996383576', 'https://openalex.org/W3095292526', 'https://openalex.org/W2134800885', 'https://openalex.org/W3097286738', 'https://openalex.org/W3099782249', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963341956', 'https://openalex.org/W3027008958', 'https://openalex.org/W2972382840', 'https://openalex.org/W2990873191', 'https://openalex.org/W2972584841', 'https://openalex.org/W3110458199', 'https://openalex.org/W2803609229', 'https://openalex.org/W2890964092', 'https://openalex.org/W2972943112', 'https://openalex.org/W3112034174']",2021-05-03
https://openalex.org/W3177829661,https://doi.org/10.48550/arxiv.2107.06546,"ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition","We present the visually-grounded language modelling track that was introduced in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the new track and discuss participation rules in detail. We also present the two baseline systems that were developed for this track.","['https://openalex.org/W2123815913', 'https://openalex.org/W2556930864', 'https://openalex.org/W3110458199', 'https://openalex.org/W2963620343', 'https://openalex.org/W2885307078', 'https://openalex.org/W385555557', 'https://openalex.org/W1905882502', 'https://openalex.org/W3157861865', 'https://openalex.org/W2346964103', 'https://openalex.org/W2194775991', 'https://openalex.org/W3100813302', 'https://openalex.org/W1575001262', 'https://openalex.org/W2989358187', 'https://openalex.org/W2950133079', 'https://openalex.org/W3114436296', 'https://openalex.org/W2784025607', 'https://openalex.org/W2991557631', 'https://openalex.org/W2842511635', 'https://openalex.org/W2107917162', 'https://openalex.org/W2108598243', 'https://openalex.org/W2984008963', 'https://openalex.org/W2971709506', 'https://openalex.org/W2940544976', 'https://openalex.org/W2114739781', 'https://openalex.org/W2149557440', 'https://openalex.org/W2586148577', 'https://openalex.org/W2972943112', 'https://openalex.org/W2415378728', 'https://openalex.org/W2920166246', 'https://openalex.org/W2157331557', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963525826', 'https://openalex.org/W1494198834', 'https://openalex.org/W3105148948', 'https://openalex.org/W3042657922']",2021-07-14
https://openalex.org/W4401700355,https://doi.org/10.1007/s10772-024-10121-9,Efficiency-oriented approaches for self-supervised speech representation learning,,"['https://openalex.org/W4386566373', 'https://openalex.org/W2193413348', 'https://openalex.org/W3217767527', 'https://openalex.org/W4285595665', 'https://openalex.org/W3213029956', 'https://openalex.org/W569478347', 'https://openalex.org/W4311724836', 'https://openalex.org/W4221145109', 'https://openalex.org/W4372266670', 'https://openalex.org/W2981413347', 'https://openalex.org/W6778883912', 'https://openalex.org/W3203140070', 'https://openalex.org/W4380558325', 'https://openalex.org/W3005680577', 'https://openalex.org/W3209984917', 'https://openalex.org/W2982568434', 'https://openalex.org/W6783944145', 'https://openalex.org/W2794753807', 'https://openalex.org/W3198429080', 'https://openalex.org/W4281758439', 'https://openalex.org/W2896457183', 'https://openalex.org/W3187244867', 'https://openalex.org/W3207924272', 'https://openalex.org/W3158793689', 'https://openalex.org/W4296070200', 'https://openalex.org/W4319862456', 'https://openalex.org/W2144499799', 'https://openalex.org/W3035060554', 'https://openalex.org/W3097777922', 'https://openalex.org/W3209059054', 'https://openalex.org/W6796581206', 'https://openalex.org/W4221162932', 'https://openalex.org/W6729448088', 'https://openalex.org/W2995181338', 'https://openalex.org/W3166140588', 'https://openalex.org/W3161223924', 'https://openalex.org/W3166440012', 'https://openalex.org/W3008525923', 'https://openalex.org/W4297841557', 'https://openalex.org/W3162090017', 'https://openalex.org/W4205991051', 'https://openalex.org/W3174770825', 'https://openalex.org/W4391021746', 'https://openalex.org/W4377111648', 'https://openalex.org/W4385570233', 'https://openalex.org/W6745245109', 'https://openalex.org/W4281492411', 'https://openalex.org/W4372342360', 'https://openalex.org/W3110458199', 'https://openalex.org/W1494198834', 'https://openalex.org/W4402112522', 'https://openalex.org/W4385807463', 'https://openalex.org/W3015995734', 'https://openalex.org/W2936774411', 'https://openalex.org/W4226380987', 'https://openalex.org/W4375869259', 'https://openalex.org/W4375869065', 'https://openalex.org/W4378767266', 'https://openalex.org/W2981852735', 'https://openalex.org/W2794209590', 'https://openalex.org/W3136987292', 'https://openalex.org/W3131922516', 'https://openalex.org/W3197411683', 'https://openalex.org/W3200887081', 'https://openalex.org/W3166702123', 'https://openalex.org/W2973049979', 'https://openalex.org/W2062164080', 'https://openalex.org/W3155162503', 'https://openalex.org/W3039348175', 'https://openalex.org/W4306887121', 'https://openalex.org/W4282961290', 'https://openalex.org/W3085139254', 'https://openalex.org/W4312847199', 'https://openalex.org/W3091156884', 'https://openalex.org/W4297841794', 'https://openalex.org/W4224821750', 'https://openalex.org/W3205533980', 'https://openalex.org/W2981857663', 'https://openalex.org/W2923014074', 'https://openalex.org/W3125498921', 'https://openalex.org/W3200129129', 'https://openalex.org/W6774054309', 'https://openalex.org/W3208860256', 'https://openalex.org/W3035160371', 'https://openalex.org/W2899423466', 'https://openalex.org/W3197580070', 'https://openalex.org/W4297841659', 'https://openalex.org/W2798858969', 'https://openalex.org/W3176828726', 'https://openalex.org/W2964084166', 'https://openalex.org/W3115894062', 'https://openalex.org/W2584641794', 'https://openalex.org/W3104896896', 'https://openalex.org/W3197259906', 'https://openalex.org/W4385823192', 'https://openalex.org/W4385823182', 'https://openalex.org/W2963425185', 'https://openalex.org/W3189296823', 'https://openalex.org/W4297841520']",2024-08-19
https://openalex.org/W4391745456,https://doi.org/10.1186/s13636-024-00329-7,"Deep learning-based expressive speech synthesis: a systematic review of approaches, challenges, and resources",,"['https://openalex.org/W2102003408', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6632668414', 'https://openalex.org/W4297536219', 'https://openalex.org/W4313155892', 'https://openalex.org/W3153370059', 'https://openalex.org/W2976159681', 'https://openalex.org/W2043003570', 'https://openalex.org/W3184592700', 'https://openalex.org/W6607786901', 'https://openalex.org/W4323896824', 'https://openalex.org/W4372266971', 'https://openalex.org/W6602226494', 'https://openalex.org/W4372346370', 'https://openalex.org/W4372260289', 'https://openalex.org/W6600274734', 'https://openalex.org/W6600741150', 'https://openalex.org/W4319985616', 'https://openalex.org/W6603173416', 'https://openalex.org/W3010916717', 'https://openalex.org/W3008691130', 'https://openalex.org/W2976532777', 'https://openalex.org/W3163339651', 'https://openalex.org/W3112336664', 'https://openalex.org/W3146550708', 'https://openalex.org/W3135644023', 'https://openalex.org/W4210777104', 'https://openalex.org/W4226421465', 'https://openalex.org/W4224928640', 'https://openalex.org/W4281562038', 'https://openalex.org/W4295036296', 'https://openalex.org/W4294311176', 'https://openalex.org/W4283689139', 'https://openalex.org/W2890287821', 'https://openalex.org/W2802968248', 'https://openalex.org/W2896886065', 'https://openalex.org/W3097795905', 'https://openalex.org/W3095401840', 'https://openalex.org/W3158374895', 'https://openalex.org/W3162791003', 'https://openalex.org/W3160844600', 'https://openalex.org/W3197704090', 'https://openalex.org/W4221165968', 'https://openalex.org/W3207354624', 'https://openalex.org/W3200756692', 'https://openalex.org/W4226487411', 'https://openalex.org/W4287212799', 'https://openalex.org/W2964138190', 'https://openalex.org/W2937870435', 'https://openalex.org/W2966387353', 'https://openalex.org/W3015841875', 'https://openalex.org/W3198123658', 'https://openalex.org/W4214883111', 'https://openalex.org/W4224926192', 'https://openalex.org/W4296069154', 'https://openalex.org/W4297841505', 'https://openalex.org/W4283771593', 'https://openalex.org/W2785364623', 'https://openalex.org/W2793479148', 'https://openalex.org/W2972956431', 'https://openalex.org/W3095491807', 'https://openalex.org/W3107262928', 'https://openalex.org/W4296068776', 'https://openalex.org/W3196843885', 'https://openalex.org/W3203313352', 'https://openalex.org/W3127721277', 'https://openalex.org/W2962691331', 'https://openalex.org/W2904459034', 'https://openalex.org/W3003284013', 'https://openalex.org/W4224924088', 'https://openalex.org/W2894755406', 'https://openalex.org/W2890606114', 'https://openalex.org/W2941649920', 'https://openalex.org/W3032080156', 'https://openalex.org/W3096830101', 'https://openalex.org/W3195366750', 'https://openalex.org/W4297841795', 'https://openalex.org/W6778883912', 'https://openalex.org/W2294130536', 'https://openalex.org/W3168542456', 'https://openalex.org/W3198791321', 'https://openalex.org/W3196866316', 'https://openalex.org/W2885800352', 'https://openalex.org/W3197216873', 'https://openalex.org/W4308273487', 'https://openalex.org/W3198104520', 'https://openalex.org/W3174285493', 'https://openalex.org/W3205316472', 'https://openalex.org/W3161822901', 'https://openalex.org/W4225306880', 'https://openalex.org/W3097892637', 'https://openalex.org/W2914049472', 'https://openalex.org/W3160329778', 'https://openalex.org/W4210723584', 'https://openalex.org/W3139170550', 'https://openalex.org/W3047443559', 'https://openalex.org/W3081488690', 'https://openalex.org/W3152136404', 'https://openalex.org/W4309874973', 'https://openalex.org/W3163003432', 'https://openalex.org/W3015645837', 'https://openalex.org/W3161492781', 'https://openalex.org/W4221167022', 'https://openalex.org/W3198609073', 'https://openalex.org/W4297841867', 'https://openalex.org/W3015796413', 'https://openalex.org/W3205631867', 'https://openalex.org/W2889092828', 'https://openalex.org/W3095505419', 'https://openalex.org/W3016021263', 'https://openalex.org/W3027876491', 'https://openalex.org/W3198311967', 'https://openalex.org/W3197113339', 'https://openalex.org/W3161113899', 'https://openalex.org/W3202098869', 'https://openalex.org/W3161732385', 'https://openalex.org/W3022876224', 'https://openalex.org/W2990883660', 'https://openalex.org/W3194208059', 'https://openalex.org/W3007067948', 'https://openalex.org/W4293523272', 'https://openalex.org/W2889141918', 'https://openalex.org/W4225300652', 'https://openalex.org/W3015212100', 'https://openalex.org/W3097003111', 'https://openalex.org/W2090777335', 'https://openalex.org/W2471520273', 'https://openalex.org/W2760103357', 'https://openalex.org/W2475287302', 'https://openalex.org/W2752796333', 'https://openalex.org/W6600473098', 'https://openalex.org/W2752782242', 'https://openalex.org/W2572730214', 'https://openalex.org/W3209059054', 'https://openalex.org/W6634817459', 'https://openalex.org/W6600274115', 'https://openalex.org/W3184324824', 'https://openalex.org/W6756821666', 'https://openalex.org/W6600013530', 'https://openalex.org/W2984342455', 'https://openalex.org/W6600424091', 'https://openalex.org/W2550497374', 'https://openalex.org/W4205742757', 'https://openalex.org/W2972359262', 'https://openalex.org/W4243499798', 'https://openalex.org/W2149628368', 'https://openalex.org/W3100400819']",2024-02-12
https://openalex.org/W4221167022,https://doi.org/10.1109/icassp43922.2022.9746323,Unsupervised Word-Level Prosody Tagging for Controllable Speech Synthesis,"Although word-level prosody modeling in neural text-to-speech (TTS) has been investigated in recent research for diverse speech synthesis, it is still challenging to control speech synthesis manually without a specific reference. This is largely due to lack of word-level prosody tags. In this work, we propose a novel approach for unsupervised word-level prosody tagging with two stages, where we first group the words into different types with a decision tree according to their phonetic content and then cluster the prosodies using GMM within each type of words separately. This design is based on the assumption that the prosodies of different type of words, such as long or short words, should be tagged with different label sets. Furthermore, a TTS system with the derived word-level prosody tags is trained for controllable speech synthesis. Experiments on LJSpeech show that the TTS model trained with word-level prosody tags not only achieves better naturalness than a typical FastSpeech2 model, but also gains the ability to manipulate word-level prosody.","['https://openalex.org/W3163003432', 'https://openalex.org/W2962691331', 'https://openalex.org/W6755300632', 'https://openalex.org/W2973158936', 'https://openalex.org/W3015440759', 'https://openalex.org/W3151450932', 'https://openalex.org/W3095491807', 'https://openalex.org/W3016021263', 'https://openalex.org/W3216941316', 'https://openalex.org/W3197216873', 'https://openalex.org/W2964138190', 'https://openalex.org/W1072663298', 'https://openalex.org/W3160844600', 'https://openalex.org/W6768443183', 'https://openalex.org/W3170320568', 'https://openalex.org/W3196843885', 'https://openalex.org/W2537421476', 'https://openalex.org/W2161807040', 'https://openalex.org/W2124551647', 'https://openalex.org/W6712610176', 'https://openalex.org/W6603582338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W6767111847', 'https://openalex.org/W86969866', 'https://openalex.org/W2970006822', 'https://openalex.org/W2400063444', 'https://openalex.org/W4289383906', 'https://openalex.org/W2996573371']",2022-04-27
https://openalex.org/W4372263384,https://doi.org/10.1109/icassp49357.2023.10096255,Investigating Content-Aware Neural Text-to-Speech MOS Prediction Using Prosodic and Linguistic Features,"Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are beneficial in the MOS prediction task, by improving the predicted MOS scores' correlation with the ground truths, both at utterance-level and system-level predictions.","['https://openalex.org/W3202278141', 'https://openalex.org/W3198270377', 'https://openalex.org/W4225956675', 'https://openalex.org/W4296068974', 'https://openalex.org/W3095389792', 'https://openalex.org/W3097206152', 'https://openalex.org/W3207932315', 'https://openalex.org/W6865327743', 'https://openalex.org/W2105854852', 'https://openalex.org/W4296068832', 'https://openalex.org/W4223503417', 'https://openalex.org/W4297570641', 'https://openalex.org/W3196225973', 'https://openalex.org/W2895146711', 'https://openalex.org/W3163003432', 'https://openalex.org/W6755207826', 'https://openalex.org/W2963521945', 'https://openalex.org/W6917585676', 'https://openalex.org/W6676245417', 'https://openalex.org/W6776148200', 'https://openalex.org/W2970641574', 'https://openalex.org/W6784545093', 'https://openalex.org/W2972394484', 'https://openalex.org/W2963035245', 'https://openalex.org/W3161558238', 'https://openalex.org/W2511311723', 'https://openalex.org/W70888257', 'https://openalex.org/W2963403924', 'https://openalex.org/W6729924827', 'https://openalex.org/W2896457183', 'https://openalex.org/W4395681663', 'https://openalex.org/W2557915412', 'https://openalex.org/W2107831318', 'https://openalex.org/W3016473712', 'https://openalex.org/W3091928890']",2023-05-05
https://openalex.org/W4309874973,https://doi.org/10.1016/j.specom.2022.11.006,Controllable speech synthesis by learning discrete phoneme-level prosodic representations,,"['https://openalex.org/W3096437652', 'https://openalex.org/W3197541421', 'https://openalex.org/W6769754352', 'https://openalex.org/W2778460379', 'https://openalex.org/W6676245417', 'https://openalex.org/W1525954971', 'https://openalex.org/W6795086571', 'https://openalex.org/W6679448200', 'https://openalex.org/W3151450932', 'https://openalex.org/W6801156636', 'https://openalex.org/W6675938391', 'https://openalex.org/W3097290232', 'https://openalex.org/W3197216873', 'https://openalex.org/W3095389792', 'https://openalex.org/W6795598533', 'https://openalex.org/W3197830622', 'https://openalex.org/W6795122432', 'https://openalex.org/W2977311057', 'https://openalex.org/W6784866656', 'https://openalex.org/W6630334170', 'https://openalex.org/W6785219129', 'https://openalex.org/W3022876224', 'https://openalex.org/W2896457183', 'https://openalex.org/W6801592945', 'https://openalex.org/W2973158936', 'https://openalex.org/W6676417963', 'https://openalex.org/W3034949308', 'https://openalex.org/W2964138190', 'https://openalex.org/W3193323418', 'https://openalex.org/W3196843885', 'https://openalex.org/W2938102059', 'https://openalex.org/W2792995953', 'https://openalex.org/W3097892637', 'https://openalex.org/W4395958166', 'https://openalex.org/W6778823374', 'https://openalex.org/W2946200149', 'https://openalex.org/W2974194285', 'https://openalex.org/W2964243274', 'https://openalex.org/W162654330', 'https://openalex.org/W2795109282', 'https://openalex.org/W6753656201', 'https://openalex.org/W3015440759', 'https://openalex.org/W3016021263', 'https://openalex.org/W3152136404', 'https://openalex.org/W3015645837', 'https://openalex.org/W3163003432', 'https://openalex.org/W3095459301', 'https://openalex.org/W2952269766', 'https://openalex.org/W6794944897', 'https://openalex.org/W3198082505', 'https://openalex.org/W2963609956', 'https://openalex.org/W2794490148', 'https://openalex.org/W2904459034', 'https://openalex.org/W2972473628', 'https://openalex.org/W3198311967', 'https://openalex.org/W3110858053', 'https://openalex.org/W2948238043', 'https://openalex.org/W3101882441', 'https://openalex.org/W3091928890', 'https://openalex.org/W3205631867', 'https://openalex.org/W1810943226', 'https://openalex.org/W2948211236', 'https://openalex.org/W2991417167']",2022-11-24
https://openalex.org/W3193323418,https://doi.org/10.21437/ssw.2021-21,Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control,"In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.","['https://openalex.org/W2095705004', 'https://openalex.org/W2972473628', 'https://openalex.org/W2946639766', 'https://openalex.org/W1749799732', 'https://openalex.org/W2963609956', 'https://openalex.org/W3082910224', 'https://openalex.org/W3097152652', 'https://openalex.org/W2148228080', 'https://openalex.org/W2409027918', 'https://openalex.org/W29794711', 'https://openalex.org/W1525613233', 'https://openalex.org/W2150658333', 'https://openalex.org/W2964243274', 'https://openalex.org/W3216401400', 'https://openalex.org/W2168510624', 'https://openalex.org/W2536210537', 'https://openalex.org/W3163003432', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015645837', 'https://openalex.org/W2973046048', 'https://openalex.org/W2516406502', 'https://openalex.org/W3095459301', 'https://openalex.org/W3021164770', 'https://openalex.org/W3095389792', 'https://openalex.org/W3081279708', 'https://openalex.org/W2778460379', 'https://openalex.org/W3096437652', 'https://openalex.org/W3133525064', 'https://openalex.org/W3095948607', 'https://openalex.org/W1525954971']",2021-08-24
https://openalex.org/W3201257217,https://doi.org/10.1007/978-3-030-87802-3_11,Improved Prosodic Clustering for Multispeaker and Speaker-Independent Phoneme-Level Prosody Control,,"['https://openalex.org/W3096437652', 'https://openalex.org/W2778460379', 'https://openalex.org/W1525954971', 'https://openalex.org/W3151450932', 'https://openalex.org/W3097290232', 'https://openalex.org/W6601587288', 'https://openalex.org/W3095389792', 'https://openalex.org/W2897548994', 'https://openalex.org/W3022876224', 'https://openalex.org/W2973158936', 'https://openalex.org/W3198575134', 'https://openalex.org/W3128170746', 'https://openalex.org/W2964138190', 'https://openalex.org/W2938102059', 'https://openalex.org/W2792995953', 'https://openalex.org/W3097892637', 'https://openalex.org/W2974194285', 'https://openalex.org/W2964243274', 'https://openalex.org/W2795109282', 'https://openalex.org/W3015440759', 'https://openalex.org/W3016021263', 'https://openalex.org/W3015645837', 'https://openalex.org/W3163003432', 'https://openalex.org/W3095459301', 'https://openalex.org/W3198082505', 'https://openalex.org/W2963609956', 'https://openalex.org/W2794490148', 'https://openalex.org/W3095505419', 'https://openalex.org/W3081565196', 'https://openalex.org/W2904459034', 'https://openalex.org/W2972473628', 'https://openalex.org/W4295731579', 'https://openalex.org/W2948238043', 'https://openalex.org/W3110858053', 'https://openalex.org/W4289383906', 'https://openalex.org/W3134835907', 'https://openalex.org/W2963927338', 'https://openalex.org/W2952269766', 'https://openalex.org/W2963272440', 'https://openalex.org/W2991417167', 'https://openalex.org/W3127721277', 'https://openalex.org/W3105004657', 'https://openalex.org/W4288079858', 'https://openalex.org/W2963568578', 'https://openalex.org/W2945544731', 'https://openalex.org/W3021117141', 'https://openalex.org/W3126404598', 'https://openalex.org/W2963691546', 'https://openalex.org/W3101882441', 'https://openalex.org/W3004823919', 'https://openalex.org/W4214968481', 'https://openalex.org/W3152136404']",2021-01-01
https://openalex.org/W3135588948,https://doi.org/10.1109/jproc.2021.3058954,Toward Causal Representation Learning,"The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.","['https://openalex.org/W6757797181', 'https://openalex.org/W6623861543', 'https://openalex.org/W6766883460', 'https://openalex.org/W2963781688', 'https://openalex.org/W2146444479', 'https://openalex.org/W6766313662', 'https://openalex.org/W6747514548', 'https://openalex.org/W6768853884', 'https://openalex.org/W1785969240', 'https://openalex.org/W2145339207', 'https://openalex.org/W6677556709', 'https://openalex.org/W6683267464', 'https://openalex.org/W6639134244', 'https://openalex.org/W6677477928', 'https://openalex.org/W6752737978', 'https://openalex.org/W2000869402', 'https://openalex.org/W2887788426', 'https://openalex.org/W6632480798', 'https://openalex.org/W6756341295', 'https://openalex.org/W2048679005', 'https://openalex.org/W2530375712', 'https://openalex.org/W6758970741', 'https://openalex.org/W6757235116', 'https://openalex.org/W2030617372', 'https://openalex.org/W2584044794', 'https://openalex.org/W2123969847', 'https://openalex.org/W6778883912', 'https://openalex.org/W2965742591', 'https://openalex.org/W6678276431', 'https://openalex.org/W6770050343', 'https://openalex.org/W6782948914', 'https://openalex.org/W6746569983', 'https://openalex.org/W6758857762', 'https://openalex.org/W6682658890', 'https://openalex.org/W6784857807', 'https://openalex.org/W6678041798', 'https://openalex.org/W2143891888', 'https://openalex.org/W6681187235', 'https://openalex.org/W2790376986', 'https://openalex.org/W6773004738', 'https://openalex.org/W6737818077', 'https://openalex.org/W2996037775', 'https://openalex.org/W2842511635', 'https://openalex.org/W6776057429', 'https://openalex.org/W2883386984', 'https://openalex.org/W6736166589', 'https://openalex.org/W2156557681', 'https://openalex.org/W6683994600', 'https://openalex.org/W2123399796', 'https://openalex.org/W2740437707', 'https://openalex.org/W6767830821', 'https://openalex.org/W6681517296', 'https://openalex.org/W6759517920', 'https://openalex.org/W1482778976', 'https://openalex.org/W6757835162', 'https://openalex.org/W6762334975', 'https://openalex.org/W2894728917', 'https://openalex.org/W6748353538', 'https://openalex.org/W6772619266', 'https://openalex.org/W6776438516', 'https://openalex.org/W6748285597', 'https://openalex.org/W4255871753', 'https://openalex.org/W192920577', 'https://openalex.org/W4388323202', 'https://openalex.org/W2919115771', 'https://openalex.org/W6780454748', 'https://openalex.org/W2101355568', 'https://openalex.org/W6784941698', 'https://openalex.org/W6776218486', 'https://openalex.org/W6633773484', 'https://openalex.org/W6755207826', 'https://openalex.org/W2108598243', 'https://openalex.org/W30853191', 'https://openalex.org/W6758420182', 'https://openalex.org/W6754927192', 'https://openalex.org/W2900954917', 'https://openalex.org/W6757212879', 'https://openalex.org/W6787532554', 'https://openalex.org/W2963528347', 'https://openalex.org/W1640882231', 'https://openalex.org/W6779809370', 'https://openalex.org/W4248181943', 'https://openalex.org/W6751846514', 'https://openalex.org/W6748923678', 'https://openalex.org/W6765285020', 'https://openalex.org/W6773762762', 'https://openalex.org/W3081175302', 'https://openalex.org/W6751831877', 'https://openalex.org/W2949736877', 'https://openalex.org/W6763088173', 'https://openalex.org/W6756663807', 'https://openalex.org/W6756192261', 'https://openalex.org/W6779879114', 'https://openalex.org/W1479807131', 'https://openalex.org/W6736803224', 'https://openalex.org/W6774314701', 'https://openalex.org/W6692812177', 'https://openalex.org/W2913340405', 'https://openalex.org/W6730420370', 'https://openalex.org/W6750069892', 'https://openalex.org/W6757469721', 'https://openalex.org/W6619377094', 'https://openalex.org/W1949013791', 'https://openalex.org/W2096388912', 'https://openalex.org/W6756444276', 'https://openalex.org/W6763012920', 'https://openalex.org/W6675134017', 'https://openalex.org/W2088170048', 'https://openalex.org/W6718795669', 'https://openalex.org/W6746842142', 'https://openalex.org/W6780588726', 'https://openalex.org/W6748445624', 'https://openalex.org/W6618771953', 'https://openalex.org/W6631792404', 'https://openalex.org/W6762970624', 'https://openalex.org/W2157050842', 'https://openalex.org/W2905810301', 'https://openalex.org/W2161086430', 'https://openalex.org/W6736057607', 'https://openalex.org/W6737849119', 'https://openalex.org/W6791012587', 'https://openalex.org/W4246455455', 'https://openalex.org/W2294065709', 'https://openalex.org/W2963535485', 'https://openalex.org/W6840451911', 'https://openalex.org/W6755672337', 'https://openalex.org/W6761731747', 'https://openalex.org/W6679709731', 'https://openalex.org/W2110139691', 'https://openalex.org/W2990645394', 'https://openalex.org/W6779326418', 'https://openalex.org/W2963062793', 'https://openalex.org/W6681474176', 'https://openalex.org/W1638081485', 'https://openalex.org/W6763552992', 'https://openalex.org/W6679560069', 'https://openalex.org/W6750253780', 'https://openalex.org/W6639568328', 'https://openalex.org/W6755223120', 'https://openalex.org/W6751973938', 'https://openalex.org/W2128152674', 'https://openalex.org/W6780464492', 'https://openalex.org/W6746092580', 'https://openalex.org/W6640425456', 'https://openalex.org/W6764422260', 'https://openalex.org/W6768501859', 'https://openalex.org/W2143612262', 'https://openalex.org/W6759669191', 'https://openalex.org/W6787728148', 'https://openalex.org/W6771638272', 'https://openalex.org/W3036205238', 'https://openalex.org/W2083211221', 'https://openalex.org/W6739365718', 'https://openalex.org/W2044326486', 'https://openalex.org/W2891765548', 'https://openalex.org/W3048817558', 'https://openalex.org/W6747786891', 'https://openalex.org/W6725082424', 'https://openalex.org/W6740743664', 'https://openalex.org/W2968131493', 'https://openalex.org/W6753209683', 'https://openalex.org/W3007913393', 'https://openalex.org/W6738893770', 'https://openalex.org/W1863227302', 'https://openalex.org/W2056833498', 'https://openalex.org/W2470207857', 'https://openalex.org/W6754733991', 'https://openalex.org/W6682361391', 'https://openalex.org/W2903703378', 'https://openalex.org/W6770998539', 'https://openalex.org/W6608164685', 'https://openalex.org/W6780793251', 'https://openalex.org/W6747014089', 'https://openalex.org/W6788072495', 'https://openalex.org/W2107726111', 'https://openalex.org/W6748646278', 'https://openalex.org/W6639052459', 'https://openalex.org/W2083689856', 'https://openalex.org/W2511484725', 'https://openalex.org/W6779372675', 'https://openalex.org/W6755996536', 'https://openalex.org/W6738870502', 'https://openalex.org/W6768934218', 'https://openalex.org/W6762730029', 'https://openalex.org/W2257979135', 'https://openalex.org/W6769616852', 'https://openalex.org/W6731227521', 'https://openalex.org/W6602568931', 'https://openalex.org/W2156163116', 'https://openalex.org/W2123081785', 'https://openalex.org/W6748320467', 'https://openalex.org/W6748223763', 'https://openalex.org/W6752279500', 'https://openalex.org/W6775298725', 'https://openalex.org/W6685100241', 'https://openalex.org/W6684191040', 'https://openalex.org/W6765456200', 'https://openalex.org/W6744110554', 'https://openalex.org/W2089858332', 'https://openalex.org/W2963305465', 'https://openalex.org/W2078804487', 'https://openalex.org/W6784313279', 'https://openalex.org/W6775715841', 'https://openalex.org/W6753518013', 'https://openalex.org/W6756923131', 'https://openalex.org/W6757521438', 'https://openalex.org/W6634417307', 'https://openalex.org/W6762931136', 'https://openalex.org/W6602916700', 'https://openalex.org/W2962843773', 'https://openalex.org/W6637162671', 'https://openalex.org/W1943063538', 'https://openalex.org/W4247772586', 'https://openalex.org/W6757555829', 'https://openalex.org/W2686124727', 'https://openalex.org/W2795857097', 'https://openalex.org/W2900466252', 'https://openalex.org/W2194775991', 'https://openalex.org/W6770717842', 'https://openalex.org/W4252036282', 'https://openalex.org/W2963026768', 'https://openalex.org/W6744627333', 'https://openalex.org/W2087883256', 'https://openalex.org/W6630846680', 'https://openalex.org/W2972809076', 'https://openalex.org/W2786615588', 'https://openalex.org/W3034321244', 'https://openalex.org/W6771825654', 'https://openalex.org/W2946896833', 'https://openalex.org/W6635461197', 'https://openalex.org/W3093469769', 'https://openalex.org/W6761037857', 'https://openalex.org/W2982316857', 'https://openalex.org/W6767228950', 'https://openalex.org/W6686823217', 'https://openalex.org/W6675601395', 'https://openalex.org/W6729508183', 'https://openalex.org/W3150893739', 'https://openalex.org/W6751796012', 'https://openalex.org/W2012762214', 'https://openalex.org/W2059100041', 'https://openalex.org/W6696344163', 'https://openalex.org/W6765715912', 'https://openalex.org/W3004483087', 'https://openalex.org/W6760368646', 'https://openalex.org/W6684389209', 'https://openalex.org/W6740361012', 'https://openalex.org/W2772580566', 'https://openalex.org/W2192422777', 'https://openalex.org/W6635962543', 'https://openalex.org/W2147881172', 'https://openalex.org/W6638661483', 'https://openalex.org/W2947782760', 'https://openalex.org/W3020457430', 'https://openalex.org/W6757712578', 'https://openalex.org/W6762284573', 'https://openalex.org/W6746445604', 'https://openalex.org/W6771823222', 'https://openalex.org/W2786385136', 'https://openalex.org/W3101372253', 'https://openalex.org/W4287641395', 'https://openalex.org/W4294016603', 'https://openalex.org/W3091252828', 'https://openalex.org/W2970364795', 'https://openalex.org/W4287777352', 'https://openalex.org/W2972944164', 'https://openalex.org/W4288019212', 'https://openalex.org/W2657268502', 'https://openalex.org/W2914607694', 'https://openalex.org/W3035090613', 'https://openalex.org/W2971202257', 'https://openalex.org/W2963608118', 'https://openalex.org/W2963704132', 'https://openalex.org/W3005031238', 'https://openalex.org/W2981628132', 'https://openalex.org/W2809146571', 'https://openalex.org/W617547846', 'https://openalex.org/W2963901280', 'https://openalex.org/W1883420340', 'https://openalex.org/W2964071299', 'https://openalex.org/W4289236818', 'https://openalex.org/W2963341956', 'https://openalex.org/W3113055895', 'https://openalex.org/W2163605009', 'https://openalex.org/W2945825390', 'https://openalex.org/W2995993311', 'https://openalex.org/W3034616174', 'https://openalex.org/W2785542264', 'https://openalex.org/W2953494151', 'https://openalex.org/W2111494971', 'https://openalex.org/W2952915411', 'https://openalex.org/W1945616565', 'https://openalex.org/W879220392', 'https://openalex.org/W3103912228', 'https://openalex.org/W3083484445', 'https://openalex.org/W24933509', 'https://openalex.org/W3119651796', 'https://openalex.org/W2962938168', 'https://openalex.org/W2963588899', 'https://openalex.org/W3203297510', 'https://openalex.org/W3035282577', 'https://openalex.org/W2184746314', 'https://openalex.org/W4294568351', 'https://openalex.org/W2140916555', 'https://openalex.org/W2955368974', 'https://openalex.org/W2951428711', 'https://openalex.org/W2163922914', 'https://openalex.org/W2288522845', 'https://openalex.org/W2996536863', 'https://openalex.org/W3101660901', 'https://openalex.org/W2907354314', 'https://openalex.org/W2907650245', 'https://openalex.org/W2806905826', 'https://openalex.org/W2963717490', 'https://openalex.org/W2132547334', 'https://openalex.org/W2771917759', 'https://openalex.org/W2963084105', 'https://openalex.org/W3034814102', 'https://openalex.org/W2995191098', 'https://openalex.org/W3098017640', 'https://openalex.org/W2151226328', 'https://openalex.org/W2945033152', 'https://openalex.org/W4294611718', 'https://openalex.org/W2463241543', 'https://openalex.org/W3112027221', 'https://openalex.org/W2613573919', 'https://openalex.org/W3035380504', 'https://openalex.org/W2118688707', 'https://openalex.org/W3102696055', 'https://openalex.org/W2944892105', 'https://openalex.org/W2552391307', 'https://openalex.org/W2907520152', 'https://openalex.org/W4287802874', 'https://openalex.org/W2624319794', 'https://openalex.org/W2949737767', 'https://openalex.org/W2606047872', 'https://openalex.org/W4293376383', 'https://openalex.org/W2805516822', 'https://openalex.org/W2122124659', 'https://openalex.org/W2952484912', 'https://openalex.org/W2122410182', 'https://openalex.org/W2990376402', 'https://openalex.org/W4288281368', 'https://openalex.org/W2753738274', 'https://openalex.org/W3021164770', 'https://openalex.org/W4295744821', 'https://openalex.org/W2963990127', 'https://openalex.org/W2992604033', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962988477', 'https://openalex.org/W2886503111', 'https://openalex.org/W2903538854', 'https://openalex.org/W3126596751', 'https://openalex.org/W3014343088', 'https://openalex.org/W4297808394', 'https://openalex.org/W3091670682', 'https://openalex.org/W2989701728', 'https://openalex.org/W4287554891', 'https://openalex.org/W2567374473', 'https://openalex.org/W3016824580', 'https://openalex.org/W2785519580', 'https://openalex.org/W3005680577', 'https://openalex.org/W2965354194', 'https://openalex.org/W4288349082', 'https://openalex.org/W3118210634', 'https://openalex.org/W3121171074', 'https://openalex.org/W3034445277', 'https://openalex.org/W2787337315', 'https://openalex.org/W4288335913', 'https://openalex.org/W2119471399', 'https://openalex.org/W2950662112', 'https://openalex.org/W2998084403', 'https://openalex.org/W2971127577', 'https://openalex.org/W2951021014', 'https://openalex.org/W1833557697', 'https://openalex.org/W2904453761', 'https://openalex.org/W2165582599', 'https://openalex.org/W2999417551', 'https://openalex.org/W2753845591', 'https://openalex.org/W2963293533', 'https://openalex.org/W2961540362', 'https://openalex.org/W2963445340', 'https://openalex.org/W2970395278', 'https://openalex.org/W4293923434', 'https://openalex.org/W2996904338', 'https://openalex.org/W1826032005', 'https://openalex.org/W2971310236', 'https://openalex.org/W2148603752', 'https://openalex.org/W2963453196', 'https://openalex.org/W1564947197', 'https://openalex.org/W609741286', 'https://openalex.org/W3134658164', 'https://openalex.org/W2617547828', 'https://openalex.org/W1532777422', 'https://openalex.org/W3124149160', 'https://openalex.org/W1850984366', 'https://openalex.org/W1515756431', 'https://openalex.org/W2105897901', 'https://openalex.org/W3097217077', 'https://openalex.org/W4288617013', 'https://openalex.org/W3100756829', 'https://openalex.org/W2962867885', 'https://openalex.org/W2145544165', 'https://openalex.org/W1544444001', 'https://openalex.org/W2949383815', 'https://openalex.org/W4288287305', 'https://openalex.org/W2951815267', 'https://openalex.org/W2788106150', 'https://openalex.org/W2971274354', 'https://openalex.org/W4361805498', 'https://openalex.org/W4300971714', 'https://openalex.org/W4298170872', 'https://openalex.org/W2900677074', 'https://openalex.org/W4297814184', 'https://openalex.org/W2153929442', 'https://openalex.org/W3000266068', 'https://openalex.org/W2903320418', 'https://openalex.org/W2604763608', 'https://openalex.org/W3176732771', 'https://openalex.org/W2144020560', 'https://openalex.org/W2623293810', 'https://openalex.org/W2963207607', 'https://openalex.org/W2962841471', 'https://openalex.org/W3132522380', 'https://openalex.org/W2604626881', 'https://openalex.org/W3014503118', 'https://openalex.org/W4288404646', 'https://openalex.org/W1515851193', 'https://openalex.org/W2804691295', 'https://openalex.org/W3037784242', 'https://openalex.org/W2118972902', 'https://openalex.org/W2788651580', 'https://openalex.org/W2963104724', 'https://openalex.org/W2884942463', 'https://openalex.org/W2990408532', 'https://openalex.org/W2996117162', 'https://openalex.org/W2788624168', 'https://openalex.org/W62639009', 'https://openalex.org/W2970315086', 'https://openalex.org/W3035524453', 'https://openalex.org/W2921112192', 'https://openalex.org/W2907502844', 'https://openalex.org/W2964124968', 'https://openalex.org/W2559857813', 'https://openalex.org/W2990366588', 'https://openalex.org/W3037171980', 'https://openalex.org/W3042860357', 'https://openalex.org/W2893363236', 'https://openalex.org/W4308831279', 'https://openalex.org/W1845812186', 'https://openalex.org/W2898158860', 'https://openalex.org/W3022566517', 'https://openalex.org/W2941175503', 'https://openalex.org/W2100600008', 'https://openalex.org/W4288095810', 'https://openalex.org/W2891620446', 'https://openalex.org/W4288612322', 'https://openalex.org/W1574788463', 'https://openalex.org/W4292779060', 'https://openalex.org/W2970692043', 'https://openalex.org/W2962696347', 'https://openalex.org/W3035060554', 'https://openalex.org/W2919112510', 'https://openalex.org/W2785777814', 'https://openalex.org/W2962688927', 'https://openalex.org/W4295246696', 'https://openalex.org/W2979174981', 'https://openalex.org/W70060515', 'https://openalex.org/W1673923490', 'https://openalex.org/W2911448865', 'https://openalex.org/W4287750666', 'https://openalex.org/W2909534157', 'https://openalex.org/W201532924', 'https://openalex.org/W2770758216', 'https://openalex.org/W2964153729', 'https://openalex.org/W2963216331', 'https://openalex.org/W4299704639', 'https://openalex.org/W3030163527', 'https://openalex.org/W2131489391', 'https://openalex.org/W3108961219', 'https://openalex.org/W3129410129', 'https://openalex.org/W3026172438', 'https://openalex.org/W2801890059', 'https://openalex.org/W3104668471', 'https://openalex.org/W2781942582', 'https://openalex.org/W2963076808', 'https://openalex.org/W2963370555', 'https://openalex.org/W3127433878', 'https://openalex.org/W2981030070', 'https://openalex.org/W2807242871', 'https://openalex.org/W2770604561', 'https://openalex.org/W3105282616', 'https://openalex.org/W4302028616', 'https://openalex.org/W2913314773', 'https://openalex.org/W3037655549', 'https://openalex.org/W2624871570', 'https://openalex.org/W3025013230', 'https://openalex.org/W4289288202', 'https://openalex.org/W2951655307', 'https://openalex.org/W3042608254', 'https://openalex.org/W2963464736', 'https://openalex.org/W2963521490', 'https://openalex.org/W2952610664', 'https://openalex.org/W3097981474', 'https://openalex.org/W2785961484', 'https://openalex.org/W2963604043', 'https://openalex.org/W3000034219', 'https://openalex.org/W1603640438', 'https://openalex.org/W2142016203', 'https://openalex.org/W2146531590', 'https://openalex.org/W4287644765', 'https://openalex.org/W2963907629', 'https://openalex.org/W1592735339', 'https://openalex.org/W2795888820', 'https://openalex.org/W1698155719', 'https://openalex.org/W4302423442', 'https://openalex.org/W2963060032', 'https://openalex.org/W335436902', 'https://openalex.org/W2171452427', 'https://openalex.org/W2952428244']",2021-02-26
https://openalex.org/W3140429000,https://doi.org/10.21437/interspeech.2021-475,Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,"We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.\n","['https://openalex.org/W2963618559', 'https://openalex.org/W2890983311', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963300588', 'https://openalex.org/W2097645910', 'https://openalex.org/W3095948607', 'https://openalex.org/W2944079609', 'https://openalex.org/W2995181338', 'https://openalex.org/W2292235217', 'https://openalex.org/W3096216486', 'https://openalex.org/W2972867623', 'https://openalex.org/W2964167449', 'https://openalex.org/W2935711438', 'https://openalex.org/W2940544976', 'https://openalex.org/W1494198834', 'https://openalex.org/W3210177631', 'https://openalex.org/W2808631503', 'https://openalex.org/W2120847449', 'https://openalex.org/W3021164770', 'https://openalex.org/W3096656254', 'https://openalex.org/W3148101939', 'https://openalex.org/W1885680957', 'https://openalex.org/W2115098197', 'https://openalex.org/W1498609987', 'https://openalex.org/W3093427098', 'https://openalex.org/W3016098186', 'https://openalex.org/W3099782249', 'https://openalex.org/W3095361818', 'https://openalex.org/W2130086727', 'https://openalex.org/W2527729766', 'https://openalex.org/W2842511635', 'https://openalex.org/W3025878903', 'https://openalex.org/W2924551963', 'https://openalex.org/W3096323553', 'https://openalex.org/W2949382160', 'https://openalex.org/W2750248772', 'https://openalex.org/W3163296124', 'https://openalex.org/W3098403858', 'https://openalex.org/W2963341956', 'https://openalex.org/W2775336875', 'https://openalex.org/W1959608418', 'https://openalex.org/W3160799772', 'https://openalex.org/W2114925438', 'https://openalex.org/W2963091184', 'https://openalex.org/W2107740512']",2021-08-27
https://openalex.org/W3092157356,https://doi.org/10.1007/s00521-020-05383-8,Deep learning for procedural content generation,,"['https://openalex.org/W2985068832', 'https://openalex.org/W3003404702', 'https://openalex.org/W3047102541', 'https://openalex.org/W2897177022', 'https://openalex.org/W2160237541', 'https://openalex.org/W2591385657', 'https://openalex.org/W2786877691', 'https://openalex.org/W2963395499', 'https://openalex.org/W118696858', 'https://openalex.org/W4236250754', 'https://openalex.org/W2001923012', 'https://openalex.org/W3042145171', 'https://openalex.org/W2944956221', 'https://openalex.org/W3159533820', 'https://openalex.org/W3043095564', 'https://openalex.org/W2020987898', 'https://openalex.org/W2946584082', 'https://openalex.org/W3094138986', 'https://openalex.org/W2994369270', 'https://openalex.org/W2963920537', 'https://openalex.org/W2963771763', 'https://openalex.org/W1689711448', 'https://openalex.org/W2897406577', 'https://openalex.org/W3083116736', 'https://openalex.org/W4288359814', 'https://openalex.org/W2139165539', 'https://openalex.org/W2045531194', 'https://openalex.org/W2792893218', 'https://openalex.org/W2167112043', 'https://openalex.org/W596778755', 'https://openalex.org/W3004202325', 'https://openalex.org/W2147223282', 'https://openalex.org/W2753904554', 'https://openalex.org/W2889869602', 'https://openalex.org/W2897852577', 'https://openalex.org/W2965298973', 'https://openalex.org/W3035574324', 'https://openalex.org/W3093908578', 'https://openalex.org/W3007724674', 'https://openalex.org/W3093875318', 'https://openalex.org/W2987963427', 'https://openalex.org/W51299923', 'https://openalex.org/W2889754627', 'https://openalex.org/W1834627138', 'https://openalex.org/W2944678459', 'https://openalex.org/W2996235414', 'https://openalex.org/W1966532833', 'https://openalex.org/W2103184652', 'https://openalex.org/W3089202871', 'https://openalex.org/W3006201994', 'https://openalex.org/W2975641859', 'https://openalex.org/W2963369679', 'https://openalex.org/W4293118177', 'https://openalex.org/W1966832848', 'https://openalex.org/W3093621925', 'https://openalex.org/W3047290399', 'https://openalex.org/W2318590641', 'https://openalex.org/W2605024226', 'https://openalex.org/W3087881384', 'https://openalex.org/W3094223921', 'https://openalex.org/W3084486398', 'https://openalex.org/W2028208918', 'https://openalex.org/W2076063813', 'https://openalex.org/W3038300925', 'https://openalex.org/W3014766492', 'https://openalex.org/W2131774270', 'https://openalex.org/W2910339659', 'https://openalex.org/W4206099414', 'https://openalex.org/W2982041717', 'https://openalex.org/W2224682476', 'https://openalex.org/W2031734221', 'https://openalex.org/W2002558294', 'https://openalex.org/W2534314849', 'https://openalex.org/W3083579202', 'https://openalex.org/W2976261784', 'https://openalex.org/W2152660354', 'https://openalex.org/W2103823214', 'https://openalex.org/W2546680972', 'https://openalex.org/W3088030554', 'https://openalex.org/W2975442767', 'https://openalex.org/W2111935653', 'https://openalex.org/W2902888275', 'https://openalex.org/W2753104841', 'https://openalex.org/W2963690854', 'https://openalex.org/W1964869099', 'https://openalex.org/W2168115594', 'https://openalex.org/W1572584819', 'https://openalex.org/W2964053787', 'https://openalex.org/W2061405844', 'https://openalex.org/W2888461232', 'https://openalex.org/W2964201809', 'https://openalex.org/W3093851578', 'https://openalex.org/W3093742137', 'https://openalex.org/W2754556368', 'https://openalex.org/W3087208910', 'https://openalex.org/W2113410615', 'https://openalex.org/W2792919371', 'https://openalex.org/W2590344567', 'https://openalex.org/W2246664447', 'https://openalex.org/W3003331972', 'https://openalex.org/W3048981493', 'https://openalex.org/W2980002625', 'https://openalex.org/W2327509600', 'https://openalex.org/W2546751049', 'https://openalex.org/W2577619789', 'https://openalex.org/W2579414847', 'https://openalex.org/W2982753834', 'https://openalex.org/W3038883695', 'https://openalex.org/W2730810975', 'https://openalex.org/W3004205295', 'https://openalex.org/W3106433390', 'https://openalex.org/W3094340113', 'https://openalex.org/W3106287232', 'https://openalex.org/W2973143215', 'https://openalex.org/W3008938201', 'https://openalex.org/W2173520492', 'https://openalex.org/W3005527422', 'https://openalex.org/W2577482072', 'https://openalex.org/W2950669295', 'https://openalex.org/W2734188398', 'https://openalex.org/W2136939460', 'https://openalex.org/W2015196195', 'https://openalex.org/W2099471712', 'https://openalex.org/W776164697', 'https://openalex.org/W2747543643', 'https://openalex.org/W3094063811', 'https://openalex.org/W2602100120', 'https://openalex.org/W2275430110', 'https://openalex.org/W2911959244', 'https://openalex.org/W2964223825', 'https://openalex.org/W3086313009', 'https://openalex.org/W2573316795', 'https://openalex.org/W2290393232', 'https://openalex.org/W3002587897', 'https://openalex.org/W2996531318', 'https://openalex.org/W2557283755', 'https://openalex.org/W2979077722', 'https://openalex.org/W3101594196', 'https://openalex.org/W2973774304', 'https://openalex.org/W3022534174', 'https://openalex.org/W2395774634', 'https://openalex.org/W2962947361', 'https://openalex.org/W2397490357', 'https://openalex.org/W2043334098', 'https://openalex.org/W1959608418', 'https://openalex.org/W3092157356', 'https://openalex.org/W2760726262', 'https://openalex.org/W2606712314', 'https://openalex.org/W3021164770', 'https://openalex.org/W2800206228', 'https://openalex.org/W3103183699', 'https://openalex.org/W2559655401', 'https://openalex.org/W2726805909', 'https://openalex.org/W2951438386']",2020-10-08
https://openalex.org/W3084979415,https://doi.org/10.1088/2632-2153/abd614,Deep learning in electron microscopy,"Abstract Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.","['https://openalex.org/W3033308515', 'https://openalex.org/W6741441694', 'https://openalex.org/W2981272655', 'https://openalex.org/W3004442222', 'https://openalex.org/W2942231644', 'https://openalex.org/W6763150729', 'https://openalex.org/W2919358988', 'https://openalex.org/W2767547957', 'https://openalex.org/W2800017313', 'https://openalex.org/W2919115771', 'https://openalex.org/W2076063813', 'https://openalex.org/W3033000587', 'https://openalex.org/W2923537029', 'https://openalex.org/W2972418846', 'https://openalex.org/W2964231206', 'https://openalex.org/W2917193084', 'https://openalex.org/W3004785580', 'https://openalex.org/W2781265533', 'https://openalex.org/W1821584645', 'https://openalex.org/W6637050416', 'https://openalex.org/W2111440402', 'https://openalex.org/W2522489477', 'https://openalex.org/W2257979135', 'https://openalex.org/W2014932765', 'https://openalex.org/W2785200097', 'https://openalex.org/W6738893770', 'https://openalex.org/W6669796702', 'https://openalex.org/W6696313205', 'https://openalex.org/W6752578362', 'https://openalex.org/W6743923790', 'https://openalex.org/W2158581396', 'https://openalex.org/W3125537303', 'https://openalex.org/W1988115241', 'https://openalex.org/W2137983211', 'https://openalex.org/W2103496339', 'https://openalex.org/W6754662975', 'https://openalex.org/W2511261638', 'https://openalex.org/W6718212895', 'https://openalex.org/W6717556742', 'https://openalex.org/W6763744544', 'https://openalex.org/W6759511767', 'https://openalex.org/W2907047316', 'https://openalex.org/W6768156728', 'https://openalex.org/W6763368363', 'https://openalex.org/W6740483536', 'https://openalex.org/W2884775584', 'https://openalex.org/W2896377340', 'https://openalex.org/W3004127905', 'https://openalex.org/W6755513224', 'https://openalex.org/W6684068896', 'https://openalex.org/W6654554885', 'https://openalex.org/W2980448402', 'https://openalex.org/W2900986313', 'https://openalex.org/W6754290342', 'https://openalex.org/W2884363302', 'https://openalex.org/W2952962542', 'https://openalex.org/W6684191040', 'https://openalex.org/W6764911208', 'https://openalex.org/W2923844663', 'https://openalex.org/W6772307453', 'https://openalex.org/W2967100019', 'https://openalex.org/W6754124787', 'https://openalex.org/W6790406559', 'https://openalex.org/W2621235041', 'https://openalex.org/W6772249017', 'https://openalex.org/W2986934761', 'https://openalex.org/W2960836233', 'https://openalex.org/W2980107940', 'https://openalex.org/W2901828037', 'https://openalex.org/W3027898822', 'https://openalex.org/W6779126078', 'https://openalex.org/W6762150201', 'https://openalex.org/W2006129368', 'https://openalex.org/W2972158684', 'https://openalex.org/W3203141884', 'https://openalex.org/W2956015785', 'https://openalex.org/W6681049983', 'https://openalex.org/W6728760129', 'https://openalex.org/W2887003815', 'https://openalex.org/W6640545043', 'https://openalex.org/W2128247256', 'https://openalex.org/W6675077989', 'https://openalex.org/W2809721801', 'https://openalex.org/W2917837960', 'https://openalex.org/W2150134853', 'https://openalex.org/W2167707673', 'https://openalex.org/W2133269072', 'https://openalex.org/W2594765097', 'https://openalex.org/W2162418536', 'https://openalex.org/W2046234722', 'https://openalex.org/W2928275747', 'https://openalex.org/W2138021820', 'https://openalex.org/W2007203285', 'https://openalex.org/W2037384528', 'https://openalex.org/W2125527601', 'https://openalex.org/W2158940042', 'https://openalex.org/W2147497470', 'https://openalex.org/W2132680427', 'https://openalex.org/W2111954528', 'https://openalex.org/W2117853853', 'https://openalex.org/W6766067947', 'https://openalex.org/W6718361291', 'https://openalex.org/W6726046157', 'https://openalex.org/W2060491502', 'https://openalex.org/W6683675504', 'https://openalex.org/W2044810215', 'https://openalex.org/W2056370875', 'https://openalex.org/W2763070725', 'https://openalex.org/W2083799719', 'https://openalex.org/W2119634769', 'https://openalex.org/W2160547390', 'https://openalex.org/W2153663612', 'https://openalex.org/W2794633896', 'https://openalex.org/W2810374434', 'https://openalex.org/W2043524356', 'https://openalex.org/W1936643607', 'https://openalex.org/W2126613041', 'https://openalex.org/W1958968158', 'https://openalex.org/W6650876797', 'https://openalex.org/W2032472661', 'https://openalex.org/W2992115354', 'https://openalex.org/W3014638265', 'https://openalex.org/W2422403945', 'https://openalex.org/W2758781421', 'https://openalex.org/W2192929716', 'https://openalex.org/W2112983792', 'https://openalex.org/W2071391306', 'https://openalex.org/W3004727103', 'https://openalex.org/W1986604552', 'https://openalex.org/W1977158025', 'https://openalex.org/W2424921251', 'https://openalex.org/W6773641685', 'https://openalex.org/W2338345154', 'https://openalex.org/W3035729027', 'https://openalex.org/W2086026583', 'https://openalex.org/W2369261734', 'https://openalex.org/W2039983843', 'https://openalex.org/W2904266777', 'https://openalex.org/W1990189104', 'https://openalex.org/W6655783564', 'https://openalex.org/W6687060745', 'https://openalex.org/W2595874288', 'https://openalex.org/W2602795050', 'https://openalex.org/W2106796763', 'https://openalex.org/W3004368719', 'https://openalex.org/W2912325502', 'https://openalex.org/W2028636490', 'https://openalex.org/W2163643825', 'https://openalex.org/W2980907181', 'https://openalex.org/W2971304985', 'https://openalex.org/W2577918786', 'https://openalex.org/W2783438149', 'https://openalex.org/W6756542712', 'https://openalex.org/W6753000339', 'https://openalex.org/W6749606257', 'https://openalex.org/W2950501364', 'https://openalex.org/W2764207251', 'https://openalex.org/W6736805236', 'https://openalex.org/W2508457857', 'https://openalex.org/W6742864230', 'https://openalex.org/W6726381175', 'https://openalex.org/W6755979202', 'https://openalex.org/W2969341539', 'https://openalex.org/W6784666873', 'https://openalex.org/W2955074792', 'https://openalex.org/W2942556962', 'https://openalex.org/W2965409284', 'https://openalex.org/W6784486254', 'https://openalex.org/W2983204536', 'https://openalex.org/W6749271710', 'https://openalex.org/W2977933365', 'https://openalex.org/W2743419246', 'https://openalex.org/W1916302465', 'https://openalex.org/W2973247625', 'https://openalex.org/W3014721303', 'https://openalex.org/W3013093835', 'https://openalex.org/W3048542366', 'https://openalex.org/W2935145387', 'https://openalex.org/W2774993480', 'https://openalex.org/W2648289572', 'https://openalex.org/W6774475056', 'https://openalex.org/W6773224681', 'https://openalex.org/W6776996033', 'https://openalex.org/W2921353139', 'https://openalex.org/W6762963088', 'https://openalex.org/W3027859052', 'https://openalex.org/W6769578054', 'https://openalex.org/W6772720012', 'https://openalex.org/W2793783688', 'https://openalex.org/W4250955649', 'https://openalex.org/W6773685554', 'https://openalex.org/W2944761405', 'https://openalex.org/W2101675075', 'https://openalex.org/W2999283151', 'https://openalex.org/W6768231645', 'https://openalex.org/W6758061364', 'https://openalex.org/W2887695188', 'https://openalex.org/W3006526713', 'https://openalex.org/W6786352824', 'https://openalex.org/W6684920339', 'https://openalex.org/W3011471544', 'https://openalex.org/W6750972829', 'https://openalex.org/W2912891417', 'https://openalex.org/W2792059030', 'https://openalex.org/W2575487434', 'https://openalex.org/W2346455557', 'https://openalex.org/W2466576871', 'https://openalex.org/W2117274978', 'https://openalex.org/W2914148352', 'https://openalex.org/W2610685511', 'https://openalex.org/W2801931667', 'https://openalex.org/W2911677241', 'https://openalex.org/W3010077841', 'https://openalex.org/W2153002160', 'https://openalex.org/W2147233946', 'https://openalex.org/W2075020252', 'https://openalex.org/W2496114304', 'https://openalex.org/W2011966906', 'https://openalex.org/W2605697941', 'https://openalex.org/W2986819561', 'https://openalex.org/W6676411538', 'https://openalex.org/W2963034234', 'https://openalex.org/W2784650278', 'https://openalex.org/W2571005879', 'https://openalex.org/W2486196027', 'https://openalex.org/W6665847820', 'https://openalex.org/W2142033246', 'https://openalex.org/W2109357213', 'https://openalex.org/W2539245688', 'https://openalex.org/W2278833395', 'https://openalex.org/W2897614091', 'https://openalex.org/W2593919340', 'https://openalex.org/W6769830378', 'https://openalex.org/W3009338239', 'https://openalex.org/W6762186144', 'https://openalex.org/W6746488556', 'https://openalex.org/W2914540228', 'https://openalex.org/W6769023967', 'https://openalex.org/W2949763090', 'https://openalex.org/W6761625229', 'https://openalex.org/W3005930772', 'https://openalex.org/W2962555351', 'https://openalex.org/W2105038642', 'https://openalex.org/W3028094643', 'https://openalex.org/W2012875423', 'https://openalex.org/W6684635885', 'https://openalex.org/W6756284689', 'https://openalex.org/W2963946985', 'https://openalex.org/W2622826443', 'https://openalex.org/W2322112093', 'https://openalex.org/W3011544698', 'https://openalex.org/W3015202896', 'https://openalex.org/W2982409629', 'https://openalex.org/W2962777415', 'https://openalex.org/W2924375708', 'https://openalex.org/W6783596713', 'https://openalex.org/W6683390034', 'https://openalex.org/W6679208599', 'https://openalex.org/W6750415123', 'https://openalex.org/W2908848055', 'https://openalex.org/W2898979141', 'https://openalex.org/W2963568497', 'https://openalex.org/W3002369363', 'https://openalex.org/W2971847579', 'https://openalex.org/W2787089463', 'https://openalex.org/W2771733300', 'https://openalex.org/W2975718969', 'https://openalex.org/W2017120884', 'https://openalex.org/W2023713006', 'https://openalex.org/W2059329127', 'https://openalex.org/W1255270367', 'https://openalex.org/W2578206263', 'https://openalex.org/W2069807182', 'https://openalex.org/W2779169403', 'https://openalex.org/W2118386984', 'https://openalex.org/W2060090293', 'https://openalex.org/W2170410701', 'https://openalex.org/W3016889634', 'https://openalex.org/W2998784361', 'https://openalex.org/W6772750526', 'https://openalex.org/W2770233088', 'https://openalex.org/W6748481559', 'https://openalex.org/W6739696289', 'https://openalex.org/W2963881378', 'https://openalex.org/W6639824700', 'https://openalex.org/W6777331411', 'https://openalex.org/W3003771668', 'https://openalex.org/W6769719881', 'https://openalex.org/W2971902377', 'https://openalex.org/W2521803624', 'https://openalex.org/W2996173932', 'https://openalex.org/W2965550490', 'https://openalex.org/W2928133111', 'https://openalex.org/W6772686471', 'https://openalex.org/W6772228894', 'https://openalex.org/W2580555377', 'https://openalex.org/W3008115128', 'https://openalex.org/W6780068424', 'https://openalex.org/W6767224305', 'https://openalex.org/W3011680720', 'https://openalex.org/W2896014703', 'https://openalex.org/W6769545421', 'https://openalex.org/W6756932269', 'https://openalex.org/W6777688320', 'https://openalex.org/W3011613797', 'https://openalex.org/W2995592124', 'https://openalex.org/W2737373222', 'https://openalex.org/W6769668337', 'https://openalex.org/W2971013993', 'https://openalex.org/W3011743383', 'https://openalex.org/W3022941278', 'https://openalex.org/W6767123329', 'https://openalex.org/W6773237479', 'https://openalex.org/W2884034690', 'https://openalex.org/W6775713670', 'https://openalex.org/W6766645163', 'https://openalex.org/W2940868118', 'https://openalex.org/W2915971115', 'https://openalex.org/W6754893925', 'https://openalex.org/W6757726594', 'https://openalex.org/W6777512502', 'https://openalex.org/W6775127117', 'https://openalex.org/W2962957359', 'https://openalex.org/W6733692865', 'https://openalex.org/W6864890602', 'https://openalex.org/W2090895104', 'https://openalex.org/W2083216680', 'https://openalex.org/W1623027047', 'https://openalex.org/W2086203184', 'https://openalex.org/W2025294066', 'https://openalex.org/W2908420789', 'https://openalex.org/W2071162717', 'https://openalex.org/W2001959977', 'https://openalex.org/W2077489619', 'https://openalex.org/W2076393258', 'https://openalex.org/W6729368694', 'https://openalex.org/W1989579918', 'https://openalex.org/W2002448684', 'https://openalex.org/W2037764594', 'https://openalex.org/W2124010969', 'https://openalex.org/W2164952352', 'https://openalex.org/W2612688942', 'https://openalex.org/W2790695487', 'https://openalex.org/W2593966629', 'https://openalex.org/W6785424041', 'https://openalex.org/W3010488723', 'https://openalex.org/W3006857755', 'https://openalex.org/W3013999312', 'https://openalex.org/W2928325490', 'https://openalex.org/W2984234582', 'https://openalex.org/W6759284863', 'https://openalex.org/W2074424001', 'https://openalex.org/W1979268352', 'https://openalex.org/W3210024070', 'https://openalex.org/W2804340011', 'https://openalex.org/W6774161039', 'https://openalex.org/W6776282039', 'https://openalex.org/W6768281725', 'https://openalex.org/W2997106510', 'https://openalex.org/W2767272762', 'https://openalex.org/W6770215180', 'https://openalex.org/W2801748224', 'https://openalex.org/W6736288088', 'https://openalex.org/W6769062451', 'https://openalex.org/W6766057927', 'https://openalex.org/W6767269543', 'https://openalex.org/W2785187918', 'https://openalex.org/W6673073000', 'https://openalex.org/W6733058949', 'https://openalex.org/W2807529265', 'https://openalex.org/W2133272039', 'https://openalex.org/W2770165898', 'https://openalex.org/W2963809228', 'https://openalex.org/W6779415093', 'https://openalex.org/W6811678227', 'https://openalex.org/W2897722020', 'https://openalex.org/W6637151318', 'https://openalex.org/W6745403943', 'https://openalex.org/W6776490414', 'https://openalex.org/W6891802897', 'https://openalex.org/W6764076272', 'https://openalex.org/W6902124583', 'https://openalex.org/W6677618333', 'https://openalex.org/W2801128564', 'https://openalex.org/W6733758817', 'https://openalex.org/W6686509673', 'https://openalex.org/W6776172991', 'https://openalex.org/W6766978945', 'https://openalex.org/W6713134421', 'https://openalex.org/W6694517276', 'https://openalex.org/W6710709672', 'https://openalex.org/W6632581263', 'https://openalex.org/W6784716568', 'https://openalex.org/W2910096450', 'https://openalex.org/W6785372702', 'https://openalex.org/W6749756498', 'https://openalex.org/W6745021780', 'https://openalex.org/W2595557940', 'https://openalex.org/W6762493768', 'https://openalex.org/W6774660541', 'https://openalex.org/W3138798301', 'https://openalex.org/W2945580137', 'https://openalex.org/W2898149606', 'https://openalex.org/W1988888548', 'https://openalex.org/W6749669830', 'https://openalex.org/W6731404320', 'https://openalex.org/W6757988111', 'https://openalex.org/W4292084264', 'https://openalex.org/W6756756286', 'https://openalex.org/W6780234213', 'https://openalex.org/W6675354045', 'https://openalex.org/W2133990480', 'https://openalex.org/W6633912811', 'https://openalex.org/W3010817165', 'https://openalex.org/W6759151774', 'https://openalex.org/W2777802649', 'https://openalex.org/W6679436768', 'https://openalex.org/W6772508046', 'https://openalex.org/W6770432743', 'https://openalex.org/W6754002923', 'https://openalex.org/W2395579298', 'https://openalex.org/W6682132143', 'https://openalex.org/W4246078117', 'https://openalex.org/W6767753571', 'https://openalex.org/W6768314899', 'https://openalex.org/W2965936382', 'https://openalex.org/W2742348474', 'https://openalex.org/W3013382035', 'https://openalex.org/W6607007744', 'https://openalex.org/W2944851425', 'https://openalex.org/W6635679246', 'https://openalex.org/W6749897870', 'https://openalex.org/W6756223733', 'https://openalex.org/W6728184133', 'https://openalex.org/W2037227137', 'https://openalex.org/W6739622702', 'https://openalex.org/W6767917410', 'https://openalex.org/W2890524002', 'https://openalex.org/W2132862423', 'https://openalex.org/W2022023730', 'https://openalex.org/W1982071625', 'https://openalex.org/W2065614234', 'https://openalex.org/W2999044305', 'https://openalex.org/W2982675299', 'https://openalex.org/W6898611122', 'https://openalex.org/W6787972765', 'https://openalex.org/W2117539524', 'https://openalex.org/W2953319022', 'https://openalex.org/W2180441778', 'https://openalex.org/W1584409434', 'https://openalex.org/W2035460215', 'https://openalex.org/W2054591207', 'https://openalex.org/W2036348412', 'https://openalex.org/W2299689158', 'https://openalex.org/W2889396403', 'https://openalex.org/W2419488691', 'https://openalex.org/W2315318154', 'https://openalex.org/W2237814382', 'https://openalex.org/W2104979868', 'https://openalex.org/W2070942956', 'https://openalex.org/W2744462161', 'https://openalex.org/W2803462582', 'https://openalex.org/W2334044140', 'https://openalex.org/W2097370873', 'https://openalex.org/W2102940758', 'https://openalex.org/W2170214641', 'https://openalex.org/W6676087360', 'https://openalex.org/W2975270375', 'https://openalex.org/W2099302183', 'https://openalex.org/W2023972500', 'https://openalex.org/W2112845989', 'https://openalex.org/W2168353268', 'https://openalex.org/W6955071965', 'https://openalex.org/W6728047685', 'https://openalex.org/W6773090988', 'https://openalex.org/W3038503126', 'https://openalex.org/W2921039670', 'https://openalex.org/W6762357419', 'https://openalex.org/W6760779191', 'https://openalex.org/W2887377515', 'https://openalex.org/W2759020546', 'https://openalex.org/W6656040699', 'https://openalex.org/W2971732167', 'https://openalex.org/W2945843801', 'https://openalex.org/W6766715767', 'https://openalex.org/W6743783158', 'https://openalex.org/W2944449873', 'https://openalex.org/W2986218565', 'https://openalex.org/W2613073940', 'https://openalex.org/W6764344362', 'https://openalex.org/W6667169455', 'https://openalex.org/W3015779532', 'https://openalex.org/W6777109051', 'https://openalex.org/W3019078705', 'https://openalex.org/W6772035149', 'https://openalex.org/W3037426931', 'https://openalex.org/W6761660184', 'https://openalex.org/W2997330623', 'https://openalex.org/W3009916009', 'https://openalex.org/W2900201388', 'https://openalex.org/W2888592790', 'https://openalex.org/W2567454507', 'https://openalex.org/W2754963868', 'https://openalex.org/W6780066040', 'https://openalex.org/W6779481038', 'https://openalex.org/W2742389902', 'https://openalex.org/W6782358392', 'https://openalex.org/W6776225289', 'https://openalex.org/W2570926664', 'https://openalex.org/W2772130716', 'https://openalex.org/W2913350752', 'https://openalex.org/W2995646127', 'https://openalex.org/W3121197550', 'https://openalex.org/W6776011318', 'https://openalex.org/W2888099522', 'https://openalex.org/W6756362395', 'https://openalex.org/W2810627707', 'https://openalex.org/W6770300615', 'https://openalex.org/W6773532678', 'https://openalex.org/W2809426407', 'https://openalex.org/W2027604750', 'https://openalex.org/W6785823854', 'https://openalex.org/W6780576921', 'https://openalex.org/W6689854868', 'https://openalex.org/W2787225861', 'https://openalex.org/W6745549460', 'https://openalex.org/W2888173901', 'https://openalex.org/W1864634822', 'https://openalex.org/W2890758909', 'https://openalex.org/W2912852556', 'https://openalex.org/W6771404727', 'https://openalex.org/W1973811978', 'https://openalex.org/W1907286193', 'https://openalex.org/W2954177755', 'https://openalex.org/W3022577033', 'https://openalex.org/W3001281321', 'https://openalex.org/W3006841776', 'https://openalex.org/W6762307758', 'https://openalex.org/W2922154398', 'https://openalex.org/W4243343826', 'https://openalex.org/W6775382722', 'https://openalex.org/W3043284824', 'https://openalex.org/W6634502982', 'https://openalex.org/W2992072841', 'https://openalex.org/W2275719597', 'https://openalex.org/W6775819401', 'https://openalex.org/W2995742707', 'https://openalex.org/W2914881009', 'https://openalex.org/W2796505325', 'https://openalex.org/W6720419606', 'https://openalex.org/W2972083580', 'https://openalex.org/W2952661137', 'https://openalex.org/W2013875781', 'https://openalex.org/W3008069193', 'https://openalex.org/W3008518202', 'https://openalex.org/W2899197801', 'https://openalex.org/W4250459844', 'https://openalex.org/W3011762972', 'https://openalex.org/W3025114356', 'https://openalex.org/W2403503694', 'https://openalex.org/W2037363234', 'https://openalex.org/W2959236235', 'https://openalex.org/W2989069299', 'https://openalex.org/W2275907536', 'https://openalex.org/W6703468476', 'https://openalex.org/W2044279302', 'https://openalex.org/W3010898682', 'https://openalex.org/W2558155811', 'https://openalex.org/W2911863776', 'https://openalex.org/W4235515987', 'https://openalex.org/W2965295081', 'https://openalex.org/W2936304837', 'https://openalex.org/W6948112432', 'https://openalex.org/W2157147558', 'https://openalex.org/W2041146478', 'https://openalex.org/W2895976516', 'https://openalex.org/W1964630790', 'https://openalex.org/W3001137683', 'https://openalex.org/W6803328758', 'https://openalex.org/W2782605624', 'https://openalex.org/W2540737557', 'https://openalex.org/W2057869304', 'https://openalex.org/W2084990704', 'https://openalex.org/W3028354808', 'https://openalex.org/W6736671799', 'https://openalex.org/W6748978342', 'https://openalex.org/W2043849877', 'https://openalex.org/W6748999769', 'https://openalex.org/W2407893127', 'https://openalex.org/W2794054005', 'https://openalex.org/W4211101666', 'https://openalex.org/W4249166508', 'https://openalex.org/W2165994754', 'https://openalex.org/W2403560111', 'https://openalex.org/W2073910589', 'https://openalex.org/W2612443886', 'https://openalex.org/W2552269303', 'https://openalex.org/W2100218187', 'https://openalex.org/W2090588076', 'https://openalex.org/W2096281865', 'https://openalex.org/W1973773465', 'https://openalex.org/W6749977729', 'https://openalex.org/W2091196167', 'https://openalex.org/W2742350913', 'https://openalex.org/W2080494989', 'https://openalex.org/W2898847797', 'https://openalex.org/W2083032710', 'https://openalex.org/W1997625448', 'https://openalex.org/W3015532679', 'https://openalex.org/W2922085361', 'https://openalex.org/W2889156240', 'https://openalex.org/W2940188016', 'https://openalex.org/W2129906827', 'https://openalex.org/W4231161067', 'https://openalex.org/W2992321344', 'https://openalex.org/W2563314032', 'https://openalex.org/W3029664174', 'https://openalex.org/W2887666816', 'https://openalex.org/W2136800949', 'https://openalex.org/W2144456601', 'https://openalex.org/W2884565747', 'https://openalex.org/W2331879422', 'https://openalex.org/W1801518737', 'https://openalex.org/W2907862134', 'https://openalex.org/W1793357152', 'https://openalex.org/W2063444962', 'https://openalex.org/W2431326462', 'https://openalex.org/W1974978266', 'https://openalex.org/W2768988890', 'https://openalex.org/W2801020963', 'https://openalex.org/W2560669091', 'https://openalex.org/W2072612262', 'https://openalex.org/W181809736', 'https://openalex.org/W6785434745', 'https://openalex.org/W2153077083', 'https://openalex.org/W1976843864', 'https://openalex.org/W2014298488', 'https://openalex.org/W3039158623', 'https://openalex.org/W2347039104', 'https://openalex.org/W2021311589', 'https://openalex.org/W6685771475', 'https://openalex.org/W2361925826', 'https://openalex.org/W6639256420', 'https://openalex.org/W2805413688', 'https://openalex.org/W2743547990', 'https://openalex.org/W6827582168', 'https://openalex.org/W2413323752', 'https://openalex.org/W2079375173', 'https://openalex.org/W152884668', 'https://openalex.org/W2734133642', 'https://openalex.org/W2586460992', 'https://openalex.org/W2015381430', 'https://openalex.org/W2151365450', 'https://openalex.org/W2072850511', 'https://openalex.org/W2164753380', 'https://openalex.org/W2079266205', 'https://openalex.org/W2015221684', 'https://openalex.org/W3035683727', 'https://openalex.org/W3014242940', 'https://openalex.org/W2790027174', 'https://openalex.org/W2055663231', 'https://openalex.org/W2609427108', 'https://openalex.org/W2096552716', 'https://openalex.org/W2897568774', 'https://openalex.org/W1964036593', 'https://openalex.org/W2965746167', 'https://openalex.org/W2952236080', 'https://openalex.org/W1978856540', 'https://openalex.org/W1524992328', 'https://openalex.org/W2972087877', 'https://openalex.org/W6769630300', 'https://openalex.org/W6756026151', 'https://openalex.org/W6760266475', 'https://openalex.org/W6691187937', 'https://openalex.org/W6695533872', 'https://openalex.org/W6679909955', 'https://openalex.org/W6948022635', 'https://openalex.org/W6755212716', 'https://openalex.org/W6637242042', 'https://openalex.org/W6682889407', 'https://openalex.org/W3012740297', 'https://openalex.org/W6640185926', 'https://openalex.org/W6750344830', 'https://openalex.org/W6777581048', 'https://openalex.org/W6785416625', 'https://openalex.org/W6757734603', 'https://openalex.org/W6697462184', 'https://openalex.org/W6772513851', 'https://openalex.org/W6744224683', 'https://openalex.org/W2760217853', 'https://openalex.org/W6769501854', 'https://openalex.org/W6685562342', 'https://openalex.org/W6739879593', 'https://openalex.org/W6770827734', 'https://openalex.org/W2407508591', 'https://openalex.org/W2106504576', 'https://openalex.org/W6756203350', 'https://openalex.org/W3023211159', 'https://openalex.org/W2911628061', 'https://openalex.org/W6767897812', 'https://openalex.org/W6746503037', 'https://openalex.org/W6697168296', 'https://openalex.org/W6697974390', 'https://openalex.org/W6679852000', 'https://openalex.org/W6677995690', 'https://openalex.org/W6743446608', 'https://openalex.org/W6734215995', 'https://openalex.org/W6781508010', 'https://openalex.org/W6746023985', 'https://openalex.org/W6779946651', 'https://openalex.org/W2792268997', 'https://openalex.org/W6759435830', 'https://openalex.org/W6730344855', 'https://openalex.org/W6637359451', 'https://openalex.org/W6681973651', 'https://openalex.org/W6762490506', 'https://openalex.org/W6640963894', 'https://openalex.org/W6637551671', 'https://openalex.org/W2902935192', 'https://openalex.org/W6731759459', 'https://openalex.org/W6728713152', 'https://openalex.org/W6755310813', 'https://openalex.org/W6638667902', 'https://openalex.org/W6751923770', 'https://openalex.org/W6758094429', 'https://openalex.org/W6759785765', 'https://openalex.org/W6750647073', 'https://openalex.org/W6763880754', 'https://openalex.org/W6696413332', 'https://openalex.org/W6726983090', 'https://openalex.org/W6750964650', 'https://openalex.org/W6743289643', 'https://openalex.org/W6746407912', 'https://openalex.org/W6759047552', 'https://openalex.org/W6785253720', 'https://openalex.org/W6780128236', 'https://openalex.org/W2965870268', 'https://openalex.org/W6687681856', 'https://openalex.org/W6733590821', 'https://openalex.org/W6718379498', 'https://openalex.org/W6762875896', 'https://openalex.org/W6749538638', 'https://openalex.org/W6780226713', 'https://openalex.org/W6767278793', 'https://openalex.org/W6724804524', 'https://openalex.org/W2962750014', 'https://openalex.org/W6720904035', 'https://openalex.org/W6640174519', 'https://openalex.org/W6736210646', 'https://openalex.org/W2583638424', 'https://openalex.org/W6749954789', 'https://openalex.org/W6756274528', 'https://openalex.org/W6753055032', 'https://openalex.org/W6751546485', 'https://openalex.org/W6701650085', 'https://openalex.org/W6703857926', 'https://openalex.org/W6689986428', 'https://openalex.org/W6695676441', 'https://openalex.org/W6761071236', 'https://openalex.org/W6744331365', 'https://openalex.org/W6748582592', 'https://openalex.org/W2052725501', 'https://openalex.org/W2098502158', 'https://openalex.org/W6738248287', 'https://openalex.org/W2122860281', 'https://openalex.org/W6768952226', 'https://openalex.org/W6745604610', 'https://openalex.org/W2273396394', 'https://openalex.org/W2762685704', 'https://openalex.org/W7038994942', 'https://openalex.org/W2117731089', 'https://openalex.org/W2101926813', 'https://openalex.org/W2068470708', 'https://openalex.org/W2113625916', 'https://openalex.org/W6675308081', 'https://openalex.org/W2112796928', 'https://openalex.org/W2132424367', 'https://openalex.org/W2805029945', 'https://openalex.org/W2905502540', 'https://openalex.org/W2935381027', 'https://openalex.org/W2980612421', 'https://openalex.org/W2901951655', 'https://openalex.org/W2752517284', 'https://openalex.org/W2913223168', 'https://openalex.org/W2809254203', 'https://openalex.org/W2772551569', 'https://openalex.org/W3013056581', 'https://openalex.org/W2995527107', 'https://openalex.org/W2592929672', 'https://openalex.org/W2893483035', 'https://openalex.org/W2884367402', 'https://openalex.org/W6761797528', 'https://openalex.org/W6775563663', 'https://openalex.org/W6768804302', 'https://openalex.org/W7042325170', 'https://openalex.org/W1976739826', 'https://openalex.org/W6676449312', 'https://openalex.org/W6637078681', 'https://openalex.org/W6774570552', 'https://openalex.org/W1996901117', 'https://openalex.org/W6760536013', 'https://openalex.org/W6763925692', 'https://openalex.org/W6756709993', 'https://openalex.org/W6714181750', 'https://openalex.org/W2964350391', 'https://openalex.org/W6686164453', 'https://openalex.org/W6674914833', 'https://openalex.org/W6741414320', 'https://openalex.org/W6688459880', 'https://openalex.org/W6743386314', 'https://openalex.org/W6687483927', 'https://openalex.org/W6759983693', 'https://openalex.org/W6750997056', 'https://openalex.org/W6732070715', 'https://openalex.org/W6768962629', 'https://openalex.org/W6780643669', 'https://openalex.org/W2927072297', 'https://openalex.org/W1993482030', 'https://openalex.org/W6638060716', 'https://openalex.org/W6746904271', 'https://openalex.org/W3003402380', 'https://openalex.org/W6727553916', 'https://openalex.org/W6740873641', 'https://openalex.org/W2535388113', 'https://openalex.org/W6737664043', 'https://openalex.org/W6753618346', 'https://openalex.org/W2438648466', 'https://openalex.org/W2109255472', 'https://openalex.org/W6749752212', 'https://openalex.org/W2102182691', 'https://openalex.org/W6746027587', 'https://openalex.org/W6642279145', 'https://openalex.org/W6652834929', 'https://openalex.org/W2098834788', 'https://openalex.org/W6684857869', 'https://openalex.org/W2061171222', 'https://openalex.org/W2123119012', 'https://openalex.org/W6691901705', 'https://openalex.org/W6746846812', 'https://openalex.org/W6637373629', 'https://openalex.org/W7074114638', 'https://openalex.org/W2087584984', 'https://openalex.org/W3018960895', 'https://openalex.org/W2964232029', 'https://openalex.org/W6747381837', 'https://openalex.org/W6729059855', 'https://openalex.org/W6730741096', 'https://openalex.org/W6737488266', 'https://openalex.org/W2790400419', 'https://openalex.org/W6731900453', 'https://openalex.org/W6727690538', 'https://openalex.org/W6626481562', 'https://openalex.org/W6780493881', 'https://openalex.org/W2945472816', 'https://openalex.org/W6725739302', 'https://openalex.org/W6747213469', 'https://openalex.org/W2740336064', 'https://openalex.org/W6743679106', 'https://openalex.org/W6752356114', 'https://openalex.org/W2942178783', 'https://openalex.org/W6741008739', 'https://openalex.org/W2121029939', 'https://openalex.org/W6638444622', 'https://openalex.org/W6739901393', 'https://openalex.org/W6682137061', 'https://openalex.org/W6628927728', 'https://openalex.org/W6684921986', 'https://openalex.org/W6696783566', 'https://openalex.org/W6683195989', 'https://openalex.org/W6762277727', 'https://openalex.org/W2984844508', 'https://openalex.org/W6756730649', 'https://openalex.org/W6726238325', 'https://openalex.org/W2133665775', 'https://openalex.org/W3026802938', 'https://openalex.org/W6759295906', 'https://openalex.org/W6749927861', 'https://openalex.org/W6762830053', 'https://openalex.org/W6738706147', 'https://openalex.org/W2962765321', 'https://openalex.org/W6734564793', 'https://openalex.org/W6768798653', 'https://openalex.org/W6751250802', 'https://openalex.org/W6741832134', 'https://openalex.org/W6735913928', 'https://openalex.org/W6745995898', 'https://openalex.org/W6777635861', 'https://openalex.org/W6752378368', 'https://openalex.org/W6755312952', 'https://openalex.org/W6747400857', 'https://openalex.org/W6751693684', 'https://openalex.org/W6734074887', 'https://openalex.org/W6759225330', 'https://openalex.org/W6734871034', 'https://openalex.org/W6639480849', 'https://openalex.org/W6688646889', 'https://openalex.org/W2150355110', 'https://openalex.org/W2998102890', 'https://openalex.org/W6683935339', 'https://openalex.org/W6630875275', 'https://openalex.org/W6639657675', 'https://openalex.org/W6767824533', 'https://openalex.org/W2993873509', 'https://openalex.org/W3019166713', 'https://openalex.org/W6776090298', 'https://openalex.org/W6763131180', 'https://openalex.org/W2962933419', 'https://openalex.org/W2800726910', 'https://openalex.org/W6750378536', 'https://openalex.org/W6755207826', 'https://openalex.org/W6682691769', 'https://openalex.org/W6674387193', 'https://openalex.org/W6748304040', 'https://openalex.org/W6679775712', 'https://openalex.org/W2963514026', 'https://openalex.org/W6691431627', 'https://openalex.org/W6636510571', 'https://openalex.org/W2885195348', 'https://openalex.org/W2136848157', 'https://openalex.org/W2064675550', 'https://openalex.org/W2157331557', 'https://openalex.org/W6732160842', 'https://openalex.org/W6733064944', 'https://openalex.org/W6638545294', 'https://openalex.org/W6747683314', 'https://openalex.org/W2594990650', 'https://openalex.org/W6616837769', 'https://openalex.org/W6640212811', 'https://openalex.org/W3038024393', 'https://openalex.org/W2963059228', 'https://openalex.org/W6605751541', 'https://openalex.org/W2319453305', 'https://openalex.org/W1689711448', 'https://openalex.org/W6744825623', 'https://openalex.org/W2058580716', 'https://openalex.org/W6631275600', 'https://openalex.org/W2083127301', 'https://openalex.org/W191292346', 'https://openalex.org/W2110485445', 'https://openalex.org/W6748947987', 'https://openalex.org/W2142058217', 'https://openalex.org/W6744423198', 'https://openalex.org/W6639497327', 'https://openalex.org/W2131774270', 'https://openalex.org/W6679434410', 'https://openalex.org/W2079735306', 'https://openalex.org/W1972340877', 'https://openalex.org/W2964199361', 'https://openalex.org/W2785128315', 'https://openalex.org/W6725939724', 'https://openalex.org/W6648819931', 'https://openalex.org/W2084486275', 'https://openalex.org/W2036317923', 'https://openalex.org/W2028418738', 'https://openalex.org/W2116653838', 'https://openalex.org/W6761837902', 'https://openalex.org/W1902237438', 'https://openalex.org/W2530887700', 'https://openalex.org/W6684821475', 'https://openalex.org/W6757140150', 'https://openalex.org/W2100495367', 'https://openalex.org/W2122538988', 'https://openalex.org/W6693326390', 'https://openalex.org/W2295124130', 'https://openalex.org/W6734035190', 'https://openalex.org/W6681096077', 'https://openalex.org/W6657028776', 'https://openalex.org/W6725552749', 'https://openalex.org/W6680588873', 'https://openalex.org/W6639488474', 'https://openalex.org/W6688386640', 'https://openalex.org/W6600645948', 'https://openalex.org/W4206566734', 'https://openalex.org/W6720208624', 'https://openalex.org/W6638896900', 'https://openalex.org/W6683819664', 'https://openalex.org/W6637970017', 'https://openalex.org/W2750692136', 'https://openalex.org/W3011299880', 'https://openalex.org/W6766859440', 'https://openalex.org/W6687506355', 'https://openalex.org/W6776010876', 'https://openalex.org/W2991267026', 'https://openalex.org/W6770867806', 'https://openalex.org/W6755346343', 'https://openalex.org/W3015832418', 'https://openalex.org/W6767171632', 'https://openalex.org/W6785131241', 'https://openalex.org/W7016021835', 'https://openalex.org/W6774995201', 'https://openalex.org/W6755176163', 'https://openalex.org/W2958110591', 'https://openalex.org/W2885993282', 'https://openalex.org/W2529996553', 'https://openalex.org/W2971690404', 'https://openalex.org/W2985931096', 'https://openalex.org/W2963609389', 'https://openalex.org/W6748329691', 'https://openalex.org/W6765411980', 'https://openalex.org/W6761249802', 'https://openalex.org/W6747352621', 'https://openalex.org/W6687241523', 'https://openalex.org/W4205893465', 'https://openalex.org/W2009797711', 'https://openalex.org/W1994616650', 'https://openalex.org/W1988720110', 'https://openalex.org/W6604254268', 'https://openalex.org/W6727883105', 'https://openalex.org/W6749646390', 'https://openalex.org/W6631190155', 'https://openalex.org/W2990346675', 'https://openalex.org/W2963433607', 'https://openalex.org/W6727249380', 'https://openalex.org/W2567576169', 'https://openalex.org/W6703652217', 'https://openalex.org/W6756692837', 'https://openalex.org/W2913907987', 'https://openalex.org/W2136145120', 'https://openalex.org/W2142029338', 'https://openalex.org/W2137827713', 'https://openalex.org/W6757751764', 'https://openalex.org/W6747620207', 'https://openalex.org/W6766196973', 'https://openalex.org/W6757107679', 'https://openalex.org/W6751231343', 'https://openalex.org/W6764063469', 'https://openalex.org/W6767164110', 'https://openalex.org/W6738693370', 'https://openalex.org/W6717367658', 'https://openalex.org/W6714477741', 'https://openalex.org/W6680455300', 'https://openalex.org/W6731982132', 'https://openalex.org/W6756137178', 'https://openalex.org/W2604744755', 'https://openalex.org/W1980287119', 'https://openalex.org/W6780943123', 'https://openalex.org/W6769349878', 'https://openalex.org/W6738491991', 'https://openalex.org/W6766758401', 'https://openalex.org/W3093964892', 'https://openalex.org/W6778239092', 'https://openalex.org/W3005658451', 'https://openalex.org/W6748504360', 'https://openalex.org/W6762238293', 'https://openalex.org/W6769049530', 'https://openalex.org/W3045004532', 'https://openalex.org/W6768204257', 'https://openalex.org/W6765949483', 'https://openalex.org/W6764804008', 'https://openalex.org/W6732046175', 'https://openalex.org/W6740007704', 'https://openalex.org/W2102013737', 'https://openalex.org/W6774468489', 'https://openalex.org/W2040049280', 'https://openalex.org/W6728621909', 'https://openalex.org/W2253093986', 'https://openalex.org/W6778156585', 'https://openalex.org/W6762933861', 'https://openalex.org/W2944345944', 'https://openalex.org/W6736066110', 'https://openalex.org/W6746622358', 'https://openalex.org/W6760945832', 'https://openalex.org/W3014193915', 'https://openalex.org/W2939893088', 'https://openalex.org/W6768561264', 'https://openalex.org/W6741178023', 'https://openalex.org/W2079777091', 'https://openalex.org/W6748267448', 'https://openalex.org/W2947137917', 'https://openalex.org/W6741674396', 'https://openalex.org/W6682642761', 'https://openalex.org/W6680021708', 'https://openalex.org/W2054217036', 'https://openalex.org/W6747335957', 'https://openalex.org/W2939742734', 'https://openalex.org/W6765941946', 'https://openalex.org/W2301541953', 'https://openalex.org/W2026131661', 'https://openalex.org/W2111072639', 'https://openalex.org/W6679935922', 'https://openalex.org/W6732665253', 'https://openalex.org/W6772592983', 'https://openalex.org/W3017677030', 'https://openalex.org/W2908261578', 'https://openalex.org/W2938321354', 'https://openalex.org/W2822752092', 'https://openalex.org/W3100789280', 'https://openalex.org/W6773319185', 'https://openalex.org/W6761312402', 'https://openalex.org/W2907052818', 'https://openalex.org/W2898035736', 'https://openalex.org/W2966012238', 'https://openalex.org/W2964657419', 'https://openalex.org/W2145339207', 'https://openalex.org/W6760834971', 'https://openalex.org/W2910273746', 'https://openalex.org/W6677183884', 'https://openalex.org/W6739193204', 'https://openalex.org/W2017957151', 'https://openalex.org/W6748839928', 'https://openalex.org/W6750645735', 'https://openalex.org/W6903722218', 'https://openalex.org/W6740092555', 'https://openalex.org/W6757058172', 'https://openalex.org/W6734517396', 'https://openalex.org/W6756287877', 'https://openalex.org/W6766694020', 'https://openalex.org/W6763629196', 'https://openalex.org/W6738087714', 'https://openalex.org/W6748081988', 'https://openalex.org/W6743684492', 'https://openalex.org/W6776438516', 'https://openalex.org/W6780404908', 'https://openalex.org/W2141559645', 'https://openalex.org/W6746809867', 'https://openalex.org/W6766596047', 'https://openalex.org/W6771956426', 'https://openalex.org/W2959334635', 'https://openalex.org/W6753278433', 'https://openalex.org/W3006913750', 'https://openalex.org/W6761960898', 'https://openalex.org/W6758492113', 'https://openalex.org/W6766622607', 'https://openalex.org/W6760738939', 'https://openalex.org/W6685961532', 'https://openalex.org/W6767994309', 'https://openalex.org/W6767886268', 'https://openalex.org/W6691153465', 'https://openalex.org/W6756394140', 'https://openalex.org/W6748587240', 'https://openalex.org/W6764013109', 'https://openalex.org/W6763347759', 'https://openalex.org/W6746582238', 'https://openalex.org/W6755501315', 'https://openalex.org/W6745728296', 'https://openalex.org/W6729956949', 'https://openalex.org/W3175956061', 'https://openalex.org/W6758531748', 'https://openalex.org/W6631943919', 'https://openalex.org/W6737185050', 'https://openalex.org/W6678818196', 'https://openalex.org/W6720565605', 'https://openalex.org/W6638114406', 'https://openalex.org/W6677548962', 'https://openalex.org/W6685315510', 'https://openalex.org/W6637381549', 'https://openalex.org/W6767340034', 'https://openalex.org/W2766151966', 'https://openalex.org/W6762309853', 'https://openalex.org/W6769739676', 'https://openalex.org/W6910615345', 'https://openalex.org/W6762875921', 'https://openalex.org/W6752931829', 'https://openalex.org/W2626017178', 'https://openalex.org/W1989574703', 'https://openalex.org/W2122825543', 'https://openalex.org/W2135046866', 'https://openalex.org/W4234698323', 'https://openalex.org/W6772129282', 'https://openalex.org/W6779786081', 'https://openalex.org/W6771936042', 'https://openalex.org/W6676018521', 'https://openalex.org/W2964045208', 'https://openalex.org/W6674330103', 'https://openalex.org/W6762269264', 'https://openalex.org/W6692201896', 'https://openalex.org/W6751180888', 'https://openalex.org/W6629129618', 'https://openalex.org/W3002842489', 'https://openalex.org/W6761303797', 'https://openalex.org/W6754484989', 'https://openalex.org/W6779600184', 'https://openalex.org/W2610880400', 'https://openalex.org/W6721871360', 'https://openalex.org/W6762806541', 'https://openalex.org/W6681513673', 'https://openalex.org/W6676315081', 'https://openalex.org/W2965985861', 'https://openalex.org/W2098874108', 'https://openalex.org/W6751901350', 'https://openalex.org/W6764000192', 'https://openalex.org/W2990476713', 'https://openalex.org/W6758153731', 'https://openalex.org/W6692956712', 'https://openalex.org/W2954996726', 'https://openalex.org/W6780189587', 'https://openalex.org/W3000568834', 'https://openalex.org/W6779659972', 'https://openalex.org/W6777993669', 'https://openalex.org/W6782140300', 'https://openalex.org/W2904440514', 'https://openalex.org/W6623329352', 'https://openalex.org/W6739365718', 'https://openalex.org/W2885948040', 'https://openalex.org/W6742058293', 'https://openalex.org/W2913340405', 'https://openalex.org/W6729482032', 'https://openalex.org/W7072166219', 'https://openalex.org/W6779032286', 'https://openalex.org/W6774047946', 'https://openalex.org/W6757207288', 'https://openalex.org/W2509374375', 'https://openalex.org/W6760993166', 'https://openalex.org/W6773960885', 'https://openalex.org/W6759535515', 'https://openalex.org/W1866529865', 'https://openalex.org/W6752891299', 'https://openalex.org/W6730601996', 'https://openalex.org/W2744983412', 'https://openalex.org/W4212883601', 'https://openalex.org/W2911964244', 'https://openalex.org/W2605782971', 'https://openalex.org/W2797928606', 'https://openalex.org/W2898280479', 'https://openalex.org/W2950627632', 'https://openalex.org/W6760805404', 'https://openalex.org/W6767064347', 'https://openalex.org/W3017883828', 'https://openalex.org/W3093369300', 'https://openalex.org/W6728216425', 'https://openalex.org/W6774302960', 'https://openalex.org/W6772610842', 'https://openalex.org/W6776320331', 'https://openalex.org/W3176483701', 'https://openalex.org/W6746370421', 'https://openalex.org/W6770625286', 'https://openalex.org/W6766990000', 'https://openalex.org/W6739513683', 'https://openalex.org/W3004633656', 'https://openalex.org/W2548662600', 'https://openalex.org/W3020975691', 'https://openalex.org/W6778747846', 'https://openalex.org/W2981731882', 'https://openalex.org/W3082925502', 'https://openalex.org/W2954503794', 'https://openalex.org/W3035371891', 'https://openalex.org/W3016840912', 'https://openalex.org/W2793668851', 'https://openalex.org/W1772076007', 'https://openalex.org/W6775500731', 'https://openalex.org/W2982680516', 'https://openalex.org/W3011684164', 'https://openalex.org/W6766263406', 'https://openalex.org/W6776358153', 'https://openalex.org/W6766931891', 'https://openalex.org/W6745168123', 'https://openalex.org/W6767207763', 'https://openalex.org/W6766831090', 'https://openalex.org/W6765527630', 'https://openalex.org/W6772688469', 'https://openalex.org/W3010219363', 'https://openalex.org/W6775305308', 'https://openalex.org/W2943560007', 'https://openalex.org/W4239147634', 'https://openalex.org/W6910625380', 'https://openalex.org/W2542768043', 'https://openalex.org/W2068306188', 'https://openalex.org/W6776575407', 'https://openalex.org/W6785735133', 'https://openalex.org/W6754138735', 'https://openalex.org/W3006834142', 'https://openalex.org/W2949836779', 'https://openalex.org/W6757954571', 'https://openalex.org/W2936368166', 'https://openalex.org/W2969153652', 'https://openalex.org/W2959589111', 'https://openalex.org/W6736878346', 'https://openalex.org/W6758617434', 'https://openalex.org/W6741825281', 'https://openalex.org/W6731959695', 'https://openalex.org/W2440757793', 'https://openalex.org/W2980420460', 'https://openalex.org/W6785774496', 'https://openalex.org/W6668075932', 'https://openalex.org/W6768249212', 'https://openalex.org/W6776218486', 'https://openalex.org/W2775487773', 'https://openalex.org/W6778883912', 'https://openalex.org/W3006127095', 'https://openalex.org/W2963935794', 'https://openalex.org/W3108032709', 'https://openalex.org/W3115871478', 'https://openalex.org/W6780120029', 'https://openalex.org/W3021206621', 'https://openalex.org/W6765378574', 'https://openalex.org/W6729410713', 'https://openalex.org/W6747935233', 'https://openalex.org/W6770993010', 'https://openalex.org/W6775434300', 'https://openalex.org/W6761732570', 'https://openalex.org/W2921743340', 'https://openalex.org/W2935878216', 'https://openalex.org/W6782236162', 'https://openalex.org/W2980609230', 'https://openalex.org/W2953641512', 'https://openalex.org/W3048756828', 'https://openalex.org/W2860018042', 'https://openalex.org/W6767629742', 'https://openalex.org/W6767230837', 'https://openalex.org/W6756910310', 'https://openalex.org/W6769389586', 'https://openalex.org/W3015001695', 'https://openalex.org/W6776674803', 'https://openalex.org/W2963178695', 'https://openalex.org/W2962700793', 'https://openalex.org/W6640425456', 'https://openalex.org/W6773231345', 'https://openalex.org/W6753356910', 'https://openalex.org/W6756497762', 'https://openalex.org/W6771245373', 'https://openalex.org/W3006799225', 'https://openalex.org/W6774185773', 'https://openalex.org/W6743688258', 'https://openalex.org/W3098654368', 'https://openalex.org/W2997574889', 'https://openalex.org/W2903382683', 'https://openalex.org/W3101398262', 'https://openalex.org/W3003533476', 'https://openalex.org/W2965857035', 'https://openalex.org/W2984509121', 'https://openalex.org/W4288364646', 'https://openalex.org/W1523025179', 'https://openalex.org/W1924770834', 'https://openalex.org/W2754764588', 'https://openalex.org/W2949458134', 'https://openalex.org/W2887630004', 'https://openalex.org/W4367295640', 'https://openalex.org/W2108729336', 'https://openalex.org/W2952229419', 'https://openalex.org/W2965696320', 'https://openalex.org/W2979739940', 'https://openalex.org/W2963800363', 'https://openalex.org/W2606441631', 'https://openalex.org/W3104150464', 'https://openalex.org/W2945325682', 'https://openalex.org/W4289754090', 'https://openalex.org/W2954455099', 'https://openalex.org/W2962727772', 'https://openalex.org/W2896060389', 'https://openalex.org/W4297778814', 'https://openalex.org/W3036825277', 'https://openalex.org/W2963208657', 'https://openalex.org/W2995453501', 'https://openalex.org/W3015777882', 'https://openalex.org/W2931743911', 'https://openalex.org/W2337584310', 'https://openalex.org/W2206197787', 'https://openalex.org/W4287828878', 'https://openalex.org/W2004915807', 'https://openalex.org/W2028499920', 'https://openalex.org/W2963373786', 'https://openalex.org/W2969456553', 'https://openalex.org/W2766099245', 'https://openalex.org/W1993378086', 'https://openalex.org/W4297798488', 'https://openalex.org/W2543580944', 'https://openalex.org/W2792263949', 'https://openalex.org/W1485147275', 'https://openalex.org/W2798705390', 'https://openalex.org/W2969509404', 'https://openalex.org/W2757196798', 'https://openalex.org/W4320930577', 'https://openalex.org/W3011742461', 'https://openalex.org/W2423689290', 'https://openalex.org/W4288413898', 'https://openalex.org/W3034058016', 'https://openalex.org/W2740592764', 'https://openalex.org/W2608696703', 'https://openalex.org/W3105432754', 'https://openalex.org/W3034128561', 'https://openalex.org/W3103772912', 'https://openalex.org/W2803681939', 'https://openalex.org/W3146803896', 'https://openalex.org/W2972830778', 'https://openalex.org/W3004338823', 'https://openalex.org/W3041133507', 'https://openalex.org/W1705588077', 'https://openalex.org/W2173248099', 'https://openalex.org/W3098596645', 'https://openalex.org/W2970217468', 'https://openalex.org/W2326533993', 'https://openalex.org/W1533861849', 'https://openalex.org/W2934810769', 'https://openalex.org/W2962685937', 'https://openalex.org/W2963680240', 'https://openalex.org/W2902451427', 'https://openalex.org/W2970911383', 'https://openalex.org/W1650736245', 'https://openalex.org/W3174414731', 'https://openalex.org/W3041781105', 'https://openalex.org/W2201581102', 'https://openalex.org/W2148053762', 'https://openalex.org/W4287633455', 'https://openalex.org/W2963982496', 'https://openalex.org/W2735479059', 'https://openalex.org/W3100064003', 'https://openalex.org/W2963485221', 'https://openalex.org/W3101356658', 'https://openalex.org/W1504193762', 'https://openalex.org/W2607291476', 'https://openalex.org/W2971107871', 'https://openalex.org/W2143612262', 'https://openalex.org/W2176412452', 'https://openalex.org/W3031790649', 'https://openalex.org/W2782864149', 'https://openalex.org/W2810898455', 'https://openalex.org/W3021182036', 'https://openalex.org/W2606722458', 'https://openalex.org/W2997208817', 'https://openalex.org/W1566376227', 'https://openalex.org/W3038041907', 'https://openalex.org/W2985252039', 'https://openalex.org/W2995315671', 'https://openalex.org/W2767356931', 'https://openalex.org/W2284050935', 'https://openalex.org/W2557191228', 'https://openalex.org/W2594833348', 'https://openalex.org/W2943353555', 'https://openalex.org/W2904759738', 'https://openalex.org/W2992895209', 'https://openalex.org/W2732724430', 'https://openalex.org/W2941290535', 'https://openalex.org/W2951897614', 'https://openalex.org/W2937284497', 'https://openalex.org/W1575174233', 'https://openalex.org/W3013568095', 'https://openalex.org/W3155981360', 'https://openalex.org/W2766196653', 'https://openalex.org/W3103559770', 'https://openalex.org/W2785952199', 'https://openalex.org/W3038680626', 'https://openalex.org/W3099425575', 'https://openalex.org/W3106459670', 'https://openalex.org/W3043584299', 'https://openalex.org/W3104087655', 'https://openalex.org/W2988014923', 'https://openalex.org/W3016150174', 'https://openalex.org/W3099839033', 'https://openalex.org/W2977801370', 'https://openalex.org/W1491716407', 'https://openalex.org/W2963446712', 'https://openalex.org/W2991448601', 'https://openalex.org/W1485981043', 'https://openalex.org/W2963211300', 'https://openalex.org/W2130942839', 'https://openalex.org/W2384495648', 'https://openalex.org/W2943371177', 'https://openalex.org/W2287011250', 'https://openalex.org/W2995383582', 'https://openalex.org/W2786746258', 'https://openalex.org/W2990737658', 'https://openalex.org/W2095705004', 'https://openalex.org/W2402144811', 'https://openalex.org/W2105672886', 'https://openalex.org/W2612445135', 'https://openalex.org/W2474421929', 'https://openalex.org/W2952634764', 'https://openalex.org/W2963207607', 'https://openalex.org/W4297801177', 'https://openalex.org/W3028572863', 'https://openalex.org/W2973029942', 'https://openalex.org/W2980002742', 'https://openalex.org/W2528062157', 'https://openalex.org/W1758610246', 'https://openalex.org/W3020825954', 'https://openalex.org/W2525167219', 'https://openalex.org/W1484210532', 'https://openalex.org/W2551147948', 'https://openalex.org/W2255738257', 'https://openalex.org/W3098957257', 'https://openalex.org/W2165736859', 'https://openalex.org/W2963920537', 'https://openalex.org/W3210644617', 'https://openalex.org/W4297801009', 'https://openalex.org/W2793022090', 'https://openalex.org/W2278108219', 'https://openalex.org/W2966284335', 'https://openalex.org/W2945589975', 'https://openalex.org/W3018620108', 'https://openalex.org/W2271840356', 'https://openalex.org/W2161893161', 'https://openalex.org/W2159889048', 'https://openalex.org/W1836465849', 'https://openalex.org/W1686810756', 'https://openalex.org/W1683511521', 'https://openalex.org/W2731468224', 'https://openalex.org/W2977485436', 'https://openalex.org/W2798291715', 'https://openalex.org/W2877984952', 'https://openalex.org/W2795072647', 'https://openalex.org/W2982634085', 'https://openalex.org/W2963148870', 'https://openalex.org/W2162931648', 'https://openalex.org/W2963111827', 'https://openalex.org/W2963273111', 'https://openalex.org/W2963608065', 'https://openalex.org/W2967121647', 'https://openalex.org/W3034910915', 'https://openalex.org/W2970803838', 'https://openalex.org/W2281746805', 'https://openalex.org/W4294567867', 'https://openalex.org/W4294170691', 'https://openalex.org/W1757796397', 'https://openalex.org/W1598796236', 'https://openalex.org/W3093943387', 'https://openalex.org/W3099445761', 'https://openalex.org/W302652777', 'https://openalex.org/W2947411064', 'https://openalex.org/W2930789748', 'https://openalex.org/W3013264884', 'https://openalex.org/W2949935872', 'https://openalex.org/W2963140169', 'https://openalex.org/W2963667932', 'https://openalex.org/W2902851082', 'https://openalex.org/W2899675781', 'https://openalex.org/W2893749619', 'https://openalex.org/W2182609383', 'https://openalex.org/W4288798638', 'https://openalex.org/W2149933564', 'https://openalex.org/W1924619199', 'https://openalex.org/W4295773261', 'https://openalex.org/W2804078698', 'https://openalex.org/W2900861164', 'https://openalex.org/W2970302604', 'https://openalex.org/W2022248256', 'https://openalex.org/W2061926706', 'https://openalex.org/W2963858717', 'https://openalex.org/W2963300719', 'https://openalex.org/W2474920236', 'https://openalex.org/W2774135397', 'https://openalex.org/W2747898905', 'https://openalex.org/W3104725225', 'https://openalex.org/W3002209584', 'https://openalex.org/W2773207997', 'https://openalex.org/W3005560345', 'https://openalex.org/W2963136578', 'https://openalex.org/W2618017917', 'https://openalex.org/W2946154049', 'https://openalex.org/W2888090831', 'https://openalex.org/W3004087721', 'https://openalex.org/W2530952963', 'https://openalex.org/W2983101505', 'https://openalex.org/W4293765466', 'https://openalex.org/W2949271354', 'https://openalex.org/W2888723145', 'https://openalex.org/W2978238437', 'https://openalex.org/W2964065616', 'https://openalex.org/W2769863663', 'https://openalex.org/W2761228892', 'https://openalex.org/W2510842514', 'https://openalex.org/W3170997252', 'https://openalex.org/W3030163527', 'https://openalex.org/W340244495', 'https://openalex.org/W2171473263', 'https://openalex.org/W1921523184', 'https://openalex.org/W1677182931', 'https://openalex.org/W2981159971', 'https://openalex.org/W3034302069', 'https://openalex.org/W3038058348', 'https://openalex.org/W4292779060', 'https://openalex.org/W2784367312', 'https://openalex.org/W209229405', 'https://openalex.org/W2182300166', 'https://openalex.org/W2963410064', 'https://openalex.org/W2093219407', 'https://openalex.org/W2787938642', 'https://openalex.org/W2913657017', 'https://openalex.org/W1882655682', 'https://openalex.org/W2304648132', 'https://openalex.org/W2998592005', 'https://openalex.org/W2908533320', 'https://openalex.org/W2964167449', 'https://openalex.org/W2962949994', 'https://openalex.org/W2101720643', 'https://openalex.org/W1798945469', 'https://openalex.org/W3103543904', 'https://openalex.org/W3035703639', 'https://openalex.org/W1959608418', 'https://openalex.org/W2623451521', 'https://openalex.org/W2548275288', 'https://openalex.org/W2950577311', 'https://openalex.org/W3021941048', 'https://openalex.org/W1815076433', 'https://openalex.org/W2134797427', 'https://openalex.org/W3104586951', 'https://openalex.org/W2963063862', 'https://openalex.org/W3034633006', 'https://openalex.org/W4303633609', 'https://openalex.org/W2963024489', 'https://openalex.org/W2501270130', 'https://openalex.org/W2963921621', 'https://openalex.org/W2772721022', 'https://openalex.org/W2970971581', 'https://openalex.org/W3127808184', 'https://openalex.org/W2964068654', 'https://openalex.org/W2951747710', 'https://openalex.org/W2950776302', 'https://openalex.org/W2963341956', 'https://openalex.org/W3202623779', 'https://openalex.org/W1847063015', 'https://openalex.org/W3003582143', 'https://openalex.org/W1882958252', 'https://openalex.org/W2161064560', 'https://openalex.org/W1873032297', 'https://openalex.org/W2780544323', 'https://openalex.org/W3011640969', 'https://openalex.org/W1895577753', 'https://openalex.org/W3001727690', 'https://openalex.org/W4361793154', 'https://openalex.org/W2970338090', 'https://openalex.org/W2585521554', 'https://openalex.org/W2069992838', 'https://openalex.org/W2002530217', 'https://openalex.org/W2979245724', 'https://openalex.org/W2964308564', 'https://openalex.org/W3005940936', 'https://openalex.org/W2949676527', 'https://openalex.org/W2963855133', 'https://openalex.org/W169849560', 'https://openalex.org/W2953030256', 'https://openalex.org/W2811135961', 'https://openalex.org/W175328650', 'https://openalex.org/W2962933129', 'https://openalex.org/W3034873438', 'https://openalex.org/W3022566517', 'https://openalex.org/W574168211', 'https://openalex.org/W1799366690', 'https://openalex.org/W2797862701', 'https://openalex.org/W2187089797', 'https://openalex.org/W2963907629', 'https://openalex.org/W2998718015', 'https://openalex.org/W3024883697', 'https://openalex.org/W2523246573', 'https://openalex.org/W2976292093', 'https://openalex.org/W2962861703', 'https://openalex.org/W2533523411', 'https://openalex.org/W2977882543', 'https://openalex.org/W2794992746', 'https://openalex.org/W3027040251', 'https://openalex.org/W2502312327', 'https://openalex.org/W160140734', 'https://openalex.org/W3026800838', 'https://openalex.org/W3106130756', 'https://openalex.org/W4288020932', 'https://openalex.org/W2125930537', 'https://openalex.org/W2131744502', 'https://openalex.org/W2964347220', 'https://openalex.org/W1522301498', 'https://openalex.org/W2970261256', 'https://openalex.org/W2953609268', 'https://openalex.org/W1514535095', 'https://openalex.org/W2102676553', 'https://openalex.org/W2963379396', 'https://openalex.org/W2938260698', 'https://openalex.org/W2611453176', 'https://openalex.org/W2612706635', 'https://openalex.org/W2996422681', 'https://openalex.org/W2907274850', 'https://openalex.org/W4297775537', 'https://openalex.org/W1525761558', 'https://openalex.org/W4392667864', 'https://openalex.org/W2963821229', 'https://openalex.org/W2940953217', 'https://openalex.org/W3015504877', 'https://openalex.org/W3026761822', 'https://openalex.org/W2963836885', 'https://openalex.org/W2135184018', 'https://openalex.org/W2592815700', 'https://openalex.org/W3101486429', 'https://openalex.org/W2037954058', 'https://openalex.org/W2964046669', 'https://openalex.org/W2186615578', 'https://openalex.org/W2106693504', 'https://openalex.org/W2964309636', 'https://openalex.org/W2979450545', 'https://openalex.org/W2099471712', 'https://openalex.org/W2953212265', 'https://openalex.org/W2970332347', 'https://openalex.org/W3000086424', 'https://openalex.org/W2895699741', 'https://openalex.org/W2045721448', 'https://openalex.org/W3140895391', 'https://openalex.org/W2134603844', 'https://openalex.org/W2510850936', 'https://openalex.org/W2755304340', 'https://openalex.org/W3105982350', 'https://openalex.org/W2962897394', 'https://openalex.org/W2156737235', 'https://openalex.org/W2910861656', 'https://openalex.org/W2949254415', 'https://openalex.org/W2766362889', 'https://openalex.org/W3037932933', 'https://openalex.org/W2963516298', 'https://openalex.org/W2964284859', 'https://openalex.org/W2155893237', 'https://openalex.org/W1549438340', 'https://openalex.org/W2123045220', 'https://openalex.org/W645151283', 'https://openalex.org/W2902298341', 'https://openalex.org/W2985039775', 'https://openalex.org/W2964335273', 'https://openalex.org/W2726140679', 'https://openalex.org/W2964050767', 'https://openalex.org/W2963644868', 'https://openalex.org/W2950367719', 'https://openalex.org/W2426005664', 'https://openalex.org/W2786407536', 'https://openalex.org/W2940752253', 'https://openalex.org/W2951248065', 'https://openalex.org/W3098900881', 'https://openalex.org/W2157364932', 'https://openalex.org/W3105259917', 'https://openalex.org/W3004117733', 'https://openalex.org/W2750854727', 'https://openalex.org/W2948120601', 'https://openalex.org/W3096456556', 'https://openalex.org/W2214409633', 'https://openalex.org/W3013300757', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963944430', 'https://openalex.org/W2963184277', 'https://openalex.org/W3101399618', 'https://openalex.org/W1560724230', 'https://openalex.org/W2979636403', 'https://openalex.org/W2588610957', 'https://openalex.org/W2553754377', 'https://openalex.org/W3013529009', 'https://openalex.org/W2964125708', 'https://openalex.org/W2969316698', 'https://openalex.org/W2950601690', 'https://openalex.org/W2253400648', 'https://openalex.org/W3017746288', 'https://openalex.org/W3014367186', 'https://openalex.org/W3019430335', 'https://openalex.org/W2977803191', 'https://openalex.org/W2963966020', 'https://openalex.org/W4297779039', 'https://openalex.org/W1945616565', 'https://openalex.org/W2409744450', 'https://openalex.org/W2981643690', 'https://openalex.org/W2785678896', 'https://openalex.org/W4297781872', 'https://openalex.org/W4287802874', 'https://openalex.org/W1570563254', 'https://openalex.org/W2764124976', 'https://openalex.org/W2806541789', 'https://openalex.org/W2963097630', 'https://openalex.org/W104184427', 'https://openalex.org/W1717365219', 'https://openalex.org/W2999607073', 'https://openalex.org/W2963686971', 'https://openalex.org/W2941910275', 'https://openalex.org/W3000346474', 'https://openalex.org/W2902666438', 'https://openalex.org/W2237315208', 'https://openalex.org/W2999803881', 'https://openalex.org/W3037261120', 'https://openalex.org/W2047028564', 'https://openalex.org/W2101234009', 'https://openalex.org/W3103959529', 'https://openalex.org/W1576278180', 'https://openalex.org/W2898422183', 'https://openalex.org/W2962843773', 'https://openalex.org/W2962947361', 'https://openalex.org/W4297736277', 'https://openalex.org/W2905265880', 'https://openalex.org/W4295270931', 'https://openalex.org/W3044481990', 'https://openalex.org/W2066636486', 'https://openalex.org/W2798113456', 'https://openalex.org/W2907747478', 'https://openalex.org/W2793416812', 'https://openalex.org/W2989221291', 'https://openalex.org/W2124450277', 'https://openalex.org/W2170114828', 'https://openalex.org/W2981021427', 'https://openalex.org/W2963399222', 'https://openalex.org/W2952716587', 'https://openalex.org/W2512098328', 'https://openalex.org/W2890166761', 'https://openalex.org/W2619307294', 'https://openalex.org/W2909558473', 'https://openalex.org/W2911884654', 'https://openalex.org/W2584271190', 'https://openalex.org/W2951720195', 'https://openalex.org/W2963418779', 'https://openalex.org/W2804512604', 'https://openalex.org/W4310116239', 'https://openalex.org/W2918941755', 'https://openalex.org/W3048577855', 'https://openalex.org/W2914611487', 'https://openalex.org/W2987705220', 'https://openalex.org/W3101675420', 'https://openalex.org/W2971019578', 'https://openalex.org/W2408279554', 'https://openalex.org/W2952194250', 'https://openalex.org/W2742079690', 'https://openalex.org/W3103174826', 'https://openalex.org/W2214802144', 'https://openalex.org/W3105936962', 'https://openalex.org/W2963775850', 'https://openalex.org/W2905342215', 'https://openalex.org/W2972854137', 'https://openalex.org/W1548328233', 'https://openalex.org/W2964024268', 'https://openalex.org/W2950621961', 'https://openalex.org/W2189162242', 'https://openalex.org/W2394812591', 'https://openalex.org/W3035665735', 'https://openalex.org/W3099524944', 'https://openalex.org/W2194775991', 'https://openalex.org/W2975301014', 'https://openalex.org/W3008835331', 'https://openalex.org/W3099802519', 'https://openalex.org/W3034786558', 'https://openalex.org/W3164053340', 'https://openalex.org/W2531092293', 'https://openalex.org/W2924351488', 'https://openalex.org/W2990979548', 'https://openalex.org/W2953384591', 'https://openalex.org/W4287999998', 'https://openalex.org/W4320013936', 'https://openalex.org/W2183341477', 'https://openalex.org/W4288375408', 'https://openalex.org/W2946866905', 'https://openalex.org/W2941013165', 'https://openalex.org/W2996822312', 'https://openalex.org/W2963703618', 'https://openalex.org/W2950775481', 'https://openalex.org/W2269675680', 'https://openalex.org/W4297779379', 'https://openalex.org/W2593414223', 'https://openalex.org/W2797383850', 'https://openalex.org/W2997277308', 'https://openalex.org/W3126390337', 'https://openalex.org/W9722547', 'https://openalex.org/W1928278792', 'https://openalex.org/W4299518610', 'https://openalex.org/W2980149079', 'https://openalex.org/W2970674435', 'https://openalex.org/W3014795415', 'https://openalex.org/W2016677154', 'https://openalex.org/W2491035845', 'https://openalex.org/W2463236597', 'https://openalex.org/W3105958861', 'https://openalex.org/W2750384547', 'https://openalex.org/W2892210823', 'https://openalex.org/W2160067530', 'https://openalex.org/W2178237821', 'https://openalex.org/W2950237263', 'https://openalex.org/W2377422330', 'https://openalex.org/W2795626670', 'https://openalex.org/W2263490141', 'https://openalex.org/W2803808038', 'https://openalex.org/W3020352423', 'https://openalex.org/W2999763775', 'https://openalex.org/W2962821147', 'https://openalex.org/W2993951401', 'https://openalex.org/W3025104046', 'https://openalex.org/W2975241285', 'https://openalex.org/W16016350', 'https://openalex.org/W2062022900', 'https://openalex.org/W2739748921', 'https://openalex.org/W3007157104', 'https://openalex.org/W2963086938', 'https://openalex.org/W2949242443', 'https://openalex.org/W4230538647', 'https://openalex.org/W2972351704', 'https://openalex.org/W2568343048', 'https://openalex.org/W3047011367', 'https://openalex.org/W3015587821', 'https://openalex.org/W3098834448', 'https://openalex.org/W2963523627', 'https://openalex.org/W2945195708', 'https://openalex.org/W2189064020', 'https://openalex.org/W3103105237', 'https://openalex.org/W2524365899', 'https://openalex.org/W2990836033', 'https://openalex.org/W2915595685', 'https://openalex.org/W2148433392', 'https://openalex.org/W2946840143', 'https://openalex.org/W2890025384', 'https://openalex.org/W3007217209', 'https://openalex.org/W2949357998', 'https://openalex.org/W2968965387', 'https://openalex.org/W2890491044', 'https://openalex.org/W2925181975', 'https://openalex.org/W3034685389', 'https://openalex.org/W2520164769', 'https://openalex.org/W2617573776', 'https://openalex.org/W2605516844', 'https://openalex.org/W2284646714', 'https://openalex.org/W4300844183', 'https://openalex.org/W1600308401', 'https://openalex.org/W2902113386', 'https://openalex.org/W2744947029', 'https://openalex.org/W2964345061', 'https://openalex.org/W4300537377', 'https://openalex.org/W3041187325', 'https://openalex.org/W2953361820', 'https://openalex.org/W2799265886', 'https://openalex.org/W2790965620', 'https://openalex.org/W2964259506', 'https://openalex.org/W2077954842', 'https://openalex.org/W2903353271', 'https://openalex.org/W2924686934', 'https://openalex.org/W2793146153', 'https://openalex.org/W2541674938', 'https://openalex.org/W2533142838', 'https://openalex.org/W3099186728', 'https://openalex.org/W3102469503', 'https://openalex.org/W1547531277', 'https://openalex.org/W2976736845', 'https://openalex.org/W1800356822', 'https://openalex.org/W3099657135', 'https://openalex.org/W2893801697', 'https://openalex.org/W2788079077', 'https://openalex.org/W2972520432', 'https://openalex.org/W2994749257', 'https://openalex.org/W2623491082', 'https://openalex.org/W2968917279', 'https://openalex.org/W2970176896', 'https://openalex.org/W2790529647', 'https://openalex.org/W2963952467', 'https://openalex.org/W2896152796', 'https://openalex.org/W2950634328', 'https://openalex.org/W3002944878', 'https://openalex.org/W2962970252', 'https://openalex.org/W2739028090', 'https://openalex.org/W2619445857', 'https://openalex.org/W2964309882', 'https://openalex.org/W2962747693', 'https://openalex.org/W2475287302', 'https://openalex.org/W3036670859', 'https://openalex.org/W2982304716', 'https://openalex.org/W4289147229', 'https://openalex.org/W2930863966', 'https://openalex.org/W3021164770', 'https://openalex.org/W3153872861', 'https://openalex.org/W2971705906', 'https://openalex.org/W2795783309', 'https://openalex.org/W2902365926', 'https://openalex.org/W2250904038', 'https://openalex.org/W2972937152', 'https://openalex.org/W2954451301', 'https://openalex.org/W2218318129', 'https://openalex.org/W4300972885', 'https://openalex.org/W2479750863', 'https://openalex.org/W3199725555', 'https://openalex.org/W4287812170', 'https://openalex.org/W4385245566', 'https://openalex.org/W2964081807', 'https://openalex.org/W2617845992', 'https://openalex.org/W2531409750', 'https://openalex.org/W2777662428', 'https://openalex.org/W2921224286', 'https://openalex.org/W2077541282', 'https://openalex.org/W2754132561', 'https://openalex.org/W2086217001', 'https://openalex.org/W2580441494', 'https://openalex.org/W2965141076', 'https://openalex.org/W3145589026', 'https://openalex.org/W2887280559', 'https://openalex.org/W1026270304', 'https://openalex.org/W2944869944', 'https://openalex.org/W2905373823', 'https://openalex.org/W3033079657', 'https://openalex.org/W3184998487', 'https://openalex.org/W2982095480', 'https://openalex.org/W2945744995', 'https://openalex.org/W2250539671', 'https://openalex.org/W2953141406', 'https://openalex.org/W3013549950', 'https://openalex.org/W3100115611', 'https://openalex.org/W2996497776', 'https://openalex.org/W2963826681', 'https://openalex.org/W2170798597', 'https://openalex.org/W2952410051', 'https://openalex.org/W3104979525', 'https://openalex.org/W4245619304', 'https://openalex.org/W2254014057', 'https://openalex.org/W2935588605', 'https://openalex.org/W4297797035', 'https://openalex.org/W2904849495', 'https://openalex.org/W2109005788', 'https://openalex.org/W3040620603', 'https://openalex.org/W3120740533', 'https://openalex.org/W3007216417', 'https://openalex.org/W2561907692', 'https://openalex.org/W2951527505', 'https://openalex.org/W2533214323', 'https://openalex.org/W2963658737', 'https://openalex.org/W2009516879', 'https://openalex.org/W2777685882', 'https://openalex.org/W1901129140', 'https://openalex.org/W2778749116', 'https://openalex.org/W3158122532', 'https://openalex.org/W2605287285', 'https://openalex.org/W2944074706', 'https://openalex.org/W3098297200', 'https://openalex.org/W2163605009', 'https://openalex.org/W2893275059', 'https://openalex.org/W2116383462', 'https://openalex.org/W2896035551', 'https://openalex.org/W1967802285', 'https://openalex.org/W3132455321', 'https://openalex.org/W4295312788', 'https://openalex.org/W2745868649', 'https://openalex.org/W4297824601', 'https://openalex.org/W2270352072', 'https://openalex.org/W2964204553', 'https://openalex.org/W2969797940', 'https://openalex.org/W4289293306', 'https://openalex.org/W1694178301', 'https://openalex.org/W2156387975', 'https://openalex.org/W3118608800', 'https://openalex.org/W3103624025', 'https://openalex.org/W2943008967', 'https://openalex.org/W1936408885', 'https://openalex.org/W2108677974', 'https://openalex.org/W3103587400', 'https://openalex.org/W2962839335', 'https://openalex.org/W2786532017', 'https://openalex.org/W2418033038', 'https://openalex.org/W2885340141', 'https://openalex.org/W2525778437', 'https://openalex.org/W3193042612', 'https://openalex.org/W2964234718', 'https://openalex.org/W2604782107', 'https://openalex.org/W2622263826', 'https://openalex.org/W4293455017', 'https://openalex.org/W2951004968', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964153283', 'https://openalex.org/W2604934189', 'https://openalex.org/W2974587585', 'https://openalex.org/W2156236051', 'https://openalex.org/W3035804495', 'https://openalex.org/W4299590935', 'https://openalex.org/W3040743533', 'https://openalex.org/W2156440942', 'https://openalex.org/W3004056434', 'https://openalex.org/W2970157301', 'https://openalex.org/W4287704709', 'https://openalex.org/W4242555270', 'https://openalex.org/W2970475648', 'https://openalex.org/W2997788524', 'https://openalex.org/W2914117107', 'https://openalex.org/W3036129238', 'https://openalex.org/W2795316674', 'https://openalex.org/W2964174623', 'https://openalex.org/W2768916635', 'https://openalex.org/W180287672', 'https://openalex.org/W2950771458', 'https://openalex.org/W2914450692', 'https://openalex.org/W2987881686', 'https://openalex.org/W581956982', 'https://openalex.org/W2796384729', 'https://openalex.org/W2580175322', 'https://openalex.org/W2941531368', 'https://openalex.org/W3033667406', 'https://openalex.org/W2950397435', 'https://openalex.org/W2949887861', 'https://openalex.org/W3013117577', 'https://openalex.org/W2972163183', 'https://openalex.org/W2761705332', 'https://openalex.org/W2767274188', 'https://openalex.org/W2749988060', 'https://openalex.org/W2147527908', 'https://openalex.org/W2977044154', 'https://openalex.org/W2338908902', 'https://openalex.org/W2764011276', 'https://openalex.org/W2921961104', 'https://openalex.org/W830076066', 'https://openalex.org/W3126095862', 'https://openalex.org/W2799042347', 'https://openalex.org/W2962858109', 'https://openalex.org/W2963126915', 'https://openalex.org/W3098269892', 'https://openalex.org/W4297747548', 'https://openalex.org/W139960808', 'https://openalex.org/W2592076308', 'https://openalex.org/W2115252128', 'https://openalex.org/W3185864054', 'https://openalex.org/W3124561108', 'https://openalex.org/W2897836315', 'https://openalex.org/W3098379356', 'https://openalex.org/W4298135002', 'https://openalex.org/W4285652445', 'https://openalex.org/W2963735467', 'https://openalex.org/W2785658572', 'https://openalex.org/W2991128910', 'https://openalex.org/W3091905774', 'https://openalex.org/W2105378942', 'https://openalex.org/W2914241732', 'https://openalex.org/W4288320011', 'https://openalex.org/W3013377162', 'https://openalex.org/W1614298861', 'https://openalex.org/W1667652561', 'https://openalex.org/W3038103902', 'https://openalex.org/W2724169821', 'https://openalex.org/W3035743198', 'https://openalex.org/W2624871570', 'https://openalex.org/W2594103415', 'https://openalex.org/W2775127214', 'https://openalex.org/W3123837026', 'https://openalex.org/W1564049492', 'https://openalex.org/W4285719527', 'https://openalex.org/W2922234936', 'https://openalex.org/W2773689216', 'https://openalex.org/W2885311373', 'https://openalex.org/W1665214252', 'https://openalex.org/W3034273654', 'https://openalex.org/W3101294892', 'https://openalex.org/W2963725279', 'https://openalex.org/W3124452217', 'https://openalex.org/W2118776487', 'https://openalex.org/W2139126750', 'https://openalex.org/W2950094539', 'https://openalex.org/W3046716611', 'https://openalex.org/W3102928651', 'https://openalex.org/W1620012354', 'https://openalex.org/W2962750597', 'https://openalex.org/W3029251950', 'https://openalex.org/W2108563286', 'https://openalex.org/W2099244020', 'https://openalex.org/W2904609564', 'https://openalex.org/W2578206533', 'https://openalex.org/W3028262105', 'https://openalex.org/W3028130721', 'https://openalex.org/W647635722', 'https://openalex.org/W2766836091', 'https://openalex.org/W3032371044', 'https://openalex.org/W2789063758', 'https://openalex.org/W3025690114', 'https://openalex.org/W2182361439', 'https://openalex.org/W2912322140', 'https://openalex.org/W2902240649', 'https://openalex.org/W3048671136', 'https://openalex.org/W2963755427', 'https://openalex.org/W2995532175', 'https://openalex.org/W2963739978', 'https://openalex.org/W2097117768', 'https://openalex.org/W2947805651', 'https://openalex.org/W2948065500', 'https://openalex.org/W2963285578', 'https://openalex.org/W3033947008', 'https://openalex.org/W2619947201', 'https://openalex.org/W3022730539', 'https://openalex.org/W2967193594', 'https://openalex.org/W2734957715', 'https://openalex.org/W4294583674', 'https://openalex.org/W897582192', 'https://openalex.org/W2567547739', 'https://openalex.org/W2736062019', 'https://openalex.org/W2906967080', 'https://openalex.org/W2514839201', 'https://openalex.org/W2171590421', 'https://openalex.org/W2593768305', 'https://openalex.org/W2963454111', 'https://openalex.org/W2044296556', 'https://openalex.org/W3131342469', 'https://openalex.org/W4311361333', 'https://openalex.org/W2949561945', 'https://openalex.org/W2905181083', 'https://openalex.org/W2291973609', 'https://openalex.org/W2785523195', 'https://openalex.org/W1501897777', 'https://openalex.org/W2963403868', 'https://openalex.org/W2994689640', 'https://openalex.org/W3046666734', 'https://openalex.org/W4293391527', 'https://openalex.org/W3037484309', 'https://openalex.org/W2963959597', 'https://openalex.org/W4289468615', 'https://openalex.org/W2902857081', 'https://openalex.org/W2143382278', 'https://openalex.org/W2112531993', 'https://openalex.org/W2913279420', 'https://openalex.org/W2963751813', 'https://openalex.org/W2964294659', 'https://openalex.org/W2614839826', 'https://openalex.org/W2580087858', 'https://openalex.org/W2963711014', 'https://openalex.org/W2133319764', 'https://openalex.org/W3127561923', 'https://openalex.org/W2984236338', 'https://openalex.org/W2951651062', 'https://openalex.org/W3009525345', 'https://openalex.org/W2966145721', 'https://openalex.org/W2948978827', 'https://openalex.org/W2924828084', 'https://openalex.org/W3014734202', 'https://openalex.org/W2986043108', 'https://openalex.org/W2962879692', 'https://openalex.org/W2963304263', 'https://openalex.org/W2153579005', 'https://openalex.org/W2145094598', 'https://openalex.org/W3134412439', 'https://openalex.org/W3048030262', 'https://openalex.org/W2467604901', 'https://openalex.org/W3017777480', 'https://openalex.org/W2135646341', 'https://openalex.org/W2963917928', 'https://openalex.org/W2788853733', 'https://openalex.org/W158003358', 'https://openalex.org/W2767434619', 'https://openalex.org/W2021527233', 'https://openalex.org/W2962914733', 'https://openalex.org/W2901096562', 'https://openalex.org/W1513879401', 'https://openalex.org/W3101623803', 'https://openalex.org/W2973160330', 'https://openalex.org/W644063332', 'https://openalex.org/W2982753834', 'https://openalex.org/W2550100435', 'https://openalex.org/W2964409475', 'https://openalex.org/W2914242365', 'https://openalex.org/W2897818259', 'https://openalex.org/W3034749675', 'https://openalex.org/W2886817722', 'https://openalex.org/W3034093514', 'https://openalex.org/W2949969799', 'https://openalex.org/W3216759837', 'https://openalex.org/W2972565221', 'https://openalex.org/W2963703360', 'https://openalex.org/W2566198195', 'https://openalex.org/W2765793020', 'https://openalex.org/W375886181', 'https://openalex.org/W3080998698', 'https://openalex.org/W2944745903', 'https://openalex.org/W1570448133', 'https://openalex.org/W2553303224', 'https://openalex.org/W2952842591', 'https://openalex.org/W2630837129', 'https://openalex.org/W2973400721', 'https://openalex.org/W4287828539', 'https://openalex.org/W2910207440', 'https://openalex.org/W3016749615', 'https://openalex.org/W2133564696', 'https://openalex.org/W3037144731', 'https://openalex.org/W1876369880', 'https://openalex.org/W3173940976', 'https://openalex.org/W2962972936', 'https://openalex.org/W2621042378', 'https://openalex.org/W2296616510', 'https://openalex.org/W2790840495', 'https://openalex.org/W1853900790', 'https://openalex.org/W3098389804', 'https://openalex.org/W2897492435', 'https://openalex.org/W2471688006', 'https://openalex.org/W4289284996', 'https://openalex.org/W2583383421', 'https://openalex.org/W4297789187', 'https://openalex.org/W3018785946', 'https://openalex.org/W4302375066', 'https://openalex.org/W3034971196', 'https://openalex.org/W3102910883', 'https://openalex.org/W2955060956', 'https://openalex.org/W3035253074', 'https://openalex.org/W3008689640', 'https://openalex.org/W3100456593', 'https://openalex.org/W2809514947', 'https://openalex.org/W2900959181', 'https://openalex.org/W2072879417', 'https://openalex.org/W2127589108', 'https://openalex.org/W2337199865', 'https://openalex.org/W2798365772', 'https://openalex.org/W2619516334', 'https://openalex.org/W2810505070', 'https://openalex.org/W4302296459', 'https://openalex.org/W2952794062', 'https://openalex.org/W2900604734', 'https://openalex.org/W2945125725', 'https://openalex.org/W2963685250', 'https://openalex.org/W3009226886', 'https://openalex.org/W2437974299', 'https://openalex.org/W2803183915', 'https://openalex.org/W2962742960', 'https://openalex.org/W3097982873', 'https://openalex.org/W2985641038', 'https://openalex.org/W2912934387', 'https://openalex.org/W2025768430', 'https://openalex.org/W2924397824', 'https://openalex.org/W2097732278', 'https://openalex.org/W3010570535', 'https://openalex.org/W3007848645', 'https://openalex.org/W1490600648', 'https://openalex.org/W2962990163', 'https://openalex.org/W2137825550', 'https://openalex.org/W2914484425', 'https://openalex.org/W2964088238', 'https://openalex.org/W2580662672', 'https://openalex.org/W2950527759', 'https://openalex.org/W2936868034', 'https://openalex.org/W4295521014', 'https://openalex.org/W3016242708', 'https://openalex.org/W2963864421', 'https://openalex.org/W2773335402', 'https://openalex.org/W4394670452', 'https://openalex.org/W4288106291', 'https://openalex.org/W2971018118', 'https://openalex.org/W3000216503', 'https://openalex.org/W2949117887']",2020-12-22
https://openalex.org/W3120243996,https://doi.org/10.48550/arxiv.2011.10650,Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them\n on Images,"We present a hierarchical VAE that, for the first time, generates samples\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\nautoregressive models, as well as faster, better models if they exist, when\nmade sufficiently deep. Despite this, autoregressive models have historically\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\nby scaling a VAE to greater stochastic depth than previously explored and\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\nsamples thousands of times faster, and are more easily applied to\nhigh-resolution images. Qualitative studies suggest this is because the VAE\nlearns efficient hierarchical visual representations. We release our source\ncode and models at https://github.com/openai/vdvae.\n","['https://openalex.org/W2949899814', 'https://openalex.org/W3034445277', 'https://openalex.org/W1909320841', 'https://openalex.org/W2963428348', 'https://openalex.org/W3030163527', 'https://openalex.org/W3041956526', 'https://openalex.org/W2963139417', 'https://openalex.org/W2764301726', 'https://openalex.org/W2140574335', 'https://openalex.org/W2963636093', 'https://openalex.org/W2964122153', 'https://openalex.org/W2963809228', 'https://openalex.org/W2911290044', 'https://openalex.org/W3150807214', 'https://openalex.org/W2949382160', 'https://openalex.org/W1583912456', 'https://openalex.org/W2963135265', 'https://openalex.org/W3021164770', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963685250', 'https://openalex.org/W2560512785', 'https://openalex.org/W2964343746', 'https://openalex.org/W3036167779', 'https://openalex.org/W3103543904', 'https://openalex.org/W2970014727', 'https://openalex.org/W2409550820', 'https://openalex.org/W2963292439', 'https://openalex.org/W2940744433', 'https://openalex.org/W2026799324', 'https://openalex.org/W2587284713', 'https://openalex.org/W2902630600', 'https://openalex.org/W2194775991', 'https://openalex.org/W1499798934', 'https://openalex.org/W2962750131']",2020-11-20
https://openalex.org/W3049247973,https://doi.org/10.48550/arxiv.2008.07142,POP909: A Pop-song Dataset for Music Arrangement Generation,"Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.","['https://openalex.org/W2475687244', 'https://openalex.org/W2018738723', 'https://openalex.org/W2919624000', 'https://openalex.org/W2123306325', 'https://openalex.org/W2949382160', 'https://openalex.org/W2591553092', 'https://openalex.org/W2963681776', 'https://openalex.org/W2937519396', 'https://openalex.org/W2962699318', 'https://openalex.org/W1519655822', 'https://openalex.org/W1819710477', 'https://openalex.org/W2396710372', 'https://openalex.org/W2906933017', 'https://openalex.org/W2963985773', 'https://openalex.org/W2903916098', 'https://openalex.org/W2396433901', 'https://openalex.org/W2398002958', 'https://openalex.org/W2964051853', 'https://openalex.org/W2964706117', 'https://openalex.org/W2797595207', 'https://openalex.org/W3112895167', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2991421901', 'https://openalex.org/W103489607', 'https://openalex.org/W2963575853', 'https://openalex.org/W2572771584', 'https://openalex.org/W2898148140', 'https://openalex.org/W3021164770', 'https://openalex.org/W2142996485', 'https://openalex.org/W3005441616', 'https://openalex.org/W2946488335', 'https://openalex.org/W579034728']",2020-08-17
https://openalex.org/W3009948090,,On the Convergence of Adam and Adagrad,"We provide a simple proof of the convergence of the optimization algorithms Adam and Adagrad with the assumptions of smooth gradients and almost sure uniform bound on the $\ell_\infty$ norm of the gradients. This work builds on the techniques introduced by Ward et al. (2019) and extends them to the Adam optimizer. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations N. This bound can be made arbitrarily small. In particular, Adam with a learning rate $\alpha=1/\sqrt{N}$ and a momentum parameter on squared gradients $\beta_2=1 - 1/N$ achieves the same rate of convergence $O(\ln(N)/\sqrt{N})$ as Adagrad. Thus, it is possible to use Adam as a finite horizon version of Adagrad, much like constant step size SGD can be used instead of its asymptotically converging decaying step size version.","['https://openalex.org/W2757196798', 'https://openalex.org/W1988720110', 'https://openalex.org/W2964121744', 'https://openalex.org/W2096840748', 'https://openalex.org/W2963450615', 'https://openalex.org/W3021164770', 'https://openalex.org/W2785523195', 'https://openalex.org/W2951395930', 'https://openalex.org/W2963613486', 'https://openalex.org/W2132211083', 'https://openalex.org/W2919115771', 'https://openalex.org/W2619516334', 'https://openalex.org/W2945414017', 'https://openalex.org/W2963563140', 'https://openalex.org/W2106709023', 'https://openalex.org/W2946511237', 'https://openalex.org/W2886463271', 'https://openalex.org/W2337540838', 'https://openalex.org/W2947578309', 'https://openalex.org/W2805767638']",2020-03-05
https://openalex.org/W3169882120,https://doi.org/10.3390/electronics10111349,Stochastic Restoration of Heavily Compressed Musical Audio Using Generative Adversarial Networks,"Lossy audio codecs compress (and decompress) digital audio streams by removing information that tends to be inaudible in human perception. Under high compression rates, such codecs may introduce a variety of impairments in the audio signal. Many works have tackled the problem of audio enhancement and compression artifact removal using deep-learning techniques. However, only a few works tackle the restoration of heavily compressed audio signals in the musical domain. In such a scenario, there is no unique solution for the restoration of the original signal. Therefore, in this study, we test a stochastic generator of a Generative Adversarial Network (GAN) architecture for this task. Such a stochastic generator, conditioned on highly compressed musical audio signals, could one day generate outputs indistinguishable from high-quality releases. Therefore, the present study may yield insights into more efficient musical data storage and transmission. We train stochastic and deterministic generators on MP3-compressed audio signals with 16, 32, and 64 kbit/s. We perform an extensive evaluation of the different experiments utilizing objective metrics and listening tests. We find that the models can improve the quality of the audio signals over the MP3 versions for 16 and 32 kbit/s and that the stochastic generators are capable of generating outputs that are closer to the original signals than those of the deterministic generators.","['https://openalex.org/W6633114069', 'https://openalex.org/W6630236247', 'https://openalex.org/W2117497587', 'https://openalex.org/W2711335087', 'https://openalex.org/W2809824582', 'https://openalex.org/W2944079609', 'https://openalex.org/W3015780049', 'https://openalex.org/W6755446875', 'https://openalex.org/W2526733715', 'https://openalex.org/W2998639482', 'https://openalex.org/W3048758284', 'https://openalex.org/W3046669506', 'https://openalex.org/W2100285470', 'https://openalex.org/W1517841224', 'https://openalex.org/W2044893557', 'https://openalex.org/W3006440693', 'https://openalex.org/W2944209089', 'https://openalex.org/W2998498479', 'https://openalex.org/W3081378361', 'https://openalex.org/W4238906441', 'https://openalex.org/W2151667147', 'https://openalex.org/W2167341025', 'https://openalex.org/W1485515796', 'https://openalex.org/W2179369514', 'https://openalex.org/W1590214901', 'https://openalex.org/W1885185971', 'https://openalex.org/W2963073614', 'https://openalex.org/W6741681139', 'https://openalex.org/W2802034954', 'https://openalex.org/W2784918340', 'https://openalex.org/W4245919820', 'https://openalex.org/W4301878342', 'https://openalex.org/W2062164080', 'https://openalex.org/W2586068394', 'https://openalex.org/W2042407743', 'https://openalex.org/W2364134690', 'https://openalex.org/W2128653836', 'https://openalex.org/W1995536493', 'https://openalex.org/W2109349638', 'https://openalex.org/W1999846783', 'https://openalex.org/W2291877678', 'https://openalex.org/W1482149378', 'https://openalex.org/W2600556233', 'https://openalex.org/W2929514213', 'https://openalex.org/W2791210709', 'https://openalex.org/W2963321191', 'https://openalex.org/W2746457594', 'https://openalex.org/W2943895317', 'https://openalex.org/W2998832642', 'https://openalex.org/W2810843531', 'https://openalex.org/W3034771406', 'https://openalex.org/W6738884980', 'https://openalex.org/W3162188526', 'https://openalex.org/W2739748921', 'https://openalex.org/W1903029394', 'https://openalex.org/W1677182931', 'https://openalex.org/W2567070169', 'https://openalex.org/W3022805095', 'https://openalex.org/W6631190155', 'https://openalex.org/W6735913928', 'https://openalex.org/W3118215210', 'https://openalex.org/W6636045042', 'https://openalex.org/W6758675244', 'https://openalex.org/W3127004306', 'https://openalex.org/W2739619458', 'https://openalex.org/W3021164770', 'https://openalex.org/W3096468295', 'https://openalex.org/W2099471712', 'https://openalex.org/W3096159803', 'https://openalex.org/W2618946976', 'https://openalex.org/W2972443522', 'https://openalex.org/W3096408984', 'https://openalex.org/W3102190437', 'https://openalex.org/W1502560247', 'https://openalex.org/W2963453742', 'https://openalex.org/W2605135824', 'https://openalex.org/W1553834069', 'https://openalex.org/W3122318757', 'https://openalex.org/W3097034112', 'https://openalex.org/W2963970792', 'https://openalex.org/W2896030002', 'https://openalex.org/W1606487971', 'https://openalex.org/W2949558265', 'https://openalex.org/W2972785266', 'https://openalex.org/W3093990297', 'https://openalex.org/W2910577860', 'https://openalex.org/W1956021273']",2021-06-05
https://openalex.org/W3139948516,https://doi.org/10.1145/3664595,Creativity and Machine Learning: A Survey,"There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.","['https://openalex.org/W3173683732', 'https://openalex.org/W2802694709', 'https://openalex.org/W2124758073', 'https://openalex.org/W4214612132', 'https://openalex.org/W2019515298', 'https://openalex.org/W2107878631', 'https://openalex.org/W6618926834', 'https://openalex.org/W3037206806', 'https://openalex.org/W3204937802', 'https://openalex.org/W4381786045', 'https://openalex.org/W2963223306', 'https://openalex.org/W4249004190', 'https://openalex.org/W2170862681', 'https://openalex.org/W2160156842', 'https://openalex.org/W2026201730', 'https://openalex.org/W2113096445', 'https://openalex.org/W302203192', 'https://openalex.org/W2088807535', 'https://openalex.org/W4312282373', 'https://openalex.org/W3133706083', 'https://openalex.org/W2137619888', 'https://openalex.org/W4372266552', 'https://openalex.org/W3180355996', 'https://openalex.org/W4281643269', 'https://openalex.org/W3015758605', 'https://openalex.org/W6809847875', 'https://openalex.org/W3162155171', 'https://openalex.org/W4391609325', 'https://openalex.org/W3011309387', 'https://openalex.org/W2963920537', 'https://openalex.org/W2550673061', 'https://openalex.org/W2161506385', 'https://openalex.org/W2490883651', 'https://openalex.org/W3002944878', 'https://openalex.org/W2529996553', 'https://openalex.org/W3046095786', 'https://openalex.org/W4365794116', 'https://openalex.org/W2753738274', 'https://openalex.org/W3175528029', 'https://openalex.org/W2064675550', 'https://openalex.org/W3006025420', 'https://openalex.org/W2963073614', 'https://openalex.org/W1516111018', 'https://openalex.org/W2147223282', 'https://openalex.org/W2312058960', 'https://openalex.org/W3177828909', 'https://openalex.org/W2736137960', 'https://openalex.org/W2127049152', 'https://openalex.org/W2962770929', 'https://openalex.org/W3035574324', 'https://openalex.org/W2901791883', 'https://openalex.org/W4206706211', 'https://openalex.org/W2948978827', 'https://openalex.org/W2788411626', 'https://openalex.org/W2151083897', 'https://openalex.org/W4205991051', 'https://openalex.org/W3034999214', 'https://openalex.org/W2963167310', 'https://openalex.org/W1997107172', 'https://openalex.org/W3022643593', 'https://openalex.org/W3092157356', 'https://openalex.org/W4312497550', 'https://openalex.org/W1975876259', 'https://openalex.org/W1560847364', 'https://openalex.org/W2998571806', 'https://openalex.org/W4288879167', 'https://openalex.org/W2145339207', 'https://openalex.org/W3155103849', 'https://openalex.org/W2026751516', 'https://openalex.org/W2557449848', 'https://openalex.org/W4229494842', 'https://openalex.org/W2250842199', 'https://openalex.org/W2981852735', 'https://openalex.org/W3101652466', 'https://openalex.org/W2170238612', 'https://openalex.org/W2792969545', 'https://openalex.org/W4312933868', 'https://openalex.org/W1972755986', 'https://openalex.org/W4386072096', 'https://openalex.org/W2792289381', 'https://openalex.org/W1546128316', 'https://openalex.org/W2034806191', 'https://openalex.org/W1979769287', 'https://openalex.org/W2586756136', 'https://openalex.org/W4313166664', 'https://openalex.org/W4312423208', 'https://openalex.org/W3100656978', 'https://openalex.org/W1507901999', 'https://openalex.org/W4213299273', 'https://openalex.org/W2965962253', 'https://openalex.org/W2963985284', 'https://openalex.org/W3199229282', 'https://openalex.org/W2110224684', 'https://openalex.org/W2001771035', 'https://openalex.org/W2909679374', 'https://openalex.org/W2909417107', 'https://openalex.org/W3021704162', 'https://openalex.org/W3134624922', 'https://openalex.org/W1989753435', 'https://openalex.org/W2119717200', 'https://openalex.org/W4284898017', 'https://openalex.org/W2963567641', 'https://openalex.org/W3158172841', 'https://openalex.org/W4294754026', 'https://openalex.org/W2891911989', 'https://openalex.org/W2964268978', 'https://openalex.org/W2115221470', 'https://openalex.org/W4214516465', 'https://openalex.org/W2962793481', 'https://openalex.org/W2969725597', 'https://openalex.org/W3132782787', 'https://openalex.org/W2604635588', 'https://openalex.org/W3169017236', 'https://openalex.org/W4212937970', 'https://openalex.org/W3098903812', 'https://openalex.org/W2145482038', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963575853', 'https://openalex.org/W2130942839', 'https://openalex.org/W2122410182', 'https://openalex.org/W2949869682', 'https://openalex.org/W2618625858', 'https://openalex.org/W2126160338', 'https://openalex.org/W2250991680', 'https://openalex.org/W2176009211', 'https://openalex.org/W2964201809', 'https://openalex.org/W2953318193', 'https://openalex.org/W2621430944', 'https://openalex.org/W3094169176', 'https://openalex.org/W2044879021', 'https://openalex.org/W2741104967', 'https://openalex.org/W2141656083', 'https://openalex.org/W2397490357', 'https://openalex.org/W2900548288', 'https://openalex.org/W2576915718', 'https://openalex.org/W2099471712', 'https://openalex.org/W2804078698', 'https://openalex.org/W2893749619', 'https://openalex.org/W2964017345', 'https://openalex.org/W2924242073', 'https://openalex.org/W1912123185', 'https://openalex.org/W2963870701', 'https://openalex.org/W1959608418', 'https://openalex.org/W2257979135', 'https://openalex.org/W1823742419', 'https://openalex.org/W1498806819', 'https://openalex.org/W2784823820', 'https://openalex.org/W4288089799', 'https://openalex.org/W1588334983', 'https://openalex.org/W2971074500', 'https://openalex.org/W2703190149', 'https://openalex.org/W2144230296', 'https://openalex.org/W2962741254', 'https://openalex.org/W2803267010', 'https://openalex.org/W3021164770', 'https://openalex.org/W3106570356', 'https://openalex.org/W2962981281', 'https://openalex.org/W2240846191', 'https://openalex.org/W2111273501', 'https://openalex.org/W2591904660', 'https://openalex.org/W2962699318', 'https://openalex.org/W2144343387', 'https://openalex.org/W2896197082', 'https://openalex.org/W4387195417', 'https://openalex.org/W1571363720', 'https://openalex.org/W2548228487', 'https://openalex.org/W1990309597', 'https://openalex.org/W2963574252', 'https://openalex.org/W2949382160', 'https://openalex.org/W2155027007', 'https://openalex.org/W776164697', 'https://openalex.org/W2962760235', 'https://openalex.org/W1678101717', 'https://openalex.org/W3105926098', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908747729', 'https://openalex.org/W2963403868', 'https://openalex.org/W1630729109', 'https://openalex.org/W2151901989', 'https://openalex.org/W2795116441', 'https://openalex.org/W2128174349', 'https://openalex.org/W2953010338', 'https://openalex.org/W3127861905', 'https://openalex.org/W2557283755', 'https://openalex.org/W3032429508', 'https://openalex.org/W2571927064', 'https://openalex.org/W2950739196', 'https://openalex.org/W4287902759', 'https://openalex.org/W4214700389', 'https://openalex.org/W2963636093', 'https://openalex.org/W2575151422', 'https://openalex.org/W4310895557', 'https://openalex.org/W2963681776', 'https://openalex.org/W1493998861', 'https://openalex.org/W2753868141', 'https://openalex.org/W2604799547', 'https://openalex.org/W4399410307', 'https://openalex.org/W2025768430', 'https://openalex.org/W2757836268', 'https://openalex.org/W2323746689', 'https://openalex.org/W2903186160', 'https://openalex.org/W2989576650', 'https://openalex.org/W2547875792', 'https://openalex.org/W2573431603']",2024-05-11
https://openalex.org/W3118605064,https://doi.org/10.48550/arxiv.2010.00654,VAEBM: A Symbiosis between Variational Autoencoders and Energy-based\n Models,"Energy-based models (EBMs) have recently been successful in representing\ncomplex distributions of small images. However, sampling from them requires\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\nsamples quickly and are equipped with a latent space that enables fast\ntraversal of the data manifold. However, VAEs tend to assign high probability\ndensity to regions in data space outside the actual data distribution and often\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\ncaptures the overall mode structure of the data distribution using a\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\nnon-data-like regions from the model and refine the image samples. Moreover,\nthe VAE component in VAEBM allows us to speed up MCMC updates by\nreparameterizing them in the VAE's latent space. Our experimental results show\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\nseveral benchmark image datasets by a large margin. It can generate\nhigh-quality images as large as 256$\\times$256 pixels with short MCMC chains.\nWe also demonstrate that VAEBM provides complete mode coverage and performs\nwell in out-of-distribution detection. The source code is available at\nhttps://github.com/NVlabs/VAEBM\n","['https://openalex.org/W2963809789', 'https://openalex.org/W2963693742', 'https://openalex.org/W3035101153', 'https://openalex.org/W3014384188', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964121744', 'https://openalex.org/W3035281230', 'https://openalex.org/W2981721547', 'https://openalex.org/W2963812505', 'https://openalex.org/W2099866409', 'https://openalex.org/W2963546708', 'https://openalex.org/W2970771798', 'https://openalex.org/W3035574324', 'https://openalex.org/W3034625979', 'https://openalex.org/W195465510', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963142510', 'https://openalex.org/W2971034910', 'https://openalex.org/W3098510582', 'https://openalex.org/W2972246420', 'https://openalex.org/W2980585949', 'https://openalex.org/W189596042', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963981733', 'https://openalex.org/W1513873506', 'https://openalex.org/W2962760235', 'https://openalex.org/W3012274818', 'https://openalex.org/W2979652999', 'https://openalex.org/W2272361785', 'https://openalex.org/W3128546090', 'https://openalex.org/W2409550820', 'https://openalex.org/W3035068567', 'https://openalex.org/W3036167779', 'https://openalex.org/W3028907532', 'https://openalex.org/W2893749619', 'https://openalex.org/W967544008', 'https://openalex.org/W1579122361', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963275229', 'https://openalex.org/W2962897886', 'https://openalex.org/W2116064496', 'https://openalex.org/W2804123849', 'https://openalex.org/W2962770929', 'https://openalex.org/W2963090522', 'https://openalex.org/W2979776030', 'https://openalex.org/W2152790380', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963408680', 'https://openalex.org/W2970607325', 'https://openalex.org/W3104876213', 'https://openalex.org/W2963836885', 'https://openalex.org/W1959608418', 'https://openalex.org/W3035384201', 'https://openalex.org/W2963685250', 'https://openalex.org/W44815768', 'https://openalex.org/W3035564323', 'https://openalex.org/W2521028896', 'https://openalex.org/W2962947361', 'https://openalex.org/W3021164770', 'https://openalex.org/W3034310115', 'https://openalex.org/W3102021454', 'https://openalex.org/W3094191864', 'https://openalex.org/W2970181183', 'https://openalex.org/W2962820504', 'https://openalex.org/W2979473621', 'https://openalex.org/W3106068426', 'https://openalex.org/W2995085126', 'https://openalex.org/W2013035813', 'https://openalex.org/W3035166812', 'https://openalex.org/W3034459047', 'https://openalex.org/W2587284713', 'https://openalex.org/W2945924057', 'https://openalex.org/W1834627138', 'https://openalex.org/W2986615800', 'https://openalex.org/W2161914416', 'https://openalex.org/W2108501770', 'https://openalex.org/W3107669106', 'https://openalex.org/W3043462782', 'https://openalex.org/W2782980316', 'https://openalex.org/W2963085620', 'https://openalex.org/W2593768305', 'https://openalex.org/W2938608316', 'https://openalex.org/W2971074500', 'https://openalex.org/W2920868047']",2020-10-01
https://openalex.org/W3209873929,https://doi.org/10.18653/v1/2022.findings-naacl.117,Hierarchical Transformers Are More Efficient Language Models,"Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.","['https://openalex.org/W4323654151', 'https://openalex.org/W3174418826', 'https://openalex.org/W3023662336', 'https://openalex.org/W2998108143', 'https://openalex.org/W2940744433', 'https://openalex.org/W3131922516', 'https://openalex.org/W3034445277', 'https://openalex.org/W3177150392', 'https://openalex.org/W3164045210', 'https://openalex.org/W2953318193', 'https://openalex.org/W1901129140', 'https://openalex.org/W2963341956', 'https://openalex.org/W2994673210', 'https://openalex.org/W3177813494', 'https://openalex.org/W3135427360', 'https://openalex.org/W3166509360', 'https://openalex.org/W2946567085', 'https://openalex.org/W3037798801', 'https://openalex.org/W4287802874', 'https://openalex.org/W2996264288', 'https://openalex.org/W2964110616', 'https://openalex.org/W3091156754', 'https://openalex.org/W3033188311', 'https://openalex.org/W4388979610', 'https://openalex.org/W3155806510', 'https://openalex.org/W3034573343', 'https://openalex.org/W3181262653', 'https://openalex.org/W3030163527', 'https://openalex.org/W3001279689', 'https://openalex.org/W4385245566', 'https://openalex.org/W4287122956', 'https://openalex.org/W3171313410', 'https://openalex.org/W4292779060', 'https://openalex.org/W3181186005', 'https://openalex.org/W4309793872', 'https://openalex.org/W3118608800', 'https://openalex.org/W3015468748', 'https://openalex.org/W2896457183', 'https://openalex.org/W3123673616', 'https://openalex.org/W3021164770', 'https://openalex.org/W2423557781']",2022-01-01
https://openalex.org/W3164680725,https://doi.org/10.1186/s13636-021-00208-5,End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network,,"['https://openalex.org/W2134031328', 'https://openalex.org/W2239141610', 'https://openalex.org/W2005418748', 'https://openalex.org/W1849277567', 'https://openalex.org/W2610961739', 'https://openalex.org/W2885005742', 'https://openalex.org/W2937977583', 'https://openalex.org/W2122262023', 'https://openalex.org/W2047116924', 'https://openalex.org/W2064675550', 'https://openalex.org/W3113545796', 'https://openalex.org/W2146334809', 'https://openalex.org/W2313339984', 'https://openalex.org/W6603242443', 'https://openalex.org/W2889462515', 'https://openalex.org/W2890964092', 'https://openalex.org/W2973181312', 'https://openalex.org/W2165880886', 'https://openalex.org/W2150769028', 'https://openalex.org/W2888869035', 'https://openalex.org/W2962850830', 'https://openalex.org/W2194775991', 'https://openalex.org/W6679338098', 'https://openalex.org/W2784665486', 'https://openalex.org/W2037441721', 'https://openalex.org/W2346454595', 'https://openalex.org/W2973034847', 'https://openalex.org/W2951975883', 'https://openalex.org/W2792764867', 'https://openalex.org/W2964243274', 'https://openalex.org/W2648194195', 'https://openalex.org/W2399733683', 'https://openalex.org/W2949117887', 'https://openalex.org/W2963799213', 'https://openalex.org/W2045528981', 'https://openalex.org/W3021164770', 'https://openalex.org/W2915606245', 'https://openalex.org/W2808706139', 'https://openalex.org/W2949382160', 'https://openalex.org/W2747506362', 'https://openalex.org/W2157331557', 'https://openalex.org/W2892071465', 'https://openalex.org/W2158061940', 'https://openalex.org/W2626778328', 'https://openalex.org/W3179321675', 'https://openalex.org/W2747664154', 'https://openalex.org/W2940259008', 'https://openalex.org/W2095705004', 'https://openalex.org/W4541777', 'https://openalex.org/W2966518489']",2021-05-12
https://openalex.org/W3174290991,https://doi.org/10.23919/dafx51585.2021.9768298,A Generative Model for Raw Audio Using Transformer Architectures,"This paper proposes a novel way of doing audio synthesis at the waveform level using Transformer architectures. We propose a deep neural network for generating waveforms, similar to wavenet [1]. This is fully probabilistic, auto-regressive, and causal, i.e. each sample generated depends on only the previously observed samples. Our approach outperforms a widely used wavenet architecture by up to 9% on a similar dataset for predicting the next step. Using the attention mechanism, we enable the architecture to learn which audio samples are important for the prediction of the future sample. We show how causal transformer generative models can be used for raw waveform synthesis. We also show that this performance can be improved by another 2% by conditioning samples over a wider context. The flexibility of the current model to synthesize audio from latent representations suggests a large number of potential applications. The novel approach of using generative transformer architectures for raw audio synthesis is, however, still far away from generating any meaningful music similar to wavenet, without using latent codes/meta-data to aid the generation process.","['https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W2963563276', 'https://openalex.org/W2981851019', 'https://openalex.org/W6774110404', 'https://openalex.org/W6776218486', 'https://openalex.org/W2752796333', 'https://openalex.org/W6761628794', 'https://openalex.org/W6763509872', 'https://openalex.org/W6794463088', 'https://openalex.org/W6755406732', 'https://openalex.org/W6778883912', 'https://openalex.org/W1566660863', 'https://openalex.org/W6768028577', 'https://openalex.org/W2398826216', 'https://openalex.org/W2963609956', 'https://openalex.org/W6748409065', 'https://openalex.org/W2516301810', 'https://openalex.org/W6739901393', 'https://openalex.org/W2593116425', 'https://openalex.org/W2108598243', 'https://openalex.org/W2560180317', 'https://openalex.org/W6843673214', 'https://openalex.org/W2936975153', 'https://openalex.org/W2134900903', 'https://openalex.org/W6784559596', 'https://openalex.org/W6755207826', 'https://openalex.org/W2120847449', 'https://openalex.org/W6750732768', 'https://openalex.org/W6747635987', 'https://openalex.org/W6761447021', 'https://openalex.org/W2916113431', 'https://openalex.org/W6780311315', 'https://openalex.org/W6753018729', 'https://openalex.org/W2072752513', 'https://openalex.org/W6752096078', 'https://openalex.org/W2963103134', 'https://openalex.org/W6751512325', 'https://openalex.org/W2777302760', 'https://openalex.org/W6713134421', 'https://openalex.org/W2749651610', 'https://openalex.org/W2963212250', 'https://openalex.org/W2191779130', 'https://openalex.org/W6631190155', 'https://openalex.org/W6729053843', 'https://openalex.org/W6697040288', 'https://openalex.org/W3003257820', 'https://openalex.org/W6755182157', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964307104', 'https://openalex.org/W2953384591', 'https://openalex.org/W3010387158', 'https://openalex.org/W4385245566', 'https://openalex.org/W3030163527', 'https://openalex.org/W2296724634', 'https://openalex.org/W2896457183', 'https://openalex.org/W2901997113', 'https://openalex.org/W2979476256', 'https://openalex.org/W4323654151', 'https://openalex.org/W2963782041', 'https://openalex.org/W2811079561', 'https://openalex.org/W4292779060', 'https://openalex.org/W3094502228', 'https://openalex.org/W4287391717', 'https://openalex.org/W3099782249', 'https://openalex.org/W3158129762', 'https://openalex.org/W4294619240', 'https://openalex.org/W4301368689', 'https://openalex.org/W2940744433', 'https://openalex.org/W3093494400', 'https://openalex.org/W2540404261', 'https://openalex.org/W4288374038', 'https://openalex.org/W3119866685', 'https://openalex.org/W3042610466', 'https://openalex.org/W2963011080', 'https://openalex.org/W3090140450', 'https://openalex.org/W2950547518', 'https://openalex.org/W4301107988', 'https://openalex.org/W2996383576', 'https://openalex.org/W2519091744', 'https://openalex.org/W4287802874', 'https://openalex.org/W2963233633', 'https://openalex.org/W3021164770', 'https://openalex.org/W3103145119', 'https://openalex.org/W3036601975', 'https://openalex.org/W2941115821', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963452667', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963799213', 'https://openalex.org/W2803963372', 'https://openalex.org/W2402144811', 'https://openalex.org/W2782490852', 'https://openalex.org/W2949382160', 'https://openalex.org/W2973049837', 'https://openalex.org/W2795935131', 'https://openalex.org/W2626778328']",2021-09-08
https://openalex.org/W3212947338,https://doi.org/10.1109/wf-iot51360.2021.9596007,Environmental Sound Classification with Tiny Transformers in Noisy Edge Environments,"The unprecedented growth of edge sensor infrastructure is driving the demand function for in situ analytics, i.e. automated decision support at the point of data collection. In the present work, we detail our state-of-the-art Environmental Sound Classification (ESC) framework that is capable of near real-time acoustic categorization directly at the edge. Existing ESC algorithms primarily train and test on pristine datasets that fail in real-world deployments due their inability to handle real-world noisy environments. Methods to denoise the sounds are often computationally expensive for edge devices and do not guarantee performance improvements. To this end, we investigate a way to make existing ESC models robust and make them work in operational resource-constrained settings. Our framework employs a noisy classification model consisting of a tiny BERT-based Transformer (less than 20,000 parameters) and considers hardening of this model through the use of transmission channel noise augmentation. We detail real-world results through its deployment on a Raspberry Pi Zero and demonstrate its classification performance.","['https://openalex.org/W2947469743', 'https://openalex.org/W2052666245', 'https://openalex.org/W2938440247', 'https://openalex.org/W2608696185', 'https://openalex.org/W2052384514', 'https://openalex.org/W2007645738', 'https://openalex.org/W2106265339', 'https://openalex.org/W6789403026', 'https://openalex.org/W2605945764', 'https://openalex.org/W3094175596', 'https://openalex.org/W2895590231', 'https://openalex.org/W2509065397', 'https://openalex.org/W1972567154', 'https://openalex.org/W3023935494', 'https://openalex.org/W6793119350', 'https://openalex.org/W3095348033', 'https://openalex.org/W3022215617', 'https://openalex.org/W2976146944', 'https://openalex.org/W1979594949', 'https://openalex.org/W2803914058', 'https://openalex.org/W3147387781', 'https://openalex.org/W3136991969', 'https://openalex.org/W3217182838', 'https://openalex.org/W4214612132', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287372095', 'https://openalex.org/W2896457183', 'https://openalex.org/W4287802874', 'https://openalex.org/W3098357269', 'https://openalex.org/W2963403868', 'https://openalex.org/W3021164770', 'https://openalex.org/W3124414150', 'https://openalex.org/W4385245566', 'https://openalex.org/W3094502228']",2021-06-14
https://openalex.org/W3159239022,https://doi.org/10.23919/dafx51585.2021.9768246,One Billion Audio Sounds from GPU-Enabled Modular Synthesis,"We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth 1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.","['https://openalex.org/W2949676527', 'https://openalex.org/W2032197604', 'https://openalex.org/W6679739311', 'https://openalex.org/W2939574508', 'https://openalex.org/W6688325169', 'https://openalex.org/W6781351833', 'https://openalex.org/W6769767169', 'https://openalex.org/W2990440871', 'https://openalex.org/W2999531799', 'https://openalex.org/W6771812881', 'https://openalex.org/W6776218486', 'https://openalex.org/W2605102758', 'https://openalex.org/W6779191341', 'https://openalex.org/W3162391496', 'https://openalex.org/W6634561062', 'https://openalex.org/W1998446618', 'https://openalex.org/W6633849734', 'https://openalex.org/W6783462664', 'https://openalex.org/W1986595194', 'https://openalex.org/W2955263139', 'https://openalex.org/W4285367288', 'https://openalex.org/W6787485125', 'https://openalex.org/W6736723571', 'https://openalex.org/W1494198834', 'https://openalex.org/W3030163527', 'https://openalex.org/W2593116425', 'https://openalex.org/W2109664771', 'https://openalex.org/W2791716806', 'https://openalex.org/W6788720092', 'https://openalex.org/W6771763809', 'https://openalex.org/W6783536103', 'https://openalex.org/W6755182157', 'https://openalex.org/W3201143670', 'https://openalex.org/W6774314701', 'https://openalex.org/W4287715222', 'https://openalex.org/W2898148140', 'https://openalex.org/W2131676173', 'https://openalex.org/W3103455452', 'https://openalex.org/W2950547518', 'https://openalex.org/W4287555654', 'https://openalex.org/W4287757056', 'https://openalex.org/W3034978746', 'https://openalex.org/W3021164770', 'https://openalex.org/W3015338123', 'https://openalex.org/W3110955493', 'https://openalex.org/W1581410183', 'https://openalex.org/W3000389243', 'https://openalex.org/W3122300013', 'https://openalex.org/W3005680577', 'https://openalex.org/W2606176153', 'https://openalex.org/W2964300898', 'https://openalex.org/W4292779060', 'https://openalex.org/W2995181338', 'https://openalex.org/W4295910257', 'https://openalex.org/W3035312337', 'https://openalex.org/W3047579126', 'https://openalex.org/W2995233853', 'https://openalex.org/W3097934054', 'https://openalex.org/W4205689591', 'https://openalex.org/W3088942650', 'https://openalex.org/W2250384498', 'https://openalex.org/W3135828102', 'https://openalex.org/W4287802874', 'https://openalex.org/W1556219185', 'https://openalex.org/W1566660863']",2021-09-08
https://openalex.org/W3092135915,https://doi.org/10.5281/zenodo.4245530,AI song contest: Human-AI co-creation in songwriting,"Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.","['https://openalex.org/W2796000004', 'https://openalex.org/W2059216172', 'https://openalex.org/W2753868141', 'https://openalex.org/W2953331651', 'https://openalex.org/W2162514427', 'https://openalex.org/W2115083233', 'https://openalex.org/W2514141612', 'https://openalex.org/W2602896535', 'https://openalex.org/W3005487936', 'https://openalex.org/W2794719876', 'https://openalex.org/W2288593361', 'https://openalex.org/W2957258179', 'https://openalex.org/W3154236293', 'https://openalex.org/W3016014797', 'https://openalex.org/W2905204858', 'https://openalex.org/W3098518646', 'https://openalex.org/W2891794946', 'https://openalex.org/W2949382160', 'https://openalex.org/W3122518304', 'https://openalex.org/W1979290264', 'https://openalex.org/W2805697608', 'https://openalex.org/W2752134738', 'https://openalex.org/W2962212541', 'https://openalex.org/W2142996485', 'https://openalex.org/W2401203361', 'https://openalex.org/W2963403868', 'https://openalex.org/W3031000691', 'https://openalex.org/W3131643527', 'https://openalex.org/W3046715528', 'https://openalex.org/W2795278834', 'https://openalex.org/W2569330962', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963575853', 'https://openalex.org/W2044115095', 'https://openalex.org/W2950429748', 'https://openalex.org/W1983033388', 'https://openalex.org/W3003673875', 'https://openalex.org/W2811079561', 'https://openalex.org/W2950547518', 'https://openalex.org/W147294073', 'https://openalex.org/W1982673570', 'https://openalex.org/W2406069903', 'https://openalex.org/W2963032576', 'https://openalex.org/W3127086053', 'https://openalex.org/W2896197082', 'https://openalex.org/W2153628411', 'https://openalex.org/W1598820975', 'https://openalex.org/W2989822430', 'https://openalex.org/W2790165607', 'https://openalex.org/W161531308', 'https://openalex.org/W2982125965']",2020-10-11
https://openalex.org/W3036033610,https://doi.org/10.5220/0010241801010112,Latent Video Transformer,"The video generation task can be formulated as a prediction of future video frames given some past frames. Recent generative models for videos face the problem of high computational requirements. Some models require up to 512 Tensor Processing Units for parallel training. In this work, we address this problem via modeling the dynamics in a latent space. After the transformation of frames into the latent space, our model predicts latent representation for the next frames in an autoregressive manner. We demonstrate the performance of our approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach tends to reduce requirements to 8 Graphical Processing Units for training the models while maintaining comparable generation quality.","['https://openalex.org/W2894175714', 'https://openalex.org/W2937423351', 'https://openalex.org/W2894946340', 'https://openalex.org/W2963629403', 'https://openalex.org/W2963034893', 'https://openalex.org/W2971074500', 'https://openalex.org/W1568514080', 'https://openalex.org/W2175030374', 'https://openalex.org/W1485009520', 'https://openalex.org/W2948412951', 'https://openalex.org/W2619947201', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963092440', 'https://openalex.org/W2778792233', 'https://openalex.org/W2982041717', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963657860', 'https://openalex.org/W2908080566', 'https://openalex.org/W3005844337', 'https://openalex.org/W2963981733', 'https://openalex.org/W2998108143', 'https://openalex.org/W2800283575', 'https://openalex.org/W2976617189', 'https://openalex.org/W2950946978', 'https://openalex.org/W2963045453', 'https://openalex.org/W2963402657', 'https://openalex.org/W2964122153', 'https://openalex.org/W2796303840', 'https://openalex.org/W2893749619', 'https://openalex.org/W2796873224', 'https://openalex.org/W2918222882', 'https://openalex.org/W2954787828', 'https://openalex.org/W2765349170', 'https://openalex.org/W3010151642', 'https://openalex.org/W2789543585', 'https://openalex.org/W2981593235', 'https://openalex.org/W2887051120', 'https://openalex.org/W2116435618', 'https://openalex.org/W2520707650', 'https://openalex.org/W2987132046', 'https://openalex.org/W3021164770', 'https://openalex.org/W2901599654', 'https://openalex.org/W2886748926', 'https://openalex.org/W2964327849', 'https://openalex.org/W3031246127', 'https://openalex.org/W2964245526', 'https://openalex.org/W2963592434', 'https://openalex.org/W2963048966', 'https://openalex.org/W3015000521', 'https://openalex.org/W2963636093', 'https://openalex.org/W2248556341']",2021-01-01
https://openalex.org/W3081378361,https://doi.org/10.5281/zenodo.4245503,DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks,"Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.","['https://openalex.org/W2963971656', 'https://openalex.org/W2995233853', 'https://openalex.org/W2212660284', 'https://openalex.org/W2970006822', 'https://openalex.org/W3029579848', 'https://openalex.org/W2963857374', 'https://openalex.org/W3021164770', 'https://openalex.org/W3118215210', 'https://openalex.org/W2962760235', 'https://openalex.org/W2963373786', 'https://openalex.org/W2548275288', 'https://openalex.org/W2997367363', 'https://openalex.org/W2606176153', 'https://openalex.org/W2964243274', 'https://openalex.org/W2950299304', 'https://openalex.org/W2108598243', 'https://openalex.org/W2796010067', 'https://openalex.org/W2962919088', 'https://openalex.org/W3015287975', 'https://openalex.org/W1677182931', 'https://openalex.org/W2804722844', 'https://openalex.org/W1959608418', 'https://openalex.org/W2910577860', 'https://openalex.org/W2903005299', 'https://openalex.org/W2519091744', 'https://openalex.org/W2948211236', 'https://openalex.org/W2746457594', 'https://openalex.org/W2962879692', 'https://openalex.org/W2099471712', 'https://openalex.org/W2955263139', 'https://openalex.org/W2909561150', 'https://openalex.org/W2905488776', 'https://openalex.org/W2963889406', 'https://openalex.org/W2267126114', 'https://openalex.org/W2774848319']",2020-10-11
https://openalex.org/W3206790237,https://doi.org/10.1115/1.4053859,Deep Generative Models in Engineering Design: A Review,"Abstract Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance. We present a review and analysis of deep generative machine learning models in engineering design. Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature. In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously. In our discussion, we identify possible solution pathways as key areas on which to target the future work.","['https://openalex.org/W2108924389', 'https://openalex.org/W4205947740', 'https://openalex.org/W6736210646', 'https://openalex.org/W6770593915', 'https://openalex.org/W6771275388', 'https://openalex.org/W6757146666', 'https://openalex.org/W2765811365', 'https://openalex.org/W6718379498', 'https://openalex.org/W6732249622', 'https://openalex.org/W6741832134', 'https://openalex.org/W6735913928', 'https://openalex.org/W6738027021', 'https://openalex.org/W3104016609', 'https://openalex.org/W6678815747', 'https://openalex.org/W6718140377', 'https://openalex.org/W3044485111', 'https://openalex.org/W6785870135', 'https://openalex.org/W3168049041', 'https://openalex.org/W6640963894', 'https://openalex.org/W1965555277', 'https://openalex.org/W2948978827', 'https://openalex.org/W6687045409', 'https://openalex.org/W6676007687', 'https://openalex.org/W2145339207', 'https://openalex.org/W6747950919', 'https://openalex.org/W6753219919', 'https://openalex.org/W6761103827', 'https://openalex.org/W6776142728', 'https://openalex.org/W3083791730', 'https://openalex.org/W4247481232', 'https://openalex.org/W2965302583', 'https://openalex.org/W3096092761', 'https://openalex.org/W2766632954', 'https://openalex.org/W2018743483', 'https://openalex.org/W2060640555', 'https://openalex.org/W2055892996', 'https://openalex.org/W6729632747', 'https://openalex.org/W6620673361', 'https://openalex.org/W6767098714', 'https://openalex.org/W6769589523', 'https://openalex.org/W6690815549', 'https://openalex.org/W6767654570', 'https://openalex.org/W6749578362', 'https://openalex.org/W6752306858', 'https://openalex.org/W6748556633', 'https://openalex.org/W2940616741', 'https://openalex.org/W6751455638', 'https://openalex.org/W2963556194', 'https://openalex.org/W3131305014', 'https://openalex.org/W3090132913', 'https://openalex.org/W2906904540', 'https://openalex.org/W2891797827', 'https://openalex.org/W2963807552', 'https://openalex.org/W3118519758', 'https://openalex.org/W2984973432', 'https://openalex.org/W2930376286', 'https://openalex.org/W2899068551', 'https://openalex.org/W2961923021', 'https://openalex.org/W2991294993', 'https://openalex.org/W2802951432', 'https://openalex.org/W3158358846', 'https://openalex.org/W2606759614', 'https://openalex.org/W4241752698', 'https://openalex.org/W2921887569', 'https://openalex.org/W2988854488', 'https://openalex.org/W2991307204', 'https://openalex.org/W3011193154', 'https://openalex.org/W2803594690', 'https://openalex.org/W3211784702', 'https://openalex.org/W6779550074', 'https://openalex.org/W6754525927', 'https://openalex.org/W6758256398', 'https://openalex.org/W3198916320', 'https://openalex.org/W2965354320', 'https://openalex.org/W6747436763', 'https://openalex.org/W2962940229', 'https://openalex.org/W2991322414', 'https://openalex.org/W3003905418', 'https://openalex.org/W3088265066', 'https://openalex.org/W2559824697', 'https://openalex.org/W4256544956', 'https://openalex.org/W3095356644', 'https://openalex.org/W3204078912', 'https://openalex.org/W6774389765', 'https://openalex.org/W3213803754', 'https://openalex.org/W2560735687', 'https://openalex.org/W2914973752', 'https://openalex.org/W2989794645', 'https://openalex.org/W3084949012', 'https://openalex.org/W3209113848', 'https://openalex.org/W2898986655', 'https://openalex.org/W2966495837', 'https://openalex.org/W3157814662', 'https://openalex.org/W2990191309', 'https://openalex.org/W3097020558', 'https://openalex.org/W3097740117', 'https://openalex.org/W3212563804', 'https://openalex.org/W2765191147', 'https://openalex.org/W2963580633', 'https://openalex.org/W2972352850', 'https://openalex.org/W3214184615', 'https://openalex.org/W2559987817', 'https://openalex.org/W2775103860', 'https://openalex.org/W3104941456', 'https://openalex.org/W6779592747', 'https://openalex.org/W3014309727', 'https://openalex.org/W2023377641', 'https://openalex.org/W2340937978', 'https://openalex.org/W2143163345', 'https://openalex.org/W2014025488', 'https://openalex.org/W1078455779', 'https://openalex.org/W2414629819', 'https://openalex.org/W2536232443', 'https://openalex.org/W1975063788', 'https://openalex.org/W6683590716', 'https://openalex.org/W6720904035', 'https://openalex.org/W6736155344', 'https://openalex.org/W6740364378', 'https://openalex.org/W6727340567', 'https://openalex.org/W6729966448', 'https://openalex.org/W6687483927', 'https://openalex.org/W6743731764', 'https://openalex.org/W6640054144', 'https://openalex.org/W2789526630', 'https://openalex.org/W6679524290', 'https://openalex.org/W4254036672', 'https://openalex.org/W2784648660', 'https://openalex.org/W6730333270', 'https://openalex.org/W6741002519', 'https://openalex.org/W2141270723', 'https://openalex.org/W6736562241', 'https://openalex.org/W2138779671', 'https://openalex.org/W3086429884', 'https://openalex.org/W2612843093', 'https://openalex.org/W6742191325', 'https://openalex.org/W6753739248', 'https://openalex.org/W2985840885', 'https://openalex.org/W2989341556', 'https://openalex.org/W2963627347', 'https://openalex.org/W6746299804', 'https://openalex.org/W6763422710', 'https://openalex.org/W6779618453', 'https://openalex.org/W2746553466', 'https://openalex.org/W6729482032', 'https://openalex.org/W2792690234', 'https://openalex.org/W3164411016', 'https://openalex.org/W3094170182', 'https://openalex.org/W2620045026', 'https://openalex.org/W2903421796', 'https://openalex.org/W6757002775', 'https://openalex.org/W6640300118', 'https://openalex.org/W3106655268', 'https://openalex.org/W3185376398', 'https://openalex.org/W6791113161', 'https://openalex.org/W3033800177', 'https://openalex.org/W3214083666', 'https://openalex.org/W2899396128', 'https://openalex.org/W2143000265', 'https://openalex.org/W2785071288', 'https://openalex.org/W3095077843', 'https://openalex.org/W2947047078', 'https://openalex.org/W6783741718', 'https://openalex.org/W3161200675', 'https://openalex.org/W2795982117', 'https://openalex.org/W2805869290', 'https://openalex.org/W6747695975', 'https://openalex.org/W2980723354', 'https://openalex.org/W6740006508', 'https://openalex.org/W6792704249', 'https://openalex.org/W6779879114', 'https://openalex.org/W6790978476', 'https://openalex.org/W6776218486', 'https://openalex.org/W6791353385', 'https://openalex.org/W2739748921', 'https://openalex.org/W2194775991', 'https://openalex.org/W3034600949', 'https://openalex.org/W3212992673', 'https://openalex.org/W2970066309', 'https://openalex.org/W3181519820', 'https://openalex.org/W2964271403', 'https://openalex.org/W2130325614', 'https://openalex.org/W3120769605', 'https://openalex.org/W3135367836', 'https://openalex.org/W2155968351', 'https://openalex.org/W2951004968', 'https://openalex.org/W2730106296', 'https://openalex.org/W3021164770', 'https://openalex.org/W2962793481', 'https://openalex.org/W2475287302', 'https://openalex.org/W2736601468', 'https://openalex.org/W2924007842', 'https://openalex.org/W3118902573', 'https://openalex.org/W2962879692', 'https://openalex.org/W2883221003', 'https://openalex.org/W3212519934', 'https://openalex.org/W3177141244', 'https://openalex.org/W2995448904', 'https://openalex.org/W2885317998', 'https://openalex.org/W3036167779', 'https://openalex.org/W2963470893', 'https://openalex.org/W3034521433', 'https://openalex.org/W2782313882', 'https://openalex.org/W1520738631', 'https://openalex.org/W2121863487', 'https://openalex.org/W2606712314', 'https://openalex.org/W2188365844', 'https://openalex.org/W2899270511', 'https://openalex.org/W3035231706', 'https://openalex.org/W3037588546', 'https://openalex.org/W3034445277', 'https://openalex.org/W1920022804', 'https://openalex.org/W2964193438', 'https://openalex.org/W2556467266', 'https://openalex.org/W637153065', 'https://openalex.org/W3104397553', 'https://openalex.org/W2786103815', 'https://openalex.org/W3121122991', 'https://openalex.org/W2784451140', 'https://openalex.org/W2963858333', 'https://openalex.org/W3139948516', 'https://openalex.org/W2715714227', 'https://openalex.org/W2966289560', 'https://openalex.org/W2888384911', 'https://openalex.org/W2783876109', 'https://openalex.org/W2909388606', 'https://openalex.org/W2099471712', 'https://openalex.org/W2150341604', 'https://openalex.org/W2560609797', 'https://openalex.org/W2963142510', 'https://openalex.org/W2806115886', 'https://openalex.org/W3162926177', 'https://openalex.org/W2125389028', 'https://openalex.org/W2548275288', 'https://openalex.org/W3169116965', 'https://openalex.org/W2970709315', 'https://openalex.org/W2962876561', 'https://openalex.org/W3035574324', 'https://openalex.org/W2963373786', 'https://openalex.org/W3127451557', 'https://openalex.org/W2107726111', 'https://openalex.org/W3103543904', 'https://openalex.org/W3105195789', 'https://openalex.org/W2806351858', 'https://openalex.org/W2963226019', 'https://openalex.org/W2963369474', 'https://openalex.org/W2581485081', 'https://openalex.org/W1903029394', 'https://openalex.org/W2961368225', 'https://openalex.org/W2963403868', 'https://openalex.org/W2605195953', 'https://openalex.org/W2190691619', 'https://openalex.org/W2468567150', 'https://openalex.org/W3092197865', 'https://openalex.org/W2950898568', 'https://openalex.org/W2703190149', 'https://openalex.org/W2963073614']",2022-02-16
https://openalex.org/W3088329082,https://doi.org/10.3390/app10186627,BassNet: A Variational Gated Autoencoder for Conditional Generation of Bass Guitar Tracks with Learned Interactive Control,"Deep learning has given AI-based methods for music creation a boost by over the past years. An important challenge in this field is to balance user control and autonomy in music generation systems. In this work, we present BassNet, a deep learning model for generating bass guitar tracks based on musical source material. An innovative aspect of our work is that the model is trained to learn a temporally stable two-dimensional latent space variable that offers interactive user control. We empirically show that the model can disentangle bass patterns that require sensitivity to harmony, instrument timbre, and rhythm. An ablation study reveals that this capability is because of the temporal stability constraint on latent space trajectories during training. We also demonstrate that models that are trained on pop/rock music learn a latent space that offers control over the diatonic characteristics of the output, among other things. Lastly, we present and discuss generated bass tracks for three different music fragments. The work that is presented here is a step toward the integration of AI-based technology in the workflow of musical content creators.","['https://openalex.org/W2137884954', 'https://openalex.org/W2020166869', 'https://openalex.org/W1819710477', 'https://openalex.org/W2186572712', 'https://openalex.org/W6929360442', 'https://openalex.org/W2560316200', 'https://openalex.org/W2169264582', 'https://openalex.org/W6681096077', 'https://openalex.org/W2007726935', 'https://openalex.org/W2141998831', 'https://openalex.org/W2120190345', 'https://openalex.org/W2136163184', 'https://openalex.org/W2997367363', 'https://openalex.org/W2809277943', 'https://openalex.org/W2146444479', 'https://openalex.org/W1999192586', 'https://openalex.org/W6647934165', 'https://openalex.org/W6605505536', 'https://openalex.org/W2774077477', 'https://openalex.org/W2963408210', 'https://openalex.org/W2283344589', 'https://openalex.org/W2619409124', 'https://openalex.org/W2772474126', 'https://openalex.org/W2884558435', 'https://openalex.org/W2919624000', 'https://openalex.org/W6758675244', 'https://openalex.org/W6755257315', 'https://openalex.org/W2606176153', 'https://openalex.org/W2792210438', 'https://openalex.org/W2956041762', 'https://openalex.org/W2998588915', 'https://openalex.org/W3081378361', 'https://openalex.org/W3015287975', 'https://openalex.org/W2782255520', 'https://openalex.org/W2901638613', 'https://openalex.org/W6912385387', 'https://openalex.org/W1972014290', 'https://openalex.org/W2741023222', 'https://openalex.org/W2160115902', 'https://openalex.org/W2033546134', 'https://openalex.org/W2009841905', 'https://openalex.org/W2138960858', 'https://openalex.org/W2076608692', 'https://openalex.org/W3081424945', 'https://openalex.org/W2962866891', 'https://openalex.org/W2405656250', 'https://openalex.org/W2962850830', 'https://openalex.org/W6687483927', 'https://openalex.org/W2968826970', 'https://openalex.org/W4285715698', 'https://openalex.org/W2952631504', 'https://openalex.org/W6631190155', 'https://openalex.org/W2619538244', 'https://openalex.org/W2746068898', 'https://openalex.org/W2997574889', 'https://openalex.org/W2951004968', 'https://openalex.org/W3021164770', 'https://openalex.org/W2584032004', 'https://openalex.org/W2579406683', 'https://openalex.org/W2151024047', 'https://openalex.org/W2951535099', 'https://openalex.org/W1989940876', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963575853', 'https://openalex.org/W2805697608', 'https://openalex.org/W3029579848', 'https://openalex.org/W1977444015', 'https://openalex.org/W2788560150', 'https://openalex.org/W2145094598', 'https://openalex.org/W2910577860', 'https://openalex.org/W2958111171', 'https://openalex.org/W2931932963', 'https://openalex.org/W2966220025', 'https://openalex.org/W3104194627', 'https://openalex.org/W2950299304', 'https://openalex.org/W2724233922', 'https://openalex.org/W2963681776', 'https://openalex.org/W134527144']",2020-09-22
https://openalex.org/W3195659928,https://doi.org/10.1109/ghci50508.2021.9514020,Music genre classification using multi-modal deep learning based fusion,"Music genre classification is extensively used in almost all music streaming applications and websites. Most of them use it either to recommend playlists to their customers (such as Spotify, Soundcloud) or simply as a product (e.g. Shazam and MusixMatch). In this paper, we present a novel approach to classify a given song by encoding both textual and music features. The contribution of this work is twofold, i) We propose a multi modal fusion network approach which enables music genre classification utilizing both the textual features (lyrics) and musical features (mel spectrogram) achieving an accuracy of 90.4%. ii) We also propose a multiframe convolutional recurrent neural network (CRNN) based classifier that uses K-nearest neighbor approach over the predictions of every frame to predict the genre of a given song. In multi-modal fusion approach, we utilize co-attention between the textual and musical features for training classification network. The advantage of CRNN based multi frame approach is that it not only enriches the classification process but also enables to generate more training data from a smaller number of music files and thus helps in data augmentation. Our models and code are available on https://github.com/laishawadhwa/Multi-modal-music-genre-classification.","['https://openalex.org/W1979932443', 'https://openalex.org/W2592168896', 'https://openalex.org/W2178339699', 'https://openalex.org/W6690394000', 'https://openalex.org/W2293255527', 'https://openalex.org/W2059652044', 'https://openalex.org/W2133824856', 'https://openalex.org/W6755207826', 'https://openalex.org/W2963176022', 'https://openalex.org/W2360958417', 'https://openalex.org/W2975625678', 'https://openalex.org/W6715395060', 'https://openalex.org/W2978099719', 'https://openalex.org/W6759094740', 'https://openalex.org/W2037487247', 'https://openalex.org/W1513915197', 'https://openalex.org/W6776218486', 'https://openalex.org/W2511041668', 'https://openalex.org/W1985514745', 'https://openalex.org/W2295598076', 'https://openalex.org/W2798418213', 'https://openalex.org/W6741588400', 'https://openalex.org/W2999819273', 'https://openalex.org/W4293878025', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287802874', 'https://openalex.org/W2242773987', 'https://openalex.org/W2962718536', 'https://openalex.org/W3021164770', 'https://openalex.org/W3101520563', 'https://openalex.org/W3105202226', 'https://openalex.org/W2896457183', 'https://openalex.org/W3097525302', 'https://openalex.org/W2414894569', 'https://openalex.org/W3102476541']",2021-02-19
https://openalex.org/W3148695041,https://doi.org/10.5281/zenodo.5624362,Symbolic Music Generation with Diffusion Models,"Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.","['https://openalex.org/W3092879656', 'https://openalex.org/W3098518646', 'https://openalex.org/W2910577860', 'https://openalex.org/W2970607325', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971074500', 'https://openalex.org/W3129651364', 'https://openalex.org/W2951535099', 'https://openalex.org/W3035384201', 'https://openalex.org/W2626778328', 'https://openalex.org/W2557449848', 'https://openalex.org/W2402144811', 'https://openalex.org/W2753868141', 'https://openalex.org/W2963981733', 'https://openalex.org/W2922772346', 'https://openalex.org/W2963223306', 'https://openalex.org/W2898148140', 'https://openalex.org/W1579853615', 'https://openalex.org/W2013035813', 'https://openalex.org/W2995233853', 'https://openalex.org/W2982125965', 'https://openalex.org/W2963145887', 'https://openalex.org/W3100572490', 'https://openalex.org/W2212660284', 'https://openalex.org/W2787069069', 'https://openalex.org/W3123097577', 'https://openalex.org/W1959608418', 'https://openalex.org/W3049174246', 'https://openalex.org/W2919624000', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963801305', 'https://openalex.org/W2064675550', 'https://openalex.org/W2995801453', 'https://openalex.org/W3121345697', 'https://openalex.org/W2167433878', 'https://openalex.org/W2805697608', 'https://openalex.org/W1710476689', 'https://openalex.org/W2962736171', 'https://openalex.org/W3034758887', 'https://openalex.org/W2579406683', 'https://openalex.org/W2971034910', 'https://openalex.org/W2997195635', 'https://openalex.org/W3092033429', 'https://openalex.org/W2963032576', 'https://openalex.org/W2963921132', 'https://openalex.org/W2567627528', 'https://openalex.org/W3021164770', 'https://openalex.org/W2475687244']",2021-11-07
https://openalex.org/W3185297410,https://doi.org/10.21428/92fbeb44.fe9a0d82,Transformer Neural Networks for Automated Rhythm Generation,"Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit.We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation.Hundreds of generations are evaluated using blindlistening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced.Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.","['https://openalex.org/W6778883912', 'https://openalex.org/W2981852735', 'https://openalex.org/W6776218486', 'https://openalex.org/W2891815651', 'https://openalex.org/W2959020461', 'https://openalex.org/W2946521317', 'https://openalex.org/W3099378280', 'https://openalex.org/W2752134738', 'https://openalex.org/W6667662476', 'https://openalex.org/W1819710477', 'https://openalex.org/W1924770834', 'https://openalex.org/W2137619888', 'https://openalex.org/W2338312508', 'https://openalex.org/W2601110281', 'https://openalex.org/W6786165050', 'https://openalex.org/W2739969482', 'https://openalex.org/W2732964176', 'https://openalex.org/W2201092681', 'https://openalex.org/W3046715528', 'https://openalex.org/W3047325651', 'https://openalex.org/W3043786446', 'https://openalex.org/W2911109671', 'https://openalex.org/W2963096510', 'https://openalex.org/W1979720880', 'https://openalex.org/W2898827701', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964110616', 'https://openalex.org/W2626778328', 'https://openalex.org/W3021164770', 'https://openalex.org/W3104194627', 'https://openalex.org/W4287802874', 'https://openalex.org/W2109886035', 'https://openalex.org/W2067621398', 'https://openalex.org/W3010506491', 'https://openalex.org/W4252982435', 'https://openalex.org/W2133564696', 'https://openalex.org/W2991108091', 'https://openalex.org/W3030163527', 'https://openalex.org/W2962826786', 'https://openalex.org/W4288089799', 'https://openalex.org/W2962968839', 'https://openalex.org/W4292779060']",2021-07-19
https://openalex.org/W3034581551,https://doi.org/10.1007/978-3-030-72116-9_22,Deep Generative Models for Musical Audio Synthesis,,"['https://openalex.org/W2126398289', 'https://openalex.org/W2434741482', 'https://openalex.org/W2061171222', 'https://openalex.org/W2949723897', 'https://openalex.org/W2890043615', 'https://openalex.org/W2811079561', 'https://openalex.org/W2408023085', 'https://openalex.org/W2152988638', 'https://openalex.org/W2026259539', 'https://openalex.org/W2546879872', 'https://openalex.org/W2248907644', 'https://openalex.org/W6712232353', 'https://openalex.org/W7038675119', 'https://openalex.org/W6637618735', 'https://openalex.org/W2964135678', 'https://openalex.org/W2963920537', 'https://openalex.org/W1986595194', 'https://openalex.org/W2120847449', 'https://openalex.org/W2194775991', 'https://openalex.org/W6601202783', 'https://openalex.org/W6600599538', 'https://openalex.org/W1978122678', 'https://openalex.org/W2914485273', 'https://openalex.org/W2890983311', 'https://openalex.org/W2962770929', 'https://openalex.org/W2962721334', 'https://openalex.org/W2587284713', 'https://openalex.org/W2067329295', 'https://openalex.org/W2043003570', 'https://openalex.org/W2963408210', 'https://openalex.org/W2760103357', 'https://openalex.org/W2963300588', 'https://openalex.org/W2610073446', 'https://openalex.org/W2931364255', 'https://openalex.org/W6718379498', 'https://openalex.org/W2964243274', 'https://openalex.org/W2150307531', 'https://openalex.org/W2074310426', 'https://openalex.org/W7000111596', 'https://openalex.org/W6756197946', 'https://openalex.org/W2423557781', 'https://openalex.org/W2752796333', 'https://openalex.org/W2963609956', 'https://openalex.org/W2079080230', 'https://openalex.org/W2725868244', 'https://openalex.org/W2804967784', 'https://openalex.org/W2924273262', 'https://openalex.org/W2962793481', 'https://openalex.org/W2159300614', 'https://openalex.org/W2950547518', 'https://openalex.org/W2989955315', 'https://openalex.org/W2910577860', 'https://openalex.org/W4205263358', 'https://openalex.org/W2963636093', 'https://openalex.org/W2884607399', 'https://openalex.org/W2963226019', 'https://openalex.org/W2777302760', 'https://openalex.org/W3021164770', 'https://openalex.org/W2125389028', 'https://openalex.org/W2803963372', 'https://openalex.org/W95184617', 'https://openalex.org/W2792210438', 'https://openalex.org/W4285719527', 'https://openalex.org/W2953246223', 'https://openalex.org/W2953331651', 'https://openalex.org/W2950299304', 'https://openalex.org/W2970006822', 'https://openalex.org/W3099425575', 'https://openalex.org/W2769810959', 'https://openalex.org/W2901997113', 'https://openalex.org/W2804722844', 'https://openalex.org/W2467604901', 'https://openalex.org/W2747023714', 'https://openalex.org/W1810943226', 'https://openalex.org/W2953030256', 'https://openalex.org/W161531308', 'https://openalex.org/W2963889406', 'https://openalex.org/W2559246505', 'https://openalex.org/W2591927543', 'https://openalex.org/W2794490148', 'https://openalex.org/W2786254735', 'https://openalex.org/W2795109282', 'https://openalex.org/W2581485081', 'https://openalex.org/W2788830260', 'https://openalex.org/W2752134738', 'https://openalex.org/W2173520492', 'https://openalex.org/W1924619199', 'https://openalex.org/W2951004968', 'https://openalex.org/W2788851830', 'https://openalex.org/W2951986497', 'https://openalex.org/W2891815651', 'https://openalex.org/W2785967511', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962942158', 'https://openalex.org/W2608207374', 'https://openalex.org/W2756184659', 'https://openalex.org/W2953318193', 'https://openalex.org/W2899775901', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963799213', 'https://openalex.org/W2949382160', 'https://openalex.org/W2883853252', 'https://openalex.org/W2766406951', 'https://openalex.org/W2577946330', 'https://openalex.org/W2951535099', 'https://openalex.org/W3101563155']",2021-01-01
https://openalex.org/W3112789166,https://doi.org/10.18653/v1/2020.inlg-1.42,Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders,"The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25% of the time.","['https://openalex.org/W2964008635', 'https://openalex.org/W2964202925', 'https://openalex.org/W2785567863', 'https://openalex.org/W2081580037', 'https://openalex.org/W2949615363', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963403868', 'https://openalex.org/W2799022285', 'https://openalex.org/W2798662960', 'https://openalex.org/W2785543907', 'https://openalex.org/W2994817889', 'https://openalex.org/W2998659916', 'https://openalex.org/W3104814493', 'https://openalex.org/W2949555952', 'https://openalex.org/W2946393904', 'https://openalex.org/W4386506836', 'https://openalex.org/W2250842199', 'https://openalex.org/W2963341956', 'https://openalex.org/W4288089799', 'https://openalex.org/W3102934792', 'https://openalex.org/W2530647954', 'https://openalex.org/W2997195635', 'https://openalex.org/W2798782765', 'https://openalex.org/W1510519407', 'https://openalex.org/W2605035112', 'https://openalex.org/W2965033324', 'https://openalex.org/W3021164770', 'https://openalex.org/W2101105183', 'https://openalex.org/W2160467647', 'https://openalex.org/W2741104967', 'https://openalex.org/W2970744242', 'https://openalex.org/W2914442349', 'https://openalex.org/W2981852735', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963631950', 'https://openalex.org/W2617566453', 'https://openalex.org/W2990427715', 'https://openalex.org/W2982399380', 'https://openalex.org/W2996264288', 'https://openalex.org/W3034999214', 'https://openalex.org/W2949806279', 'https://openalex.org/W2768440179', 'https://openalex.org/W2053154970', 'https://openalex.org/W2573032371', 'https://openalex.org/W131896369', 'https://openalex.org/W4385245566', 'https://openalex.org/W2962768052', 'https://openalex.org/W3013167650', 'https://openalex.org/W622113263']",2020-01-01
https://openalex.org/W3095617022,,Self-supervised Pitch Detection by Inverse Audio Synthesis,,"['https://openalex.org/W2963706553', 'https://openalex.org/W2963609956', 'https://openalex.org/W2006681603', 'https://openalex.org/W2194775991', 'https://openalex.org/W1946152311', 'https://openalex.org/W3021164770', 'https://openalex.org/W3168220982', 'https://openalex.org/W2902812770', 'https://openalex.org/W2399706197', 'https://openalex.org/W2964121744', 'https://openalex.org/W3101943858', 'https://openalex.org/W2962942158', 'https://openalex.org/W2995233853', 'https://openalex.org/W1594957066', 'https://openalex.org/W1975079546', 'https://openalex.org/W645380233', 'https://openalex.org/W2088432713', 'https://openalex.org/W2004126765', 'https://openalex.org/W2951535099', 'https://openalex.org/W4161657', 'https://openalex.org/W2910577860', 'https://openalex.org/W2120263555', 'https://openalex.org/W2955263139', 'https://openalex.org/W2962866891', 'https://openalex.org/W1583791393', 'https://openalex.org/W2124539664', 'https://openalex.org/W2155978642', 'https://openalex.org/W2963551352', 'https://openalex.org/W2168103573', 'https://openalex.org/W2088992427', 'https://openalex.org/W3146431201', 'https://openalex.org/W2067265757', 'https://openalex.org/W2140020944', 'https://openalex.org/W2898148140', 'https://openalex.org/W2774581901', 'https://openalex.org/W2044222806']",2020-06-10
https://openalex.org/W3211354215,https://doi.org/10.26615/978-954-452-072-4_153,Watching a Language Model Learning Chess,"We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics.With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time.However, model size does not show such a clear influence.It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here.Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.","['https://openalex.org/W3118210634', 'https://openalex.org/W2902907165', 'https://openalex.org/W2963587345', 'https://openalex.org/W3156643189', 'https://openalex.org/W3134307371', 'https://openalex.org/W2966280323', 'https://openalex.org/W3095645723', 'https://openalex.org/W3127394918', 'https://openalex.org/W2091785129', 'https://openalex.org/W2981731882', 'https://openalex.org/W2963096510', 'https://openalex.org/W3100715140', 'https://openalex.org/W3174394143', 'https://openalex.org/W2795247881', 'https://openalex.org/W4287670290', 'https://openalex.org/W4295312788', 'https://openalex.org/W2896457183', 'https://openalex.org/W3097132740', 'https://openalex.org/W2063244929', 'https://openalex.org/W3021164770', 'https://openalex.org/W3133029875', 'https://openalex.org/W3006436762', 'https://openalex.org/W2136939460', 'https://openalex.org/W3001279689', 'https://openalex.org/W2798273548', 'https://openalex.org/W2766447205', 'https://openalex.org/W2964159778', 'https://openalex.org/W4385245566', 'https://openalex.org/W4287214056', 'https://openalex.org/W4287802874', 'https://openalex.org/W2963341956', 'https://openalex.org/W4300936792', 'https://openalex.org/W4292779060', 'https://openalex.org/W3047988254', 'https://openalex.org/W3129831491', 'https://openalex.org/W2892156241', 'https://openalex.org/W2938704169', 'https://openalex.org/W1951216520']",2021-01-01
https://openalex.org/W3097306158,https://doi.org/10.1101/2020.10.28.359885,Crispr2vec: Machine Learning Model Predicts Off-Target Cuts of CRISPR systems,"1 Abstract Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects — unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.","['https://openalex.org/W2765610460', 'https://openalex.org/W2899948177', 'https://openalex.org/W1019830208', 'https://openalex.org/W2115603949', 'https://openalex.org/W3017334101', 'https://openalex.org/W2726053863', 'https://openalex.org/W2127749855', 'https://openalex.org/W2606377603', 'https://openalex.org/W2467139031', 'https://openalex.org/W2105464583', 'https://openalex.org/W2064815984', 'https://openalex.org/W2896457183', 'https://openalex.org/W2561754210', 'https://openalex.org/W2252568502', 'https://openalex.org/W2050480410', 'https://openalex.org/W1997281938', 'https://openalex.org/W2810756255', 'https://openalex.org/W2791311992', 'https://openalex.org/W2461013690', 'https://openalex.org/W2096261947', 'https://openalex.org/W2036176224', 'https://openalex.org/W2045435533', 'https://openalex.org/W3021896513', 'https://openalex.org/W2345512687', 'https://openalex.org/W2273244674', 'https://openalex.org/W2984059576', 'https://openalex.org/W2224398381', 'https://openalex.org/W2888908334', 'https://openalex.org/W2163605009', 'https://openalex.org/W2782551527', 'https://openalex.org/W2003171404', 'https://openalex.org/W2157205823', 'https://openalex.org/W1915704240', 'https://openalex.org/W2889664156', 'https://openalex.org/W2070614858', 'https://openalex.org/W1977709885', 'https://openalex.org/W2096733369', 'https://openalex.org/W1986316308', 'https://openalex.org/W2479945688', 'https://openalex.org/W2188223075', 'https://openalex.org/W1990149668', 'https://openalex.org/W2611661371', 'https://openalex.org/W2003797386', 'https://openalex.org/W2051255304', 'https://openalex.org/W2260024370', 'https://openalex.org/W2433743436', 'https://openalex.org/W2951760474', 'https://openalex.org/W2198606573', 'https://openalex.org/W2901218091', 'https://openalex.org/W2963341956', 'https://openalex.org/W2949382160', 'https://openalex.org/W3021164770', 'https://openalex.org/W3099206234', 'https://openalex.org/W2935703330', 'https://openalex.org/W2902781388', 'https://openalex.org/W2187089797', 'https://openalex.org/W2951054376']",2020-10-29
https://openalex.org/W3036165773,https://doi.org/10.48550/arxiv.2006.12878,Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures,"Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.","['https://openalex.org/W2964115671', 'https://openalex.org/W2768308213', 'https://openalex.org/W2994689640', 'https://openalex.org/W2964182926', 'https://openalex.org/W2962998014', 'https://openalex.org/W2950739196', 'https://openalex.org/W3021164770', 'https://openalex.org/W2973727699', 'https://openalex.org/W3093873179', 'https://openalex.org/W2187089797', 'https://openalex.org/W2964288609', 'https://openalex.org/W3016489761', 'https://openalex.org/W2558748708', 'https://openalex.org/W2003357516', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963323070', 'https://openalex.org/W2525332836', 'https://openalex.org/W2483215953', 'https://openalex.org/W2074694452', 'https://openalex.org/W3011537551', 'https://openalex.org/W2116341502', 'https://openalex.org/W2963348321', 'https://openalex.org/W2997130580', 'https://openalex.org/W189596042', 'https://openalex.org/W2990583358', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963026768', 'https://openalex.org/W413857758', 'https://openalex.org/W2964015378', 'https://openalex.org/W1606458877', 'https://openalex.org/W2994802534', 'https://openalex.org/W3100398946', 'https://openalex.org/W2962734330', 'https://openalex.org/W2962784628', 'https://openalex.org/W1777649940', 'https://openalex.org/W2962711740', 'https://openalex.org/W3103313582', 'https://openalex.org/W2964311892', 'https://openalex.org/W1492899839', 'https://openalex.org/W2968257580', 'https://openalex.org/W2552737632', 'https://openalex.org/W2951065015', 'https://openalex.org/W2107433900', 'https://openalex.org/W2159110831', 'https://openalex.org/W2963341956', 'https://openalex.org/W3021293129', 'https://openalex.org/W2786738752', 'https://openalex.org/W2949657144', 'https://openalex.org/W3105114834', 'https://openalex.org/W2153959628', 'https://openalex.org/W3167195305', 'https://openalex.org/W2295739661', 'https://openalex.org/W3016391357', 'https://openalex.org/W2963140169', 'https://openalex.org/W637153065', 'https://openalex.org/W2963403868', 'https://openalex.org/W2935850529', 'https://openalex.org/W2990704537', 'https://openalex.org/W2766736793', 'https://openalex.org/W2953042818', 'https://openalex.org/W3030163527', 'https://openalex.org/W3033381843', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963858333', 'https://openalex.org/W1501856433', 'https://openalex.org/W3121926921', 'https://openalex.org/W2950898568', 'https://openalex.org/W2475334473', 'https://openalex.org/W2160821628', 'https://openalex.org/W2887997457', 'https://openalex.org/W2758175304', 'https://openalex.org/W2997927168', 'https://openalex.org/W2964308564', 'https://openalex.org/W2110278466', 'https://openalex.org/W2964321699', 'https://openalex.org/W2937662123', 'https://openalex.org/W2918342466', 'https://openalex.org/W2911586496', 'https://openalex.org/W2963047948', 'https://openalex.org/W2936709240', 'https://openalex.org/W2554952599', 'https://openalex.org/W2971278627', 'https://openalex.org/W2951001079', 'https://openalex.org/W3006801027', 'https://openalex.org/W2948223045', 'https://openalex.org/W2903723132', 'https://openalex.org/W2935958002']",2020-06-23
https://openalex.org/W3215960001,https://doi.org/10.1007/978-3-030-98358-1_44,A-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody,,"['https://openalex.org/W2162911105', 'https://openalex.org/W6602657927', 'https://openalex.org/W6602203279', 'https://openalex.org/W2772474126', 'https://openalex.org/W2601110281', 'https://openalex.org/W6638598322', 'https://openalex.org/W2782255520', 'https://openalex.org/W2514141612', 'https://openalex.org/W6739901393', 'https://openalex.org/W2992790584', 'https://openalex.org/W6601215536', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963681776', 'https://openalex.org/W3119914886', 'https://openalex.org/W2788612614', 'https://openalex.org/W3104194627', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963403868', 'https://openalex.org/W2798861951', 'https://openalex.org/W2746068898', 'https://openalex.org/W2902184207', 'https://openalex.org/W2963575853', 'https://openalex.org/W1819710477', 'https://openalex.org/W2134195709', 'https://openalex.org/W3022035305', 'https://openalex.org/W2949382160']",2022-01-01
https://openalex.org/W3216722162,https://doi.org/10.23919/eusipco58844.2023.10289768,Upsampling Layers for Music Source Separation,"Upsampling artifacts are caused by problematic upsampling layers, and due to spectral replicas that emerge while upsampling. Also, depending on the used upsampling layer, such artifacts can either be tonal (additive high-frequency noise) or filtering artifacts (substractive, attenuating some bands). We investigate the practical implications of having upsampling artifacts in the resulting audio, by studying how different artifacts interact and assessing their impact on the models' performance. To that end, we benchmark a large set of upsampling layers for music source separation: different transposed and sub-pixel convolution setups, different interpolation upsamplers (including two novel layers based on stretch and sinc interpolation), and different wavelet-based upsamplers (including a novel learnable wavelet layer). Our results show that filtering artifacts, associated with interpolation upsamplers, are perceptually preferrable, even if they tend to achieve worse objective scores.","['https://openalex.org/W6740873641', 'https://openalex.org/W2476548250', 'https://openalex.org/W2020919250', 'https://openalex.org/W6732646663', 'https://openalex.org/W2972443522', 'https://openalex.org/W6751512325', 'https://openalex.org/W6755257315', 'https://openalex.org/W6731370813', 'https://openalex.org/W2127851351', 'https://openalex.org/W6631190155', 'https://openalex.org/W6724804524', 'https://openalex.org/W6776218486', 'https://openalex.org/W3015724290', 'https://openalex.org/W2535388113', 'https://openalex.org/W3015197852', 'https://openalex.org/W6785363610', 'https://openalex.org/W6770315077', 'https://openalex.org/W6767111847', 'https://openalex.org/W2894295011', 'https://openalex.org/W3162366763', 'https://openalex.org/W2964121744', 'https://openalex.org/W584173323', 'https://openalex.org/W2502312327', 'https://openalex.org/W2970006822', 'https://openalex.org/W2567070169', 'https://openalex.org/W2990594533', 'https://openalex.org/W2580221632', 'https://openalex.org/W2963341071', 'https://openalex.org/W3021164770', 'https://openalex.org/W3046970875', 'https://openalex.org/W2963970792', 'https://openalex.org/W4287802874', 'https://openalex.org/W2734957715', 'https://openalex.org/W2669032454', 'https://openalex.org/W2963452667', 'https://openalex.org/W4297817572', 'https://openalex.org/W1522301498', 'https://openalex.org/W4298310324']",2023-09-04
https://openalex.org/W3207925746,https://doi.org/10.1007/978-3-030-74478-6_10,Deep Learning for Audio and Music,,"['https://openalex.org/W3012504141', 'https://openalex.org/W2894946340', 'https://openalex.org/W2153275212', 'https://openalex.org/W2422305492', 'https://openalex.org/W2619697695', 'https://openalex.org/W2963115079', 'https://openalex.org/W2044222806', 'https://openalex.org/W2920728711', 'https://openalex.org/W2127589108', 'https://openalex.org/W2892104732', 'https://openalex.org/W4296458960', 'https://openalex.org/W811578723', 'https://openalex.org/W2773294482', 'https://openalex.org/W2076608692', 'https://openalex.org/W2350911425', 'https://openalex.org/W2619623002', 'https://openalex.org/W2954914497', 'https://openalex.org/W3138621071', 'https://openalex.org/W2414894569', 'https://openalex.org/W2740560081', 'https://openalex.org/W2906042495', 'https://openalex.org/W3080857286', 'https://openalex.org/W3100270690', 'https://openalex.org/W2404176928', 'https://openalex.org/W2906289885', 'https://openalex.org/W2954312627', 'https://openalex.org/W3015962533', 'https://openalex.org/W2166209411', 'https://openalex.org/W2059652044', 'https://openalex.org/W3137420527', 'https://openalex.org/W2910577860', 'https://openalex.org/W2903005299', 'https://openalex.org/W2995233853', 'https://openalex.org/W1482149378', 'https://openalex.org/W288512320', 'https://openalex.org/W2606176153', 'https://openalex.org/W197865394', 'https://openalex.org/W2990946792', 'https://openalex.org/W2766465839', 'https://openalex.org/W3011176162', 'https://openalex.org/W2120847449', 'https://openalex.org/W2012897754', 'https://openalex.org/W2143235280', 'https://openalex.org/W2296610835', 'https://openalex.org/W2964706117', 'https://openalex.org/W2138621090', 'https://openalex.org/W2221409856', 'https://openalex.org/W2160815625', 'https://openalex.org/W2165712214', 'https://openalex.org/W2901739194', 'https://openalex.org/W2136922672', 'https://openalex.org/W2064675550', 'https://openalex.org/W2523097914', 'https://openalex.org/W2194775991', 'https://openalex.org/W4249840851', 'https://openalex.org/W2132037657', 'https://openalex.org/W2774707525', 'https://openalex.org/W2766527293', 'https://openalex.org/W2962770929', 'https://openalex.org/W2964218314', 'https://openalex.org/W2953343412', 'https://openalex.org/W2163605009', 'https://openalex.org/W2897802972', 'https://openalex.org/W2112796928', 'https://openalex.org/W1974186229', 'https://openalex.org/W2962935966', 'https://openalex.org/W2952218014', 'https://openalex.org/W2003020982', 'https://openalex.org/W2107789863', 'https://openalex.org/W2972653970', 'https://openalex.org/W2127870748', 'https://openalex.org/W2132984323', 'https://openalex.org/W2771265026', 'https://openalex.org/W3047125220', 'https://openalex.org/W3015530480', 'https://openalex.org/W2775794021', 'https://openalex.org/W2584032004', 'https://openalex.org/W2290318471', 'https://openalex.org/W2803963372', 'https://openalex.org/W3034739183', 'https://openalex.org/W3081378361', 'https://openalex.org/W3005914890', 'https://openalex.org/W2937400686', 'https://openalex.org/W1978298999', 'https://openalex.org/W2963341071', 'https://openalex.org/W2996889020', 'https://openalex.org/W2886396981', 'https://openalex.org/W2437181147', 'https://openalex.org/W3015470225', 'https://openalex.org/W2962845248', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964052309', 'https://openalex.org/W1901129140', 'https://openalex.org/W1498436455', 'https://openalex.org/W2173520492', 'https://openalex.org/W2015433306', 'https://openalex.org/W2758739367', 'https://openalex.org/W2997987128', 'https://openalex.org/W1503560067', 'https://openalex.org/W2805288670', 'https://openalex.org/W2096733369', 'https://openalex.org/W2097117768', 'https://openalex.org/W2149368536', 'https://openalex.org/W2964243274', 'https://openalex.org/W2088432713', 'https://openalex.org/W2601450892', 'https://openalex.org/W2963971656', 'https://openalex.org/W2519994964', 'https://openalex.org/W2130942839', 'https://openalex.org/W1600744878', 'https://openalex.org/W2398826216', 'https://openalex.org/W1686810756', 'https://openalex.org/W2432717477', 'https://openalex.org/W2752796333', 'https://openalex.org/W3016156006', 'https://openalex.org/W2759976799', 'https://openalex.org/W1895577753', 'https://openalex.org/W2541674938', 'https://openalex.org/W2037874181', 'https://openalex.org/W3015289235', 'https://openalex.org/W2173629880', 'https://openalex.org/W2046869671', 'https://openalex.org/W2775621926', 'https://openalex.org/W2990440871', 'https://openalex.org/W2962865004', 'https://openalex.org/W2962793481', 'https://openalex.org/W2951535099', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962760235', 'https://openalex.org/W2902922910', 'https://openalex.org/W2950299304', 'https://openalex.org/W2432004435', 'https://openalex.org/W2902437809', 'https://openalex.org/W2952405182', 'https://openalex.org/W3124061379', 'https://openalex.org/W2903006902', 'https://openalex.org/W2792764867', 'https://openalex.org/W2504108613', 'https://openalex.org/W2963889406', 'https://openalex.org/W2962835968', 'https://openalex.org/W3099330747', 'https://openalex.org/W2963684088', 'https://openalex.org/W3007948068', 'https://openalex.org/W3021164770', 'https://openalex.org/W2544224704', 'https://openalex.org/W1553004968', 'https://openalex.org/W3121952123', 'https://openalex.org/W2963452667', 'https://openalex.org/W3000389243', 'https://openalex.org/W2117671523', 'https://openalex.org/W4285719527', 'https://openalex.org/W2593179621', 'https://openalex.org/W3022805095', 'https://openalex.org/W3125709657', 'https://openalex.org/W2557283755', 'https://openalex.org/W3099206234', 'https://openalex.org/W1959608418', 'https://openalex.org/W2133564696', 'https://openalex.org/W2105622288', 'https://openalex.org/W2331927446', 'https://openalex.org/W2949756029']",2012-02-24
https://openalex.org/W3178321840,https://doi.org/10.1109/taslp.2021.3129994,SoundStream: An End-to-End Neural Audio Codec,"We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.","['https://openalex.org/W2148371116', 'https://openalex.org/W3163243746', 'https://openalex.org/W3015780049', 'https://openalex.org/W3169418678', 'https://openalex.org/W4205169642', 'https://openalex.org/W2775336875', 'https://openalex.org/W2935711438', 'https://openalex.org/W2963091184', 'https://openalex.org/W3163662330', 'https://openalex.org/W6639363673', 'https://openalex.org/W1481955708', 'https://openalex.org/W6635645706', 'https://openalex.org/W6732429163', 'https://openalex.org/W6843673214', 'https://openalex.org/W6748409065', 'https://openalex.org/W2890983311', 'https://openalex.org/W6767111847', 'https://openalex.org/W6783867762', 'https://openalex.org/W1973681148', 'https://openalex.org/W2963341071', 'https://openalex.org/W2972443522', 'https://openalex.org/W2963103134', 'https://openalex.org/W2963321191', 'https://openalex.org/W2296581541', 'https://openalex.org/W2609317876', 'https://openalex.org/W2802034954', 'https://openalex.org/W2150593711', 'https://openalex.org/W2134383396', 'https://openalex.org/W6678914141', 'https://openalex.org/W4244017338', 'https://openalex.org/W2002182716', 'https://openalex.org/W2151626637', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W6753018729', 'https://openalex.org/W6776218486', 'https://openalex.org/W1700083040', 'https://openalex.org/W2131738223', 'https://openalex.org/W2129913307', 'https://openalex.org/W2963182577', 'https://openalex.org/W3140429000', 'https://openalex.org/W2972354707', 'https://openalex.org/W3186609711', 'https://openalex.org/W3160576174', 'https://openalex.org/W6685562342', 'https://openalex.org/W6769196770', 'https://openalex.org/W6674330103', 'https://openalex.org/W6780218876', 'https://openalex.org/W3095095816', 'https://openalex.org/W2768814045', 'https://openalex.org/W6771763809', 'https://openalex.org/W6781251213', 'https://openalex.org/W2760103357', 'https://openalex.org/W2972359262', 'https://openalex.org/W6746278845', 'https://openalex.org/W6678969435', 'https://openalex.org/W6631190155', 'https://openalex.org/W1552314771', 'https://openalex.org/W6636170946', 'https://openalex.org/W3037038648', 'https://openalex.org/W2286601668', 'https://openalex.org/W2165291881', 'https://openalex.org/W6780365925', 'https://openalex.org/W6772349387', 'https://openalex.org/W2176412452', 'https://openalex.org/W2963799213', 'https://openalex.org/W1607435270', 'https://openalex.org/W2953331651', 'https://openalex.org/W3036682213', 'https://openalex.org/W2095705004', 'https://openalex.org/W2947590261', 'https://openalex.org/W1885680957', 'https://openalex.org/W2950237361', 'https://openalex.org/W2811079561', 'https://openalex.org/W3021164770', 'https://openalex.org/W3000389243', 'https://openalex.org/W2963782041', 'https://openalex.org/W2949382160', 'https://openalex.org/W2127870748', 'https://openalex.org/W2998572311', 'https://openalex.org/W2788851830', 'https://openalex.org/W2770119437', 'https://openalex.org/W3046970875', 'https://openalex.org/W2970006822', 'https://openalex.org/W2127218421', 'https://openalex.org/W2913399920', 'https://openalex.org/W3099782249', 'https://openalex.org/W3130248090', 'https://openalex.org/W3036601975', 'https://openalex.org/W2810843531', 'https://openalex.org/W2963921132', 'https://openalex.org/W3092028330']",2021-11-23
https://openalex.org/W3216137029,https://doi.org/10.1007/978-3-031-08337-2_35,Transfer Learning with Jukebox for Music Source Separation,,"['https://openalex.org/W6702248584', 'https://openalex.org/W2801492038', 'https://openalex.org/W2770456481', 'https://openalex.org/W3037149862', 'https://openalex.org/W2405569690', 'https://openalex.org/W2658431580', 'https://openalex.org/W3204633953', 'https://openalex.org/W2602768412', 'https://openalex.org/W6910746531', 'https://openalex.org/W3001377302', 'https://openalex.org/W3160050861', 'https://openalex.org/W3006926732', 'https://openalex.org/W2972411915', 'https://openalex.org/W3021164770', 'https://openalex.org/W2971458685', 'https://openalex.org/W3082274269', 'https://openalex.org/W3197781391', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963452667', 'https://openalex.org/W2896457183', 'https://openalex.org/W3092111032', 'https://openalex.org/W4287802874', 'https://openalex.org/W4288089799']",2022-01-01
https://openalex.org/W3081769760,https://doi.org/10.1109/cog47356.2020.9231927,Towards Game Design via Creative Machine Learning (GDCML),"In recent years, machine learning (ML) systems have been increasingly applied for performing creative tasks. Such creative ML approaches have seen wide use in the domains of visual art and music for applications such as image and music generation and style transfer. However, similar creative ML techniques have not been as widely adopted in the domain of game design despite the emergence of ML-based methods for generating game content. In this paper, we argue for leveraging and repurposing such creative techniques for designing content for games, referring to these as approaches for Game Design via Creative ML (GDCML). We highlight existing systems that enable GDCML and illustrate how creative ML can inform new systems via example applications and a proposed system.","['https://openalex.org/W2964201809', 'https://openalex.org/W6732156205', 'https://openalex.org/W2152660354', 'https://openalex.org/W2108806781', 'https://openalex.org/W2168115594', 'https://openalex.org/W2534314849', 'https://openalex.org/W6740220813', 'https://openalex.org/W6696872630', 'https://openalex.org/W2963690854', 'https://openalex.org/W1976575031', 'https://openalex.org/W2953360045', 'https://openalex.org/W6765622595', 'https://openalex.org/W6758675244', 'https://openalex.org/W6698256120', 'https://openalex.org/W6756006421', 'https://openalex.org/W2742165006', 'https://openalex.org/W2990300161', 'https://openalex.org/W3038300925', 'https://openalex.org/W2949869682', 'https://openalex.org/W6754645375', 'https://openalex.org/W6755312952', 'https://openalex.org/W6752378368', 'https://openalex.org/W6755681954', 'https://openalex.org/W2962750014', 'https://openalex.org/W2194775991', 'https://openalex.org/W6637373629', 'https://openalex.org/W6640963894', 'https://openalex.org/W6774569665', 'https://openalex.org/W6639732818', 'https://openalex.org/W2603777577', 'https://openalex.org/W3088030554', 'https://openalex.org/W6740248136', 'https://openalex.org/W6758068272', 'https://openalex.org/W6696997655', 'https://openalex.org/W6754219667', 'https://openalex.org/W6689467388', 'https://openalex.org/W6732528621', 'https://openalex.org/W2896966958', 'https://openalex.org/W6774082622', 'https://openalex.org/W6729938639', 'https://openalex.org/W2097517378', 'https://openalex.org/W2962770929', 'https://openalex.org/W6760664951', 'https://openalex.org/W6743002019', 'https://openalex.org/W2772474126', 'https://openalex.org/W6749351710', 'https://openalex.org/W6746569791', 'https://openalex.org/W6750274022', 'https://openalex.org/W6769544987', 'https://openalex.org/W2108598243', 'https://openalex.org/W6776218486', 'https://openalex.org/W6684191040', 'https://openalex.org/W2892295629', 'https://openalex.org/W6714995145', 'https://openalex.org/W6760593330', 'https://openalex.org/W6773772742', 'https://openalex.org/W6696901769', 'https://openalex.org/W1498730170', 'https://openalex.org/W1551571670', 'https://openalex.org/W3087881384', 'https://openalex.org/W3125572598', 'https://openalex.org/W6756040250', 'https://openalex.org/W6721127235', 'https://openalex.org/W2982700443', 'https://openalex.org/W6748816842', 'https://openalex.org/W6750267149', 'https://openalex.org/W2975116180', 'https://openalex.org/W2963771763', 'https://openalex.org/W3083116736', 'https://openalex.org/W2963525668', 'https://openalex.org/W6683590716', 'https://openalex.org/W6697028695', 'https://openalex.org/W2963073614', 'https://openalex.org/W2962793481', 'https://openalex.org/W6744627333', 'https://openalex.org/W6730558285', 'https://openalex.org/W2144343387', 'https://openalex.org/W6687045409', 'https://openalex.org/W6950401703', 'https://openalex.org/W2327509600', 'https://openalex.org/W6732135373', 'https://openalex.org/W6756630480', 'https://openalex.org/W6771075017', 'https://openalex.org/W4236250754', 'https://openalex.org/W3009830825', 'https://openalex.org/W2920929112', 'https://openalex.org/W2753738274', 'https://openalex.org/W2187089797', 'https://openalex.org/W3007399319', 'https://openalex.org/W2911959244', 'https://openalex.org/W2963575853', 'https://openalex.org/W1924619199', 'https://openalex.org/W2949382160', 'https://openalex.org/W2982125965', 'https://openalex.org/W2957258179', 'https://openalex.org/W2412515132', 'https://openalex.org/W2964098754', 'https://openalex.org/W2899771611', 'https://openalex.org/W3203565460', 'https://openalex.org/W2951438386', 'https://openalex.org/W2188365844', 'https://openalex.org/W2726805909', 'https://openalex.org/W2893749619', 'https://openalex.org/W2950893734', 'https://openalex.org/W2474605630', 'https://openalex.org/W2769811909', 'https://openalex.org/W1959608418', 'https://openalex.org/W2798405286', 'https://openalex.org/W2898422183', 'https://openalex.org/W2577482072', 'https://openalex.org/W2910577860', 'https://openalex.org/W2295991281', 'https://openalex.org/W2901601067', 'https://openalex.org/W2163605009', 'https://openalex.org/W3208504180', 'https://openalex.org/W1909320841', 'https://openalex.org/W3008938201', 'https://openalex.org/W2887335294', 'https://openalex.org/W2964223825', 'https://openalex.org/W3098518646', 'https://openalex.org/W2228991311', 'https://openalex.org/W2295130376', 'https://openalex.org/W2729456299', 'https://openalex.org/W2953100410', 'https://openalex.org/W2556225777', 'https://openalex.org/W2099471712', 'https://openalex.org/W3000654067', 'https://openalex.org/W3014144626', 'https://openalex.org/W2796897898', 'https://openalex.org/W2926317162', 'https://openalex.org/W2990880251', 'https://openalex.org/W2964074081', 'https://openalex.org/W2620076854', 'https://openalex.org/W2963681776', 'https://openalex.org/W2395774634', 'https://openalex.org/W3021164770', 'https://openalex.org/W2794719876', 'https://openalex.org/W2752134738', 'https://openalex.org/W2746068898', 'https://openalex.org/W1686810756', 'https://openalex.org/W2892144052', 'https://openalex.org/W3014204396', 'https://openalex.org/W2302243225', 'https://openalex.org/W2963032576', 'https://openalex.org/W2786672974', 'https://openalex.org/W2964193438']",2020-08-01
https://openalex.org/W3135935356,https://doi.org/10.1007/978-3-030-72914-1_2,Network Bending: Expressive Manipulation of Deep Generative Models,,"['https://openalex.org/W2985068832', 'https://openalex.org/W3106976604', 'https://openalex.org/W2961363059', 'https://openalex.org/W2963749936', 'https://openalex.org/W2901107321', 'https://openalex.org/W2897177022', 'https://openalex.org/W183625566', 'https://openalex.org/W2059515884', 'https://openalex.org/W3015758605', 'https://openalex.org/W2012897754', 'https://openalex.org/W2963073614', 'https://openalex.org/W2611393743', 'https://openalex.org/W2962770929', 'https://openalex.org/W3035574324', 'https://openalex.org/W2150593711', 'https://openalex.org/W2962974533', 'https://openalex.org/W2970971581', 'https://openalex.org/W3034431451', 'https://openalex.org/W2133547499', 'https://openalex.org/W6602254124', 'https://openalex.org/W1849277567', 'https://openalex.org/W2963125010', 'https://openalex.org/W2737258237', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963684088', 'https://openalex.org/W1489608363', 'https://openalex.org/W3183747836', 'https://openalex.org/W3101254953', 'https://openalex.org/W2962851944', 'https://openalex.org/W2963996492', 'https://openalex.org/W2962897886', 'https://openalex.org/W3122887115', 'https://openalex.org/W3116843471', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964144352', 'https://openalex.org/W2995327724', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962760235', 'https://openalex.org/W2994062013', 'https://openalex.org/W1959608418', 'https://openalex.org/W967544008', 'https://openalex.org/W2111294903', 'https://openalex.org/W2893749619', 'https://openalex.org/W2964074081', 'https://openalex.org/W2963522749', 'https://openalex.org/W2964121744']",2021-01-01
https://openalex.org/W3209221083,https://doi.org/10.21428/92fbeb44.06e2d5f4,What to Play and How to Play it: Guiding Generative Music Models with Multiple Demonstrations,"We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both ""what to play"" (via scores in MIDI format) and ""how to play it"" (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.","['https://openalex.org/W6776218486', 'https://openalex.org/W2901601067', 'https://openalex.org/W2891794946', 'https://openalex.org/W6783970415', 'https://openalex.org/W2957258179', 'https://openalex.org/W6606416387', 'https://openalex.org/W6755406732', 'https://openalex.org/W2343635552', 'https://openalex.org/W2948574791', 'https://openalex.org/W2936858163', 'https://openalex.org/W6771763809', 'https://openalex.org/W6751598888', 'https://openalex.org/W3049637270', 'https://openalex.org/W3049174246', 'https://openalex.org/W2946521317', 'https://openalex.org/W2990617740', 'https://openalex.org/W3014498426', 'https://openalex.org/W3046218710', 'https://openalex.org/W6640963894', 'https://openalex.org/W6893759657', 'https://openalex.org/W6785120823', 'https://openalex.org/W3047453285', 'https://openalex.org/W2132184901', 'https://openalex.org/W2161012827', 'https://openalex.org/W2059322641', 'https://openalex.org/W6712232353', 'https://openalex.org/W2115083233', 'https://openalex.org/W2982125965', 'https://openalex.org/W2772692325', 'https://openalex.org/W2792210438', 'https://openalex.org/W4287643030', 'https://openalex.org/W3021164770', 'https://openalex.org/W2805697608', 'https://openalex.org/W3092135915', 'https://openalex.org/W2982647297', 'https://openalex.org/W3000389243', 'https://openalex.org/W2396281659', 'https://openalex.org/W2995233853', 'https://openalex.org/W2169264582', 'https://openalex.org/W1959608418', 'https://openalex.org/W3154171150', 'https://openalex.org/W2962930338', 'https://openalex.org/W2991178431', 'https://openalex.org/W3098518646', 'https://openalex.org/W3112895167', 'https://openalex.org/W4288366059', 'https://openalex.org/W2962699318', 'https://openalex.org/W3111037961', 'https://openalex.org/W4287802874']",2021-07-19
https://openalex.org/W3189022627,https://doi.org/10.5281/zenodo.5624597,DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models,"Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.","['https://openalex.org/W3021164770', 'https://openalex.org/W2909703423', 'https://openalex.org/W3183357160', 'https://openalex.org/W1986975415', 'https://openalex.org/W2613740626', 'https://openalex.org/W3080857286', 'https://openalex.org/W2404407838', 'https://openalex.org/W2902076983', 'https://openalex.org/W2964706117', 'https://openalex.org/W2891757161', 'https://openalex.org/W2940744433', 'https://openalex.org/W2737347211', 'https://openalex.org/W3187060660', 'https://openalex.org/W2898148140', 'https://openalex.org/W2964110616', 'https://openalex.org/W3147925821', 'https://openalex.org/W2998490864', 'https://openalex.org/W3034559727', 'https://openalex.org/W2962941684', 'https://openalex.org/W2958623036', 'https://openalex.org/W2129192849', 'https://openalex.org/W3049247973', 'https://openalex.org/W2394782790', 'https://openalex.org/W2261080303', 'https://openalex.org/W2963494889', 'https://openalex.org/W2574842964', 'https://openalex.org/W3092879656', 'https://openalex.org/W2296230291', 'https://openalex.org/W3183478142', 'https://openalex.org/W3175663427', 'https://openalex.org/W2963403868', 'https://openalex.org/W2919624000', 'https://openalex.org/W3163784447', 'https://openalex.org/W2377006970', 'https://openalex.org/W3093121331', 'https://openalex.org/W2921495374', 'https://openalex.org/W3185497054', 'https://openalex.org/W2741766967', 'https://openalex.org/W2991248973', 'https://openalex.org/W2990716218']",2021-11-07
https://openalex.org/W3191340970,https://doi.org/10.5281/zenodo.5624507,DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis With GANs,"Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called ""soft labels"") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.","['https://openalex.org/W2948211236', 'https://openalex.org/W2962760235', 'https://openalex.org/W2962919088', 'https://openalex.org/W2774848319', 'https://openalex.org/W2750384547', 'https://openalex.org/W2593116425', 'https://openalex.org/W3137586311', 'https://openalex.org/W3015780049', 'https://openalex.org/W3094550259', 'https://openalex.org/W2910577860', 'https://openalex.org/W3109547540', 'https://openalex.org/W3127854286', 'https://openalex.org/W2987809065', 'https://openalex.org/W3034431451', 'https://openalex.org/W2962756039', 'https://openalex.org/W2108598243', 'https://openalex.org/W2099471712', 'https://openalex.org/W2746457594', 'https://openalex.org/W2963726581', 'https://openalex.org/W2962974533', 'https://openalex.org/W2950299304', 'https://openalex.org/W2804722844', 'https://openalex.org/W2680270903', 'https://openalex.org/W2294370754', 'https://openalex.org/W3136272958', 'https://openalex.org/W3029579848', 'https://openalex.org/W3118215210', 'https://openalex.org/W2561196672', 'https://openalex.org/W2796010067', 'https://openalex.org/W2532781556', 'https://openalex.org/W2955263139', 'https://openalex.org/W2962879692', 'https://openalex.org/W3015338123', 'https://openalex.org/W3021164770', 'https://openalex.org/W3098403858', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963889406', 'https://openalex.org/W2963927126', 'https://openalex.org/W2903005299', 'https://openalex.org/W2905488776', 'https://openalex.org/W2963402808', 'https://openalex.org/W2996286887', 'https://openalex.org/W2893749619', 'https://openalex.org/W1602122992', 'https://openalex.org/W3034526383', 'https://openalex.org/W2519091744', 'https://openalex.org/W2548275288', 'https://openalex.org/W3035574324', 'https://openalex.org/W2134797427', 'https://openalex.org/W2963971656', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963373786', 'https://openalex.org/W2606176153', 'https://openalex.org/W1821462560']",2021-11-07
https://openalex.org/W3141966616,https://doi.org/10.1007/s44163-023-00047-7,CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN,,"['https://openalex.org/W2982753834', 'https://openalex.org/W2032041077', 'https://openalex.org/W2514141612', 'https://openalex.org/W2997586082', 'https://openalex.org/W3092850823', 'https://openalex.org/W2128196382', 'https://openalex.org/W2962793481', 'https://openalex.org/W2798418213', 'https://openalex.org/W2964070952', 'https://openalex.org/W2963073614', 'https://openalex.org/W2966727317', 'https://openalex.org/W2739969482', 'https://openalex.org/W2616321149', 'https://openalex.org/W2997367363', 'https://openalex.org/W2601110281', 'https://openalex.org/W2782255520', 'https://openalex.org/W2809621972', 'https://openalex.org/W3034498211', 'https://openalex.org/W2161493891', 'https://openalex.org/W3102302752', 'https://openalex.org/W2963300588', 'https://openalex.org/W2990440871', 'https://openalex.org/W3015710813', 'https://openalex.org/W2914745777', 'https://openalex.org/W2198584637', 'https://openalex.org/W2945956523', 'https://openalex.org/W2891757161', 'https://openalex.org/W2962904371', 'https://openalex.org/W1981276685', 'https://openalex.org/W2898827701', 'https://openalex.org/W2952218014', 'https://openalex.org/W2478051194', 'https://openalex.org/W2103869314', 'https://openalex.org/W2054527121', 'https://openalex.org/W2004453603', 'https://openalex.org/W2120847449', 'https://openalex.org/W2331128040', 'https://openalex.org/W2963470893', 'https://openalex.org/W2339754110', 'https://openalex.org/W2794560802', 'https://openalex.org/W2602577565', 'https://openalex.org/W2976929305', 'https://openalex.org/W3008653537', 'https://openalex.org/W1522301498', 'https://openalex.org/W2559110679', 'https://openalex.org/W2402588270', 'https://openalex.org/W1511373337', 'https://openalex.org/W2950547518', 'https://openalex.org/W2579406683', 'https://openalex.org/W3021164770', 'https://openalex.org/W1819710477', 'https://openalex.org/W3104194627', 'https://openalex.org/W2948211236', 'https://openalex.org/W3077305422', 'https://openalex.org/W2803963372', 'https://openalex.org/W3097679990', 'https://openalex.org/W2963575853', 'https://openalex.org/W2766393356', 'https://openalex.org/W2962942158', 'https://openalex.org/W2612690371', 'https://openalex.org/W4212774754', 'https://openalex.org/W2963981733', 'https://openalex.org/W2760038648', 'https://openalex.org/W2953331651', 'https://openalex.org/W2746068898', 'https://openalex.org/W2949382160', 'https://openalex.org/W2792210438', 'https://openalex.org/W3099330747', 'https://openalex.org/W3104096215', 'https://openalex.org/W2919624000', 'https://openalex.org/W2971458685', 'https://openalex.org/W2792918004']",2023-01-19
https://openalex.org/W3154159596,https://doi.org/10.1109/iccv48922.2021.01409,Geometry-Free View Synthesis: Transformers and no 3D Priors,"Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. Code is available at https://git.io/JOnwn","['https://openalex.org/W2336968928', 'https://openalex.org/W3035524985', 'https://openalex.org/W2133665775', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962785568', 'https://openalex.org/W2609883120', 'https://openalex.org/W2960236945', 'https://openalex.org/W2963749150', 'https://openalex.org/W6687259251', 'https://openalex.org/W6786406055', 'https://openalex.org/W2963557767', 'https://openalex.org/W6704970058', 'https://openalex.org/W3048675442', 'https://openalex.org/W2551052086', 'https://openalex.org/W6729448088', 'https://openalex.org/W3204568876', 'https://openalex.org/W6639102338', 'https://openalex.org/W3109376949', 'https://openalex.org/W2063366997', 'https://openalex.org/W6637412569', 'https://openalex.org/W3034528136', 'https://openalex.org/W2963872754', 'https://openalex.org/W2943445277', 'https://openalex.org/W2895191479', 'https://openalex.org/W6765779288', 'https://openalex.org/W2118791399', 'https://openalex.org/W2554759989', 'https://openalex.org/W2495603374', 'https://openalex.org/W6674887261', 'https://openalex.org/W1971866894', 'https://openalex.org/W2942074357', 'https://openalex.org/W3034801905', 'https://openalex.org/W2752796333', 'https://openalex.org/W3108325989', 'https://openalex.org/W2536680313', 'https://openalex.org/W2520707372', 'https://openalex.org/W2985775862', 'https://openalex.org/W2770128544', 'https://openalex.org/W2294985758', 'https://openalex.org/W2901982540', 'https://openalex.org/W2811153284', 'https://openalex.org/W6790978476', 'https://openalex.org/W2028687412', 'https://openalex.org/W2519683295', 'https://openalex.org/W2471962767', 'https://openalex.org/W6732492507', 'https://openalex.org/W6784864557', 'https://openalex.org/W3109244019', 'https://openalex.org/W6762931180', 'https://openalex.org/W6782298681', 'https://openalex.org/W2884654623', 'https://openalex.org/W6776218486', 'https://openalex.org/W6726497184', 'https://openalex.org/W1893585201', 'https://openalex.org/W6694559427', 'https://openalex.org/W6787283782', 'https://openalex.org/W2949657144', 'https://openalex.org/W2129404737', 'https://openalex.org/W2300779272', 'https://openalex.org/W6679434410', 'https://openalex.org/W6780226713', 'https://openalex.org/W6779879114', 'https://openalex.org/W2238402354', 'https://openalex.org/W2990256309', 'https://openalex.org/W6730998768', 'https://openalex.org/W2758175304', 'https://openalex.org/W1971719398', 'https://openalex.org/W2985299701', 'https://openalex.org/W3034921716', 'https://openalex.org/W2598591334', 'https://openalex.org/W2413794162', 'https://openalex.org/W6730091202', 'https://openalex.org/W6757817989', 'https://openalex.org/W3109585842', 'https://openalex.org/W2949825757', 'https://openalex.org/W2963857374', 'https://openalex.org/W2273818272', 'https://openalex.org/W2908510526', 'https://openalex.org/W2963981733', 'https://openalex.org/W2188956040', 'https://openalex.org/W2964122153', 'https://openalex.org/W3034776267', 'https://openalex.org/W2962820504', 'https://openalex.org/W3167914619', 'https://openalex.org/W2963799213', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963403868', 'https://openalex.org/W2548228487', 'https://openalex.org/W1776042733', 'https://openalex.org/W1861492603', 'https://openalex.org/W3021164770', 'https://openalex.org/W2547875792', 'https://openalex.org/W2099471712', 'https://openalex.org/W2964308564', 'https://openalex.org/W3111114371', 'https://openalex.org/W2971074500', 'https://openalex.org/W3180355996', 'https://openalex.org/W2348664362', 'https://openalex.org/W3034445277', 'https://openalex.org/W1691728462', 'https://openalex.org/W2963263347', 'https://openalex.org/W3129576130', 'https://openalex.org/W3081167590']",2021-10-01
https://openalex.org/W3082087924,https://doi.org/10.5281/zenodo.4245584,Hierarchical timbre-painting and articulation generation,"We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.","['https://openalex.org/W2949382160', 'https://openalex.org/W2606176153', 'https://openalex.org/W2593414223', 'https://openalex.org/W2888169323', 'https://openalex.org/W2963889406', 'https://openalex.org/W2989708046', 'https://openalex.org/W2963493667', 'https://openalex.org/W1565658805', 'https://openalex.org/W3015710813', 'https://openalex.org/W2962793481', 'https://openalex.org/W2995233853', 'https://openalex.org/W2331128040', 'https://openalex.org/W2963799213', 'https://openalex.org/W2982041717', 'https://openalex.org/W2951535099', 'https://openalex.org/W2963233633', 'https://openalex.org/W3021164770', 'https://openalex.org/W2962760235', 'https://openalex.org/W2962866891', 'https://openalex.org/W3015338123', 'https://openalex.org/W2964121744', 'https://openalex.org/W3101943858']",2020-10-11
https://openalex.org/W3206801991,https://doi.org/10.1109/icassp43922.2022.9747441,KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms,"In this paper, we propose a novel neural network model called KaraSinger for a less-studied singing voice synthesis (SVS) task named score-free SVS, in which the prosody and melody are spontaneously decided by machine. KaraSinger comprises a vector-quantized variational autoencoder (VQ-VAE) that compresses the Mel-spectrograms of singing audio to sequences of discrete codes, and a language model (LM) that learns to predict the discrete codes given the corresponding lyrics. For the VQ-VAE part, we employ a Connectionist Temporal Classification (CTC) loss to encourage the discrete codes to carry phoneme-related information. For the LM part, we use location-sensitive attention for learning a robust alignment between the input phoneme sequence and the output discrete code. We keep the architecture of both the VQ-VAE and LM light-weight for fast training and inference speed. We validate the effectiveness of the proposed design choices using a proprietary collection of 550 English pop songs sung by multiple amateur singers. The result of a listening test shows that KaraSinger achieves high scores in intelligibility, musicality, and the overall quality.","['https://openalex.org/W6761551260', 'https://openalex.org/W6767111847', 'https://openalex.org/W6779709467', 'https://openalex.org/W6777028661', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015315843', 'https://openalex.org/W3197103763', 'https://openalex.org/W2972867623', 'https://openalex.org/W3081279708', 'https://openalex.org/W6783382068', 'https://openalex.org/W3097514409', 'https://openalex.org/W6796730497', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2973046048', 'https://openalex.org/W3096437652', 'https://openalex.org/W3015499232', 'https://openalex.org/W3133525064', 'https://openalex.org/W6739901393', 'https://openalex.org/W2516406502', 'https://openalex.org/W6790720088', 'https://openalex.org/W6678318511', 'https://openalex.org/W6623517193', 'https://openalex.org/W29794711', 'https://openalex.org/W2889244839', 'https://openalex.org/W2921576841', 'https://openalex.org/W3204116061', 'https://openalex.org/W3015437531', 'https://openalex.org/W2030149476', 'https://openalex.org/W6771665576', 'https://openalex.org/W6776218486', 'https://openalex.org/W3024973272', 'https://openalex.org/W6748381668', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W6762931180', 'https://openalex.org/W2970006822', 'https://openalex.org/W4287802874', 'https://openalex.org/W2953022181', 'https://openalex.org/W3082910224', 'https://openalex.org/W2938704169', 'https://openalex.org/W3024605872', 'https://openalex.org/W3034573343', 'https://openalex.org/W2789543585', 'https://openalex.org/W3037798801', 'https://openalex.org/W2971074500', 'https://openalex.org/W2996287690', 'https://openalex.org/W2963403868', 'https://openalex.org/W3133405188', 'https://openalex.org/W2408435475', 'https://openalex.org/W3174758275', 'https://openalex.org/W2124097505', 'https://openalex.org/W3021164770', 'https://openalex.org/W2964026424', 'https://openalex.org/W3114301328', 'https://openalex.org/W2963799213', 'https://openalex.org/W854541894', 'https://openalex.org/W4385245566']",2022-04-27
https://openalex.org/W3135367836,https://doi.org/10.48550/arxiv.2103.00020,Learning Transferable Visual Models From Natural Language Supervision,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.","['https://openalex.org/W2953937638', 'https://openalex.org/W2944828972', 'https://openalex.org/W2518108298', 'https://openalex.org/W3035688398', 'https://openalex.org/W2100031962', 'https://openalex.org/W2145215286', 'https://openalex.org/W3037492894', 'https://openalex.org/W2888166343', 'https://openalex.org/W2402144811', 'https://openalex.org/W3036982689', 'https://openalex.org/W2137471889', 'https://openalex.org/W2971155163', 'https://openalex.org/W3005188920', 'https://openalex.org/W3035267217', 'https://openalex.org/W3034588855', 'https://openalex.org/W3047092785', 'https://openalex.org/W2842511635', 'https://openalex.org/W2274287116', 'https://openalex.org/W3035160371', 'https://openalex.org/W2103163130', 'https://openalex.org/W2952610664', 'https://openalex.org/W2953276893', 'https://openalex.org/W2483215953', 'https://openalex.org/W2975501350', 'https://openalex.org/W2788481061', 'https://openalex.org/W2101234009', 'https://openalex.org/W3037309139', 'https://openalex.org/W2809324505', 'https://openalex.org/W2970018230', 'https://openalex.org/W2787214294', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963028402', 'https://openalex.org/W3035635319', 'https://openalex.org/W2108598243', 'https://openalex.org/W2979382951', 'https://openalex.org/W2250539671', 'https://openalex.org/W2109586012', 'https://openalex.org/W2774267535', 'https://openalex.org/W1880262756', 'https://openalex.org/W3047398678', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963060032', 'https://openalex.org/W3034408878', 'https://openalex.org/W2963026768', 'https://openalex.org/W3001555892', 'https://openalex.org/W2081613070', 'https://openalex.org/W3099903704', 'https://openalex.org/W1849277567', 'https://openalex.org/W3013644914', 'https://openalex.org/W3009561768', 'https://openalex.org/W2963341956', 'https://openalex.org/W2969280433', 'https://openalex.org/W2992604033', 'https://openalex.org/W2784121710', 'https://openalex.org/W3100511085', 'https://openalex.org/W2989168403', 'https://openalex.org/W3118051320', 'https://openalex.org/W2963403868', 'https://openalex.org/W2806857275', 'https://openalex.org/W3038476992', 'https://openalex.org/W2947707615', 'https://openalex.org/W3091546937', 'https://openalex.org/W2112912048', 'https://openalex.org/W2134270519', 'https://openalex.org/W2914002162', 'https://openalex.org/W2066255970', 'https://openalex.org/W2153579005', 'https://openalex.org/W2167905777', 'https://openalex.org/W3005680577', 'https://openalex.org/W1812645736', 'https://openalex.org/W3102950138', 'https://openalex.org/W2950541952', 'https://openalex.org/W2041616772', 'https://openalex.org/W2758782048', 'https://openalex.org/W2980149079', 'https://openalex.org/W3097436010', 'https://openalex.org/W3020257313', 'https://openalex.org/W1527575280', 'https://openalex.org/W1522301498', 'https://openalex.org/W3095169522', 'https://openalex.org/W3001279689', 'https://openalex.org/W3045687178', 'https://openalex.org/W2948636190', 'https://openalex.org/W2886641317', 'https://openalex.org/W2886604692', 'https://openalex.org/W3103856189', 'https://openalex.org/W2172191903', 'https://openalex.org/W2910458567', 'https://openalex.org/W2963855133', 'https://openalex.org/W3047467323', 'https://openalex.org/W2951655307', 'https://openalex.org/W3040002795', 'https://openalex.org/W2949801941', 'https://openalex.org/W2804935296', 'https://openalex.org/W2185175083', 'https://openalex.org/W2961193895', 'https://openalex.org/W2970692043', 'https://openalex.org/W3022116759', 'https://openalex.org/W3004542466', 'https://openalex.org/W2335728318', 'https://openalex.org/W2080171500', 'https://openalex.org/W2963756346', 'https://openalex.org/W3034445277', 'https://openalex.org/W2963207607', 'https://openalex.org/W2250384498', 'https://openalex.org/W2946948417', 'https://openalex.org/W2961301154', 'https://openalex.org/W2164587673', 'https://openalex.org/W2194775991', 'https://openalex.org/W2131744502', 'https://openalex.org/W3035058308', 'https://openalex.org/W2142996775', 'https://openalex.org/W2763421725', 'https://openalex.org/W24089286', 'https://openalex.org/W2799269579', 'https://openalex.org/W3016923549', 'https://openalex.org/W2949316785', 'https://openalex.org/W2963319085', 'https://openalex.org/W1981283549', 'https://openalex.org/W2895392434', 'https://openalex.org/W1967134278', 'https://openalex.org/W2149557440', 'https://openalex.org/W3034555021', 'https://openalex.org/W2998356391', 'https://openalex.org/W3016970897', 'https://openalex.org/W2963310665', 'https://openalex.org/W1677182931', 'https://openalex.org/W2462831000', 'https://openalex.org/W2117876524', 'https://openalex.org/W2964194231', 'https://openalex.org/W2949117887', 'https://openalex.org/W2963956526', 'https://openalex.org/W2987929641', 'https://openalex.org/W68733909', 'https://openalex.org/W2132339004', 'https://openalex.org/W2962714319', 'https://openalex.org/W2150066425', 'https://openalex.org/W2899136066', 'https://openalex.org/W3030163527', 'https://openalex.org/W2170973209', 'https://openalex.org/W3002578915', 'https://openalex.org/W2163284576', 'https://openalex.org/W2284646714', 'https://openalex.org/W2555897561', 'https://openalex.org/W2338908902', 'https://openalex.org/W2962784628', 'https://openalex.org/W3126960149', 'https://openalex.org/W2969958763', 'https://openalex.org/W2606220156', 'https://openalex.org/W2963785020', 'https://openalex.org/W2911681509', 'https://openalex.org/W3099878876', 'https://openalex.org/W2899663614', 'https://openalex.org/W2775461895', 'https://openalex.org/W2251939518', 'https://openalex.org/W3094502228', 'https://openalex.org/W2984008963', 'https://openalex.org/W3023989664', 'https://openalex.org/W2968993450', 'https://openalex.org/W2967013449', 'https://openalex.org/W3112156821', 'https://openalex.org/W2102765684', 'https://openalex.org/W2968880719', 'https://openalex.org/W2157487986', 'https://openalex.org/W2963873275', 'https://openalex.org/W2997972888', 'https://openalex.org/W2018881137', 'https://openalex.org/W3035524453', 'https://openalex.org/W2964213180', 'https://openalex.org/W1977295328', 'https://openalex.org/W2970608575', 'https://openalex.org/W2951292523', 'https://openalex.org/W1964806982', 'https://openalex.org/W2145607950', 'https://openalex.org/W2787560479', 'https://openalex.org/W2963281204', 'https://openalex.org/W2949517790', 'https://openalex.org/W2328078142', 'https://openalex.org/W3103585759', 'https://openalex.org/W2971315489', 'https://openalex.org/W3008526508', 'https://openalex.org/W2997371611', 'https://openalex.org/W2124219775', 'https://openalex.org/W2277195237', 'https://openalex.org/W2562153041', 'https://openalex.org/W3035060554', 'https://openalex.org/W2898970033', 'https://openalex.org/W2970115835', 'https://openalex.org/W3015146382', 'https://openalex.org/W27961112', 'https://openalex.org/W2612573399', 'https://openalex.org/W3021164770', 'https://openalex.org/W2969862959', 'https://openalex.org/W2119775030', 'https://openalex.org/W2981852735']",2021-02-26
https://openalex.org/W3162926177,https://doi.org/10.48550/arxiv.2105.05233,Diffusion Models Beat GANs on Image Synthesis,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion","['https://openalex.org/W3119800657', 'https://openalex.org/W2953054324', 'https://openalex.org/W2970241862', 'https://openalex.org/W2964144352', 'https://openalex.org/W2116064496', 'https://openalex.org/W2962770929', 'https://openalex.org/W2963981733', 'https://openalex.org/W2129069237', 'https://openalex.org/W3035384201', 'https://openalex.org/W3036167779', 'https://openalex.org/W3122887982', 'https://openalex.org/W2920684403', 'https://openalex.org/W3135367836', 'https://openalex.org/W2937274663', 'https://openalex.org/W2432004435', 'https://openalex.org/W2026799324', 'https://openalex.org/W2959300817', 'https://openalex.org/W2893749619', 'https://openalex.org/W2752253833', 'https://openalex.org/W3135058862', 'https://openalex.org/W2949382160', 'https://openalex.org/W2994434574', 'https://openalex.org/W2125389028', 'https://openalex.org/W3087665158', 'https://openalex.org/W3030163527', 'https://openalex.org/W2949457404', 'https://openalex.org/W3110257065', 'https://openalex.org/W3041956526', 'https://openalex.org/W2949605076', 'https://openalex.org/W3085640138', 'https://openalex.org/W2950541952', 'https://openalex.org/W2167433878', 'https://openalex.org/W2564591810', 'https://openalex.org/W3129576130', 'https://openalex.org/W3159875059', 'https://openalex.org/W2995327724', 'https://openalex.org/W2922772346', 'https://openalex.org/W2947590261', 'https://openalex.org/W3034445277', 'https://openalex.org/W3140373187', 'https://openalex.org/W1673923490', 'https://openalex.org/W2787579267', 'https://openalex.org/W3155072588', 'https://openalex.org/W2763421725', 'https://openalex.org/W3082563516', 'https://openalex.org/W2951402970', 'https://openalex.org/W3092442149', 'https://openalex.org/W2990706204', 'https://openalex.org/W1834627138', 'https://openalex.org/W1522301498', 'https://openalex.org/W3021164770', 'https://openalex.org/W3106570356', 'https://openalex.org/W2989576588', 'https://openalex.org/W2952020226', 'https://openalex.org/W2963836885', 'https://openalex.org/W2042492924', 'https://openalex.org/W3153854932', 'https://openalex.org/W2782980316', 'https://openalex.org/W2963799213', 'https://openalex.org/W2760103357']",2021-05-11
https://openalex.org/W3177813494,https://doi.org/10.48550/arxiv.2107.03374,Evaluating Large Language Models Trained on Code,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","['https://openalex.org/W2922911405', 'https://openalex.org/W603420404', 'https://openalex.org/W1581407678', 'https://openalex.org/W2996287690', 'https://openalex.org/W3190860428', 'https://openalex.org/W2055289715', 'https://openalex.org/W3170962973', 'https://openalex.org/W2972082064', 'https://openalex.org/W2951008357', 'https://openalex.org/W2947096529', 'https://openalex.org/W3132736064', 'https://openalex.org/W2902630600', 'https://openalex.org/W3146944767', 'https://openalex.org/W2964335063', 'https://openalex.org/W3021164770', 'https://openalex.org/W3098605233', 'https://openalex.org/W2980789587', 'https://openalex.org/W2949382160', 'https://openalex.org/W2304240348', 'https://openalex.org/W3117572899', 'https://openalex.org/W2145124323', 'https://openalex.org/W2992113183', 'https://openalex.org/W3126675481', 'https://openalex.org/W2963341956', 'https://openalex.org/W3037831233', 'https://openalex.org/W3034445277', 'https://openalex.org/W2970608575', 'https://openalex.org/W2734404947', 'https://openalex.org/W3082274269', 'https://openalex.org/W2146105230', 'https://openalex.org/W2153579005', 'https://openalex.org/W3162948197', 'https://openalex.org/W2787560479', 'https://openalex.org/W3156891177', 'https://openalex.org/W2973529529', 'https://openalex.org/W3040955294', 'https://openalex.org/W3135367836', 'https://openalex.org/W2973049837', 'https://openalex.org/W3046453918', 'https://openalex.org/W2887364112', 'https://openalex.org/W2037237472', 'https://openalex.org/W3155981360', 'https://openalex.org/W3118592163', 'https://openalex.org/W2768661419', 'https://openalex.org/W3089307846', 'https://openalex.org/W2728773317', 'https://openalex.org/W3148330722', 'https://openalex.org/W3033638351', 'https://openalex.org/W2890007640', 'https://openalex.org/W1978661986', 'https://openalex.org/W3033187248', 'https://openalex.org/W1843474218', 'https://openalex.org/W1977384182', 'https://openalex.org/W3082115681', 'https://openalex.org/W3105247453', 'https://openalex.org/W3164133168', 'https://openalex.org/W2153869077', 'https://openalex.org/W3099782249', 'https://openalex.org/W2965373594', 'https://openalex.org/W2951766328', 'https://openalex.org/W1860267373', 'https://openalex.org/W2626778328', 'https://openalex.org/W2267126114', 'https://openalex.org/W3123221944', 'https://openalex.org/W3001279689', 'https://openalex.org/W2153881107', 'https://openalex.org/W2550100435', 'https://openalex.org/W1534516911', 'https://openalex.org/W3118781290', 'https://openalex.org/W3095312831', 'https://openalex.org/W1551431154', 'https://openalex.org/W3133255051', 'https://openalex.org/W3129576130', 'https://openalex.org/W3036601975', 'https://openalex.org/W2170973209', 'https://openalex.org/W3161457214', 'https://openalex.org/W3212496002', 'https://openalex.org/W1660562555', 'https://openalex.org/W2599934248', 'https://openalex.org/W2964325845', 'https://openalex.org/W2142403498', 'https://openalex.org/W2949888546', 'https://openalex.org/W3168154341', 'https://openalex.org/W2842511635', 'https://openalex.org/W2950527759', 'https://openalex.org/W3007855629', 'https://openalex.org/W2173051530', 'https://openalex.org/W3193439412', 'https://openalex.org/W1810943226', 'https://openalex.org/W2132525863', 'https://openalex.org/W2530887700', 'https://openalex.org/W2940744433', 'https://openalex.org/W3133702157', 'https://openalex.org/W2950621612', 'https://openalex.org/W3170863103', 'https://openalex.org/W2972926655']",2021-07-07
https://openalex.org/W3129576130,https://doi.org/10.48550/arxiv.2102.12092,Zero-Shot Text-to-Image Generation,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.","['https://openalex.org/W2626778328', 'https://openalex.org/W1710476689', 'https://openalex.org/W2250384498', 'https://openalex.org/W2946900926', 'https://openalex.org/W3035207248', 'https://openalex.org/W2962784628', 'https://openalex.org/W2112796928', 'https://openalex.org/W3025935268', 'https://openalex.org/W2886641317', 'https://openalex.org/W3021164770', 'https://openalex.org/W2952165242', 'https://openalex.org/W2210838531', 'https://openalex.org/W2750822049', 'https://openalex.org/W2963981733', 'https://openalex.org/W2963373786', 'https://openalex.org/W3089089004', 'https://openalex.org/W2753738274', 'https://openalex.org/W2947590261', 'https://openalex.org/W2963163163', 'https://openalex.org/W2962741254', 'https://openalex.org/W2963799213', 'https://openalex.org/W2402144811', 'https://openalex.org/W2557449848', 'https://openalex.org/W2963281204', 'https://openalex.org/W2405756170', 'https://openalex.org/W2940744433', 'https://openalex.org/W2977720775', 'https://openalex.org/W2962843773', 'https://openalex.org/W2013494846', 'https://openalex.org/W3118580076', 'https://openalex.org/W2687693326', 'https://openalex.org/W2302255633', 'https://openalex.org/W2964122153', 'https://openalex.org/W14333344', 'https://openalex.org/W3016635207', 'https://openalex.org/W3048484056', 'https://openalex.org/W2547875792', 'https://openalex.org/W1522301498', 'https://openalex.org/W3101100041', 'https://openalex.org/W3113055895', 'https://openalex.org/W2952434594', 'https://openalex.org/W2951004968', 'https://openalex.org/W2763421725', 'https://openalex.org/W3034445277', 'https://openalex.org/W2950541952', 'https://openalex.org/W2170413022', 'https://openalex.org/W2242818861', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963966654', 'https://openalex.org/W2108598243', 'https://openalex.org/W2155292833', 'https://openalex.org/W2964024144', 'https://openalex.org/W2965289598', 'https://openalex.org/W3135367836', 'https://openalex.org/W2564591810']",2021-02-24
https://openalex.org/W3095645723,https://doi.org/10.48550/arxiv.2010.14701,Scaling Laws for Autoregressive Generative Modeling,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.","['https://openalex.org/W2741430497', 'https://openalex.org/W3009686669', 'https://openalex.org/W3021164770', 'https://openalex.org/W3023786569', 'https://openalex.org/W1544092585', 'https://openalex.org/W2768282280', 'https://openalex.org/W2963799213', 'https://openalex.org/W2940744433', 'https://openalex.org/W2912007050', 'https://openalex.org/W2952060378', 'https://openalex.org/W2903697572', 'https://openalex.org/W3022131108', 'https://openalex.org/W3001279689', 'https://openalex.org/W2994760783', 'https://openalex.org/W3019142305', 'https://openalex.org/W3034340181', 'https://openalex.org/W2970217468', 'https://openalex.org/W2776610675', 'https://openalex.org/W2809090039', 'https://openalex.org/W2981037730', 'https://openalex.org/W2996151541', 'https://openalex.org/W2787214294', 'https://openalex.org/W3036991069']",2020-10-28
https://openalex.org/W3152733922,https://doi.org/10.48550/arxiv.2104.10157,VideoGPT: Video Generation using VQ-VAE and Transformers,"We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html","['https://openalex.org/W2963871073', 'https://openalex.org/W2099471712', 'https://openalex.org/W24089286', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963293463', 'https://openalex.org/W2994903658', 'https://openalex.org/W2971034910', 'https://openalex.org/W3133405188', 'https://openalex.org/W2963300588', 'https://openalex.org/W3036033610', 'https://openalex.org/W2952453038', 'https://openalex.org/W2964122153', 'https://openalex.org/W2950946978', 'https://openalex.org/W2998108143', 'https://openalex.org/W2963092440', 'https://openalex.org/W2971074500', 'https://openalex.org/W2940744433', 'https://openalex.org/W2963345606', 'https://openalex.org/W2796303840', 'https://openalex.org/W2520707650', 'https://openalex.org/W1522734439', 'https://openalex.org/W2886748926', 'https://openalex.org/W2788033868', 'https://openalex.org/W3129576130', 'https://openalex.org/W3021164770', 'https://openalex.org/W3030163527', 'https://openalex.org/W2409550820', 'https://openalex.org/W2173520492', 'https://openalex.org/W2963629403', 'https://openalex.org/W2769810959', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963045453', 'https://openalex.org/W2115907784', 'https://openalex.org/W3036167779', 'https://openalex.org/W2140196014', 'https://openalex.org/W2981689412', 'https://openalex.org/W3041956526', 'https://openalex.org/W2901599654', 'https://openalex.org/W3034445277', 'https://openalex.org/W2949382160', 'https://openalex.org/W3010151642', 'https://openalex.org/W2963402657', 'https://openalex.org/W2129069237', 'https://openalex.org/W2994586138', 'https://openalex.org/W1583912456', 'https://openalex.org/W2964245526', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962770929', 'https://openalex.org/W2948412951', 'https://openalex.org/W2770201307', 'https://openalex.org/W2423557781', 'https://openalex.org/W3031246127', 'https://openalex.org/W2626778328', 'https://openalex.org/W2953212265', 'https://openalex.org/W2765363933', 'https://openalex.org/W2963373786', 'https://openalex.org/W2765349170', 'https://openalex.org/W2194775991', 'https://openalex.org/W2976617189', 'https://openalex.org/W2975414524', 'https://openalex.org/W3102720105', 'https://openalex.org/W3013229294', 'https://openalex.org/W2194321275', 'https://openalex.org/W2116435618', 'https://openalex.org/W3106570356', 'https://openalex.org/W2601686579', 'https://openalex.org/W2963467180', 'https://openalex.org/W2894946340', 'https://openalex.org/W2951682695', 'https://openalex.org/W2778792233', 'https://openalex.org/W2362143032', 'https://openalex.org/W2893749619', 'https://openalex.org/W2267126114', 'https://openalex.org/W2766527293']",2021-04-20
https://openalex.org/W3099378280,https://doi.org/10.48550/arxiv.2011.06801,"A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions","The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.","['https://openalex.org/W2153579005', 'https://openalex.org/W2997586082', 'https://openalex.org/W1586966408', 'https://openalex.org/W3102568015', 'https://openalex.org/W2994781558', 'https://openalex.org/W2962721334', 'https://openalex.org/W3029866750', 'https://openalex.org/W2088807535', 'https://openalex.org/W2043003570', 'https://openalex.org/W2054623822', 'https://openalex.org/W2262643855', 'https://openalex.org/W1970183352', 'https://openalex.org/W2892051461', 'https://openalex.org/W2982059008', 'https://openalex.org/W2007842460', 'https://openalex.org/W1904711963', 'https://openalex.org/W2963276790', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963985773', 'https://openalex.org/W3187060660', 'https://openalex.org/W2794234266', 'https://openalex.org/W2945460072', 'https://openalex.org/W2767052532', 'https://openalex.org/W3014271217', 'https://openalex.org/W3105183525', 'https://openalex.org/W2903214502', 'https://openalex.org/W2150933458', 'https://openalex.org/W3010671658', 'https://openalex.org/W1543786473', 'https://openalex.org/W3016250102', 'https://openalex.org/W2963636093', 'https://openalex.org/W2771726741', 'https://openalex.org/W2020166869', 'https://openalex.org/W1544473997', 'https://openalex.org/W1662133657', 'https://openalex.org/W3098518646', 'https://openalex.org/W2963681776', 'https://openalex.org/W2900265519', 'https://openalex.org/W2899775901', 'https://openalex.org/W2890231126', 'https://openalex.org/W2343635552', 'https://openalex.org/W2962947361', 'https://openalex.org/W149236175', 'https://openalex.org/W2396304210', 'https://openalex.org/W2975687953', 'https://openalex.org/W2786254735', 'https://openalex.org/W3109050852', 'https://openalex.org/W2102870814', 'https://openalex.org/W2516406502', 'https://openalex.org/W2964110616', 'https://openalex.org/W2963237661', 'https://openalex.org/W3111868559', 'https://openalex.org/W2800106835', 'https://openalex.org/W2995416527', 'https://openalex.org/W2963889406', 'https://openalex.org/W2998706454', 'https://openalex.org/W2321639984', 'https://openalex.org/W2601110281', 'https://openalex.org/W2985518820', 'https://openalex.org/W2936858163', 'https://openalex.org/W2293272996', 'https://openalex.org/W3092850823', 'https://openalex.org/W2962942158', 'https://openalex.org/W2752134738', 'https://openalex.org/W3136272958', 'https://openalex.org/W2963411769', 'https://openalex.org/W2515336442', 'https://openalex.org/W2963466318', 'https://openalex.org/W2553749962', 'https://openalex.org/W2954914497', 'https://openalex.org/W2606176153', 'https://openalex.org/W2566969886', 'https://openalex.org/W3049272330', 'https://openalex.org/W2991421901', 'https://openalex.org/W2963358591', 'https://openalex.org/W2995670387', 'https://openalex.org/W2164498268', 'https://openalex.org/W2129142580', 'https://openalex.org/W2143129936', 'https://openalex.org/W2973046048', 'https://openalex.org/W2401489644', 'https://openalex.org/W2990617740', 'https://openalex.org/W2891815651', 'https://openalex.org/W2940405045', 'https://openalex.org/W2124539664', 'https://openalex.org/W2952480865', 'https://openalex.org/W2963032576', 'https://openalex.org/W2798113456', 'https://openalex.org/W3011892665', 'https://openalex.org/W1947206766', 'https://openalex.org/W3010943931', 'https://openalex.org/W2963011080', 'https://openalex.org/W2401283236', 'https://openalex.org/W2747023714', 'https://openalex.org/W3015499232', 'https://openalex.org/W2937242376', 'https://openalex.org/W2760038648', 'https://openalex.org/W2125389028', 'https://openalex.org/W1644402181', 'https://openalex.org/W2560662714', 'https://openalex.org/W2001358307', 'https://openalex.org/W2295485174', 'https://openalex.org/W3035430139', 'https://openalex.org/W2239199480', 'https://openalex.org/W3123961192', 'https://openalex.org/W2990692574', 'https://openalex.org/W3007429516', 'https://openalex.org/W2892428174', 'https://openalex.org/W3000168898', 'https://openalex.org/W3104194627', 'https://openalex.org/W2131686285', 'https://openalex.org/W2894749360', 'https://openalex.org/W2903035274', 'https://openalex.org/W2514141612', 'https://openalex.org/W3015516707', 'https://openalex.org/W2283344589', 'https://openalex.org/W2774077477', 'https://openalex.org/W2889244839', 'https://openalex.org/W3112895167', 'https://openalex.org/W2591984255', 'https://openalex.org/W3029579848', 'https://openalex.org/W2963408210', 'https://openalex.org/W2008209667', 'https://openalex.org/W2962046549', 'https://openalex.org/W3034498211', 'https://openalex.org/W2975414524', 'https://openalex.org/W2604567995', 'https://openalex.org/W1556624199', 'https://openalex.org/W3022195800', 'https://openalex.org/W1540596182', 'https://openalex.org/W3093121331', 'https://openalex.org/W2912259457', 'https://openalex.org/W2135341757', 'https://openalex.org/W2949950073', 'https://openalex.org/W2592737436', 'https://openalex.org/W3099425575', 'https://openalex.org/W2513234653', 'https://openalex.org/W2991178431', 'https://openalex.org/W1924770834', 'https://openalex.org/W2890043615', 'https://openalex.org/W2978471471', 'https://openalex.org/W2265822310', 'https://openalex.org/W2902922910', 'https://openalex.org/W2919624000', 'https://openalex.org/W2564034046', 'https://openalex.org/W2778460379', 'https://openalex.org/W3122518304', 'https://openalex.org/W2137619888', 'https://openalex.org/W2031410675', 'https://openalex.org/W2761241264', 'https://openalex.org/W2999344275', 'https://openalex.org/W2902235484', 'https://openalex.org/W2963253162', 'https://openalex.org/W3090619461', 'https://openalex.org/W2963782041', 'https://openalex.org/W3183478142', 'https://openalex.org/W2288593361', 'https://openalex.org/W3003673875', 'https://openalex.org/W107523896', 'https://openalex.org/W2118763712', 'https://openalex.org/W2805697608', 'https://openalex.org/W2899000566', 'https://openalex.org/W1987905533', 'https://openalex.org/W2962981281', 'https://openalex.org/W2921576841', 'https://openalex.org/W2160473997', 'https://openalex.org/W2994986888', 'https://openalex.org/W2978309747', 'https://openalex.org/W1819710477', 'https://openalex.org/W2963233633', 'https://openalex.org/W2052265952', 'https://openalex.org/W3010903955', 'https://openalex.org/W2753868141', 'https://openalex.org/W2892104732', 'https://openalex.org/W2946521317', 'https://openalex.org/W2982125965', 'https://openalex.org/W2990713144', 'https://openalex.org/W2984106626', 'https://openalex.org/W2962883485', 'https://openalex.org/W2288119145', 'https://openalex.org/W2970730223', 'https://openalex.org/W2575455538', 'https://openalex.org/W2899124505', 'https://openalex.org/W2043162293', 'https://openalex.org/W2962941684', 'https://openalex.org/W2475287302', 'https://openalex.org/W1946974084', 'https://openalex.org/W3012498027', 'https://openalex.org/W2067709094', 'https://openalex.org/W2809621972', 'https://openalex.org/W2964286535', 'https://openalex.org/W3049247973', 'https://openalex.org/W2963557407', 'https://openalex.org/W2165527202', 'https://openalex.org/W2989708046', 'https://openalex.org/W1977444015', 'https://openalex.org/W2725868244', 'https://openalex.org/W2898148140', 'https://openalex.org/W2898827701', 'https://openalex.org/W2129192849', 'https://openalex.org/W60920252', 'https://openalex.org/W3030187020', 'https://openalex.org/W2945053121', 'https://openalex.org/W2963656263', 'https://openalex.org/W3092135915', 'https://openalex.org/W2788612614', 'https://openalex.org/W37402343', 'https://openalex.org/W2991108091', 'https://openalex.org/W2327437958', 'https://openalex.org/W2963568710', 'https://openalex.org/W2940676016', 'https://openalex.org/W2981967511', 'https://openalex.org/W2948211236', 'https://openalex.org/W2049367655', 'https://openalex.org/W2964016415', 'https://openalex.org/W2789365424', 'https://openalex.org/W3015843452', 'https://openalex.org/W2112349754', 'https://openalex.org/W2584032004', 'https://openalex.org/W2120847449', 'https://openalex.org/W3024973272', 'https://openalex.org/W2064675550', 'https://openalex.org/W3094138986', 'https://openalex.org/W2903005299', 'https://openalex.org/W12861549', 'https://openalex.org/W2896788168', 'https://openalex.org/W2775473773', 'https://openalex.org/W2964307104', 'https://openalex.org/W2963493667', 'https://openalex.org/W2963551352', 'https://openalex.org/W2142980624', 'https://openalex.org/W2901024736', 'https://openalex.org/W2799213409', 'https://openalex.org/W3040218036', 'https://openalex.org/W2964289981', 'https://openalex.org/W2914745777', 'https://openalex.org/W2786651925', 'https://openalex.org/W2891757161', 'https://openalex.org/W3031000691', 'https://openalex.org/W2946488335', 'https://openalex.org/W2338312508', 'https://openalex.org/W2174291476', 'https://openalex.org/W2963575853', 'https://openalex.org/W2884558435', 'https://openalex.org/W3038344852', 'https://openalex.org/W3018535504', 'https://openalex.org/W2067621398', 'https://openalex.org/W2076769328', 'https://openalex.org/W1496920063', 'https://openalex.org/W3125080906', 'https://openalex.org/W2964268978', 'https://openalex.org/W3092879656', 'https://openalex.org/W2941633540', 'https://openalex.org/W3154236293', 'https://openalex.org/W2887720658', 'https://openalex.org/W3173531154', 'https://openalex.org/W2795201383', 'https://openalex.org/W2972812066', 'https://openalex.org/W2074310426', 'https://openalex.org/W2950577311', 'https://openalex.org/W2894295011', 'https://openalex.org/W3154171150', 'https://openalex.org/W3036051869', 'https://openalex.org/W2951535099', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962714411', 'https://openalex.org/W3049174246', 'https://openalex.org/W3048830653', 'https://openalex.org/W3031005264', 'https://openalex.org/W3021164770', 'https://openalex.org/W2034161986', 'https://openalex.org/W3109510123', 'https://openalex.org/W2964105398', 'https://openalex.org/W763753110', 'https://openalex.org/W2899724567', 'https://openalex.org/W2997367363', 'https://openalex.org/W3016014797', 'https://openalex.org/W2963468922', 'https://openalex.org/W2145027674', 'https://openalex.org/W3036376755', 'https://openalex.org/W3048084370', 'https://openalex.org/W1990611383', 'https://openalex.org/W2912618358', 'https://openalex.org/W2964706117', 'https://openalex.org/W2963857374', 'https://openalex.org/W2592549691', 'https://openalex.org/W2582502747', 'https://openalex.org/W2086699924', 'https://openalex.org/W1556219185', 'https://openalex.org/W2921857201', 'https://openalex.org/W2788156829', 'https://openalex.org/W2808437126', 'https://openalex.org/W2972910332', 'https://openalex.org/W3100883077', 'https://openalex.org/W3108488605', 'https://openalex.org/W2995005087', 'https://openalex.org/W2327497238', 'https://openalex.org/W2989822430', 'https://openalex.org/W2963158134', 'https://openalex.org/W2579406683', 'https://openalex.org/W2739969482', 'https://openalex.org/W2978885571', 'https://openalex.org/W2572852026', 'https://openalex.org/W3048092838', 'https://openalex.org/W134527144', 'https://openalex.org/W3008434711', 'https://openalex.org/W3111037961', 'https://openalex.org/W2793183272', 'https://openalex.org/W2982431856', 'https://openalex.org/W2142996485', 'https://openalex.org/W2989919526', 'https://openalex.org/W3097679990', 'https://openalex.org/W2408435475', 'https://openalex.org/W3097469673', 'https://openalex.org/W3019084079', 'https://openalex.org/W2995233853', 'https://openalex.org/W2989955315', 'https://openalex.org/W1979251464', 'https://openalex.org/W2893613496', 'https://openalex.org/W3131643527', 'https://openalex.org/W2965526162', 'https://openalex.org/W2907543363', 'https://openalex.org/W2794719876', 'https://openalex.org/W2990885710', 'https://openalex.org/W2559110679', 'https://openalex.org/W3175179074', 'https://openalex.org/W2902184207']",2020-11-13
https://openalex.org/W3196163807,https://doi.org/10.48550/arxiv.2108.08827,ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis,"Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.","['https://openalex.org/W2475287302', 'https://openalex.org/W3118608800', 'https://openalex.org/W2129069237', 'https://openalex.org/W3162926177', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962785568', 'https://openalex.org/W3180355996', 'https://openalex.org/W2559823555', 'https://openalex.org/W2962820504', 'https://openalex.org/W2922386270', 'https://openalex.org/W2962990490', 'https://openalex.org/W2005308357', 'https://openalex.org/W648786980', 'https://openalex.org/W2962897886', 'https://openalex.org/W3098903812', 'https://openalex.org/W2963636093', 'https://openalex.org/W2798434327', 'https://openalex.org/W3034445277', 'https://openalex.org/W2108598243', 'https://openalex.org/W2963135265', 'https://openalex.org/W3135367836', 'https://openalex.org/W1710476689', 'https://openalex.org/W2963403868', 'https://openalex.org/W3098510582', 'https://openalex.org/W3152733922', 'https://openalex.org/W2893749619', 'https://openalex.org/W2560512785', 'https://openalex.org/W2962770929', 'https://openalex.org/W3121345697', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964122153', 'https://openalex.org/W967544008', 'https://openalex.org/W2971034910', 'https://openalex.org/W3021164770', 'https://openalex.org/W2962985516', 'https://openalex.org/W3103781353', 'https://openalex.org/W3120243996', 'https://openalex.org/W2572816092', 'https://openalex.org/W2542835211', 'https://openalex.org/W3100572490', 'https://openalex.org/W3150134882', 'https://openalex.org/W2963373786', 'https://openalex.org/W3099188186', 'https://openalex.org/W2962859665', 'https://openalex.org/W2903739847', 'https://openalex.org/W3035574324', 'https://openalex.org/W3034776267', 'https://openalex.org/W2949416428', 'https://openalex.org/W2970014727', 'https://openalex.org/W2963292439', 'https://openalex.org/W2985308740', 'https://openalex.org/W3135058862', 'https://openalex.org/W1866230956', 'https://openalex.org/W3129576130', 'https://openalex.org/W2331128040', 'https://openalex.org/W3119997354', 'https://openalex.org/W2906083776', 'https://openalex.org/W3122887982', 'https://openalex.org/W2962974533', 'https://openalex.org/W3112993007', 'https://openalex.org/W2963981733', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963341956', 'https://openalex.org/W2522389179', 'https://openalex.org/W2963174698', 'https://openalex.org/W3128876955', 'https://openalex.org/W3159647175', 'https://openalex.org/W2963586005']",2021-08-19
https://openalex.org/W3204896549,https://doi.org/10.48550/arxiv.2110.03675,ATISS: Autoregressive Transformers for Indoor Scene Synthesis,"The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.","['https://openalex.org/W2557269700', 'https://openalex.org/W3112776202', 'https://openalex.org/W2060268527', 'https://openalex.org/W3201922548', 'https://openalex.org/W2981010689', 'https://openalex.org/W3132535424', 'https://openalex.org/W2103815986', 'https://openalex.org/W3034573343', 'https://openalex.org/W2964122153', 'https://openalex.org/W2419448466', 'https://openalex.org/W2893749619', 'https://openalex.org/W2250673545', 'https://openalex.org/W2985936292', 'https://openalex.org/W3137120824', 'https://openalex.org/W3153854932', 'https://openalex.org/W3035574324', 'https://openalex.org/W2940744433', 'https://openalex.org/W3202579984', 'https://openalex.org/W3040538742', 'https://openalex.org/W2963807318', 'https://openalex.org/W3034445277', 'https://openalex.org/W2953273646', 'https://openalex.org/W2962695743', 'https://openalex.org/W3035046407', 'https://openalex.org/W3000176874', 'https://openalex.org/W2963341956', 'https://openalex.org/W2519091744', 'https://openalex.org/W2902539442', 'https://openalex.org/W2990222759', 'https://openalex.org/W2149846167', 'https://openalex.org/W3098903812', 'https://openalex.org/W2980088508', 'https://openalex.org/W2163605009', 'https://openalex.org/W2009188425', 'https://openalex.org/W2099471712', 'https://openalex.org/W3119786062', 'https://openalex.org/W3035108870', 'https://openalex.org/W2798622261', 'https://openalex.org/W2962770929', 'https://openalex.org/W2770875015', 'https://openalex.org/W2963767194', 'https://openalex.org/W2099854727', 'https://openalex.org/W3168314764', 'https://openalex.org/W3088333848', 'https://openalex.org/W2130634053', 'https://openalex.org/W2964121744', 'https://openalex.org/W2989341556', 'https://openalex.org/W2963403868', 'https://openalex.org/W2968638474', 'https://openalex.org/W3110015970', 'https://openalex.org/W2117741646', 'https://openalex.org/W2963981733', 'https://openalex.org/W3003555839', 'https://openalex.org/W2117088012', 'https://openalex.org/W2963676163', 'https://openalex.org/W2964334375', 'https://openalex.org/W3034600949', 'https://openalex.org/W2162559028', 'https://openalex.org/W2963139417', 'https://openalex.org/W2108598243', 'https://openalex.org/W3021164770', 'https://openalex.org/W2194775991', 'https://openalex.org/W3047258236', 'https://openalex.org/W3180355996', 'https://openalex.org/W3016025127', 'https://openalex.org/W3087067522', 'https://openalex.org/W3180251767', 'https://openalex.org/W3095066959', 'https://openalex.org/W1959608418', 'https://openalex.org/W2557465155', 'https://openalex.org/W3034727889', 'https://openalex.org/W2963925437', 'https://openalex.org/W3096609285', 'https://openalex.org/W2963601843', 'https://openalex.org/W2065745355', 'https://openalex.org/W2960202457', 'https://openalex.org/W3116489684', 'https://openalex.org/W2810181048']",2021-10-07
https://openalex.org/W3106570356,https://doi.org/10.48550/arxiv.2011.10650,Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images,"We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.","['https://openalex.org/W2963664914', 'https://openalex.org/W2963139417', 'https://openalex.org/W2949382160', 'https://openalex.org/W2560512785', 'https://openalex.org/W2911290044', 'https://openalex.org/W2899663614', 'https://openalex.org/W2940744433', 'https://openalex.org/W3030163527', 'https://openalex.org/W2949899814', 'https://openalex.org/W2963135265', 'https://openalex.org/W2964122153', 'https://openalex.org/W3041956526', 'https://openalex.org/W2764301726', 'https://openalex.org/W2970014727', 'https://openalex.org/W3150807214', 'https://openalex.org/W1499798934', 'https://openalex.org/W2963809228', 'https://openalex.org/W2026799324', 'https://openalex.org/W3036167779', 'https://openalex.org/W2140574335', 'https://openalex.org/W2964343746', 'https://openalex.org/W1583912456', 'https://openalex.org/W2962750131', 'https://openalex.org/W3034445277', 'https://openalex.org/W2587284713', 'https://openalex.org/W2409550820', 'https://openalex.org/W2963292439', 'https://openalex.org/W2963685250', 'https://openalex.org/W2907121943', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963428348', 'https://openalex.org/W1909320841', 'https://openalex.org/W2963636093', 'https://openalex.org/W2902630600', 'https://openalex.org/W3103543904', 'https://openalex.org/W3021164770']",2020-11-20
https://openalex.org/W3159309302,https://doi.org/10.48550/arxiv.2105.02769,Computer-Aided Design as Language,"Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation.","['https://openalex.org/W2971074500', 'https://openalex.org/W2139819695', 'https://openalex.org/W3160403431', 'https://openalex.org/W3034727889', 'https://openalex.org/W3044870283', 'https://openalex.org/W2903638214', 'https://openalex.org/W3011021499', 'https://openalex.org/W2111665814', 'https://openalex.org/W2972931660', 'https://openalex.org/W179875071', 'https://openalex.org/W1810943226', 'https://openalex.org/W3029625373', 'https://openalex.org/W2310393397', 'https://openalex.org/W3094502228', 'https://openalex.org/W2328200067', 'https://openalex.org/W2795456585', 'https://openalex.org/W1986755021', 'https://openalex.org/W1959608418', 'https://openalex.org/W2606712314', 'https://openalex.org/W2626778328', 'https://openalex.org/W3129576130', 'https://openalex.org/W1575207918', 'https://openalex.org/W3126753585', 'https://openalex.org/W3021164770', 'https://openalex.org/W2952254971', 'https://openalex.org/W2963636093', 'https://openalex.org/W2417863416', 'https://openalex.org/W3030163527', 'https://openalex.org/W2619125611', 'https://openalex.org/W2132339004', 'https://openalex.org/W3107232127', 'https://openalex.org/W2145135145', 'https://openalex.org/W2938704169', 'https://openalex.org/W2946359373', 'https://openalex.org/W3043729931', 'https://openalex.org/W2028928253']",2021-05-06
https://openalex.org/W3036520878,https://doi.org/10.48550/arxiv.2006.13202,Simple and Effective VAE Training with Calibrated Decoders,"Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of image and video datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. Project website is at https://orybkin.github.io/sigma-vae/","['https://openalex.org/W3041019643', 'https://openalex.org/W1909320841', 'https://openalex.org/W1834627138', 'https://openalex.org/W2768959015', 'https://openalex.org/W2804397569', 'https://openalex.org/W2335728318', 'https://openalex.org/W2528489519', 'https://openalex.org/W2910781732', 'https://openalex.org/W2990155616', 'https://openalex.org/W2992977009', 'https://openalex.org/W1970789124', 'https://openalex.org/W2953046278', 'https://openalex.org/W3101283005', 'https://openalex.org/W3034764327', 'https://openalex.org/W2963960193', 'https://openalex.org/W1516111018', 'https://openalex.org/W2922007426', 'https://openalex.org/W2600383743', 'https://openalex.org/W3097740127', 'https://openalex.org/W2970971581', 'https://openalex.org/W2567948266', 'https://openalex.org/W2955863859', 'https://openalex.org/W2963632741', 'https://openalex.org/W2911290044', 'https://openalex.org/W2963209089', 'https://openalex.org/W2963135265', 'https://openalex.org/W2892806280', 'https://openalex.org/W1959608418', 'https://openalex.org/W2808266363', 'https://openalex.org/W2753738274', 'https://openalex.org/W1850742715', 'https://openalex.org/W3100572490', 'https://openalex.org/W2343938449', 'https://openalex.org/W2805745179', 'https://openalex.org/W2622563070', 'https://openalex.org/W2970014727', 'https://openalex.org/W2900152462', 'https://openalex.org/W2519430864', 'https://openalex.org/W3021164770', 'https://openalex.org/W2789776893', 'https://openalex.org/W2683470288', 'https://openalex.org/W2978956737', 'https://openalex.org/W2765934517', 'https://openalex.org/W2963435596', 'https://openalex.org/W3118608800', 'https://openalex.org/W2788017640', 'https://openalex.org/W2796303840', 'https://openalex.org/W2769811909', 'https://openalex.org/W2963430173', 'https://openalex.org/W2927928207', 'https://openalex.org/W2949899814', 'https://openalex.org/W2188365844', 'https://openalex.org/W2412589713', 'https://openalex.org/W2950067852', 'https://openalex.org/W2963857374', 'https://openalex.org/W2991019415', 'https://openalex.org/W2963981733', 'https://openalex.org/W2125612430', 'https://openalex.org/W2587284713', 'https://openalex.org/W2626967530', 'https://openalex.org/W2964122153', 'https://openalex.org/W2023943903', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964327849', 'https://openalex.org/W1599263113']",2020-06-23
https://openalex.org/W3158129762,https://doi.org/10.48550/arxiv.2105.00335,Audio Transformers,"Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.","['https://openalex.org/W2782490852', 'https://openalex.org/W2402144811', 'https://openalex.org/W2108598243', 'https://openalex.org/W3119866685', 'https://openalex.org/W2593116425', 'https://openalex.org/W2963212250', 'https://openalex.org/W2981851019', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963341956', 'https://openalex.org/W3099782249', 'https://openalex.org/W3036601975', 'https://openalex.org/W3094502228', 'https://openalex.org/W2936975153', 'https://openalex.org/W2996383576', 'https://openalex.org/W2795935131', 'https://openalex.org/W3103145119', 'https://openalex.org/W2963563276', 'https://openalex.org/W1495590748', 'https://openalex.org/W3030163527', 'https://openalex.org/W2940744433', 'https://openalex.org/W1522301498', 'https://openalex.org/W2516301810', 'https://openalex.org/W2963799213', 'https://openalex.org/W2194775991', 'https://openalex.org/W2626778328', 'https://openalex.org/W3093494400', 'https://openalex.org/W3010387158', 'https://openalex.org/W2916113431', 'https://openalex.org/W3090388844', 'https://openalex.org/W3105661801']",2021-05-01
https://openalex.org/W3096312061,https://doi.org/10.48550/arxiv.2003.02395,A Simple Convergence Proof of Adam and Adagrad,"We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer, the dimension $d$, and the total number of iterations $N$. This bound can be made arbitrarily small, and with the right hyper-parameters, Adam can be shown to converge with the same rate of convergence $O(d\ln(N)/\sqrt{N})$. When used with the default parameters, Adam doesn't converge, however, and just like constant step-size SGD, it moves away from the initialization point faster than Adagrad, which might explain its practical success. Finally, we obtain the tightest dependency on the heavy ball momentum decay rate $β_1$ among all previous convergence bounds for non-convex Adam and Adagrad, improving from $O((1-β_1)^{-3})$ to $O((1-β_1)^{-1})$.","['https://openalex.org/W1518461880', 'https://openalex.org/W2947578309', 'https://openalex.org/W2963698657', 'https://openalex.org/W2337540838', 'https://openalex.org/W2886463271', 'https://openalex.org/W2146502635', 'https://openalex.org/W2945414017', 'https://openalex.org/W2963613486', 'https://openalex.org/W2785523195', 'https://openalex.org/W2963450615', 'https://openalex.org/W2757196798', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963208657', 'https://openalex.org/W2963341956', 'https://openalex.org/W2946511237', 'https://openalex.org/W2963563140', 'https://openalex.org/W2106709023', 'https://openalex.org/W1988720110']",2020-03-05
https://openalex.org/W3133405188,https://doi.org/10.48550/arxiv.2103.01950,Predicting Video with VQVAE,"In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.","['https://openalex.org/W2242818861', 'https://openalex.org/W2963428348', 'https://openalex.org/W2802701183', 'https://openalex.org/W2982494373', 'https://openalex.org/W2964318715', 'https://openalex.org/W2989708447', 'https://openalex.org/W2607738331', 'https://openalex.org/W2902630600', 'https://openalex.org/W2994760783', 'https://openalex.org/W2962839378', 'https://openalex.org/W2949382160', 'https://openalex.org/W2795093135', 'https://openalex.org/W2996375490', 'https://openalex.org/W1568514080', 'https://openalex.org/W2951108950', 'https://openalex.org/W2963092440', 'https://openalex.org/W2470142083', 'https://openalex.org/W2963435596', 'https://openalex.org/W2963482775', 'https://openalex.org/W2796303840', 'https://openalex.org/W2963857374', 'https://openalex.org/W2952453038', 'https://openalex.org/W2971074500', 'https://openalex.org/W2902437806', 'https://openalex.org/W2981803355', 'https://openalex.org/W2891145043', 'https://openalex.org/W2901599654', 'https://openalex.org/W2963125871', 'https://openalex.org/W2922386270', 'https://openalex.org/W2728326942', 'https://openalex.org/W2963799213', 'https://openalex.org/W2034328688', 'https://openalex.org/W2963402657', 'https://openalex.org/W2962876561', 'https://openalex.org/W2964327849', 'https://openalex.org/W2964270168', 'https://openalex.org/W2976617189', 'https://openalex.org/W2764019261', 'https://openalex.org/W2949099979', 'https://openalex.org/W1544092585', 'https://openalex.org/W2962942158', 'https://openalex.org/W2953133772', 'https://openalex.org/W2962750131', 'https://openalex.org/W3010151642', 'https://openalex.org/W2766527293', 'https://openalex.org/W2963669520', 'https://openalex.org/W2175030374', 'https://openalex.org/W2949527053', 'https://openalex.org/W2963547393', 'https://openalex.org/W2887051120', 'https://openalex.org/W3031246127', 'https://openalex.org/W2963406904', 'https://openalex.org/W2962841471', 'https://openalex.org/W3021164770', 'https://openalex.org/W2893749619', 'https://openalex.org/W2953318193', 'https://openalex.org/W2964295739', 'https://openalex.org/W2952595815', 'https://openalex.org/W2799055999']",2021-03-02
https://openalex.org/W3186883833,https://doi.org/10.48550/arxiv.2107.12979,Predictive Coding: a Theoretical and Experimental Review,"Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.","['https://openalex.org/W1973845142', 'https://openalex.org/W2231077521', 'https://openalex.org/W2981710180', 'https://openalex.org/W2286419477', 'https://openalex.org/W2962759636', 'https://openalex.org/W3125869599', 'https://openalex.org/W299440670', 'https://openalex.org/W1504499192', 'https://openalex.org/W3106570356', 'https://openalex.org/W2131352719', 'https://openalex.org/W2026799324', 'https://openalex.org/W3100445868', 'https://openalex.org/W2803735852', 'https://openalex.org/W2266468563', 'https://openalex.org/W3160726659', 'https://openalex.org/W1599969036', 'https://openalex.org/W2530226740', 'https://openalex.org/W2132003317', 'https://openalex.org/W2128123567', 'https://openalex.org/W3102087395', 'https://openalex.org/W2905333383', 'https://openalex.org/W3008716087', 'https://openalex.org/W2040891197', 'https://openalex.org/W2137411342', 'https://openalex.org/W2401640538', 'https://openalex.org/W2118615399', 'https://openalex.org/W81409850', 'https://openalex.org/W2146520244', 'https://openalex.org/W3016391357', 'https://openalex.org/W2963830987', 'https://openalex.org/W3021164770', 'https://openalex.org/W2520592730', 'https://openalex.org/W3024322256', 'https://openalex.org/W2734822889', 'https://openalex.org/W2963440770', 'https://openalex.org/W2444466369', 'https://openalex.org/W3023772498', 'https://openalex.org/W1516111018', 'https://openalex.org/W1991012308', 'https://openalex.org/W2048009237', 'https://openalex.org/W2100495367', 'https://openalex.org/W1966572806', 'https://openalex.org/W2127958135', 'https://openalex.org/W2949382160', 'https://openalex.org/W2091163800', 'https://openalex.org/W2095087224', 'https://openalex.org/W2096765209', 'https://openalex.org/W2787984105', 'https://openalex.org/W2951004968', 'https://openalex.org/W2766447205', 'https://openalex.org/W2979357328', 'https://openalex.org/W2056760934', 'https://openalex.org/W2003357516', 'https://openalex.org/W2157487943', 'https://openalex.org/W2030520068', 'https://openalex.org/W3025773901', 'https://openalex.org/W3008513355', 'https://openalex.org/W2885097534', 'https://openalex.org/W2978726661', 'https://openalex.org/W2105934661', 'https://openalex.org/W2120536867', 'https://openalex.org/W1456772534', 'https://openalex.org/W2918957485', 'https://openalex.org/W2157754891', 'https://openalex.org/W2513914902', 'https://openalex.org/W2036632921', 'https://openalex.org/W2276520418', 'https://openalex.org/W2794677645', 'https://openalex.org/W2992035660', 'https://openalex.org/W2120079537', 'https://openalex.org/W2912889105', 'https://openalex.org/W317397600', 'https://openalex.org/W3160026928', 'https://openalex.org/W2919878953', 'https://openalex.org/W1966897368', 'https://openalex.org/W1984205520', 'https://openalex.org/W2552810632', 'https://openalex.org/W2237059274', 'https://openalex.org/W3092785289', 'https://openalex.org/W3048338207', 'https://openalex.org/W2988666471', 'https://openalex.org/W2039546655', 'https://openalex.org/W3047432572', 'https://openalex.org/W2743406877', 'https://openalex.org/W2098580305', 'https://openalex.org/W2119885245', 'https://openalex.org/W3030163527', 'https://openalex.org/W2682853549', 'https://openalex.org/W1597739853', 'https://openalex.org/W1977628053', 'https://openalex.org/W2578935686', 'https://openalex.org/W3033108376', 'https://openalex.org/W3101380508', 'https://openalex.org/W3020107996', 'https://openalex.org/W3091211810', 'https://openalex.org/W2130195834', 'https://openalex.org/W2126750729', 'https://openalex.org/W2617660370', 'https://openalex.org/W2033422410', 'https://openalex.org/W1911093723', 'https://openalex.org/W3011691990', 'https://openalex.org/W621546036', 'https://openalex.org/W2161064560', 'https://openalex.org/W3161961853', 'https://openalex.org/W1969258103', 'https://openalex.org/W2984844508', 'https://openalex.org/W1988013646', 'https://openalex.org/W2128666650', 'https://openalex.org/W3013386261', 'https://openalex.org/W3124478039', 'https://openalex.org/W2935958002', 'https://openalex.org/W2527798464', 'https://openalex.org/W2031650972', 'https://openalex.org/W2127389037', 'https://openalex.org/W3129488543', 'https://openalex.org/W2626778328', 'https://openalex.org/W3135367836', 'https://openalex.org/W2898208057', 'https://openalex.org/W3005923475', 'https://openalex.org/W3133991449', 'https://openalex.org/W2547057105', 'https://openalex.org/W900292989', 'https://openalex.org/W2787636496', 'https://openalex.org/W3001279689', 'https://openalex.org/W2160195521', 'https://openalex.org/W1995875735', 'https://openalex.org/W2552737632', 'https://openalex.org/W3036730253', 'https://openalex.org/W2842511635', 'https://openalex.org/W2153156486', 'https://openalex.org/W2170478537', 'https://openalex.org/W1992040936', 'https://openalex.org/W1998794714', 'https://openalex.org/W2150070703', 'https://openalex.org/W2157586156', 'https://openalex.org/W3024686689', 'https://openalex.org/W2063934960', 'https://openalex.org/W2138309709', 'https://openalex.org/W2560145178', 'https://openalex.org/W2102409316', 'https://openalex.org/W3043133474', 'https://openalex.org/W2153791616', 'https://openalex.org/W2049633694', 'https://openalex.org/W2129531883', 'https://openalex.org/W2147008239', 'https://openalex.org/W1620203002', 'https://openalex.org/W2059320470', 'https://openalex.org/W2063062105', 'https://openalex.org/W1520661674', 'https://openalex.org/W3086298921', 'https://openalex.org/W2168202614', 'https://openalex.org/W1527571715', 'https://openalex.org/W2604086949', 'https://openalex.org/W2121863487', 'https://openalex.org/W2058616551', 'https://openalex.org/W2116360511', 'https://openalex.org/W1988472059', 'https://openalex.org/W2171717581', 'https://openalex.org/W2793221085', 'https://openalex.org/W1510016939', 'https://openalex.org/W3118210634', 'https://openalex.org/W2096962395', 'https://openalex.org/W2052685640', 'https://openalex.org/W3163192420']",2021-07-27
https://openalex.org/W3180663620,https://doi.org/10.48550/arxiv.2107.05677,Codified audio language modeling learns useful representations for music information retrieval,"We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox (Dhariwal et al. 2020): a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, emotion recognition, and key detection. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.","['https://openalex.org/W1556219185', 'https://openalex.org/W2191779130', 'https://openalex.org/W2626778328', 'https://openalex.org/W2133824856', 'https://openalex.org/W2962942158', 'https://openalex.org/W2019360207', 'https://openalex.org/W1960352584', 'https://openalex.org/W3036601975', 'https://openalex.org/W2952502547', 'https://openalex.org/W3000400453', 'https://openalex.org/W3102568015', 'https://openalex.org/W3021164770', 'https://openalex.org/W3139080614', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035137491', 'https://openalex.org/W2940744433', 'https://openalex.org/W2946359678', 'https://openalex.org/W2964274466', 'https://openalex.org/W2765119701', 'https://openalex.org/W3099782249', 'https://openalex.org/W2955142818', 'https://openalex.org/W2963430224', 'https://openalex.org/W2765325162', 'https://openalex.org/W2962739339', 'https://openalex.org/W3155776249', 'https://openalex.org/W2964307104', 'https://openalex.org/W3118485687', 'https://openalex.org/W3095513901', 'https://openalex.org/W2962904371', 'https://openalex.org/W2296751288', 'https://openalex.org/W3143852976', 'https://openalex.org/W2293255527', 'https://openalex.org/W3093494400', 'https://openalex.org/W2991108091', 'https://openalex.org/W2949382160', 'https://openalex.org/W2295991281', 'https://openalex.org/W2988736778', 'https://openalex.org/W3001279689', 'https://openalex.org/W2136129419', 'https://openalex.org/W335809467', 'https://openalex.org/W2137619888', 'https://openalex.org/W2990244497', 'https://openalex.org/W3034978746', 'https://openalex.org/W1849277567', 'https://openalex.org/W3029858316', 'https://openalex.org/W2794150026', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964051853', 'https://openalex.org/W2407685581', 'https://openalex.org/W1989445502', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963550089', 'https://openalex.org/W3034445277', 'https://openalex.org/W2922565841', 'https://openalex.org/W3139211892', 'https://openalex.org/W3010903955', 'https://openalex.org/W2127870748', 'https://openalex.org/W2584032004', 'https://openalex.org/W2896197082', 'https://openalex.org/W2906658932', 'https://openalex.org/W2023001347']",2021-07-12
https://openalex.org/W3046970875,https://doi.org/10.48550/arxiv.2008.01160,A Spectral Energy Distance for Parallel Speech Synthesis,"Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.","['https://openalex.org/W2194775991', 'https://openalex.org/W2193413348', 'https://openalex.org/W3021164770', 'https://openalex.org/W2949382160', 'https://openalex.org/W2893749619', 'https://openalex.org/W2963989027', 'https://openalex.org/W2212660284', 'https://openalex.org/W3000389243', 'https://openalex.org/W2993118648', 'https://openalex.org/W2409550820', 'https://openalex.org/W1522301498', 'https://openalex.org/W2943554574', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963300588', 'https://openalex.org/W3015338123', 'https://openalex.org/W2587284713', 'https://openalex.org/W1779010541', 'https://openalex.org/W2785678896', 'https://openalex.org/W2949995983', 'https://openalex.org/W2808235392', 'https://openalex.org/W2975414524', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963139417', 'https://openalex.org/W2804704270', 'https://openalex.org/W2970006822', 'https://openalex.org/W1992272902', 'https://openalex.org/W2769810959', 'https://openalex.org/W2962878605', 'https://openalex.org/W2963981733', 'https://openalex.org/W2883853252', 'https://openalex.org/W2899882692', 'https://openalex.org/W2125930537', 'https://openalex.org/W1487641199', 'https://openalex.org/W2963175743', 'https://openalex.org/W2025720061', 'https://openalex.org/W2788851830']",2020-08-03
https://openalex.org/W3207498282,https://doi.org/10.48550/arxiv.2110.08791,Taming Visually Guided Sound Generation,"Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU. We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples. Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/SpecVQGAN","['https://openalex.org/W2799055999', 'https://openalex.org/W3045082460', 'https://openalex.org/W3094917204', 'https://openalex.org/W3170016573', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963092440', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963799213', 'https://openalex.org/W3171443854', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962835968', 'https://openalex.org/W2905488776', 'https://openalex.org/W3133428285', 'https://openalex.org/W2963807156', 'https://openalex.org/W3015584015', 'https://openalex.org/W3178321840', 'https://openalex.org/W2964024144', 'https://openalex.org/W2963066677', 'https://openalex.org/W3109606338', 'https://openalex.org/W787785461', 'https://openalex.org/W2964121744', 'https://openalex.org/W2405756170', 'https://openalex.org/W2893813411', 'https://openalex.org/W2242818861', 'https://openalex.org/W3140373187', 'https://openalex.org/W2108598243', 'https://openalex.org/W2996286887', 'https://openalex.org/W2962754210', 'https://openalex.org/W3153296362', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963981733', 'https://openalex.org/W2908510526', 'https://openalex.org/W3163662330', 'https://openalex.org/W3165647589', 'https://openalex.org/W2981851635', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963663420', 'https://openalex.org/W2912947616', 'https://openalex.org/W3034445277', 'https://openalex.org/W3034521057', 'https://openalex.org/W2999531799', 'https://openalex.org/W3101943858', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963373786', 'https://openalex.org/W3120602499', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962919088', 'https://openalex.org/W2971074500', 'https://openalex.org/W3017343282', 'https://openalex.org/W2952064829', 'https://openalex.org/W3093209529', 'https://openalex.org/W2981876524', 'https://openalex.org/W3046890131', 'https://openalex.org/W2964345931', 'https://openalex.org/W3185439532', 'https://openalex.org/W2526050071', 'https://openalex.org/W2548275288', 'https://openalex.org/W3152733922', 'https://openalex.org/W2163605009', 'https://openalex.org/W3160582690', 'https://openalex.org/W2984529706', 'https://openalex.org/W2099471712', 'https://openalex.org/W3110422536', 'https://openalex.org/W3103837916', 'https://openalex.org/W2962865004', 'https://openalex.org/W2519091744', 'https://openalex.org/W2783879794', 'https://openalex.org/W2963073614', 'https://openalex.org/W3015371781', 'https://openalex.org/W2963841322', 'https://openalex.org/W2963966654', 'https://openalex.org/W2950971447', 'https://openalex.org/W3136272958', 'https://openalex.org/W3150091376', 'https://openalex.org/W2796303840', 'https://openalex.org/W2064675550', 'https://openalex.org/W2120847449']",2021-10-17
https://openalex.org/W3039049049,https://doi.org/10.48550/arxiv.2007.00800,A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks,"Deep neural networks are typically trained under a supervised learning framework where a model learns a single task using labeled data. Instead of relying solely on labeled data, practitioners can harness unlabeled or related data to improve model performance, which is often more accessible and ubiquitous. Self-supervised pre-training for transfer learning is becoming an increasingly popular technique to improve state-of-the-art results using unlabeled data. It involves first pre-training a model on a large amount of unlabeled data, then adapting the model to target tasks of interest. In this review, we survey self-supervised learning methods and their applications within the sequential transfer learning framework. We provide an overview of the taxonomy for self-supervised learning and transfer learning, and highlight some prominent methods for designing pre-training tasks across different domains. Finally, we discuss recent trends and suggest areas for future investigation.","['https://openalex.org/W2125999363', 'https://openalex.org/W2963403868', 'https://openalex.org/W2604763608', 'https://openalex.org/W2963351454', 'https://openalex.org/W3015146382', 'https://openalex.org/W2963799213', 'https://openalex.org/W2170973209', 'https://openalex.org/W2147152072', 'https://openalex.org/W2792210438', 'https://openalex.org/W2798991696', 'https://openalex.org/W2116435618', 'https://openalex.org/W2146444479', 'https://openalex.org/W2963310665', 'https://openalex.org/W2105464873', 'https://openalex.org/W3090449556', 'https://openalex.org/W2165698076', 'https://openalex.org/W2516255829', 'https://openalex.org/W2089468765', 'https://openalex.org/W3013571468', 'https://openalex.org/W2969728147', 'https://openalex.org/W3021164770', 'https://openalex.org/W3082274269', 'https://openalex.org/W2575671312', 'https://openalex.org/W2979736514', 'https://openalex.org/W2963826423', 'https://openalex.org/W1614298861', 'https://openalex.org/W2064675550', 'https://openalex.org/W2326925005', 'https://openalex.org/W2138857742', 'https://openalex.org/W1748744376', 'https://openalex.org/W2487442924', 'https://openalex.org/W2949117887', 'https://openalex.org/W2099471712', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972867623', 'https://openalex.org/W2919115771', 'https://openalex.org/W3171229070', 'https://openalex.org/W2753709519', 'https://openalex.org/W2952468927', 'https://openalex.org/W2619697695', 'https://openalex.org/W2110798204', 'https://openalex.org/W3035524453', 'https://openalex.org/W2015140204', 'https://openalex.org/W2412320034', 'https://openalex.org/W3006924828', 'https://openalex.org/W2025768430', 'https://openalex.org/W2963813662', 'https://openalex.org/W2963341956', 'https://openalex.org/W3025035610', 'https://openalex.org/W2995480165', 'https://openalex.org/W3007373432', 'https://openalex.org/W2951873722', 'https://openalex.org/W3024171804', 'https://openalex.org/W2755926663', 'https://openalex.org/W2980360762', 'https://openalex.org/W2965373594', 'https://openalex.org/W2108384452', 'https://openalex.org/W3104240813', 'https://openalex.org/W2624871570', 'https://openalex.org/W2842511635', 'https://openalex.org/W3034999214', 'https://openalex.org/W2108665656', 'https://openalex.org/W2962728572', 'https://openalex.org/W2950726992', 'https://openalex.org/W2944828972', 'https://openalex.org/W343636949', 'https://openalex.org/W2963420272', 'https://openalex.org/W2913340405', 'https://openalex.org/W2157364932', 'https://openalex.org/W3010874390', 'https://openalex.org/W1665214252', 'https://openalex.org/W2786808285', 'https://openalex.org/W2911779594', 'https://openalex.org/W2963114873', 'https://openalex.org/W1853900790', 'https://openalex.org/W2123421115', 'https://openalex.org/W2153579005', 'https://openalex.org/W2964212578', 'https://openalex.org/W2145680191', 'https://openalex.org/W830076066', 'https://openalex.org/W1970958875', 'https://openalex.org/W2938830017', 'https://openalex.org/W3010768098', 'https://openalex.org/W1959608418', 'https://openalex.org/W2980895577', 'https://openalex.org/W2042492924', 'https://openalex.org/W2599837529', 'https://openalex.org/W2788388592', 'https://openalex.org/W3034942609', 'https://openalex.org/W3011411500', 'https://openalex.org/W2119717200', 'https://openalex.org/W3030163527', 'https://openalex.org/W2975501350', 'https://openalex.org/W2108496475', 'https://openalex.org/W3034429116', 'https://openalex.org/W3034340181', 'https://openalex.org/W3033638351', 'https://openalex.org/W2163922914', 'https://openalex.org/W2136922672', 'https://openalex.org/W2948433173', 'https://openalex.org/W3005680577', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963756346', 'https://openalex.org/W2970597249', 'https://openalex.org/W2308529009', 'https://openalex.org/W2558661413', 'https://openalex.org/W2963854351', 'https://openalex.org/W2138621090', 'https://openalex.org/W2470220343', 'https://openalex.org/W2613634265', 'https://openalex.org/W2959020461', 'https://openalex.org/W2970119519', 'https://openalex.org/W2100495367', 'https://openalex.org/W2321533354']",2020-07-01
https://openalex.org/W3111853169,https://doi.org/10.48550/arxiv.2012.03478,Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements,"We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.","['https://openalex.org/W2919624000', 'https://openalex.org/W2948211236', 'https://openalex.org/W2963799213', 'https://openalex.org/W2883853252', 'https://openalex.org/W3101943858', 'https://openalex.org/W2962795401', 'https://openalex.org/W2962865004', 'https://openalex.org/W2963782041', 'https://openalex.org/W2964109005', 'https://openalex.org/W2910577860', 'https://openalex.org/W2963804063', 'https://openalex.org/W2951004968', 'https://openalex.org/W3035250874', 'https://openalex.org/W2769811909', 'https://openalex.org/W3017343282', 'https://openalex.org/W2950547518', 'https://openalex.org/W2784435047', 'https://openalex.org/W2963403868', 'https://openalex.org/W2903831537', 'https://openalex.org/W3037391061', 'https://openalex.org/W2981851635', 'https://openalex.org/W2738406145', 'https://openalex.org/W2979157532', 'https://openalex.org/W2556930864', 'https://openalex.org/W3045082460', 'https://openalex.org/W3034548564', 'https://openalex.org/W2964345931', 'https://openalex.org/W2914217321', 'https://openalex.org/W3021164770', 'https://openalex.org/W2619697695', 'https://openalex.org/W3046890131', 'https://openalex.org/W2787919227', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963218389', 'https://openalex.org/W2511428026', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963066677', 'https://openalex.org/W2963807156', 'https://openalex.org/W2949382160', 'https://openalex.org/W3102619627', 'https://openalex.org/W3016096605', 'https://openalex.org/W2899724567', 'https://openalex.org/W2995416527', 'https://openalex.org/W2962756039']",2020-12-07
https://openalex.org/W3196954682,https://doi.org/10.48550/arxiv.2109.00663,Controllable deep melody generation via hierarchical music structure representation,"Recent advances in deep learning have expanded possibilities to generate music, but generating a customizable full piece of music with consistent long-term structure remains a challenge. This paper introduces MusicFrameworks, a hierarchical music structure representation and a multi-step generative process to create a full-length melody guided by long-term repetitive structure, chord, melodic contour, and rhythm constraints. We first organize the full melody with section and phrase-level structure. To generate melody in each phrase, we generate rhythm and basic melody using two separate transformer-based networks, and then generate the melody conditioned on the basic melody, rhythm and chords in an auto-regressive manner. By factoring music generation into sub-problems, our approach allows simpler models and requires less data. To customize or add variety, one can alter chords, basic melody, and rhythm structure in the music frameworks, letting our networks generate the melody accordingly. Additionally, we introduce new features to encode musical positional information, rhythm patterns, and melodic contours based on musical domain knowledge. A listening test reveals that melodies generated by our method are rated as good as or better than human-composed music in the POP909 dataset about half the time.","['https://openalex.org/W2982753834', 'https://openalex.org/W3124126546', 'https://openalex.org/W2902052009', 'https://openalex.org/W579034728', 'https://openalex.org/W3003673875', 'https://openalex.org/W2952428868', 'https://openalex.org/W2948211236', 'https://openalex.org/W3049637270', 'https://openalex.org/W561116024', 'https://openalex.org/W2148730083', 'https://openalex.org/W3046715528', 'https://openalex.org/W3125080906', 'https://openalex.org/W1819710477', 'https://openalex.org/W3154171150', 'https://openalex.org/W1977444015', 'https://openalex.org/W2053952057', 'https://openalex.org/W3097469673', 'https://openalex.org/W2074308058', 'https://openalex.org/W1710476689', 'https://openalex.org/W2948574791', 'https://openalex.org/W3183478142', 'https://openalex.org/W2792210438', 'https://openalex.org/W2608172077', 'https://openalex.org/W3021164770', 'https://openalex.org/W3049247973', 'https://openalex.org/W3162151557', 'https://openalex.org/W2963681776', 'https://openalex.org/W288571436', 'https://openalex.org/W3030056709', 'https://openalex.org/W2963575853', 'https://openalex.org/W2312878813', 'https://openalex.org/W2964268978', 'https://openalex.org/W2951004968']",2021-09-02
https://openalex.org/W3136991969,https://doi.org/10.48550/arxiv.2103.12157,Tiny Transformers for Environmental Sound Classification at the Edge,"With the growth of the Internet of Things and the rise of Big Data, data processing and machine learning applications are being moved to cheap and low size, weight, and power (SWaP) devices at the edge, often in the form of mobile phones, embedded systems, or microcontrollers. The field of Cyber-Physical Measurements and Signature Intelligence (MASINT) makes use of these devices to analyze and exploit data in ways not otherwise possible, which results in increased data quality, increased security, and decreased bandwidth. However, methods to train and deploy models at the edge are limited, and models with sufficient accuracy are often too large for the edge device. Therefore, there is a clear need for techniques to create efficient AI/ML at the edge. This work presents training techniques for audio models in the field of environmental sound classification at the edge. Specifically, we design and train Transformers to classify office sounds in audio clips. Results show that a BERT-based Transformer, trained on Mel spectrograms, can outperform a CNN using 99.85% fewer parameters. To achieve this result, we first tested several audio feature extraction techniques designed for Transformers, using ESC-50 for evaluation, along with various augmentations. Our final model outperforms the state-of-the-art MFCC-based CNN on the office sounds dataset, using just over 6,000 parameters -- small enough to run on a microcontroller.","['https://openalex.org/W2593451766', 'https://openalex.org/W2972247414', 'https://openalex.org/W2949117887', 'https://openalex.org/W2962910554', 'https://openalex.org/W2529337537', 'https://openalex.org/W1997527178', 'https://openalex.org/W2751841560', 'https://openalex.org/W3046125265', 'https://openalex.org/W3082878506', 'https://openalex.org/W2963341956', 'https://openalex.org/W2525778437', 'https://openalex.org/W1972567154', 'https://openalex.org/W2914312680', 'https://openalex.org/W2975429091', 'https://openalex.org/W3030163527', 'https://openalex.org/W2970597249', 'https://openalex.org/W2996428491', 'https://openalex.org/W3016195076', 'https://openalex.org/W2963799213', 'https://openalex.org/W2978017171', 'https://openalex.org/W2943493972', 'https://openalex.org/W2768083292', 'https://openalex.org/W2795138957', 'https://openalex.org/W2963403868', 'https://openalex.org/W2981852735', 'https://openalex.org/W3045733172', 'https://openalex.org/W3035623224', 'https://openalex.org/W3021164770', 'https://openalex.org/W2038484192', 'https://openalex.org/W1979594949', 'https://openalex.org/W2909327627', 'https://openalex.org/W3098357269', 'https://openalex.org/W3046727238', 'https://openalex.org/W2676925568', 'https://openalex.org/W3094175596', 'https://openalex.org/W3034445277', 'https://openalex.org/W2963838685', 'https://openalex.org/W2969515962', 'https://openalex.org/W2949382160', 'https://openalex.org/W3043803126', 'https://openalex.org/W3027588817', 'https://openalex.org/W2766087987', 'https://openalex.org/W2593116425', 'https://openalex.org/W1996160964', 'https://openalex.org/W3094502228', 'https://openalex.org/W3095348033', 'https://openalex.org/W2052666245', 'https://openalex.org/W2766397045', 'https://openalex.org/W2008415856', 'https://openalex.org/W2313745396', 'https://openalex.org/W3044604993']",2021-03-22
https://openalex.org/W3168452307,https://doi.org/10.48550/arxiv.2106.05931,Score-based Generative Modeling in Latent Space,"Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .","['https://openalex.org/W2963428348', 'https://openalex.org/W2013035813', 'https://openalex.org/W3131237641', 'https://openalex.org/W2962844293', 'https://openalex.org/W2963223306', 'https://openalex.org/W3037879915', 'https://openalex.org/W2989221291', 'https://openalex.org/W2970014727', 'https://openalex.org/W2156047073', 'https://openalex.org/W2962676938', 'https://openalex.org/W3006076983', 'https://openalex.org/W2962937159', 'https://openalex.org/W3092033429', 'https://openalex.org/W3038406429', 'https://openalex.org/W2963981733', 'https://openalex.org/W3167749434', 'https://openalex.org/W2962897886', 'https://openalex.org/W3147652402', 'https://openalex.org/W2412510955', 'https://openalex.org/W2946928580', 'https://openalex.org/W2560512785', 'https://openalex.org/W3028907532', 'https://openalex.org/W3121370741', 'https://openalex.org/W2949649223', 'https://openalex.org/W3021164770', 'https://openalex.org/W2970324680', 'https://openalex.org/W3180196270', 'https://openalex.org/W3034625979', 'https://openalex.org/W3108645709', 'https://openalex.org/W2113300847', 'https://openalex.org/W2940744433', 'https://openalex.org/W2970181183', 'https://openalex.org/W2144144709', 'https://openalex.org/W3104876213', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964122153', 'https://openalex.org/W2963641970', 'https://openalex.org/W2964378242', 'https://openalex.org/W3148695041', 'https://openalex.org/W2963799213', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963532523', 'https://openalex.org/W3120243996', 'https://openalex.org/W3118474514', 'https://openalex.org/W2108501770', 'https://openalex.org/W2963082441', 'https://openalex.org/W3180355996', 'https://openalex.org/W3035564323', 'https://openalex.org/W2963755523', 'https://openalex.org/W2902630600', 'https://openalex.org/W2134842679', 'https://openalex.org/W3177150392', 'https://openalex.org/W3162926177', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963408680', 'https://openalex.org/W3121345697', 'https://openalex.org/W3140633421', 'https://openalex.org/W2963522047', 'https://openalex.org/W3106068426', 'https://openalex.org/W3082563516', 'https://openalex.org/W3122887982', 'https://openalex.org/W3119800657', 'https://openalex.org/W3034509000', 'https://openalex.org/W2962820504', 'https://openalex.org/W3176823897', 'https://openalex.org/W3118605064', 'https://openalex.org/W2971034910', 'https://openalex.org/W2963139417', 'https://openalex.org/W2962738009', 'https://openalex.org/W3007419529', 'https://openalex.org/W2963819570', 'https://openalex.org/W2962760235', 'https://openalex.org/W2970938715', 'https://openalex.org/W3132151890', 'https://openalex.org/W2963135265', 'https://openalex.org/W2962736171', 'https://openalex.org/W2963090522', 'https://openalex.org/W3041956526', 'https://openalex.org/W2964108670', 'https://openalex.org/W2963275229', 'https://openalex.org/W1505878979', 'https://openalex.org/W3118608800', 'https://openalex.org/W2587284713', 'https://openalex.org/W3169386841', 'https://openalex.org/W3182978476', 'https://openalex.org/W3171313410', 'https://openalex.org/W3098708719', 'https://openalex.org/W3170636531', 'https://openalex.org/W2970709315', 'https://openalex.org/W2963801305', 'https://openalex.org/W3114951884', 'https://openalex.org/W2963812505', 'https://openalex.org/W1991111872', 'https://openalex.org/W3087665158', 'https://openalex.org/W3214252557', 'https://openalex.org/W2913314773', 'https://openalex.org/W3194571797']",2021-06-10
https://openalex.org/W3086298921,https://doi.org/10.48550/arxiv.2009.05359,Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain,"The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.","['https://openalex.org/W3008716087', 'https://openalex.org/W2552737632', 'https://openalex.org/W2989815357', 'https://openalex.org/W3160026928', 'https://openalex.org/W3030163527', 'https://openalex.org/W3001063045', 'https://openalex.org/W3033108376', 'https://openalex.org/W3118210634', 'https://openalex.org/W3021164770', 'https://openalex.org/W2099471712', 'https://openalex.org/W2062967916', 'https://openalex.org/W2789634097', 'https://openalex.org/W2626778328', 'https://openalex.org/W2131352719', 'https://openalex.org/W2750384547', 'https://openalex.org/W3001279689', 'https://openalex.org/W2006814548', 'https://openalex.org/W2128123567', 'https://openalex.org/W2131045394', 'https://openalex.org/W2787849668', 'https://openalex.org/W2842511635', 'https://openalex.org/W2027968420', 'https://openalex.org/W2935958002', 'https://openalex.org/W2964122153', 'https://openalex.org/W2578935686', 'https://openalex.org/W2766447205', 'https://openalex.org/W2949382160', 'https://openalex.org/W2527798464', 'https://openalex.org/W2194775991', 'https://openalex.org/W2078529082', 'https://openalex.org/W2064675550', 'https://openalex.org/W2040036684', 'https://openalex.org/W2771247232', 'https://openalex.org/W3037315641', 'https://openalex.org/W3091211810', 'https://openalex.org/W2173520492', 'https://openalex.org/W2109059823', 'https://openalex.org/W2003357516', 'https://openalex.org/W1855112655', 'https://openalex.org/W2127389037', 'https://openalex.org/W3005798058', 'https://openalex.org/W2134664762', 'https://openalex.org/W2054341944', 'https://openalex.org/W2736601468', 'https://openalex.org/W2050233777', 'https://openalex.org/W2949304624', 'https://openalex.org/W2166206801', 'https://openalex.org/W2963440770', 'https://openalex.org/W2127958135', 'https://openalex.org/W1931823119', 'https://openalex.org/W2963830987', 'https://openalex.org/W2905333383', 'https://openalex.org/W2091163800', 'https://openalex.org/W2519766976', 'https://openalex.org/W2045908891', 'https://openalex.org/W2604086949', 'https://openalex.org/W2119170562', 'https://openalex.org/W2145404397', 'https://openalex.org/W3016391357', 'https://openalex.org/W2900152462', 'https://openalex.org/W2529004582', 'https://openalex.org/W2901943374', 'https://openalex.org/W2145339207', 'https://openalex.org/W2477844827', 'https://openalex.org/W2482290902', 'https://openalex.org/W2885097534', 'https://openalex.org/W2507556850']",2020-09-11
https://openalex.org/W3135058862,https://doi.org/10.48550/arxiv.2103.03841,Generating Images with Sparse Representations,"The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.","['https://openalex.org/W2267126114', 'https://openalex.org/W2970607325', 'https://openalex.org/W2133665775', 'https://openalex.org/W2079145130', 'https://openalex.org/W3001279689', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963981733', 'https://openalex.org/W1959608418', 'https://openalex.org/W2156886646', 'https://openalex.org/W3021164770', 'https://openalex.org/W2893749619', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963090522', 'https://openalex.org/W3034445277', 'https://openalex.org/W3100971012', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963799213', 'https://openalex.org/W2990440987', 'https://openalex.org/W2117539524', 'https://openalex.org/W3125056032', 'https://openalex.org/W3131922516', 'https://openalex.org/W2031614119', 'https://openalex.org/W2964121744', 'https://openalex.org/W2561715562', 'https://openalex.org/W967544008', 'https://openalex.org/W2140196014', 'https://openalex.org/W2962897886', 'https://openalex.org/W2962770929', 'https://openalex.org/W2902630600', 'https://openalex.org/W2994673210', 'https://openalex.org/W3035574324', 'https://openalex.org/W3010768098', 'https://openalex.org/W2962760235', 'https://openalex.org/W2971128425', 'https://openalex.org/W2963403868', 'https://openalex.org/W2940744433']",2021-03-05
https://openalex.org/W3173413685,,Transflower: probabilistic autoregressive dance generation with multimodal attention.,"Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.","['https://openalex.org/W2981852735', 'https://openalex.org/W3081605501', 'https://openalex.org/W2423557781', 'https://openalex.org/W3157199281', 'https://openalex.org/W3194843441', 'https://openalex.org/W3068510429', 'https://openalex.org/W2084866186', 'https://openalex.org/W2962974533', 'https://openalex.org/W2964243274', 'https://openalex.org/W2971856312', 'https://openalex.org/W3132487446', 'https://openalex.org/W3107914916', 'https://openalex.org/W2962770929', 'https://openalex.org/W2249800031', 'https://openalex.org/W2893749619', 'https://openalex.org/W3036601975', 'https://openalex.org/W2350911425', 'https://openalex.org/W3009042479', 'https://openalex.org/W2901285216', 'https://openalex.org/W2967443589', 'https://openalex.org/W3138041058', 'https://openalex.org/W2004167999', 'https://openalex.org/W2243558602', 'https://openalex.org/W3135287914', 'https://openalex.org/W2912298597', 'https://openalex.org/W3123650687', 'https://openalex.org/W2949382160', 'https://openalex.org/W2626778328', 'https://openalex.org/W3021164770', 'https://openalex.org/W3147968035', 'https://openalex.org/W2989787517', 'https://openalex.org/W2987870723', 'https://openalex.org/W1487977235', 'https://openalex.org/W2938535942', 'https://openalex.org/W2402172128', 'https://openalex.org/W3153832461', 'https://openalex.org/W2602100120', 'https://openalex.org/W2611706523', 'https://openalex.org/W3001279689', 'https://openalex.org/W2963300588', 'https://openalex.org/W2315431645', 'https://openalex.org/W2996287690', 'https://openalex.org/W2027247754', 'https://openalex.org/W2962916650', 'https://openalex.org/W3095645723', 'https://openalex.org/W3129576130', 'https://openalex.org/W2902968583', 'https://openalex.org/W3184510571', 'https://openalex.org/W3134144764', 'https://openalex.org/W3034445277', 'https://openalex.org/W3113184484', 'https://openalex.org/W3129503315', 'https://openalex.org/W1977259876', 'https://openalex.org/W2729615412', 'https://openalex.org/W3009385801', 'https://openalex.org/W3113891126', 'https://openalex.org/W2136625416', 'https://openalex.org/W2963389355', 'https://openalex.org/W2946729629', 'https://openalex.org/W3150807214', 'https://openalex.org/W2469134594', 'https://openalex.org/W2594167370', 'https://openalex.org/W2404298818', 'https://openalex.org/W3105160031', 'https://openalex.org/W2336236730', 'https://openalex.org/W2144481990', 'https://openalex.org/W2272039902', 'https://openalex.org/W3094502228', 'https://openalex.org/W2894766094', 'https://openalex.org/W3133405188', 'https://openalex.org/W3130319171', 'https://openalex.org/W3048550213', 'https://openalex.org/W3159578394', 'https://openalex.org/W3153551559', 'https://openalex.org/W2996375490', 'https://openalex.org/W2963799213', 'https://openalex.org/W2124609748', 'https://openalex.org/W2163410866', 'https://openalex.org/W3048539707', 'https://openalex.org/W2963139417', 'https://openalex.org/W3143728625', 'https://openalex.org/W3048665613', 'https://openalex.org/W2919624000', 'https://openalex.org/W2029903115', 'https://openalex.org/W2796290181', 'https://openalex.org/W3215052027', 'https://openalex.org/W3030163527']",2021-06-25
https://openalex.org/W3209501356,https://doi.org/10.48550/arxiv.2010.02917,A Contrastive Learning Approach for Training Variational Autoencoder Priors,"Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.","['https://openalex.org/W3034720584', 'https://openalex.org/W2897371726', 'https://openalex.org/W2963812505', 'https://openalex.org/W2788017640', 'https://openalex.org/W189596042', 'https://openalex.org/W2136922672', 'https://openalex.org/W2963408680', 'https://openalex.org/W2982553922', 'https://openalex.org/W3021164770', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963420686', 'https://openalex.org/W2579277680', 'https://openalex.org/W2906007967', 'https://openalex.org/W3034625979', 'https://openalex.org/W2962738009', 'https://openalex.org/W1964155876', 'https://openalex.org/W1522301498', 'https://openalex.org/W2949382160', 'https://openalex.org/W3034667906', 'https://openalex.org/W2587284713', 'https://openalex.org/W2108501770', 'https://openalex.org/W2981721547', 'https://openalex.org/W2767286248', 'https://openalex.org/W2962771722', 'https://openalex.org/W3121345697', 'https://openalex.org/W1909320841', 'https://openalex.org/W2963746531', 'https://openalex.org/W2963801305', 'https://openalex.org/W2152790380', 'https://openalex.org/W3118605064', 'https://openalex.org/W2970014727', 'https://openalex.org/W3036167779', 'https://openalex.org/W2963082441', 'https://openalex.org/W2988793532', 'https://openalex.org/W2965850471', 'https://openalex.org/W2267126114', 'https://openalex.org/W2970771798', 'https://openalex.org/W1834627138', 'https://openalex.org/W3035101153', 'https://openalex.org/W2766527293', 'https://openalex.org/W2962770929', 'https://openalex.org/W2994610548', 'https://openalex.org/W2963981733', 'https://openalex.org/W3035068567', 'https://openalex.org/W3106570356', 'https://openalex.org/W2951870209', 'https://openalex.org/W2970607325', 'https://openalex.org/W2979473621', 'https://openalex.org/W2116064496', 'https://openalex.org/W2519430864', 'https://openalex.org/W1959608418', 'https://openalex.org/W2518108298', 'https://openalex.org/W2963139417', 'https://openalex.org/W2949899814', 'https://openalex.org/W1710476689', 'https://openalex.org/W3106068426', 'https://openalex.org/W2947490203', 'https://openalex.org/W2965095304', 'https://openalex.org/W2989574682', 'https://openalex.org/W2963135265', 'https://openalex.org/W3035166812', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963147844', 'https://openalex.org/W2949649223', 'https://openalex.org/W1836465849', 'https://openalex.org/W3098510582', 'https://openalex.org/W3035564323', 'https://openalex.org/W2893749619', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962820504', 'https://openalex.org/W3034310115', 'https://openalex.org/W3118608800', 'https://openalex.org/W2970554285', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963686907']",2020-10-06
https://openalex.org/W3180777572,https://doi.org/10.48550/arxiv.2107.06252,Dance2Music: Automatic Dance-driven Music Generation,"Dance and music typically go hand in hand. The complexities in dance, music, and their synchronisation make them fascinating to study from a computational creativity perspective. While several works have looked at generating dance for a given music, automatically generating music for a given dance remains under-explored. This capability could have several creative expression and entertainment applications. We present some early explorations in this direction. We present a search-based offline approach that generates music after processing the entire dance video and an online approach that uses a deep neural network to generate music on-the-fly as the video proceeds. We compare these approaches to a strong heuristic baseline via human studies and present our findings. We have integrated our online approach in a live demo! A video of the demo can be found here: https://sites.google.com/view/dance2music/live-demo.","['https://openalex.org/W3045082460', 'https://openalex.org/W2989787517', 'https://openalex.org/W3011635019', 'https://openalex.org/W3046757598', 'https://openalex.org/W2962730651', 'https://openalex.org/W2987870723', 'https://openalex.org/W2786532020', 'https://openalex.org/W3000528734', 'https://openalex.org/W3035911596', 'https://openalex.org/W2080489767', 'https://openalex.org/W3021164770', 'https://openalex.org/W3035486405', 'https://openalex.org/W2170550215', 'https://openalex.org/W2055494099', 'https://openalex.org/W2984529706']",2021-07-13
https://openalex.org/W3091165558,https://doi.org/10.48550/arxiv.2010.00654,VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models,"Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. The source code is available at https://github.com/NVlabs/VAEBM","['https://openalex.org/W2979776030', 'https://openalex.org/W2804123849', 'https://openalex.org/W2962820504', 'https://openalex.org/W189596042', 'https://openalex.org/W2963981733', 'https://openalex.org/W2587284713', 'https://openalex.org/W2099866409', 'https://openalex.org/W967544008', 'https://openalex.org/W3104876213', 'https://openalex.org/W2152790380', 'https://openalex.org/W2962793481', 'https://openalex.org/W2979652999', 'https://openalex.org/W44815768', 'https://openalex.org/W3102021454', 'https://openalex.org/W3035068567', 'https://openalex.org/W2949382160', 'https://openalex.org/W2161914416', 'https://openalex.org/W1959608418', 'https://openalex.org/W3035166812', 'https://openalex.org/W1513873506', 'https://openalex.org/W2971034910', 'https://openalex.org/W3035101153', 'https://openalex.org/W2972246420', 'https://openalex.org/W2963812505', 'https://openalex.org/W3034625979', 'https://openalex.org/W3035564323', 'https://openalex.org/W2963408680', 'https://openalex.org/W3098510582', 'https://openalex.org/W2945924057', 'https://openalex.org/W2963373786', 'https://openalex.org/W3043462782', 'https://openalex.org/W2963139417', 'https://openalex.org/W3014384188', 'https://openalex.org/W2963142510', 'https://openalex.org/W3035574324', 'https://openalex.org/W2980585949', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963809789', 'https://openalex.org/W2970607325', 'https://openalex.org/W3094191864', 'https://openalex.org/W2013035813', 'https://openalex.org/W2521028896', 'https://openalex.org/W1579122361', 'https://openalex.org/W3035281230', 'https://openalex.org/W2963685250', 'https://openalex.org/W2986615800', 'https://openalex.org/W3034310115', 'https://openalex.org/W2893749619', 'https://openalex.org/W3034459047', 'https://openalex.org/W195465510', 'https://openalex.org/W3035384201', 'https://openalex.org/W2409550820', 'https://openalex.org/W2938608316', 'https://openalex.org/W2593768305', 'https://openalex.org/W2272361785', 'https://openalex.org/W3106068426', 'https://openalex.org/W1834627138', 'https://openalex.org/W2970181183', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963546708', 'https://openalex.org/W2963836885', 'https://openalex.org/W2962947361', 'https://openalex.org/W2962770929', 'https://openalex.org/W2108501770', 'https://openalex.org/W2962760235', 'https://openalex.org/W3028907532', 'https://openalex.org/W2963693742', 'https://openalex.org/W2116064496', 'https://openalex.org/W2962897886', 'https://openalex.org/W2981721547', 'https://openalex.org/W2970771798', 'https://openalex.org/W3107669106', 'https://openalex.org/W2979473621', 'https://openalex.org/W2995085126', 'https://openalex.org/W2099471712', 'https://openalex.org/W2782980316', 'https://openalex.org/W3012274818', 'https://openalex.org/W3128546090', 'https://openalex.org/W2963090522', 'https://openalex.org/W2971074500', 'https://openalex.org/W2920868047', 'https://openalex.org/W2963275229', 'https://openalex.org/W3036167779', 'https://openalex.org/W2963085620']",2020-10-01
https://openalex.org/W3110624927,https://doi.org/10.48550/arxiv.2011.12596,MTCRNN: A multi-scale RNN for directed audio texture synthesis,"Audio textures are a subset of environmental sounds, often defined as having stable statistical characteristics within an adequately large window of time but may be unstructured locally. They include common everyday sounds such as from rain, wind, and engines. Given that these complex sounds contain patterns on multiple timescales, they are a challenge to model with traditional methods. We introduce a novel modelling approach for textures, combining recurrent neural networks trained at different levels of abstraction with a conditioning strategy that allows for user-directed synthesis. We demonstrate the model's performance on a variety of datasets, examine its performance on various metrics, and discuss some potential applications.","['https://openalex.org/W2950547518', 'https://openalex.org/W2905488776', 'https://openalex.org/W2962942158', 'https://openalex.org/W3021164770', 'https://openalex.org/W2953331651', 'https://openalex.org/W2963889406', 'https://openalex.org/W1579853615', 'https://openalex.org/W2924273262', 'https://openalex.org/W2949382160', 'https://openalex.org/W2951986497', 'https://openalex.org/W2512517145']",2020-11-25
https://openalex.org/W3163120468,https://doi.org/10.48550/arxiv.2105.03928,Which transformer architecture fits my data? A vocabulary bottleneck in self-attention,"After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\%-50\%$ in leading NLP models such as ALBERT and T5.","['https://openalex.org/W2962716426', 'https://openalex.org/W2611378001', 'https://openalex.org/W3099782249', 'https://openalex.org/W2949867299', 'https://openalex.org/W2994760783', 'https://openalex.org/W3169483174', 'https://openalex.org/W2963537482', 'https://openalex.org/W2963005248', 'https://openalex.org/W3130893544', 'https://openalex.org/W2963044468', 'https://openalex.org/W3035691519', 'https://openalex.org/W1984113816', 'https://openalex.org/W3001279689', 'https://openalex.org/W2935584311', 'https://openalex.org/W3026674654', 'https://openalex.org/W3034708491', 'https://openalex.org/W3119786062', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035563045', 'https://openalex.org/W2912516940', 'https://openalex.org/W3095645723', 'https://openalex.org/W2982316857', 'https://openalex.org/W3164045210', 'https://openalex.org/W3098903812', 'https://openalex.org/W2963347649', 'https://openalex.org/W1980428773', 'https://openalex.org/W2121879602', 'https://openalex.org/W2934842096', 'https://openalex.org/W2006891733', 'https://openalex.org/W2963652331', 'https://openalex.org/W3034445277', 'https://openalex.org/W2626778328', 'https://openalex.org/W2995446988', 'https://openalex.org/W2940744433', 'https://openalex.org/W3082274269', 'https://openalex.org/W3169291081', 'https://openalex.org/W2970565456', 'https://openalex.org/W2962784628', 'https://openalex.org/W3021164770', 'https://openalex.org/W3114951884', 'https://openalex.org/W2996428491']",2021-05-09
https://openalex.org/W3171895902,https://doi.org/10.48550/arxiv.2106.05258,Generative Models as a Data Source for Multiview Representation Learning,"Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple ""views"" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more ""model zoos"" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.","['https://openalex.org/W3100345210', 'https://openalex.org/W2101032778', 'https://openalex.org/W2893749619', 'https://openalex.org/W3101254953', 'https://openalex.org/W3020933450', 'https://openalex.org/W3034693603', 'https://openalex.org/W3091633069', 'https://openalex.org/W2063971957', 'https://openalex.org/W3034445277', 'https://openalex.org/W299440670', 'https://openalex.org/W2970241862', 'https://openalex.org/W2951004968', 'https://openalex.org/W2804935296', 'https://openalex.org/W3034851704', 'https://openalex.org/W2978481653', 'https://openalex.org/W2173520492', 'https://openalex.org/W2943466823', 'https://openalex.org/W2963636093', 'https://openalex.org/W3021164770', 'https://openalex.org/W2944828972', 'https://openalex.org/W2326925005', 'https://openalex.org/W2962770929', 'https://openalex.org/W3159042725', 'https://openalex.org/W3035524453', 'https://openalex.org/W117096852', 'https://openalex.org/W3108989460', 'https://openalex.org/W2842511635', 'https://openalex.org/W3129576130', 'https://openalex.org/W2995801453', 'https://openalex.org/W3129910129', 'https://openalex.org/W3100388886', 'https://openalex.org/W2971337561', 'https://openalex.org/W2951309005', 'https://openalex.org/W3010094231', 'https://openalex.org/W2963709863', 'https://openalex.org/W2963265008', 'https://openalex.org/W2613718673', 'https://openalex.org/W2530741948', 'https://openalex.org/W3005680577', 'https://openalex.org/W2576289912', 'https://openalex.org/W3120167236', 'https://openalex.org/W3116569640', 'https://openalex.org/W2963052201', 'https://openalex.org/W3026092005', 'https://openalex.org/W2798991696', 'https://openalex.org/W3113143462', 'https://openalex.org/W3030163527', 'https://openalex.org/W3092869576', 'https://openalex.org/W3107537120', 'https://openalex.org/W1589591577', 'https://openalex.org/W3011199201', 'https://openalex.org/W3101694814', 'https://openalex.org/W2949517790', 'https://openalex.org/W2411541852', 'https://openalex.org/W2962808524', 'https://openalex.org/W2963341956', 'https://openalex.org/W2108598243', 'https://openalex.org/W3012949032', 'https://openalex.org/W3154192710', 'https://openalex.org/W2431874326', 'https://openalex.org/W2138621090', 'https://openalex.org/W2031489346', 'https://openalex.org/W3034431451', 'https://openalex.org/W2962742544', 'https://openalex.org/W3003162010', 'https://openalex.org/W3135500843', 'https://openalex.org/W3103102495', 'https://openalex.org/W2152926413']",2021-06-09
https://openalex.org/W3206916870,https://doi.org/10.48550/arxiv.2110.10139,Chunked Autoregressive GAN for Conditional Waveform Synthesis,"Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.","['https://openalex.org/W2086381917', 'https://openalex.org/W3135287914', 'https://openalex.org/W3001377302', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963300588', 'https://openalex.org/W2962866891', 'https://openalex.org/W2971074500', 'https://openalex.org/W2120847449', 'https://openalex.org/W3020570669', 'https://openalex.org/W2767785892', 'https://openalex.org/W2091425152', 'https://openalex.org/W2964307104', 'https://openalex.org/W3035068567', 'https://openalex.org/W2975414524', 'https://openalex.org/W1993950587', 'https://openalex.org/W2785678896', 'https://openalex.org/W3021164770', 'https://openalex.org/W1583912456', 'https://openalex.org/W2888916623', 'https://openalex.org/W2527729766', 'https://openalex.org/W2953331651', 'https://openalex.org/W2952344559', 'https://openalex.org/W3047796745', 'https://openalex.org/W2963139417', 'https://openalex.org/W2950547518', 'https://openalex.org/W3092028330', 'https://openalex.org/W3123097577', 'https://openalex.org/W2940744433', 'https://openalex.org/W3112587064', 'https://openalex.org/W2964167449', 'https://openalex.org/W2959300817', 'https://openalex.org/W3036167779', 'https://openalex.org/W2950635152', 'https://openalex.org/W3169905056', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963981733', 'https://openalex.org/W2471520273', 'https://openalex.org/W3034794073', 'https://openalex.org/W2949382160', 'https://openalex.org/W1505878979', 'https://openalex.org/W2950541952', 'https://openalex.org/W2069501481', 'https://openalex.org/W3033913438', 'https://openalex.org/W2980709326']",2021-10-19
https://openalex.org/W3119914886,https://doi.org/10.48550/arxiv.2101.04785,"MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN","We present a deep convolutional GAN which leverages techniques from MP3/Vorbis audio compression to produce long, high-quality audio samples with long-range coherence. The model uses a Modified Discrete Cosine Transform (MDCT) data representation, which includes all phase information. Phase generation is hence integral part of the model. We leverage the auditory masking and psychoacoustic perception limit of the human ear to widen the true distribution and stabilize the training process. The model architecture is a deep 2D convolutional network, where each subsequent generator model block increases the resolution along the time axis and adds a higher octave along the frequency axis. The deeper layers are connected with all parts of the output and have the context of the full track. This enables generation of samples which exhibit long-range coherence. We use MP3net to create 95s stereo tracks with a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An additional benefit of the CNN-based model architecture is that generation of new songs is almost instantaneous.","['https://openalex.org/W2962770929', 'https://openalex.org/W2953030256', 'https://openalex.org/W2910577860', 'https://openalex.org/W2535388113', 'https://openalex.org/W2893749619', 'https://openalex.org/W1916685473', 'https://openalex.org/W2948211236', 'https://openalex.org/W2962914733', 'https://openalex.org/W3215191595', 'https://openalex.org/W2894295011', 'https://openalex.org/W2120847449', 'https://openalex.org/W2803968356', 'https://openalex.org/W2963575853', 'https://openalex.org/W2808631503', 'https://openalex.org/W2965141076', 'https://openalex.org/W2617573776', 'https://openalex.org/W2962760235', 'https://openalex.org/W2917245127', 'https://openalex.org/W2962879692', 'https://openalex.org/W2963681776', 'https://openalex.org/W2898148140', 'https://openalex.org/W2964201867', 'https://openalex.org/W1710476689', 'https://openalex.org/W2096588881', 'https://openalex.org/W2804078698', 'https://openalex.org/W2982039329', 'https://openalex.org/W1502560247', 'https://openalex.org/W3021164770', 'https://openalex.org/W2004453603', 'https://openalex.org/W2949382160']",2021-01-12
https://openalex.org/W3171443854,https://doi.org/10.48550/arxiv.2106.06426,Catch-A-Waveform: Learning to Generate Audio from a Single Short Example,"Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Specifically, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modifications to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), filling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in all cases, no more than 20 seconds of training audio commonly suffice for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general.","['https://openalex.org/W3102195007', 'https://openalex.org/W2963276790', 'https://openalex.org/W3124359805', 'https://openalex.org/W2963975282', 'https://openalex.org/W2894295011', 'https://openalex.org/W2962879692', 'https://openalex.org/W2970844204', 'https://openalex.org/W1989337816', 'https://openalex.org/W3024973272', 'https://openalex.org/W3177361240', 'https://openalex.org/W2982041717', 'https://openalex.org/W3006440693', 'https://openalex.org/W3022195800', 'https://openalex.org/W3098304089', 'https://openalex.org/W2888169323', 'https://openalex.org/W2995254904', 'https://openalex.org/W2996158613', 'https://openalex.org/W2963300588', 'https://openalex.org/W2963341071', 'https://openalex.org/W2996286887', 'https://openalex.org/W2984106626', 'https://openalex.org/W2962882868', 'https://openalex.org/W2962981281', 'https://openalex.org/W3012382391', 'https://openalex.org/W2339754110', 'https://openalex.org/W2995233853', 'https://openalex.org/W2771705696', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964121744', 'https://openalex.org/W3016027511', 'https://openalex.org/W2963237661', 'https://openalex.org/W2963840672', 'https://openalex.org/W2989894932', 'https://openalex.org/W2944438111', 'https://openalex.org/W2133824856', 'https://openalex.org/W2963073614', 'https://openalex.org/W3109050852', 'https://openalex.org/W2584032004', 'https://openalex.org/W2739748921', 'https://openalex.org/W2423557781', 'https://openalex.org/W2161304220', 'https://openalex.org/W2998639482', 'https://openalex.org/W3036344793', 'https://openalex.org/W2963192573', 'https://openalex.org/W2963782041', 'https://openalex.org/W3021164770', 'https://openalex.org/W2284050935', 'https://openalex.org/W3035068567']",2021-06-11
https://openalex.org/W3192307151,https://doi.org/10.48550/arxiv.2108.00443,A Survey on Audio Synthesis and Audio-Visual Multimodal Processing,"With the development of deep learning and artificial intelligence, audio synthesis has a pivotal role in the area of machine learning and shows strong applicability in the industry. Meanwhile, significant efforts have been dedicated by researchers to handle multimodal tasks at present such as audio-visual multimodal processing. In this paper, we conduct a survey on audio synthesis and audio-visual multimodal processing, which helps understand current research and future trends. This review focuses on text to speech(TTS), music generation and some tasks that combine visual and acoustic information. The corresponding technical methods are comprehensively classified and introduced, and their future development trends are prospected. This survey can provide some guidance for researchers who are interested in the areas like audio synthesis and audio-visual multimodal processing.","['https://openalex.org/W3006233221', 'https://openalex.org/W3005841600', 'https://openalex.org/W3025528898', 'https://openalex.org/W2894964039', 'https://openalex.org/W3000932375', 'https://openalex.org/W1556219185', 'https://openalex.org/W2746068898', 'https://openalex.org/W2963082324', 'https://openalex.org/W2980709326', 'https://openalex.org/W2726515241', 'https://openalex.org/W3137160206', 'https://openalex.org/W2963807156', 'https://openalex.org/W2797090057', 'https://openalex.org/W2964638774', 'https://openalex.org/W2894295011', 'https://openalex.org/W3091357435', 'https://openalex.org/W3015282541', 'https://openalex.org/W3004910693', 'https://openalex.org/W1710476689', 'https://openalex.org/W3174758275', 'https://openalex.org/W2950547518', 'https://openalex.org/W2963091184', 'https://openalex.org/W3098403858', 'https://openalex.org/W3044996352', 'https://openalex.org/W3016916515', 'https://openalex.org/W2587284713', 'https://openalex.org/W2953100410', 'https://openalex.org/W2971074500', 'https://openalex.org/W2997321404', 'https://openalex.org/W3035665982', 'https://openalex.org/W2471520273', 'https://openalex.org/W3155036195', 'https://openalex.org/W3033411150', 'https://openalex.org/W3021164770', 'https://openalex.org/W3148652073', 'https://openalex.org/W2970840374', 'https://openalex.org/W2915476573', 'https://openalex.org/W2796931171', 'https://openalex.org/W2951004968', 'https://openalex.org/W2941198294', 'https://openalex.org/W2964281804', 'https://openalex.org/W2899882692', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963032576', 'https://openalex.org/W2953331651', 'https://openalex.org/W3101611481', 'https://openalex.org/W3090121172', 'https://openalex.org/W2963403868', 'https://openalex.org/W2585824449', 'https://openalex.org/W3046890131', 'https://openalex.org/W3124306134', 'https://openalex.org/W2903739847', 'https://openalex.org/W3120774255', 'https://openalex.org/W2029199293', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963290645', 'https://openalex.org/W2963019222', 'https://openalex.org/W2952746495', 'https://openalex.org/W2946200149', 'https://openalex.org/W2944294033', 'https://openalex.org/W2594690981', 'https://openalex.org/W3113587569', 'https://openalex.org/W2944488554', 'https://openalex.org/W3025498998', 'https://openalex.org/W2992790584', 'https://openalex.org/W2996286887', 'https://openalex.org/W2798861951', 'https://openalex.org/W2901997113', 'https://openalex.org/W2613087992', 'https://openalex.org/W2907183510', 'https://openalex.org/W2475687244', 'https://openalex.org/W2951418500', 'https://openalex.org/W2963782041', 'https://openalex.org/W3169905056', 'https://openalex.org/W2173520492', 'https://openalex.org/W2184335310', 'https://openalex.org/W2971230840', 'https://openalex.org/W2963575853', 'https://openalex.org/W2015143272', 'https://openalex.org/W3172355142', 'https://openalex.org/W3023463084', 'https://openalex.org/W3034896527', 'https://openalex.org/W2949382160', 'https://openalex.org/W2578229578', 'https://openalex.org/W2963681776', 'https://openalex.org/W3024147341', 'https://openalex.org/W2559110679', 'https://openalex.org/W2064675550', 'https://openalex.org/W2953867371', 'https://openalex.org/W2948211236', 'https://openalex.org/W3035068567', 'https://openalex.org/W1821462560', 'https://openalex.org/W2998572311', 'https://openalex.org/W3114386489', 'https://openalex.org/W3026874504', 'https://openalex.org/W2883853252', 'https://openalex.org/W3123318516', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963300588', 'https://openalex.org/W2527729766', 'https://openalex.org/W3117494933', 'https://openalex.org/W2766812927', 'https://openalex.org/W3049360585', 'https://openalex.org/W2963081548']",2021-08-01
https://openalex.org/W3041199652,https://doi.org/10.48550/arxiv.2007.04590,DeepSinger: Singing Voice Synthesis with Data Mined From the Web,"In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in https://speechresearch.github.io/deepsinger/.)","['https://openalex.org/W2963403868', 'https://openalex.org/W2984106626', 'https://openalex.org/W2950161879', 'https://openalex.org/W2584505851', 'https://openalex.org/W2515336442', 'https://openalex.org/W2902351815', 'https://openalex.org/W3033411150', 'https://openalex.org/W2972359262', 'https://openalex.org/W2964243274', 'https://openalex.org/W2964307104', 'https://openalex.org/W2598638573', 'https://openalex.org/W2736648940', 'https://openalex.org/W2747874407', 'https://openalex.org/W2482558056', 'https://openalex.org/W2108862644', 'https://openalex.org/W2767052532', 'https://openalex.org/W2940405045', 'https://openalex.org/W2134387846', 'https://openalex.org/W3021164770', 'https://openalex.org/W2284628133', 'https://openalex.org/W2778460379', 'https://openalex.org/W3035430139', 'https://openalex.org/W182406043', 'https://openalex.org/W1875231349', 'https://openalex.org/W2949382160', 'https://openalex.org/W2973046048', 'https://openalex.org/W2150658333', 'https://openalex.org/W1522301498', 'https://openalex.org/W2955316069', 'https://openalex.org/W2007815473', 'https://openalex.org/W2970597249', 'https://openalex.org/W2995670387', 'https://openalex.org/W2935701729', 'https://openalex.org/W2937242376', 'https://openalex.org/W2946200149', 'https://openalex.org/W1904711963', 'https://openalex.org/W2962936105', 'https://openalex.org/W2743200750']",2020-07-09
https://openalex.org/W3196114627,https://doi.org/10.48550/arxiv.2108.11213,AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer,"Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.","['https://openalex.org/W3049637270', 'https://openalex.org/W3092850823', 'https://openalex.org/W3049174246', 'https://openalex.org/W3021164770', 'https://openalex.org/W2884558435', 'https://openalex.org/W2773479825', 'https://openalex.org/W2809621972', 'https://openalex.org/W3005441616', 'https://openalex.org/W2793183272', 'https://openalex.org/W3093276169', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964121744', 'https://openalex.org/W3049247973', 'https://openalex.org/W2746068898', 'https://openalex.org/W2561662441', 'https://openalex.org/W2142996485', 'https://openalex.org/W2021142183', 'https://openalex.org/W2400684038', 'https://openalex.org/W2797595207', 'https://openalex.org/W2142384583', 'https://openalex.org/W1966910303', 'https://openalex.org/W2948574791', 'https://openalex.org/W2963032576', 'https://openalex.org/W2591553092', 'https://openalex.org/W1993297272', 'https://openalex.org/W2399081453', 'https://openalex.org/W2963681776', 'https://openalex.org/W2978885571', 'https://openalex.org/W1665214252']",2021-08-25
https://openalex.org/W3209092256,https://doi.org/10.48550/arxiv.2101.08919,Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks,"As users increasingly rely on cloud-based computing services, it is important to ensure that uploaded speech data remains private. Existing solutions rely either on server-side methods or focus on hiding speaker identity. While these approaches reduce certain security concerns, they do not give users client-side control over whether their biometric information is sent to the server. In this paper, we formally define client-side privacy and discuss its three unique technical challenges: (1) direct manipulation of raw data on client devices, (2) adaptability with a broad range of server-side processing models, and (3) low time and space complexity for compatibility with limited-bandwidth devices. Solving these challenges requires new models that achieve high-fidelity reconstruction, privacy preservation of sensitive personal attributes, and efficiency during training and inference. As a step towards client-side privacy for speech recognition, we investigate three techniques spanning signal processing, disentangled representation learning, and adversarial training. Through a series of gender and accent masking tasks, we observe that each method has its unique strengths, but none manage to effectively balance the trade-offs between performance, privacy, and complexity. These insights call for more research in client-side privacy to ensure a safer deployment of cloud-based speech processing services.","['https://openalex.org/W2950888501', 'https://openalex.org/W3092368332', 'https://openalex.org/W2896422817', 'https://openalex.org/W3092178912', 'https://openalex.org/W1966092470', 'https://openalex.org/W1967689022', 'https://openalex.org/W2124221655', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963207607', 'https://openalex.org/W2777914285', 'https://openalex.org/W2951082691', 'https://openalex.org/W3119841746', 'https://openalex.org/W2791827614', 'https://openalex.org/W3103802018', 'https://openalex.org/W3156977108', 'https://openalex.org/W3098439673', 'https://openalex.org/W2128977689', 'https://openalex.org/W2527729766', 'https://openalex.org/W3109527089', 'https://openalex.org/W2122332734', 'https://openalex.org/W3043999252', 'https://openalex.org/W2807692250', 'https://openalex.org/W2963374540', 'https://openalex.org/W3111653141', 'https://openalex.org/W1568514080', 'https://openalex.org/W3163573274', 'https://openalex.org/W2903652364', 'https://openalex.org/W2726515241', 'https://openalex.org/W2293829681', 'https://openalex.org/W3034515139', 'https://openalex.org/W2121727866', 'https://openalex.org/W2099471712', 'https://openalex.org/W1993152556', 'https://openalex.org/W3100779497', 'https://openalex.org/W2120847449', 'https://openalex.org/W3099782249', 'https://openalex.org/W2896479683', 'https://openalex.org/W2318363391', 'https://openalex.org/W2962835968', 'https://openalex.org/W2535690855', 'https://openalex.org/W2473418344', 'https://openalex.org/W2903538854', 'https://openalex.org/W1886794424', 'https://openalex.org/W1965752326', 'https://openalex.org/W2584455886', 'https://openalex.org/W3021164770', 'https://openalex.org/W3095232705', 'https://openalex.org/W2519091744', 'https://openalex.org/W3018464563', 'https://openalex.org/W2972608701', 'https://openalex.org/W2996974038', 'https://openalex.org/W3045412325', 'https://openalex.org/W2951246258', 'https://openalex.org/W2035985557', 'https://openalex.org/W3098557217', 'https://openalex.org/W2962780374', 'https://openalex.org/W3162893999', 'https://openalex.org/W1982697667', 'https://openalex.org/W2972659941', 'https://openalex.org/W3082522567', 'https://openalex.org/W2104779640', 'https://openalex.org/W2753738274', 'https://openalex.org/W2502312327', 'https://openalex.org/W2603777577', 'https://openalex.org/W1494198834', 'https://openalex.org/W3110801570', 'https://openalex.org/W2902113386']",2021-01-22
https://openalex.org/W3209411894,https://doi.org/10.48550/arxiv.2111.03017,MT3: Multi-Task Multitrack Music Transcription,"Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information. Further, many AMT datasets are ""low-resource"", as even expert musicians find music transcription difficult and time-consuming. Thus, prior work has focused on task-specific architectures, tailored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT.","['https://openalex.org/W3021164770', 'https://openalex.org/W2963341956', 'https://openalex.org/W2475687244', 'https://openalex.org/W3134881075', 'https://openalex.org/W3163652268', 'https://openalex.org/W3174290991', 'https://openalex.org/W3122606659', 'https://openalex.org/W2296149406', 'https://openalex.org/W2939988664', 'https://openalex.org/W3101943858', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015542311', 'https://openalex.org/W2766393356', 'https://openalex.org/W2118774185', 'https://openalex.org/W2938774173', 'https://openalex.org/W3014809999', 'https://openalex.org/W3025165719', 'https://openalex.org/W2559688696', 'https://openalex.org/W3177067699', 'https://openalex.org/W3090619461', 'https://openalex.org/W3179260535', 'https://openalex.org/W3015213533', 'https://openalex.org/W3158129762', 'https://openalex.org/W2950547518', 'https://openalex.org/W1494198834', 'https://openalex.org/W3185739472', 'https://openalex.org/W3030163527', 'https://openalex.org/W2998490864', 'https://openalex.org/W2144414181', 'https://openalex.org/W2902076983', 'https://openalex.org/W2981852735', 'https://openalex.org/W3180663620', 'https://openalex.org/W3187921808', 'https://openalex.org/W2152937398', 'https://openalex.org/W2950335938', 'https://openalex.org/W3146639881', 'https://openalex.org/W3185760609', 'https://openalex.org/W3093517588', 'https://openalex.org/W2963403868']",2021-11-04
https://openalex.org/W3025878903,https://doi.org/10.48550/arxiv.2005.07884,Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction,"Vector Quantized Variational AutoEncoders (VQ-VAE) are a powerful representation learning framework that can discover discrete groups of features from a speech signal without supervision. Until now, the VQ-VAE architecture has previously modeled individual types of speech features, such as only phones or only F0. This paper introduces an important extension to VQ-VAE for learning F0-related suprasegmental information simultaneously along with traditional phone features.The proposed framework uses two encoders such that the F0 trajectory and speech waveform are both input to the system, therefore two separate codebooks are learned. We used a WaveRNN vocoder as the decoder component of VQ-VAE. Our speaker-independent VQ-VAE was trained with raw speech waveforms from multi-speaker Japanese speech databases. Experimental results show that the proposed extension reduces F0 distortion of reconstructed speech for all unseen test speakers, and results in significantly higher preference scores from a listening test. We additionally conducted experiments using single-speaker Mandarin speech to demonstrate advantages of our architecture in another language which relies heavily on F0.","['https://openalex.org/W2951873722', 'https://openalex.org/W2884607399', 'https://openalex.org/W2047203463', 'https://openalex.org/W2935711438', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963371159', 'https://openalex.org/W2972849140', 'https://openalex.org/W2963799213', 'https://openalex.org/W2788851830', 'https://openalex.org/W2982602185', 'https://openalex.org/W1901129140', 'https://openalex.org/W2947591107', 'https://openalex.org/W2921306346', 'https://openalex.org/W3020570669', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973224100', 'https://openalex.org/W3021164770', 'https://openalex.org/W2191779130', 'https://openalex.org/W1522301498', 'https://openalex.org/W2962866891', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963425185', 'https://openalex.org/W2969049672']",2020-05-16
https://openalex.org/W3160695487,https://doi.org/10.48550/arxiv.2105.08164,Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics,"This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.","['https://openalex.org/W3021164770', 'https://openalex.org/W2138243089', 'https://openalex.org/W2950788055', 'https://openalex.org/W2990594533', 'https://openalex.org/W3004714107', 'https://openalex.org/W3035369987', 'https://openalex.org/W2963420272', 'https://openalex.org/W2963782041', 'https://openalex.org/W2964122153', 'https://openalex.org/W3121345697', 'https://openalex.org/W2964058413', 'https://openalex.org/W3035586199', 'https://openalex.org/W3097649562', 'https://openalex.org/W2058676219', 'https://openalex.org/W2173213060', 'https://openalex.org/W2970006822', 'https://openalex.org/W3105013723', 'https://openalex.org/W2135181320', 'https://openalex.org/W2503750833', 'https://openalex.org/W3030163527', 'https://openalex.org/W3010316883', 'https://openalex.org/W2990504376', 'https://openalex.org/W2971034910', 'https://openalex.org/W2604885021', 'https://openalex.org/W3118608800', 'https://openalex.org/W2267126114', 'https://openalex.org/W2963575853', 'https://openalex.org/W3172257043', 'https://openalex.org/W3129651364', 'https://openalex.org/W2963975282', 'https://openalex.org/W2971074500', 'https://openalex.org/W2296153915', 'https://openalex.org/W2964101377', 'https://openalex.org/W2123597716', 'https://openalex.org/W2952218014', 'https://openalex.org/W2118297240', 'https://openalex.org/W2894295011', 'https://openalex.org/W3123097577', 'https://openalex.org/W2940744433', 'https://openalex.org/W2951781666', 'https://openalex.org/W2595294663', 'https://openalex.org/W2949382160', 'https://openalex.org/W2996287690', 'https://openalex.org/W2962882868', 'https://openalex.org/W2963186101', 'https://openalex.org/W2902662422', 'https://openalex.org/W2478027467', 'https://openalex.org/W3035068567', 'https://openalex.org/W2929274168', 'https://openalex.org/W2167433878', 'https://openalex.org/W2963300588', 'https://openalex.org/W2584032004']",2021-05-17
https://openalex.org/W3201970400,https://doi.org/10.48550/arxiv.2110.02891,Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models,"Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algorithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.","['https://openalex.org/W3016160783', 'https://openalex.org/W2902070858', 'https://openalex.org/W1810943226', 'https://openalex.org/W3034909238', 'https://openalex.org/W2168254078', 'https://openalex.org/W2962972936', 'https://openalex.org/W3082130377', 'https://openalex.org/W2972359262', 'https://openalex.org/W3109645351', 'https://openalex.org/W2964121744', 'https://openalex.org/W2949281321', 'https://openalex.org/W2890964092', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963539064', 'https://openalex.org/W3021164770', 'https://openalex.org/W2963801463', 'https://openalex.org/W2963073614', 'https://openalex.org/W3101254953', 'https://openalex.org/W2787887017', 'https://openalex.org/W592244745', 'https://openalex.org/W2969985801', 'https://openalex.org/W3024920698', 'https://openalex.org/W2808631503', 'https://openalex.org/W2903141607', 'https://openalex.org/W2911424785', 'https://openalex.org/W3098304089', 'https://openalex.org/W3035446294', 'https://openalex.org/W3126649386', 'https://openalex.org/W2588445447', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963712897', 'https://openalex.org/W2963300588', 'https://openalex.org/W3015645837', 'https://openalex.org/W2962752582', 'https://openalex.org/W2112606109', 'https://openalex.org/W3092947864', 'https://openalex.org/W3015212100', 'https://openalex.org/W2339254090', 'https://openalex.org/W2972667718', 'https://openalex.org/W2963920537', 'https://openalex.org/W3034794073', 'https://openalex.org/W2962793481', 'https://openalex.org/W3035574324', 'https://openalex.org/W3108367042', 'https://openalex.org/W3128910262', 'https://openalex.org/W2963272440', 'https://openalex.org/W2963403868', 'https://openalex.org/W2913314773', 'https://openalex.org/W2937579788', 'https://openalex.org/W3033913438', 'https://openalex.org/W1990457366', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963691546', 'https://openalex.org/W2784512264', 'https://openalex.org/W3122089380', 'https://openalex.org/W3139980562', 'https://openalex.org/W3003162010', 'https://openalex.org/W2963568578', 'https://openalex.org/W1959608418', 'https://openalex.org/W3041816239', 'https://openalex.org/W3034431451', 'https://openalex.org/W3034776267', 'https://openalex.org/W3020570669', 'https://openalex.org/W3113687514', 'https://openalex.org/W3033411150', 'https://openalex.org/W2893749619', 'https://openalex.org/W3035653890', 'https://openalex.org/W2964243274', 'https://openalex.org/W2985832137']",2021-10-06
https://openalex.org/W3028988798,https://doi.org/10.48550/arxiv.2005.13835,Speech-to-Singing Conversion based on Boundary Equilibrium GAN,"This paper investigates the use of generative adversarial network (GAN)-based models for converting the spectrogram of a speech signal into that of a singing one, without reference to the phoneme sequence underlying the speech. This is achieved by viewing speech-to-singing conversion as a style transfer problem. Specifically, given a speech input, and optionally the F0 contour of the target singing, the proposed model generates as the output a singing signal with a progressive-growing encoder/decoder architecture and boundary equilibrium GAN loss functions. Our quantitative and qualitative analysis show that the proposed model generates singing voices with much higher naturalness than an existing non adversarially-trained baseline. For reproducibility, the code will be publicly available at a GitHub repository upon paper publication.","['https://openalex.org/W3015843452', 'https://openalex.org/W2605195953', 'https://openalex.org/W2067709094', 'https://openalex.org/W2150933458', 'https://openalex.org/W2962866891', 'https://openalex.org/W2949281321', 'https://openalex.org/W2964607787', 'https://openalex.org/W2973046048', 'https://openalex.org/W2803963372', 'https://openalex.org/W2777302760', 'https://openalex.org/W3015645837', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962760235', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963403868', 'https://openalex.org/W2849790107', 'https://openalex.org/W2191779130', 'https://openalex.org/W3092879656', 'https://openalex.org/W2962793481', 'https://openalex.org/W2948211236', 'https://openalex.org/W2099471712', 'https://openalex.org/W2946809691', 'https://openalex.org/W2916985722', 'https://openalex.org/W2970006822', 'https://openalex.org/W2996286887', 'https://openalex.org/W2995005087', 'https://openalex.org/W2120847449', 'https://openalex.org/W2108382268', 'https://openalex.org/W2795783309', 'https://openalex.org/W2905741299', 'https://openalex.org/W2937242376', 'https://openalex.org/W2407685581', 'https://openalex.org/W3019084079', 'https://openalex.org/W2502312327', 'https://openalex.org/W3021164770', 'https://openalex.org/W3024973272']",2020-05-28
https://openalex.org/W3092769093,https://doi.org/10.48550/arxiv.2010.08123,Melody Classifier with Stacked-LSTM,"Attempts to use generative models for music generation have been common in recent years, and some of them have achieved good results. Pieces generated by some of these models are almost indistinguishable from those being composed by human composers. However, the research on the evaluation system for machine-generated music is still at a relatively early stage, and there is no uniform standard for such tasks. This paper proposes a stacked-LSTM binary classifier based on a language model, which can be used to distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.","['https://openalex.org/W2143612262', 'https://openalex.org/W2963681776', 'https://openalex.org/W3021164770', 'https://openalex.org/W2949382160']",2020-10-16
https://openalex.org/W3110955493,https://doi.org/10.48550/arxiv.2012.04572,I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at Pitch,"Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.","['https://openalex.org/W2939574508', 'https://openalex.org/W2032197604', 'https://openalex.org/W2964153729', 'https://openalex.org/W2154455356', 'https://openalex.org/W2624871570', 'https://openalex.org/W2963945466', 'https://openalex.org/W2074619556', 'https://openalex.org/W3094917204', 'https://openalex.org/W2054139811', 'https://openalex.org/W2963542245', 'https://openalex.org/W2935711438', 'https://openalex.org/W1821462560', 'https://openalex.org/W2054941444', 'https://openalex.org/W2250539671', 'https://openalex.org/W1969351858', 'https://openalex.org/W3095717210', 'https://openalex.org/W2970006822', 'https://openalex.org/W3100511085', 'https://openalex.org/W2886022419', 'https://openalex.org/W1559569201', 'https://openalex.org/W3090388844', 'https://openalex.org/W2948445958', 'https://openalex.org/W2098950531', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972597685', 'https://openalex.org/W3015338123', 'https://openalex.org/W2995233853', 'https://openalex.org/W2995254904', 'https://openalex.org/W3021164770', 'https://openalex.org/W3047579126', 'https://openalex.org/W2584032004', 'https://openalex.org/W2890043615', 'https://openalex.org/W2962785568', 'https://openalex.org/W3095617022', 'https://openalex.org/W1966365432', 'https://openalex.org/W2963975282', 'https://openalex.org/W2071640183', 'https://openalex.org/W2024310757', 'https://openalex.org/W2964307104', 'https://openalex.org/W2084044763', 'https://openalex.org/W2963799213', 'https://openalex.org/W3124061379', 'https://openalex.org/W2124229063', 'https://openalex.org/W2012922111', 'https://openalex.org/W3103557498', 'https://openalex.org/W3097934054', 'https://openalex.org/W2972443522', 'https://openalex.org/W3016115629', 'https://openalex.org/W1976878132', 'https://openalex.org/W2519091744', 'https://openalex.org/W1986595194', 'https://openalex.org/W3100307207', 'https://openalex.org/W2124864596', 'https://openalex.org/W3083833857', 'https://openalex.org/W3125709657', 'https://openalex.org/W3037149862', 'https://openalex.org/W85455704', 'https://openalex.org/W2158899491', 'https://openalex.org/W3099782249', 'https://openalex.org/W3021066808', 'https://openalex.org/W2962866891', 'https://openalex.org/W2774707525', 'https://openalex.org/W2963609956', 'https://openalex.org/W2980286501', 'https://openalex.org/W2605102758', 'https://openalex.org/W2008066450', 'https://openalex.org/W2103869314', 'https://openalex.org/W2989708046', 'https://openalex.org/W3094749184', 'https://openalex.org/W2606176153', 'https://openalex.org/W2111460811', 'https://openalex.org/W2526050071']",2020-12-08
https://openalex.org/W3191781868,https://doi.org/10.48550/arxiv.2108.01576,A Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset,"This paper proposes a new benchmark task for generat-ing musical passages in the audio domain by using thedrum loops from the FreeSound Loop Dataset, which arepublicly re-distributable. Moreover, we use a larger col-lection of drum loops from Looperman to establish fourmodel-based objective metrics for evaluation, releasingthese metrics as a library for quantifying and facilitatingthe progress of musical audio generation. Under this eval-uation framework, we benchmark the performance of threerecent deep generative adversarial network (GAN) mod-els we customize to generate loops, including StyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these models. Our evaluation shows that theone based on StyleGAN2 performs the best in both objec-tive and subjective metrics.","['https://openalex.org/W2572771584', 'https://openalex.org/W2963373786', 'https://openalex.org/W2405656250', 'https://openalex.org/W2805697608', 'https://openalex.org/W3107097936', 'https://openalex.org/W2962760235', 'https://openalex.org/W2948211236', 'https://openalex.org/W3014809999', 'https://openalex.org/W2894295011', 'https://openalex.org/W3048830653', 'https://openalex.org/W2117330939', 'https://openalex.org/W2754229890', 'https://openalex.org/W2792826772', 'https://openalex.org/W2801401223', 'https://openalex.org/W3092850823', 'https://openalex.org/W2606176153', 'https://openalex.org/W3160235471', 'https://openalex.org/W3119914886', 'https://openalex.org/W2268463237', 'https://openalex.org/W3157668818', 'https://openalex.org/W3015287975', 'https://openalex.org/W2991439390', 'https://openalex.org/W2584032004', 'https://openalex.org/W2980709326', 'https://openalex.org/W3095488131', 'https://openalex.org/W3131428244', 'https://openalex.org/W3021164770', 'https://openalex.org/W2949382160', 'https://openalex.org/W2901024736', 'https://openalex.org/W2956041762', 'https://openalex.org/W2997367363', 'https://openalex.org/W2953030256', 'https://openalex.org/W2962981281', 'https://openalex.org/W3024973272', 'https://openalex.org/W2963804063', 'https://openalex.org/W2962793481', 'https://openalex.org/W2070063887', 'https://openalex.org/W2946521317', 'https://openalex.org/W3029858316', 'https://openalex.org/W2905488776', 'https://openalex.org/W3129651364', 'https://openalex.org/W2995005087', 'https://openalex.org/W2099471712', 'https://openalex.org/W2951535099', 'https://openalex.org/W2782980316', 'https://openalex.org/W3136272958', 'https://openalex.org/W2931632499', 'https://openalex.org/W3125056307', 'https://openalex.org/W3035574324', 'https://openalex.org/W2963300588', 'https://openalex.org/W2962770929']",2021-08-03
https://openalex.org/W3030072033,,Network Bending: Manipulating The Inner Representations of Deep Generative Models.,"We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such black-box systems can be more meaningfully interpreted.","['https://openalex.org/W2970971581', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963577681', 'https://openalex.org/W1909320841', 'https://openalex.org/W2121949863', 'https://openalex.org/W2610018085', 'https://openalex.org/W2962851944', 'https://openalex.org/W2901107321', 'https://openalex.org/W183625566', 'https://openalex.org/W2963996492', 'https://openalex.org/W2099471712', 'https://openalex.org/W2150593711', 'https://openalex.org/W1915485278', 'https://openalex.org/W2962770929', 'https://openalex.org/W2148349024', 'https://openalex.org/W2059515884', 'https://openalex.org/W2737258237', 'https://openalex.org/W2963684088', 'https://openalex.org/W3014852036', 'https://openalex.org/W2934375473', 'https://openalex.org/W2133547499', 'https://openalex.org/W2995327724', 'https://openalex.org/W3021164770']",2020-05-25
https://openalex.org/W3159257553,https://doi.org/10.48550/arxiv.2105.01573,Exploring Disentanglement with Multilingual and Monolingual VQ-VAE,"This work examines the content and usefulness of disentangled phone and speaker representations from two separately trained VQ-VAE systems: one trained on multilingual data and another trained on monolingual data. We explore the multi- and monolingual models using four small proof-of-concept tasks: copy-synthesis, voice transformation, linguistic code-switching, and content-based privacy masking. From these tasks, we reflect on how disentangled phone and speaker representations can be used to manipulate speech in a meaningful way. Our experiments demonstrate that the VQ representations are suitable for these tasks, including creating new voices by mixing speaker representations together. We also present our novel technique to conceal the content of targeted words within an utterance by manipulating phone VQ codes, while retaining speaker identity and intelligibility of surrounding words. Finally, we discuss recommendations for further increasing the viability of disentangled representations.","['https://openalex.org/W2998572311', 'https://openalex.org/W3008391559', 'https://openalex.org/W2199505332', 'https://openalex.org/W2982290890', 'https://openalex.org/W3015484365', 'https://openalex.org/W2073495057', 'https://openalex.org/W3163338468', 'https://openalex.org/W2470254032', 'https://openalex.org/W2972921407', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972473628', 'https://openalex.org/W3082522567', 'https://openalex.org/W2972849140', 'https://openalex.org/W3015434413', 'https://openalex.org/W3160584619', 'https://openalex.org/W3043999252', 'https://openalex.org/W3021164770', 'https://openalex.org/W3097297926', 'https://openalex.org/W2747874407', 'https://openalex.org/W2150769028', 'https://openalex.org/W3094917204', 'https://openalex.org/W2790857402', 'https://openalex.org/W2964002616', 'https://openalex.org/W3024962219', 'https://openalex.org/W3162390194', 'https://openalex.org/W3097286738', 'https://openalex.org/W2407075323']",2021-05-04
https://openalex.org/W3170636531,https://doi.org/10.48550/arxiv.2106.06819,D2C: Diffusion-Denoising Models for Few-shot Conditional Generation,"Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.","['https://openalex.org/W2922772346', 'https://openalex.org/W2963592272', 'https://openalex.org/W2963800363', 'https://openalex.org/W2963522749', 'https://openalex.org/W2321533354', 'https://openalex.org/W3034279703', 'https://openalex.org/W3035068567', 'https://openalex.org/W3045338699', 'https://openalex.org/W2767757834', 'https://openalex.org/W2842511635', 'https://openalex.org/W2687693326', 'https://openalex.org/W3135367836', 'https://openalex.org/W2964167449', 'https://openalex.org/W2963073614', 'https://openalex.org/W2559626456', 'https://openalex.org/W2129069237', 'https://openalex.org/W2963799213', 'https://openalex.org/W2949649223', 'https://openalex.org/W3129576130', 'https://openalex.org/W2617620476', 'https://openalex.org/W2788017640', 'https://openalex.org/W2405756170', 'https://openalex.org/W3135995094', 'https://openalex.org/W2910601191', 'https://openalex.org/W3110257065', 'https://openalex.org/W2963785576', 'https://openalex.org/W2785678896', 'https://openalex.org/W2988793532', 'https://openalex.org/W2029164135', 'https://openalex.org/W2947708079', 'https://openalex.org/W2904367110', 'https://openalex.org/W3174194560', 'https://openalex.org/W2981479880', 'https://openalex.org/W2412320034', 'https://openalex.org/W3092442149', 'https://openalex.org/W2125389028', 'https://openalex.org/W2962927829', 'https://openalex.org/W2622563070', 'https://openalex.org/W2805177834', 'https://openalex.org/W3162926177', 'https://openalex.org/W2953318193', 'https://openalex.org/W2904539038', 'https://openalex.org/W2409550820', 'https://openalex.org/W2963981733', 'https://openalex.org/W3035058308', 'https://openalex.org/W3092033429', 'https://openalex.org/W3035060554', 'https://openalex.org/W3102061158', 'https://openalex.org/W3037879915', 'https://openalex.org/W2949382160', 'https://openalex.org/W3030163527', 'https://openalex.org/W3042786565', 'https://openalex.org/W3135404760', 'https://openalex.org/W2116064496', 'https://openalex.org/W2808746463', 'https://openalex.org/W3021164770', 'https://openalex.org/W3040462728', 'https://openalex.org/W1583912456', 'https://openalex.org/W2411541852', 'https://openalex.org/W2789776893', 'https://openalex.org/W2594057160', 'https://openalex.org/W2123958887', 'https://openalex.org/W3106570356', 'https://openalex.org/W2799251726', 'https://openalex.org/W3140373187', 'https://openalex.org/W2530741948', 'https://openalex.org/W2164452299', 'https://openalex.org/W2766527293', 'https://openalex.org/W3133191702', 'https://openalex.org/W2944828972', 'https://openalex.org/W3145450063', 'https://openalex.org/W3107668149', 'https://openalex.org/W3209501356', 'https://openalex.org/W3041956526', 'https://openalex.org/W2557449848', 'https://openalex.org/W2947590261', 'https://openalex.org/W2809595412', 'https://openalex.org/W3180268133', 'https://openalex.org/W2587284713', 'https://openalex.org/W1861492603', 'https://openalex.org/W2963341956', 'https://openalex.org/W2951004968', 'https://openalex.org/W2959300817', 'https://openalex.org/W3148695041', 'https://openalex.org/W3035574324', 'https://openalex.org/W3098708719', 'https://openalex.org/W3005680577', 'https://openalex.org/W3034431451', 'https://openalex.org/W1834627138', 'https://openalex.org/W2163605009', 'https://openalex.org/W2987283559', 'https://openalex.org/W299440670', 'https://openalex.org/W2911290044', 'https://openalex.org/W2893749619', 'https://openalex.org/W2962937159', 'https://openalex.org/W2099471712', 'https://openalex.org/W3036167779', 'https://openalex.org/W2985060393', 'https://openalex.org/W2524985544', 'https://openalex.org/W2949650786', 'https://openalex.org/W3009561768', 'https://openalex.org/W2964268978', 'https://openalex.org/W2155292833']",2021-06-12
https://openalex.org/W3204992386,https://doi.org/10.48550/arxiv.2110.05313,Unsupervised Source Separation via Bayesian Inference in the Latent Domain,"State of the art audio source separation models rely on supervised data-driven approaches, which can be expensive in terms of labeling resources. On the other hand, approaches for training these models without any direct supervision are typically high-demanding in terms of memory and time requirements, and remain impractical to be used at inference time. We aim to tackle these limitations by proposing a simple yet effective unsupervised separation algorithm, which operates directly on a latent representation of time-domain signals. Our algorithm relies on deep Bayesian priors in the form of pre-trained autoregressive networks to model the probability distributions of each source. We leverage the low cardinality of the discrete latent space, trained with a novel loss term imposing a precise arithmetic structure on it, to perform exact Bayesian inference without relying on an approximation strategy. We validate our approach on the Slakh dataset arXiv:1909.08494, demonstrating results in line with state of the art supervised approaches while requiring fewer resources with respect to other unsupervised methods.","['https://openalex.org/W2991107219', 'https://openalex.org/W3160695487', 'https://openalex.org/W2998490864', 'https://openalex.org/W2669032454', 'https://openalex.org/W2971074500', 'https://openalex.org/W2892365986', 'https://openalex.org/W2990594533', 'https://openalex.org/W3035369987', 'https://openalex.org/W3093725184', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963799213', 'https://openalex.org/W3090496396', 'https://openalex.org/W2099741732', 'https://openalex.org/W1594771616', 'https://openalex.org/W2135181320', 'https://openalex.org/W2168793898', 'https://openalex.org/W3021164770', 'https://openalex.org/W3030191531', 'https://openalex.org/W2162514423']",2021-10-11
https://openalex.org/W3120142428,https://doi.org/10.48550/arxiv.2101.00169,Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers,"Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.","['https://openalex.org/W2888723145', 'https://openalex.org/W3021164770']",2021-01-01
https://openalex.org/W3163985612,https://doi.org/10.5626/jok.2021.48.8.940,Deep Neural Networks and End-to-End Learning for Audio Compression,"Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.","['https://openalex.org/W2963091184', 'https://openalex.org/W3021164770', 'https://openalex.org/W2800934755', 'https://openalex.org/W1884859883', 'https://openalex.org/W2753738274', 'https://openalex.org/W3093390800', 'https://openalex.org/W2064675550', 'https://openalex.org/W2950237361', 'https://openalex.org/W3118215210', 'https://openalex.org/W592244745', 'https://openalex.org/W2963182577', 'https://openalex.org/W2962942158', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963619462', 'https://openalex.org/W2963799213', 'https://openalex.org/W2808041683', 'https://openalex.org/W2964339599', 'https://openalex.org/W1959608418', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963279312', 'https://openalex.org/W1498436455']",2021-08-31
https://openalex.org/W3170685585,,"Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral","The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.","['https://openalex.org/W3036010930', 'https://openalex.org/W1989661300', 'https://openalex.org/W2971073828', 'https://openalex.org/W3104876213', 'https://openalex.org/W2963383053', 'https://openalex.org/W2171585891', 'https://openalex.org/W2171846551', 'https://openalex.org/W2187207766', 'https://openalex.org/W2964316188', 'https://openalex.org/W1638306878', 'https://openalex.org/W3107128594', 'https://openalex.org/W1585701772', 'https://openalex.org/W2158195707', 'https://openalex.org/W2593414223', 'https://openalex.org/W2963373786', 'https://openalex.org/W3034723908', 'https://openalex.org/W2117058215', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963981733', 'https://openalex.org/W2963305780', 'https://openalex.org/W2194775991', 'https://openalex.org/W2134237567', 'https://openalex.org/W2971128425', 'https://openalex.org/W3021164770', 'https://openalex.org/W3118608800', 'https://openalex.org/W2149268774', 'https://openalex.org/W2159178073', 'https://openalex.org/W2963289805', 'https://openalex.org/W2082092506', 'https://openalex.org/W2059800182', 'https://openalex.org/W2964121406', 'https://openalex.org/W2129058877', 'https://openalex.org/W2804456710', 'https://openalex.org/W2946193058', 'https://openalex.org/W597395834', 'https://openalex.org/W2964000438', 'https://openalex.org/W2753300133', 'https://openalex.org/W2963224888', 'https://openalex.org/W2166944917', 'https://openalex.org/W2805984778', 'https://openalex.org/W2151835418', 'https://openalex.org/W2014404824', 'https://openalex.org/W2114207451', 'https://openalex.org/W1999120268', 'https://openalex.org/W3098824823', 'https://openalex.org/W2971008823', 'https://openalex.org/W2962919088', 'https://openalex.org/W2055309977', 'https://openalex.org/W2963494889', 'https://openalex.org/W2803713489', 'https://openalex.org/W2962770929', 'https://openalex.org/W3037652471', 'https://openalex.org/W2125232050', 'https://openalex.org/W3098903812', 'https://openalex.org/W583278954']",2021-06-15
https://openalex.org/W3178354449,https://doi.org/10.48550/arxiv.2107.05009,PocketVAE: A Two-step Model for Groove Generation and Control,"Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.","['https://openalex.org/W1924770834', 'https://openalex.org/W3107097936', 'https://openalex.org/W2963408210', 'https://openalex.org/W2963522749', 'https://openalex.org/W3136272958', 'https://openalex.org/W2283630710', 'https://openalex.org/W2253682605', 'https://openalex.org/W2963799213', 'https://openalex.org/W3210904914', 'https://openalex.org/W2927380283', 'https://openalex.org/W2016063099', 'https://openalex.org/W2946521317', 'https://openalex.org/W2990617740', 'https://openalex.org/W2963330667', 'https://openalex.org/W3127854286', 'https://openalex.org/W3098518646', 'https://openalex.org/W3161301744', 'https://openalex.org/W2397980357', 'https://openalex.org/W3021164770', 'https://openalex.org/W3014809999', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963800363', 'https://openalex.org/W2962212541', 'https://openalex.org/W2511667032', 'https://openalex.org/W2087870700', 'https://openalex.org/W3015287975']",2021-07-11
https://openalex.org/W3181311346,https://doi.org/10.48550/arxiv.2107.03443,BumbleBee: A Transformer for Music,"We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)","['https://openalex.org/W2963681776', 'https://openalex.org/W3021164770', 'https://openalex.org/W2753868141', 'https://openalex.org/W2992790584', 'https://openalex.org/W3015468748', 'https://openalex.org/W2792210438', 'https://openalex.org/W2963575853', 'https://openalex.org/W2919624000', 'https://openalex.org/W3035618017']",2021-07-07
https://openalex.org/W3192483709,https://doi.org/10.48550/arxiv.2108.01043,Musical Speech: A Transformer-based Composition Tool,"In this paper, we propose a new compositional tool that will generate a musical outline of speech recorded/provided by the user for use as a musical building block in their compositions. The tool allows any user to use their own speech to generate musical material, while still being able to hear the direct connection between their recorded speech and the resulting music. The tool is built on our proposed pipeline. This pipeline begins with speech-based signal processing, after which some simple musical heuristics are applied, and finally these pre-processed signals are passed through Transformer models trained on new musical tasks. We illustrate the effectiveness of our pipeline -- which does not require a paired dataset for training -- through examples of music created by musicians making use of our tool.","['https://openalex.org/W2891794946', 'https://openalex.org/W3122518304', 'https://openalex.org/W2943532091', 'https://openalex.org/W3036120435', 'https://openalex.org/W2049515993', 'https://openalex.org/W3040087772', 'https://openalex.org/W3098518646', 'https://openalex.org/W1560013842', 'https://openalex.org/W3082274269', 'https://openalex.org/W2611369375', 'https://openalex.org/W2009254313', 'https://openalex.org/W2096774922', 'https://openalex.org/W2753868141', 'https://openalex.org/W1875231349', 'https://openalex.org/W2139301061', 'https://openalex.org/W3000389243', 'https://openalex.org/W3031000691', 'https://openalex.org/W2803963372', 'https://openalex.org/W2588767398', 'https://openalex.org/W2963408210', 'https://openalex.org/W2963403868', 'https://openalex.org/W3021164770', 'https://openalex.org/W2919624000', 'https://openalex.org/W1563089615', 'https://openalex.org/W2963341956', 'https://openalex.org/W2898148140', 'https://openalex.org/W1992153276', 'https://openalex.org/W2281488037', 'https://openalex.org/W3011411500', 'https://openalex.org/W2566129194', 'https://openalex.org/W2950813464', 'https://openalex.org/W1494198834', 'https://openalex.org/W2091425152', 'https://openalex.org/W3092135915', 'https://openalex.org/W3016714241', 'https://openalex.org/W3161779466', 'https://openalex.org/W2142518024', 'https://openalex.org/W2794719876', 'https://openalex.org/W3029764700', 'https://openalex.org/W1965635292', 'https://openalex.org/W2744457411', 'https://openalex.org/W5537492', 'https://openalex.org/W2948211236', 'https://openalex.org/W2962942158', 'https://openalex.org/W2471520273', 'https://openalex.org/W1997876077', 'https://openalex.org/W1966264494', 'https://openalex.org/W1986845175', 'https://openalex.org/W2604567995', 'https://openalex.org/W2131864930']",2021-08-02
https://openalex.org/W3209109096,https://doi.org/10.48550/arxiv.2110.13071,Unsupervised Source Separation By Steering Pretrained Music Models,"We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.","['https://openalex.org/W3015289235', 'https://openalex.org/W1561135842', 'https://openalex.org/W3180355996', 'https://openalex.org/W2972411915', 'https://openalex.org/W3180663620', 'https://openalex.org/W2963207607', 'https://openalex.org/W2414894569', 'https://openalex.org/W3104704316', 'https://openalex.org/W3160751854', 'https://openalex.org/W2127851351', 'https://openalex.org/W2296573765', 'https://openalex.org/W3082274269', 'https://openalex.org/W2950060770', 'https://openalex.org/W3030163527', 'https://openalex.org/W3015213533', 'https://openalex.org/W3135367836', 'https://openalex.org/W2027884847', 'https://openalex.org/W3197781391', 'https://openalex.org/W2998490864', 'https://openalex.org/W2971074500', 'https://openalex.org/W2127870748', 'https://openalex.org/W3029858316', 'https://openalex.org/W2990594533', 'https://openalex.org/W2963992487', 'https://openalex.org/W3100954996', 'https://openalex.org/W3021164770']",2021-10-25
https://openalex.org/W3209900408,https://doi.org/10.1145/3474085.3475529,Actions Speak Louder than Listening,"The subjective evaluation of music generation techniques has been mostly done with questionnaire-based listening tests while ignoring the perspectives from music composition, arrangement, and soundtrack editing. In this paper, we propose an editing test to evaluate users' editing experience of music generation models in a systematic way. To do this, we design a new music style transfer model combining the non-chronological inference architecture, autoregressive models and the Transformer, which serves as an improvement from the baseline model on the same style transfer task. Then, we compare the performance of the two models with a conventional listening test and the proposed editing test, in which the quality of generated samples is assessed by the amount of effort (e.g., the number of required keyboard and mouse actions) spent by users to polish a music clip. Results on two target styles indicate that the improvement over the baseline model can be reflected by the editing test quantitatively. Also, the editing test provides profound insights which are not accessible from usual listening tests. The major contribution of this paper is the systematic presentation of the editing test and the corresponding insights, while the proposed music style transfer model based on state-of-the-art neural networks represents another contribution.","['https://openalex.org/W2891757161', 'https://openalex.org/W3092879656', 'https://openalex.org/W2963351448', 'https://openalex.org/W2962775980', 'https://openalex.org/W2511667032', 'https://openalex.org/W2898827701', 'https://openalex.org/W3124507227', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963403868', 'https://openalex.org/W2782490852', 'https://openalex.org/W3141966616', 'https://openalex.org/W2962699318', 'https://openalex.org/W2129192849', 'https://openalex.org/W2747023714', 'https://openalex.org/W2774077477', 'https://openalex.org/W2986842658', 'https://openalex.org/W2962968839', 'https://openalex.org/W3034758887', 'https://openalex.org/W3106383471', 'https://openalex.org/W3011240691', 'https://openalex.org/W2902922910', 'https://openalex.org/W3035435378', 'https://openalex.org/W2991377373', 'https://openalex.org/W2754229890', 'https://openalex.org/W2606176153', 'https://openalex.org/W2951535099', 'https://openalex.org/W2954914497', 'https://openalex.org/W2963575853', 'https://openalex.org/W2793183272', 'https://openalex.org/W2753868141', 'https://openalex.org/W2963032576', 'https://openalex.org/W2267126114', 'https://openalex.org/W3021164770', 'https://openalex.org/W2989656344', 'https://openalex.org/W2586947700', 'https://openalex.org/W2963341956', 'https://openalex.org/W2904596301', 'https://openalex.org/W2936497627']",2021-10-17
https://openalex.org/W3212222439,https://doi.org/10.48550/arxiv.2111.07657,Symbolic Music Loop Generation with VQ-VAE,"Music is a repetition of patterns and rhythms. It can be composed by repeating a certain number of bars in a structured way. In this paper, the objective is to generate a loop of 8 bars that can be used as a building block of music. Even considering musical diversity, we assume that music patterns familiar to humans can be defined in a finite set. With explicit rules to extract loops from music, we found that discrete representations are sufficient to model symbolic music sequences. Among VAE family, musical properties from VQ-VAE are better observed rather than other models. Further, to emphasize musical structure, we have manipulated discrete latent features to be repetitive so that the properties are more strengthened. Quantitative and qualitative experiments are extensively conducted to verify our assumptions.","['https://openalex.org/W2805697608', 'https://openalex.org/W2963408210', 'https://openalex.org/W2908510526', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963681776', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963032576', 'https://openalex.org/W2946521317', 'https://openalex.org/W3119786062', 'https://openalex.org/W3034758887', 'https://openalex.org/W2475687244', 'https://openalex.org/W3021164770', 'https://openalex.org/W1959608418', 'https://openalex.org/W2753738274', 'https://openalex.org/W3018535504', 'https://openalex.org/W3099378280', 'https://openalex.org/W2963403868', 'https://openalex.org/W1677182931', 'https://openalex.org/W2963799213', 'https://openalex.org/W1884029234', 'https://openalex.org/W648786980']",2021-11-15
https://openalex.org/W3213657258,https://doi.org/10.1007/978-3-030-82681-9_17,3D Spatial Sound Individualization with Perceptual Feedback,,"['https://openalex.org/W2006144938', 'https://openalex.org/W1964832178', 'https://openalex.org/W2051368408', 'https://openalex.org/W2138537392', 'https://openalex.org/W1981276685', 'https://openalex.org/W2054141820', 'https://openalex.org/W2024697317', 'https://openalex.org/W2108668360', 'https://openalex.org/W4229982299', 'https://openalex.org/W2031554514', 'https://openalex.org/W1522734439', 'https://openalex.org/W2902572630', 'https://openalex.org/W6631455383', 'https://openalex.org/W6602203279', 'https://openalex.org/W3048790906', 'https://openalex.org/W2136340340', 'https://openalex.org/W2188365844', 'https://openalex.org/W3092290890', 'https://openalex.org/W2163371195', 'https://openalex.org/W2907337720', 'https://openalex.org/W2770331385', 'https://openalex.org/W2100811130', 'https://openalex.org/W2999811719', 'https://openalex.org/W3094721881', 'https://openalex.org/W2887595520', 'https://openalex.org/W2769850798', 'https://openalex.org/W2169003314', 'https://openalex.org/W1952738065', 'https://openalex.org/W1968571232', 'https://openalex.org/W3023257870', 'https://openalex.org/W3021164770', 'https://openalex.org/W2962770929', 'https://openalex.org/W2152329502', 'https://openalex.org/W3093493879', 'https://openalex.org/W2895535699', 'https://openalex.org/W3129525271', 'https://openalex.org/W3129576130', 'https://openalex.org/W2941048526', 'https://openalex.org/W2963624663', 'https://openalex.org/W3080878640', 'https://openalex.org/W2246664447', 'https://openalex.org/W2963575853', 'https://openalex.org/W2579496681', 'https://openalex.org/W2949416428', 'https://openalex.org/W2944145673', 'https://openalex.org/W3015826515', 'https://openalex.org/W1497256448', 'https://openalex.org/W2963681776', 'https://openalex.org/W1522301498', 'https://openalex.org/W3098161516', 'https://openalex.org/W2469134594']",2021-01-01
https://openalex.org/W3217025373,https://doi.org/10.48550/arxiv.2112.01516,Ownership and Creativity in Generative Models,"Machine learning generated content such as image artworks, textual poems and music become prominent in recent years. These tools attract much attention from the media, artists, researchers, and investors. Because these tools are data-driven, they are inherently different than the traditional creative tools which arises the question - who may own the content that is generated by these tools? In this paper we aim to address this question, we start by providing a background to this problem, raising several candidates that may own the content and arguments for each one of them. Then we propose a possible algorithmic solution in the vision-based model's regime. Finally, we discuss the broader implications of this problem.","['https://openalex.org/W2331128040', 'https://openalex.org/W1686810756', 'https://openalex.org/W2962770929', 'https://openalex.org/W2574971047', 'https://openalex.org/W2963174698', 'https://openalex.org/W2099471712', 'https://openalex.org/W3192148059', 'https://openalex.org/W2096293551', 'https://openalex.org/W3109585842', 'https://openalex.org/W3030163527', 'https://openalex.org/W2626778328', 'https://openalex.org/W3135367836', 'https://openalex.org/W2145482038', 'https://openalex.org/W1849277567', 'https://openalex.org/W2963159690', 'https://openalex.org/W1538131130', 'https://openalex.org/W2131018663', 'https://openalex.org/W3035574324', 'https://openalex.org/W2143283816', 'https://openalex.org/W2963341956', 'https://openalex.org/W2962793481', 'https://openalex.org/W3129576130', 'https://openalex.org/W2313566778', 'https://openalex.org/W2962843773', 'https://openalex.org/W2703190149', 'https://openalex.org/W3109228002', 'https://openalex.org/W2962974533', 'https://openalex.org/W2962785568', 'https://openalex.org/W2960274051', 'https://openalex.org/W2963310665', 'https://openalex.org/W2397490357', 'https://openalex.org/W2963748441', 'https://openalex.org/W3021164770', 'https://openalex.org/W2893749619']",2021-12-02
https://openalex.org/W4312400623,https://doi.org/10.1007/978-3-031-19797-0_8,VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder,,"['https://openalex.org/W6605755270', 'https://openalex.org/W6631448583', 'https://openalex.org/W2963839617', 'https://openalex.org/W4313021454', 'https://openalex.org/W3174166237', 'https://openalex.org/W2963676087', 'https://openalex.org/W2969985801', 'https://openalex.org/W2955625300', 'https://openalex.org/W54257720', 'https://openalex.org/W3180355996', 'https://openalex.org/W3181955432', 'https://openalex.org/W2475287302', 'https://openalex.org/W3035570181', 'https://openalex.org/W2963073614', 'https://openalex.org/W2331128040', 'https://openalex.org/W2962770929', 'https://openalex.org/W3035574324', 'https://openalex.org/W2963312584', 'https://openalex.org/W3089425003', 'https://openalex.org/W3106746951', 'https://openalex.org/W2963089432', 'https://openalex.org/W2963372104', 'https://openalex.org/W3034352949', 'https://openalex.org/W2102166818', 'https://openalex.org/W2963743395', 'https://openalex.org/W2752796333', 'https://openalex.org/W3035002246', 'https://openalex.org/W3180391059', 'https://openalex.org/W4312326192', 'https://openalex.org/W3215495615', 'https://openalex.org/W3021195052', 'https://openalex.org/W3167297682', 'https://openalex.org/W2895542678', 'https://openalex.org/W2508457857', 'https://openalex.org/W2962785568', 'https://openalex.org/W2966926453', 'https://openalex.org/W3093222528']",2022-01-01
https://openalex.org/W4390880595,https://doi.org/10.1038/s42256-023-00778-3,Assessing antibody and nanobody nativeness for hit selection and humanization with AbNatiV,,"['https://openalex.org/W1971715214', 'https://openalex.org/W3024676672', 'https://openalex.org/W4310875677', 'https://openalex.org/W2095553811', 'https://openalex.org/W2169610187', 'https://openalex.org/W2317056394', 'https://openalex.org/W2030522563', 'https://openalex.org/W2011261569', 'https://openalex.org/W2896148359', 'https://openalex.org/W4319842310', 'https://openalex.org/W2105521580', 'https://openalex.org/W2140674483', 'https://openalex.org/W1998383538', 'https://openalex.org/W2123708888', 'https://openalex.org/W2004714495', 'https://openalex.org/W2578029714', 'https://openalex.org/W2999958881', 'https://openalex.org/W2665805494', 'https://openalex.org/W1912933662', 'https://openalex.org/W2758496793', 'https://openalex.org/W2804969359', 'https://openalex.org/W3197623424', 'https://openalex.org/W4321748896', 'https://openalex.org/W4315619796', 'https://openalex.org/W2992583967', 'https://openalex.org/W3106152681', 'https://openalex.org/W4220959747', 'https://openalex.org/W4213244137', 'https://openalex.org/W2913847447', 'https://openalex.org/W3030808598', 'https://openalex.org/W3089425003', 'https://openalex.org/W2025768430', 'https://openalex.org/W3206187363', 'https://openalex.org/W4220757565', 'https://openalex.org/W2970493999', 'https://openalex.org/W3169434891', 'https://openalex.org/W3062976875', 'https://openalex.org/W2016690055', 'https://openalex.org/W2053319140', 'https://openalex.org/W1989053318', 'https://openalex.org/W2117292655', 'https://openalex.org/W4221053240', 'https://openalex.org/W3095441755', 'https://openalex.org/W2053222732', 'https://openalex.org/W2053887681', 'https://openalex.org/W1488587546', 'https://openalex.org/W4378714914', 'https://openalex.org/W2079576324', 'https://openalex.org/W3096751000', 'https://openalex.org/W2078070662', 'https://openalex.org/W3132323068', 'https://openalex.org/W2893755931', 'https://openalex.org/W2996962244', 'https://openalex.org/W3205114076', 'https://openalex.org/W3029308247', 'https://openalex.org/W3134185574', 'https://openalex.org/W3144239152', 'https://openalex.org/W4206087583', 'https://openalex.org/W2980789587', 'https://openalex.org/W4283744078', 'https://openalex.org/W4379919536', 'https://openalex.org/W2513558090', 'https://openalex.org/W2159047076', 'https://openalex.org/W3130039396', 'https://openalex.org/W1855237549', 'https://openalex.org/W4287083215', 'https://openalex.org/W3206395542', 'https://openalex.org/W2789543585', 'https://openalex.org/W2970971581', 'https://openalex.org/W3081874799', 'https://openalex.org/W3024610894', 'https://openalex.org/W2041741213', 'https://openalex.org/W2037914743', 'https://openalex.org/W2974982237', 'https://openalex.org/W2001900040', 'https://openalex.org/W2097356092', 'https://openalex.org/W2142808579', 'https://openalex.org/W2156707027', 'https://openalex.org/W6892450193', 'https://openalex.org/W4206948363']",2024-01-15
https://openalex.org/W4386057807,https://doi.org/10.1109/cvpr52729.2023.01759,CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language,"Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.","['https://openalex.org/W2194775991', 'https://openalex.org/W4312933868', 'https://openalex.org/W4312388283', 'https://openalex.org/W3201927751', 'https://openalex.org/W6765779288', 'https://openalex.org/W6809885388', 'https://openalex.org/W6762931180', 'https://openalex.org/W6790978476', 'https://openalex.org/W2987068636', 'https://openalex.org/W6748208425', 'https://openalex.org/W3173631825', 'https://openalex.org/W6840815571', 'https://openalex.org/W6810165261', 'https://openalex.org/W6842746547', 'https://openalex.org/W4312708649', 'https://openalex.org/W2963926543', 'https://openalex.org/W4312638666', 'https://openalex.org/W4312910764', 'https://openalex.org/W4313041309', 'https://openalex.org/W6810452849', 'https://openalex.org/W4313036632', 'https://openalex.org/W4312971576', 'https://openalex.org/W6804566349', 'https://openalex.org/W2986615800', 'https://openalex.org/W2752796333', 'https://openalex.org/W6843813467', 'https://openalex.org/W6776218486', 'https://openalex.org/W2342277278', 'https://openalex.org/W4313021454', 'https://openalex.org/W2962849139', 'https://openalex.org/W6749625137', 'https://openalex.org/W6801810553', 'https://openalex.org/W2963799213', 'https://openalex.org/W2971074500', 'https://openalex.org/W4225598930', 'https://openalex.org/W4289785095', 'https://openalex.org/W2949671016', 'https://openalex.org/W3216237230', 'https://openalex.org/W4394671432', 'https://openalex.org/W4224035735', 'https://openalex.org/W2964078384', 'https://openalex.org/W3166396011', 'https://openalex.org/W3089425003', 'https://openalex.org/W4287802874', 'https://openalex.org/W3129576130', 'https://openalex.org/W4288099666', 'https://openalex.org/W4225432580', 'https://openalex.org/W4307475428', 'https://openalex.org/W4312911498', 'https://openalex.org/W4301206121', 'https://openalex.org/W4383682679', 'https://openalex.org/W4294959213', 'https://openalex.org/W4286225000', 'https://openalex.org/W4303440777', 'https://openalex.org/W4298187450', 'https://openalex.org/W4226125322', 'https://openalex.org/W3203511201']",2023-06-01
https://openalex.org/W3168380356,https://doi.org/10.3390/rs13112127,A Deep Vector Quantization Clustering Method for Polarimetric SAR Images,"Convolutional Neural Network (CNN) models are widely used in supervised Polarimetric Synthetic Aperture Radar (PolSAR) image classification. They are powerful tools to capture the non-linear dependency between adjacent pixels and outperform traditional methods on various benchmarks. On the contrary, research works investigating unsupervised PolSAR classification are quite rare, because most CNN models need to be trained with labeled data. In this paper, we propose a completely unsupervised model by fusing the Convolutional Autoencoder (CAE) with Vector Quantization (VQ). An auxiliary Gaussian smoothing loss is adopted for better semantic consistency in the output classification map. Qualitative and quantitative experiments are carried out on satellite and airborne full polarization data (RadarSat2/E-SAR, AIRSAR). The proposed model achieves 91.87%, 83.58% and 96.93% overall accuracy (OA) on the three datasets, which are much higher than the traditional H/alpha-Wishart method, and it exhibits better visual quality as well.","['https://openalex.org/W1538058927', 'https://openalex.org/W2115226769', 'https://openalex.org/W2133989913', 'https://openalex.org/W2146623218', 'https://openalex.org/W2037095848', 'https://openalex.org/W2008826820', 'https://openalex.org/W1997889957', 'https://openalex.org/W2913812556', 'https://openalex.org/W6640054144', 'https://openalex.org/W1901129140', 'https://openalex.org/W2560023338', 'https://openalex.org/W1923697677', 'https://openalex.org/W2412782625', 'https://openalex.org/W2572389516', 'https://openalex.org/W3088975408', 'https://openalex.org/W2136655611', 'https://openalex.org/W2987176577', 'https://openalex.org/W2001337397', 'https://openalex.org/W2976971678', 'https://openalex.org/W2089391573', 'https://openalex.org/W2033995914', 'https://openalex.org/W2819744022', 'https://openalex.org/W3021829869', 'https://openalex.org/W2794398988', 'https://openalex.org/W3024525511', 'https://openalex.org/W2559324447', 'https://openalex.org/W3033931765', 'https://openalex.org/W2754361766', 'https://openalex.org/W2793189836', 'https://openalex.org/W3089425003', 'https://openalex.org/W3046165398', 'https://openalex.org/W2328417713', 'https://openalex.org/W3098095976', 'https://openalex.org/W2395611524']",2021-05-28
https://openalex.org/W4292213097,https://doi.org/10.1109/icpr56361.2022.9956102,Leveraging Vector-Quantized Variational Autoencoder Inner Metrics for Anomaly Detection,International audience,"['https://openalex.org/W3025427857', 'https://openalex.org/W6811051256', 'https://openalex.org/W6751494907', 'https://openalex.org/W2979688502', 'https://openalex.org/W3135287914', 'https://openalex.org/W6640963894', 'https://openalex.org/W2948978827', 'https://openalex.org/W3034648032', 'https://openalex.org/W6771530079', 'https://openalex.org/W3109715690', 'https://openalex.org/W2752796333', 'https://openalex.org/W6790978476', 'https://openalex.org/W6790617458', 'https://openalex.org/W6780405429', 'https://openalex.org/W3107147681', 'https://openalex.org/W2957120403', 'https://openalex.org/W3089028909', 'https://openalex.org/W6785596320', 'https://openalex.org/W6631708475', 'https://openalex.org/W6763324549', 'https://openalex.org/W6787540315', 'https://openalex.org/W6763294777', 'https://openalex.org/W2049058890', 'https://openalex.org/W3118868805', 'https://openalex.org/W2115627867', 'https://openalex.org/W2948982773', 'https://openalex.org/W6787972765', 'https://openalex.org/W2122361470', 'https://openalex.org/W2809705434', 'https://openalex.org/W6762931180', 'https://openalex.org/W3003449356', 'https://openalex.org/W2133665775', 'https://openalex.org/W4226358929', 'https://openalex.org/W4287869122', 'https://openalex.org/W1531103298', 'https://openalex.org/W3165699580', 'https://openalex.org/W3204937802', 'https://openalex.org/W2971074500', 'https://openalex.org/W3129166376', 'https://openalex.org/W3129576130', 'https://openalex.org/W3101017490', 'https://openalex.org/W3089425003', 'https://openalex.org/W2963799213', 'https://openalex.org/W3133297714', 'https://openalex.org/W3118608800', 'https://openalex.org/W4288335160', 'https://openalex.org/W2803697594', 'https://openalex.org/W2948571844', 'https://openalex.org/W3147184966', 'https://openalex.org/W1959608418', 'https://openalex.org/W3040536599']",2022-08-21
https://openalex.org/W4367366153,https://doi.org/10.1101/2023.04.28.538712,"AbNatiV: VQ-VAE-based assessment of antibody and nanobody nativeness for hit selection, humanisation, and engineering","Abstract Monoclonal antibodies have emerged as key therapeutics, and nanobodies are rapidly gaining momentum following the approval of the first nanobody drug in 2019. Nonetheless, the development of these biologics as therapeutics remains a challenge. Despite the availability of established in vitro directed evolution technologies that are relatively fast and cheap to deploy, the gold standard for generating therapeutic antibodies remains discovery from animal immunization or patients. Immune-system derived antibodies tend to have favourable properties in vivo, including long half-life, low reactivity with self-antigens, and low toxicity. Here, we present AbNatiV, a deep-learning tool for assessing the nativeness of antibodies and nanobodies, i.e., their likelihood of belonging to the distribution of immune-system derived human antibodies or camelid nanobodies. AbNatiV is a multi-purpose tool that accurately predicts the nativeness of Fv sequences from any source, including synthetic libraries and computational design. It provides an interpretable score that predicts the likelihood of immunogenicity, and a residue-level profile that can guide the engineering of antibodies and nanobodies indistinguishable from immune-system-derived ones. We further introduce an automated humanisation pipeline, which we applied to two nanobodies. Wet-lab experiments show that AbNatiV-humanized nanobodies retain binding and stability at par or better than their wild type, unlike nanobodies humanised relying on conventional structural and residue-frequency analysis. We make AbNatiV available as downloadable software and as a webserver.","['https://openalex.org/W1971715214', 'https://openalex.org/W3024676672', 'https://openalex.org/W4310875677', 'https://openalex.org/W2095553811', 'https://openalex.org/W2169610187', 'https://openalex.org/W2317056394', 'https://openalex.org/W1515836930', 'https://openalex.org/W2011261569', 'https://openalex.org/W2896148359', 'https://openalex.org/W4319842310', 'https://openalex.org/W2052332203', 'https://openalex.org/W2105521580', 'https://openalex.org/W2140674483', 'https://openalex.org/W1998383538', 'https://openalex.org/W2123708888', 'https://openalex.org/W2004714495', 'https://openalex.org/W2578029714', 'https://openalex.org/W2999958881', 'https://openalex.org/W2665805494', 'https://openalex.org/W1912933662', 'https://openalex.org/W4308915416', 'https://openalex.org/W2758496793', 'https://openalex.org/W2804969359', 'https://openalex.org/W3197623424', 'https://openalex.org/W4321748896', 'https://openalex.org/W4315619796', 'https://openalex.org/W2992583967', 'https://openalex.org/W3000377151', 'https://openalex.org/W4220959747', 'https://openalex.org/W4213244137', 'https://openalex.org/W2913847447', 'https://openalex.org/W3030808598', 'https://openalex.org/W2752796333', 'https://openalex.org/W3089425003', 'https://openalex.org/W2025768430', 'https://openalex.org/W3191938286', 'https://openalex.org/W3169434891', 'https://openalex.org/W3062976875', 'https://openalex.org/W2016690055', 'https://openalex.org/W2053319140', 'https://openalex.org/W1989053318', 'https://openalex.org/W2117292655', 'https://openalex.org/W4221053240', 'https://openalex.org/W2986725225', 'https://openalex.org/W3095441755', 'https://openalex.org/W2053222732', 'https://openalex.org/W1488587546', 'https://openalex.org/W4308220076', 'https://openalex.org/W2079576324', 'https://openalex.org/W3096751000', 'https://openalex.org/W2078070662', 'https://openalex.org/W3132323068', 'https://openalex.org/W2893755931', 'https://openalex.org/W2996962244', 'https://openalex.org/W3205114076', 'https://openalex.org/W3029308247', 'https://openalex.org/W3134185574', 'https://openalex.org/W3144239152', 'https://openalex.org/W4206087583', 'https://openalex.org/W2980789587', 'https://openalex.org/W4283744078', 'https://openalex.org/W4379919536', 'https://openalex.org/W2520345180', 'https://openalex.org/W2159047076', 'https://openalex.org/W3130039396', 'https://openalex.org/W1855237549', 'https://openalex.org/W6802517614', 'https://openalex.org/W6748381668', 'https://openalex.org/W2970971581', 'https://openalex.org/W3081874799', 'https://openalex.org/W3024610894', 'https://openalex.org/W2041741213', 'https://openalex.org/W2037914743', 'https://openalex.org/W2974982237', 'https://openalex.org/W2001900040', 'https://openalex.org/W2097356092', 'https://openalex.org/W2142808579', 'https://openalex.org/W1746465121', 'https://openalex.org/W4206948363']",2023-04-29
https://openalex.org/W4318752101,https://doi.org/10.1109/slt54892.2023.10022796,"Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech","This work adapts two recent architectures of generative models and evaluates their effectiveness for the conversion of whispered speech to normal speech. We incorporate the normal target speech into the training criterion of vector-quantized variational autoencoders (VQ-VAEs) and Mel-GANs, thereby conditioning the systems to recover voiced speech from whispered inputs. Objective and subjective quality measures indicate that both VQ-VAEs and MelGANs can be modified to perform the conversion task. We find that the proposed approaches significantly improve the Mel cepstral distortion (MCD) metric by at least 25% relative to a Disco-GAN baseline. Subjective listening tests suggest that the MelGAN-based system significantly improves naturalness, intelligibility, and voicing compared to the whispered input speech. A novel evaluation measure based on differences between latent speech representations also indicates that our MelGAN-based approach yields improvements relative to the baseline.","['https://openalex.org/W6735204497', 'https://openalex.org/W6687506355', 'https://openalex.org/W2987496713', 'https://openalex.org/W1963637322', 'https://openalex.org/W2962793481', 'https://openalex.org/W3197642003', 'https://openalex.org/W2963341071', 'https://openalex.org/W2962795274', 'https://openalex.org/W6810426031', 'https://openalex.org/W4238582696', 'https://openalex.org/W4237182219', 'https://openalex.org/W2752796333', 'https://openalex.org/W6767111847', 'https://openalex.org/W3100270690', 'https://openalex.org/W6751821244', 'https://openalex.org/W6753018729', 'https://openalex.org/W2067295501', 'https://openalex.org/W3089425003', 'https://openalex.org/W6640963894', 'https://openalex.org/W2963300588', 'https://openalex.org/W6690026940', 'https://openalex.org/W3096359062', 'https://openalex.org/W1538607601', 'https://openalex.org/W6780218876', 'https://openalex.org/W2972471621', 'https://openalex.org/W6802929171', 'https://openalex.org/W1540787848', 'https://openalex.org/W2889417860', 'https://openalex.org/W6792367290', 'https://openalex.org/W1936400760', 'https://openalex.org/W2187089797', 'https://openalex.org/W2911495555', 'https://openalex.org/W2804145368', 'https://openalex.org/W3140957872', 'https://openalex.org/W96541173', 'https://openalex.org/W2963799213', 'https://openalex.org/W3208900277', 'https://openalex.org/W3036601975', 'https://openalex.org/W4319750153', 'https://openalex.org/W1583837637', 'https://openalex.org/W2242818861', 'https://openalex.org/W2970006822']",2023-01-09
https://openalex.org/W4385570501,https://doi.org/10.18653/v1/2023.acl-long.473,Attributable and Scalable Opinion Summarization,"We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.","['https://openalex.org/W1700083040', 'https://openalex.org/W4285150129', 'https://openalex.org/W4205742210', 'https://openalex.org/W4380887855', 'https://openalex.org/W4297971002', 'https://openalex.org/W2160660844', 'https://openalex.org/W4226278401', 'https://openalex.org/W3034999214', 'https://openalex.org/W3170180819', 'https://openalex.org/W2594978815', 'https://openalex.org/W3206890901', 'https://openalex.org/W2963341956', 'https://openalex.org/W3089425003', 'https://openalex.org/W4285118796', 'https://openalex.org/W2963748441', 'https://openalex.org/W2144211451', 'https://openalex.org/W3035043191', 'https://openalex.org/W3186655327', 'https://openalex.org/W4226118367', 'https://openalex.org/W4287824654', 'https://openalex.org/W3173759686', 'https://openalex.org/W3198269165', 'https://openalex.org/W1939882552', 'https://openalex.org/W1522301498', 'https://openalex.org/W2983771799', 'https://openalex.org/W2971074500', 'https://openalex.org/W2913407944', 'https://openalex.org/W4205169447', 'https://openalex.org/W2154652894', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963721761', 'https://openalex.org/W2119913432', 'https://openalex.org/W3173655578', 'https://openalex.org/W2548228487', 'https://openalex.org/W4226157795', 'https://openalex.org/W3215615641', 'https://openalex.org/W4229907684', 'https://openalex.org/W4385245566', 'https://openalex.org/W2110693578', 'https://openalex.org/W3153621364', 'https://openalex.org/W3213990450', 'https://openalex.org/W3191640844', 'https://openalex.org/W2547875792']",2023-01-01
https://openalex.org/W4402716316,https://doi.org/10.1109/cvpr52733.2024.00729,Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis,,"['https://openalex.org/W6798447524', 'https://openalex.org/W6621543089', 'https://openalex.org/W6779997284', 'https://openalex.org/W4313021454', 'https://openalex.org/W6779879114', 'https://openalex.org/W6747491877', 'https://openalex.org/W2108598243', 'https://openalex.org/W6795288823', 'https://openalex.org/W6796242362', 'https://openalex.org/W6800989748', 'https://openalex.org/W3180355996', 'https://openalex.org/W4312633146', 'https://openalex.org/W3096831136', 'https://openalex.org/W6804432862', 'https://openalex.org/W4312400623', 'https://openalex.org/W3035524453', 'https://openalex.org/W4313156423', 'https://openalex.org/W6779823529', 'https://openalex.org/W2603777577', 'https://openalex.org/W2950689937', 'https://openalex.org/W2962770929', 'https://openalex.org/W3035574324', 'https://openalex.org/W2963250244', 'https://openalex.org/W3089425003', 'https://openalex.org/W4312974539', 'https://openalex.org/W4312535427', 'https://openalex.org/W3087124270', 'https://openalex.org/W6788990321', 'https://openalex.org/W6763509872', 'https://openalex.org/W6791353385', 'https://openalex.org/W6790978476', 'https://openalex.org/W6762931180', 'https://openalex.org/W6810595431', 'https://openalex.org/W2423557781', 'https://openalex.org/W2752796333', 'https://openalex.org/W4312326192', 'https://openalex.org/W2016589492', 'https://openalex.org/W4312804044', 'https://openalex.org/W2962785568', 'https://openalex.org/W6796698253', 'https://openalex.org/W3009561768', 'https://openalex.org/W3166396011', 'https://openalex.org/W2963799213', 'https://openalex.org/W4283388932', 'https://openalex.org/W4301914798', 'https://openalex.org/W4287083626', 'https://openalex.org/W3034445277', 'https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W3165647589', 'https://openalex.org/W3215434919', 'https://openalex.org/W3213836217', 'https://openalex.org/W4283373024', 'https://openalex.org/W3213665720', 'https://openalex.org/W4281632497', 'https://openalex.org/W3168053944', 'https://openalex.org/W3129576130', 'https://openalex.org/W1686810756', 'https://openalex.org/W4287282209', 'https://openalex.org/W2952716587', 'https://openalex.org/W967544008', 'https://openalex.org/W3036224891', 'https://openalex.org/W2942415903', 'https://openalex.org/W3206395542', 'https://openalex.org/W2938704169', 'https://openalex.org/W3152733922', 'https://openalex.org/W3036167779', 'https://openalex.org/W3170863103', 'https://openalex.org/W4296564887', 'https://openalex.org/W4288099666', 'https://openalex.org/W2962784628', 'https://openalex.org/W4281771798', 'https://openalex.org/W3128876955', 'https://openalex.org/W2971074500']",2024-06-16
https://openalex.org/W4401409675,https://doi.org/10.1101/2024.08.06.606920,Tokenized and Continuous Embedding Compressions of Protein Sequence and Structure,"Abstract Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000× higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128× along the channel and 8× along the length, while retaining structure information at &lt;2Å scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone . We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.","['https://openalex.org/W2242818861', 'https://openalex.org/W2130479394', 'https://openalex.org/W3195577433', 'https://openalex.org/W4313021454', 'https://openalex.org/W4296032638', 'https://openalex.org/W3177500196', 'https://openalex.org/W3180355996', 'https://openalex.org/W4399114836', 'https://openalex.org/W4400231442', 'https://openalex.org/W2194775991', 'https://openalex.org/W3211983881', 'https://openalex.org/W4280491725', 'https://openalex.org/W3036167779', 'https://openalex.org/W4223581484', 'https://openalex.org/W4376654479', 'https://openalex.org/W3177828909', 'https://openalex.org/W3089425003', 'https://openalex.org/W4327550249', 'https://openalex.org/W4387158294', 'https://openalex.org/W3095583226', 'https://openalex.org/W3209873929', 'https://openalex.org/W4399115900', 'https://openalex.org/W2947590261', 'https://openalex.org/W4312933868', 'https://openalex.org/W2118265845', 'https://openalex.org/W4281485151', 'https://openalex.org/W3106745904', 'https://openalex.org/W3092442149', 'https://openalex.org/W4392271820', 'https://openalex.org/W2102461176', 'https://openalex.org/W4280528743', 'https://openalex.org/W4320855461', 'https://openalex.org/W2752796333', 'https://openalex.org/W4210840673', 'https://openalex.org/W4392381140', 'https://openalex.org/W4281719345']",2024-08-08
https://openalex.org/W4285271096,https://doi.org/10.1007/978-3-031-06427-2_45,CVGAN: Image Generation with Capsule Vector-VAE,,"['https://openalex.org/W6631448583', 'https://openalex.org/W2561196672', 'https://openalex.org/W2889943009', 'https://openalex.org/W2972867623', 'https://openalex.org/W3180355996', 'https://openalex.org/W4233924556', 'https://openalex.org/W3096831136', 'https://openalex.org/W2963073614', 'https://openalex.org/W2963143796', 'https://openalex.org/W6630827713', 'https://openalex.org/W2962770929', 'https://openalex.org/W3089425003', 'https://openalex.org/W3080299296', 'https://openalex.org/W3162745578', 'https://openalex.org/W4212810481', 'https://openalex.org/W3176860327', 'https://openalex.org/W2117539524', 'https://openalex.org/W2972374322', 'https://openalex.org/W1970890968', 'https://openalex.org/W2895526696', 'https://openalex.org/W2123237149']",2022-01-01
https://openalex.org/W4285599837,https://doi.org/10.24963/ijcai.2022/564,PCVAE: Generating Prior Context for Dialogue Response Generation,"Conditional Variational AutoEncoder (CVAE) is promising for modeling one-to-many relationships in dialogue generation, as it can naturally generate many responses from a given context. However, the conventional used continual latent variables in CVAE are more likely to generate generic rather than distinct and specific responses. To resolve this problem, we introduce a novel discrete variable called prior context which enables the generation of favorable responses. Specifically, we present Prior Context VAE (PCVAE), a hierarchical VAE that learns prior context from data automatically for dialogue generation. Meanwhile, we design Active Codeword Transport (ACT) to help the model actively discover potential prior context. Moreover, we propose Autoregressive Compatible Arrangement (ACA) that enables modeling prior context in autoregressive style, which is crucial for selecting appropriate prior context according to a given context. Extensive experiments demonstrate that PCVAE can generate distinct responses and significantly outperforms strong baselines.","['https://openalex.org/W2811079561', 'https://openalex.org/W2963411289', 'https://openalex.org/W3089425003', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251023345', 'https://openalex.org/W2242818861', 'https://openalex.org/W2963799213', 'https://openalex.org/W4394662461', 'https://openalex.org/W3045703328', 'https://openalex.org/W2123301721', 'https://openalex.org/W2971199636', 'https://openalex.org/W1902237438', 'https://openalex.org/W2962717182', 'https://openalex.org/W1521626219', 'https://openalex.org/W2951883832', 'https://openalex.org/W2188365844', 'https://openalex.org/W1959608418', 'https://openalex.org/W2807791032', 'https://openalex.org/W3175460367', 'https://openalex.org/W2250539671', 'https://openalex.org/W2125320996', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963567641', 'https://openalex.org/W2157331557', 'https://openalex.org/W3035044096', 'https://openalex.org/W4287802874', 'https://openalex.org/W2997955173', 'https://openalex.org/W2950142196', 'https://openalex.org/W2158131535', 'https://openalex.org/W1518951372']",2022-07-01
https://openalex.org/W4402959532,https://doi.org/10.1007/978-3-031-72998-0_17,GIVT: Generative Infinite-Vocabulary Transformers,,"['https://openalex.org/W4315705838', 'https://openalex.org/W4313484371', 'https://openalex.org/W4313021454', 'https://openalex.org/W6779879114', 'https://openalex.org/W6730998768', 'https://openalex.org/W3034469748', 'https://openalex.org/W6601796474', 'https://openalex.org/W3180355996', 'https://openalex.org/W2963096510', 'https://openalex.org/W6732322125', 'https://openalex.org/W2753738274', 'https://openalex.org/W6603963165', 'https://openalex.org/W4386065765', 'https://openalex.org/W2999219213', 'https://openalex.org/W3173785189', 'https://openalex.org/W4386901782', 'https://openalex.org/W3089425003', 'https://openalex.org/W2977321685', 'https://openalex.org/W4386071584', 'https://openalex.org/W4312446817', 'https://openalex.org/W3171884590', 'https://openalex.org/W4390874546', 'https://openalex.org/W3034743539', 'https://openalex.org/W6763509872', 'https://openalex.org/W4390872297', 'https://openalex.org/W4312933868', 'https://openalex.org/W2117539524', 'https://openalex.org/W4285981784', 'https://openalex.org/W125693051', 'https://openalex.org/W4214893857', 'https://openalex.org/W4285289426', 'https://openalex.org/W6739901393', 'https://openalex.org/W7004972456', 'https://openalex.org/W3216270236', 'https://openalex.org/W6802517614', 'https://openalex.org/W4283388932', 'https://openalex.org/W3172942063', 'https://openalex.org/W3177318507']",2024-09-29
https://openalex.org/W4389633906,https://doi.org/10.1109/access.2023.3341919,Phase-Aware Speech Enhancement With Complex Wiener Filter,"In speech enhancement, accurate phase reconstruction can significantly improve speech quality. While phase-aware speech enhancement methods using the complex ideal ratio mask (cIRM) have shown promise, the estimation difficulty of the phase is shared with the real and imaginary parts of the cIRM. The pattern lacking in the imaginary part poses particular difficulties. To address this issue, we proposed a phase-aware speech enhancement method that uses a complex Wiener filter, which delegates the estimation of speech and noise amplitude properties and the phase property to different models, mitigating the issues with the cIRM and improving the effectiveness of neural-network training. Our method uses a speech-variance estimation model with a noise-robust vector-quantized variational autoencoder and a phase corrector that maximizes the scale-invariant signal-to-noise ratio in the time domain. To further improve speech-variance estimation, we propose a loss function that uses a categorical distribution of fundamental frequency (F0) for enhancing the spectral fine structure of estimated speech variance. We evaluated our method on the open dataset released by Valentini et al. to directly compare it with other speech-enhancement methods. Our method achieved a perceptual evaluation of speech quality score of 2.86 and short-time objective intelligibility score of 0.94, better than the state-of-the-art method based on cIRM estimation during the 2020 Deep Noise Challenge. Our comprehensive analysis shows that incorporating the proposed loss function for spectral-fine-structure enhancement improves speech quality, especially when the F0 is low.","['https://openalex.org/W2128653836', 'https://openalex.org/W2113131123', 'https://openalex.org/W3147539069', 'https://openalex.org/W2013590534', 'https://openalex.org/W2963341071', 'https://openalex.org/W2952218014', 'https://openalex.org/W6798721538', 'https://openalex.org/W2044893557', 'https://openalex.org/W160800111', 'https://openalex.org/W190004713', 'https://openalex.org/W2405774341', 'https://openalex.org/W2790760353', 'https://openalex.org/W3092864146', 'https://openalex.org/W2027840501', 'https://openalex.org/W2766672686', 'https://openalex.org/W2883322837', 'https://openalex.org/W3131332223', 'https://openalex.org/W6640963894', 'https://openalex.org/W2039844283', 'https://openalex.org/W2070126272', 'https://openalex.org/W3097312999', 'https://openalex.org/W3045520545', 'https://openalex.org/W3096408984', 'https://openalex.org/W3160085755', 'https://openalex.org/W3095057960', 'https://openalex.org/W3198680319', 'https://openalex.org/W2291877678', 'https://openalex.org/W6738884980', 'https://openalex.org/W6757632829', 'https://openalex.org/W3135915893', 'https://openalex.org/W4297841734', 'https://openalex.org/W2752796333', 'https://openalex.org/W2514828952', 'https://openalex.org/W3100270690', 'https://openalex.org/W2022554507', 'https://openalex.org/W4225298533', 'https://openalex.org/W6729448088', 'https://openalex.org/W2094721231', 'https://openalex.org/W4232282348', 'https://openalex.org/W3199367817', 'https://openalex.org/W3120336970', 'https://openalex.org/W3089425003', 'https://openalex.org/W1552314771', 'https://openalex.org/W2067295501', 'https://openalex.org/W2802304149', 'https://openalex.org/W6751512325', 'https://openalex.org/W6762114000', 'https://openalex.org/W3081955528', 'https://openalex.org/W3117879684', 'https://openalex.org/W2889597349', 'https://openalex.org/W2963452667', 'https://openalex.org/W2949756029', 'https://openalex.org/W3099330747']",2023-01-01
https://openalex.org/W4389519310,https://doi.org/10.18653/v1/2023.emnlp-main.1032,Reader: Model-based language-instructed reinforcement learning,"We explore how we can build accurate world models, which are partially specified by language, and how we can plan with them in the face of novelty and uncertainty. We propose the first model-based reinforcement learning approach to tackle the environment Read To Fight Monsters (Zhong et al., 2019), a grounded policy learning problem. In RTFM an agent has to reason over a set of rules and a goal, both described in a language manual, and the observations, while taking into account the uncertainty arising from the stochasticity of the environment, in order to generalize successfully its policy to test episodes. We demonstrate the superior performance and sample efficiency of our model-based approach to the existing model-free SOTA agents in eight variants of RTFM. Furthermore, we show how the agent’s plans can be inspected, which represents progress towards more interpretable agents.","['https://openalex.org/W4302010357', 'https://openalex.org/W3122690883', 'https://openalex.org/W3010805364', 'https://openalex.org/W3037871539', 'https://openalex.org/W3214506775', 'https://openalex.org/W3172082158', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963799213', 'https://openalex.org/W4289294484', 'https://openalex.org/W2995298643', 'https://openalex.org/W1625390266', 'https://openalex.org/W3122267274', 'https://openalex.org/W3094502228', 'https://openalex.org/W2979727876', 'https://openalex.org/W3089425003', 'https://openalex.org/W2902729397', 'https://openalex.org/W2994943647', 'https://openalex.org/W3034758614', 'https://openalex.org/W2257979135', 'https://openalex.org/W3118210634', 'https://openalex.org/W2980853364', 'https://openalex.org/W2949059942']",2023-01-01
https://openalex.org/W4410589960,https://doi.org/10.1007/978-3-031-92805-5_11,SQUAD: Scalar Quantized Representation Learning for Unsupervised Anomaly Detection and Localization,,"['https://openalex.org/W2981613960', 'https://openalex.org/W2948982773', 'https://openalex.org/W3034314048', 'https://openalex.org/W2809705434', 'https://openalex.org/W3153381206', 'https://openalex.org/W3147184966', 'https://openalex.org/W4312772600', 'https://openalex.org/W4390872785', 'https://openalex.org/W3120612350', 'https://openalex.org/W3183588514', 'https://openalex.org/W4200434100', 'https://openalex.org/W4292264335', 'https://openalex.org/W3089425003', 'https://openalex.org/W3166166117', 'https://openalex.org/W4393154503', 'https://openalex.org/W2963351448', 'https://openalex.org/W4386065890', 'https://openalex.org/W4312443924', 'https://openalex.org/W2962914239', 'https://openalex.org/W3209793239', 'https://openalex.org/W1966832848', 'https://openalex.org/W3212044949', 'https://openalex.org/W3169651898', 'https://openalex.org/W4312570668', 'https://openalex.org/W4386075837', 'https://openalex.org/W2752796333', 'https://openalex.org/W2982602185', 'https://openalex.org/W2133665775', 'https://openalex.org/W4386076493', 'https://openalex.org/W4214694907', 'https://openalex.org/W3092704883', 'https://openalex.org/W4312239247', 'https://openalex.org/W4386071651', 'https://openalex.org/W4390872231', 'https://openalex.org/W4312605624', 'https://openalex.org/W3101017490']",2025-01-01
https://openalex.org/W4392411961,https://doi.org/10.1109/ijcb57857.2023.10449102,GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes,"Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.","['https://openalex.org/W3042230461', 'https://openalex.org/W2029241756', 'https://openalex.org/W4214722801', 'https://openalex.org/W3128064349', 'https://openalex.org/W6774314701', 'https://openalex.org/W4319298095', 'https://openalex.org/W4322743576', 'https://openalex.org/W4295215046', 'https://openalex.org/W2078179989', 'https://openalex.org/W3180355996', 'https://openalex.org/W4205268692', 'https://openalex.org/W6795007937', 'https://openalex.org/W2924334974', 'https://openalex.org/W1536680647', 'https://openalex.org/W4200634815', 'https://openalex.org/W4319299814', 'https://openalex.org/W3158576998', 'https://openalex.org/W4206286792', 'https://openalex.org/W6755977528', 'https://openalex.org/W6765779288', 'https://openalex.org/W3114632476', 'https://openalex.org/W3214185843', 'https://openalex.org/W6631190155', 'https://openalex.org/W3089425003', 'https://openalex.org/W4214538268', 'https://openalex.org/W4304080803', 'https://openalex.org/W6841592900', 'https://openalex.org/W4312674262', 'https://openalex.org/W3035225512', 'https://openalex.org/W4313056154', 'https://openalex.org/W4287509562', 'https://openalex.org/W2983925976', 'https://openalex.org/W3153832461', 'https://openalex.org/W4297981470', 'https://openalex.org/W6849932897', 'https://openalex.org/W6762931180', 'https://openalex.org/W4295338723', 'https://openalex.org/W3204207201', 'https://openalex.org/W2948058585', 'https://openalex.org/W6810831447', 'https://openalex.org/W4312671789', 'https://openalex.org/W6639794760', 'https://openalex.org/W6686164453', 'https://openalex.org/W3195852174', 'https://openalex.org/W4313145975', 'https://openalex.org/W6777179611', 'https://openalex.org/W2752796333', 'https://openalex.org/W4233762729', 'https://openalex.org/W3110190397', 'https://openalex.org/W4312612133', 'https://openalex.org/W3187415662', 'https://openalex.org/W2983796203', 'https://openalex.org/W2963076818', 'https://openalex.org/W2998792609', 'https://openalex.org/W6802517614', 'https://openalex.org/W2739325416', 'https://openalex.org/W2104335344', 'https://openalex.org/W3215615641', 'https://openalex.org/W4386076288', 'https://openalex.org/W6849682310', 'https://openalex.org/W6781523853', 'https://openalex.org/W2963854019', 'https://openalex.org/W4312652114', 'https://openalex.org/W4214564952', 'https://openalex.org/W2183341477', 'https://openalex.org/W4320853786', 'https://openalex.org/W2971074500', 'https://openalex.org/W4287245321', 'https://openalex.org/W3206395542', 'https://openalex.org/W4289785378', 'https://openalex.org/W2963799213', 'https://openalex.org/W4320085220', 'https://openalex.org/W3125736290', 'https://openalex.org/W4301206121', 'https://openalex.org/W1904924995', 'https://openalex.org/W3005680577', 'https://openalex.org/W1522301498', 'https://openalex.org/W3026092005', 'https://openalex.org/W2899663614', 'https://openalex.org/W3046205681']",2023-09-25
https://openalex.org/W4394625601,https://doi.org/10.1109/wacv57701.2024.00508,ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-based Blind Face Restoration,"We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.","['https://openalex.org/W3174166237', 'https://openalex.org/W2963676087', 'https://openalex.org/W4224313400', 'https://openalex.org/W2969985801', 'https://openalex.org/W2955625300', 'https://openalex.org/W3180355996', 'https://openalex.org/W3035570181', 'https://openalex.org/W4312400623', 'https://openalex.org/W6765779288', 'https://openalex.org/W3173435145', 'https://openalex.org/W2950689937', 'https://openalex.org/W2962770929', 'https://openalex.org/W3035574324', 'https://openalex.org/W2618530766', 'https://openalex.org/W3089425003', 'https://openalex.org/W3106746951', 'https://openalex.org/W3035048074', 'https://openalex.org/W2963089432', 'https://openalex.org/W4306732596', 'https://openalex.org/W3179253672', 'https://openalex.org/W3034352949', 'https://openalex.org/W2102166818', 'https://openalex.org/W4312896571', 'https://openalex.org/W3176913662', 'https://openalex.org/W2963743395', 'https://openalex.org/W3178406257', 'https://openalex.org/W4385245566', 'https://openalex.org/W3035002246', 'https://openalex.org/W3180391059', 'https://openalex.org/W4312326192', 'https://openalex.org/W3097589886', 'https://openalex.org/W3035022492', 'https://openalex.org/W3021195052', 'https://openalex.org/W3167297682', 'https://openalex.org/W2895542678', 'https://openalex.org/W2962785568', 'https://openalex.org/W2919046835', 'https://openalex.org/W4312568827', 'https://openalex.org/W3093222528', 'https://openalex.org/W2969834519', 'https://openalex.org/W4283373024', 'https://openalex.org/W4301206121', 'https://openalex.org/W1522301498', 'https://openalex.org/W1686810756', 'https://openalex.org/W4221142438']",2024-01-03
https://openalex.org/W4294831550,https://doi.org/10.1007/978-3-031-15919-0_25,Neural-Gas VAE,,"['https://openalex.org/W1787224781', 'https://openalex.org/W3100270690', 'https://openalex.org/W2194775991', 'https://openalex.org/W3089425003', 'https://openalex.org/W2972542211', 'https://openalex.org/W2112796159', 'https://openalex.org/W2148554573', 'https://openalex.org/W4229494842', 'https://openalex.org/W2053186076', 'https://openalex.org/W2616247523', 'https://openalex.org/W3003449356', 'https://openalex.org/W3102564565']",2022-01-01
https://openalex.org/W4389298345,https://doi.org/10.1007/978-981-99-8696-5_5,Text-Conditioned Graph Generation Using Discrete Graph Variational Autoencoders,,"['https://openalex.org/W2124637492', 'https://openalex.org/W2784007581', 'https://openalex.org/W6604212504', 'https://openalex.org/W2913058692', 'https://openalex.org/W4385572894', 'https://openalex.org/W6604372272', 'https://openalex.org/W2102907934', 'https://openalex.org/W3089425003', 'https://openalex.org/W2970709315', 'https://openalex.org/W4235019172', 'https://openalex.org/W6631455383', 'https://openalex.org/W6790978476', 'https://openalex.org/W2160268549', 'https://openalex.org/W4312933868', 'https://openalex.org/W1973716318', 'https://openalex.org/W4289436753', 'https://openalex.org/W2752796333', 'https://openalex.org/W3186540330', 'https://openalex.org/W2112090702', 'https://openalex.org/W2963101956', 'https://openalex.org/W6607144799', 'https://openalex.org/W3103967557']",2023-12-04
https://openalex.org/W4389524339,https://doi.org/10.18653/v1/2023.findings-emnlp.893,Vector-Quantized Prompt Learning for Paraphrase Generation,"Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance.","['https://openalex.org/W2949832505', 'https://openalex.org/W4292779060', 'https://openalex.org/W3197133407', 'https://openalex.org/W2755124548', 'https://openalex.org/W4280581589', 'https://openalex.org/W2953369829', 'https://openalex.org/W2132019450', 'https://openalex.org/W2167170026', 'https://openalex.org/W2126034021', 'https://openalex.org/W4307079201', 'https://openalex.org/W4287906183', 'https://openalex.org/W3034531294', 'https://openalex.org/W1861492603', 'https://openalex.org/W2997279057', 'https://openalex.org/W2118119027', 'https://openalex.org/W2962953307', 'https://openalex.org/W2103081392', 'https://openalex.org/W3089425003', 'https://openalex.org/W4285150129', 'https://openalex.org/W2016172157', 'https://openalex.org/W3177034458', 'https://openalex.org/W4288089799', 'https://openalex.org/W4386506836', 'https://openalex.org/W3176490950', 'https://openalex.org/W174630521', 'https://openalex.org/W4385245566', 'https://openalex.org/W3035368872', 'https://openalex.org/W2131726681', 'https://openalex.org/W4281477763', 'https://openalex.org/W4283794478', 'https://openalex.org/W2964202145', 'https://openalex.org/W2540645427']",2023-01-01
https://openalex.org/W4401892801,https://doi.org/10.3390/electronics13173364,TAG2G: A Diffusion-Based Approach to Interlocutor-Aware Co-Speech Gesture Generation,"Extended reality (XR) systems are about to be integrated into our daily lives and will provide support in a variety of fields such as education and coaching. Enhancing user experience demands agents that are capable of displaying realistic affective and social behaviors within these systems, and, as a prerequisite, with the capability of understanding their interaction partner and responding appropriately. Based on our literature review of recent works published in the field of co-speech gesture generation, researchers have developed complex models capable of generating gestures characterized by a high level of human-likeness and speaker appropriateness. Nevertheless, this is only true in settings where the agent has an active status (i.e., the agent acts as the speaker), or it is delivering a monologue in a non-interactive setting. However, as illustrated in multiple works and competitions like the GENEA Challenge, these models remain inadequate in generating interlocutor-aware gestures. We consider interlocutor-aware gesture generation the process of displaying gestures that take into account the conversation partner’s behavior. Moreover, in settings where the agent is the listener, generated gestures lack the level of naturalness that we expect from a face-to-face conversation. To overcome these issues, we have designed a pipeline, called TAG2G, composed of a diffusion model, which was demonstrated to be a stable and powerful tool in gesture generation, and a vector-quantized variational auto-encoder (VQVAE), widely employed to produce meaningful gesture embeddings. Refocusing from monadic to dyadic multimodal input settings (i.e., taking into account text, audio, and previous gestures of both participants of a conversation) allows us to explore and infer the complex interaction mechanisms that lie in a balanced two-sided conversation. As per our results, a multi-agent conversational input setup improves the generated gestures’ appropriateness with respect to the conversational counterparts. Conversely, when the agent is speaking, a monadic approach performs better in terms of the generated gestures’ appropriateness in relation to the speech.","['https://openalex.org/W4364377334', 'https://openalex.org/W2903438696', 'https://openalex.org/W6735044664', 'https://openalex.org/W3119399459', 'https://openalex.org/W4387421378', 'https://openalex.org/W3132983845', 'https://openalex.org/W2008208299', 'https://openalex.org/W1575301740', 'https://openalex.org/W2135431835', 'https://openalex.org/W4221142137', 'https://openalex.org/W3133090439', 'https://openalex.org/W4292945985', 'https://openalex.org/W1208039178', 'https://openalex.org/W4312719027', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W2780932362', 'https://openalex.org/W4312309978', 'https://openalex.org/W4312437946', 'https://openalex.org/W4387423983', 'https://openalex.org/W4298128030', 'https://openalex.org/W4285981714', 'https://openalex.org/W4385764101', 'https://openalex.org/W4386755513', 'https://openalex.org/W2022214202', 'https://openalex.org/W4389274031', 'https://openalex.org/W4387446259', 'https://openalex.org/W4387968147', 'https://openalex.org/W3083173864', 'https://openalex.org/W2752796333', 'https://openalex.org/W3089425003', 'https://openalex.org/W6779823529', 'https://openalex.org/W4388979610', 'https://openalex.org/W4287509562', 'https://openalex.org/W3204559413', 'https://openalex.org/W203345490', 'https://openalex.org/W3153551559', 'https://openalex.org/W3036167779', 'https://openalex.org/W2598745374', 'https://openalex.org/W3125775899']",2024-08-24
https://openalex.org/W4402351835,https://doi.org/10.1109/ijcnn60899.2024.10650185,"VQStarGAN: An Unseen-to-Unseen, Zero-Shot, Non-Parallel, Unsupervised Voice Conversion",,"['https://openalex.org/W3196667132', 'https://openalex.org/W2902070858', 'https://openalex.org/W2972667718', 'https://openalex.org/W6775421620', 'https://openalex.org/W3160584619', 'https://openalex.org/W2156142001', 'https://openalex.org/W3089425003', 'https://openalex.org/W3096524539', 'https://openalex.org/W2932319787', 'https://openalex.org/W2603777577', 'https://openalex.org/W2526425061', 'https://openalex.org/W6762533536', 'https://openalex.org/W2962793481', 'https://openalex.org/W6772349387', 'https://openalex.org/W3015338123', 'https://openalex.org/W4221141917', 'https://openalex.org/W2998572311', 'https://openalex.org/W2908510526', 'https://openalex.org/W3015434413']",2024-06-30
https://openalex.org/W4408866119,https://doi.org/10.1109/eiecc64539.2024.10929116,Multi-head Discretization improves Learning Ability in Vector Quantization,,"['https://openalex.org/W3137147200', 'https://openalex.org/W3153990350', 'https://openalex.org/W2042492924', 'https://openalex.org/W3180355996', 'https://openalex.org/W4407933263', 'https://openalex.org/W4398164016', 'https://openalex.org/W3089425003', 'https://openalex.org/W3215615641', 'https://openalex.org/W3140854437', 'https://openalex.org/W3155739706']",2024-12-27
https://openalex.org/W4412837214,https://doi.org/10.1109/taslpro.2025.3594955,Optimizing Contextual Speech Recognition Using Vector Quantization for Efficient Retrieval,,"['https://openalex.org/W4388017359', 'https://openalex.org/W2916997151', 'https://openalex.org/W2964107261', 'https://openalex.org/W2898599211', 'https://openalex.org/W2972625221', 'https://openalex.org/W3140235797', 'https://openalex.org/W2395440424', 'https://openalex.org/W2889012072', 'https://openalex.org/W3008037978', 'https://openalex.org/W3163560333', 'https://openalex.org/W2886319145', 'https://openalex.org/W4225985539', 'https://openalex.org/W4224918838', 'https://openalex.org/W2937402758', 'https://openalex.org/W3097794466', 'https://openalex.org/W4226462878', 'https://openalex.org/W3202725408', 'https://openalex.org/W4402111737', 'https://openalex.org/W2064675550', 'https://openalex.org/W4385245566', 'https://openalex.org/W6861581687', 'https://openalex.org/W4402112104', 'https://openalex.org/W4391021623', 'https://openalex.org/W4392903872', 'https://openalex.org/W4408352893', 'https://openalex.org/W4372260607', 'https://openalex.org/W4385822295', 'https://openalex.org/W4402683949', 'https://openalex.org/W4375869128', 'https://openalex.org/W2973172693', 'https://openalex.org/W4385822583', 'https://openalex.org/W4392903019', 'https://openalex.org/W2752796333', 'https://openalex.org/W6857461762', 'https://openalex.org/W4408345654', 'https://openalex.org/W4372260124', 'https://openalex.org/W4392902876', 'https://openalex.org/W6967539382', 'https://openalex.org/W4401044228', 'https://openalex.org/W4402112322', 'https://openalex.org/W4319862261', 'https://openalex.org/W4402671610', 'https://openalex.org/W4402111987', 'https://openalex.org/W4392904569', 'https://openalex.org/W4385822246', 'https://openalex.org/W4389519450', 'https://openalex.org/W4402671877', 'https://openalex.org/W4391021656', 'https://openalex.org/W4385822970', 'https://openalex.org/W4375869024', 'https://openalex.org/W2526425061', 'https://openalex.org/W6853998256', 'https://openalex.org/W6869902183', 'https://openalex.org/W4392902925', 'https://openalex.org/W4392903288', 'https://openalex.org/W6860809563', 'https://openalex.org/W4372259777', 'https://openalex.org/W4221160683', 'https://openalex.org/W4391021773', 'https://openalex.org/W6847363464', 'https://openalex.org/W6876279097', 'https://openalex.org/W3146366485', 'https://openalex.org/W6838322825', 'https://openalex.org/W6857061474', 'https://openalex.org/W3198004110', 'https://openalex.org/W4225289150', 'https://openalex.org/W4319862232', 'https://openalex.org/W4385822684', 'https://openalex.org/W2955058313', 'https://openalex.org/W4375869422', 'https://openalex.org/W4402670461', 'https://openalex.org/W4244017338', 'https://openalex.org/W6762931180', 'https://openalex.org/W3215615641', 'https://openalex.org/W6852581948', 'https://openalex.org/W3166762869', 'https://openalex.org/W4312974539', 'https://openalex.org/W6751821244', 'https://openalex.org/W3089425003', 'https://openalex.org/W6802517614', 'https://openalex.org/W6857566199', 'https://openalex.org/W4392910591', 'https://openalex.org/W6690026940', 'https://openalex.org/W4375869390', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963250244', 'https://openalex.org/W6850625674', 'https://openalex.org/W3197478142', 'https://openalex.org/W2739883972', 'https://openalex.org/W4385567350', 'https://openalex.org/W6858023062', 'https://openalex.org/W6777399232', 'https://openalex.org/W6864984424']",2025-01-01
https://openalex.org/W4413278281,https://doi.org/10.1109/icip55913.2025.11084395,Rapid Object Modeling Initialization for Vector Quantized-Variational AutoEncoder,,"['https://openalex.org/W4408146288', 'https://openalex.org/W3215615641', 'https://openalex.org/W3180355996', 'https://openalex.org/W3089425003', 'https://openalex.org/W2962837320', 'https://openalex.org/W3172295205', 'https://openalex.org/W4388630573', 'https://openalex.org/W4205778870']",2025-08-18
https://openalex.org/W4413925991,https://doi.org/10.1109/icra55743.2025.11127383,Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving,,"['https://openalex.org/W3127647470', 'https://openalex.org/W4391985078', 'https://openalex.org/W4307234290', 'https://openalex.org/W3182474098', 'https://openalex.org/W4247726808', 'https://openalex.org/W4323796747', 'https://openalex.org/W1965455100', 'https://openalex.org/W2056877664', 'https://openalex.org/W2089080831', 'https://openalex.org/W2011931151', 'https://openalex.org/W1973934431', 'https://openalex.org/W2344057403', 'https://openalex.org/W2044519836', 'https://openalex.org/W3093456443', 'https://openalex.org/W2957228561', 'https://openalex.org/W2886622679', 'https://openalex.org/W2734024016', 'https://openalex.org/W2100411291', 'https://openalex.org/W2626702320', 'https://openalex.org/W2127135061', 'https://openalex.org/W2580495915', 'https://openalex.org/W1833552892', 'https://openalex.org/W1590571558', 'https://openalex.org/W1914302117', 'https://openalex.org/W2102847084', 'https://openalex.org/W2794908222', 'https://openalex.org/W4385430497', 'https://openalex.org/W3108316907', 'https://openalex.org/W4313009245', 'https://openalex.org/W3162391496', 'https://openalex.org/W3089425003', 'https://openalex.org/W3180355996', 'https://openalex.org/W4403337227', 'https://openalex.org/W2109606373', 'https://openalex.org/W2594201198', 'https://openalex.org/W2949676527']",2025-05-19
https://openalex.org/W2982223350,https://doi.org/10.1109/icassp40776.2020.9054458,Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders,"We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.\n","['https://openalex.org/W2947445680', 'https://openalex.org/W6607333740', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780226713', 'https://openalex.org/W2964089206', 'https://openalex.org/W2972451902', 'https://openalex.org/W6737778391', 'https://openalex.org/W2270070752', 'https://openalex.org/W6766673545', 'https://openalex.org/W6768021236', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963425185', 'https://openalex.org/W2972943112', 'https://openalex.org/W2842511635', 'https://openalex.org/W2979476256', 'https://openalex.org/W2973049979', 'https://openalex.org/W3100270690', 'https://openalex.org/W2998649947', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962739339', 'https://openalex.org/W6631190155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2747874407', 'https://openalex.org/W6674330103', 'https://openalex.org/W2883409523', 'https://openalex.org/W2996428491', 'https://openalex.org/W2943493972', 'https://openalex.org/W4297808394', 'https://openalex.org/W3125709657', 'https://openalex.org/W2613904329', 'https://openalex.org/W2336585117', 'https://openalex.org/W2975059944', 'https://openalex.org/W2787560479', 'https://openalex.org/W2965373594', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W2996383576', 'https://openalex.org/W179875071', 'https://openalex.org/W2896457183', 'https://openalex.org/W2626778328', 'https://openalex.org/W3037932933', 'https://openalex.org/W2095705004']",2020-04-09
https://openalex.org/W3041561163,https://doi.org/10.1109/taslp.2021.3095662,TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech,"We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.\n","['https://openalex.org/W3096171739', 'https://openalex.org/W6763701032', 'https://openalex.org/W2927746189', 'https://openalex.org/W6768021236', 'https://openalex.org/W2962739339', 'https://openalex.org/W6607333740', 'https://openalex.org/W2127141656', 'https://openalex.org/W6739901393', 'https://openalex.org/W6784776607', 'https://openalex.org/W6763238093', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W2982223350', 'https://openalex.org/W3003875258', 'https://openalex.org/W6777232839', 'https://openalex.org/W3096626135', 'https://openalex.org/W3096485810', 'https://openalex.org/W6778265221', 'https://openalex.org/W6777859476', 'https://openalex.org/W2794209590', 'https://openalex.org/W2125496931', 'https://openalex.org/W2962901777', 'https://openalex.org/W2995181338', 'https://openalex.org/W2002342963', 'https://openalex.org/W2964227577', 'https://openalex.org/W2077804127', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3096017728', 'https://openalex.org/W3015265920', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W3100270690', 'https://openalex.org/W2947445680', 'https://openalex.org/W2982039329', 'https://openalex.org/W2973157397', 'https://openalex.org/W3033038061', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W6773205534', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016181583', 'https://openalex.org/W6631362777', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972451902', 'https://openalex.org/W1494198834', 'https://openalex.org/W6757817989', 'https://openalex.org/W3015412890', 'https://openalex.org/W2936774411', 'https://openalex.org/W3024182269', 'https://openalex.org/W1635512741', 'https://openalex.org/W2943493972', 'https://openalex.org/W2981991061', 'https://openalex.org/W3125709657', 'https://openalex.org/W2980708516', 'https://openalex.org/W4385245566', 'https://openalex.org/W1524333225', 'https://openalex.org/W3160345865', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964303116', 'https://openalex.org/W2950577311', 'https://openalex.org/W3030987249', 'https://openalex.org/W1614298861', 'https://openalex.org/W179875071', 'https://openalex.org/W3099782249', 'https://openalex.org/W3102342027', 'https://openalex.org/W3036601975', 'https://openalex.org/W2947454875', 'https://openalex.org/W2979476256', 'https://openalex.org/W3095292526', 'https://openalex.org/W2950813464', 'https://openalex.org/W3148040514', 'https://openalex.org/W2842511635', 'https://openalex.org/W2996428491', 'https://openalex.org/W4297808394', 'https://openalex.org/W3104896896', 'https://openalex.org/W2908510526', 'https://openalex.org/W1608367484', 'https://openalex.org/W2949667497', 'https://openalex.org/W2970597249', 'https://openalex.org/W3015412285', 'https://openalex.org/W3198858531', 'https://openalex.org/W4288348042', 'https://openalex.org/W2988736778', 'https://openalex.org/W2996383576', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963341956', 'https://openalex.org/W3026957705', 'https://openalex.org/W3026842484', 'https://openalex.org/W2963403868']",2021-01-01
https://openalex.org/W2940544976,https://doi.org/10.21437/interspeech.2019-2904,The Zero Resource Speech Challenge 2019: TTS Without T,"We present the Zero Resource Speech Challenge 2019, which proposes to build a\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\n(text-to-speech without text). We provide raw audio for a target voice in an\nunknown language (the Voice dataset), but no alignment, text or labels.\nParticipants must discover subword units in an unsupervised way (using the Unit\nDiscovery dataset) and align them to the voice recordings in a way that works\nbest for the purpose of synthesizing novel utterances from novel speakers,\nsimilar to the target speaker's voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit discovery\nplus a standard TTS system, and a topline TTS using gold phoneme\ntranscriptions. We present an overview of the 19 submitted systems from 10\nteams and discuss the main results.\n","['https://openalex.org/W3125709657', 'https://openalex.org/W2962693497', 'https://openalex.org/W2963830550', 'https://openalex.org/W1524333225', 'https://openalex.org/W2892140764', 'https://openalex.org/W2134202996', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W2774848319', 'https://openalex.org/W2962699523', 'https://openalex.org/W2973013862', 'https://openalex.org/W2947445680', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963620343', 'https://openalex.org/W2950414763', 'https://openalex.org/W2547039119', 'https://openalex.org/W2598638573', 'https://openalex.org/W2745710152', 'https://openalex.org/W2519091744', 'https://openalex.org/W2972374322', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964135678', 'https://openalex.org/W2532494225', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972964185', 'https://openalex.org/W2346964103', 'https://openalex.org/W2787447541', 'https://openalex.org/W2964115348', 'https://openalex.org/W2020607164']",2019-09-13
https://openalex.org/W3096524539,https://doi.org/10.21437/interspeech.2020-1443,VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture,"Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content.It is still a challenging work, especially in a one-shot setting.Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers.The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN).However, the imperfect disentanglement may harm the quality of output speech.In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system.We find that to leverage the U-Net architecture, a strong information bottleneck is necessary.The VQ-based method, which quantizes the latent vectors, can serve the purpose.The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.","['https://openalex.org/W2972659941', 'https://openalex.org/W2532494225', 'https://openalex.org/W2970006822', 'https://openalex.org/W1901129140', 'https://openalex.org/W4294643831', 'https://openalex.org/W2973154337', 'https://openalex.org/W2902070858', 'https://openalex.org/W2518172956', 'https://openalex.org/W2603777577', 'https://openalex.org/W2963539064', 'https://openalex.org/W2156142001', 'https://openalex.org/W4288337064', 'https://openalex.org/W2947445680', 'https://openalex.org/W2945478979', 'https://openalex.org/W3114301328', 'https://openalex.org/W2120605154', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963830550', 'https://openalex.org/W2502312327', 'https://openalex.org/W2948211236', 'https://openalex.org/W2527729766', 'https://openalex.org/W3015434413', 'https://openalex.org/W2963799213', 'https://openalex.org/W2105160541', 'https://openalex.org/W2888922217', 'https://openalex.org/W3015419784', 'https://openalex.org/W3015805741', 'https://openalex.org/W4320013936']",2020-10-25
https://openalex.org/W2995680346,https://doi.org/10.48550/arxiv.1911.09602,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded\n Speech,"In this paper, we present a method for learning discrete linguistic units by\nincorporating vector quantization layers into neural models of visually\ngrounded speech. We show that our method is capable of capturing both\nword-level and sub-word units, depending on how it is configured. What\ndifferentiates this paper from prior work on speech unit learning is the choice\nof training objective. Rather than using a reconstruction-based loss, we use a\ndiscriminative, multimodal grounding objective which forces the learned units\nto be useful for semantic image retrieval. We evaluate the sub-word units on\nthe ZeroSpeech 2019 challenge, achieving a 27.3\\% reduction in ABX error rate\nover the top-performing submission, while keeping the bitrate approximately the\nsame. We also present experiments demonstrating the noise robustness of these\nunits. Finally, we show that a model with multiple quantizers can\nsimultaneously learn phone-like detectors at a lower layer and word-like\ndetectors at a higher layer. We show that these detectors are highly accurate,\ndiscovering 279 words with an F1 score of greater than 0.5.\n","['https://openalex.org/W2586148577', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963330681', 'https://openalex.org/W2100768664', 'https://openalex.org/W2972345028', 'https://openalex.org/W3125709657', 'https://openalex.org/W2889313720', 'https://openalex.org/W2964249784', 'https://openalex.org/W2947445680', 'https://openalex.org/W2963317665', 'https://openalex.org/W2964001192', 'https://openalex.org/W2953114965', 'https://openalex.org/W2950414763', 'https://openalex.org/W30845872', 'https://openalex.org/W2295297373', 'https://openalex.org/W2972867623', 'https://openalex.org/W2468716020', 'https://openalex.org/W2964115348', 'https://openalex.org/W2106053110', 'https://openalex.org/W2107917162', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963618559', 'https://openalex.org/W2078769636', 'https://openalex.org/W2920166246', 'https://openalex.org/W2118841860', 'https://openalex.org/W2963525826', 'https://openalex.org/W2938991416', 'https://openalex.org/W2113896236', 'https://openalex.org/W2989358187', 'https://openalex.org/W2962850167', 'https://openalex.org/W2395899413', 'https://openalex.org/W2962753610', 'https://openalex.org/W2973157397', 'https://openalex.org/W2962824709', 'https://openalex.org/W2767754137', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963417023', 'https://openalex.org/W2134670479', 'https://openalex.org/W2117041980', 'https://openalex.org/W2971709506', 'https://openalex.org/W2988907666', 'https://openalex.org/W2556930864', 'https://openalex.org/W1942713348', 'https://openalex.org/W1778492285', 'https://openalex.org/W2114347655', 'https://openalex.org/W2126203737', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962732076', 'https://openalex.org/W2973026522', 'https://openalex.org/W2483390977', 'https://openalex.org/W2758849341', 'https://openalex.org/W385555557', 'https://openalex.org/W2972943112', 'https://openalex.org/W2964099072', 'https://openalex.org/W2347098582', 'https://openalex.org/W2404799143', 'https://openalex.org/W2962978519', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963902314', 'https://openalex.org/W2132921748', 'https://openalex.org/W2057007397', 'https://openalex.org/W2950133079', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972892814', 'https://openalex.org/W2973135958', 'https://openalex.org/W2242818861', 'https://openalex.org/W2927673779']",2019-11-21
https://openalex.org/W3007068036,https://doi.org/10.1109/asru46091.2019.9003853,Speech-to-Speech Translation Between Untranscribed Unknown Languages,"In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target languages discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.","['https://openalex.org/W2963581463', 'https://openalex.org/W2963796886', 'https://openalex.org/W2962699523', 'https://openalex.org/W6681644459', 'https://openalex.org/W6678262379', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W2947445680', 'https://openalex.org/W6763445112', 'https://openalex.org/W2752796333', 'https://openalex.org/W2025768430', 'https://openalex.org/W6748381668', 'https://openalex.org/W6757152577', 'https://openalex.org/W6608432165', 'https://openalex.org/W2466918907', 'https://openalex.org/W2120847449', 'https://openalex.org/W6736180185', 'https://openalex.org/W2161742089', 'https://openalex.org/W6732953234', 'https://openalex.org/W2962680099', 'https://openalex.org/W6679434410', 'https://openalex.org/W6973666849', 'https://openalex.org/W6623517193', 'https://openalex.org/W1902237438', 'https://openalex.org/W6679436768', 'https://openalex.org/W2064675550', 'https://openalex.org/W2884852625', 'https://openalex.org/W2949328740', 'https://openalex.org/W206967138', 'https://openalex.org/W1836465849', 'https://openalex.org/W2605131327', 'https://openalex.org/W2130942839', 'https://openalex.org/W2972495969', 'https://openalex.org/W2123301721', 'https://openalex.org/W2963609956', 'https://openalex.org/W2144600658', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2949117887', 'https://openalex.org/W1921523184', 'https://openalex.org/W2101105183', 'https://openalex.org/W1522301498', 'https://openalex.org/W2789543585', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964026424', 'https://openalex.org/W2786608204', 'https://openalex.org/W2972374322', 'https://openalex.org/W2906111771', 'https://openalex.org/W2191779130', 'https://openalex.org/W2973026522', 'https://openalex.org/W854541894']",2019-12-01
https://openalex.org/W3198082505,https://doi.org/10.21437/interspeech.2021-1990,Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion,"Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.","['https://openalex.org/W3168542456', 'https://openalex.org/W2889061305', 'https://openalex.org/W3134921434', 'https://openalex.org/W2972659941', 'https://openalex.org/W4299812321', 'https://openalex.org/W2963226019', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963830550', 'https://openalex.org/W3034303964', 'https://openalex.org/W2970006822', 'https://openalex.org/W3163475957', 'https://openalex.org/W3034794073', 'https://openalex.org/W4287690582', 'https://openalex.org/W3199364993', 'https://openalex.org/W2794490148', 'https://openalex.org/W2527729766', 'https://openalex.org/W2982223350', 'https://openalex.org/W3095035471', 'https://openalex.org/W2937020545', 'https://openalex.org/W1731081199', 'https://openalex.org/W3197411683', 'https://openalex.org/W3126283728', 'https://openalex.org/W3133667170', 'https://openalex.org/W3030987249', 'https://openalex.org/W3201143670', 'https://openalex.org/W3095361818', 'https://openalex.org/W4295731579', 'https://openalex.org/W4232345992', 'https://openalex.org/W3015645837', 'https://openalex.org/W4287614031', 'https://openalex.org/W2947445680', 'https://openalex.org/W3015434413', 'https://openalex.org/W3015699566', 'https://openalex.org/W2972921407']",2021-08-27
https://openalex.org/W4295308567,https://doi.org/10.1109/jstsp.2022.3206084,Self-Supervised Language Learning From Raw Audio: Lessons From the Zero Resource Speech Challenge,International audience,"['https://openalex.org/W2483390977', 'https://openalex.org/W1529056402', 'https://openalex.org/W3197580070', 'https://openalex.org/W2098500169', 'https://openalex.org/W2117041980', 'https://openalex.org/W2170659185', 'https://openalex.org/W6675022971', 'https://openalex.org/W2025482506', 'https://openalex.org/W2020607164', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W3093096176', 'https://openalex.org/W3197259906', 'https://openalex.org/W2396043527', 'https://openalex.org/W2402366697', 'https://openalex.org/W2399576818', 'https://openalex.org/W2745710152', 'https://openalex.org/W2345811097', 'https://openalex.org/W1796128977', 'https://openalex.org/W2522012644', 'https://openalex.org/W2404799143', 'https://openalex.org/W2400549570', 'https://openalex.org/W2787223168', 'https://openalex.org/W2786902352', 'https://openalex.org/W2787447541', 'https://openalex.org/W2787426069', 'https://openalex.org/W2785415724', 'https://openalex.org/W3100270690', 'https://openalex.org/W3144810982', 'https://openalex.org/W2057007397', 'https://openalex.org/W2964169922', 'https://openalex.org/W2407614114', 'https://openalex.org/W2398490608', 'https://openalex.org/W4313182775', 'https://openalex.org/W4296710617', 'https://openalex.org/W2468716020', 'https://openalex.org/W3097159218', 'https://openalex.org/W3097485645', 'https://openalex.org/W2748009955', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972374322', 'https://openalex.org/W2972964185', 'https://openalex.org/W2972867623', 'https://openalex.org/W2950414763', 'https://openalex.org/W2947445680', 'https://openalex.org/W3003750857', 'https://openalex.org/W2973013862', 'https://openalex.org/W3097056138', 'https://openalex.org/W3161215977', 'https://openalex.org/W3024040651', 'https://openalex.org/W3096262326', 'https://openalex.org/W3097692357', 'https://openalex.org/W3096359985', 'https://openalex.org/W3095361818', 'https://openalex.org/W3096216486', 'https://openalex.org/W6786696081', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W3197381195', 'https://openalex.org/W6809593508', 'https://openalex.org/W4292825791', 'https://openalex.org/W3204915839', 'https://openalex.org/W6839738141', 'https://openalex.org/W3209993061', 'https://openalex.org/W6731521493', 'https://openalex.org/W6728823536', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W6844194202', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6811170316', 'https://openalex.org/W1993755070', 'https://openalex.org/W6676025551', 'https://openalex.org/W2110627398', 'https://openalex.org/W3212943633', 'https://openalex.org/W4283332789', 'https://openalex.org/W3156899336', 'https://openalex.org/W4285250921', 'https://openalex.org/W6840200333', 'https://openalex.org/W4297841853', 'https://openalex.org/W2114347655', 'https://openalex.org/W2079460648', 'https://openalex.org/W2049142189', 'https://openalex.org/W1967924372', 'https://openalex.org/W2126377586', 'https://openalex.org/W3044967013', 'https://openalex.org/W2117126688', 'https://openalex.org/W3198782837', 'https://openalex.org/W2964115348', 'https://openalex.org/W2598638573', 'https://openalex.org/W6790356757', 'https://openalex.org/W2995181338', 'https://openalex.org/W3016181583', 'https://openalex.org/W2014307400', 'https://openalex.org/W2996728628', 'https://openalex.org/W2963425185', 'https://openalex.org/W2170682101', 'https://openalex.org/W2103318667', 'https://openalex.org/W2080100102', 'https://openalex.org/W6691746754', 'https://openalex.org/W1854884267', 'https://openalex.org/W2963366649', 'https://openalex.org/W2176085882', 'https://openalex.org/W6680094886', 'https://openalex.org/W2026487812', 'https://openalex.org/W2142625445', 'https://openalex.org/W1494198834', 'https://openalex.org/W6729977899', 'https://openalex.org/W2619697695', 'https://openalex.org/W2586148577', 'https://openalex.org/W2949387496', 'https://openalex.org/W3160251434', 'https://openalex.org/W2332013542', 'https://openalex.org/W2069857264', 'https://openalex.org/W3209984917', 'https://openalex.org/W3198217962', 'https://openalex.org/W6810531757', 'https://openalex.org/W4225726571', 'https://openalex.org/W3093121832', 'https://openalex.org/W3148101939', 'https://openalex.org/W3007068036', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6799081819', 'https://openalex.org/W4287854499', 'https://openalex.org/W4287591426', 'https://openalex.org/W2973026522', 'https://openalex.org/W3098643042', 'https://openalex.org/W4307680525', 'https://openalex.org/W3099142230', 'https://openalex.org/W4285483774', 'https://openalex.org/W1973746598', 'https://openalex.org/W3186843219', 'https://openalex.org/W3096196861', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394671563', 'https://openalex.org/W3160704724', 'https://openalex.org/W3097682198', 'https://openalex.org/W2132631284', 'https://openalex.org/W4221161768', 'https://openalex.org/W4226199158']",2022-09-12
https://openalex.org/W3143523927,https://doi.org/10.1109/slt48900.2021.9383498,How Far Are We from Robust Voice Conversion: A Survey,"Voice conversion technologies have been greatly improved in recent years with the help of deep learning, but their capabilities of producing natural sounding utterances in different conditions remain unclear. In this paper, we gave a thorough study of the robustness of known VC models. We also modified these models, such as the replacement of speaker embeddings, to further improve their performances. We found that the sampling rate and audio duration greatly influence voice conversion. All the VC models suffer from unseen data, but AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained is more suitable for voice conversion than those trained on speaker identification.","['https://openalex.org/W2808631503', 'https://openalex.org/W6740167877', 'https://openalex.org/W6631362777', 'https://openalex.org/W6767111847', 'https://openalex.org/W1991938399', 'https://openalex.org/W6776440307', 'https://openalex.org/W2973154337', 'https://openalex.org/W6776390925', 'https://openalex.org/W2972659941', 'https://openalex.org/W6762533536', 'https://openalex.org/W2046056978', 'https://openalex.org/W6727697161', 'https://openalex.org/W6777157395', 'https://openalex.org/W2121415728', 'https://openalex.org/W2890964092', 'https://openalex.org/W2120847449', 'https://openalex.org/W3096524539', 'https://openalex.org/W6695553435', 'https://openalex.org/W2947196194', 'https://openalex.org/W2963830550', 'https://openalex.org/W2532494225', 'https://openalex.org/W2888922217', 'https://openalex.org/W2963539064', 'https://openalex.org/W2902070858', 'https://openalex.org/W2947445680', 'https://openalex.org/W6755932049', 'https://openalex.org/W2603777577', 'https://openalex.org/W6640963894', 'https://openalex.org/W2972359262', 'https://openalex.org/W2962788625', 'https://openalex.org/W6603838645', 'https://openalex.org/W1494198834', 'https://openalex.org/W4320013936', 'https://openalex.org/W2099471712', 'https://openalex.org/W3143465836', 'https://openalex.org/W4288337064', 'https://openalex.org/W2284628133', 'https://openalex.org/W3020570669', 'https://openalex.org/W3015805741', 'https://openalex.org/W2726515241', 'https://openalex.org/W2527729766', 'https://openalex.org/W95152782', 'https://openalex.org/W3024610089', 'https://openalex.org/W1959608418', 'https://openalex.org/W2519091744', 'https://openalex.org/W2972689158', 'https://openalex.org/W2970006822', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949281321', 'https://openalex.org/W2945478979', 'https://openalex.org/W2898786057', 'https://openalex.org/W3034794073', 'https://openalex.org/W2970351109']",2021-01-19
https://openalex.org/W3006777338,https://doi.org/10.1109/asru46091.2019.9004008,Bootstrapping Non-Parallel Voice Conversion from Speaker-Adaptive Text-to-Speech,"Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a similar objective, generating speech with a target voice. However, they are usually developed independently under vastly different frameworks. In this paper, we propose a methodology to bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Moreover by offloading the heavy data demand to the training stage of the TTS model, our VC system can be built using a small amount of target speaker speech data. It also opens up the possibility of using speech in a foreign unseen language to build the system. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.","['https://openalex.org/W2800289214', 'https://openalex.org/W2963035245', 'https://openalex.org/W113498433', 'https://openalex.org/W6679569544', 'https://openalex.org/W1981457580', 'https://openalex.org/W6726528559', 'https://openalex.org/W2889329491', 'https://openalex.org/W2963808252', 'https://openalex.org/W2532494225', 'https://openalex.org/W2964069186', 'https://openalex.org/W2885820941', 'https://openalex.org/W2899877258', 'https://openalex.org/W2938583109', 'https://openalex.org/W1974745215', 'https://openalex.org/W6632366324', 'https://openalex.org/W2013996527', 'https://openalex.org/W2161476805', 'https://openalex.org/W2977798327', 'https://openalex.org/W2086796102', 'https://openalex.org/W6602007935', 'https://openalex.org/W33533989', 'https://openalex.org/W1570629387', 'https://openalex.org/W6635216677', 'https://openalex.org/W2120605154', 'https://openalex.org/W2156142001', 'https://openalex.org/W2035962301', 'https://openalex.org/W2507912506', 'https://openalex.org/W2527729766', 'https://openalex.org/W49412823', 'https://openalex.org/W2774848319', 'https://openalex.org/W2518172956', 'https://openalex.org/W2950414763', 'https://openalex.org/W2134202996', 'https://openalex.org/W2808706139', 'https://openalex.org/W2887264325', 'https://openalex.org/W2963432880', 'https://openalex.org/W1588266896', 'https://openalex.org/W2940544976', 'https://openalex.org/W2972374322', 'https://openalex.org/W2947445680', 'https://openalex.org/W4298174729', 'https://openalex.org/W2973034126', 'https://openalex.org/W1538607601', 'https://openalex.org/W2972867623', 'https://openalex.org/W2972999331', 'https://openalex.org/W2950224550', 'https://openalex.org/W2962739369', 'https://openalex.org/W3101689408']",2019-12-01
https://openalex.org/W4387609229,https://doi.org/10.1109/taes.2023.3323443,Self-Supervised Contrastive Learning for Extracting Radar Word in the Hierarchical Model of Multifunction Radar,"The analysis of intercepted multifunction radar (MFR) signals has attracted considerable attention in the field of cognitive electronic reconnaissance. The agility of pulse parameters makes it difficult to recognize their behavior states. Currently, most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a noncooperative way. This study develops a contrastive self-supervised method to adaptively segment and cluster MFR pulse sequences into radar words with little prior information. First, a convolutional neural network (CNN) is trained in a self-supervised manner to differentiate between adjacent and nonadjacent pulse group embeddings to learn robust feature representations from sequences of pulse descriptor words (PDWs). Next, based on the learned embeddings, test statistics are calculated for each PDW sequence to detect change points, which are, in turn, used to segment the sequence into multiple subsegments, each containing only one radar word. Finally, feature vectors are produced for clustering the subsegments into groups of radar words. Simulation results show that without using any labeled data, the proposed method can effectively extract the radar words of a hypothetical MFR under corrupted and overlapped pulse parameters and performs only slightly worse than the state-of-the-art fully supervised method.","['https://openalex.org/W1977782550', 'https://openalex.org/W2108777121', 'https://openalex.org/W2768783378', 'https://openalex.org/W6677224193', 'https://openalex.org/W3005701803', 'https://openalex.org/W2295032983', 'https://openalex.org/W3130749205', 'https://openalex.org/W2113592174', 'https://openalex.org/W6630754547', 'https://openalex.org/W3007858997', 'https://openalex.org/W3107981615', 'https://openalex.org/W3021364482', 'https://openalex.org/W3161492984', 'https://openalex.org/W3173837271', 'https://openalex.org/W3034171073', 'https://openalex.org/W3164071761', 'https://openalex.org/W3023371261', 'https://openalex.org/W3110446398', 'https://openalex.org/W3114632476', 'https://openalex.org/W3096656254', 'https://openalex.org/W2947445680', 'https://openalex.org/W1531268951', 'https://openalex.org/W6682948231', 'https://openalex.org/W1498436455', 'https://openalex.org/W6774314701', 'https://openalex.org/W6668990524', 'https://openalex.org/W2949488362', 'https://openalex.org/W1987971958', 'https://openalex.org/W2599882013', 'https://openalex.org/W3013889479', 'https://openalex.org/W6766978945', 'https://openalex.org/W6631190155', 'https://openalex.org/W6746904271', 'https://openalex.org/W6844194202', 'https://openalex.org/W130754613', 'https://openalex.org/W2649995573', 'https://openalex.org/W6683161245', 'https://openalex.org/W2744335156', 'https://openalex.org/W2509841691', 'https://openalex.org/W1673567634', 'https://openalex.org/W1975684011', 'https://openalex.org/W4235169531', 'https://openalex.org/W1511910497', 'https://openalex.org/W2073459066', 'https://openalex.org/W2115790511', 'https://openalex.org/W4297808394', 'https://openalex.org/W2775127214']",2023-10-13
https://openalex.org/W4389248327,https://doi.org/10.1016/j.neunet.2023.11.068,Decentralized policy learning with partial observation and mechanical constraints for multiperson modeling,,"['https://openalex.org/W2424778531', 'https://openalex.org/W2924816077', 'https://openalex.org/W2112164016', 'https://openalex.org/W6759048875', 'https://openalex.org/W2088956500', 'https://openalex.org/W2896332777', 'https://openalex.org/W6752307458', 'https://openalex.org/W6617744952', 'https://openalex.org/W2915130814', 'https://openalex.org/W2143969246', 'https://openalex.org/W3035032111', 'https://openalex.org/W2548815430', 'https://openalex.org/W1885639605', 'https://openalex.org/W6744063608', 'https://openalex.org/W6712395597', 'https://openalex.org/W3132362810', 'https://openalex.org/W2405096793', 'https://openalex.org/W2963638622', 'https://openalex.org/W6762821723', 'https://openalex.org/W2986244508', 'https://openalex.org/W6798280219', 'https://openalex.org/W2329185630', 'https://openalex.org/W1985102978', 'https://openalex.org/W6743827229', 'https://openalex.org/W3035179769', 'https://openalex.org/W2998212266', 'https://openalex.org/W2963001155', 'https://openalex.org/W2167052694', 'https://openalex.org/W6740217080', 'https://openalex.org/W2982202190', 'https://openalex.org/W2894976951', 'https://openalex.org/W6749387567', 'https://openalex.org/W6753579488', 'https://openalex.org/W6751139674', 'https://openalex.org/W6719357382', 'https://openalex.org/W2168359464', 'https://openalex.org/W2396178844', 'https://openalex.org/W2991398269', 'https://openalex.org/W6631190155', 'https://openalex.org/W6640963894', 'https://openalex.org/W6748320467', 'https://openalex.org/W6735698609', 'https://openalex.org/W2947445680', 'https://openalex.org/W2998367975', 'https://openalex.org/W2548228487', 'https://openalex.org/W2901886291', 'https://openalex.org/W2160490717', 'https://openalex.org/W2785811906', 'https://openalex.org/W2982745079', 'https://openalex.org/W1931877416', 'https://openalex.org/W2557519264', 'https://openalex.org/W6767342764', 'https://openalex.org/W6787950781', 'https://openalex.org/W3189569006', 'https://openalex.org/W2064527819', 'https://openalex.org/W2963091558', 'https://openalex.org/W2016589492', 'https://openalex.org/W2949269657', 'https://openalex.org/W6746321160', 'https://openalex.org/W2910285800', 'https://openalex.org/W6729895595', 'https://openalex.org/W2612690371', 'https://openalex.org/W4321795339', 'https://openalex.org/W2950302464', 'https://openalex.org/W4376502360', 'https://openalex.org/W2973036502', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963562809', 'https://openalex.org/W1959608418', 'https://openalex.org/W2133564696', 'https://openalex.org/W4253942557', 'https://openalex.org/W2773355582', 'https://openalex.org/W2914524146', 'https://openalex.org/W2905173465', 'https://openalex.org/W2944612698', 'https://openalex.org/W2396566817', 'https://openalex.org/W2553756201', 'https://openalex.org/W4288019944', 'https://openalex.org/W2949886883', 'https://openalex.org/W2751471435', 'https://openalex.org/W3213136423', 'https://openalex.org/W4300830895', 'https://openalex.org/W2885318751', 'https://openalex.org/W3093010610', 'https://openalex.org/W4281918658', 'https://openalex.org/W2962715980', 'https://openalex.org/W2559655401', 'https://openalex.org/W4294107275', 'https://openalex.org/W2786619647', 'https://openalex.org/W2787337315', 'https://openalex.org/W2464234964', 'https://openalex.org/W4312558117', 'https://openalex.org/W3175775237', 'https://openalex.org/W2803155336', 'https://openalex.org/W4212774754', 'https://openalex.org/W3114820637', 'https://openalex.org/W2340149117', 'https://openalex.org/W4287755157', 'https://openalex.org/W2991489433', 'https://openalex.org/W2986406093', 'https://openalex.org/W2963755523', 'https://openalex.org/W592244745', 'https://openalex.org/W4302289548']",2023-12-01
https://openalex.org/W4392903591,https://doi.org/10.1109/icassp48485.2024.10445804,Promptvc: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts,"Stylistic voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.","['https://openalex.org/W3098557217', 'https://openalex.org/W4372260214', 'https://openalex.org/W4283689139', 'https://openalex.org/W4372267192', 'https://openalex.org/W4205742757', 'https://openalex.org/W3096939667', 'https://openalex.org/W2947445680', 'https://openalex.org/W2938833595', 'https://openalex.org/W4372260157', 'https://openalex.org/W3163573274', 'https://openalex.org/W4225264140', 'https://openalex.org/W3216296943', 'https://openalex.org/W6750489868', 'https://openalex.org/W4226474318', 'https://openalex.org/W4398152753', 'https://openalex.org/W4385822787', 'https://openalex.org/W4375869257', 'https://openalex.org/W3209059054', 'https://openalex.org/W6795807602', 'https://openalex.org/W2519091744', 'https://openalex.org/W6763832098', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W4312933868', 'https://openalex.org/W2107860279', 'https://openalex.org/W2067295501', 'https://openalex.org/W1574447377']",2024-03-18
https://openalex.org/W4372260509,https://doi.org/10.1109/icassp49357.2023.10095776,Multi-Speaker Expressive Speech Synthesis via Multiple Factors Decoupling,"This paper aims to synthesize the target speaker's speech with desired speaking style and emotion by transferring the style and emotion from reference speech recorded by other speakers. We address this challenging problem with a two-stage framework composed of a text-to-style-and-emotion (Text2SE) module and a style-and- emotion-to-wave (SE2Wave) module, bridging by neural bottleneck (BN) features. To further solve the multi-factor (speaker timbre, speaking style and emotion) decoupling problem, we adopt the multi-label binary vector (MBV) and mutual information (MI) minimization to respectively discretize the extracted embeddings and disentangle these highly entangled factors in both Text2SE and SE2Wave modules. Moreover, we introduce a semi-supervised training strategy to leverage data from multiple speakers, including emotion-labeled data, style-labeled data, and unlabeled data. To better transfer the fine-grained expression from references to the target speaker in non-parallel transfer, we introduce a reference-candidate pool and propose an attention-based reference selection approach. Extensive experiments demonstrate the good design of our model.","['https://openalex.org/W6796730497', 'https://openalex.org/W6778823374', 'https://openalex.org/W6783867762', 'https://openalex.org/W6796464841', 'https://openalex.org/W6849600165', 'https://openalex.org/W3096457008', 'https://openalex.org/W4210777104', 'https://openalex.org/W3198048667', 'https://openalex.org/W6746238782', 'https://openalex.org/W2890287821', 'https://openalex.org/W2904459034', 'https://openalex.org/W3135644023', 'https://openalex.org/W3198082505', 'https://openalex.org/W6760861152', 'https://openalex.org/W4294311176', 'https://openalex.org/W6757422803', 'https://openalex.org/W4226421465', 'https://openalex.org/W4226039367', 'https://openalex.org/W4221151538', 'https://openalex.org/W3163475957', 'https://openalex.org/W3163568691', 'https://openalex.org/W3162508345', 'https://openalex.org/W2947445680', 'https://openalex.org/W6779459370', 'https://openalex.org/W4385245566', 'https://openalex.org/W6750489868', 'https://openalex.org/W6795807602', 'https://openalex.org/W3197659778', 'https://openalex.org/W3197478142', 'https://openalex.org/W3024869864', 'https://openalex.org/W3174758275', 'https://openalex.org/W2770743791', 'https://openalex.org/W2187089797', 'https://openalex.org/W4320451749', 'https://openalex.org/W2932022923']",2023-05-05
https://openalex.org/W3020975377,https://doi.org/10.21437/interspeech.2020-1542,Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data,"We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at https://mindslab-ai.github.io/cotatron, and the code with a pre-trained model will be made available soon.","['https://openalex.org/W2545656684', 'https://openalex.org/W2996286887', 'https://openalex.org/W2970351109', 'https://openalex.org/W2963827314', 'https://openalex.org/W2953190524', 'https://openalex.org/W2964121744', 'https://openalex.org/W2995327724', 'https://openalex.org/W2937343983', 'https://openalex.org/W2972999331', 'https://openalex.org/W3101689408', 'https://openalex.org/W2963799213', 'https://openalex.org/W2981382721', 'https://openalex.org/W2949281321', 'https://openalex.org/W2982174878', 'https://openalex.org/W2928664166', 'https://openalex.org/W2572730214', 'https://openalex.org/W2885820941', 'https://openalex.org/W2970006822', 'https://openalex.org/W2972849140', 'https://openalex.org/W2518172956', 'https://openalex.org/W2804998325', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963927338', 'https://openalex.org/W2947445680', 'https://openalex.org/W2954930777', 'https://openalex.org/W2904818793', 'https://openalex.org/W2970971581', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963830550', 'https://openalex.org/W2998572311']",2020-10-25
https://openalex.org/W4372266960,https://doi.org/10.1109/icassp49357.2023.10095565,A Unified One-Shot Prosody and Speaker Conversion System with Self-Supervised Discrete Speech Units,"We present a unified system to realize one-shot voice conversion (VC) on the pitch, rhythm, and speaker attributes. Existing works generally ignore the correlation between prosody and language content, leading to the degradation of naturalness in converted speech. Additionally, the lack of proper language features prevents these systems from accurately preserving language content after conversion. To address these issues, we devise a cascaded modular system leveraging self-supervised discrete speech units as language representation. These discrete units provide duration information essential for rhythm modeling. Our system first extracts utterance-level prosody and speaker representations from the raw waveform. Given the prosody representation, a prosody predictor estimates pitch, energy, and duration for each discrete unit in the utterance. A synthesizer further reconstructs speech based on the predicted prosody, speaker representation, and discrete units. Experiments show that our system outperforms previous approaches in naturalness, intelligibility, speaker transferability, and prosody transferability. Code and samples are publicly available. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W3207300132', 'https://openalex.org/W6803547063', 'https://openalex.org/W2962866891', 'https://openalex.org/W2143827132', 'https://openalex.org/W3163573274', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015805741', 'https://openalex.org/W6762533536', 'https://openalex.org/W6780226713', 'https://openalex.org/W6607364320', 'https://openalex.org/W2593414223', 'https://openalex.org/W3205631867', 'https://openalex.org/W6783867762', 'https://openalex.org/W3198217962', 'https://openalex.org/W2972359262', 'https://openalex.org/W6772349387', 'https://openalex.org/W6763832098', 'https://openalex.org/W4287887366', 'https://openalex.org/W3209059054', 'https://openalex.org/W6809829207', 'https://openalex.org/W3016160783', 'https://openalex.org/W4225939199', 'https://openalex.org/W6776390925', 'https://openalex.org/W4296068587', 'https://openalex.org/W3096524539', 'https://openalex.org/W2947445680', 'https://openalex.org/W6796577156', 'https://openalex.org/W3197659778', 'https://openalex.org/W3034794073', 'https://openalex.org/W4361994820', 'https://openalex.org/W3036601975', 'https://openalex.org/W2187089797', 'https://openalex.org/W2998572311', 'https://openalex.org/W181056519', 'https://openalex.org/W3092028330', 'https://openalex.org/W3169739675', 'https://openalex.org/W4301371414', 'https://openalex.org/W2945478979', 'https://openalex.org/W2946200149']",2023-05-05
https://openalex.org/W4386902717,https://doi.org/10.1109/taslp.2023.3313426,Noisy-to-Noisy Voice Conversion Under Variations of Noisy Condition,"Voiceconversion (VC) refers to the transformation of the speaker identity of a speech to the target one without altering the linguistic content. As recent VC techniques have made significant progress, implementing them in real-world scenarios is also considered, where speech data have some inevitable interferences, the most common of which are background sounds. On the other hand, background sounds are informative and need to be retained in some applications, such as VC in movies/videos. To address these issues, we have proposed a noisy-to-noisy (N2N) VC framework that does not rely on clean VC data and models the noisy speech directly by using noise as conditions. Previous experimental results have proven its effectiveness. In this article, we further improve its performance by introducing the pre-trained noise-conditioned VC model. Moreover, to further explore the impacts of introducing noise conditions, the performance in more realistic situations is evaluated in which the training set possesses speaker-dependent noisy conditions. The experimental results demonstrate the effectiveness of the pre-training strategy and the degradation of its performance under strict noisy conditions. We then proposed a noise augmentation method to overcome the limitation. Further experiments showed the effectiveness of the augmentation method.","['https://openalex.org/W4210774711', 'https://openalex.org/W2964058413', 'https://openalex.org/W2972544500', 'https://openalex.org/W3095361818', 'https://openalex.org/W2899877258', 'https://openalex.org/W2963035245', 'https://openalex.org/W2947445680', 'https://openalex.org/W2954386831', 'https://openalex.org/W3097906045', 'https://openalex.org/W2907262790', 'https://openalex.org/W2804998325', 'https://openalex.org/W3096408984', 'https://openalex.org/W2946555236', 'https://openalex.org/W3197042120', 'https://openalex.org/W2013996527', 'https://openalex.org/W2017425464', 'https://openalex.org/W4232282348', 'https://openalex.org/W6810681921', 'https://openalex.org/W2052666245', 'https://openalex.org/W3143523927', 'https://openalex.org/W6802128655', 'https://openalex.org/W4205582447', 'https://openalex.org/W2997069303', 'https://openalex.org/W4226075807', 'https://openalex.org/W3206503703', 'https://openalex.org/W6732045268', 'https://openalex.org/W2107860279', 'https://openalex.org/W2603567530', 'https://openalex.org/W2972659941', 'https://openalex.org/W4283768328', 'https://openalex.org/W3143465836', 'https://openalex.org/W3095936335', 'https://openalex.org/W2937579788', 'https://openalex.org/W2532494225', 'https://openalex.org/W3082130377', 'https://openalex.org/W2156142001', 'https://openalex.org/W2902070858', 'https://openalex.org/W2963539064', 'https://openalex.org/W6772349387', 'https://openalex.org/W3200209270', 'https://openalex.org/W2998572311', 'https://openalex.org/W2577350056', 'https://openalex.org/W4226225516']",2023-01-01
https://openalex.org/W3132220150,https://doi.org/10.1109/access.2021.3058382,Many-to-Many Unsupervised Speech Conversion From Nonparallel Corpora,"We address a nonparallel data-driven many-to-many speech modeling and multimodal style conversion method. In this work, we train a speech conversion model for multiple domains rather than a specific source and target domain pair, and we generate diverse output speech signals from a given source domain speech by transferring some speech style-related characteristics while preserving its linguistic content information. The proposed method comprises a variational autoencoder (VAE)-based many-to-many speech conversion network with a Wasserstein generative adversarial network (WGAN) and a skip-connected autoencoder-based self-supervised learning network. The proposed conversion network trains the models by decomposing the spectral features of the input speech signal into a content factor that represents domain-invariant information and a style factor that represents domain-related information to automatically estimate the various speech styles of each domain, and the network converts the input speech signal to another domain using the computed content factor with the target style factor we want to change. Diverse and multimodal outputs can be generated by sampling different style factors. We also train models in a stable manner and improve the quality of generated outputs by sharing the discriminator of the VAE-based speech conversion network and that of the self-supervised learning network. We apply the proposed method to speaker conversion and perform the perceptual evaluations. Experimental results revealed that the proposed method obtained high accuracy of converted spectra, significantly improved the sound quality and speaker similarity of the converted speech, and contributed to stable model training.","['https://openalex.org/W2936774411', 'https://openalex.org/W2962780374', 'https://openalex.org/W2515028311', 'https://openalex.org/W6745535286', 'https://openalex.org/W6741832134', 'https://openalex.org/W2963035245', 'https://openalex.org/W6917638038', 'https://openalex.org/W2120605154', 'https://openalex.org/W2603777577', 'https://openalex.org/W1974843131', 'https://openalex.org/W2532494225', 'https://openalex.org/W6745117592', 'https://openalex.org/W2804998325', 'https://openalex.org/W2972544500', 'https://openalex.org/W2963539064', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963073614', 'https://openalex.org/W2962896155', 'https://openalex.org/W6756002706', 'https://openalex.org/W2806000759', 'https://openalex.org/W4245885054', 'https://openalex.org/W2947445680', 'https://openalex.org/W2142300631', 'https://openalex.org/W2067175291', 'https://openalex.org/W2576309025', 'https://openalex.org/W2515020857', 'https://openalex.org/W2056852181', 'https://openalex.org/W2017742648', 'https://openalex.org/W6687506355', 'https://openalex.org/W1590808459', 'https://openalex.org/W2963890275', 'https://openalex.org/W2962824366', 'https://openalex.org/W2953327099', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W343636949', 'https://openalex.org/W6777232839', 'https://openalex.org/W3148040514', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963618559', 'https://openalex.org/W4320013936', 'https://openalex.org/W2964167449', 'https://openalex.org/W2899361462', 'https://openalex.org/W2099471712', 'https://openalex.org/W3025035610', 'https://openalex.org/W2739748921', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963746531']",2021-01-01
https://openalex.org/W4385569627,https://doi.org/10.1109/taslp.2023.3301212,Parallel Synthesis for Autoregressive Speech Generation,"Autoregressive neural vocoders have achieved outstanding performance and are widely used in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it can generate highly natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech time sequence in parallel and then proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is first split into different frequency subbands. The proposed model generates a subband conditioned on the previously generated one. A full-band speech can then be reconstructed from these generated subbands. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance's length but to the number of subbands/bits. The inference efficiency is hence significantly increased. Besides, a post-filter is employed to sample audio signals from output posteriors, and its training objective is designed based on the characteristics of the proposed autoregressive methods. The experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with the baseline autoregressive and non-autoregressive vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability while synthesizing 44 kHz speech or utterances from unseen speakers.","['https://openalex.org/W2120847449', 'https://openalex.org/W2148770129', 'https://openalex.org/W2598638573', 'https://openalex.org/W2143280761', 'https://openalex.org/W2885820941', 'https://openalex.org/W6733471323', 'https://openalex.org/W2963830550', 'https://openalex.org/W1494895475', 'https://openalex.org/W6774645763', 'https://openalex.org/W6839643428', 'https://openalex.org/W6711777497', 'https://openalex.org/W2118806033', 'https://openalex.org/W2471520273', 'https://openalex.org/W3144035034', 'https://openalex.org/W3161704465', 'https://openalex.org/W2947445680', 'https://openalex.org/W2784918340', 'https://openalex.org/W6762533536', 'https://openalex.org/W4231807801', 'https://openalex.org/W2423557781', 'https://openalex.org/W4283817920', 'https://openalex.org/W6943912960', 'https://openalex.org/W6781251213', 'https://openalex.org/W3141224548', 'https://openalex.org/W2963542120', 'https://openalex.org/W6778883912', 'https://openalex.org/W6739901393', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783867762', 'https://openalex.org/W2118774185', 'https://openalex.org/W3097895838', 'https://openalex.org/W2996797022', 'https://openalex.org/W6783182287', 'https://openalex.org/W2963654953', 'https://openalex.org/W3034999214', 'https://openalex.org/W2111284386', 'https://openalex.org/W2102003408', 'https://openalex.org/W6748409065', 'https://openalex.org/W6776390925', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W3097828251', 'https://openalex.org/W6767111847', 'https://openalex.org/W2107860279', 'https://openalex.org/W6773094808', 'https://openalex.org/W6631190155', 'https://openalex.org/W3015338123', 'https://openalex.org/W6603838645', 'https://openalex.org/W6843673214', 'https://openalex.org/W2963300588', 'https://openalex.org/W6753855596', 'https://openalex.org/W3096702751', 'https://openalex.org/W2888169323', 'https://openalex.org/W3097538987', 'https://openalex.org/W2964041258', 'https://openalex.org/W3095459301', 'https://openalex.org/W6917585676', 'https://openalex.org/W3095419383', 'https://openalex.org/W6786325012', 'https://openalex.org/W6778823374', 'https://openalex.org/W3197294703', 'https://openalex.org/W6771024825', 'https://openalex.org/W6756159577', 'https://openalex.org/W6766837696', 'https://openalex.org/W2963103134', 'https://openalex.org/W2805217154', 'https://openalex.org/W1521637132', 'https://openalex.org/W3162770051', 'https://openalex.org/W2172065531', 'https://openalex.org/W6771719367', 'https://openalex.org/W6802838302', 'https://openalex.org/W3095851005', 'https://openalex.org/W2890983311', 'https://openalex.org/W3100270690', 'https://openalex.org/W3163662330', 'https://openalex.org/W4377079846', 'https://openalex.org/W3154504973', 'https://openalex.org/W2954386831', 'https://openalex.org/W2889606145', 'https://openalex.org/W3140429000', 'https://openalex.org/W3034363136', 'https://openalex.org/W2785516183', 'https://openalex.org/W2963091184', 'https://openalex.org/W2912237252', 'https://openalex.org/W6752910514', 'https://openalex.org/W2963434219', 'https://openalex.org/W6746208923', 'https://openalex.org/W2969937244', 'https://openalex.org/W4294619240', 'https://openalex.org/W2970006822', 'https://openalex.org/W2999160446', 'https://openalex.org/W4287694050', 'https://openalex.org/W3122318757', 'https://openalex.org/W4292779060', 'https://openalex.org/W4283388932', 'https://openalex.org/W4385245566', 'https://openalex.org/W2587284713', 'https://openalex.org/W3008758407', 'https://openalex.org/W2945478979', 'https://openalex.org/W4298580827', 'https://openalex.org/W3105124182', 'https://openalex.org/W3033411150', 'https://openalex.org/W2519091744', 'https://openalex.org/W3109064156', 'https://openalex.org/W3103913581', 'https://openalex.org/W4289305009', 'https://openalex.org/W2953318193', 'https://openalex.org/W4225304461', 'https://openalex.org/W2963975282', 'https://openalex.org/W2993118648', 'https://openalex.org/W3034794073', 'https://openalex.org/W3123097577', 'https://openalex.org/W4320013936', 'https://openalex.org/W4289761690', 'https://openalex.org/W3092028330', 'https://openalex.org/W3129651364', 'https://openalex.org/W2994373303', 'https://openalex.org/W2767206889', 'https://openalex.org/W2395578248', 'https://openalex.org/W95152782', 'https://openalex.org/W1522301498', 'https://openalex.org/W4286899907']",2023-01-01
https://openalex.org/W4206192208,https://doi.org/10.36227/techrxiv.17976062,Towards Self-supervised Learning for Multi-function Radar Behavior State Detection and Recognition,"&lt;div&gt;The analysis of intercepted multi-function radar (MFR) signals has gained considerable attention in the field of cognitive electronic reconnaissance. With the rapid development of MFR, the switch between different work modes is becoming more flexible, increasing the agility of pulse parameters. Most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a non-cooperative way. This study develops a novel hierarchical contrastive self-supervise-based method for segmenting and clustering MFR pulse sequences. First, a convolutional neural network (CNN) with a limited receptive field is trained in a contrastive way to distinguish between pulse descriptor words (PDW) in the original order and the samples created by random permutations to detect the boundary between each radar word and perform segmentation. Afterward, the K-means++ algorithm with cosine distances is established to cluster the segmented PDWs according to the output vectors of the CNN’s last layer for radar words extraction. This segmenting and clustering process continues to go in the extracted radar word sequence, radar phase sequence, and so on, finishing the automatic extraction of MFR behavior states in the MFR hierarchical model. Simulation results show that without using any labeled data, the proposed method can effectively mine distinguishable patterns in the sequentially arriving PDWs and recognize the MFR behavior states under corrupted, overlapped pulse parameters.&lt;/div&gt;","['https://openalex.org/W2896457183', 'https://openalex.org/W3173837271', 'https://openalex.org/W3161492984', 'https://openalex.org/W4295312788', 'https://openalex.org/W3007858997', 'https://openalex.org/W2152790380', 'https://openalex.org/W130754613', 'https://openalex.org/W1498436455', 'https://openalex.org/W1511910497', 'https://openalex.org/W2076337359', 'https://openalex.org/W4304111904', 'https://openalex.org/W1987971958', 'https://openalex.org/W3096656254', 'https://openalex.org/W1614298861', 'https://openalex.org/W2113592174', 'https://openalex.org/W2947445680', 'https://openalex.org/W2973049979', 'https://openalex.org/W3021364482', 'https://openalex.org/W3013889479', 'https://openalex.org/W3034171073', 'https://openalex.org/W2949488362', 'https://openalex.org/W3023371261', 'https://openalex.org/W3107981615', 'https://openalex.org/W2073459066', 'https://openalex.org/W4297744580', 'https://openalex.org/W1977782550', 'https://openalex.org/W2886082382', 'https://openalex.org/W2962739339', 'https://openalex.org/W2157444450', 'https://openalex.org/W2115790511', 'https://openalex.org/W2997791733', 'https://openalex.org/W3110446398', 'https://openalex.org/W2599882013', 'https://openalex.org/W2768783378', 'https://openalex.org/W2108777121', 'https://openalex.org/W1531268951', 'https://openalex.org/W3164071761']",2022-01-11
https://openalex.org/W3114104342,https://doi.org/10.1142/s0218126621501887,High-Quality Many-to-Many Voice Conversion Using Transitive Star Generative Adversarial Networks with Adaptive Instance Normalization,"This paper proposes a novel high-quality nonparallel many-to-many voice conversion method based on transitive star generative adversarial networks with adaptive instance normalization (Trans-StarGAN-VC with AdaIN). First, we improve the structure of generator with TransNets to make full use of hierarchical features associated with speech naturalness. In TransNets, many shortcut connections share hierarchical features between encoding and decoding part to capture sufficient linguistic and semantic information, which helps to provide natural sounding converted speech and accelerate the convergence of training process. Second, by incorporating AdaIN for style transfer, we enable the generator to learn sufficient speaker characteristic information directly from speech instead of using attribute labels, which also provides a promising framework for one-shot VC. Objective and subjective experiments with nonparallel training data show that our method significantly outperforms StarGAN-VC in both speech naturalness and speaker similarity. The mean values of mean opinion score (MOS) and ABX are increased by 24.5% and 10.7%, respectively. The comparison of spectrogram also shows that our method can provide more complete harmonic structures and details, and effectively bridge the gap between converted speech and target speech.","['https://openalex.org/W2996797022', 'https://openalex.org/W2071431631', 'https://openalex.org/W3006824058', 'https://openalex.org/W2011378162', 'https://openalex.org/W2911494248', 'https://openalex.org/W2290946177', 'https://openalex.org/W2947445680', 'https://openalex.org/W2121095010', 'https://openalex.org/W2135832479', 'https://openalex.org/W2161476805', 'https://openalex.org/W2475998840', 'https://openalex.org/W2111550316', 'https://openalex.org/W2973135352', 'https://openalex.org/W2972544500', 'https://openalex.org/W3002433751', 'https://openalex.org/W2889061305', 'https://openalex.org/W2964069186', 'https://openalex.org/W2963539064', 'https://openalex.org/W2902070858', 'https://openalex.org/W2972667718', 'https://openalex.org/W2892734764', 'https://openalex.org/W2517513811', 'https://openalex.org/W2982444549', 'https://openalex.org/W2973127257', 'https://openalex.org/W2603777577', 'https://openalex.org/W2972452545', 'https://openalex.org/W2972659941', 'https://openalex.org/W2471520273', 'https://openalex.org/W2123771434', 'https://openalex.org/W2972399707', 'https://openalex.org/W3102628737', 'https://openalex.org/W3105124182', 'https://openalex.org/W3100378519', 'https://openalex.org/W2898832121']",2020-12-24
https://openalex.org/W3096262326,https://doi.org/10.21437/interspeech.2020-2731,Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020,"A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.","['https://openalex.org/W2402146185', 'https://openalex.org/W1796128977', 'https://openalex.org/W2972964185', 'https://openalex.org/W2972867623', 'https://openalex.org/W2947445680', 'https://openalex.org/W2949382160', 'https://openalex.org/W2335906338', 'https://openalex.org/W2347098582', 'https://openalex.org/W2015876361', 'https://openalex.org/W1524333225', 'https://openalex.org/W2100768664', 'https://openalex.org/W2547039119', 'https://openalex.org/W1967924372', 'https://openalex.org/W2963300588', 'https://openalex.org/W2598638573']",2020-10-25
https://openalex.org/W3129009457,https://doi.org/10.48550/arxiv.2102.01192,Generative Spoken Language Modeling from Raw Audio,"We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.","['https://openalex.org/W2787560479', 'https://openalex.org/W2996383576', 'https://openalex.org/W3097787369', 'https://openalex.org/W3096216486', 'https://openalex.org/W2100768664', 'https://openalex.org/W3095698432', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035202887', 'https://openalex.org/W2890983311', 'https://openalex.org/W2949382160', 'https://openalex.org/W1494198834', 'https://openalex.org/W3098403858', 'https://openalex.org/W3148040514', 'https://openalex.org/W3039910566', 'https://openalex.org/W2937090315', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972374322', 'https://openalex.org/W2973157397', 'https://openalex.org/W3095361818', 'https://openalex.org/W3163296124', 'https://openalex.org/W3095948607', 'https://openalex.org/W2963618559', 'https://openalex.org/W2973049979', 'https://openalex.org/W3095292526', 'https://openalex.org/W3049206033', 'https://openalex.org/W2933138175', 'https://openalex.org/W2750248772', 'https://openalex.org/W3096323553', 'https://openalex.org/W2127141656', 'https://openalex.org/W2971274815', 'https://openalex.org/W2950180292', 'https://openalex.org/W3003875258', 'https://openalex.org/W2888911345', 'https://openalex.org/W2963403868', 'https://openalex.org/W2160473997', 'https://openalex.org/W2963799213', 'https://openalex.org/W3096359985', 'https://openalex.org/W2947445680', 'https://openalex.org/W3099782249', 'https://openalex.org/W3003750857', 'https://openalex.org/W3033038061', 'https://openalex.org/W3112034174', 'https://openalex.org/W2347098582', 'https://openalex.org/W3015213852', 'https://openalex.org/W3015265920', 'https://openalex.org/W2346964103', 'https://openalex.org/W3125087428', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963300588', 'https://openalex.org/W2483390977', 'https://openalex.org/W2962850167', 'https://openalex.org/W2963456134', 'https://openalex.org/W2577366047', 'https://openalex.org/W3024040651', 'https://openalex.org/W3093096176', 'https://openalex.org/W2982399380', 'https://openalex.org/W3148101939', 'https://openalex.org/W2965373594', 'https://openalex.org/W3114436296', 'https://openalex.org/W3015356564', 'https://openalex.org/W2972943112']",2021-02-01
https://openalex.org/W3034058691,https://doi.org/10.48550/arxiv.2006.04154,VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture,"Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content. It is still a challenging work, especially in a one-shot setting. Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers. The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN). However, the imperfect disentanglement may harm the quality of output speech. In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system. We find that to leverage the U-Net architecture, a strong information bottleneck is necessary. The VQ-based method, which quantizes the latent vectors, can serve the purpose. The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.","['https://openalex.org/W2099471712', 'https://openalex.org/W2945478979', 'https://openalex.org/W2972659941', 'https://openalex.org/W2518172956', 'https://openalex.org/W2156142001', 'https://openalex.org/W2970351109', 'https://openalex.org/W2766527293', 'https://openalex.org/W1901129140', 'https://openalex.org/W2105160541', 'https://openalex.org/W2603777577', 'https://openalex.org/W2963539064', 'https://openalex.org/W2502312327', 'https://openalex.org/W3015805741', 'https://openalex.org/W2995005087', 'https://openalex.org/W2527729766', 'https://openalex.org/W2902070858', 'https://openalex.org/W3015434413', 'https://openalex.org/W3125709657', 'https://openalex.org/W2947445680', 'https://openalex.org/W2970006822', 'https://openalex.org/W2532494225', 'https://openalex.org/W3015419784', 'https://openalex.org/W2948211236', 'https://openalex.org/W2888922217', 'https://openalex.org/W2120605154', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963830550']",2020-06-07
https://openalex.org/W2991557631,https://doi.org/10.48550/arxiv.1911.09602,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,"In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3\% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.","['https://openalex.org/W2962753610', 'https://openalex.org/W2134670479', 'https://openalex.org/W1942713348', 'https://openalex.org/W2106053110', 'https://openalex.org/W2947590261', 'https://openalex.org/W2404799143', 'https://openalex.org/W2194775991', 'https://openalex.org/W2938991416', 'https://openalex.org/W2950133079', 'https://openalex.org/W2057007397', 'https://openalex.org/W2586148577', 'https://openalex.org/W2117041980', 'https://openalex.org/W2947445680', 'https://openalex.org/W2972345028', 'https://openalex.org/W2242818861', 'https://openalex.org/W30845872', 'https://openalex.org/W2963525826', 'https://openalex.org/W3125709657', 'https://openalex.org/W2962824709', 'https://openalex.org/W2758849341', 'https://openalex.org/W2964121744', 'https://openalex.org/W2889313720', 'https://openalex.org/W2483390977', 'https://openalex.org/W1778492285', 'https://openalex.org/W2395899413', 'https://openalex.org/W2964001192', 'https://openalex.org/W2971709506', 'https://openalex.org/W2953114965', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963317665', 'https://openalex.org/W2962850167', 'https://openalex.org/W2767754137', 'https://openalex.org/W2126203737', 'https://openalex.org/W2964169922', 'https://openalex.org/W2347098582', 'https://openalex.org/W2118841860', 'https://openalex.org/W2963330681', 'https://openalex.org/W2078769636', 'https://openalex.org/W2963902314', 'https://openalex.org/W2296607128', 'https://openalex.org/W2100768664', 'https://openalex.org/W2962732076', 'https://openalex.org/W2964099072', 'https://openalex.org/W2295297373', 'https://openalex.org/W2132921748', 'https://openalex.org/W2972943112', 'https://openalex.org/W2556930864', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972892814', 'https://openalex.org/W2964115348', 'https://openalex.org/W2989358187', 'https://openalex.org/W2786608204', 'https://openalex.org/W385555557', 'https://openalex.org/W2973135958', 'https://openalex.org/W2927673779', 'https://openalex.org/W2107917162', 'https://openalex.org/W2920166246', 'https://openalex.org/W2963417023', 'https://openalex.org/W2973157397', 'https://openalex.org/W2988907666', 'https://openalex.org/W2113896236', 'https://openalex.org/W2962862718', 'https://openalex.org/W2964249784', 'https://openalex.org/W2972867623', 'https://openalex.org/W2842511635', 'https://openalex.org/W2114347655', 'https://openalex.org/W2962978519', 'https://openalex.org/W2963618559', 'https://openalex.org/W2468716020', 'https://openalex.org/W2950414763']",2019-11-21
https://openalex.org/W3042203388,,Policy learning with partial observation and mechanical constraints for multi-person modeling,"Extracting the rules of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. Biological agents generally have limited observation and mechanical constraints; however, most of the conventional data-driven models ignore such assumptions, resulting in lack of biological plausibility and model interpretability for behavioral analyses in biological and cognitive science. Here we propose sequential generative models with partial observation and mechanical constraints, which can visualize whose information the agents utilize and can generate biologically plausible actions. We formulate this as a decentralized multi-agent imitation learning problem, leveraging binary partial observation models with a Gumbel-Softmax reparameterization and policy models based on hierarchical variational recurrent neural networks with physical and biomechanical constraints. We investigate the empirical performances using real-world multi-person motion datasets from basketball and soccer games.","['https://openalex.org/W2950950272', 'https://openalex.org/W2885318751', 'https://openalex.org/W2143969246', 'https://openalex.org/W2016589492', 'https://openalex.org/W2963166838', 'https://openalex.org/W2970116586', 'https://openalex.org/W2962957031', 'https://openalex.org/W2962715980', 'https://openalex.org/W2963562809', 'https://openalex.org/W2962695963', 'https://openalex.org/W2964121744', 'https://openalex.org/W2168359464', 'https://openalex.org/W2963062607', 'https://openalex.org/W2464234964', 'https://openalex.org/W2991489433', 'https://openalex.org/W2548228487', 'https://openalex.org/W2963717208', 'https://openalex.org/W2962947707', 'https://openalex.org/W2167052694', 'https://openalex.org/W2601465345', 'https://openalex.org/W2424778531', 'https://openalex.org/W1959608418', 'https://openalex.org/W2553756201', 'https://openalex.org/W2963464736', 'https://openalex.org/W2405096793', 'https://openalex.org/W2963279312', 'https://openalex.org/W2166302491', 'https://openalex.org/W2947445680', 'https://openalex.org/W2557519264', 'https://openalex.org/W2964308564', 'https://openalex.org/W2329185630', 'https://openalex.org/W2964199361', 'https://openalex.org/W2088956500', 'https://openalex.org/W2112164016', 'https://openalex.org/W2963091558', 'https://openalex.org/W2548815430', 'https://openalex.org/W2064675550', 'https://openalex.org/W2986244508', 'https://openalex.org/W2793483732', 'https://openalex.org/W2971179953', 'https://openalex.org/W2963755523', 'https://openalex.org/W2982202190', 'https://openalex.org/W2964339599', 'https://openalex.org/W2773355582', 'https://openalex.org/W2998212266', 'https://openalex.org/W2914524146', 'https://openalex.org/W3093010610', 'https://openalex.org/W592244745', 'https://openalex.org/W2998367975', 'https://openalex.org/W2905173465', 'https://openalex.org/W2949269657', 'https://openalex.org/W2962966033', 'https://openalex.org/W1985102978', 'https://openalex.org/W2982745079', 'https://openalex.org/W2963001155', 'https://openalex.org/W1885639605', 'https://openalex.org/W2915130814', 'https://openalex.org/W2064527819', 'https://openalex.org/W2896332777', 'https://openalex.org/W2971106883']",2020-07-07
https://openalex.org/W2972805867,https://doi.org/10.48550/arxiv.1909.06532,Bootstrapping non-parallel voice conversion from speaker-adaptive text-to-speech,"Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a similar objective, generating speech with a target voice. However, they are usually developed independently under vastly different frameworks. In this paper, we propose a methodology to bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Moreover by offloading the heavy data demand to the training stage of the TTS model, our VC system can be built using a small amount of target speaker speech data. It also opens up the possibility of using speech in a foreign unseen language to build the system. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.","['https://openalex.org/W1974745215', 'https://openalex.org/W2013996527', 'https://openalex.org/W2938583109', 'https://openalex.org/W1981457580', 'https://openalex.org/W2774848319', 'https://openalex.org/W2507912506', 'https://openalex.org/W2964069186', 'https://openalex.org/W2120605154', 'https://openalex.org/W2889329491', 'https://openalex.org/W2518172956', 'https://openalex.org/W113498433', 'https://openalex.org/W2527729766', 'https://openalex.org/W2899877258', 'https://openalex.org/W2947445680', 'https://openalex.org/W1588266896', 'https://openalex.org/W2940544976', 'https://openalex.org/W2788266530', 'https://openalex.org/W2977798327', 'https://openalex.org/W2950414763', 'https://openalex.org/W2963035245', 'https://openalex.org/W33533989', 'https://openalex.org/W3101689408', 'https://openalex.org/W2800289214', 'https://openalex.org/W2086796102', 'https://openalex.org/W2929315483', 'https://openalex.org/W49412823', 'https://openalex.org/W2936295285', 'https://openalex.org/W2887264325', 'https://openalex.org/W2156142001', 'https://openalex.org/W2532494225', 'https://openalex.org/W2134202996', 'https://openalex.org/W2950224550', 'https://openalex.org/W2161476805', 'https://openalex.org/W2947591107', 'https://openalex.org/W2963808252', 'https://openalex.org/W2973034126', 'https://openalex.org/W2808706139', 'https://openalex.org/W2885820941', 'https://openalex.org/W1538607601']",2019-09-14
https://openalex.org/W2977997709,https://doi.org/10.48550/arxiv.1910.00795,Speech-to-speech Translation between Untranscribed Unknown Languages,"In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target language's discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.","['https://openalex.org/W2101105183', 'https://openalex.org/W2962699523', 'https://openalex.org/W2789543585', 'https://openalex.org/W2786608204', 'https://openalex.org/W2964026424', 'https://openalex.org/W2947445680', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963796886', 'https://openalex.org/W1522301498', 'https://openalex.org/W2906111771', 'https://openalex.org/W854541894', 'https://openalex.org/W2963609956', 'https://openalex.org/W2161742089', 'https://openalex.org/W2963581463', 'https://openalex.org/W2884852625', 'https://openalex.org/W2123301721', 'https://openalex.org/W1902237438', 'https://openalex.org/W2025768430', 'https://openalex.org/W2130942839', 'https://openalex.org/W2582956876', 'https://openalex.org/W2949117887', 'https://openalex.org/W2972374322', 'https://openalex.org/W2936184970', 'https://openalex.org/W2964308564', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963620343', 'https://openalex.org/W2973026522', 'https://openalex.org/W2466918907', 'https://openalex.org/W2605131327', 'https://openalex.org/W2144600658', 'https://openalex.org/W1921523184', 'https://openalex.org/W2064675550', 'https://openalex.org/W206967138', 'https://openalex.org/W2191779130', 'https://openalex.org/W2962680099']",2019-10-02
https://openalex.org/W3127721277,https://doi.org/10.48550/arxiv.2102.00184,Adversarially learning disentangled speech representations for robust multi-factor voice conversion,"Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speechfactors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment,which however is hard to ensure robust speech representationdisentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP)network inspired by BERT. The adversarial network is used tominimize the correlations between the speech representations,by randomly masking and predicting one of the representationsfrom the others. Experimental results show that the proposedframework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to3.30 and decreasing the MCD from 3.89 to 3.58.","['https://openalex.org/W3130305523', 'https://openalex.org/W2981461916', 'https://openalex.org/W3137768380', 'https://openalex.org/W2963927338', 'https://openalex.org/W2946809691', 'https://openalex.org/W1731081199', 'https://openalex.org/W3030987249', 'https://openalex.org/W3095361818', 'https://openalex.org/W3015434413', 'https://openalex.org/W2462831000', 'https://openalex.org/W3133667170', 'https://openalex.org/W2890964092', 'https://openalex.org/W3096255817', 'https://openalex.org/W3015645837', 'https://openalex.org/W3163475957', 'https://openalex.org/W2945478979', 'https://openalex.org/W2951298482', 'https://openalex.org/W2963226019', 'https://openalex.org/W2937020545', 'https://openalex.org/W3197411683', 'https://openalex.org/W2527729766', 'https://openalex.org/W2970006822', 'https://openalex.org/W3020570669', 'https://openalex.org/W3034303964', 'https://openalex.org/W3048845410', 'https://openalex.org/W2963272440', 'https://openalex.org/W3015699566', 'https://openalex.org/W3100378519', 'https://openalex.org/W3122989584', 'https://openalex.org/W2947445680', 'https://openalex.org/W2042204882', 'https://openalex.org/W2928664166', 'https://openalex.org/W2046056978', 'https://openalex.org/W2939131199', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963830550', 'https://openalex.org/W2972921407', 'https://openalex.org/W3135828102', 'https://openalex.org/W2964058423', 'https://openalex.org/W2899877258', 'https://openalex.org/W2889061305', 'https://openalex.org/W2982223350', 'https://openalex.org/W3168542456', 'https://openalex.org/W2972659941', 'https://openalex.org/W3095035471']",2021-01-30
https://openalex.org/W3084014658,https://doi.org/10.48550/arxiv.2009.04983,Exploration of End-to-end Synthesisers forZero Resource Speech Challenge 2020,"A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.","['https://openalex.org/W2964243274', 'https://openalex.org/W2402146185', 'https://openalex.org/W2335906338', 'https://openalex.org/W1796128977', 'https://openalex.org/W2972374322', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963300588', 'https://openalex.org/W2547039119', 'https://openalex.org/W2972964185', 'https://openalex.org/W2519091744', 'https://openalex.org/W2015876361', 'https://openalex.org/W1967924372', 'https://openalex.org/W2892140764', 'https://openalex.org/W2020607164', 'https://openalex.org/W2748488820', 'https://openalex.org/W2347098582', 'https://openalex.org/W2598638573', 'https://openalex.org/W2947445680', 'https://openalex.org/W2890964092', 'https://openalex.org/W1524333225', 'https://openalex.org/W2973026522', 'https://openalex.org/W2100768664', 'https://openalex.org/W2191779130', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963139417']",2020-09-10
https://openalex.org/W3109037759,https://doi.org/10.48550/arxiv.2011.12063,How Far Are We from Robust Voice Conversion: A Survey,"Voice conversion technologies have been greatly improved in recent years with the help of deep learning, but their capabilities of producing natural sounding utterances in different conditions remain unclear. In this paper, we gave a thorough study of the robustness of known VC models. We also modified these models, such as the replacement of speaker embeddings, to further improve their performances. We found that the sampling rate and audio duration greatly influence voice conversion. All the VC models suffer from unseen data, but AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained is more suitable for voice conversion than those trained on speaker identification.","['https://openalex.org/W2973154337', 'https://openalex.org/W2120847449', 'https://openalex.org/W2947445680', 'https://openalex.org/W2963539064', 'https://openalex.org/W2890964092', 'https://openalex.org/W2951004968', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963830550', 'https://openalex.org/W95152782', 'https://openalex.org/W2970351109', 'https://openalex.org/W3024610089', 'https://openalex.org/W2970006822', 'https://openalex.org/W2972359262', 'https://openalex.org/W1524333225', 'https://openalex.org/W2519091744', 'https://openalex.org/W2808631503', 'https://openalex.org/W2972659941', 'https://openalex.org/W2949281321', 'https://openalex.org/W2046056978', 'https://openalex.org/W2121415728', 'https://openalex.org/W2284628133', 'https://openalex.org/W2898786057', 'https://openalex.org/W2603777577', 'https://openalex.org/W2527729766', 'https://openalex.org/W2902070858', 'https://openalex.org/W1991938399', 'https://openalex.org/W2532494225', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962788625', 'https://openalex.org/W2888922217', 'https://openalex.org/W2726515241', 'https://openalex.org/W3020570669']",2020-11-24
https://openalex.org/W4205215624,https://doi.org/10.36227/techrxiv.17976062.v1,Towards Self-supervised Learning for Multi-function Radar Behavior State Detection and Recognition,"&lt;div&gt;The analysis of intercepted multi-function radar (MFR) signals has gained considerable attention in the field of cognitive electronic reconnaissance. With the rapid development of MFR, the switch between different work modes is becoming more flexible, increasing the agility of pulse parameters. Most of the existing approaches for recognizing MFR behaviors heavily depend on prior information, which can hardly be obtained in a non-cooperative way. This study develops a novel hierarchical contrastive self-supervise-based method for segmenting and clustering MFR pulse sequences. First, a convolutional neural network (CNN) with a limited receptive field is trained in a contrastive way to distinguish between pulse descriptor words (PDW) in the original order and the samples created by random permutations to detect the boundary between each radar word and perform segmentation. Afterward, the K-means++ algorithm with cosine distances is established to cluster the segmented PDWs according to the output vectors of the CNN’s last layer for radar words extraction. This segmenting and clustering process continues to go in the extracted radar word sequence, radar phase sequence, and so on, finishing the automatic extraction of MFR behavior states in the MFR hierarchical model. Simulation results show that without using any labeled data, the proposed method can effectively mine distinguishable patterns in the sequentially arriving PDWs and recognize the MFR behavior states under corrupted, overlapped pulse parameters.&lt;/div&gt;","['https://openalex.org/W1977782550', 'https://openalex.org/W2108777121', 'https://openalex.org/W2768783378', 'https://openalex.org/W2115790511', 'https://openalex.org/W6630754547', 'https://openalex.org/W3021364482', 'https://openalex.org/W3007858997', 'https://openalex.org/W3107981615', 'https://openalex.org/W3161492984', 'https://openalex.org/W2886082382', 'https://openalex.org/W3173837271', 'https://openalex.org/W3034171073', 'https://openalex.org/W3164071761', 'https://openalex.org/W2911779594', 'https://openalex.org/W3039616371', 'https://openalex.org/W3133631758', 'https://openalex.org/W3043897409', 'https://openalex.org/W2947445680', 'https://openalex.org/W1531268951', 'https://openalex.org/W6669402789', 'https://openalex.org/W2113592174', 'https://openalex.org/W2152790380', 'https://openalex.org/W2939710050', 'https://openalex.org/W1498436455', 'https://openalex.org/W2949488362', 'https://openalex.org/W1987971958', 'https://openalex.org/W1614298861', 'https://openalex.org/W2599882013', 'https://openalex.org/W3013889479', 'https://openalex.org/W2970971581', 'https://openalex.org/W130754613', 'https://openalex.org/W2896457183', 'https://openalex.org/W6748634344', 'https://openalex.org/W4304111904', 'https://openalex.org/W2962739339', 'https://openalex.org/W3110446398', 'https://openalex.org/W3023371261', 'https://openalex.org/W3096656254', 'https://openalex.org/W2076337359', 'https://openalex.org/W2157444450', 'https://openalex.org/W4295312788', 'https://openalex.org/W2073459066', 'https://openalex.org/W1511910497', 'https://openalex.org/W2973049979', 'https://openalex.org/W4297744580', 'https://openalex.org/W2997791733']",2022-01-11
https://openalex.org/W4224937968,https://doi.org/10.48550/arxiv.2204.11806,Parallel Synthesis for Autoregressive Speech Generation,"Autoregressive neural vocoders have achieved outstanding performance in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it synthesizes natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech sequence in parallel and proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is split into frequency subbands, and a subband is generated conditioned on the previously generated one. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance length but to the number of subbands/bits, significantly increasing inference efficiency. Besides, a post-filter is employed to sample signals from output posteriors; its training objective is designed based on the characteristics of the proposed methods. Experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with baseline vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability for unseen speakers and 44 kHz speech.","['https://openalex.org/W2598638573', 'https://openalex.org/W3034794073', 'https://openalex.org/W2172065531', 'https://openalex.org/W3163662330', 'https://openalex.org/W3197294703', 'https://openalex.org/W2969937244', 'https://openalex.org/W4294619240', 'https://openalex.org/W2107860279', 'https://openalex.org/W4287327373', 'https://openalex.org/W2993118648', 'https://openalex.org/W2395578248', 'https://openalex.org/W2767206889', 'https://openalex.org/W2587284713', 'https://openalex.org/W2954386831', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385245566', 'https://openalex.org/W3161704465', 'https://openalex.org/W4289761690', 'https://openalex.org/W3097538987', 'https://openalex.org/W3125709657', 'https://openalex.org/W2118774185', 'https://openalex.org/W3034999214', 'https://openalex.org/W4225304461', 'https://openalex.org/W3015338123', 'https://openalex.org/W2785516183', 'https://openalex.org/W2423557781', 'https://openalex.org/W2964041258', 'https://openalex.org/W4287694050', 'https://openalex.org/W1522301498', 'https://openalex.org/W4231807801', 'https://openalex.org/W2102003408', 'https://openalex.org/W2148770129', 'https://openalex.org/W4286899907', 'https://openalex.org/W4298580827', 'https://openalex.org/W3123097577', 'https://openalex.org/W2970006822', 'https://openalex.org/W2143280761', 'https://openalex.org/W3109064156', 'https://openalex.org/W3034363136', 'https://openalex.org/W3095459301', 'https://openalex.org/W2890983311', 'https://openalex.org/W3096702751', 'https://openalex.org/W2118806033', 'https://openalex.org/W2999160446', 'https://openalex.org/W3033411150', 'https://openalex.org/W2885820941', 'https://openalex.org/W3162770051', 'https://openalex.org/W3092028330', 'https://openalex.org/W2901710569', 'https://openalex.org/W2120847449', 'https://openalex.org/W4283817920', 'https://openalex.org/W4320013936', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963609956', 'https://openalex.org/W2953318193', 'https://openalex.org/W2519091744', 'https://openalex.org/W2805217154', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963091184', 'https://openalex.org/W3095419383', 'https://openalex.org/W2963103134', 'https://openalex.org/W95152782', 'https://openalex.org/W4377079846', 'https://openalex.org/W2964243274', 'https://openalex.org/W3105124182', 'https://openalex.org/W4292779060', 'https://openalex.org/W3097828251', 'https://openalex.org/W1521637132', 'https://openalex.org/W2963434219', 'https://openalex.org/W2888169323', 'https://openalex.org/W4289305009', 'https://openalex.org/W4287375043', 'https://openalex.org/W3095851005', 'https://openalex.org/W2945478979', 'https://openalex.org/W2947445680', 'https://openalex.org/W3144035034', 'https://openalex.org/W2994373303', 'https://openalex.org/W2784918340', 'https://openalex.org/W1494895475', 'https://openalex.org/W2889606145', 'https://openalex.org/W2912237252', 'https://openalex.org/W2963300588', 'https://openalex.org/W2111284386', 'https://openalex.org/W3008758407', 'https://openalex.org/W3129651364', 'https://openalex.org/W3097895838', 'https://openalex.org/W4283388932']",2022-04-25
https://openalex.org/W2995181338,https://doi.org/10.1109/icassp40776.2020.9052942,Libri-Light: A Benchmark for ASR with Limited or No Supervision,"We introduce a new collection of spoken English audio suitable for training\nspeech recognition systems under limited or no supervision. It is derived from\nopen-source audio books from the LibriVox project. It contains over 60K hours\nof audio, which is, to our knowledge, the largest freely-available corpus of\nspeech. The audio has been segmented using voice activity detection and is\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\nbaseline systems and evaluation metrics working under three settings: (1) the\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\nstandard LibriSpeech dev and test sets for comparison with the supervised\nstate-of-the-art.\n","['https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6629717138', 'https://openalex.org/W2937197076', 'https://openalex.org/W6712444837', 'https://openalex.org/W2592866267', 'https://openalex.org/W2953190524', 'https://openalex.org/W3005511757', 'https://openalex.org/W6751433836', 'https://openalex.org/W6770514103', 'https://openalex.org/W6756326128', 'https://openalex.org/W1970890968', 'https://openalex.org/W6656619859', 'https://openalex.org/W2127141656', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6775452034', 'https://openalex.org/W2346964103', 'https://openalex.org/W4234016251', 'https://openalex.org/W6747270024', 'https://openalex.org/W6679855610', 'https://openalex.org/W2944255943', 'https://openalex.org/W6748342566', 'https://openalex.org/W2972630480', 'https://openalex.org/W2963425185', 'https://openalex.org/W2161482971', 'https://openalex.org/W4288107125', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963340922', 'https://openalex.org/W2899377381', 'https://openalex.org/W4297818305', 'https://openalex.org/W2134800885', 'https://openalex.org/W2593779438', 'https://openalex.org/W3103005696', 'https://openalex.org/W3016181583', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2804648901', 'https://openalex.org/W2025198378', 'https://openalex.org/W2161391345', 'https://openalex.org/W2787447541', 'https://openalex.org/W2926063217', 'https://openalex.org/W2794753807', 'https://openalex.org/W4300047444', 'https://openalex.org/W3015522062', 'https://openalex.org/W2395899413', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2973026522', 'https://openalex.org/W2781384251']",2020-04-09
https://openalex.org/W2786608204,https://doi.org/10.21437/interspeech.2015-638,The zero resource speech challenge 2015,"établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.","['https://openalex.org/W2052697931', 'https://openalex.org/W2114347655', 'https://openalex.org/W1524333225', 'https://openalex.org/W2787426069', 'https://openalex.org/W2079460648', 'https://openalex.org/W2572097499', 'https://openalex.org/W2101509422', 'https://openalex.org/W2787447541', 'https://openalex.org/W2466918907', 'https://openalex.org/W2055408826', 'https://openalex.org/W2170659185', 'https://openalex.org/W2049142189', 'https://openalex.org/W2251025892', 'https://openalex.org/W2150389998', 'https://openalex.org/W2284628133', 'https://openalex.org/W979905723', 'https://openalex.org/W2117126688', 'https://openalex.org/W1796128977', 'https://openalex.org/W2786608204', 'https://openalex.org/W1922655562', 'https://openalex.org/W1545920196', 'https://openalex.org/W2404799143', 'https://openalex.org/W2128032727', 'https://openalex.org/W1967924372', 'https://openalex.org/W91681889', 'https://openalex.org/W2126377586', 'https://openalex.org/W2238331496', 'https://openalex.org/W2078769636', 'https://openalex.org/W2786902352', 'https://openalex.org/W2395899413', 'https://openalex.org/W2247128061', 'https://openalex.org/W2483390977', 'https://openalex.org/W2117041980', 'https://openalex.org/W2949328740', 'https://openalex.org/W2100768664', 'https://openalex.org/W2406349064', 'https://openalex.org/W2101281673', 'https://openalex.org/W2044138293', 'https://openalex.org/W2346964103', 'https://openalex.org/W2057007397', 'https://openalex.org/W2020607164', 'https://openalex.org/W2787223168', 'https://openalex.org/W2399576818', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963211739', 'https://openalex.org/W2785860501']",2015-09-06
https://openalex.org/W3144810982,https://doi.org/10.48550/arxiv.2007.00991,Data Augmenting Contrastive Learning of Speech Representations in the\n Time Domain,"Contrastive Predictive Coding (CPC), based on predicting future segments of\nspeech based on past segments is emerging as a powerful algorithm for\nrepresentation learning of speech signal. However, it still under-performs\nother methods on unsupervised evaluation benchmarks. Here, we introduce\nWavAugment, a time-domain data augmentation library and find that applying\naugmentation in the past is generally more efficient and yields better\nperformances than other methods. We find that a combination of pitch\nmodification, additive noise and reverberation substantially increase the\nperformance of CPC (relative improvement of 18-22%), beating the reference\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\ntime-domain data augmentation can push CPC to be on par with the state of the\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\naugmentation consistently improves downstream limited-supervision phoneme\nclassification tasks by a factor of 12-15% relative.\n","['https://openalex.org/W3015213852', 'https://openalex.org/W2148349024', 'https://openalex.org/W2346964103', 'https://openalex.org/W2593779438', 'https://openalex.org/W2407080277', 'https://openalex.org/W3005680577', 'https://openalex.org/W2972943112', 'https://openalex.org/W2696967604', 'https://openalex.org/W2184343439', 'https://openalex.org/W2990583358', 'https://openalex.org/W2963620343', 'https://openalex.org/W2055408826', 'https://openalex.org/W2883725317', 'https://openalex.org/W3100270690', 'https://openalex.org/W2973049979', 'https://openalex.org/W4288107125', 'https://openalex.org/W2786608204', 'https://openalex.org/W3016011332', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016181583', 'https://openalex.org/W3002741552', 'https://openalex.org/W2973026522', 'https://openalex.org/W2930682606', 'https://openalex.org/W2100768664', 'https://openalex.org/W2787447541', 'https://openalex.org/W3093427098', 'https://openalex.org/W3015783745', 'https://openalex.org/W3102342027', 'https://openalex.org/W4300047444', 'https://openalex.org/W3093096176', 'https://openalex.org/W2940544976', 'https://openalex.org/W3125709657', 'https://openalex.org/W4214784181', 'https://openalex.org/W2983785920', 'https://openalex.org/W2952217990', 'https://openalex.org/W2963074118', 'https://openalex.org/W2509930204', 'https://openalex.org/W2117041980', 'https://openalex.org/W2946822591', 'https://openalex.org/W1494198834', 'https://openalex.org/W2219249508', 'https://openalex.org/W2970971581', 'https://openalex.org/W2842511635', 'https://openalex.org/W2347098582', 'https://openalex.org/W3003875258', 'https://openalex.org/W4295312788', 'https://openalex.org/W1989674786', 'https://openalex.org/W2020607164', 'https://openalex.org/W2936774411']",2020-07-02
https://openalex.org/W4289946024,https://doi.org/10.1007/s13735-022-00245-6,"Contrastive self-supervised learning: review, progress, challenges and future research directions",,"['https://openalex.org/W2108598243', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963446712', 'https://openalex.org/W2102605133', 'https://openalex.org/W1903029394', 'https://openalex.org/W2612769033', 'https://openalex.org/W6702248584', 'https://openalex.org/W6600424091', 'https://openalex.org/W6832427677', 'https://openalex.org/W2962922117', 'https://openalex.org/W2963748441', 'https://openalex.org/W2889787757', 'https://openalex.org/W2962858109', 'https://openalex.org/W2163922914', 'https://openalex.org/W2964101952', 'https://openalex.org/W2973049979', 'https://openalex.org/W3145450063', 'https://openalex.org/W3159481202', 'https://openalex.org/W6605755270', 'https://openalex.org/W4313156423', 'https://openalex.org/W3160799772', 'https://openalex.org/W2096375888', 'https://openalex.org/W2148764920', 'https://openalex.org/W3102174132', 'https://openalex.org/W2619697695', 'https://openalex.org/W2963115079', 'https://openalex.org/W2964037671', 'https://openalex.org/W3034781633', 'https://openalex.org/W2550462002', 'https://openalex.org/W2799087757', 'https://openalex.org/W2798271879', 'https://openalex.org/W2766042539', 'https://openalex.org/W3145385912', 'https://openalex.org/W2970119519', 'https://openalex.org/W3105966348', 'https://openalex.org/W3015356564', 'https://openalex.org/W6600195515', 'https://openalex.org/W4226033575', 'https://openalex.org/W3204696009', 'https://openalex.org/W6605905859', 'https://openalex.org/W3035725276', 'https://openalex.org/W6602522970', 'https://openalex.org/W2986615800', 'https://openalex.org/W3164825936', 'https://openalex.org/W2164122462', 'https://openalex.org/W2025768430', 'https://openalex.org/W16016350', 'https://openalex.org/W2558661413', 'https://openalex.org/W2966661', 'https://openalex.org/W3175593095', 'https://openalex.org/W3096655658', 'https://openalex.org/W3114632476', 'https://openalex.org/W2798991696', 'https://openalex.org/W3035524453', 'https://openalex.org/W2963465221', 'https://openalex.org/W3108655343', 'https://openalex.org/W2768489488', 'https://openalex.org/W3173783447', 'https://openalex.org/W3026732421', 'https://openalex.org/W3156636935', 'https://openalex.org/W3175362188', 'https://openalex.org/W2963098487', 'https://openalex.org/W4287798997', 'https://openalex.org/W6601223237', 'https://openalex.org/W2963680395', 'https://openalex.org/W2986131686', 'https://openalex.org/W3108367559', 'https://openalex.org/W2982619606', 'https://openalex.org/W3034576826', 'https://openalex.org/W6634814803', 'https://openalex.org/W3166500718', 'https://openalex.org/W6600018615', 'https://openalex.org/W6601883215', 'https://openalex.org/W4285602451', 'https://openalex.org/W2096733369', 'https://openalex.org/W2921310091', 'https://openalex.org/W2157364932', 'https://openalex.org/W2138621090', 'https://openalex.org/W2963026686', 'https://openalex.org/W6680962578', 'https://openalex.org/W6859103034', 'https://openalex.org/W6600266142', 'https://openalex.org/W3169172630', 'https://openalex.org/W6600146248', 'https://openalex.org/W2063971957', 'https://openalex.org/W2127589108', 'https://openalex.org/W3171975879', 'https://openalex.org/W6604662147', 'https://openalex.org/W2971296908', 'https://openalex.org/W6635373764', 'https://openalex.org/W3132311534', 'https://openalex.org/W3104453885', 'https://openalex.org/W3101066076', 'https://openalex.org/W6600047755', 'https://openalex.org/W3106743555', 'https://openalex.org/W3034381931', 'https://openalex.org/W6605475740', 'https://openalex.org/W3176780013', 'https://openalex.org/W6600076646', 'https://openalex.org/W2990503944', 'https://openalex.org/W3035046783', 'https://openalex.org/W3010874390', 'https://openalex.org/W3115964123', 'https://openalex.org/W3010035902', 'https://openalex.org/W2883725317', 'https://openalex.org/W2998388430', 'https://openalex.org/W6600234944', 'https://openalex.org/W3048735073', 'https://openalex.org/W3110190397', 'https://openalex.org/W3163676133', 'https://openalex.org/W3046614818', 'https://openalex.org/W3047425522', 'https://openalex.org/W3116298410', 'https://openalex.org/W3035635319', 'https://openalex.org/W3127324885', 'https://openalex.org/W3093096176', 'https://openalex.org/W2055408826', 'https://openalex.org/W2963620343', 'https://openalex.org/W6726178946', 'https://openalex.org/W3003875258', 'https://openalex.org/W2787447541', 'https://openalex.org/W2936774411', 'https://openalex.org/W3206495532', 'https://openalex.org/W3162391496', 'https://openalex.org/W4221023051', 'https://openalex.org/W6602342606', 'https://openalex.org/W3166743354', 'https://openalex.org/W6610450269', 'https://openalex.org/W6605559661', 'https://openalex.org/W6606191167', 'https://openalex.org/W3175300676', 'https://openalex.org/W6831923202', 'https://openalex.org/W6602976299', 'https://openalex.org/W3017343282', 'https://openalex.org/W3034742263', 'https://openalex.org/W6600157417', 'https://openalex.org/W3015734344', 'https://openalex.org/W3105816068', 'https://openalex.org/W2970641574', 'https://openalex.org/W4205371973', 'https://openalex.org/W3155146092', 'https://openalex.org/W3087124270', 'https://openalex.org/W3169978599', 'https://openalex.org/W3173972271', 'https://openalex.org/W3204453541', 'https://openalex.org/W4221163898', 'https://openalex.org/W4223948957', 'https://openalex.org/W3101999878']",2022-08-05
https://openalex.org/W3170201991,https://doi.org/10.1016/j.specom.2022.02.005,Unsupervised Automatic Speech Recognition: A review,"Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.\n","['https://openalex.org/W2395342389', 'https://openalex.org/W2962777061', 'https://openalex.org/W6744797106', 'https://openalex.org/W2523298034', 'https://openalex.org/W2578392894', 'https://openalex.org/W6655470396', 'https://openalex.org/W2396043527', 'https://openalex.org/W6780218876', 'https://openalex.org/W2074546930', 'https://openalex.org/W2064699871', 'https://openalex.org/W2407151108', 'https://openalex.org/W2399576818', 'https://openalex.org/W6748489002', 'https://openalex.org/W2972706021', 'https://openalex.org/W2962824709', 'https://openalex.org/W2586148577', 'https://openalex.org/W2510842514', 'https://openalex.org/W2963425185', 'https://openalex.org/W2972943112', 'https://openalex.org/W2804648901', 'https://openalex.org/W2963571336', 'https://openalex.org/W6607396543', 'https://openalex.org/W6758654326', 'https://openalex.org/W2144800021', 'https://openalex.org/W2097207027', 'https://openalex.org/W2940544976', 'https://openalex.org/W6697293080', 'https://openalex.org/W3093096176', 'https://openalex.org/W2110485445', 'https://openalex.org/W2107959623', 'https://openalex.org/W2107038463', 'https://openalex.org/W2758697525', 'https://openalex.org/W6677207036', 'https://openalex.org/W2056486423', 'https://openalex.org/W2111732304', 'https://openalex.org/W2160407676', 'https://openalex.org/W2122228338', 'https://openalex.org/W2126377586', 'https://openalex.org/W6622607008', 'https://openalex.org/W2127141656', 'https://openalex.org/W3097777922', 'https://openalex.org/W3097056138', 'https://openalex.org/W2889282842', 'https://openalex.org/W2972630480', 'https://openalex.org/W6680628865', 'https://openalex.org/W6761083334', 'https://openalex.org/W2556930864', 'https://openalex.org/W6685484048', 'https://openalex.org/W2194775991', 'https://openalex.org/W2345811097', 'https://openalex.org/W6748342566', 'https://openalex.org/W2090861223', 'https://openalex.org/W2160815625', 'https://openalex.org/W6754770876', 'https://openalex.org/W6804246771', 'https://openalex.org/W3114632476', 'https://openalex.org/W6656737381', 'https://openalex.org/W2407964052', 'https://openalex.org/W6664486393', 'https://openalex.org/W2126953647', 'https://openalex.org/W2126449874', 'https://openalex.org/W4252331534', 'https://openalex.org/W2059824090', 'https://openalex.org/W6771812881', 'https://openalex.org/W6756098772', 'https://openalex.org/W6632653590', 'https://openalex.org/W2468716020', 'https://openalex.org/W6735305794', 'https://openalex.org/W2407080277', 'https://openalex.org/W3096656254', 'https://openalex.org/W6773511912', 'https://openalex.org/W6765510844', 'https://openalex.org/W6634688400', 'https://openalex.org/W2100768664', 'https://openalex.org/W1778492285', 'https://openalex.org/W6665204316', 'https://openalex.org/W2114510609', 'https://openalex.org/W2618238855', 'https://openalex.org/W2962799225', 'https://openalex.org/W2078834097', 'https://openalex.org/W1965635292', 'https://openalex.org/W6684442412', 'https://openalex.org/W2963137467', 'https://openalex.org/W6766240100', 'https://openalex.org/W2089458547', 'https://openalex.org/W2096330373', 'https://openalex.org/W2083393647', 'https://openalex.org/W2114347655', 'https://openalex.org/W2145410271', 'https://openalex.org/W2013588070', 'https://openalex.org/W2750499125', 'https://openalex.org/W2072054026', 'https://openalex.org/W2010188467', 'https://openalex.org/W2398490608', 'https://openalex.org/W2768381684', 'https://openalex.org/W130754613', 'https://openalex.org/W6773553514', 'https://openalex.org/W2115867364', 'https://openalex.org/W6670225552', 'https://openalex.org/W2395899413', 'https://openalex.org/W6743102125', 'https://openalex.org/W3102519966', 'https://openalex.org/W6754496211', 'https://openalex.org/W2158266063', 'https://openalex.org/W3096359985', 'https://openalex.org/W3111682954', 'https://openalex.org/W2786608204', 'https://openalex.org/W6677007964', 'https://openalex.org/W2962799131', 'https://openalex.org/W6751975539', 'https://openalex.org/W6684399791', 'https://openalex.org/W2906122999', 'https://openalex.org/W6738476557', 'https://openalex.org/W2511733680', 'https://openalex.org/W2973013862', 'https://openalex.org/W6684944071', 'https://openalex.org/W6678470480', 'https://openalex.org/W6670629611', 'https://openalex.org/W6643712209', 'https://openalex.org/W2890964092', 'https://openalex.org/W3097485645', 'https://openalex.org/W2193413348', 'https://openalex.org/W4365806309', 'https://openalex.org/W2786902352', 'https://openalex.org/W3095361818', 'https://openalex.org/W3148186152', 'https://openalex.org/W2171019095', 'https://openalex.org/W2057007397', 'https://openalex.org/W2972937794', 'https://openalex.org/W4248805241', 'https://openalex.org/W792183615', 'https://openalex.org/W4297808394', 'https://openalex.org/W2520160253', 'https://openalex.org/W1974540032', 'https://openalex.org/W3036601975', 'https://openalex.org/W180242331', 'https://openalex.org/W2988736778', 'https://openalex.org/W3098643042', 'https://openalex.org/W3099142230', 'https://openalex.org/W2079623482', 'https://openalex.org/W2122364000', 'https://openalex.org/W3110761489', 'https://openalex.org/W4287173589', 'https://openalex.org/W3006094508', 'https://openalex.org/W2614103613', 'https://openalex.org/W4254816979', 'https://openalex.org/W3016181583', 'https://openalex.org/W4299579390', 'https://openalex.org/W2991213871', 'https://openalex.org/W3093427098', 'https://openalex.org/W2915722758', 'https://openalex.org/W2799046698', 'https://openalex.org/W1576278180', 'https://openalex.org/W2103810867', 'https://openalex.org/W2950577311', 'https://openalex.org/W2748009955', 'https://openalex.org/W1578200545', 'https://openalex.org/W1614298861', 'https://openalex.org/W2614542633', 'https://openalex.org/W2995181338', 'https://openalex.org/W30845872', 'https://openalex.org/W3216034039', 'https://openalex.org/W2963403868', 'https://openalex.org/W4303941982', 'https://openalex.org/W2913668833', 'https://openalex.org/W2963311389', 'https://openalex.org/W3152218910', 'https://openalex.org/W2798908575', 'https://openalex.org/W2949640717', 'https://openalex.org/W4288107125', 'https://openalex.org/W2952343510', 'https://openalex.org/W3158802984', 'https://openalex.org/W2899377381', 'https://openalex.org/W4238614906', 'https://openalex.org/W2963720603', 'https://openalex.org/W2020607164', 'https://openalex.org/W2926827382', 'https://openalex.org/W2151660570', 'https://openalex.org/W2912526802', 'https://openalex.org/W2802557066', 'https://openalex.org/W4285719527', 'https://openalex.org/W2126203737', 'https://openalex.org/W2037959956', 'https://openalex.org/W2601836666', 'https://openalex.org/W3150635893', 'https://openalex.org/W4385245566', 'https://openalex.org/W3099782249', 'https://openalex.org/W2950561535', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963499843', 'https://openalex.org/W2114777034', 'https://openalex.org/W3204917342', 'https://openalex.org/W2079460648', 'https://openalex.org/W2167715705', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963340922', 'https://openalex.org/W2003333103', 'https://openalex.org/W2963118869', 'https://openalex.org/W2137010615', 'https://openalex.org/W2973026522', 'https://openalex.org/W1545920196', 'https://openalex.org/W2169992508', 'https://openalex.org/W4300047444', 'https://openalex.org/W2964266061', 'https://openalex.org/W2059652594', 'https://openalex.org/W2117786207', 'https://openalex.org/W2963735467', 'https://openalex.org/W2004833594', 'https://openalex.org/W1579848672', 'https://openalex.org/W2964347276', 'https://openalex.org/W2025482506', 'https://openalex.org/W3198134274', 'https://openalex.org/W2787447541', 'https://openalex.org/W2964079874', 'https://openalex.org/W3165666670', 'https://openalex.org/W2927673779']",2022-03-09
https://openalex.org/W2972374322,https://doi.org/10.21437/interspeech.2019-3232,VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019,"We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.","['https://openalex.org/W2963971656', 'https://openalex.org/W2786608204', 'https://openalex.org/W2593414223', 'https://openalex.org/W2547039119', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963799213', 'https://openalex.org/W2101234009', 'https://openalex.org/W2399576818', 'https://openalex.org/W1959608418', 'https://openalex.org/W2666408839', 'https://openalex.org/W2964069186', 'https://openalex.org/W2962699523', 'https://openalex.org/W4300047444', 'https://openalex.org/W2962896155', 'https://openalex.org/W2940544976', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W4320013936', 'https://openalex.org/W2787447541', 'https://openalex.org/W2888858245', 'https://openalex.org/W2963796886', 'https://openalex.org/W4394670483', 'https://openalex.org/W2911340057', 'https://openalex.org/W1836465849', 'https://openalex.org/W2899771611', 'https://openalex.org/W3125709657', 'https://openalex.org/W2191779130']",2019-09-13
https://openalex.org/W3125087428,https://doi.org/10.1073/pnas.2001844118,Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input,"Significance Infants become attuned to the sounds of their native language(s) before they even speak. Hypotheses about what is being learned by infants have traditionally driven researchers’ attempts to understand this surprising phenomenon. Here, we propose to start, instead, from hypotheses about how infants might learn. To implement this mechanism-driven approach, we introduce a quantitative modeling framework based on large-scale simulation of the learning process on realistic input. It allows learning mechanisms to be systematically linked to testable predictions regarding infants’ attunement to their native language(s). Through this framework, we obtain evidence for an account of infants’ attunement that challenges established theories about what infants are learning.","['https://openalex.org/W1994492508', 'https://openalex.org/W2062956221', 'https://openalex.org/W1985728169', 'https://openalex.org/W2037249514', 'https://openalex.org/W2095459173', 'https://openalex.org/W1968703923', 'https://openalex.org/W2140661818', 'https://openalex.org/W1564039035', 'https://openalex.org/W2803143571', 'https://openalex.org/W2911249026', 'https://openalex.org/W2063525438', 'https://openalex.org/W2142152793', 'https://openalex.org/W2113363891', 'https://openalex.org/W1925965306', 'https://openalex.org/W2184332496', 'https://openalex.org/W122673323', 'https://openalex.org/W2089883580', 'https://openalex.org/W2103091632', 'https://openalex.org/W2751798917', 'https://openalex.org/W2104752510', 'https://openalex.org/W2142633459', 'https://openalex.org/W2145810838', 'https://openalex.org/W2766298282', 'https://openalex.org/W2160464066', 'https://openalex.org/W1993755070', 'https://openalex.org/W111481704', 'https://openalex.org/W4300419953', 'https://openalex.org/W2095458199', 'https://openalex.org/W153534061', 'https://openalex.org/W2153767712', 'https://openalex.org/W2108145097', 'https://openalex.org/W2169991335', 'https://openalex.org/W1900890596', 'https://openalex.org/W2407869992', 'https://openalex.org/W2110627398', 'https://openalex.org/W2250874882', 'https://openalex.org/W2610616322', 'https://openalex.org/W2880673012', 'https://openalex.org/W2010188467', 'https://openalex.org/W1964201439', 'https://openalex.org/W2022042240', 'https://openalex.org/W2096563020', 'https://openalex.org/W135984148', 'https://openalex.org/W2117041980', 'https://openalex.org/W2114347655', 'https://openalex.org/W2100768664', 'https://openalex.org/W2025482506', 'https://openalex.org/W2052697931', 'https://openalex.org/W2786608204', 'https://openalex.org/W2346964103', 'https://openalex.org/W2347098582', 'https://openalex.org/W2399576818', 'https://openalex.org/W2404799143', 'https://openalex.org/W1545920196', 'https://openalex.org/W1796128977', 'https://openalex.org/W2400549570', 'https://openalex.org/W2345811097', 'https://openalex.org/W2787447541', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963620343', 'https://openalex.org/W2483390977', 'https://openalex.org/W2980286501', 'https://openalex.org/W281094599', 'https://openalex.org/W2394503152', 'https://openalex.org/W1949377791', 'https://openalex.org/W2086880169', 'https://openalex.org/W1993049588', 'https://openalex.org/W2131070395', 'https://openalex.org/W2133514592', 'https://openalex.org/W2113153226', 'https://openalex.org/W2252142375', 'https://openalex.org/W2963648280', 'https://openalex.org/W2963234797', 'https://openalex.org/W2024490156', 'https://openalex.org/W66627554', 'https://openalex.org/W2145410271', 'https://openalex.org/W3034729383', 'https://openalex.org/W4234482113', 'https://openalex.org/W2146163948', 'https://openalex.org/W2395899413', 'https://openalex.org/W2593779438', 'https://openalex.org/W2157423901', 'https://openalex.org/W1995422333', 'https://openalex.org/W2001584575', 'https://openalex.org/W2182214061', 'https://openalex.org/W292738443', 'https://openalex.org/W2005311247', 'https://openalex.org/W2142124981', 'https://openalex.org/W2135563147', 'https://openalex.org/W2143022183', 'https://openalex.org/W2164203346', 'https://openalex.org/W2080079179', 'https://openalex.org/W2086022490', 'https://openalex.org/W2067039424', 'https://openalex.org/W2970859978', 'https://openalex.org/W2053819145', 'https://openalex.org/W1997141060', 'https://openalex.org/W1967974940', 'https://openalex.org/W1984549614', 'https://openalex.org/W2140014531', 'https://openalex.org/W3047822531', 'https://openalex.org/W2098363562', 'https://openalex.org/W2404952642', 'https://openalex.org/W2963137467', 'https://openalex.org/W2791647162', 'https://openalex.org/W2101509422', 'https://openalex.org/W1983870751', 'https://openalex.org/W2343593471', 'https://openalex.org/W2595479191', 'https://openalex.org/W2772732614', 'https://openalex.org/W6682569104', 'https://openalex.org/W2128032727', 'https://openalex.org/W2078993594', 'https://openalex.org/W2019575913', 'https://openalex.org/W2160719354', 'https://openalex.org/W1949929726', 'https://openalex.org/W4246559809', 'https://openalex.org/W4301823191', 'https://openalex.org/W1998662048', 'https://openalex.org/W2151967501', 'https://openalex.org/W3125709657', 'https://openalex.org/W3134136786', 'https://openalex.org/W2963618559', 'https://openalex.org/W598767079', 'https://openalex.org/W1985098633', 'https://openalex.org/W2082256905', 'https://openalex.org/W2164770604']",2021-01-28
https://openalex.org/W4230289889,https://doi.org/10.31234/osf.io/fc4wh,Early phonetic learning without phonetic categories -- Insights from large-scale simulations on realistic input,"Before they even speak, infants become attuned to the sounds of the language(s) they hear, processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing English [ɹ] and [l], as in ‘rock’ vs ‘lock’, relative to infants learning Japanese. Influential accounts of this early phonetic learning phenomenon initially proposed that infants group sounds into native vowel- and consonant-like phonetic categories—like [ɹ] and [l] in English—through a statistical clustering mechanism dubbed ‘distributional learning’. The feasibility of this mechanism for learning phonetic categories has been challenged, however. Here we demonstrate that a distributional learning algorithm operating on naturalistic speech can predict early phonetic learning as observed in Japanese and American English infants, suggesting that infants might learn through distributional learning after all. We further show, however, that contrary to the original distributional learning proposal, our model learns units too brief and too fine-grained acoustically to correspond to phonetic categories. This challenges the influential idea that what infants learn are phonetic categories. More broadly, our work introduces a novel mechanism-driven approach to the study of early phonetic learning, together with a quantitative modeling framework that can handle realistic input. This allows, for the first time, accounts of early phonetic learning to be linked to concrete, systematic predictions regarding infants’ attunement.","['https://openalex.org/W2345811097', 'https://openalex.org/W2133514592', 'https://openalex.org/W2096563020', 'https://openalex.org/W66627554', 'https://openalex.org/W2347098582', 'https://openalex.org/W2394503152', 'https://openalex.org/W3133848337', 'https://openalex.org/W2024490156', 'https://openalex.org/W2005311247', 'https://openalex.org/W2113153226', 'https://openalex.org/W1964201439', 'https://openalex.org/W1967974940', 'https://openalex.org/W2117041980', 'https://openalex.org/W3034729383', 'https://openalex.org/W2490706771', 'https://openalex.org/W4246559809', 'https://openalex.org/W2086880169', 'https://openalex.org/W4253947715', 'https://openalex.org/W4230637005', 'https://openalex.org/W2146163948', 'https://openalex.org/W2125624224', 'https://openalex.org/W2157423901', 'https://openalex.org/W2067039424', 'https://openalex.org/W2114347655', 'https://openalex.org/W1993049588', 'https://openalex.org/W2128032727', 'https://openalex.org/W2786608204', 'https://openalex.org/W2030196776', 'https://openalex.org/W2153767712', 'https://openalex.org/W3119948327', 'https://openalex.org/W1925965306', 'https://openalex.org/W2124789099', 'https://openalex.org/W3038687092', 'https://openalex.org/W2091143423', 'https://openalex.org/W2766298282', 'https://openalex.org/W2404952642', 'https://openalex.org/W2029008609', 'https://openalex.org/W281094599', 'https://openalex.org/W4299602607', 'https://openalex.org/W1900890596', 'https://openalex.org/W2142124981', 'https://openalex.org/W2086022490', 'https://openalex.org/W2142633459', 'https://openalex.org/W282666689', 'https://openalex.org/W2274405424', 'https://openalex.org/W2131070395', 'https://openalex.org/W2113363891', 'https://openalex.org/W2052697931', 'https://openalex.org/W2791647162', 'https://openalex.org/W2911249026', 'https://openalex.org/W2135535894', 'https://openalex.org/W2022042240', 'https://openalex.org/W2182214061', 'https://openalex.org/W2980286501', 'https://openalex.org/W2963234797', 'https://openalex.org/W2483390977', 'https://openalex.org/W2395899413', 'https://openalex.org/W2140014531', 'https://openalex.org/W1949377791', 'https://openalex.org/W111481704', 'https://openalex.org/W2171377672', 'https://openalex.org/W1564039035', 'https://openalex.org/W2000405415', 'https://openalex.org/W2162882125', 'https://openalex.org/W2160464066', 'https://openalex.org/W2151967501', 'https://openalex.org/W2751798917', 'https://openalex.org/W2169991335', 'https://openalex.org/W2110627398', 'https://openalex.org/W153534061', 'https://openalex.org/W1545920196', 'https://openalex.org/W2103091632', 'https://openalex.org/W1545077353', 'https://openalex.org/W2401877034', 'https://openalex.org/W2803143571', 'https://openalex.org/W2951066214', 'https://openalex.org/W2972967522', 'https://openalex.org/W1524333225', 'https://openalex.org/W2025482506', 'https://openalex.org/W2143437412', 'https://openalex.org/W2346964103', 'https://openalex.org/W2252142375', 'https://openalex.org/W2145410271', 'https://openalex.org/W2037720659', 'https://openalex.org/W1796128977', 'https://openalex.org/W3125709657', 'https://openalex.org/W2080079179', 'https://openalex.org/W2880673012', 'https://openalex.org/W1993755070', 'https://openalex.org/W2164203346', 'https://openalex.org/W2108145097', 'https://openalex.org/W2940544976', 'https://openalex.org/W2115646488', 'https://openalex.org/W2100768664', 'https://openalex.org/W2400549570', 'https://openalex.org/W2787447541', 'https://openalex.org/W2016200089', 'https://openalex.org/W2581714092', 'https://openalex.org/W2095458199', 'https://openalex.org/W1995422333', 'https://openalex.org/W4300419953', 'https://openalex.org/W135984148', 'https://openalex.org/W2103563049', 'https://openalex.org/W2610616322', 'https://openalex.org/W2404799143', 'https://openalex.org/W2135563147', 'https://openalex.org/W2143022183', 'https://openalex.org/W2963648280', 'https://openalex.org/W2021330098', 'https://openalex.org/W1998549181', 'https://openalex.org/W4300584482', 'https://openalex.org/W1596515083', 'https://openalex.org/W1997141060', 'https://openalex.org/W2001584575', 'https://openalex.org/W2399576818', 'https://openalex.org/W2407869992']",2019-05-01
https://openalex.org/W2998284473,https://doi.org/10.1609/aaai.v34i05.6341,Towards Zero-Shot Learning for Automatic Phonemic Transcription,"Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.","['https://openalex.org/W2787760419', 'https://openalex.org/W2476082370', 'https://openalex.org/W2025401819', 'https://openalex.org/W2788381663', 'https://openalex.org/W6664307575', 'https://openalex.org/W2166637769', 'https://openalex.org/W7027429494', 'https://openalex.org/W2787447541', 'https://openalex.org/W2791647162', 'https://openalex.org/W2672120608', 'https://openalex.org/W1559678887', 'https://openalex.org/W2286443923', 'https://openalex.org/W6980779235', 'https://openalex.org/W2134270519', 'https://openalex.org/W2965549619', 'https://openalex.org/W1736701665', 'https://openalex.org/W2883972335', 'https://openalex.org/W2572670101', 'https://openalex.org/W6752124048', 'https://openalex.org/W3217769081', 'https://openalex.org/W6682222085', 'https://openalex.org/W1494198834', 'https://openalex.org/W2067392428', 'https://openalex.org/W652269744', 'https://openalex.org/W2250357346', 'https://openalex.org/W2894690744', 'https://openalex.org/W1980850109', 'https://openalex.org/W6678360021', 'https://openalex.org/W63916190', 'https://openalex.org/W2116648050', 'https://openalex.org/W2407897255', 'https://openalex.org/W2633221078', 'https://openalex.org/W2972797781', 'https://openalex.org/W6973666849', 'https://openalex.org/W2397721308', 'https://openalex.org/W2550821151', 'https://openalex.org/W2025482506', 'https://openalex.org/W2055408826', 'https://openalex.org/W2805993470', 'https://openalex.org/W2124033848', 'https://openalex.org/W2316803017', 'https://openalex.org/W3143107425', 'https://openalex.org/W2408712009', 'https://openalex.org/W1486697269', 'https://openalex.org/W2150295085', 'https://openalex.org/W2963292011', 'https://openalex.org/W2786608204']",2020-04-03
https://openalex.org/W3005511757,https://doi.org/10.1109/icassp40776.2020.9054548,Unsupervised Pretraining Transfers Well Across Languages,"Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources.","['https://openalex.org/W3102342027', 'https://openalex.org/W2123798005', 'https://openalex.org/W2972581290', 'https://openalex.org/W2963292011', 'https://openalex.org/W2963431393', 'https://openalex.org/W6675751002', 'https://openalex.org/W2096733369', 'https://openalex.org/W6682691769', 'https://openalex.org/W6682250724', 'https://openalex.org/W6736894310', 'https://openalex.org/W6754278344', 'https://openalex.org/W2730658205', 'https://openalex.org/W2842511635', 'https://openalex.org/W3100270690', 'https://openalex.org/W1970890968', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W6713634518', 'https://openalex.org/W2033436836', 'https://openalex.org/W2157364932', 'https://openalex.org/W2025198378', 'https://openalex.org/W2131042651', 'https://openalex.org/W1978660892', 'https://openalex.org/W6763442200', 'https://openalex.org/W6739901393', 'https://openalex.org/W6638667902', 'https://openalex.org/W6712444837', 'https://openalex.org/W2127141656', 'https://openalex.org/W2787447541', 'https://openalex.org/W6697293080', 'https://openalex.org/W2963495051', 'https://openalex.org/W2296607128', 'https://openalex.org/W2949117887', 'https://openalex.org/W2395899413', 'https://openalex.org/W2933976917', 'https://openalex.org/W2153579005', 'https://openalex.org/W3099206234', 'https://openalex.org/W2963762683', 'https://openalex.org/W2963403868', 'https://openalex.org/W2949517790', 'https://openalex.org/W2951873722', 'https://openalex.org/W2148349024', 'https://openalex.org/W3125709657', 'https://openalex.org/W2106053110']",2020-04-09
https://openalex.org/W2945769669,https://doi.org/10.18653/v1/n19-1007,Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders,"Cory Shain, Micha Elsner. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.","['https://openalex.org/W2964169922', 'https://openalex.org/W1966264494', 'https://openalex.org/W1976587578', 'https://openalex.org/W2758697525', 'https://openalex.org/W2011238950', 'https://openalex.org/W2402366697', 'https://openalex.org/W2052384514', 'https://openalex.org/W2059450962', 'https://openalex.org/W2145410271', 'https://openalex.org/W2964121744', 'https://openalex.org/W2407614114', 'https://openalex.org/W2044138293', 'https://openalex.org/W2963620343', 'https://openalex.org/W2250874882', 'https://openalex.org/W2271840356', 'https://openalex.org/W2106048795', 'https://openalex.org/W2242818861', 'https://openalex.org/W1576255537', 'https://openalex.org/W1836465849', 'https://openalex.org/W2314715977', 'https://openalex.org/W157872538', 'https://openalex.org/W2120713167', 'https://openalex.org/W1519956846', 'https://openalex.org/W2577416417', 'https://openalex.org/W2077938266', 'https://openalex.org/W2136653392', 'https://openalex.org/W2779379590', 'https://openalex.org/W2785860501', 'https://openalex.org/W2083053184', 'https://openalex.org/W2010800472', 'https://openalex.org/W1545920196', 'https://openalex.org/W2095517176', 'https://openalex.org/W2152134037', 'https://openalex.org/W2161025448', 'https://openalex.org/W2468716020', 'https://openalex.org/W2119536496', 'https://openalex.org/W1966072864', 'https://openalex.org/W2101509422', 'https://openalex.org/W2963735467', 'https://openalex.org/W2252956398', 'https://openalex.org/W2087985520', 'https://openalex.org/W2087384787', 'https://openalex.org/W1549321558', 'https://openalex.org/W1995422333', 'https://openalex.org/W2400668844', 'https://openalex.org/W1984689963', 'https://openalex.org/W2016090750', 'https://openalex.org/W2140661818', 'https://openalex.org/W2050864369', 'https://openalex.org/W2106125881', 'https://openalex.org/W2077382402', 'https://openalex.org/W2100768664', 'https://openalex.org/W2251846372', 'https://openalex.org/W2980286501', 'https://openalex.org/W1990351858', 'https://openalex.org/W2171752983', 'https://openalex.org/W2320209943', 'https://openalex.org/W2160464066', 'https://openalex.org/W2787447541', 'https://openalex.org/W2613332842', 'https://openalex.org/W2332494297', 'https://openalex.org/W2005741747', 'https://openalex.org/W52412328', 'https://openalex.org/W2250527913', 'https://openalex.org/W2152824855', 'https://openalex.org/W2117041980', 'https://openalex.org/W2184341689', 'https://openalex.org/W2146163948', 'https://openalex.org/W2136756756', 'https://openalex.org/W1977531436', 'https://openalex.org/W2307400721', 'https://openalex.org/W2785415724', 'https://openalex.org/W2768381684', 'https://openalex.org/W2086841290', 'https://openalex.org/W2786902352', 'https://openalex.org/W2111312122', 'https://openalex.org/W2964308564', 'https://openalex.org/W2919115771', 'https://openalex.org/W2168266939', 'https://openalex.org/W2008775952', 'https://openalex.org/W1796128977', 'https://openalex.org/W2066908170', 'https://openalex.org/W2063832546', 'https://openalex.org/W2398490608', 'https://openalex.org/W2153767712', 'https://openalex.org/W1970688873', 'https://openalex.org/W2143022183', 'https://openalex.org/W1937295144', 'https://openalex.org/W2058455850', 'https://openalex.org/W2345811097', 'https://openalex.org/W1986174057', 'https://openalex.org/W2194775991', 'https://openalex.org/W1976977765', 'https://openalex.org/W1598851216', 'https://openalex.org/W2319920447', 'https://openalex.org/W2786608204', 'https://openalex.org/W2787223168', 'https://openalex.org/W2804446378', 'https://openalex.org/W1967177300', 'https://openalex.org/W2138615112', 'https://openalex.org/W2339016247', 'https://openalex.org/W2399576818', 'https://openalex.org/W2401464865', 'https://openalex.org/W2522012644', 'https://openalex.org/W2039201033', 'https://openalex.org/W2184045248', 'https://openalex.org/W2090861223', 'https://openalex.org/W1993755070', 'https://openalex.org/W2121997342', 'https://openalex.org/W2094818253', 'https://openalex.org/W2040870580', 'https://openalex.org/W2400549570', 'https://openalex.org/W2035126305', 'https://openalex.org/W2011145053', 'https://openalex.org/W2156447271', 'https://openalex.org/W2160017196', 'https://openalex.org/W2787426069']",2019-01-01
https://openalex.org/W3148101939,https://doi.org/10.1109/slt48900.2021.9383461,Towards Unsupervised Learning of Speech Features in the Wild,"Recent work on unsupervised contrastive learning of speech representation has shown promising results, but so far has mostly been applied to clean, curated speech datasets. Can it also be used with unprepared audio data ""in the wild""? Here, we explore three potential problems in this setting: (i) presence of non-speech data, (ii) noisy or low quality speech data, and (iii) imbalance in speaker distribution. We show that on the Libri-light train set, which is itself a relatively clean speech-only dataset, these problems combined can already have a performance cost of up to 30% relative for the ABX score. We show that the first two problems can be alleviated by data filtering, with voice activity detection selecting speech segments, while perplexity of a model trained with clean data helping to discard entire files. We show that the third problem can be alleviated by learning a speaker embedding in the predictive branch of the model. We show that these techniques build more robust speech features that can be transferred to an ASR task in the low resource setting.","['https://openalex.org/W6743986254', 'https://openalex.org/W2127141656', 'https://openalex.org/W6769455919', 'https://openalex.org/W3015213852', 'https://openalex.org/W6770514103', 'https://openalex.org/W6714100551', 'https://openalex.org/W3015783745', 'https://openalex.org/W1494198834', 'https://openalex.org/W6739901393', 'https://openalex.org/W2842511635', 'https://openalex.org/W2983785920', 'https://openalex.org/W6780483730', 'https://openalex.org/W6761553608', 'https://openalex.org/W2111702745', 'https://openalex.org/W2124558353', 'https://openalex.org/W2147590749', 'https://openalex.org/W2430252546', 'https://openalex.org/W2079623482', 'https://openalex.org/W2889374926', 'https://openalex.org/W2963381607', 'https://openalex.org/W3015501067', 'https://openalex.org/W6712444837', 'https://openalex.org/W2973049979', 'https://openalex.org/W3093096176', 'https://openalex.org/W6697293080', 'https://openalex.org/W3016181583', 'https://openalex.org/W6771812881', 'https://openalex.org/W3003875258', 'https://openalex.org/W3102342027', 'https://openalex.org/W2346964103', 'https://openalex.org/W6844194202', 'https://openalex.org/W2940544976', 'https://openalex.org/W2796339975', 'https://openalex.org/W2133223948', 'https://openalex.org/W6750523955', 'https://openalex.org/W2787447541', 'https://openalex.org/W6780218876', 'https://openalex.org/W2950414763', 'https://openalex.org/W2055408826', 'https://openalex.org/W2962850179', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289564011', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W2988736778', 'https://openalex.org/W2795282075', 'https://openalex.org/W3093427098', 'https://openalex.org/W2407151108', 'https://openalex.org/W2963403868', 'https://openalex.org/W3144810982', 'https://openalex.org/W3099782249', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963620343', 'https://openalex.org/W4385245566', 'https://openalex.org/W2990583358', 'https://openalex.org/W2593779438', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963371670', 'https://openalex.org/W2753008876', 'https://openalex.org/W2786608204', 'https://openalex.org/W2395899413', 'https://openalex.org/W2930682606', 'https://openalex.org/W2953190524']",2021-01-19
https://openalex.org/W3039910566,https://doi.org/10.1109/slt48900.2021.9383605,Data Augmenting Contrastive Learning of Speech Representations in the Time Domain,"Contrastive Predictive Coding (CPC), based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC (relative improvement of 18-22%), beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15% relative.","['https://openalex.org/W6762619590', 'https://openalex.org/W1494198834', 'https://openalex.org/W6774314701', 'https://openalex.org/W6682250724', 'https://openalex.org/W2883725317', 'https://openalex.org/W6688816777', 'https://openalex.org/W3015783745', 'https://openalex.org/W6766978945', 'https://openalex.org/W3102342027', 'https://openalex.org/W3003875258', 'https://openalex.org/W2509930204', 'https://openalex.org/W6748342566', 'https://openalex.org/W3100270690', 'https://openalex.org/W2940544976', 'https://openalex.org/W2020607164', 'https://openalex.org/W3015213852', 'https://openalex.org/W6677620606', 'https://openalex.org/W6675022971', 'https://openalex.org/W2696967604', 'https://openalex.org/W2346964103', 'https://openalex.org/W6713762819', 'https://openalex.org/W6844194202', 'https://openalex.org/W2936774411', 'https://openalex.org/W6697293080', 'https://openalex.org/W6771812881', 'https://openalex.org/W6761176036', 'https://openalex.org/W3093096176', 'https://openalex.org/W3005511757', 'https://openalex.org/W2055408826', 'https://openalex.org/W2347098582', 'https://openalex.org/W6761553608', 'https://openalex.org/W2983785920', 'https://openalex.org/W6769455919', 'https://openalex.org/W6760822226', 'https://openalex.org/W6686369942', 'https://openalex.org/W1989674786', 'https://openalex.org/W2995181338', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963620343', 'https://openalex.org/W2842511635', 'https://openalex.org/W2148349024', 'https://openalex.org/W2593779438', 'https://openalex.org/W2930682606', 'https://openalex.org/W3005680577', 'https://openalex.org/W2973049979', 'https://openalex.org/W2219249508', 'https://openalex.org/W2973026522', 'https://openalex.org/W3016181583', 'https://openalex.org/W2787447541', 'https://openalex.org/W2407080277', 'https://openalex.org/W2990583358', 'https://openalex.org/W2970971581', 'https://openalex.org/W2972943112']",2021-01-19
https://openalex.org/W2950414763,https://doi.org/10.21437/interspeech.2019-1337,Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling,"This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.","['https://openalex.org/W2516890051', 'https://openalex.org/W1882958252', 'https://openalex.org/W2826003142', 'https://openalex.org/W2128032727', 'https://openalex.org/W2786608204', 'https://openalex.org/W1545920196', 'https://openalex.org/W2399576818', 'https://openalex.org/W2963620343', 'https://openalex.org/W2758785877', 'https://openalex.org/W2598638573', 'https://openalex.org/W2964121744', 'https://openalex.org/W2889228998', 'https://openalex.org/W2940544976', 'https://openalex.org/W1522301498', 'https://openalex.org/W2786902352', 'https://openalex.org/W4288107125', 'https://openalex.org/W2949510815', 'https://openalex.org/W2796339975', 'https://openalex.org/W4289564011', 'https://openalex.org/W2587088898', 'https://openalex.org/W2906459023', 'https://openalex.org/W2402144811', 'https://openalex.org/W1796128977', 'https://openalex.org/W2963618559', 'https://openalex.org/W3125709657', 'https://openalex.org/W2547039119', 'https://openalex.org/W2787426069', 'https://openalex.org/W1524333225', 'https://openalex.org/W2785860501', 'https://openalex.org/W2936120996', 'https://openalex.org/W2787447541', 'https://openalex.org/W2404799143', 'https://openalex.org/W2953384591', 'https://openalex.org/W4300047444', 'https://openalex.org/W2963826681']",2019-09-13
https://openalex.org/W3194502352,https://doi.org/10.1016/j.csl.2021.101275,Feature learning for efficient ASR-free keyword spotting in low-resource languages,,"['https://openalex.org/W2533125211', 'https://openalex.org/W2963311389', 'https://openalex.org/W2020607164', 'https://openalex.org/W2396043527', 'https://openalex.org/W2555077524', 'https://openalex.org/W2407151108', 'https://openalex.org/W2786902352', 'https://openalex.org/W2034940213', 'https://openalex.org/W2291975472', 'https://openalex.org/W2168013545', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963620343', 'https://openalex.org/W2972867623', 'https://openalex.org/W1970088388', 'https://openalex.org/W2038810952', 'https://openalex.org/W2171019095', 'https://openalex.org/W2787447541', 'https://openalex.org/W2780786457', 'https://openalex.org/W2791647162', 'https://openalex.org/W3016368932', 'https://openalex.org/W2160815625', 'https://openalex.org/W2407964052', 'https://openalex.org/W6697187339', 'https://openalex.org/W2065582593', 'https://openalex.org/W1545920196', 'https://openalex.org/W2190506272', 'https://openalex.org/W4246962055', 'https://openalex.org/W2510719762', 'https://openalex.org/W2098044214', 'https://openalex.org/W2963624290', 'https://openalex.org/W2962961542', 'https://openalex.org/W2972764223', 'https://openalex.org/W2672106177', 'https://openalex.org/W2508907749', 'https://openalex.org/W2114347655', 'https://openalex.org/W2402146185', 'https://openalex.org/W6631362777', 'https://openalex.org/W1796128977', 'https://openalex.org/W2962850179', 'https://openalex.org/W2750393323', 'https://openalex.org/W2105099419', 'https://openalex.org/W2407023693', 'https://openalex.org/W2084534958', 'https://openalex.org/W2964187693', 'https://openalex.org/W2127982613', 'https://openalex.org/W2746778230', 'https://openalex.org/W6714136293', 'https://openalex.org/W2346964103', 'https://openalex.org/W1970890968', 'https://openalex.org/W2407441242', 'https://openalex.org/W2785415724', 'https://openalex.org/W2516890051', 'https://openalex.org/W2035424729', 'https://openalex.org/W2126203737', 'https://openalex.org/W1165382972', 'https://openalex.org/W2295061986', 'https://openalex.org/W4289564011', 'https://openalex.org/W2973026522', 'https://openalex.org/W88081813', 'https://openalex.org/W2404687737', 'https://openalex.org/W14941018', 'https://openalex.org/W1522301498', 'https://openalex.org/W1524333225', 'https://openalex.org/W2384495648', 'https://openalex.org/W319941341', 'https://openalex.org/W6908809', 'https://openalex.org/W3105242324']",2021-08-12
https://openalex.org/W3097692357,https://doi.org/10.21437/interspeech.2020-1785,Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders,This is a repository copy of Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders.,"['https://openalex.org/W1959608418', 'https://openalex.org/W2005708641', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W2603777577', 'https://openalex.org/W2785860501', 'https://openalex.org/W2972841524', 'https://openalex.org/W2519091744', 'https://openalex.org/W2395899413', 'https://openalex.org/W2758785877', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963691546', 'https://openalex.org/W2972943112', 'https://openalex.org/W1665214252', 'https://openalex.org/W2787447541', 'https://openalex.org/W2402146185', 'https://openalex.org/W2128032727', 'https://openalex.org/W2598638573', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963830550', 'https://openalex.org/W2502312327', 'https://openalex.org/W2789543585', 'https://openalex.org/W2972659941', 'https://openalex.org/W4288107125', 'https://openalex.org/W1522301498', 'https://openalex.org/W2547039119', 'https://openalex.org/W4295312788', 'https://openalex.org/W3125709657', 'https://openalex.org/W2952161038', 'https://openalex.org/W2514741789', 'https://openalex.org/W2242818861', 'https://openalex.org/W2972374322', 'https://openalex.org/W2979476256', 'https://openalex.org/W2786902352', 'https://openalex.org/W2950414763']",2020-10-25
https://openalex.org/W3006358483,https://doi.org/10.1109/lsp.2020.2973798,Unsupervised Feature Learning for Speech Using Correspondence and Siamese Networks,"In zero-resource settings where transcribed speech audio is unavailable, unsupervised feature learning is essential for downstream speech processing tasks. Here we compare two recent methods for frame-level acoustic feature learning. For both methods, unsupervised term discovery is used to find pairs of word examples of the same unknown type. Dynamic programming is then used to align the feature frames between each word pair, serving as weak top-down supervision for the two models. For the correspondence autoencoder (CAE), matching frames are presented as input-output pairs. The Triamese network uses a contrastive loss to reduce the distance between frames of the same predicted word type while increasing the distance between negative examples. For the first time, these feature extractors are compared on the same discrimination tasks using the same weak supervision pairs. We find that, on the two datasets considered here, the CAE outperforms the Triamese network. However, we show that a new hybrid correspondence-Triamese approach (CTriamese), consistently outperforms both the CAE and Triamese models in terms of average precision and ABX error rates on both English and Xitsonga evaluation data.","['https://openalex.org/W6714100551', 'https://openalex.org/W6600284362', 'https://openalex.org/W2972764223', 'https://openalex.org/W2791647162', 'https://openalex.org/W2468716020', 'https://openalex.org/W6734095835', 'https://openalex.org/W2044138293', 'https://openalex.org/W2145410271', 'https://openalex.org/W6633682082', 'https://openalex.org/W2963720603', 'https://openalex.org/W2787447541', 'https://openalex.org/W6734901337', 'https://openalex.org/W2780786457', 'https://openalex.org/W2940544976', 'https://openalex.org/W2020607164', 'https://openalex.org/W6712202099', 'https://openalex.org/W6638159135', 'https://openalex.org/W2826003142', 'https://openalex.org/W2972867623', 'https://openalex.org/W2052697931', 'https://openalex.org/W2404799143', 'https://openalex.org/W1545920196', 'https://openalex.org/W2962980711', 'https://openalex.org/W6712444837', 'https://openalex.org/W1577418252', 'https://openalex.org/W2025482506', 'https://openalex.org/W2802557066', 'https://openalex.org/W2346964103', 'https://openalex.org/W6973666849', 'https://openalex.org/W2126203737', 'https://openalex.org/W2091746061', 'https://openalex.org/W2963620343', 'https://openalex.org/W2400549570', 'https://openalex.org/W2962850179', 'https://openalex.org/W2516890051', 'https://openalex.org/W1967924372', 'https://openalex.org/W2171590421', 'https://openalex.org/W2057007397', 'https://openalex.org/W2114347655']",2020-01-01
https://openalex.org/W2936295285,https://doi.org/10.21437/interspeech.2019-1518,Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks,"For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.","['https://openalex.org/W2951004968', 'https://openalex.org/W2025482506', 'https://openalex.org/W1577418252', 'https://openalex.org/W2547039119', 'https://openalex.org/W2911249026', 'https://openalex.org/W2964115348', 'https://openalex.org/W2949382160', 'https://openalex.org/W2347098582', 'https://openalex.org/W2476548250', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963620343', 'https://openalex.org/W2964121744', 'https://openalex.org/W2091746061', 'https://openalex.org/W2963830550', 'https://openalex.org/W2242818861', 'https://openalex.org/W1967924372', 'https://openalex.org/W2963799213', 'https://openalex.org/W2890983311', 'https://openalex.org/W2395899413', 'https://openalex.org/W2134202996', 'https://openalex.org/W2945769669', 'https://openalex.org/W2548228487', 'https://openalex.org/W2963619462', 'https://openalex.org/W2598638573', 'https://openalex.org/W3125709657', 'https://openalex.org/W2826003142', 'https://openalex.org/W2148154194', 'https://openalex.org/W2404799143', 'https://openalex.org/W2035424729', 'https://openalex.org/W2126203737', 'https://openalex.org/W2020607164', 'https://openalex.org/W2962790638', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963149687', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963618559', 'https://openalex.org/W2547875792', 'https://openalex.org/W52412328', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396043527', 'https://openalex.org/W2010188467', 'https://openalex.org/W2787447541']",2019-09-13
https://openalex.org/W2296607128,https://doi.org/10.1109/asru.2017.8268953,The zero resource speech challenge 2017,We describe a new challenge aimed at discovering subword and word units from raw speech. This challenge is the followup to the Zero Resource Speech Challenge 2015. It aims at constructing systems that generalize across languages and adapt to new speakers. The design features and evaluation metrics of the challenge are presented and the results of seventeen models are discussed.,"['https://openalex.org/W2964169922', 'https://openalex.org/W2126377586', 'https://openalex.org/W6748325621', 'https://openalex.org/W2057007397', 'https://openalex.org/W6973666849', 'https://openalex.org/W6704726871', 'https://openalex.org/W91681889', 'https://openalex.org/W2101509422', 'https://openalex.org/W2150389998', 'https://openalex.org/W6631362777', 'https://openalex.org/W6731521493', 'https://openalex.org/W2395899413', 'https://openalex.org/W6713745070', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713256719', 'https://openalex.org/W2055408826', 'https://openalex.org/W2466918907', 'https://openalex.org/W2785415724', 'https://openalex.org/W2101281673', 'https://openalex.org/W2582956876', 'https://openalex.org/W2963211739', 'https://openalex.org/W6712553779', 'https://openalex.org/W6678947187', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786902352', 'https://openalex.org/W2785860501', 'https://openalex.org/W2787223168', 'https://openalex.org/W2284628133', 'https://openalex.org/W2599585580', 'https://openalex.org/W2399576818', 'https://openalex.org/W2572097499', 'https://openalex.org/W2128032727', 'https://openalex.org/W2346964103', 'https://openalex.org/W2483390977', 'https://openalex.org/W2963620343']",2017-12-01
https://openalex.org/W2949510815,https://doi.org/10.21437/interspeech.2019-1338,Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation,"This study tackles unsupervised subword modeling in the zero-resource\nscenario, learning frame-level speech representation that is phonetically\ndiscriminative and speaker-invariant, using only untranscribed speech for\ntarget languages. Frame label acquisition is an essential step in solving this\nproblem. High quality frame labels should be in good consistency with golden\ntranscriptions and robust to speaker variation. We propose to improve frame\nlabel acquisition in our previously adopted deep neural network-bottleneck\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\nand speaker identity information encoded in speech. By discarding or unifying\nspeaker information, speaker-invariant features are learned and fed as inputs\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\%$ and $0.6\\%$\nabsolute ABX error rate reductions in across- and within-speaker conditions,\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\napproaches significantly outperform vocal tract length normalization in\nimproving frame labeling and subword modeling.\n","['https://openalex.org/W2914746235', 'https://openalex.org/W2889228998', 'https://openalex.org/W2106284094', 'https://openalex.org/W2963134917', 'https://openalex.org/W2025482506', 'https://openalex.org/W2345811097', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964121744', 'https://openalex.org/W2787447541', 'https://openalex.org/W1975728937', 'https://openalex.org/W2962689740', 'https://openalex.org/W2513125788', 'https://openalex.org/W2758785877', 'https://openalex.org/W2786902352', 'https://openalex.org/W4300047444', 'https://openalex.org/W2483390977', 'https://openalex.org/W4298028408', 'https://openalex.org/W2128032727', 'https://openalex.org/W2964245029', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963620343', 'https://openalex.org/W2767122664', 'https://openalex.org/W2963618559', 'https://openalex.org/W2786608204']",2019-09-13
https://openalex.org/W2971041032,https://doi.org/10.1109/taslp.2019.2937953,Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling,"This research addresses the problem of acoustic modeling of low-resource\nlanguages for which transcribed training data is absent. The goal is to learn\nrobust frame-level feature representations that can be used to identify and\ndistinguish subword-level speech units. The proposed feature representations\ncomprise various types of multilingual bottleneck features (BNFs) that are\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\nkey problems is how to acquire high-quality frame labels for untranscribed\ntraining data to facilitate supervised DNN training. It is shown that learning\nof robust BNF representations can be achieved by effectively leveraging\ntranscribed speech data and well-trained automatic speech recognition (ASR)\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\nASR systems can be applied to perform speaker adaptation with untranscribed\ntraining data of the target language, and to decode the training speech into\nframe-level labels for DNN training. It is also found that better frame labels\ncan be generated by considering temporal dependency in speech when performing\nframe clustering. The proposed methods of feature learning are evaluated on the\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\n2017 Challenge. The best performance achieved by our system is $9.7\\%$ in terms\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\nthe best systems reported recently. Lastly, our investigation reveals that the\ncloseness between target languages and out-of-domain languages and the amount\nof available training data for individual target languages could have\nsignificant impact on the goodness of learned features.\n","['https://openalex.org/W2747192917', 'https://openalex.org/W1975728937', 'https://openalex.org/W6677803786', 'https://openalex.org/W2785860501', 'https://openalex.org/W2826003142', 'https://openalex.org/W2767122664', 'https://openalex.org/W2167845555', 'https://openalex.org/W6640828828', 'https://openalex.org/W2921843068', 'https://openalex.org/W2594951208', 'https://openalex.org/W3100270690', 'https://openalex.org/W6638159135', 'https://openalex.org/W2785415724', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963266252', 'https://openalex.org/W2787447541', 'https://openalex.org/W2586754519', 'https://openalex.org/W2759889345', 'https://openalex.org/W2895297209', 'https://openalex.org/W2810166208', 'https://openalex.org/W2509930204', 'https://openalex.org/W4239943352', 'https://openalex.org/W6631362777', 'https://openalex.org/W2057653135', 'https://openalex.org/W1528778941', 'https://openalex.org/W2513125788', 'https://openalex.org/W2614542633', 'https://openalex.org/W2295297373', 'https://openalex.org/W2786902352', 'https://openalex.org/W2787223168', 'https://openalex.org/W2889228998', 'https://openalex.org/W6677154653', 'https://openalex.org/W1545920196', 'https://openalex.org/W6786045457', 'https://openalex.org/W6678947187', 'https://openalex.org/W6712553779', 'https://openalex.org/W2514600732', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963620343', 'https://openalex.org/W6748325621', 'https://openalex.org/W6640777149', 'https://openalex.org/W2055408826', 'https://openalex.org/W2072563337', 'https://openalex.org/W6973666849', 'https://openalex.org/W2005708641', 'https://openalex.org/W6696934422', 'https://openalex.org/W2400549570', 'https://openalex.org/W2120209245', 'https://openalex.org/W162588823', 'https://openalex.org/W2345811097']",2019-08-28
https://openalex.org/W3096359985,https://doi.org/10.21437/interspeech.2020-2559,Cyclic Spectral Modeling for Unsupervised Unit Discovery into Voice Conversion with Excitation and Waveform Modeling,,"['https://openalex.org/W2346964103', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W2946555236', 'https://openalex.org/W2547039119', 'https://openalex.org/W2972867623', 'https://openalex.org/W2940544976', 'https://openalex.org/W2532494225', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963620343', 'https://openalex.org/W2025482506', 'https://openalex.org/W2786868129', 'https://openalex.org/W2134202996', 'https://openalex.org/W2964115348', 'https://openalex.org/W2471520273', 'https://openalex.org/W2749651610', 'https://openalex.org/W2055408826', 'https://openalex.org/W3015878662', 'https://openalex.org/W2406349064', 'https://openalex.org/W2964121744', 'https://openalex.org/W2972544500', 'https://openalex.org/W2395899413']",2020-10-25
https://openalex.org/W2889228998,https://doi.org/10.21437/interspeech.2018-1081,Exploiting Speaker and Phonetic Diversity of Mismatched Language Resources for Unsupervised Subword Modeling,,"['https://openalex.org/W2128032727', 'https://openalex.org/W2152051032', 'https://openalex.org/W2399576818', 'https://openalex.org/W2516890051', 'https://openalex.org/W2184045248', 'https://openalex.org/W2963381607', 'https://openalex.org/W2002342963', 'https://openalex.org/W2787447541', 'https://openalex.org/W2404799143', 'https://openalex.org/W2963684067']",2018-08-28
https://openalex.org/W2972841524,https://doi.org/10.21437/interspeech.2019-2052,Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages,,"['https://openalex.org/W2126203737', 'https://openalex.org/W2796339975', 'https://openalex.org/W2509930204', 'https://openalex.org/W2395899413', 'https://openalex.org/W2513125788', 'https://openalex.org/W2963826681', 'https://openalex.org/W112696851', 'https://openalex.org/W2962693497', 'https://openalex.org/W2399576818', 'https://openalex.org/W1635512741', 'https://openalex.org/W2002342963', 'https://openalex.org/W2043878967', 'https://openalex.org/W2826003142', 'https://openalex.org/W2001933206', 'https://openalex.org/W1524333225', 'https://openalex.org/W2510867321', 'https://openalex.org/W2404799143', 'https://openalex.org/W2119187236', 'https://openalex.org/W2963620343', 'https://openalex.org/W66167291', 'https://openalex.org/W1545920196', 'https://openalex.org/W2100768664', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786902352', 'https://openalex.org/W2791647162', 'https://openalex.org/W2406349064']",2019-09-13
https://openalex.org/W3044483536,https://doi.org/10.21437/interspeech.2020-1170,Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling,"This study addresses unsupervised subword modeling, i.e., learning feature\nrepresentations that can distinguish subword units of a language. The proposed\napproach adopts a two-stage bottleneck feature (BNF) learning framework,\nconsisting of autoregressive predictive coding (APC) as a front-end and a\nDNN-BNF model as a back-end. APC pretrained features are set as input features\nto a DNN-BNF model. A language-mismatched ASR system is used to provide\ncross-lingual phone labels for DNN-BNF model training. Finally, BNFs are\nextracted as the subword-discriminative feature representation. A second aim of\nthis work is to investigate the robustness of our approach's effectiveness to\ndifferent amounts of training data. The results on Libri-light and the\nZeroSpeech 2017 databases show that APC is effective in front-end feature\npretraining. Our whole system outperforms the state of the art on both\ndatabases. Cross-lingual phone labels for English data by a Dutch ASR\noutperform those by a Mandarin ASR, possibly linked to the larger similarity of\nDutch compared to Mandarin with English. Our system is less sensitive to\ntraining data amount when the training data is over 50 hours. APC pretraining\nleads to a reduction of needed training material from over 5,000 hours to\naround 200 hours with little performance degradation.\n","['https://openalex.org/W2012897754', 'https://openalex.org/W2064675550', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2842511635', 'https://openalex.org/W29952999', 'https://openalex.org/W2786902352', 'https://openalex.org/W162588823', 'https://openalex.org/W3016181583', 'https://openalex.org/W2964121744', 'https://openalex.org/W2927191280', 'https://openalex.org/W2468716020', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950414763', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963799213', 'https://openalex.org/W1524333225', 'https://openalex.org/W2787426069', 'https://openalex.org/W3104842308', 'https://openalex.org/W2949510815', 'https://openalex.org/W2758785877', 'https://openalex.org/W2085628288', 'https://openalex.org/W1494198834', 'https://openalex.org/W2293634267', 'https://openalex.org/W3016011332', 'https://openalex.org/W2399576818', 'https://openalex.org/W2786608204', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963620343', 'https://openalex.org/W4300047444', 'https://openalex.org/W2514741789', 'https://openalex.org/W2787447541']",2020-10-25
https://openalex.org/W3102519966,https://doi.org/10.18653/v1/2020.conll-1.15,Acquiring language from speech by learning to remember and predict,"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.","['https://openalex.org/W2786608204', 'https://openalex.org/W2170716495', 'https://openalex.org/W2400668844', 'https://openalex.org/W1836465849', 'https://openalex.org/W2925135347', 'https://openalex.org/W2398490608', 'https://openalex.org/W2153903782', 'https://openalex.org/W1965511524', 'https://openalex.org/W2898208057', 'https://openalex.org/W2101234009', 'https://openalex.org/W2033523384', 'https://openalex.org/W4237595250', 'https://openalex.org/W2129056111', 'https://openalex.org/W2345811097', 'https://openalex.org/W2907911668', 'https://openalex.org/W2963735467', 'https://openalex.org/W2779379590', 'https://openalex.org/W2340316421', 'https://openalex.org/W74826805', 'https://openalex.org/W2963137467', 'https://openalex.org/W2964169922', 'https://openalex.org/W2468716020', 'https://openalex.org/W2176412452', 'https://openalex.org/W2963285578', 'https://openalex.org/W1994343286', 'https://openalex.org/W2145410271', 'https://openalex.org/W4241400578', 'https://openalex.org/W2126377586', 'https://openalex.org/W3193078085', 'https://openalex.org/W2160635596', 'https://openalex.org/W3014415613', 'https://openalex.org/W2785860501', 'https://openalex.org/W2945769669', 'https://openalex.org/W1796128977', 'https://openalex.org/W2963620343', 'https://openalex.org/W2980286501', 'https://openalex.org/W2972867623', 'https://openalex.org/W4248358431', 'https://openalex.org/W2519091744', 'https://openalex.org/W1507893557', 'https://openalex.org/W2040361537', 'https://openalex.org/W4300047444', 'https://openalex.org/W1545920196', 'https://openalex.org/W2089242029', 'https://openalex.org/W2964121744', 'https://openalex.org/W1984588653', 'https://openalex.org/W2006529618', 'https://openalex.org/W2549835527', 'https://openalex.org/W2148764920', 'https://openalex.org/W2062663442', 'https://openalex.org/W2553292466', 'https://openalex.org/W2963799213', 'https://openalex.org/W4242487065', 'https://openalex.org/W1994278000', 'https://openalex.org/W2141440284', 'https://openalex.org/W4245860799', 'https://openalex.org/W2044222806', 'https://openalex.org/W2259472270', 'https://openalex.org/W2104996750', 'https://openalex.org/W1598851216', 'https://openalex.org/W1993859479', 'https://openalex.org/W1584473438', 'https://openalex.org/W2092458178', 'https://openalex.org/W2795342569', 'https://openalex.org/W2147241151', 'https://openalex.org/W4211073357', 'https://openalex.org/W2787447541', 'https://openalex.org/W2188968740', 'https://openalex.org/W1994463350', 'https://openalex.org/W2949382160', 'https://openalex.org/W1969005071', 'https://openalex.org/W2617093514', 'https://openalex.org/W2150315647', 'https://openalex.org/W2477383735', 'https://openalex.org/W2786902352', 'https://openalex.org/W2012084635', 'https://openalex.org/W4385245566', 'https://openalex.org/W2128084896', 'https://openalex.org/W2005126631', 'https://openalex.org/W2145889472', 'https://openalex.org/W2319920447', 'https://openalex.org/W2400549570', 'https://openalex.org/W2153568660', 'https://openalex.org/W2095872964', 'https://openalex.org/W2522012644', 'https://openalex.org/W2788924045', 'https://openalex.org/W2116533103', 'https://openalex.org/W2130028098', 'https://openalex.org/W2964335542', 'https://openalex.org/W3125709657', 'https://openalex.org/W1522301498', 'https://openalex.org/W3035750922', 'https://openalex.org/W2064675550', 'https://openalex.org/W2758697525', 'https://openalex.org/W1992337477', 'https://openalex.org/W2242818861', 'https://openalex.org/W2125930537', 'https://openalex.org/W2399576818', 'https://openalex.org/W2100768664', 'https://openalex.org/W1735013697', 'https://openalex.org/W2128957129', 'https://openalex.org/W2119871874', 'https://openalex.org/W2785415724', 'https://openalex.org/W3100739365', 'https://openalex.org/W2034663031', 'https://openalex.org/W2118276816', 'https://openalex.org/W2151834591', 'https://openalex.org/W2044138293', 'https://openalex.org/W3098643042', 'https://openalex.org/W2402366697', 'https://openalex.org/W2152134037', 'https://openalex.org/W1533861849', 'https://openalex.org/W2828202920', 'https://openalex.org/W2074376560', 'https://openalex.org/W2787426069', 'https://openalex.org/W4254436426', 'https://openalex.org/W2101711363', 'https://openalex.org/W2122741244', 'https://openalex.org/W2994914858', 'https://openalex.org/W2160138058', 'https://openalex.org/W1992844041', 'https://openalex.org/W2964243640', 'https://openalex.org/W1980862600', 'https://openalex.org/W2793227081', 'https://openalex.org/W2103142096', 'https://openalex.org/W2544743505', 'https://openalex.org/W2152824855', 'https://openalex.org/W2137027522', 'https://openalex.org/W2087946919', 'https://openalex.org/W2613332842', 'https://openalex.org/W1497204367', 'https://openalex.org/W2155106314', 'https://openalex.org/W2075201173', 'https://openalex.org/W2165348108', 'https://openalex.org/W2787223168', 'https://openalex.org/W1597739853', 'https://openalex.org/W2396043527', 'https://openalex.org/W2407614114', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963403868', 'https://openalex.org/W2326588846']",2020-01-01
https://openalex.org/W3093121832,https://doi.org/10.21437/interspeech.2020-1671,Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling,"In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.","['https://openalex.org/W1524333225', 'https://openalex.org/W2785415724', 'https://openalex.org/W2963620343', 'https://openalex.org/W2786902352', 'https://openalex.org/W2787223168', 'https://openalex.org/W2911249026', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963980299', 'https://openalex.org/W4288279357', 'https://openalex.org/W1568183767', 'https://openalex.org/W2787426069', 'https://openalex.org/W3023172065', 'https://openalex.org/W2730658205', 'https://openalex.org/W2517346207', 'https://openalex.org/W2759889345', 'https://openalex.org/W2399576818', 'https://openalex.org/W2786337938', 'https://openalex.org/W4288107125', 'https://openalex.org/W2786608204', 'https://openalex.org/W2325526696', 'https://openalex.org/W2787447541', 'https://openalex.org/W4300047444', 'https://openalex.org/W2889500720', 'https://openalex.org/W2768352418']",2020-10-25
https://openalex.org/W3198395459,https://doi.org/10.21437/interspeech.2021-1340,Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages,,"['https://openalex.org/W2963620343', 'https://openalex.org/W2964321699', 'https://openalex.org/W2940544976', 'https://openalex.org/W2972374322', 'https://openalex.org/W2972867623', 'https://openalex.org/W2787447541', 'https://openalex.org/W3093427098', 'https://openalex.org/W3163591408', 'https://openalex.org/W3094124570', 'https://openalex.org/W2395899413', 'https://openalex.org/W2918342466', 'https://openalex.org/W2963799213', 'https://openalex.org/W3095361818', 'https://openalex.org/W2399576818', 'https://openalex.org/W2406349064', 'https://openalex.org/W3020724926', 'https://openalex.org/W3096125675', 'https://openalex.org/W2895297209', 'https://openalex.org/W2963653811', 'https://openalex.org/W2107904695']",2021-08-27
https://openalex.org/W4390887450,https://doi.org/10.1109/taslp.2024.3350888,Slowness Regularized Contrastive Predictive Coding for Acoustic Unit Discovery,"Self-supervised methods such as Contrastive predictive Coding (CPC) have greatly improved the quality of the unsupervised representations. These representations significantly reduce the amount of labeled data needed for downstream task performance, such as automatic speech recognition. CPC learns representations by learning to predict future frames given current frames. Based on the observation that the acoustic information, e.g., phones, changes slower than the feature extraction rate in CPC, we propose regularization techniques that impose slowness constraints on the features. Here we propose two regularization techniques: Self-expressing constraint and Left-or-Right regularization. We evaluate the proposed model on ABX and linear phone classification tasks, acoustic unit discovery, and automatic speech recognition. The regularized CPC trained on 100 hours of unlabeled data matches the performance of the baseline CPC trained on 360 hours of unlabeled data. We also show that our regularization techniques are complementary to data augmentation and can further boost the system's performance. In monolingual, cross-lingual, or multilingual settings, with/without data augmentation, regardless of the amount of data used for training, our regularized models outperformed the baseline CPC models on the ABX task.","['https://openalex.org/W2160815625', 'https://openalex.org/W2150769028', 'https://openalex.org/W4301204483', 'https://openalex.org/W2057007397', 'https://openalex.org/W2020607164', 'https://openalex.org/W2117041980', 'https://openalex.org/W2170659185', 'https://openalex.org/W6675022971', 'https://openalex.org/W2078769636', 'https://openalex.org/W2468716020', 'https://openalex.org/W2747192917', 'https://openalex.org/W2964169922', 'https://openalex.org/W2890718354', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3016181583', 'https://openalex.org/W6755207826', 'https://openalex.org/W6766673545', 'https://openalex.org/W3159481202', 'https://openalex.org/W4385822676', 'https://openalex.org/W3100270690', 'https://openalex.org/W3198782837', 'https://openalex.org/W3197974236', 'https://openalex.org/W3209993061', 'https://openalex.org/W6838949706', 'https://openalex.org/W3097159218', 'https://openalex.org/W2936774411', 'https://openalex.org/W3144810982', 'https://openalex.org/W2786608204', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6799172387', 'https://openalex.org/W2963720603', 'https://openalex.org/W1796128977', 'https://openalex.org/W6640963894', 'https://openalex.org/W2752796333', 'https://openalex.org/W3096656254', 'https://openalex.org/W3095361818', 'https://openalex.org/W2146444479', 'https://openalex.org/W6774456908', 'https://openalex.org/W3198134274', 'https://openalex.org/W4313182775', 'https://openalex.org/W3204915839', 'https://openalex.org/W2395899413', 'https://openalex.org/W4385245566', 'https://openalex.org/W2787447541', 'https://openalex.org/W3198815374']",2024-01-01
https://openalex.org/W2810166208,https://doi.org/10.1109/taslp.2018.2852500,Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling,"We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.","['https://openalex.org/W2152051032', 'https://openalex.org/W2090861223', 'https://openalex.org/W6712444837', 'https://openalex.org/W1994897900', 'https://openalex.org/W2072169887', 'https://openalex.org/W6631362777', 'https://openalex.org/W2044138293', 'https://openalex.org/W2145410271', 'https://openalex.org/W6636447677', 'https://openalex.org/W6712553779', 'https://openalex.org/W2124629003', 'https://openalex.org/W6748489002', 'https://openalex.org/W6704305767', 'https://openalex.org/W2509930204', 'https://openalex.org/W6748342566', 'https://openalex.org/W1920769845', 'https://openalex.org/W1977286842', 'https://openalex.org/W2586754519', 'https://openalex.org/W2101998432', 'https://openalex.org/W6674922813', 'https://openalex.org/W2062373184', 'https://openalex.org/W6678007500', 'https://openalex.org/W1971807270', 'https://openalex.org/W6677150433', 'https://openalex.org/W6973666849', 'https://openalex.org/W1967687583', 'https://openalex.org/W6697293080', 'https://openalex.org/W2069429561', 'https://openalex.org/W6678007862', 'https://openalex.org/W6602662854', 'https://openalex.org/W6678947187', 'https://openalex.org/W1599512239', 'https://openalex.org/W6684363390', 'https://openalex.org/W2106554350', 'https://openalex.org/W1975427826', 'https://openalex.org/W2162021827', 'https://openalex.org/W2002342963', 'https://openalex.org/W1560013842', 'https://openalex.org/W2399576818', 'https://openalex.org/W2128032727', 'https://openalex.org/W1613448136', 'https://openalex.org/W2395899413', 'https://openalex.org/W3101455611', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786902352', 'https://openalex.org/W1589170661', 'https://openalex.org/W2786608204', 'https://openalex.org/W2345811097', 'https://openalex.org/W1524333225', 'https://openalex.org/W2615915281', 'https://openalex.org/W2405618829', 'https://openalex.org/W2963620343', 'https://openalex.org/W2120636621', 'https://openalex.org/W2600717762', 'https://openalex.org/W2100163972', 'https://openalex.org/W2113037082', 'https://openalex.org/W2120432176', 'https://openalex.org/W64187236']",2018-07-02
https://openalex.org/W2972982164,https://doi.org/10.21437/interspeech.2019-2938,SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders,,"['https://openalex.org/W2963317665', 'https://openalex.org/W1524333225', 'https://openalex.org/W2126203737', 'https://openalex.org/W2395899413', 'https://openalex.org/W1494198834', 'https://openalex.org/W2190506272', 'https://openalex.org/W2020607164', 'https://openalex.org/W2951008357', 'https://openalex.org/W2099415988', 'https://openalex.org/W2090861223', 'https://openalex.org/W2399869768', 'https://openalex.org/W2131774270', 'https://openalex.org/W2895297209', 'https://openalex.org/W2593779438', 'https://openalex.org/W2078993594', 'https://openalex.org/W2399576818', 'https://openalex.org/W1545920196', 'https://openalex.org/W2398776621', 'https://openalex.org/W2964308564', 'https://openalex.org/W1902237438', 'https://openalex.org/W1796128977', 'https://openalex.org/W2787447541', 'https://openalex.org/W2073459066']",2019-09-13
https://openalex.org/W3110371022,https://doi.org/10.1109/taslp.2020.3042016,Tackling Perception Bias in Unsupervised Phoneme Discovery Using DPGMM-RNN Hybrid Model and Functional Load,"The human perception of phonemes is biased against speech sounds. The lack of correspondence between perceptual phonemes and acoustic signals forms a big challenge in designing unsupervised algorithms to distinguish phonemes from sound. We propose the DPGMM-RNN hybrid model that improves phoneme categorization by relieving the fragmentation problem. We also merge segments with low functional load, which is the work done by segment contrasts to differentiate between utterances, just like humans who convert unambiguous segments into phonemes as units for immediate perception. Our results show that the DPGMM-RNN hybrid model relieves the fragmentation problem and improves phoneme discriminability. The minimal functional load merge compresses a segment system, preserves information and keeps phoneme discriminability.","['https://openalex.org/W2579555219', 'https://openalex.org/W2051512859', 'https://openalex.org/W6711866534', 'https://openalex.org/W2252172689', 'https://openalex.org/W1994158173', 'https://openalex.org/W6824062597', 'https://openalex.org/W2586754519', 'https://openalex.org/W6674922813', 'https://openalex.org/W6677461952', 'https://openalex.org/W6602180557', 'https://openalex.org/W6713016582', 'https://openalex.org/W2038101708', 'https://openalex.org/W6678947187', 'https://openalex.org/W2940544976', 'https://openalex.org/W4254499902', 'https://openalex.org/W2117041980', 'https://openalex.org/W6675022971', 'https://openalex.org/W91681889', 'https://openalex.org/W2068247585', 'https://openalex.org/W1977531436', 'https://openalex.org/W2015394094', 'https://openalex.org/W2011334394', 'https://openalex.org/W4246695671', 'https://openalex.org/W6682569104', 'https://openalex.org/W1600008395', 'https://openalex.org/W6713593416', 'https://openalex.org/W2950414763', 'https://openalex.org/W6745117592', 'https://openalex.org/W2752796333', 'https://openalex.org/W2095089846', 'https://openalex.org/W6675354045', 'https://openalex.org/W6734901337', 'https://openalex.org/W6680970901', 'https://openalex.org/W6713256719', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963830550', 'https://openalex.org/W2759889345', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W6712553779', 'https://openalex.org/W2056133372', 'https://openalex.org/W2057007397', 'https://openalex.org/W2170659185', 'https://openalex.org/W6973666849', 'https://openalex.org/W2114347655', 'https://openalex.org/W2020607164', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W2010548701', 'https://openalex.org/W2017339534', 'https://openalex.org/W2506406911', 'https://openalex.org/W2509930204', 'https://openalex.org/W2826003142', 'https://openalex.org/W2089381378', 'https://openalex.org/W2345811097', 'https://openalex.org/W2750248772', 'https://openalex.org/W2403015869', 'https://openalex.org/W3145269374', 'https://openalex.org/W2151967501', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W1984314602', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396733358', 'https://openalex.org/W2054496875', 'https://openalex.org/W52412328', 'https://openalex.org/W2101234009', 'https://openalex.org/W2796346319', 'https://openalex.org/W2963799213', 'https://openalex.org/W2054168569', 'https://openalex.org/W2963620343', 'https://openalex.org/W2786608204', 'https://openalex.org/W2100768664', 'https://openalex.org/W1635512741', 'https://openalex.org/W2758785877', 'https://openalex.org/W2973026522', 'https://openalex.org/W2138615112', 'https://openalex.org/W2963618559', 'https://openalex.org/W2100163972', 'https://openalex.org/W2895297209', 'https://openalex.org/W2402366697', 'https://openalex.org/W2117126688', 'https://openalex.org/W2399576818', 'https://openalex.org/W2404799143', 'https://openalex.org/W2593779438', 'https://openalex.org/W2128032727']",2020-12-02
https://openalex.org/W3143569452,https://doi.org/10.1109/slt48900.2021.9383597,Incorporating Discriminative DPGMM Posteriorgrams for Low-Resource ASR,"The first step in building an ASR system is to extract proper speech features. The ideal speech features for ASR must also have high discriminabilities between linguistic units and be robust to such non-linguistic factors as gender, age, emotions, or noise. The discriminabilities of various features have been compared in several Zerospeech challenges to discover linguistic units without any transcriptions, in which the posteriorgrams of DPGMM clustering show strong discriminability and get several top results of ABX discrimination scores between phonemes. This paper appends DPGMM posteriorgrams to increase the discriminability of acoustic features to enhance ASR systems. To the best of our knowledge, DPGMM features, which are usually applied to such tasks as spoken term detection and zero resources tasks, have not been applied to large vocabulary continuous speech recognition (LVCSR) before. DPGMM clustering can dynamically change the number of Gaussians until each one fits one segmental pattern of the whole speech corpus with the highest probability such that the linguistic units of different segmental patterns are clearly discriminated. Our experimental results on the WSJ corpora show our proposal stably improves ASR systems and provides even more improvement for smaller datasets with fewer resources.","['https://openalex.org/W2345811097', 'https://openalex.org/W6726320248', 'https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W2526425061', 'https://openalex.org/W2587741066', 'https://openalex.org/W6638545294', 'https://openalex.org/W6631190155', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963620343', 'https://openalex.org/W3009565979', 'https://openalex.org/W2940544976', 'https://openalex.org/W6638159135', 'https://openalex.org/W2020607164', 'https://openalex.org/W6712202099', 'https://openalex.org/W6713256719', 'https://openalex.org/W2972374322', 'https://openalex.org/W2759889345', 'https://openalex.org/W6675022971', 'https://openalex.org/W2347098582', 'https://openalex.org/W6631362777', 'https://openalex.org/W6682569104', 'https://openalex.org/W6633431331', 'https://openalex.org/W2787447541', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712553779', 'https://openalex.org/W2090861223', 'https://openalex.org/W6973666849', 'https://openalex.org/W2148154194', 'https://openalex.org/W2750248772', 'https://openalex.org/W2103387126', 'https://openalex.org/W2950414763', 'https://openalex.org/W6824062597', 'https://openalex.org/W2156983866', 'https://openalex.org/W2024490156', 'https://openalex.org/W3015213852', 'https://openalex.org/W2165712214', 'https://openalex.org/W193119429', 'https://openalex.org/W1524333225', 'https://openalex.org/W2396043527', 'https://openalex.org/W3102516861', 'https://openalex.org/W2404799143', 'https://openalex.org/W1522301498', 'https://openalex.org/W1560013842', 'https://openalex.org/W2786608204', 'https://openalex.org/W2964121744', 'https://openalex.org/W2399576818', 'https://openalex.org/W1635512741', 'https://openalex.org/W1796128977', 'https://openalex.org/W2395899413', 'https://openalex.org/W2151967501', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963347649', 'https://openalex.org/W2525778437', 'https://openalex.org/W1815076433', 'https://openalex.org/W2100768664']",2021-01-19
https://openalex.org/W4210997382,https://doi.org/10.1109/taslp.2022.3150220,Modeling Unsupervised Empirical Adaptation by DPGMM and DPGMM-RNN Hybrid Model to Extract Perceptual Features for Low-Resource ASR,"Speech feature extraction is critical for ASR systems. Such successful features as MFCC and PLP use filterbank techniques to model log-scaled speech perception but fail to model the adaptation of human speech perception by hearing experiences. Infant perception that is adapted by hearing speech without text may cause permanent brain state modifications (engrams) that serve as a physical fundamental basis for lifetime speech perception formation. This realization motivates us to propose to model such an unsupervised adaptation process, where adaptation denotes perception that is affected or changed by the history of experiences, with the Dirichlet Process Gaussian Mixture Model (DPGMM) and the DPGMM-RNN hybrid model to extract perceptual features to improve ASR. Our proposed features extend MFCC features with posteriorgrams extracted from the DPGMM algorithm or the DPGMM-RNN hybrid model. Our analysis shows that the DPGMM and DPGMM-RNN model perplexities agree with infant auditory perplexity to support that the proposed features are perceptual. Our ASR results verify the effectiveness of the proposed unsupervised features in such tasks as LVCSR on WSJ and ASR on noisy low-resource telephone conversations, compared with the supervised bottleneck features from Kaldi in ASR performance.","['https://openalex.org/W2148154194', 'https://openalex.org/W2090861223', 'https://openalex.org/W2101509422', 'https://openalex.org/W4255383621', 'https://openalex.org/W2998470023', 'https://openalex.org/W2610261218', 'https://openalex.org/W2082235945', 'https://openalex.org/W2155505956', 'https://openalex.org/W2083750191', 'https://openalex.org/W1980006923', 'https://openalex.org/W2943518292', 'https://openalex.org/W2148389554', 'https://openalex.org/W2041018266', 'https://openalex.org/W2040455912', 'https://openalex.org/W625960733', 'https://openalex.org/W4206040902', 'https://openalex.org/W2466905317', 'https://openalex.org/W2082396705', 'https://openalex.org/W2096162685', 'https://openalex.org/W4300166480', 'https://openalex.org/W6808634324', 'https://openalex.org/W198924044', 'https://openalex.org/W2165712214', 'https://openalex.org/W179875071', 'https://openalex.org/W2069429561', 'https://openalex.org/W2787447541', 'https://openalex.org/W2399576818', 'https://openalex.org/W2950414763', 'https://openalex.org/W2395899413', 'https://openalex.org/W1796128977', 'https://openalex.org/W2404799143', 'https://openalex.org/W2972374322', 'https://openalex.org/W2759889345', 'https://openalex.org/W2347098582', 'https://openalex.org/W2750248772', 'https://openalex.org/W6675022971', 'https://openalex.org/W3143569452', 'https://openalex.org/W6683650587', 'https://openalex.org/W6602180557', 'https://openalex.org/W2143022183', 'https://openalex.org/W2104752510', 'https://openalex.org/W2095458199', 'https://openalex.org/W2169991335', 'https://openalex.org/W4254499902', 'https://openalex.org/W4251668937', 'https://openalex.org/W3110371022', 'https://openalex.org/W2895297209', 'https://openalex.org/W2099111195', 'https://openalex.org/W6680970901', 'https://openalex.org/W1635512741', 'https://openalex.org/W2404126548', 'https://openalex.org/W6744702808', 'https://openalex.org/W6631362777', 'https://openalex.org/W2151967501', 'https://openalex.org/W6842558766', 'https://openalex.org/W2345811097', 'https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W3095361818', 'https://openalex.org/W4245838404', 'https://openalex.org/W2526425061', 'https://openalex.org/W3009565979', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W2037662195', 'https://openalex.org/W2011238950', 'https://openalex.org/W2076493153', 'https://openalex.org/W2077382402', 'https://openalex.org/W2038142680', 'https://openalex.org/W2070095866', 'https://openalex.org/W2018567671', 'https://openalex.org/W6751796006', 'https://openalex.org/W2019200248', 'https://openalex.org/W1991923060', 'https://openalex.org/W2007105567', 'https://openalex.org/W1497856780', 'https://openalex.org/W4320013820', 'https://openalex.org/W1608461387', 'https://openalex.org/W2973026522', 'https://openalex.org/W2804109700']",2022-01-01
https://openalex.org/W4224918488,https://doi.org/10.1109/icassp43922.2022.9747259,Self Supervised Representation Learning with Deep Clustering for Acoustic Unit Discovery from Raw Speech,"The automatic discovery of acoustic sub-word units from raw speech, without any text or labels, is a growing field of research. The key challenge is to derive representations of speech that can be categorized into a small number of phoneme-like units which are speaker invariant and can broadly capture the content variability of speech. In this work, we propose a novel neural network paradigm that uses the deep clustering loss along with the autoregressive contrastive predictive coding (CPC) loss. Both the loss functions, the CPC and the clustering loss, are self-supervised. The clustering cost involves the loss function using the phoneme-like labels generated with an iterative k-means algorithm. The inclusion of this loss ensures that the model representations can be categorized into a small number of automatic speech units. We experiment with several sub-tasks described as part of the Zerospeech 2021 challenge to illustrate the effectiveness of the framework. In these experiments, we show that proposed representation learning approach improves significantly over the previous self-supervision based models as well as the wav2vec family of models on a range of word-level similarity tasks and language modeling tasks.","['https://openalex.org/W2741692265', 'https://openalex.org/W6771812881', 'https://openalex.org/W6629717138', 'https://openalex.org/W6770941677', 'https://openalex.org/W2787447541', 'https://openalex.org/W2785860501', 'https://openalex.org/W3095361818', 'https://openalex.org/W2787223168', 'https://openalex.org/W2935542736', 'https://openalex.org/W3100270690', 'https://openalex.org/W3015213852', 'https://openalex.org/W3041561163', 'https://openalex.org/W3197381195', 'https://openalex.org/W3035725276', 'https://openalex.org/W6753000030', 'https://openalex.org/W6640828828', 'https://openalex.org/W6755207826', 'https://openalex.org/W1949782964', 'https://openalex.org/W2786608204', 'https://openalex.org/W2124509324', 'https://openalex.org/W2117041980', 'https://openalex.org/W3093096176', 'https://openalex.org/W2963620343', 'https://openalex.org/W2114347655', 'https://openalex.org/W3197259906', 'https://openalex.org/W6790356757', 'https://openalex.org/W3096216486', 'https://openalex.org/W2973049979', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W2752796333', 'https://openalex.org/W2842511635', 'https://openalex.org/W2883725317', 'https://openalex.org/W1957665339', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W2896457183', 'https://openalex.org/W4394671563', 'https://openalex.org/W2187089797', 'https://openalex.org/W2979476256', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996728628', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297808394']",2022-04-27
https://openalex.org/W3145811386,https://doi.org/10.21437/interspeech.2021-1664,Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation,"This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.","['https://openalex.org/W2964169922', 'https://openalex.org/W2972943112', 'https://openalex.org/W1957665339', 'https://openalex.org/W2100768664', 'https://openalex.org/W2973026522', 'https://openalex.org/W2594951208', 'https://openalex.org/W2962693497', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972794572', 'https://openalex.org/W3095361818', 'https://openalex.org/W3044483536', 'https://openalex.org/W2147768505', 'https://openalex.org/W2995181338', 'https://openalex.org/W2963620343', 'https://openalex.org/W3104842308', 'https://openalex.org/W2347145335', 'https://openalex.org/W3093096176', 'https://openalex.org/W1524333225', 'https://openalex.org/W2972574141', 'https://openalex.org/W2787447541', 'https://openalex.org/W2101234009', 'https://openalex.org/W3045485643', 'https://openalex.org/W3016181583', 'https://openalex.org/W2895297209', 'https://openalex.org/W2950414763', 'https://openalex.org/W3100202343', 'https://openalex.org/W2750248772', 'https://openalex.org/W1557247526', 'https://openalex.org/W2514741789', 'https://openalex.org/W3025286576', 'https://openalex.org/W2762715843', 'https://openalex.org/W2996383576', 'https://openalex.org/W3094197178', 'https://openalex.org/W3095732712', 'https://openalex.org/W1631260214', 'https://openalex.org/W2327501763', 'https://openalex.org/W2347098582', 'https://openalex.org/W1975728937', 'https://openalex.org/W2963678298', 'https://openalex.org/W2399576818', 'https://openalex.org/W2059652594', 'https://openalex.org/W66627554']",2021-08-27
https://openalex.org/W3171005929,https://doi.org/10.1109/ccai50917.2021.9447504,Unsupervised Feature Representation Learning using Sequence-to-sequence Autoencoder Architecture for Low-resource Language,"In this paper, we aim to improve the traditional bottleneck feature extraction under the low-resource scenario. We employ the factorized hierarchical variational autoencoder (FHVAE) to learn an unsupervised feature representation by encoding the linguistic-relevant information into latent variables. In order to obtain more significant latent variables, the attention mechanism is introduced into the encoders of FHVAE. In addition to the reconstruction decoder of FHVAE, the phonetic-aware decoder is introduced to backward transmit the phonemic information into the latent variables, enhancing the performance of feature representation learning. The idea of multi-task learning is used to organize the encoders of FHVAE, the reconstruction decoder of FHVAE and the phonetic-aware decoder into the training process. To demonstrate the effectiveness of the proposed method, the ABX discriminability and the language identification are evaluated on the ZeroSpeech 2017 and the LRE 2017 respectively. These experimental results shown that the learned feature representation outperforms traditional acoustic feature.","['https://openalex.org/W6712553779', 'https://openalex.org/W2509930204', 'https://openalex.org/W2972841524', 'https://openalex.org/W3094658127', 'https://openalex.org/W3096210705', 'https://openalex.org/W2889162002', 'https://openalex.org/W3094861110', 'https://openalex.org/W2787223168', 'https://openalex.org/W2787447541', 'https://openalex.org/W1984076147', 'https://openalex.org/W3007486152', 'https://openalex.org/W2809271438', 'https://openalex.org/W2963620343', 'https://openalex.org/W2786902352', 'https://openalex.org/W2972314145', 'https://openalex.org/W2826003142', 'https://openalex.org/W2614542633', 'https://openalex.org/W2785860501', 'https://openalex.org/W3097649098', 'https://openalex.org/W3100270690', 'https://openalex.org/W2916297645', 'https://openalex.org/W6745117592', 'https://openalex.org/W2406392101', 'https://openalex.org/W2890964092', 'https://openalex.org/W2758785877', 'https://openalex.org/W3125709657', 'https://openalex.org/W2398046886', 'https://openalex.org/W2963618559', 'https://openalex.org/W2399576818']",2021-05-07
https://openalex.org/W4297841546,https://doi.org/10.21437/interspeech.2022-11369,A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery,"Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents.No temporal information is used in the model.However, there is often a relationship between the corresponding topics of consecutive tokens.In this paper, we present an extension to LDA that uses a Markov chain to model temporal information.We use this new model for acoustic unit discovery from speech.As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes.The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones.In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another.This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA.Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.","['https://openalex.org/W3204081232', 'https://openalex.org/W1590183771', 'https://openalex.org/W4231510805', 'https://openalex.org/W4297808394', 'https://openalex.org/W1532325895', 'https://openalex.org/W2972867623', 'https://openalex.org/W2145410271', 'https://openalex.org/W2780786457', 'https://openalex.org/W2347098582', 'https://openalex.org/W4288107125', 'https://openalex.org/W1593793857', 'https://openalex.org/W2963799213', 'https://openalex.org/W1965555277', 'https://openalex.org/W3093427098', 'https://openalex.org/W4254018622', 'https://openalex.org/W3210530853', 'https://openalex.org/W2787447541', 'https://openalex.org/W2091746061', 'https://openalex.org/W3198134274', 'https://openalex.org/W4313182775', 'https://openalex.org/W4287591426', 'https://openalex.org/W2927191280', 'https://openalex.org/W1511986666', 'https://openalex.org/W130754613', 'https://openalex.org/W2750248772', 'https://openalex.org/W1506806321', 'https://openalex.org/W2962693497', 'https://openalex.org/W2347145335', 'https://openalex.org/W4221151701', 'https://openalex.org/W3095361818', 'https://openalex.org/W2641832364', 'https://openalex.org/W2100768664', 'https://openalex.org/W3198395459', 'https://openalex.org/W3205900445']",2022-09-16
https://openalex.org/W4389317789,https://doi.org/10.1109/taslp.2023.3337670,Representation Learning With Hidden Unit Clustering for Low Resource Speech Applications,"In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned ""time-frequency"" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula> -means algorithm. We explore techniques that improve the speaker invariance of the learned representations and illustrate the effectiveness of the proposed approach on two settings, i) completely unsupervised speech applications on the sub-tasks described as part of the ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition (ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi dataset. In these experiments, we achieve state-of-art results for various Zerospeech tasks. Further, on the ASR experiments, the HUC representations are shown to improve over other established benchmarks based on wav2vec, HuBERT and Best-RQ.","['https://openalex.org/W6790356757', 'https://openalex.org/W2114347655', 'https://openalex.org/W3197259906', 'https://openalex.org/W3035725276', 'https://openalex.org/W6769196770', 'https://openalex.org/W3197381195', 'https://openalex.org/W2972943112', 'https://openalex.org/W2394873997', 'https://openalex.org/W2756577849', 'https://openalex.org/W2972984069', 'https://openalex.org/W2889087444', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W2896457183', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963620343', 'https://openalex.org/W3093096176', 'https://openalex.org/W2787447541', 'https://openalex.org/W2785860501', 'https://openalex.org/W3095361818', 'https://openalex.org/W2787223168', 'https://openalex.org/W3100270690', 'https://openalex.org/W3015213852', 'https://openalex.org/W3041561163', 'https://openalex.org/W4385822823', 'https://openalex.org/W4385822567', 'https://openalex.org/W4224918488', 'https://openalex.org/W2883725317', 'https://openalex.org/W2295598076', 'https://openalex.org/W2124509324', 'https://openalex.org/W3209059054', 'https://openalex.org/W6739901393', 'https://openalex.org/W3011411500', 'https://openalex.org/W6810673746', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W2097749765', 'https://openalex.org/W1973041621', 'https://openalex.org/W2395899413', 'https://openalex.org/W2995181338', 'https://openalex.org/W2741692265', 'https://openalex.org/W2996728628', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W4385571911', 'https://openalex.org/W6636915900', 'https://openalex.org/W6688816777', 'https://openalex.org/W1635512741', 'https://openalex.org/W2766219058', 'https://openalex.org/W3163793923', 'https://openalex.org/W6996577779', 'https://openalex.org/W2107223151', 'https://openalex.org/W4239510810', 'https://openalex.org/W3006926732', 'https://openalex.org/W2726515241', 'https://openalex.org/W2030931454', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394671563', 'https://openalex.org/W2029685080', 'https://openalex.org/W2187089797', 'https://openalex.org/W2219249508']",2023-12-04
https://openalex.org/W3112085130,https://doi.org/10.1109/ojsp.2021.3076914,The Effectiveness of Unsupervised Subword Modeling With Autoregressive and Cross-Lingual Phone-Aware Networks,"This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding (APC) as the front-end and a cross-lingual deep neural network (DNN) as the back-end. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies. Comprehensive and systematic analyses at the phoneme- and articulatory feature (AF)-level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information. Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.&lt;br/&gt;","['https://openalex.org/W2085628288', 'https://openalex.org/W6713762819', 'https://openalex.org/W2905332681', 'https://openalex.org/W2020607164', 'https://openalex.org/W2973157397', 'https://openalex.org/W2786902352', 'https://openalex.org/W2972841524', 'https://openalex.org/W6678947187', 'https://openalex.org/W3044483536', 'https://openalex.org/W2759889345', 'https://openalex.org/W2785860501', 'https://openalex.org/W2895297209', 'https://openalex.org/W2785415724', 'https://openalex.org/W2940544976', 'https://openalex.org/W6780483730', 'https://openalex.org/W2826003142', 'https://openalex.org/W2025722797', 'https://openalex.org/W2889228998', 'https://openalex.org/W6748325621', 'https://openalex.org/W6601192135', 'https://openalex.org/W2972943112', 'https://openalex.org/W6631190155', 'https://openalex.org/W6631362777', 'https://openalex.org/W6636811518', 'https://openalex.org/W3005578234', 'https://openalex.org/W1984076147', 'https://openalex.org/W2787223168', 'https://openalex.org/W2995181338', 'https://openalex.org/W6745117592', 'https://openalex.org/W2842511635', 'https://openalex.org/W2516890051', 'https://openalex.org/W3006358483', 'https://openalex.org/W1494198834', 'https://openalex.org/W2514741789', 'https://openalex.org/W2327501763', 'https://openalex.org/W6675365184', 'https://openalex.org/W6696934422', 'https://openalex.org/W6781819162', 'https://openalex.org/W2927673779', 'https://openalex.org/W2748598007', 'https://openalex.org/W3016368932', 'https://openalex.org/W2950414763', 'https://openalex.org/W6973666849', 'https://openalex.org/W2962824366', 'https://openalex.org/W1778492285', 'https://openalex.org/W2962693497', 'https://openalex.org/W2787447541', 'https://openalex.org/W2752796333', 'https://openalex.org/W2971041032', 'https://openalex.org/W2963620343', 'https://openalex.org/W2513125788', 'https://openalex.org/W2594951208', 'https://openalex.org/W2147768505', 'https://openalex.org/W2160815625', 'https://openalex.org/W1975728937', 'https://openalex.org/W6675022971', 'https://openalex.org/W2641832364', 'https://openalex.org/W6712553779', 'https://openalex.org/W3111682954', 'https://openalex.org/W2972374322', 'https://openalex.org/W2964245029', 'https://openalex.org/W3093096176', 'https://openalex.org/W3005511757', 'https://openalex.org/W2949510815', 'https://openalex.org/W3095361818', 'https://openalex.org/W3100270690', 'https://openalex.org/W3096216486', 'https://openalex.org/W3096359985']",2021-01-01
https://openalex.org/W3105242324,,,"Subword modeling for zero-resource languages aims to learn low-level representations of speech audio without using transcriptions or other resources from the target language (such as text corpora or pronunciation dictionaries). A good representation should capture phonetic content and abstract away from other types of variability, such as speaker differences and channel noise. Previous work in this area has primarily focused unsupervised learning from target language data only, and has been evaluated only intrinsically. Here we directly compare multiple methods, including some that use only target language speech data and some that use transcribed speech from other (non-target) languages, and we evaluate using two intrinsic measures as well as on a downstream unsupervised word segmentation and clustering task. We find that combining two existing target-language-only methods yields better features than either method alone. Nevertheless, even better results are obtained by extracting target language bottleneck features using a model trained on other languages. Cross-lingual training using just one other language is enough to provide this benefit, but multilingual training helps even more. In addition to these results, which hold across both intrinsic measures and the extrinsic task, we discuss the qualitative differences between the different types of learned features.","['https://openalex.org/W2785415724', 'https://openalex.org/W1778492285', 'https://openalex.org/W2057007397', 'https://openalex.org/W2020607164', 'https://openalex.org/W2395899413', 'https://openalex.org/W2803143571', 'https://openalex.org/W2059652594', 'https://openalex.org/W2035424729', 'https://openalex.org/W2038810952', 'https://openalex.org/W4288107125', 'https://openalex.org/W2296607128', 'https://openalex.org/W2826003142', 'https://openalex.org/W2044138293', 'https://openalex.org/W2511733680', 'https://openalex.org/W2079623482', 'https://openalex.org/W2509930204', 'https://openalex.org/W2786902352', 'https://openalex.org/W2052697931', 'https://openalex.org/W3125118953', 'https://openalex.org/W2787447541', 'https://openalex.org/W2787426069', 'https://openalex.org/W2127982613', 'https://openalex.org/W2022058071', 'https://openalex.org/W2786608204', 'https://openalex.org/W2746778230', 'https://openalex.org/W2407151108', 'https://openalex.org/W2468716020', 'https://openalex.org/W3098643042', 'https://openalex.org/W2791647162', 'https://openalex.org/W1967924372', 'https://openalex.org/W2780786457', 'https://openalex.org/W4300047444', 'https://openalex.org/W2516890051', 'https://openalex.org/W2940544976', 'https://openalex.org/W2291975472', 'https://openalex.org/W4289564011', 'https://openalex.org/W2396043527', 'https://openalex.org/W2345968833', 'https://openalex.org/W2002342963', 'https://openalex.org/W2122364000', 'https://openalex.org/W2533125211', 'https://openalex.org/W2117671523', 'https://openalex.org/W1545920196', 'https://openalex.org/W2126203737', 'https://openalex.org/W2024490156', 'https://openalex.org/W2048060899', 'https://openalex.org/W2399576818', 'https://openalex.org/W1970890968', 'https://openalex.org/W2084534958', 'https://openalex.org/W1796128977', 'https://openalex.org/W2402146185', 'https://openalex.org/W2239847623', 'https://openalex.org/W2295297373', 'https://openalex.org/W2170659185', 'https://openalex.org/W2963620343', 'https://openalex.org/W2963624290', 'https://openalex.org/W319941341', 'https://openalex.org/W1524333225', 'https://openalex.org/W2883755346', 'https://openalex.org/W2810166208', 'https://openalex.org/W2614542633']",
https://openalex.org/W2947591107,https://doi.org/10.48550/arxiv.1905.11449,VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for Zerospeech Challenge 2019,"We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.","['https://openalex.org/W2547039119', 'https://openalex.org/W2120847449', 'https://openalex.org/W2973026522', 'https://openalex.org/W2899771611', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963796886', 'https://openalex.org/W2101234009', 'https://openalex.org/W2786608204', 'https://openalex.org/W2593414223', 'https://openalex.org/W2963799213', 'https://openalex.org/W2962699523', 'https://openalex.org/W2949117887', 'https://openalex.org/W2191779130', 'https://openalex.org/W2949382160', 'https://openalex.org/W2099471712', 'https://openalex.org/W2787447541', 'https://openalex.org/W2399576818', 'https://openalex.org/W2964069186', 'https://openalex.org/W2963581463', 'https://openalex.org/W2911340057', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963620343', 'https://openalex.org/W2608338293', 'https://openalex.org/W2666408839', 'https://openalex.org/W2963971656']",2019-05-27
https://openalex.org/W4226199158,https://doi.org/10.48550/arxiv.2203.01829,A Brief Overview of Unsupervised Neural Speech Representation Learning,"Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.","['https://openalex.org/W3016181583', 'https://openalex.org/W2107789863', 'https://openalex.org/W2963137467', 'https://openalex.org/W2963925452', 'https://openalex.org/W1686810756', 'https://openalex.org/W3197259906', 'https://openalex.org/W3146777637', 'https://openalex.org/W4237840503', 'https://openalex.org/W2988736778', 'https://openalex.org/W3198782837', 'https://openalex.org/W3093427098', 'https://openalex.org/W4297808394', 'https://openalex.org/W3041561163', 'https://openalex.org/W2786608204', 'https://openalex.org/W2122538988', 'https://openalex.org/W1945356021', 'https://openalex.org/W2587284713', 'https://openalex.org/W2750248772', 'https://openalex.org/W2981991061', 'https://openalex.org/W3134881075', 'https://openalex.org/W3097777922', 'https://openalex.org/W3035202887', 'https://openalex.org/W2937090315', 'https://openalex.org/W44815768', 'https://openalex.org/W343636949', 'https://openalex.org/W2746710273', 'https://openalex.org/W2020607164', 'https://openalex.org/W3033038061', 'https://openalex.org/W2888911345', 'https://openalex.org/W3160345865', 'https://openalex.org/W2395899413', 'https://openalex.org/W3204915839', 'https://openalex.org/W4287374065', 'https://openalex.org/W2767754137', 'https://openalex.org/W1959608418', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963420272', 'https://openalex.org/W3209993061', 'https://openalex.org/W2896457183', 'https://openalex.org/W2973157397', 'https://openalex.org/W2396566817', 'https://openalex.org/W3209984917', 'https://openalex.org/W179875071', 'https://openalex.org/W2972943112', 'https://openalex.org/W3015265920', 'https://openalex.org/W2788991015', 'https://openalex.org/W2963317665', 'https://openalex.org/W3016011332', 'https://openalex.org/W2519091744', 'https://openalex.org/W2242818861', 'https://openalex.org/W3036622477', 'https://openalex.org/W3096656254', 'https://openalex.org/W1967924372', 'https://openalex.org/W138345131', 'https://openalex.org/W3112034174', 'https://openalex.org/W2889313720', 'https://openalex.org/W2108598243', 'https://openalex.org/W3198858531', 'https://openalex.org/W4286849919', 'https://openalex.org/W3162875390', 'https://openalex.org/W2059652594', 'https://openalex.org/W4226103796', 'https://openalex.org/W4214784181', 'https://openalex.org/W1545920196', 'https://openalex.org/W1909320841', 'https://openalex.org/W2018168021', 'https://openalex.org/W1796128977', 'https://openalex.org/W2146444479', 'https://openalex.org/W3102342027', 'https://openalex.org/W2548228487', 'https://openalex.org/W3160303932', 'https://openalex.org/W3160235762', 'https://openalex.org/W4287173589', 'https://openalex.org/W4288310870', 'https://openalex.org/W4295177495', 'https://openalex.org/W2347098582', 'https://openalex.org/W3034749675', 'https://openalex.org/W2787447541', 'https://openalex.org/W2327501763', 'https://openalex.org/W3125709657', 'https://openalex.org/W3161223924', 'https://openalex.org/W2758785877', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962799131', 'https://openalex.org/W4294170691', 'https://openalex.org/W2767224889', 'https://openalex.org/W3015949486', 'https://openalex.org/W3008499099', 'https://openalex.org/W2136922672', 'https://openalex.org/W2963720603', 'https://openalex.org/W3148040514', 'https://openalex.org/W2097012520', 'https://openalex.org/W2950151997', 'https://openalex.org/W3198134274', 'https://openalex.org/W3202710890', 'https://openalex.org/W3097286738', 'https://openalex.org/W4287887773', 'https://openalex.org/W3148001440', 'https://openalex.org/W4300047444', 'https://openalex.org/W2593011301', 'https://openalex.org/W4385245566', 'https://openalex.org/W592244745', 'https://openalex.org/W2963571336', 'https://openalex.org/W2076878942', 'https://openalex.org/W2936774411', 'https://openalex.org/W2097117768', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297786395', 'https://openalex.org/W4287824654', 'https://openalex.org/W3198429080', 'https://openalex.org/W4288574863', 'https://openalex.org/W2100768664', 'https://openalex.org/W2163922914', 'https://openalex.org/W3197580070', 'https://openalex.org/W2152790380', 'https://openalex.org/W2168013545', 'https://openalex.org/W2406349064', 'https://openalex.org/W3015213852', 'https://openalex.org/W2194775991', 'https://openalex.org/W3209059054', 'https://openalex.org/W3036601975', 'https://openalex.org/W2962850167', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963223306', 'https://openalex.org/W2982223350', 'https://openalex.org/W3203098807']",2022-03-01
https://openalex.org/W3049206033,https://doi.org/10.48550/arxiv.2008.06892,Unsupervised Acoustic Unit Representation Learning for Voice Conversion using WaveNet Auto-encoders,"Unsupervised representation learning of speech has been of keen interest in recent years, which is for example evident in the wide interest of the ZeroSpeech challenges. This work presents a new method for learning frame level representations based on WaveNet auto-encoders. Of particular interest in the ZeroSpeech Challenge 2019 were models with discrete latent variable such as the Vector Quantized Variational Auto-Encoder (VQVAE). However these models generate speech with relatively poor quality. In this work we aim to address this with two approaches: first WaveNet is used as the decoder and to generate waveform data directly from the latent representation; second, the low complexity of latent representations is improved with two alternative disentanglement learning methods, namely instance normalization and sliced vector quantization. The method was developed and tested in the context of the recent ZeroSpeech challenge 2020. The system output submitted to the challenge obtained the top position for naturalness (Mean Opinion Score 4.06), top position for intelligibility (Character Error Rate 0.15), and third position for the quality of the representation (ABX test score 12.5). These and further analysis in this paper illustrates that quality of the converted speech and the acoustic units representation can be well balanced.","['https://openalex.org/W2964243274', 'https://openalex.org/W1522301498', 'https://openalex.org/W2603777577', 'https://openalex.org/W2963618559', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963799213', 'https://openalex.org/W2346964103', 'https://openalex.org/W2972374322', 'https://openalex.org/W2005708641', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963830550', 'https://openalex.org/W2547039119', 'https://openalex.org/W2786608204', 'https://openalex.org/W2972659941', 'https://openalex.org/W2808697642', 'https://openalex.org/W2963609956', 'https://openalex.org/W2792995953', 'https://openalex.org/W2786902352', 'https://openalex.org/W2950414763', 'https://openalex.org/W2940544976', 'https://openalex.org/W2402146185', 'https://openalex.org/W2395899413', 'https://openalex.org/W2972943112', 'https://openalex.org/W2519091744', 'https://openalex.org/W2996383576', 'https://openalex.org/W2789543585', 'https://openalex.org/W2128032727', 'https://openalex.org/W2972841524', 'https://openalex.org/W2964026424', 'https://openalex.org/W2514741789', 'https://openalex.org/W2785860501', 'https://openalex.org/W1665214252', 'https://openalex.org/W2242818861', 'https://openalex.org/W2970971581', 'https://openalex.org/W2598638573', 'https://openalex.org/W2787447541', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972867623', 'https://openalex.org/W2502312327']",2020-08-16
https://openalex.org/W3007736951,https://doi.org/10.48550/arxiv.2002.11781,Towards Zero-shot Learning for Automatic Phonemic Transcription,"Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.","['https://openalex.org/W2972797781', 'https://openalex.org/W1980850109', 'https://openalex.org/W2572670101', 'https://openalex.org/W2963292011', 'https://openalex.org/W2787447541', 'https://openalex.org/W2116648050', 'https://openalex.org/W2408712009', 'https://openalex.org/W2791647162', 'https://openalex.org/W2633221078', 'https://openalex.org/W63916190', 'https://openalex.org/W2166637769', 'https://openalex.org/W2407897255', 'https://openalex.org/W2134270519', 'https://openalex.org/W2894690744', 'https://openalex.org/W2972581290', 'https://openalex.org/W2025482506', 'https://openalex.org/W2124033848', 'https://openalex.org/W2250357346', 'https://openalex.org/W2672120608', 'https://openalex.org/W2150295085', 'https://openalex.org/W2883972335', 'https://openalex.org/W2963211739', 'https://openalex.org/W2805993470', 'https://openalex.org/W2067392428', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127141656', 'https://openalex.org/W2787760419', 'https://openalex.org/W2550821151', 'https://openalex.org/W2286443923', 'https://openalex.org/W1992153276', 'https://openalex.org/W2963620343', 'https://openalex.org/W2476082370', 'https://openalex.org/W2025401819', 'https://openalex.org/W2397721308']",2020-02-26
https://openalex.org/W3026505300,https://doi.org/10.48550/arxiv.2005.09282,Bayesian Subspace HMM for the Zerospeech 2020 Challenge,"In this paper we describe our submission to the Zerospeech 2020 challenge, where the participants are required to discover latent representations from unannotated speech, and to use those representations to perform speech synthesis, with synthesis quality used as a proxy metric for the unit quality. In our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit discovery. The SHMM models each unit as an HMM whose parameters are constrained to lie in a low dimensional subspace of the total parameter space which is trained to model phonetic variability. Our system compares favorably with the baseline on the human-evaluated character error rate while maintaining significantly lower unit bitrate.","['https://openalex.org/W2972374322', 'https://openalex.org/W2641832364', 'https://openalex.org/W2962693497', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786608204', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972867623', 'https://openalex.org/W1959608418', 'https://openalex.org/W2125838338', 'https://openalex.org/W2972574141', 'https://openalex.org/W2127498532', 'https://openalex.org/W2963620343', 'https://openalex.org/W2973026522', 'https://openalex.org/W2400549570', 'https://openalex.org/W2598638573', 'https://openalex.org/W2547039119', 'https://openalex.org/W1545920196', 'https://openalex.org/W2963720603', 'https://openalex.org/W2468716020', 'https://openalex.org/W2347098582', 'https://openalex.org/W2973013862', 'https://openalex.org/W66627554', 'https://openalex.org/W2750248772', 'https://openalex.org/W2100768664', 'https://openalex.org/W2404799143', 'https://openalex.org/W2888911345']",2020-05-19
https://openalex.org/W3045592404,https://doi.org/10.48550/arxiv.2007.15074,Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages,"(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.","['https://openalex.org/W2962693497', 'https://openalex.org/W2963826681', 'https://openalex.org/W2345811097', 'https://openalex.org/W2120209245', 'https://openalex.org/W2963618559', 'https://openalex.org/W2026858810', 'https://openalex.org/W2963620343', 'https://openalex.org/W2402963799', 'https://openalex.org/W2396043527', 'https://openalex.org/W1975728937', 'https://openalex.org/W1599512239', 'https://openalex.org/W2102113734', 'https://openalex.org/W1967924372', 'https://openalex.org/W2024490156', 'https://openalex.org/W1833498382', 'https://openalex.org/W2124558353', 'https://openalex.org/W2115685217', 'https://openalex.org/W2962826786', 'https://openalex.org/W2513125788', 'https://openalex.org/W2112688413', 'https://openalex.org/W2786902352', 'https://openalex.org/W2399978376', 'https://openalex.org/W2888911345', 'https://openalex.org/W2972706021', 'https://openalex.org/W2293634267', 'https://openalex.org/W2964245029', 'https://openalex.org/W2025198378', 'https://openalex.org/W2072563337', 'https://openalex.org/W2404799143', 'https://openalex.org/W2895297209', 'https://openalex.org/W1957665339', 'https://openalex.org/W2641832364', 'https://openalex.org/W2963134917', 'https://openalex.org/W2787223168', 'https://openalex.org/W2962689740', 'https://openalex.org/W2162021827', 'https://openalex.org/W2586754519', 'https://openalex.org/W2399576818', 'https://openalex.org/W1545920196', 'https://openalex.org/W3125709657', 'https://openalex.org/W2401464865', 'https://openalex.org/W2110589736', 'https://openalex.org/W2187089797', 'https://openalex.org/W2048526313', 'https://openalex.org/W2509930204', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962799225', 'https://openalex.org/W1778492285', 'https://openalex.org/W2936120996', 'https://openalex.org/W2295297373', 'https://openalex.org/W2128032727', 'https://openalex.org/W2057007397', 'https://openalex.org/W1959608418', 'https://openalex.org/W2889228998', 'https://openalex.org/W1524333225', 'https://openalex.org/W2398071208', 'https://openalex.org/W2787426069', 'https://openalex.org/W2796339975', 'https://openalex.org/W1631260214', 'https://openalex.org/W2963381607', 'https://openalex.org/W2972812366', 'https://openalex.org/W2787447541', 'https://openalex.org/W2122272452', 'https://openalex.org/W2080972498', 'https://openalex.org/W1526516369', 'https://openalex.org/W2125838338', 'https://openalex.org/W1965842648', 'https://openalex.org/W2514012605', 'https://openalex.org/W2152051032', 'https://openalex.org/W2614542633', 'https://openalex.org/W3095049654', 'https://openalex.org/W2748598007', 'https://openalex.org/W2056786202', 'https://openalex.org/W308497914', 'https://openalex.org/W2403015869', 'https://openalex.org/W1950396994', 'https://openalex.org/W2002342963', 'https://openalex.org/W2160815625', 'https://openalex.org/W2785860501', 'https://openalex.org/W1984076147', 'https://openalex.org/W162588823', 'https://openalex.org/W2963137467', 'https://openalex.org/W2100768664', 'https://openalex.org/W2914746235', 'https://openalex.org/W2005708641', 'https://openalex.org/W2963684067', 'https://openalex.org/W2062914951', 'https://openalex.org/W2057653135', 'https://openalex.org/W2015688877', 'https://openalex.org/W3145738572', 'https://openalex.org/W2810166208', 'https://openalex.org/W2100969003', 'https://openalex.org/W2594951208', 'https://openalex.org/W2400549570', 'https://openalex.org/W2083904075', 'https://openalex.org/W2962684181', 'https://openalex.org/W2140567543', 'https://openalex.org/W2072349636', 'https://openalex.org/W1965555277', 'https://openalex.org/W2116422968', 'https://openalex.org/W3007486152', 'https://openalex.org/W2785415724', 'https://openalex.org/W2171752983', 'https://openalex.org/W2078769636', 'https://openalex.org/W2750248772', 'https://openalex.org/W2347098582', 'https://openalex.org/W2405703722', 'https://openalex.org/W185189022', 'https://openalex.org/W1528778941', 'https://openalex.org/W2168319451', 'https://openalex.org/W2766219058', 'https://openalex.org/W1796128977', 'https://openalex.org/W3105242324', 'https://openalex.org/W2106554350', 'https://openalex.org/W2518312472', 'https://openalex.org/W2121997342', 'https://openalex.org/W2972574141', 'https://openalex.org/W2786608204', 'https://openalex.org/W1578200545', 'https://openalex.org/W2127498532', 'https://openalex.org/W2826003142']",2020-07-29
https://openalex.org/W3110585608,https://doi.org/10.48550/arxiv.2011.14060,Unsupervised Spoken Term Discovery on Untranscribed Speech,"(Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived ""phonemes"". The audio are labelled with these ""phonemes"" to obtain ""phoneme"" sequences. Unsupervised pattern discovery searches for repetitive patterns in the ""phoneme"" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.","['https://openalex.org/W3021051756', 'https://openalex.org/W1778492285', 'https://openalex.org/W2746778230', 'https://openalex.org/W3105242324', 'https://openalex.org/W2129634963', 'https://openalex.org/W1821462560', 'https://openalex.org/W1501500081', 'https://openalex.org/W1555148682', 'https://openalex.org/W1128809682', 'https://openalex.org/W2964169922', 'https://openalex.org/W2142416747', 'https://openalex.org/W2468716020', 'https://openalex.org/W2103359087', 'https://openalex.org/W2950577311', 'https://openalex.org/W2171752983', 'https://openalex.org/W1604771987', 'https://openalex.org/W2126377586', 'https://openalex.org/W2165712214', 'https://openalex.org/W2293634267', 'https://openalex.org/W2573062194', 'https://openalex.org/W1978660892', 'https://openalex.org/W2127141656', 'https://openalex.org/W2072563337', 'https://openalex.org/W2122538988', 'https://openalex.org/W2406504561', 'https://openalex.org/W2594951208', 'https://openalex.org/W2090861223', 'https://openalex.org/W2118841860', 'https://openalex.org/W1538131130', 'https://openalex.org/W1981706894', 'https://openalex.org/W2147768505', 'https://openalex.org/W1970890968', 'https://openalex.org/W1995562189', 'https://openalex.org/W1975728937', 'https://openalex.org/W2172097686', 'https://openalex.org/W2115870554', 'https://openalex.org/W2962826786', 'https://openalex.org/W2110087182', 'https://openalex.org/W2087064593', 'https://openalex.org/W2044048901', 'https://openalex.org/W2913066018', 'https://openalex.org/W1525783482', 'https://openalex.org/W2098044214', 'https://openalex.org/W3144750446', 'https://openalex.org/W2111732304', 'https://openalex.org/W1998871699', 'https://openalex.org/W2752630748', 'https://openalex.org/W3023071679', 'https://openalex.org/W2148154194', 'https://openalex.org/W2127218421', 'https://openalex.org/W2114347655', 'https://openalex.org/W2087863531', 'https://openalex.org/W2604132379', 'https://openalex.org/W2161482971', 'https://openalex.org/W1902405276', 'https://openalex.org/W2166782149', 'https://openalex.org/W2057007397', 'https://openalex.org/W2526425061', 'https://openalex.org/W2124558353', 'https://openalex.org/W2116422968', 'https://openalex.org/W2121997342', 'https://openalex.org/W1507597442', 'https://openalex.org/W2110589736', 'https://openalex.org/W2184045248', 'https://openalex.org/W2787447541', 'https://openalex.org/W2023812146', 'https://openalex.org/W2786902352', 'https://openalex.org/W2078769636', 'https://openalex.org/W2901751252', 'https://openalex.org/W2963620343', 'https://openalex.org/W2083904075', 'https://openalex.org/W2090958120', 'https://openalex.org/W1957665339', 'https://openalex.org/W2399869768', 'https://openalex.org/W2060895888', 'https://openalex.org/W151377110', 'https://openalex.org/W2024490156', 'https://openalex.org/W2064675550', 'https://openalex.org/W1600310449', 'https://openalex.org/W2143612262', 'https://openalex.org/W2159948109', 'https://openalex.org/W2680270903', 'https://openalex.org/W2025198378', 'https://openalex.org/W2115008841', 'https://openalex.org/W2962799225', 'https://openalex.org/W2130942839', 'https://openalex.org/W2402196427', 'https://openalex.org/W2187089797', 'https://openalex.org/W3127686677', 'https://openalex.org/W1902237438', 'https://openalex.org/W2102113734', 'https://openalex.org/W2103869314', 'https://openalex.org/W854541894', 'https://openalex.org/W2173629880', 'https://openalex.org/W2043513994', 'https://openalex.org/W2403015869', 'https://openalex.org/W1815792', 'https://openalex.org/W162588823', 'https://openalex.org/W2402146185', 'https://openalex.org/W2100768664', 'https://openalex.org/W2806921177', 'https://openalex.org/W2950919476', 'https://openalex.org/W1673310716', 'https://openalex.org/W3095049654']",2020-11-28
https://openalex.org/W3158457675,https://doi.org/10.48550/arxiv.2105.01786,Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery,"Discovering speaker independent acoustic units purely from spoken input is known to be a hard problem. In this work we propose an unsupervised speaker normalization technique prior to unit discovery. It is based on separating speaker related from content induced variations in a speech signal with an adversarial contrastive predictive coding approach. This technique does neither require transcribed speech nor speaker labels, and, furthermore, can be trained in a multilingual fashion, thus achieving speaker normalization even if only few unlabeled data is available from the target language. The speaker normalization is done by mapping all utterances to a medoid style which is representative for the whole database. We demonstrate the effectiveness of the approach by conducting acoustic unit discovery with a hidden Markov model variational autoencoder noting, however, that the proposed speaker normalization can serve as a front end to any unit discovery system. Experiments on English, Yoruba and Mboshi show improvements compared to using non-normalized input.","['https://openalex.org/W2962695963', 'https://openalex.org/W2753738274', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963799213', 'https://openalex.org/W1967924372', 'https://openalex.org/W1494198834', 'https://openalex.org/W2106119541', 'https://openalex.org/W2940544976', 'https://openalex.org/W2951004968', 'https://openalex.org/W2762715843', 'https://openalex.org/W3127686677', 'https://openalex.org/W2750248772', 'https://openalex.org/W2963620343', 'https://openalex.org/W2142384583', 'https://openalex.org/W2888911345', 'https://openalex.org/W2933771351', 'https://openalex.org/W2963618559', 'https://openalex.org/W2157590573', 'https://openalex.org/W1522301498', 'https://openalex.org/W2842511635', 'https://openalex.org/W3030987249', 'https://openalex.org/W2483390977', 'https://openalex.org/W2787447541', 'https://openalex.org/W2025482506', 'https://openalex.org/W2950414763', 'https://openalex.org/W3092791109', 'https://openalex.org/W3101380508', 'https://openalex.org/W2022058071', 'https://openalex.org/W3100202343', 'https://openalex.org/W3162390194', 'https://openalex.org/W2140567543']",2021-05-04
https://openalex.org/W4214942696,https://doi.org/10.1109/ieeeconf53345.2021.9723318,The effectiveness of self-supervised representation learning in zero-resource subword modeling,"For a language with no transcribed speech available (the zero-resource scenario), conventional acoustic modeling algorithms are not applicable. Recently, zero-resource acoustic modeling has gained much interest. One research problem is unsupervised subword modeling (USM), i.e., learning a feature representation that can distinguish subword units and is robust to speaker variation. Previous studies showed that self-supervised learning (SSL) has the potential to separate speaker and phonetic information in speech in an unsupervised manner, which is highly desired in USM. This paper compares two representative SSL algorithms, namely, contrastive predictive coding (CPC) and autoregressive predictive coding (APC), as a front-end method of a recently proposed, state-of-the art two-stage approach, to learn a representation as input to a back-end cross-lingual DNN. Experiments show that the bottleneck features extracted by the back-end achieved state of the art in a subword ABX task on the Libri-light and ZeroSpeech databases. In general, CPC is more effective than APC as the front-end in our approach, which is independent of the choice of the out-domain language identity in the back-end cross-lingual DNN and the training data amount. With very limited training data, APC is found similar or more effective than CPC when test data consists of long utterances.","['https://openalex.org/W2787447541', 'https://openalex.org/W3016181583', 'https://openalex.org/W2971041032', 'https://openalex.org/W2752796333', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W6748325621', 'https://openalex.org/W2889228998', 'https://openalex.org/W3112085130', 'https://openalex.org/W3040997499', 'https://openalex.org/W3148101939', 'https://openalex.org/W3044483536', 'https://openalex.org/W2514741789', 'https://openalex.org/W2786608204', 'https://openalex.org/W3196459653', 'https://openalex.org/W2347098582', 'https://openalex.org/W6697293080', 'https://openalex.org/W3100270690', 'https://openalex.org/W6675022971', 'https://openalex.org/W2399576818', 'https://openalex.org/W3005578234', 'https://openalex.org/W3150635893', 'https://openalex.org/W6601192135', 'https://openalex.org/W2995181338', 'https://openalex.org/W6631362777', 'https://openalex.org/W6631190155', 'https://openalex.org/W1522301498', 'https://openalex.org/W3160200310', 'https://openalex.org/W2100768664', 'https://openalex.org/W2296607128', 'https://openalex.org/W29952999', 'https://openalex.org/W2787426069', 'https://openalex.org/W1524333225', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963799213']",2021-10-31
https://openalex.org/W4386597471,https://doi.org/10.1109/frse58934.2023.00052,MIPAD: Mini Program Analysis for Clone Detection using Static Analysis Techniques,"In recent years, third-party platform-mounted applications, referred to as mini programs, such as health QR codes, transport codes, and utilities, have been gradually replacing traditional mobile applications due to their no-installation-uninstallation and use-it-and-go feature. However, the massive growth of mini programs has led to concerns about protecting the copyright of their code. Currently, there is not enough research on clone detection for mini programs, and the language features of mini programs make it difficult to detect plagiarism due to incomplete behaviour observation and challenges in calculating similarity. To address this gap, we propose MIPAD, a detection method based on static feature analysis, including statistical features (SF) for clustering analysis, layout features (LF), and code features (CFF, FDF, TLDF) for similarity detection. To enhance the robustness of the LF and CFF, FDF, TLDF features during the feature extraction phase, we used a fuzzy hash algorithm. To speed up the dependency graph similarity computation, we propose a fast anchor-based similarity computation algorithm. To address the lack of publicly available large sample datasets in this domain, we designed a mini program crawler method that can fuzzy crawl samples based on a seed list and expand the list in real-time, and we used this method to crawl 100,000-level mini program samples. Using these samples, we evaluated MIPAD using a Random Forest as a classifier and X-means as a clusterizer, which showed an accuracy of 90.5% and an average sample time overhead of 15. 83s, demonstrating that MIPAD can detect cloned mini programs quickly and effectively.","['https://openalex.org/W1964940342', 'https://openalex.org/W2911964244', 'https://openalex.org/W1977177161', 'https://openalex.org/W2963213304', 'https://openalex.org/W2128698639', 'https://openalex.org/W2157532207', 'https://openalex.org/W3142518606', 'https://openalex.org/W2741705590', 'https://openalex.org/W2128782367', 'https://openalex.org/W2138756793', 'https://openalex.org/W2767717989', 'https://openalex.org/W2104301886', 'https://openalex.org/W6681648988', 'https://openalex.org/W2111295912', 'https://openalex.org/W2088479623', 'https://openalex.org/W6635035540', 'https://openalex.org/W6681029592', 'https://openalex.org/W2787447541', 'https://openalex.org/W2002083372', 'https://openalex.org/W2146659255', 'https://openalex.org/W1585610988', 'https://openalex.org/W2142498761']",2023-06-01
https://openalex.org/W4388017359,https://doi.org/10.1109/taslp.2023.3328283,End-to-End Speech Recognition: A Survey,"In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">end-to-end</i> (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.","['https://openalex.org/W6715097780', 'https://openalex.org/W2125838338', 'https://openalex.org/W2394932179', 'https://openalex.org/W98857008', 'https://openalex.org/W2165712214', 'https://openalex.org/W1588735863', 'https://openalex.org/W6680532216', 'https://openalex.org/W2056590938', 'https://openalex.org/W2408093180', 'https://openalex.org/W2288217446', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W6623517193', 'https://openalex.org/W2327501763', 'https://openalex.org/W2143564602', 'https://openalex.org/W6683738474', 'https://openalex.org/W6675365184', 'https://openalex.org/W2889129739', 'https://openalex.org/W2962760690', 'https://openalex.org/W2129545859', 'https://openalex.org/W2100180150', 'https://openalex.org/W6780218876', 'https://openalex.org/W3008762051', 'https://openalex.org/W2962699523', 'https://openalex.org/W2972889948', 'https://openalex.org/W2545177271', 'https://openalex.org/W2962784628', 'https://openalex.org/W6728811460', 'https://openalex.org/W6734588641', 'https://openalex.org/W2899879954', 'https://openalex.org/W3198455051', 'https://openalex.org/W2121879602', 'https://openalex.org/W2046932483', 'https://openalex.org/W2889187401', 'https://openalex.org/W3197991202', 'https://openalex.org/W2750499125', 'https://openalex.org/W2064675550', 'https://openalex.org/W2105482032', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W2746192915', 'https://openalex.org/W3016234571', 'https://openalex.org/W2143612262', 'https://openalex.org/W3211040052', 'https://openalex.org/W6690026940', 'https://openalex.org/W3008174054', 'https://openalex.org/W3028545098', 'https://openalex.org/W2962826786', 'https://openalex.org/W6727690538', 'https://openalex.org/W2889504751', 'https://openalex.org/W2915977493', 'https://openalex.org/W2745439869', 'https://openalex.org/W3008898571', 'https://openalex.org/W6685711979', 'https://openalex.org/W6735706088', 'https://openalex.org/W6747158283', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962742956', 'https://openalex.org/W2936123380', 'https://openalex.org/W2972995428', 'https://openalex.org/W6793472422', 'https://openalex.org/W4319862683', 'https://openalex.org/W3015671919', 'https://openalex.org/W2514741789', 'https://openalex.org/W6727336983', 'https://openalex.org/W66978610', 'https://openalex.org/W2748816379', 'https://openalex.org/W4319862408', 'https://openalex.org/W2963211739', 'https://openalex.org/W6640090968', 'https://openalex.org/W2608712415', 'https://openalex.org/W2577366047', 'https://openalex.org/W2530876040', 'https://openalex.org/W2963303028', 'https://openalex.org/W2964012862', 'https://openalex.org/W6754576867', 'https://openalex.org/W2889163603', 'https://openalex.org/W3008525923', 'https://openalex.org/W2131968858', 'https://openalex.org/W2606722458', 'https://openalex.org/W2766219058', 'https://openalex.org/W2973122799', 'https://openalex.org/W3011339933', 'https://openalex.org/W3163203022', 'https://openalex.org/W6784400248', 'https://openalex.org/W6784800133', 'https://openalex.org/W2972625221', 'https://openalex.org/W2886319145', 'https://openalex.org/W2886025712', 'https://openalex.org/W2937402758', 'https://openalex.org/W2889374926', 'https://openalex.org/W3095173472', 'https://openalex.org/W2892009249', 'https://openalex.org/W3016010032', 'https://openalex.org/W6839026989', 'https://openalex.org/W2972818416', 'https://openalex.org/W3163793923', 'https://openalex.org/W3197976839', 'https://openalex.org/W3015686596', 'https://openalex.org/W3160551958', 'https://openalex.org/W3161375121', 'https://openalex.org/W3202419788', 'https://openalex.org/W4319862418', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W3008284571', 'https://openalex.org/W3015927303', 'https://openalex.org/W3015383801', 'https://openalex.org/W3198116002', 'https://openalex.org/W1806891645', 'https://openalex.org/W6910546390', 'https://openalex.org/W2105594594', 'https://openalex.org/W6679128827', 'https://openalex.org/W1991133427', 'https://openalex.org/W2155368638', 'https://openalex.org/W4224518768', 'https://openalex.org/W6796656850', 'https://openalex.org/W3202184514', 'https://openalex.org/W2024539680', 'https://openalex.org/W3198439131', 'https://openalex.org/W3206876927', 'https://openalex.org/W4372259859', 'https://openalex.org/W2033565080', 'https://openalex.org/W2157749010', 'https://openalex.org/W1979136262', 'https://openalex.org/W2114016253', 'https://openalex.org/W2001679125', 'https://openalex.org/W2953561564', 'https://openalex.org/W2963747784', 'https://openalex.org/W3095697114', 'https://openalex.org/W2900209846', 'https://openalex.org/W2963144852', 'https://openalex.org/W2892124901', 'https://openalex.org/W3095376166', 'https://openalex.org/W2136922672', 'https://openalex.org/W2110798204', 'https://openalex.org/W2471933213', 'https://openalex.org/W2799923439', 'https://openalex.org/W2799800213', 'https://openalex.org/W6757856092', 'https://openalex.org/W2963571336', 'https://openalex.org/W2951974815', 'https://openalex.org/W1501286448', 'https://openalex.org/W1975550806', 'https://openalex.org/W6752630080', 'https://openalex.org/W2963431393', 'https://openalex.org/W3096215352', 'https://openalex.org/W3096032230', 'https://openalex.org/W4210463634', 'https://openalex.org/W3204696009', 'https://openalex.org/W4226120743', 'https://openalex.org/W2033245860', 'https://openalex.org/W1587755118', 'https://openalex.org/W2000200144', 'https://openalex.org/W6757817989', 'https://openalex.org/W6745410505', 'https://openalex.org/W2963026768', 'https://openalex.org/W6863618527', 'https://openalex.org/W3163842339', 'https://openalex.org/W2151834591', 'https://openalex.org/W2296073425', 'https://openalex.org/W6687566353', 'https://openalex.org/W3097747488', 'https://openalex.org/W3017474798', 'https://openalex.org/W1988720110', 'https://openalex.org/W6604254268', 'https://openalex.org/W6631190155', 'https://openalex.org/W3198654230', 'https://openalex.org/W4206410067', 'https://openalex.org/W6681151457', 'https://openalex.org/W2145249131', 'https://openalex.org/W6692956712', 'https://openalex.org/W6640036494', 'https://openalex.org/W2618530766', 'https://openalex.org/W2331143823', 'https://openalex.org/W2972451902', 'https://openalex.org/W3162249256', 'https://openalex.org/W6600213771', 'https://openalex.org/W6714142977', 'https://openalex.org/W2183341477', 'https://openalex.org/W6621543089', 'https://openalex.org/W6749075489', 'https://openalex.org/W2057653135', 'https://openalex.org/W6749518548', 'https://openalex.org/W3163839574', 'https://openalex.org/W1989674786', 'https://openalex.org/W2407080277', 'https://openalex.org/W6675409298', 'https://openalex.org/W2937780860', 'https://openalex.org/W3015995734', 'https://openalex.org/W3095189764', 'https://openalex.org/W2883586237', 'https://openalex.org/W4225334634', 'https://openalex.org/W2964107261', 'https://openalex.org/W1583239513', 'https://openalex.org/W2050526637', 'https://openalex.org/W1986184096', 'https://openalex.org/W2516457973', 'https://openalex.org/W2808939837', 'https://openalex.org/W2127095586', 'https://openalex.org/W2914018192', 'https://openalex.org/W2888909726', 'https://openalex.org/W206545267', 'https://openalex.org/W4473315', 'https://openalex.org/W6793076252', 'https://openalex.org/W6785891320', 'https://openalex.org/W2944255943', 'https://openalex.org/W3026041220', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015726069', 'https://openalex.org/W2291975472', 'https://openalex.org/W2971840980', 'https://openalex.org/W108866686', 'https://openalex.org/W3097882114', 'https://openalex.org/W6774835902', 'https://openalex.org/W3096160024', 'https://openalex.org/W3016167541', 'https://openalex.org/W3197140813', 'https://openalex.org/W3206573929', 'https://openalex.org/W3094713728', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963260202', 'https://openalex.org/W2886180730', 'https://openalex.org/W6854240803', 'https://openalex.org/W4299649720', 'https://openalex.org/W2151058131', 'https://openalex.org/W2008554732', 'https://openalex.org/W2066378046', 'https://openalex.org/W6765658108', 'https://openalex.org/W3015190365', 'https://openalex.org/W1710082047', 'https://openalex.org/W3015974384', 'https://openalex.org/W6770245836', 'https://openalex.org/W3097973766', 'https://openalex.org/W3197304116', 'https://openalex.org/W2972977747', 'https://openalex.org/W3148654612', 'https://openalex.org/W2963022149', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963240019', 'https://openalex.org/W3163300396', 'https://openalex.org/W2972780808', 'https://openalex.org/W2787663903', 'https://openalex.org/W3008912312', 'https://openalex.org/W2972837679', 'https://openalex.org/W2938348542', 'https://openalex.org/W3197478142', 'https://openalex.org/W6797037654', 'https://openalex.org/W2972953886', 'https://openalex.org/W3005302685', 'https://openalex.org/W6713134421', 'https://openalex.org/W2933138175', 'https://openalex.org/W6760633627', 'https://openalex.org/W2962728618', 'https://openalex.org/W2972630480', 'https://openalex.org/W2949975180', 'https://openalex.org/W3015369343', 'https://openalex.org/W2091981305', 'https://openalex.org/W2972528057', 'https://openalex.org/W3016053754', 'https://openalex.org/W2964103964', 'https://openalex.org/W2963382396', 'https://openalex.org/W2970692082', 'https://openalex.org/W2078354939', 'https://openalex.org/W2136617108', 'https://openalex.org/W3015501067', 'https://openalex.org/W2972799770', 'https://openalex.org/W2972621414', 'https://openalex.org/W6752334204', 'https://openalex.org/W2097927681', 'https://openalex.org/W179875071', 'https://openalex.org/W2402268235', 'https://openalex.org/W2566563465', 'https://openalex.org/W6731370813', 'https://openalex.org/W6757424787', 'https://openalex.org/W2940180244', 'https://openalex.org/W2963088785', 'https://openalex.org/W2943845043', 'https://openalex.org/W2964110616', 'https://openalex.org/W2150355110', 'https://openalex.org/W6770251742', 'https://openalex.org/W6640059789', 'https://openalex.org/W2888779557', 'https://openalex.org/W2939111082', 'https://openalex.org/W3008037978', 'https://openalex.org/W3152221657', 'https://openalex.org/W3205201903', 'https://openalex.org/W6755207826', 'https://openalex.org/W3024308166', 'https://openalex.org/W2962745521', 'https://openalex.org/W3197507772', 'https://openalex.org/W1966812932', 'https://openalex.org/W2014151772', 'https://openalex.org/W2080213370', 'https://openalex.org/W1985258458', 'https://openalex.org/W2396464458', 'https://openalex.org/W2166637769', 'https://openalex.org/W1494198834', 'https://openalex.org/W3008191852', 'https://openalex.org/W3209059054', 'https://openalex.org/W6770506093', 'https://openalex.org/W3147414526', 'https://openalex.org/W2995181338', 'https://openalex.org/W2981857663', 'https://openalex.org/W4319862255', 'https://openalex.org/W2972692349', 'https://openalex.org/W2962824709', 'https://openalex.org/W3007528493', 'https://openalex.org/W3094667432', 'https://openalex.org/W3048407879', 'https://openalex.org/W3162665866', 'https://openalex.org/W3161873870', 'https://openalex.org/W3015194534', 'https://openalex.org/W3160766462', 'https://openalex.org/W3198442913', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W3148001440', 'https://openalex.org/W3205644108', 'https://openalex.org/W4225319488', 'https://openalex.org/W4319862474', 'https://openalex.org/W2963739817', 'https://openalex.org/W6735168207', 'https://openalex.org/W3211278025', 'https://openalex.org/W4221155340', 'https://openalex.org/W2079656678', 'https://openalex.org/W2952230511', 'https://openalex.org/W3034775979', 'https://openalex.org/W4394662461', 'https://openalex.org/W2792376130', 'https://openalex.org/W4297781872', 'https://openalex.org/W1922655562', 'https://openalex.org/W3207222250', 'https://openalex.org/W3147187328', 'https://openalex.org/W3007328579', 'https://openalex.org/W3151269043', 'https://openalex.org/W1508165687', 'https://openalex.org/W1904365287', 'https://openalex.org/W3170405627', 'https://openalex.org/W4381827575', 'https://openalex.org/W4378501656', 'https://openalex.org/W4383605108', 'https://openalex.org/W3167895882', 'https://openalex.org/W2987019345', 'https://openalex.org/W3100910367', 'https://openalex.org/W2525778437', 'https://openalex.org/W1915251500', 'https://openalex.org/W2242818861', 'https://openalex.org/W2411921399', 'https://openalex.org/W2808640845', 'https://openalex.org/W2520160253', 'https://openalex.org/W3092122846', 'https://openalex.org/W3105532142', 'https://openalex.org/W2928941594', 'https://openalex.org/W4288290348', 'https://openalex.org/W1553004968', 'https://openalex.org/W2904818793', 'https://openalex.org/W3094957294', 'https://openalex.org/W3103005696']",2023-10-30
https://openalex.org/W4391021666,https://doi.org/10.1109/asru57964.2023.10389705,On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration,"Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The ""decoder-only"" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.","['https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W6850625674', 'https://openalex.org/W6739901393', 'https://openalex.org/W6851847159', 'https://openalex.org/W6851513886', 'https://openalex.org/W6852326057', 'https://openalex.org/W6852818750', 'https://openalex.org/W3211278025', 'https://openalex.org/W6851950068', 'https://openalex.org/W6851592950', 'https://openalex.org/W6852489829', 'https://openalex.org/W6810334672', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6851317108', 'https://openalex.org/W3153583341', 'https://openalex.org/W6796581206', 'https://openalex.org/W2901607128', 'https://openalex.org/W3034625919', 'https://openalex.org/W6846175339', 'https://openalex.org/W4224137820', 'https://openalex.org/W4385823495', 'https://openalex.org/W6780680273', 'https://openalex.org/W2963532001', 'https://openalex.org/W6769627184', 'https://openalex.org/W6732953234', 'https://openalex.org/W2605131327', 'https://openalex.org/W6847363464', 'https://openalex.org/W2739883972', 'https://openalex.org/W6773820404', 'https://openalex.org/W4388979610', 'https://openalex.org/W6757817989', 'https://openalex.org/W2963250244', 'https://openalex.org/W6854218657', 'https://openalex.org/W4366330503', 'https://openalex.org/W4393178509', 'https://openalex.org/W3168867926', 'https://openalex.org/W4391021781', 'https://openalex.org/W4361866031', 'https://openalex.org/W4323651091', 'https://openalex.org/W4292779060', 'https://openalex.org/W4381827575', 'https://openalex.org/W4301581299', 'https://openalex.org/W4375958083', 'https://openalex.org/W4322718191', 'https://openalex.org/W4288089799', 'https://openalex.org/W2908510526', 'https://openalex.org/W4385245566', 'https://openalex.org/W4367628410', 'https://openalex.org/W4391021457', 'https://openalex.org/W4377372369', 'https://openalex.org/W4224308101', 'https://openalex.org/W4313679638', 'https://openalex.org/W4225323055', 'https://openalex.org/W4366850747', 'https://openalex.org/W4364382977', 'https://openalex.org/W3043665049', 'https://openalex.org/W4378501656']",2023-12-16
https://openalex.org/W4406417959,https://doi.org/10.1109/taslpro.2025.3530270,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,,"['https://openalex.org/W2964243274', 'https://openalex.org/W6763832098', 'https://openalex.org/W2903739847', 'https://openalex.org/W4297841605', 'https://openalex.org/W6796464841', 'https://openalex.org/W6755135894', 'https://openalex.org/W3095035471', 'https://openalex.org/W6748588790', 'https://openalex.org/W6805710207', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W4307323391', 'https://openalex.org/W4392903704', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W6727697161', 'https://openalex.org/W6790220310', 'https://openalex.org/W3213544594', 'https://openalex.org/W6752888775', 'https://openalex.org/W2963371159', 'https://openalex.org/W6796730497', 'https://openalex.org/W4225746985', 'https://openalex.org/W6795261426', 'https://openalex.org/W6849953009', 'https://openalex.org/W4372259784', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6790356757', 'https://openalex.org/W4381786045', 'https://openalex.org/W3140429000', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W6753797277', 'https://openalex.org/W3205644108', 'https://openalex.org/W2972374322', 'https://openalex.org/W6810189000', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W4390075359', 'https://openalex.org/W4402671568', 'https://openalex.org/W6852870047', 'https://openalex.org/W6854199242', 'https://openalex.org/W6861001475', 'https://openalex.org/W6864145738', 'https://openalex.org/W6856434366', 'https://openalex.org/W6861342446', 'https://openalex.org/W6853188576', 'https://openalex.org/W4313021454', 'https://openalex.org/W6853937136', 'https://openalex.org/W4393147067', 'https://openalex.org/W6851724922', 'https://openalex.org/W6862144568', 'https://openalex.org/W6779823529', 'https://openalex.org/W4252812408', 'https://openalex.org/W6859583170', 'https://openalex.org/W6846539466', 'https://openalex.org/W4406417959', 'https://openalex.org/W6853165267', 'https://openalex.org/W6855885476', 'https://openalex.org/W4372270198', 'https://openalex.org/W6852581948', 'https://openalex.org/W6853515095', 'https://openalex.org/W4392903389', 'https://openalex.org/W4402672068', 'https://openalex.org/W4402670057', 'https://openalex.org/W6852381208', 'https://openalex.org/W6856126247', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W6852800892', 'https://openalex.org/W2519091744', 'https://openalex.org/W6769196770', 'https://openalex.org/W4226132755', 'https://openalex.org/W3173715137', 'https://openalex.org/W2963300588', 'https://openalex.org/W3161480375', 'https://openalex.org/W3025844872', 'https://openalex.org/W3146550708', 'https://openalex.org/W6753049143', 'https://openalex.org/W4381827575', 'https://openalex.org/W4394671563', 'https://openalex.org/W4394007232', 'https://openalex.org/W4323651091', 'https://openalex.org/W4390306858', 'https://openalex.org/W2527729766', 'https://openalex.org/W4313679638', 'https://openalex.org/W4387323811', 'https://openalex.org/W4372279529', 'https://openalex.org/W4384615685', 'https://openalex.org/W4391833199', 'https://openalex.org/W4379540238', 'https://openalex.org/W4386384714', 'https://openalex.org/W4224308101', 'https://openalex.org/W4377010126', 'https://openalex.org/W4379924545', 'https://openalex.org/W4400111385', 'https://openalex.org/W2965373594', 'https://openalex.org/W4390962167', 'https://openalex.org/W4378501656', 'https://openalex.org/W4303647933', 'https://openalex.org/W4379259581', 'https://openalex.org/W2810914326', 'https://openalex.org/W3174758275']",2025-01-01
https://openalex.org/W4392903389,https://doi.org/10.1109/icassp48485.2024.10447523,"FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec","This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.","['https://openalex.org/W6639363673', 'https://openalex.org/W1481955708', 'https://openalex.org/W6767111847', 'https://openalex.org/W3215615641', 'https://openalex.org/W6783867762', 'https://openalex.org/W4307323391', 'https://openalex.org/W3095095816', 'https://openalex.org/W3163243746', 'https://openalex.org/W2064675550', 'https://openalex.org/W4385245566', 'https://openalex.org/W4372190822', 'https://openalex.org/W4375869380', 'https://openalex.org/W4372348514', 'https://openalex.org/W4372260101', 'https://openalex.org/W4375869436', 'https://openalex.org/W6848735303', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W4372270198', 'https://openalex.org/W6853515095', 'https://openalex.org/W2752796333', 'https://openalex.org/W4221159457', 'https://openalex.org/W2798405286', 'https://openalex.org/W6631362777', 'https://openalex.org/W3209059054', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963242190', 'https://openalex.org/W6754473786', 'https://openalex.org/W3203407300', 'https://openalex.org/W3198694222', 'https://openalex.org/W1728888090', 'https://openalex.org/W2889048668', 'https://openalex.org/W4313679638', 'https://openalex.org/W4205788663', 'https://openalex.org/W2963799213', 'https://openalex.org/W2970006822', 'https://openalex.org/W1524333225', 'https://openalex.org/W4378501656', 'https://openalex.org/W4381827575', 'https://openalex.org/W3092028330', 'https://openalex.org/W4380551955']",2024-03-18
https://openalex.org/W4393236964,https://doi.org/10.1016/j.enbenv.2024.03.010,"Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry","Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.","['https://openalex.org/W3168442825', 'https://openalex.org/W4283388850', 'https://openalex.org/W3092537909', 'https://openalex.org/W4360618334', 'https://openalex.org/W6857939860', 'https://openalex.org/W6793792376', 'https://openalex.org/W4385566703', 'https://openalex.org/W1976960445', 'https://openalex.org/W2804844200', 'https://openalex.org/W2079074068', 'https://openalex.org/W2917212430', 'https://openalex.org/W6854039168', 'https://openalex.org/W6810608687', 'https://openalex.org/W2587347436', 'https://openalex.org/W1776296525', 'https://openalex.org/W1973944436', 'https://openalex.org/W3132263092', 'https://openalex.org/W4283770334', 'https://openalex.org/W4380987486', 'https://openalex.org/W4390498569', 'https://openalex.org/W4316012702', 'https://openalex.org/W6795217850', 'https://openalex.org/W6777023006', 'https://openalex.org/W3153794886', 'https://openalex.org/W4388248488', 'https://openalex.org/W4363671827', 'https://openalex.org/W4322718191', 'https://openalex.org/W4223442835', 'https://openalex.org/W3162029264', 'https://openalex.org/W4377865110', 'https://openalex.org/W4362693613', 'https://openalex.org/W4381487652', 'https://openalex.org/W4378501656', 'https://openalex.org/W4360891289']",2024-03-27
https://openalex.org/W4392904805,https://doi.org/10.1109/icassp48485.2024.10447112,"VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks","We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.","['https://openalex.org/W6778883912', 'https://openalex.org/W6790356757', 'https://openalex.org/W4381786045', 'https://openalex.org/W6777028661', 'https://openalex.org/W6848735303', 'https://openalex.org/W4388017359', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6802465204', 'https://openalex.org/W4226120743', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6803092890', 'https://openalex.org/W6852381208', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6811340617', 'https://openalex.org/W6852781825', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492', 'https://openalex.org/W4385822683', 'https://openalex.org/W6783867762', 'https://openalex.org/W2890964092', 'https://openalex.org/W6786696081', 'https://openalex.org/W2972394484', 'https://openalex.org/W3202278141', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W2972359262', 'https://openalex.org/W2998572311', 'https://openalex.org/W6631190155', 'https://openalex.org/W6796464841', 'https://openalex.org/W4319862255', 'https://openalex.org/W4313679638', 'https://openalex.org/W2899575547', 'https://openalex.org/W3024605872', 'https://openalex.org/W4378501656', 'https://openalex.org/W3207222250', 'https://openalex.org/W4379540238', 'https://openalex.org/W4394671563', 'https://openalex.org/W4229005866', 'https://openalex.org/W4377865046']",2024-03-18
https://openalex.org/W4392909760,https://doi.org/10.1109/icassp48485.2024.10447751,Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS,"Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.","['https://openalex.org/W4281492411', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385823092', 'https://openalex.org/W4385823152', 'https://openalex.org/W4381786045', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6770514103', 'https://openalex.org/W6755207826', 'https://openalex.org/W4385822683', 'https://openalex.org/W4226132755', 'https://openalex.org/W6853244311', 'https://openalex.org/W4385823130', 'https://openalex.org/W6853611000', 'https://openalex.org/W3211278025', 'https://openalex.org/W3197580070', 'https://openalex.org/W2936774411', 'https://openalex.org/W3096159803', 'https://openalex.org/W3097777922', 'https://openalex.org/W6778823374', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W2963242190', 'https://openalex.org/W2962784628', 'https://openalex.org/W4283700324', 'https://openalex.org/W6857062747', 'https://openalex.org/W3015686596', 'https://openalex.org/W4372260432', 'https://openalex.org/W2972359262', 'https://openalex.org/W6853515095', 'https://openalex.org/W4380551955', 'https://openalex.org/W4387799863', 'https://openalex.org/W4380714544', 'https://openalex.org/W2988736778', 'https://openalex.org/W4378501656']",2024-03-18
https://openalex.org/W4392904154,https://doi.org/10.1109/icassp48485.2024.10446998,Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition,"Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.","['https://openalex.org/W6778883912', 'https://openalex.org/W6854866820', 'https://openalex.org/W6847363464', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W3209984917', 'https://openalex.org/W6850218400', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6853611000', 'https://openalex.org/W6849105126', 'https://openalex.org/W4381786045', 'https://openalex.org/W6853998256', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W3205878676', 'https://openalex.org/W4391021542', 'https://openalex.org/W2969985801', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244', 'https://openalex.org/W2808631503', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W6678809451', 'https://openalex.org/W6770434673', 'https://openalex.org/W6711908931', 'https://openalex.org/W6771467084', 'https://openalex.org/W3119308075', 'https://openalex.org/W2936774411', 'https://openalex.org/W4381827575', 'https://openalex.org/W4378501656', 'https://openalex.org/W3030437843', 'https://openalex.org/W4313679638', 'https://openalex.org/W4320013820', 'https://openalex.org/W4323066695', 'https://openalex.org/W4311000453', 'https://openalex.org/W4323651091', 'https://openalex.org/W2622566932', 'https://openalex.org/W4292779060', 'https://openalex.org/W4318351475', 'https://openalex.org/W4380551955', 'https://openalex.org/W4384918448']",2024-03-18
https://openalex.org/W4392902656,https://doi.org/10.1109/icassp48485.2024.10447296,Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W6857968694', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769196770', 'https://openalex.org/W6770514103', 'https://openalex.org/W4385822683', 'https://openalex.org/W4319862255', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W6850218400', 'https://openalex.org/W2963979492', 'https://openalex.org/W2962784628', 'https://openalex.org/W2113158412', 'https://openalex.org/W6638523607', 'https://openalex.org/W2183341477', 'https://openalex.org/W1494198834', 'https://openalex.org/W2407080277', 'https://openalex.org/W4323066695', 'https://openalex.org/W4381827575', 'https://openalex.org/W2747329762', 'https://openalex.org/W4387595589', 'https://openalex.org/W4385245566', 'https://openalex.org/W4378501656', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718191', 'https://openalex.org/W2988736778']",2024-03-18
https://openalex.org/W4400070537,https://doi.org/10.1109/lsp.2024.3419719,Tuning Large Language Model for Speech Recognition With Mixed-Scale Re-Tokenization,,"['https://openalex.org/W6800875267', 'https://openalex.org/W6850625674', 'https://openalex.org/W4391021666', 'https://openalex.org/W6857054612', 'https://openalex.org/W6861626678', 'https://openalex.org/W4381786045', 'https://openalex.org/W4389524500', 'https://openalex.org/W6852447913', 'https://openalex.org/W6853611000', 'https://openalex.org/W6855691466', 'https://openalex.org/W6855801281', 'https://openalex.org/W4292825791', 'https://openalex.org/W4385573012', 'https://openalex.org/W6855885476', 'https://openalex.org/W4385822683', 'https://openalex.org/W4392902656', 'https://openalex.org/W3180374548', 'https://openalex.org/W4385822337', 'https://openalex.org/W4391021773', 'https://openalex.org/W4389520395', 'https://openalex.org/W4391021779', 'https://openalex.org/W4391021623', 'https://openalex.org/W6860809563', 'https://openalex.org/W4226507725', 'https://openalex.org/W2963250244', 'https://openalex.org/W6797019122', 'https://openalex.org/W4385573788', 'https://openalex.org/W4385571769', 'https://openalex.org/W4385805202', 'https://openalex.org/W6856179682', 'https://openalex.org/W4390190613', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209873929', 'https://openalex.org/W1494198834', 'https://openalex.org/W6796581206', 'https://openalex.org/W4322718191', 'https://openalex.org/W4386351448', 'https://openalex.org/W4378501656', 'https://openalex.org/W4385328213', 'https://openalex.org/W4378711593', 'https://openalex.org/W4391835492']",2024-01-01
https://openalex.org/W4391021557,https://doi.org/10.1109/asru57964.2023.10389660,Few-Shot Spoken Language Understanding Via Joint Speech-Text Models,"Recent work on speech representation models jointly pre-trained with text has demonstrated the potential of improving speech representations by encoding speech and text in a shared space. In this paper, we leverage such shared representations to address the persistent challenge of limited data availability in spoken language understanding tasks. By employing a pre-trained speech-text model, we find that models fine-tuned on text can be effectively transferred to speech testing data. With as little as 1 hour of labeled speech data, our proposed approach achieves comparable performance on spoken language understanding tasks (specifically, sentiment analysis and named entity recognition) when compared to previous methods using speech-only pre-trained models fine-tuned on 10 times more data. Beyond the proof-of-concept study, we also analyze the latent representations. We find that the bottom layers of speech-text models are largely task-agnostic and align speech and text representations into a shared space, while the top layers are more task-specific.","['https://openalex.org/W3016181583', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W4281492411', 'https://openalex.org/W3140429000', 'https://openalex.org/W6790356757', 'https://openalex.org/W3180374548', 'https://openalex.org/W4287854499', 'https://openalex.org/W4385823328', 'https://openalex.org/W6854228366', 'https://openalex.org/W3205644108', 'https://openalex.org/W6803092890', 'https://openalex.org/W6845338303', 'https://openalex.org/W4385573012', 'https://openalex.org/W6848735303', 'https://openalex.org/W3122044994', 'https://openalex.org/W6767737316', 'https://openalex.org/W4226103796', 'https://openalex.org/W4287887773', 'https://openalex.org/W6852410838', 'https://openalex.org/W4283749766', 'https://openalex.org/W4297841873', 'https://openalex.org/W6755207826', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W4372260307', 'https://openalex.org/W4223622550', 'https://openalex.org/W6847568899', 'https://openalex.org/W6850334629', 'https://openalex.org/W6853611000', 'https://openalex.org/W4375869005', 'https://openalex.org/W6846865774', 'https://openalex.org/W6850218400', 'https://openalex.org/W2952638691', 'https://openalex.org/W2970854433', 'https://openalex.org/W3103368673', 'https://openalex.org/W1494198834', 'https://openalex.org/W6847181065', 'https://openalex.org/W4237723258', 'https://openalex.org/W2970820321', 'https://openalex.org/W4226380987', 'https://openalex.org/W2747874407', 'https://openalex.org/W6765469073', 'https://openalex.org/W6776039324', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972359262', 'https://openalex.org/W4394671563', 'https://openalex.org/W3169905056', 'https://openalex.org/W2958953787', 'https://openalex.org/W4319862218', 'https://openalex.org/W4323651091', 'https://openalex.org/W4221155340', 'https://openalex.org/W4377864748', 'https://openalex.org/W4394773771', 'https://openalex.org/W3156326119', 'https://openalex.org/W2914120296', 'https://openalex.org/W3207222250', 'https://openalex.org/W4392979802', 'https://openalex.org/W4312052802', 'https://openalex.org/W2896457183', 'https://openalex.org/W3037217258', 'https://openalex.org/W4378501656', 'https://openalex.org/W4310826357', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323066695', 'https://openalex.org/W3036601975']",2023-12-16
https://openalex.org/W4406461437,https://doi.org/10.1109/slt61566.2024.10832289,"ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs For Audio, Music, and Speech",,"['https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810673746', 'https://openalex.org/W6857589352', 'https://openalex.org/W3197580070', 'https://openalex.org/W4385822439', 'https://openalex.org/W3203140070', 'https://openalex.org/W4385822683', 'https://openalex.org/W4392909068', 'https://openalex.org/W4372347505', 'https://openalex.org/W6777028661', 'https://openalex.org/W3180374548', 'https://openalex.org/W4372260052', 'https://openalex.org/W4385565440', 'https://openalex.org/W4392909760', 'https://openalex.org/W6848735303', 'https://openalex.org/W6853096648', 'https://openalex.org/W6856434366', 'https://openalex.org/W4389524500', 'https://openalex.org/W4402111789', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W2962780374', 'https://openalex.org/W3016160783', 'https://openalex.org/W6802659129', 'https://openalex.org/W4280572880', 'https://openalex.org/W4402111799', 'https://openalex.org/W4297841651', 'https://openalex.org/W3151851237', 'https://openalex.org/W4385823192', 'https://openalex.org/W4372270198', 'https://openalex.org/W6852581948', 'https://openalex.org/W4392903389', 'https://openalex.org/W4406461271', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W1728888090', 'https://openalex.org/W6857134842', 'https://openalex.org/W3163243746', 'https://openalex.org/W6762931180', 'https://openalex.org/W3015338123', 'https://openalex.org/W3161236344', 'https://openalex.org/W6783867762', 'https://openalex.org/W3163217847', 'https://openalex.org/W4402111636', 'https://openalex.org/W3161480375', 'https://openalex.org/W4225956675', 'https://openalex.org/W4385823256', 'https://openalex.org/W6869945801', 'https://openalex.org/W6847363464', 'https://openalex.org/W6853611000', 'https://openalex.org/W4392902656', 'https://openalex.org/W4390224291', 'https://openalex.org/W4402111612', 'https://openalex.org/W4402112534', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W2964243274', 'https://openalex.org/W4400111385', 'https://openalex.org/W6869301554', 'https://openalex.org/W6862144568', 'https://openalex.org/W4402111456', 'https://openalex.org/W4297841773', 'https://openalex.org/W3024869864', 'https://openalex.org/W4392904154', 'https://openalex.org/W4385756463', 'https://openalex.org/W4380434618', 'https://openalex.org/W4392903251', 'https://openalex.org/W4392909101', 'https://openalex.org/W4319862462', 'https://openalex.org/W4402112643', 'https://openalex.org/W4379797396', 'https://openalex.org/W4402111425', 'https://openalex.org/W4402111669', 'https://openalex.org/W4385823416', 'https://openalex.org/W3160427568', 'https://openalex.org/W4391021746', 'https://openalex.org/W4385807463', 'https://openalex.org/W4391021652', 'https://openalex.org/W2972359262', 'https://openalex.org/W3198533616', 'https://openalex.org/W3170787215', 'https://openalex.org/W3153263923', 'https://openalex.org/W4285345683', 'https://openalex.org/W6892481026', 'https://openalex.org/W3159302906', 'https://openalex.org/W4296068763', 'https://openalex.org/W4402112388', 'https://openalex.org/W6779090866', 'https://openalex.org/W3081800019', 'https://openalex.org/W3158762648', 'https://openalex.org/W4402670057', 'https://openalex.org/W1494198834', 'https://openalex.org/W2726515241', 'https://openalex.org/W2995181338', 'https://openalex.org/W3100460087', 'https://openalex.org/W4392538788', 'https://openalex.org/W4313679638', 'https://openalex.org/W3024605872', 'https://openalex.org/W3206375275', 'https://openalex.org/W4399425629', 'https://openalex.org/W4378501656', 'https://openalex.org/W4387323811', 'https://openalex.org/W4399794707']",2024-12-02
https://openalex.org/W4391021547,https://doi.org/10.1109/asru57964.2023.10389672,Towards General-Purpose Text-Instruction-Guided Voice Conversion,"This paper introduces a novel voice conversion (VC) model, guided by text instructions such as ""articulate slowly with a deep tone"" or ""speak in a cheerful boyish voice"". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, using text instruction adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6810738896', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853888607', 'https://openalex.org/W6852240503', 'https://openalex.org/W6849416043', 'https://openalex.org/W4390075359', 'https://openalex.org/W6853188576', 'https://openalex.org/W6848735303', 'https://openalex.org/W6849109464', 'https://openalex.org/W6850334629', 'https://openalex.org/W6853096648', 'https://openalex.org/W6853611000', 'https://openalex.org/W4385822787', 'https://openalex.org/W4375869257', 'https://openalex.org/W6845479124', 'https://openalex.org/W3098557217', 'https://openalex.org/W2963035245', 'https://openalex.org/W2576309025', 'https://openalex.org/W6840412704', 'https://openalex.org/W4205742757', 'https://openalex.org/W4307323391', 'https://openalex.org/W6852781825', 'https://openalex.org/W3215615641', 'https://openalex.org/W4372270198', 'https://openalex.org/W4381786045', 'https://openalex.org/W4226162428', 'https://openalex.org/W6852859116', 'https://openalex.org/W6849798658', 'https://openalex.org/W2077801020', 'https://openalex.org/W2148846882', 'https://openalex.org/W2899361462', 'https://openalex.org/W4385574033', 'https://openalex.org/W3025680351', 'https://openalex.org/W3015241559', 'https://openalex.org/W1509691205', 'https://openalex.org/W2120605154', 'https://openalex.org/W6761580982', 'https://openalex.org/W6762533536', 'https://openalex.org/W6803547063', 'https://openalex.org/W6805710207', 'https://openalex.org/W3168719651', 'https://openalex.org/W2963539064', 'https://openalex.org/W2972667718', 'https://openalex.org/W3196667132', 'https://openalex.org/W6746801104', 'https://openalex.org/W2964069186', 'https://openalex.org/W2902070858', 'https://openalex.org/W2937579788', 'https://openalex.org/W3162512456', 'https://openalex.org/W3095936335', 'https://openalex.org/W2963830550', 'https://openalex.org/W6769216209', 'https://openalex.org/W2532494225', 'https://openalex.org/W2962896155', 'https://openalex.org/W2889061305', 'https://openalex.org/W2946555236', 'https://openalex.org/W3016243847', 'https://openalex.org/W3015805741', 'https://openalex.org/W3197763626', 'https://openalex.org/W3161627112', 'https://openalex.org/W3210530853', 'https://openalex.org/W4221146610', 'https://openalex.org/W4312732823', 'https://openalex.org/W3207300132', 'https://openalex.org/W3161695192', 'https://openalex.org/W3140429000', 'https://openalex.org/W3034999214', 'https://openalex.org/W3196475561', 'https://openalex.org/W4382603054', 'https://openalex.org/W4301371414', 'https://openalex.org/W2972659941', 'https://openalex.org/W4378501656', 'https://openalex.org/W4225680573', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323651091', 'https://openalex.org/W4367365521', 'https://openalex.org/W4377010126', 'https://openalex.org/W4318718996', 'https://openalex.org/W2945478979', 'https://openalex.org/W4380136719', 'https://openalex.org/W4379539302', 'https://openalex.org/W2979938509', 'https://openalex.org/W4318752004', 'https://openalex.org/W4398152753', 'https://openalex.org/W4381827575', 'https://openalex.org/W3102628737', 'https://openalex.org/W4300980117', 'https://openalex.org/W4390912423', 'https://openalex.org/W4377865046', 'https://openalex.org/W2774848319', 'https://openalex.org/W4226278401']",2023-12-16
https://openalex.org/W4392931320,https://doi.org/10.1109/icassp48485.2024.10446991,A Study on the Adverse Impact of Synthetic Speech on Speech Recognition,"The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.","['https://openalex.org/W6783867762', 'https://openalex.org/W4297841605', 'https://openalex.org/W6848735303', 'https://openalex.org/W3006752097', 'https://openalex.org/W3016008406', 'https://openalex.org/W3080248383', 'https://openalex.org/W4281712850', 'https://openalex.org/W4225873749', 'https://openalex.org/W4385573729', 'https://openalex.org/W4298633873', 'https://openalex.org/W3134568285', 'https://openalex.org/W4372259861', 'https://openalex.org/W3013139777', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W6847363464', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853611000', 'https://openalex.org/W2963242190', 'https://openalex.org/W4283067311', 'https://openalex.org/W4385822407', 'https://openalex.org/W4313679638', 'https://openalex.org/W4292779060', 'https://openalex.org/W4226278401', 'https://openalex.org/W3092028330', 'https://openalex.org/W4378501656', 'https://openalex.org/W4381827575']",2024-03-18
https://openalex.org/W4406461468,https://doi.org/10.1109/slt61566.2024.10832186,Mamba-Based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition,,"['https://openalex.org/W3097777922', 'https://openalex.org/W4385823140', 'https://openalex.org/W6803444062', 'https://openalex.org/W6839235732', 'https://openalex.org/W4313442864', 'https://openalex.org/W4375869285', 'https://openalex.org/W4372260519', 'https://openalex.org/W4385822815', 'https://openalex.org/W4392910298', 'https://openalex.org/W6810325043', 'https://openalex.org/W4385823047', 'https://openalex.org/W4385822478', 'https://openalex.org/W6859298233', 'https://openalex.org/W6861387779', 'https://openalex.org/W6860920680', 'https://openalex.org/W4406461266', 'https://openalex.org/W6863679278', 'https://openalex.org/W6868207820', 'https://openalex.org/W4402112499', 'https://openalex.org/W2739883972', 'https://openalex.org/W6778883912', 'https://openalex.org/W4297841687', 'https://openalex.org/W4391021666', 'https://openalex.org/W4392904094', 'https://openalex.org/W6860809563', 'https://openalex.org/W6859099255', 'https://openalex.org/W4392902656', 'https://openalex.org/W6856400107', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W4392904805', 'https://openalex.org/W6868789466', 'https://openalex.org/W6841205203', 'https://openalex.org/W3180374548', 'https://openalex.org/W4381786045', 'https://openalex.org/W4392909760', 'https://openalex.org/W4392909068', 'https://openalex.org/W2962780374', 'https://openalex.org/W2963250244', 'https://openalex.org/W6861376004', 'https://openalex.org/W6769627184', 'https://openalex.org/W1494198834', 'https://openalex.org/W3209984917', 'https://openalex.org/W2799473636', 'https://openalex.org/W3198694222', 'https://openalex.org/W2963242190', 'https://openalex.org/W6601563604', 'https://openalex.org/W4403727496', 'https://openalex.org/W4378501656', 'https://openalex.org/W4398230172', 'https://openalex.org/W4393943345', 'https://openalex.org/W4381827575', 'https://openalex.org/W4391244998', 'https://openalex.org/W4386875055', 'https://openalex.org/W4391013663', 'https://openalex.org/W4389326242', 'https://openalex.org/W4388718054', 'https://openalex.org/W3101648800']",2024-12-02
https://openalex.org/W4408352608,https://doi.org/10.1109/icassp49660.2025.10888128,MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders,,"['https://openalex.org/W4384918448', 'https://openalex.org/W4387891768', 'https://openalex.org/W4388718054', 'https://openalex.org/W4377372369', 'https://openalex.org/W4393903963', 'https://openalex.org/W4396945500', 'https://openalex.org/W4376312115', 'https://openalex.org/W4406033011', 'https://openalex.org/W4378501656', 'https://openalex.org/W4386351448', 'https://openalex.org/W4381827575', 'https://openalex.org/W4391591726', 'https://openalex.org/W4404784428', 'https://openalex.org/W4311000453', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4312048190', 'https://openalex.org/W4391021627', 'https://openalex.org/W3168867926', 'https://openalex.org/W4395022840', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963686995', 'https://openalex.org/W4313014461', 'https://openalex.org/W2962854302', 'https://openalex.org/W4400104114', 'https://openalex.org/W4387994989', 'https://openalex.org/W4395474395', 'https://openalex.org/W2936774411']",2025-03-12
https://openalex.org/W4412570610,https://doi.org/10.1002/aidi.202500085,"Large Language Model in Materials Science: Roles, Challenges, and Strategic Outlook","Large language models (LLMs) are creating a new paradigm for materials science by transforming textual insights into experimental findings. Leveraging their strengths in natural language understanding, multimodal alignment, and few‐shot reasoning, LLMs already show potential in property prediction, synthesis planning, and uncertainty quantification. This perspective highlights four key roles, Oracle, Surrogate, Quant, and Arbiter, to systematize recent advancements of LLMs in knowledge extraction, property inference, risk assessment, and decision‐making. Experience suggests that true value arises from integrating these capabilities into a verifiable, traceable loop rather than merely scaling model size. However, LLMs still face challenges due to data heterogeneity, limited interpretability, hallucination control, and misalignment with scientific tasks. To address these issues, we propose three forward‐looking directions: developing domain‐adapted foundation models infused with materials science context, establishing a standardized cross‐modal data infrastructure, and incorporating expert feedback alongside robotic automated experimentation into a fully traceable research loop. Through enhanced human–AI collaboration and methodological innovation, LLMs can transform from general‐purpose language tools into scientifically aware partners, advancing materials discovery toward a more efficient, interpretable, and sustainable future.","['https://openalex.org/W4392737546', 'https://openalex.org/W4306705676', 'https://openalex.org/W4405622351', 'https://openalex.org/W2919115771', 'https://openalex.org/W3180477523', 'https://openalex.org/W2962843773', 'https://openalex.org/W2008056655', 'https://openalex.org/W3014504517', 'https://openalex.org/W2163605009', 'https://openalex.org/W1485009520', 'https://openalex.org/W3010336026', 'https://openalex.org/W4298289240', 'https://openalex.org/W4385245566', 'https://openalex.org/W3195577433', 'https://openalex.org/W4327810158', 'https://openalex.org/W4390041933', 'https://openalex.org/W4384918448', 'https://openalex.org/W4405903187', 'https://openalex.org/W4283026156', 'https://openalex.org/W4391709329', 'https://openalex.org/W4323697341', 'https://openalex.org/W4402683897', 'https://openalex.org/W4404782859', 'https://openalex.org/W4281690148', 'https://openalex.org/W4399453708', 'https://openalex.org/W4321013654', 'https://openalex.org/W4367000100', 'https://openalex.org/W4394745423', 'https://openalex.org/W4387561135', 'https://openalex.org/W4393027318', 'https://openalex.org/W4390529036', 'https://openalex.org/W4394673016', 'https://openalex.org/W4389727268', 'https://openalex.org/W4385645323', 'https://openalex.org/W4380994269', 'https://openalex.org/W4402386435', 'https://openalex.org/W4389991792', 'https://openalex.org/W3020710627', 'https://openalex.org/W2902762889', 'https://openalex.org/W3110165646', 'https://openalex.org/W4382318938', 'https://openalex.org/W4408196005', 'https://openalex.org/W3047184465', 'https://openalex.org/W4402926388', 'https://openalex.org/W4388274718', 'https://openalex.org/W4321392130', 'https://openalex.org/W4294435850', 'https://openalex.org/W6680532216', 'https://openalex.org/W4297734170', 'https://openalex.org/W4386977391', 'https://openalex.org/W4392903956', 'https://openalex.org/W4378501656', 'https://openalex.org/W4310418667', 'https://openalex.org/W2097927681', 'https://openalex.org/W2896457183', 'https://openalex.org/W2965373594', 'https://openalex.org/W3198659451', 'https://openalex.org/W6810081322', 'https://openalex.org/W4322718191', 'https://openalex.org/W4375956160', 'https://openalex.org/W4388623498', 'https://openalex.org/W4387334721', 'https://openalex.org/W2064675550', 'https://openalex.org/W1986760892', 'https://openalex.org/W4213019189', 'https://openalex.org/W3178976598', 'https://openalex.org/W3118781290', 'https://openalex.org/W3129576130', 'https://openalex.org/W2791753912', 'https://openalex.org/W4310230601', 'https://openalex.org/W4362707064', 'https://openalex.org/W4389984066', 'https://openalex.org/W4389519118', 'https://openalex.org/W4387156634', 'https://openalex.org/W4309674289', 'https://openalex.org/W4386721614', 'https://openalex.org/W4389713766', 'https://openalex.org/W4399115485', 'https://openalex.org/W3168867926', 'https://openalex.org/W2736601468', 'https://openalex.org/W4311991106', 'https://openalex.org/W4378771755', 'https://openalex.org/W4386437475', 'https://openalex.org/W3034954837', 'https://openalex.org/W4379801285', 'https://openalex.org/W4399803256', 'https://openalex.org/W4404192248', 'https://openalex.org/W4393335770', 'https://openalex.org/W4392011666', 'https://openalex.org/W4403591916', 'https://openalex.org/W4292719230', 'https://openalex.org/W4391836235', 'https://openalex.org/W4394007314', 'https://openalex.org/W4407479096', 'https://openalex.org/W4399290669', 'https://openalex.org/W4400222628', 'https://openalex.org/W4391421491', 'https://openalex.org/W4404349998', 'https://openalex.org/W4402427294', 'https://openalex.org/W4404354593', 'https://openalex.org/W4390511840', 'https://openalex.org/W6967048876', 'https://openalex.org/W4390722655', 'https://openalex.org/W4412025960', 'https://openalex.org/W4399912458', 'https://openalex.org/W4411120014', 'https://openalex.org/W4405355411', 'https://openalex.org/W4387694367', 'https://openalex.org/W6966985652', 'https://openalex.org/W4407015717', 'https://openalex.org/W4407922392', 'https://openalex.org/W3109615746', 'https://openalex.org/W3195348523', 'https://openalex.org/W4396745882', 'https://openalex.org/W4400025514', 'https://openalex.org/W3203974877', 'https://openalex.org/W4404781665', 'https://openalex.org/W4403228802', 'https://openalex.org/W4401132497', 'https://openalex.org/W4401442519', 'https://openalex.org/W4388843229', 'https://openalex.org/W2474706806', 'https://openalex.org/W4376641465', 'https://openalex.org/W4401549128', 'https://openalex.org/W4226050570', 'https://openalex.org/W4385490607', 'https://openalex.org/W4296154431', 'https://openalex.org/W4389132715', 'https://openalex.org/W4362664882', 'https://openalex.org/W4381510318', 'https://openalex.org/W4390011017', 'https://openalex.org/W4386501199', 'https://openalex.org/W3139002032', 'https://openalex.org/W3186870698', 'https://openalex.org/W3092530885', 'https://openalex.org/W3152966503', 'https://openalex.org/W3151042244', 'https://openalex.org/W4307138865', 'https://openalex.org/W4401789752', 'https://openalex.org/W4406779522', 'https://openalex.org/W4322767099', 'https://openalex.org/W3042021489', 'https://openalex.org/W4212913440', 'https://openalex.org/W3031157367', 'https://openalex.org/W4403884259', 'https://openalex.org/W2897969207', 'https://openalex.org/W2783249037', 'https://openalex.org/W2998311588', 'https://openalex.org/W3207309731', 'https://openalex.org/W4404356992', 'https://openalex.org/W4280578746', 'https://openalex.org/W4385497042', 'https://openalex.org/W3107581532', 'https://openalex.org/W4389921502', 'https://openalex.org/W4319323461', 'https://openalex.org/W4404691742', 'https://openalex.org/W4364380196', 'https://openalex.org/W4399176479', 'https://openalex.org/W3139463858', 'https://openalex.org/W4405419007', 'https://openalex.org/W4399377978', 'https://openalex.org/W4224308101', 'https://openalex.org/W2122410182', 'https://openalex.org/W4393065599', 'https://openalex.org/W4404936357', 'https://openalex.org/W4253877692', 'https://openalex.org/W2998704965', 'https://openalex.org/W4200551341', 'https://openalex.org/W4391561379', 'https://openalex.org/W3092557781', 'https://openalex.org/W2618530766', 'https://openalex.org/W4318718899', 'https://openalex.org/W4366330503', 'https://openalex.org/W2158195707', 'https://openalex.org/W4408534627']",2025-07-22
https://openalex.org/W4409640766,https://doi.org/10.1109/lsp.2025.3562825,Boosting Context-Aware Speech Translation With Large Language Models,,"['https://openalex.org/W4386590854', 'https://openalex.org/W6839510803', 'https://openalex.org/W4390778590', 'https://openalex.org/W4385570101', 'https://openalex.org/W3173657420', 'https://openalex.org/W4392902935', 'https://openalex.org/W3103878009', 'https://openalex.org/W3211978535', 'https://openalex.org/W3173691187', 'https://openalex.org/W4225318329', 'https://openalex.org/W6778883912', 'https://openalex.org/W4285294723', 'https://openalex.org/W6810081322', 'https://openalex.org/W4389524500', 'https://openalex.org/W4379540238', 'https://openalex.org/W6859099255', 'https://openalex.org/W6870775139', 'https://openalex.org/W6809646742', 'https://openalex.org/W3196509775', 'https://openalex.org/W6847363464', 'https://openalex.org/W4385245566', 'https://openalex.org/W6796581206', 'https://openalex.org/W4408352362', 'https://openalex.org/W2963532001', 'https://openalex.org/W2758950307', 'https://openalex.org/W3035252911', 'https://openalex.org/W3092424727', 'https://openalex.org/W4391021666', 'https://openalex.org/W4389600306', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853014061', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177']",2025-01-01
https://openalex.org/W4386931042,https://doi.org/10.1007/978-3-031-44198-1_29,I$$^2$$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding,,"['https://openalex.org/W1649407914', 'https://openalex.org/W1975244201', 'https://openalex.org/W4385822432', 'https://openalex.org/W2094472029', 'https://openalex.org/W2474111273', 'https://openalex.org/W2946085385', 'https://openalex.org/W4385822317', 'https://openalex.org/W4375869190', 'https://openalex.org/W3106751511', 'https://openalex.org/W2791169651', 'https://openalex.org/W2898856000', 'https://openalex.org/W3098101716', 'https://openalex.org/W3034724424', 'https://openalex.org/W4224212267', 'https://openalex.org/W4385573356', 'https://openalex.org/W4385572615', 'https://openalex.org/W4385571399', 'https://openalex.org/W4375868795', 'https://openalex.org/W3195336977', 'https://openalex.org/W3161463965', 'https://openalex.org/W3173794693', 'https://openalex.org/W4385573163', 'https://openalex.org/W4385573070', 'https://openalex.org/W3166631396', 'https://openalex.org/W6638523607', 'https://openalex.org/W2743289088', 'https://openalex.org/W2739879705', 'https://openalex.org/W2803392141', 'https://openalex.org/W4379540238', 'https://openalex.org/W4385807416', 'https://openalex.org/W4390872315', 'https://openalex.org/W4385822428', 'https://openalex.org/W4390874127', 'https://openalex.org/W4372265871', 'https://openalex.org/W2970850076', 'https://openalex.org/W3003813211', 'https://openalex.org/W3100251125', 'https://openalex.org/W3098800734', 'https://openalex.org/W3103469330', 'https://openalex.org/W3207981220']",2023-01-01
https://openalex.org/W4391331299,https://doi.org/10.1109/smc53992.2023.10393998,CMM: Code-Switching with Manifold Mixup for Cross-Lingual Spoken Language Understanding,"Spoken language understanding (SLU) is an important task which involves two subtasks, including intent detection and slot filling. Although it has achieved great success in high-resource languages, it still remains challenging in low-resource languages due to the lack of labeled training data. Consequently, there is growing interest in code-switching method for cross-lingual SLU to solve the problem in the low-resource languages. However, despite the success of existing models, most of these methods fail to effectively leverage the code-switched utterances. In this paper, we propose a novel framework termed CMM for zero-shot cross-lingual SLU which simplifies the learning task for the model. Specifically, we apply both mixup and curriculum learning method to dynamically combine the information from pure utterances and code-switched utterances. Experimental results demonstrate that the proposed framework improves the performance compared to several strong baselines and achieves the state-of-the-art performance on MultiATIS++ dataset, with a relative improvement of 3.0% in terms of overall accuracy over the previous best model.","['https://openalex.org/W4385572615', 'https://openalex.org/W4385571399', 'https://openalex.org/W4385822317', 'https://openalex.org/W4385822945', 'https://openalex.org/W4385822432', 'https://openalex.org/W4390874127', 'https://openalex.org/W2153962611', 'https://openalex.org/W3098101716', 'https://openalex.org/W2997214274', 'https://openalex.org/W3034724424', 'https://openalex.org/W2970850076', 'https://openalex.org/W4375839990', 'https://openalex.org/W2996987694', 'https://openalex.org/W3175665465', 'https://openalex.org/W6846505686', 'https://openalex.org/W4385822428', 'https://openalex.org/W4390872315', 'https://openalex.org/W4385807416', 'https://openalex.org/W2803392141', 'https://openalex.org/W4375869190', 'https://openalex.org/W4224212267', 'https://openalex.org/W4385573356', 'https://openalex.org/W6755207826', 'https://openalex.org/W3166631396', 'https://openalex.org/W3003813211', 'https://openalex.org/W3100251125', 'https://openalex.org/W6838684908', 'https://openalex.org/W6631190155', 'https://openalex.org/W3098800734', 'https://openalex.org/W3173794693', 'https://openalex.org/W2917128112', 'https://openalex.org/W4280593916', 'https://openalex.org/W4379540238', 'https://openalex.org/W4327810154', 'https://openalex.org/W4288099666', 'https://openalex.org/W3103469330']",2023-10-01
https://openalex.org/W4385573012,https://doi.org/10.18653/v1/2022.emnlp-main.108,SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training,"The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with a shared unit encoder. Leveraging hidden-unit as an interface to align speech and text, we can decompose the speech-to-text model into a speech-to-unit model and a unit-to-text model, which can be jointly pre-trained with unpaired speech and text data respectively. Our proposed SpeechUT is fine-tuned and evaluated on automatic speech recognition (ASR) and speech translation (ST) tasks. Experimental results show that SpeechUT gets substantial improvements over strong baselines, and achieves state-of-the-art performance on both the LibriSpeech ASR and MuST-C ST tasks. To better understand the proposed SpeechUT, detailed analyses are conducted. The code and pre-trained models are available at https://aka.ms/SpeechUT.","['https://openalex.org/W3193521535', 'https://openalex.org/W2933138175', 'https://openalex.org/W2766219058', 'https://openalex.org/W2963925437', 'https://openalex.org/W2945260553', 'https://openalex.org/W4226278833', 'https://openalex.org/W3162313915', 'https://openalex.org/W4221145109', 'https://openalex.org/W3209059054', 'https://openalex.org/W2970597249', 'https://openalex.org/W3036601975', 'https://openalex.org/W2945700568', 'https://openalex.org/W3160525311', 'https://openalex.org/W4224934179', 'https://openalex.org/W2739883972', 'https://openalex.org/W2187089797', 'https://openalex.org/W4320194748', 'https://openalex.org/W4221163209', 'https://openalex.org/W3205644108', 'https://openalex.org/W3207222250', 'https://openalex.org/W2991213871', 'https://openalex.org/W1915251500', 'https://openalex.org/W4300980246', 'https://openalex.org/W2963799213', 'https://openalex.org/W4282958795', 'https://openalex.org/W4226120743', 'https://openalex.org/W4287854499', 'https://openalex.org/W3093579165', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W4287329822', 'https://openalex.org/W3175963743', 'https://openalex.org/W4223622550', 'https://openalex.org/W3209984917', 'https://openalex.org/W4283324001', 'https://openalex.org/W2979476256', 'https://openalex.org/W3034999214', 'https://openalex.org/W2127141656', 'https://openalex.org/W4287079508', 'https://openalex.org/W4287890956', 'https://openalex.org/W3209371554', 'https://openalex.org/W3176382501', 'https://openalex.org/W3148001440', 'https://openalex.org/W3161302809', 'https://openalex.org/W4221155340', 'https://openalex.org/W4226033575', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956']",2022-01-01
https://openalex.org/W4372270126,https://doi.org/10.1109/icassp49357.2023.10096988,Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages,"We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.","['https://openalex.org/W3097777922', 'https://openalex.org/W3200129129', 'https://openalex.org/W3095173472', 'https://openalex.org/W6675354045', 'https://openalex.org/W6638749077', 'https://openalex.org/W2963347649', 'https://openalex.org/W6810007534', 'https://openalex.org/W6729239390', 'https://openalex.org/W2933138175', 'https://openalex.org/W2995181338', 'https://openalex.org/W2953190524', 'https://openalex.org/W6668990524', 'https://openalex.org/W2962780374', 'https://openalex.org/W2142838865', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6623517193', 'https://openalex.org/W3095918555', 'https://openalex.org/W6679436768', 'https://openalex.org/W2914417638', 'https://openalex.org/W3198299542', 'https://openalex.org/W2327501763', 'https://openalex.org/W6769627184', 'https://openalex.org/W3034999214', 'https://openalex.org/W6755207826', 'https://openalex.org/W6769196770', 'https://openalex.org/W3213029956', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226278833', 'https://openalex.org/W3001434439', 'https://openalex.org/W6850036870', 'https://openalex.org/W3173767661', 'https://openalex.org/W6790356757', 'https://openalex.org/W3205644108', 'https://openalex.org/W6601894380', 'https://openalex.org/W4226103796', 'https://openalex.org/W3197580070', 'https://openalex.org/W6727336983', 'https://openalex.org/W6795346631', 'https://openalex.org/W6780218876', 'https://openalex.org/W6781927165', 'https://openalex.org/W1494198834', 'https://openalex.org/W4287887773', 'https://openalex.org/W1828163288', 'https://openalex.org/W2101234009', 'https://openalex.org/W3160799772', 'https://openalex.org/W2130942839', 'https://openalex.org/W3036601975', 'https://openalex.org/W46679369', 'https://openalex.org/W4394671563', 'https://openalex.org/W4221145109', 'https://openalex.org/W854541894', 'https://openalex.org/W2520160253', 'https://openalex.org/W4297808394', 'https://openalex.org/W3054645415', 'https://openalex.org/W4288089799', 'https://openalex.org/W4372270126', 'https://openalex.org/W2073459066', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W2549416390']",2023-05-05
https://openalex.org/W4285112515,https://doi.org/10.18653/v1/2022.iwslt-1.11,The YiTrans Speech Translation System for IWSLT 2022 Offline Shared Task,"This paper describes the submission of our end-to-end YiTrans speech translation system for the IWSLT 2022 offline task, which translates from English audio to German, Chinese, and Japanese. The YiTrans system is built on large-scale pre-trained encoder-decoder models. More specifically, we first design a multi-stage pre-training strategy to build a multi-modality model with a large amount of labeled and unlabeled data. We then fine-tune the corresponding components of the model for the downstream speech translation tasks. Moreover, we make various efforts to improve performance, such as data filtering, data augmentation, speech segmentation, model ensemble, and so on. Experimental results show that our YiTrans system obtains a significant improvement than the strong baseline on three translation directions, and it achieves +5.2 BLEU improvements over last year's optimal end-to-end system on tst2021 English-German.","['https://openalex.org/W2933138175', 'https://openalex.org/W3015783745', 'https://openalex.org/W4226278833', 'https://openalex.org/W2963925437', 'https://openalex.org/W2250357346', 'https://openalex.org/W2419539795', 'https://openalex.org/W2963216553', 'https://openalex.org/W3119308075', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092085609', 'https://openalex.org/W4285158119', 'https://openalex.org/W3207222250', 'https://openalex.org/W2083751884', 'https://openalex.org/W4308349017', 'https://openalex.org/W3034999214', 'https://openalex.org/W4287694131', 'https://openalex.org/W3120929527', 'https://openalex.org/W2885250264', 'https://openalex.org/W3186761682', 'https://openalex.org/W3176711365', 'https://openalex.org/W2184135559', 'https://openalex.org/W1522301498', 'https://openalex.org/W3102816807', 'https://openalex.org/W3101648800', 'https://openalex.org/W2110006374', 'https://openalex.org/W3186672448', 'https://openalex.org/W3186200218', 'https://openalex.org/W2945700568', 'https://openalex.org/W3037003839', 'https://openalex.org/W22168010', 'https://openalex.org/W2766219058', 'https://openalex.org/W3015703505', 'https://openalex.org/W2563351168', 'https://openalex.org/W3209059054', 'https://openalex.org/W3030437843', 'https://openalex.org/W3043665049', 'https://openalex.org/W3209783775', 'https://openalex.org/W1494198834', 'https://openalex.org/W2148708890', 'https://openalex.org/W2964161387']",2022-01-01
https://openalex.org/W4375869113,https://doi.org/10.1109/icassp49357.2023.10094922,CTCBERT: Advancing Hidden-Unit Bert with CTC Objectives,"In this work, we present a simple but effective method, CTCBERT, for advancing hidden-unit BERT (HuBERT). HuBERT applies a frame-level cross-entropy (CE) loss, which is similar to most acoustic model training. However, CTCBERT performs the model training with the Connectionist Temporal Classification (CTC) objective after removing duplicated IDs in each masked region. The idea stems from the observation that there can be significant errors in alignments when using clustered or aligned IDs. CTC learns alignments implicitly, indicating that learning with CTC can be more flexible when misalignment exists. We examine CTCBERT on IDs from HuBERT Iter1, HuBERT Iter2, and PBERT. The CTC training brings consistent improvements compared to the CE training. Furthermore, when loading blank-related parameters during finetuning, slight improvements are observed. Evaluated on the Librispeech 960-100h setting, the relative WER improvements of CTCBERT are 2%-11% over HuBERT and PERT on test-other data.","['https://openalex.org/W2972943112', 'https://openalex.org/W6755207826', 'https://openalex.org/W3097286738', 'https://openalex.org/W3016011332', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953190524', 'https://openalex.org/W4293793697', 'https://openalex.org/W4226507725', 'https://openalex.org/W4226278833', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W3160345865', 'https://openalex.org/W3041561163', 'https://openalex.org/W2127141656', 'https://openalex.org/W6838909421', 'https://openalex.org/W1494198834', 'https://openalex.org/W2889068726', 'https://openalex.org/W2973049979', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W2514741789', 'https://openalex.org/W2888867175', 'https://openalex.org/W6757817989', 'https://openalex.org/W3204696009', 'https://openalex.org/W4297841844', 'https://openalex.org/W3209984917', 'https://openalex.org/W4224934179', 'https://openalex.org/W4281672148', 'https://openalex.org/W4281492411', 'https://openalex.org/W4287889722', 'https://openalex.org/W2908510526', 'https://openalex.org/W4283324001', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4400900236,https://doi.org/10.1016/j.specom.2024.103109,Decoupled structure for improved adaptability of end-to-end models,,"['https://openalex.org/W3205644108', 'https://openalex.org/W4226278833', 'https://openalex.org/W4285192675', 'https://openalex.org/W4210663600', 'https://openalex.org/W3198039885', 'https://openalex.org/W2327501763', 'https://openalex.org/W3202419788', 'https://openalex.org/W6623517193', 'https://openalex.org/W4226158189', 'https://openalex.org/W3177132412', 'https://openalex.org/W3198429080', 'https://openalex.org/W2147768505', 'https://openalex.org/W3172862365', 'https://openalex.org/W4210758944', 'https://openalex.org/W4224292837', 'https://openalex.org/W4372266510', 'https://openalex.org/W4385573113', 'https://openalex.org/W6601894380', 'https://openalex.org/W3015686596', 'https://openalex.org/W2127141656', 'https://openalex.org/W2102113734', 'https://openalex.org/W2143612262', 'https://openalex.org/W3097777922', 'https://openalex.org/W4385567350', 'https://openalex.org/W2160815625', 'https://openalex.org/W2064675550', 'https://openalex.org/W3198771897', 'https://openalex.org/W4226054021', 'https://openalex.org/W3037217258', 'https://openalex.org/W3015698636', 'https://openalex.org/W2963240019', 'https://openalex.org/W3211278025', 'https://openalex.org/W2940322076', 'https://openalex.org/W3095311338', 'https://openalex.org/W3007905516', 'https://openalex.org/W4312203699', 'https://openalex.org/W3205080563', 'https://openalex.org/W3008037978', 'https://openalex.org/W3205788551', 'https://openalex.org/W3152221657', 'https://openalex.org/W2933138175', 'https://openalex.org/W1586176709', 'https://openalex.org/W1494198834', 'https://openalex.org/W2101105183', 'https://openalex.org/W2972880214', 'https://openalex.org/W3012492057', 'https://openalex.org/W3197661863', 'https://openalex.org/W2251321385', 'https://openalex.org/W2939111082', 'https://openalex.org/W3160475509', 'https://openalex.org/W2888779557', 'https://openalex.org/W4225308107', 'https://openalex.org/W4283031490', 'https://openalex.org/W3016234571', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964539095', 'https://openalex.org/W2962780374', 'https://openalex.org/W2766219058', 'https://openalex.org/W3203453034', 'https://openalex.org/W2158069733', 'https://openalex.org/W3198439131', 'https://openalex.org/W3016010032', 'https://openalex.org/W2936078256', 'https://openalex.org/W3162244132', 'https://openalex.org/W3205201903', 'https://openalex.org/W46679369', 'https://openalex.org/W4375868904', 'https://openalex.org/W1828163288', 'https://openalex.org/W4319862418', 'https://openalex.org/W1915251500', 'https://openalex.org/W4375869444']",2024-07-23
https://openalex.org/W4385489023,https://doi.org/10.1109/icasspw59220.2023.10193218,Channel-Aware Pretraining Of Joint Encoder-Decoder Self-Supervised Model For Telephonic-Speech ASR,"This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of $\sim 4$% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.","['https://openalex.org/W2912668608', 'https://openalex.org/W3209059054', 'https://openalex.org/W2962780374', 'https://openalex.org/W6850036870', 'https://openalex.org/W4226278833', 'https://openalex.org/W2013598660', 'https://openalex.org/W2133856945', 'https://openalex.org/W3198771897', 'https://openalex.org/W2973141395', 'https://openalex.org/W4281672148', 'https://openalex.org/W2400830530', 'https://openalex.org/W1517841224', 'https://openalex.org/W2749459361', 'https://openalex.org/W2399742709', 'https://openalex.org/W4372270126']",2023-06-04
https://openalex.org/W4372260156,https://doi.org/10.1109/icassp49357.2023.10096788,Do Coarser Units Benefit Cluster Prediction-Based Speech Pre-Training?,International audience,"['https://openalex.org/W6845951457', 'https://openalex.org/W6810461289', 'https://openalex.org/W130754613', 'https://openalex.org/W6680970901', 'https://openalex.org/W6678277124', 'https://openalex.org/W2963979492', 'https://openalex.org/W6810047917', 'https://openalex.org/W4224875474', 'https://openalex.org/W4287854499', 'https://openalex.org/W2995181338', 'https://openalex.org/W6780218876', 'https://openalex.org/W2752796333', 'https://openalex.org/W6766673545', 'https://openalex.org/W6755207826', 'https://openalex.org/W3035524453', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810673746', 'https://openalex.org/W3209984917', 'https://openalex.org/W2962784628', 'https://openalex.org/W4226507725', 'https://openalex.org/W6774314701', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6629717138', 'https://openalex.org/W6769311223', 'https://openalex.org/W6802045652', 'https://openalex.org/W6810531757', 'https://openalex.org/W6790356757', 'https://openalex.org/W3140429000', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3034999214', 'https://openalex.org/W2963799213', 'https://openalex.org/W2138615112', 'https://openalex.org/W4385245566', 'https://openalex.org/W2121227244', 'https://openalex.org/W2965373594', 'https://openalex.org/W4307680525', 'https://openalex.org/W4221161761', 'https://openalex.org/W3005680577', 'https://openalex.org/W4313182775', 'https://openalex.org/W3036601975', 'https://openalex.org/W4226278833', 'https://openalex.org/W4297808394', 'https://openalex.org/W1494198834', 'https://openalex.org/W3203140070', 'https://openalex.org/W4303649106', 'https://openalex.org/W2896457183', 'https://openalex.org/W4394671563']",2023-05-05
https://openalex.org/W4390482765,https://doi.org/10.3390/electronics13010190,Adapting Pre-Trained Self-Supervised Learning Model for Speech Recognition with Light-Weight Adapters,"Self-supervised learning (SSL) is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks. However, effectively incorporating a pre-trained SSL model into an automatic speech recognition (ASR) system remains challenging. In this paper, we propose a network architecture with light-weight adapters to adapt a pre-trained SSL model for an end-to-end (E2E) ASR. An adapter is introduced in each SSL network layer and trained on the downstream ASR task, while the parameters of the pre-trained SSL network layers remain unchanged. By carrying over all pre-trained parameters, we avoid the catastrophic forgetting problem. At the same time, we allow the network to quickly adapt to ASR task with light-weight adapters. The experiments using LibriSpeech and Wall Street Journal (WSJ) datasets show that (1) the proposed adapter-based fine-tuning consistently outperforms full-fledged training in low-resource scenarios, with up to 17.5%/12.2% relative word error rate (WER) reduction on the 10 min LibriSpeech split; (2) the adapter-based adaptation also shows competitive performance in high-resource scenarios, which further validates the effectiveness of the adapters.","['https://openalex.org/W2766219058', 'https://openalex.org/W2327501763', 'https://openalex.org/W2102113734', 'https://openalex.org/W6623517193', 'https://openalex.org/W2962824709', 'https://openalex.org/W4221161761', 'https://openalex.org/W2982223350', 'https://openalex.org/W3016011332', 'https://openalex.org/W3160345865', 'https://openalex.org/W3100270690', 'https://openalex.org/W3035202887', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W3198608154', 'https://openalex.org/W3096485810', 'https://openalex.org/W6780361010', 'https://openalex.org/W3209984917', 'https://openalex.org/W3015356564', 'https://openalex.org/W3003875258', 'https://openalex.org/W3122931219', 'https://openalex.org/W1682403713', 'https://openalex.org/W3175604467', 'https://openalex.org/W4221148459', 'https://openalex.org/W4319586905', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W3206252155', 'https://openalex.org/W3202370288', 'https://openalex.org/W4224933800', 'https://openalex.org/W3049256661', 'https://openalex.org/W4389799531', 'https://openalex.org/W3171007011', 'https://openalex.org/W3173783447', 'https://openalex.org/W179875071', 'https://openalex.org/W2950813464', 'https://openalex.org/W3016181583', 'https://openalex.org/W2616957565', 'https://openalex.org/W4312884055', 'https://openalex.org/W4390190100', 'https://openalex.org/W4385571322', 'https://openalex.org/W6739901393', 'https://openalex.org/W2194775991', 'https://openalex.org/W2908336025', 'https://openalex.org/W4226278833', 'https://openalex.org/W3015265920', 'https://openalex.org/W3026041220', 'https://openalex.org/W3160235762', 'https://openalex.org/W2962780374', 'https://openalex.org/W3041561163', 'https://openalex.org/W3155427814']",2024-01-01
https://openalex.org/W4317564564,https://doi.org/10.1109/nics56915.2022.10013370,Improving Automatic Speech Recognition for Low-Resource Language by Data Augmentation,"Automatic speech recognition (ASR) is one of the emergency tasks in human-computer interaction. There are many studies work in the field of building network architecture to deal with this task. While data augmentation was deeply discovered in computer vision, it is a big lag behind in the field of speech. Large data collection is not trivial, and in some cases it is impossible. The problem with data size is even more serious in some low-resource languages, such as Vietnamese. This study focuses on the data augmentation approach to deal with the small-size datasets to help the deep learning network better coverage in the ASR task. The experiment results on various configures of the VIVOS dataset, and two variations of the Conformer network architecture show that our proposed method gets promising improvement.","['https://openalex.org/W6640090968', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W3198214208', 'https://openalex.org/W3201468377', 'https://openalex.org/W3163605596', 'https://openalex.org/W4226278833', 'https://openalex.org/W2407080277', 'https://openalex.org/W3203483482', 'https://openalex.org/W2696967604', 'https://openalex.org/W3109486980', 'https://openalex.org/W3080135102', 'https://openalex.org/W2936774411', 'https://openalex.org/W3196824004', 'https://openalex.org/W3197976714', 'https://openalex.org/W3015654635', 'https://openalex.org/W1899504021', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972451902', 'https://openalex.org/W6670489057', 'https://openalex.org/W1828163288', 'https://openalex.org/W4288088457', 'https://openalex.org/W3035682985']",2022-10-31
https://openalex.org/W4408353174,https://doi.org/10.1109/icassp49660.2025.10888110,XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models,"Self-supervised pretrained models exhibit competitive performance in automatic speech recognition (ASR) on finetuning, even with limited in-domain supervised data. However, popular pretrained models are not suitable for streaming ASR because they are trained with full attention context. In this paper, we introduce XLSR-Transducer, where the XLSR-53 model is used as encoder in transducer setup. Our experiments on the AMI dataset reveal that the XLSR-Transducer achieves 4% absolute WER improvement over Whisper large-v2 and 8% over a Zipformer transducer model trained from scratch. To enable streaming capabilities, we investigate different attention masking patterns in the self-attention computation of transformer layers within the XLSR-53 model. We validate XLSR-Transducer on AMI and 5 languages from CommonVoice under low-resource scenarios. Finally, with the introduction of attention sinks, we reduce the left context by half while achieving a relative 12% improvement in WER.","['https://openalex.org/W6847363464', 'https://openalex.org/W2766219058', 'https://openalex.org/W4385822820', 'https://openalex.org/W4388017359', 'https://openalex.org/W108866686', 'https://openalex.org/W2144499799', 'https://openalex.org/W4283700324', 'https://openalex.org/W4385245566', 'https://openalex.org/W6769806307', 'https://openalex.org/W3016010032', 'https://openalex.org/W3160766462', 'https://openalex.org/W4392902568', 'https://openalex.org/W3015927303', 'https://openalex.org/W3198429080', 'https://openalex.org/W6852909395', 'https://openalex.org/W3015237657', 'https://openalex.org/W2903601559', 'https://openalex.org/W2972981541', 'https://openalex.org/W4226278833', 'https://openalex.org/W4226033575', 'https://openalex.org/W3205644108', 'https://openalex.org/W3213618310', 'https://openalex.org/W6810259195', 'https://openalex.org/W4375869390', 'https://openalex.org/W6857690716', 'https://openalex.org/W6810673746', 'https://openalex.org/W3015686596', 'https://openalex.org/W2064675550', 'https://openalex.org/W3097777922', 'https://openalex.org/W6857062747', 'https://openalex.org/W4391021542', 'https://openalex.org/W3198484663', 'https://openalex.org/W6781533629', 'https://openalex.org/W4401042914', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125336414', 'https://openalex.org/W6771467084', 'https://openalex.org/W2933138175', 'https://openalex.org/W6631190155', 'https://openalex.org/W4404782769']",2025-03-12
https://openalex.org/W4385573456,https://doi.org/10.18653/v1/2022.findings-emnlp.81,Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings,"Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5 0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.","['https://openalex.org/W2939710050', 'https://openalex.org/W3035524453', 'https://openalex.org/W3173783447', 'https://openalex.org/W3197259906', 'https://openalex.org/W3213018012', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963250244', 'https://openalex.org/W3197876970', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287854499', 'https://openalex.org/W4320194748', 'https://openalex.org/W2126400076', 'https://openalex.org/W2802557066', 'https://openalex.org/W2963341956', 'https://openalex.org/W4226278833', 'https://openalex.org/W2970641574', 'https://openalex.org/W4319862670', 'https://openalex.org/W2462305634', 'https://openalex.org/W3101066076', 'https://openalex.org/W2133458109', 'https://openalex.org/W2962980711', 'https://openalex.org/W3156636935', 'https://openalex.org/W3169320628', 'https://openalex.org/W3104033643', 'https://openalex.org/W4246183800', 'https://openalex.org/W2100768664', 'https://openalex.org/W2110625382', 'https://openalex.org/W3093096176', 'https://openalex.org/W2167338739', 'https://openalex.org/W3198217962', 'https://openalex.org/W3036120435', 'https://openalex.org/W4300047444', 'https://openalex.org/W3030437843', 'https://openalex.org/W4297808394', 'https://openalex.org/W3198266945', 'https://openalex.org/W2171019095', 'https://openalex.org/W4286359908', 'https://openalex.org/W2251861449', 'https://openalex.org/W2347098582', 'https://openalex.org/W4226507725', 'https://openalex.org/W2973049979', 'https://openalex.org/W2997574889', 'https://openalex.org/W2794557536', 'https://openalex.org/W3095361818', 'https://openalex.org/W3118062200', 'https://openalex.org/W1496120315', 'https://openalex.org/W4296070387', 'https://openalex.org/W4287890956', 'https://openalex.org/W2962736743', 'https://openalex.org/W4292825791', 'https://openalex.org/W4296069143', 'https://openalex.org/W3140429000', 'https://openalex.org/W3096918678', 'https://openalex.org/W114193738', 'https://openalex.org/W2979826702', 'https://openalex.org/W2889313720', 'https://openalex.org/W2786608204', 'https://openalex.org/W4287173589', 'https://openalex.org/W2963918774', 'https://openalex.org/W3180374548', 'https://openalex.org/W4286959591', 'https://openalex.org/W2963288440', 'https://openalex.org/W4224313754', 'https://openalex.org/W2973026522', 'https://openalex.org/W2153962611', 'https://openalex.org/W2120375264', 'https://openalex.org/W2931212643', 'https://openalex.org/W4287887366', 'https://openalex.org/W3197580070', 'https://openalex.org/W3210177631', 'https://openalex.org/W1778492285', 'https://openalex.org/W2996428491', 'https://openalex.org/W2963804993', 'https://openalex.org/W3213029956', 'https://openalex.org/W2152180407', 'https://openalex.org/W3209984917', 'https://openalex.org/W2965373594', 'https://openalex.org/W2190506272', 'https://openalex.org/W4200635076', 'https://openalex.org/W2015688877']",2022-01-01
https://openalex.org/W4386076575,https://doi.org/10.1109/cvpr52729.2023.00251,Gloss Attention for Gloss-free Sign Language Translation,"Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.","['https://openalex.org/W2759302818', 'https://openalex.org/W2157331557', 'https://openalex.org/W2970641574', 'https://openalex.org/W2746301562', 'https://openalex.org/W2963918774', 'https://openalex.org/W2962784628', 'https://openalex.org/W6898505805', 'https://openalex.org/W3045196480', 'https://openalex.org/W2896323483', 'https://openalex.org/W6846322403', 'https://openalex.org/W6766978945', 'https://openalex.org/W6755207826', 'https://openalex.org/W2908497602', 'https://openalex.org/W6631943919', 'https://openalex.org/W3205840949', 'https://openalex.org/W3127711005', 'https://openalex.org/W2112382942', 'https://openalex.org/W2951456627', 'https://openalex.org/W1902237438', 'https://openalex.org/W3147467731', 'https://openalex.org/W6675921673', 'https://openalex.org/W6784631112', 'https://openalex.org/W3034269985', 'https://openalex.org/W3206578098', 'https://openalex.org/W2108325777', 'https://openalex.org/W6601502966', 'https://openalex.org/W6639432524', 'https://openalex.org/W3034765865', 'https://openalex.org/W4312396348', 'https://openalex.org/W6776048684', 'https://openalex.org/W6679434410', 'https://openalex.org/W2799020610', 'https://openalex.org/W3009828227', 'https://openalex.org/W2954798773', 'https://openalex.org/W6629028937', 'https://openalex.org/W2970756316', 'https://openalex.org/W2941870244', 'https://openalex.org/W4304083163', 'https://openalex.org/W3206487321', 'https://openalex.org/W6631190155', 'https://openalex.org/W4285216477', 'https://openalex.org/W6743384090', 'https://openalex.org/W6679775712', 'https://openalex.org/W2963250244', 'https://openalex.org/W2997931247', 'https://openalex.org/W3173290664', 'https://openalex.org/W1506441995', 'https://openalex.org/W3114337930', 'https://openalex.org/W6745159025', 'https://openalex.org/W3205234797', 'https://openalex.org/W6846143095', 'https://openalex.org/W6781533629', 'https://openalex.org/W6682889407', 'https://openalex.org/W3202850984', 'https://openalex.org/W6739901393', 'https://openalex.org/W2194775991', 'https://openalex.org/W4312878209', 'https://openalex.org/W4363672350', 'https://openalex.org/W4294533720', 'https://openalex.org/W6838789019', 'https://openalex.org/W4304099317', 'https://openalex.org/W6638667902', 'https://openalex.org/W2146221819', 'https://openalex.org/W6756891207', 'https://openalex.org/W6686164453', 'https://openalex.org/W4299589691', 'https://openalex.org/W1522301498', 'https://openalex.org/W4306175879', 'https://openalex.org/W4308164075', 'https://openalex.org/W36434594', 'https://openalex.org/W3109271037', 'https://openalex.org/W4386075791', 'https://openalex.org/W1533861849', 'https://openalex.org/W4287704453', 'https://openalex.org/W2104874772', 'https://openalex.org/W2133564696', 'https://openalex.org/W2156387975', 'https://openalex.org/W2903314716', 'https://openalex.org/W1836465849', 'https://openalex.org/W3126451397', 'https://openalex.org/W2896457183', 'https://openalex.org/W3093061880', 'https://openalex.org/W1889081078', 'https://openalex.org/W2183341477', 'https://openalex.org/W4295312788', 'https://openalex.org/W4385245566', 'https://openalex.org/W2131744502', 'https://openalex.org/W2762022354', 'https://openalex.org/W3015468748', 'https://openalex.org/W4281789500', 'https://openalex.org/W2752172973']",2023-06-01
https://openalex.org/W3206191467,https://doi.org/10.1145/3503161.3547854,SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation,"Deep generative models have achieved significant progress in speech synthesis to date, while high-fidelity singing voice synthesis is still an open problem for its long continuous pronunciation, rich high-frequency parts, and strong expressiveness. Existing neural vocoders designed for text-to-speech cannot directly be applied to singing voice synthesis because they result in glitches and poor high-frequency reconstruction. In this work, we propose SingGAN, a generative adversarial network designed for high-fidelity singing voice synthesis. Specifically, 1) to alleviate the glitch problem in the generated samples, we propose source excitation with the adaptive feature learning filters to expand the receptive field patterns and stabilize long continuous signal generation; and 2) SingGAN introduces global and local discriminators at different scales to enrich low-frequency details and promote high-frequency reconstruction; and 3) To improve the training efficiency, SingGAN includes auxiliary spectrogram losses and sub-band feature matching penalty loss. To the best of our knowledge, SingGAN is the first work designed toward high-fidelity singing voice vocoding. Our evaluation of SingGAN demonstrates the state-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN enables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti GPU. We further show that SingGAN generalizes well to the mel-spectrogram inversion of unseen singers, and the end-to-end singing voice synthesis system SingGAN-SVS enjoys a two-stage pipeline to transform the music scores into expressive singing voices. Audio samples are available at \url{https://SingGAN.github.io/}","['https://openalex.org/W4285345683', 'https://openalex.org/W2107860279', 'https://openalex.org/W2067295501', 'https://openalex.org/W2990440871', 'https://openalex.org/W3015338123', 'https://openalex.org/W3144035034', 'https://openalex.org/W3103104054', 'https://openalex.org/W3100054454', 'https://openalex.org/W3045748506', 'https://openalex.org/W4214912006', 'https://openalex.org/W2971753973', 'https://openalex.org/W4287184558', 'https://openalex.org/W4280542470', 'https://openalex.org/W2949382160', 'https://openalex.org/W1552314771', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964243274', 'https://openalex.org/W3167318608', 'https://openalex.org/W4287672314', 'https://openalex.org/W3130016944', 'https://openalex.org/W2970730223', 'https://openalex.org/W3092028330', 'https://openalex.org/W2903739847', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963300588', 'https://openalex.org/W3081279708', 'https://openalex.org/W4298580827', 'https://openalex.org/W3205398360', 'https://openalex.org/W3158762648', 'https://openalex.org/W3162673269', 'https://openalex.org/W4294619240', 'https://openalex.org/W3172148458', 'https://openalex.org/W3019084079', 'https://openalex.org/W3048423403', 'https://openalex.org/W3128910262', 'https://openalex.org/W4303519914', 'https://openalex.org/W3169635929', 'https://openalex.org/W2160473997', 'https://openalex.org/W2788851830', 'https://openalex.org/W3082910224', 'https://openalex.org/W4289361892', 'https://openalex.org/W2970997853', 'https://openalex.org/W3097566756', 'https://openalex.org/W3129651364', 'https://openalex.org/W3021066808', 'https://openalex.org/W3168527213', 'https://openalex.org/W4224309908', 'https://openalex.org/W3015516707', 'https://openalex.org/W3133525064', 'https://openalex.org/W3101119695', 'https://openalex.org/W3033411150', 'https://openalex.org/W3123097577', 'https://openalex.org/W3026874504', 'https://openalex.org/W3046970875', 'https://openalex.org/W4287117308', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015499232', 'https://openalex.org/W4281789500']",2022-10-10
https://openalex.org/W4385570550,https://doi.org/10.18653/v1/2023.acl-long.872,UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units,"Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W3193521535', 'https://openalex.org/W2972495969', 'https://openalex.org/W4223622550', 'https://openalex.org/W2963799213', 'https://openalex.org/W2752047430', 'https://openalex.org/W3186200218', 'https://openalex.org/W3006381853', 'https://openalex.org/W2949328740', 'https://openalex.org/W4307770080', 'https://openalex.org/W2292087804', 'https://openalex.org/W2250357346', 'https://openalex.org/W2161192091', 'https://openalex.org/W3142316150', 'https://openalex.org/W3035390927', 'https://openalex.org/W4226033575', 'https://openalex.org/W2133564696', 'https://openalex.org/W3140429000', 'https://openalex.org/W2972625221', 'https://openalex.org/W2933138175', 'https://openalex.org/W4372349107', 'https://openalex.org/W2963779652', 'https://openalex.org/W2972802841', 'https://openalex.org/W3168212167', 'https://openalex.org/W3107826490', 'https://openalex.org/W3095410713', 'https://openalex.org/W2763421725', 'https://openalex.org/W4280617721', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287333042', 'https://openalex.org/W4285188582', 'https://openalex.org/W2958953787', 'https://openalex.org/W2605131327', 'https://openalex.org/W3175301726', 'https://openalex.org/W3175871055', 'https://openalex.org/W2972448360', 'https://openalex.org/W4287072252', 'https://openalex.org/W2136545725', 'https://openalex.org/W3011339933', 'https://openalex.org/W2938973646', 'https://openalex.org/W4285158119', 'https://openalex.org/W2572474373', 'https://openalex.org/W4221155340', 'https://openalex.org/W4226444650', 'https://openalex.org/W3103182178', 'https://openalex.org/W3172862365', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385893869', 'https://openalex.org/W3135335819', 'https://openalex.org/W3169320628', 'https://openalex.org/W3091928890', 'https://openalex.org/W3176711365', 'https://openalex.org/W3196509775', 'https://openalex.org/W3007068036', 'https://openalex.org/W2995181338', 'https://openalex.org/W3015698636', 'https://openalex.org/W4385245566', 'https://openalex.org/W3173767661', 'https://openalex.org/W2130942839', 'https://openalex.org/W3119308075', 'https://openalex.org/W2964161387', 'https://openalex.org/W2183341477', 'https://openalex.org/W2046932483', 'https://openalex.org/W4300558631', 'https://openalex.org/W22168010', 'https://openalex.org/W2963250244', 'https://openalex.org/W1538023239', 'https://openalex.org/W4287213456', 'https://openalex.org/W3139878283', 'https://openalex.org/W2936969148', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963979492', 'https://openalex.org/W2806412155', 'https://openalex.org/W3092028330', 'https://openalex.org/W2157331557', 'https://openalex.org/W3012492057', 'https://openalex.org/W2973122799', 'https://openalex.org/W4287694131', 'https://openalex.org/W2963887123', 'https://openalex.org/W3139918052', 'https://openalex.org/W2903739847', 'https://openalex.org/W2095705004', 'https://openalex.org/W4226054021', 'https://openalex.org/W3100806282', 'https://openalex.org/W4281789500', 'https://openalex.org/W3174864715', 'https://openalex.org/W4287854499', 'https://openalex.org/W4221153524', 'https://openalex.org/W4296070387', 'https://openalex.org/W2963532001', 'https://openalex.org/W338621447', 'https://openalex.org/W3118578889']",2023-01-01
https://openalex.org/W4385571229,https://doi.org/10.18653/v1/2023.findings-acl.307,Speech-to-Speech Translation for a Real-world Unwritten Language,"Peng-Jen Chen, Kevin Tran, Yilin Yang, Jingfei Du, Justine Kao, Yu-An Chung, Paden Tomasello, Paul-Ambroise Duquenne, Holger Schwenk, Hongyu Gong, Hirofumi Inaguma, Sravya Popuri, Changhan Wang, Juan Pino, Wei-Ning Hsu, Ann Lee. Findings of the Association for Computational Linguistics: ACL 2023. 2023.","['https://openalex.org/W3193521535', 'https://openalex.org/W2933138175', 'https://openalex.org/W4308756394', 'https://openalex.org/W2972495969', 'https://openalex.org/W2949328740', 'https://openalex.org/W3030437843', 'https://openalex.org/W4287854499', 'https://openalex.org/W4221153524', 'https://openalex.org/W2963250244', 'https://openalex.org/W3196509775', 'https://openalex.org/W3210177631', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963532001', 'https://openalex.org/W4288025890', 'https://openalex.org/W3101648800', 'https://openalex.org/W630532510', 'https://openalex.org/W3213018012', 'https://openalex.org/W4307770080', 'https://openalex.org/W2995181338', 'https://openalex.org/W3117789470', 'https://openalex.org/W2250600805', 'https://openalex.org/W2121016490', 'https://openalex.org/W2945700568', 'https://openalex.org/W4281789500', 'https://openalex.org/W3120929527', 'https://openalex.org/W4287072252', 'https://openalex.org/W3175871055', 'https://openalex.org/W2975381464', 'https://openalex.org/W4226033575', 'https://openalex.org/W4226444650', 'https://openalex.org/W2136545725', 'https://openalex.org/W4291566970', 'https://openalex.org/W3119308075', 'https://openalex.org/W3142316150', 'https://openalex.org/W2914120296', 'https://openalex.org/W3092085609', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385570550', 'https://openalex.org/W2973088264', 'https://openalex.org/W4296070387', 'https://openalex.org/W4367841185', 'https://openalex.org/W3035390927', 'https://openalex.org/W3097777922', 'https://openalex.org/W3015698636', 'https://openalex.org/W3007068036', 'https://openalex.org/W3107826490', 'https://openalex.org/W3169320628', 'https://openalex.org/W3174864715', 'https://openalex.org/W3036601975']",2023-01-01
https://openalex.org/W4372349107,https://doi.org/10.1109/icassp49357.2023.10096797,Textless Direct Speech-to-Speech Translation with Discrete Speech Representation,"Research on speech-to-speech translation (S2ST) has progressed rapidly in recent years. Many end-to-end systems have been proposed and show advantages over conventional cascade systems, which are often composed of recognition, translation and synthesis sub-systems. However, most of end-to-end systems still rely on intermediate textual supervision during training, which makes it infeasible to work for languages without written forms. In this work, we propose a novel model, Textless Translatotron, which is based on Translatotron 2 [1], for training an end-to-end direct S2ST model without any textual supervision. Instead of jointly training with an auxiliary task predicting target phonemes as in Translatotron 2, the proposed model uses an auxiliary task predicting discrete speech representations which are obtained from learned or random speech quantizers. When a speech encoder pre-trained with unsupervised speech data is used for both models, the proposed model obtains translation quality nearly on-par with Translatotron 2 on the multilingual CVSS-C corpus [2] as well as the bilingual Fisher Spanish-English corpus [3]. On the latter, it outperforms the prior state-of-the-art textless model by +18.5 BLEU.","['https://openalex.org/W4287848452', 'https://openalex.org/W4280601369', 'https://openalex.org/W3175871055', 'https://openalex.org/W3007068036', 'https://openalex.org/W3026041220', 'https://openalex.org/W6760633627', 'https://openalex.org/W4221153524', 'https://openalex.org/W3142316150', 'https://openalex.org/W3213029956', 'https://openalex.org/W6810701745', 'https://openalex.org/W6841035593', 'https://openalex.org/W6802857679', 'https://openalex.org/W3180374548', 'https://openalex.org/W6838789019', 'https://openalex.org/W4287854499', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W6810673746', 'https://openalex.org/W4226033575', 'https://openalex.org/W4296070387', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W6784333009', 'https://openalex.org/W6810259195', 'https://openalex.org/W6631943919', 'https://openalex.org/W4285181910', 'https://openalex.org/W4296069143', 'https://openalex.org/W2972495969', 'https://openalex.org/W3037465386', 'https://openalex.org/W4388277788', 'https://openalex.org/W3207738474', 'https://openalex.org/W60702959', 'https://openalex.org/W3094502228', 'https://openalex.org/W1533861849', 'https://openalex.org/W4221161761', 'https://openalex.org/W4281789500', 'https://openalex.org/W2928941594', 'https://openalex.org/W4226444650', 'https://openalex.org/W4297808394', 'https://openalex.org/W4221155340', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287072252', 'https://openalex.org/W3036601975', 'https://openalex.org/W3186200218']",2023-05-05
https://openalex.org/W4390873467,https://doi.org/10.1109/iccv51070.2023.01442,MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition,"Multi-media communications facilitate global interaction among people. However, despite researchers exploring cross-lingual translation techniques such as machine translation and audio speech translation to overcome language barriers, there is still a shortage of cross-lingual studies on visual speech. This lack of research is mainly due to the absence of datasets containing visual speech and translated text pairs. In this paper, we present AVMuST-TED, the first dataset for Audio-Visual Multilingual Speech Translation, derived from TED talks. Nonetheless, visual speech is not as distinguishable as audio speech, making it difficult to develop a mapping from source speech phonemes to the target language text. To address this issue, we propose MixSpeech, a cross-modality self-learning framework that utilizes audio speech to regularize the training of visual speech tasks. To further minimize the cross-modality gap and its impact on knowledge transfer, we suggest adopting mixed speech, which is created by interpolating audio and visual streams, along with a curriculum learning strategy to adjust the mixing ratio as needed. MixSpeech enhances speech translation in noisy environments, improving BLEU scores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves state-of-the-art performance in lip reading on CMLR (11.1%), LRS2 (25.5%), and LRS3 (28.0%).","['https://openalex.org/W2890952074', 'https://openalex.org/W6754420807', 'https://openalex.org/W3015830103', 'https://openalex.org/W6732872814', 'https://openalex.org/W2578392894', 'https://openalex.org/W6780218876', 'https://openalex.org/W2296073425', 'https://openalex.org/W2582956876', 'https://openalex.org/W2035301451', 'https://openalex.org/W3035542229', 'https://openalex.org/W4385571016', 'https://openalex.org/W2808631503', 'https://openalex.org/W2604379605', 'https://openalex.org/W2015143272', 'https://openalex.org/W4285077564', 'https://openalex.org/W4280601369', 'https://openalex.org/W4221155857', 'https://openalex.org/W4221163209', 'https://openalex.org/W2891021639', 'https://openalex.org/W2029199293', 'https://openalex.org/W3209059054', 'https://openalex.org/W6803063772', 'https://openalex.org/W4285605725', 'https://openalex.org/W4385570538', 'https://openalex.org/W6846143095', 'https://openalex.org/W4304099317', 'https://openalex.org/W6838789019', 'https://openalex.org/W6769728370', 'https://openalex.org/W6677618333', 'https://openalex.org/W3175779516', 'https://openalex.org/W3180374548', 'https://openalex.org/W4385571670', 'https://openalex.org/W2076029968', 'https://openalex.org/W3206578098', 'https://openalex.org/W3001434439', 'https://openalex.org/W2972448360', 'https://openalex.org/W3197567540', 'https://openalex.org/W3080253586', 'https://openalex.org/W2056716515', 'https://openalex.org/W6782374147', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963532001', 'https://openalex.org/W4312638101', 'https://openalex.org/W3081492798', 'https://openalex.org/W6791353385', 'https://openalex.org/W3167917117', 'https://openalex.org/W6604367424', 'https://openalex.org/W2697044473', 'https://openalex.org/W6810168380', 'https://openalex.org/W4297841641', 'https://openalex.org/W6688816777', 'https://openalex.org/W2551572271', 'https://openalex.org/W4312955540', 'https://openalex.org/W2605202026', 'https://openalex.org/W3162037819', 'https://openalex.org/W4385478049', 'https://openalex.org/W4385572710', 'https://openalex.org/W3119308075', 'https://openalex.org/W6839213706', 'https://openalex.org/W6852879717', 'https://openalex.org/W2897492880', 'https://openalex.org/W3196833881', 'https://openalex.org/W4307823382', 'https://openalex.org/W6784078097', 'https://openalex.org/W2981501041', 'https://openalex.org/W2999528291', 'https://openalex.org/W2996970093', 'https://openalex.org/W2219249508', 'https://openalex.org/W3082089446', 'https://openalex.org/W2115252128', 'https://openalex.org/W2949328740', 'https://openalex.org/W4298112588', 'https://openalex.org/W2978223337', 'https://openalex.org/W3101631197', 'https://openalex.org/W2891205112', 'https://openalex.org/W3036601975', 'https://openalex.org/W3212132290', 'https://openalex.org/W4306175879', 'https://openalex.org/W3125645205', 'https://openalex.org/W4281789500', 'https://openalex.org/W4378513185', 'https://openalex.org/W2987228815', 'https://openalex.org/W4221153068']",2023-10-01
https://openalex.org/W4392903062,https://doi.org/10.1109/icassp48485.2024.10448426,Translatotron 3: Speech to Speech Translation with Monolingual Data,"This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.","['https://openalex.org/W6841035593', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963216553', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W4313156423', 'https://openalex.org/W2936774411', 'https://openalex.org/W6744957266', 'https://openalex.org/W3175871055', 'https://openalex.org/W4287854499', 'https://openalex.org/W6838789019', 'https://openalex.org/W4223622550', 'https://openalex.org/W4280601369', 'https://openalex.org/W3142316150', 'https://openalex.org/W4385893869', 'https://openalex.org/W6847247304', 'https://openalex.org/W6845958605', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226033575', 'https://openalex.org/W4307323391', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3007068036', 'https://openalex.org/W6853998256', 'https://openalex.org/W4372349107', 'https://openalex.org/W6855650468', 'https://openalex.org/W4372191700', 'https://openalex.org/W4385570101', 'https://openalex.org/W3205644108', 'https://openalex.org/W4285112515', 'https://openalex.org/W4392979802', 'https://openalex.org/W2741602058', 'https://openalex.org/W2964266061', 'https://openalex.org/W2899134946', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964161387', 'https://openalex.org/W6771467084', 'https://openalex.org/W4319862245', 'https://openalex.org/W6765510844', 'https://openalex.org/W6846600677', 'https://openalex.org/W3197324626', 'https://openalex.org/W6784545093', 'https://openalex.org/W6748409065', 'https://openalex.org/W6810701745', 'https://openalex.org/W3196509775', 'https://openalex.org/W4385970143', 'https://openalex.org/W4226444650', 'https://openalex.org/W4381827575', 'https://openalex.org/W4307783813', 'https://openalex.org/W3091928890', 'https://openalex.org/W4299579390', 'https://openalex.org/W3036601975', 'https://openalex.org/W4298393544', 'https://openalex.org/W4281789500', 'https://openalex.org/W4385570550']",2024-03-18
https://openalex.org/W4389519221,https://doi.org/10.18653/v1/2023.emnlp-main.990,ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer,"Text-to-speech(TTS) has undergone remarkable improvements in performance, particularly with the advent of Denoising Diffusion Probabilistic Models (DDPMs). However, the perceived quality of audio depends not solely on its content, pitch, rhythm, and energy, but also on the physical environment.In this work, we propose ViT-TTS, the first visual TTS model with scalable diffusion transformers. ViT-TTS complement the phoneme sequence with the visual information to generate high-perceived audio, opening up new avenues for practical applications of AR and VR to allow a more immersive and realistic audio experience. To mitigate the data scarcity in learning visual acoustic information, we 1) introduce a self-supervised learning framework to enhance both the visual-text encoder and denoiser decoder; 2) leverage the diffusion transformer scalable in terms of parameters and capacity to learn visual scene information. Experimental results demonstrate that ViT-TTS achieves new state-of-the-art results, outperforming cascaded systems and other baselines regardless of the visibility of the scene. With low-resource data (1h, 2h, 5h), ViT-TTS achieves comparative results with rich-resource baselines.","['https://openalex.org/W3193521535', 'https://openalex.org/W4318718996', 'https://openalex.org/W4320831569', 'https://openalex.org/W3036167779', 'https://openalex.org/W3036601975', 'https://openalex.org/W4390872297', 'https://openalex.org/W4297808394', 'https://openalex.org/W1997836753', 'https://openalex.org/W2972677740', 'https://openalex.org/W3193411928', 'https://openalex.org/W4287824654', 'https://openalex.org/W1901129140', 'https://openalex.org/W3163735648', 'https://openalex.org/W2058751961', 'https://openalex.org/W3172148458', 'https://openalex.org/W3033411150', 'https://openalex.org/W4324319985', 'https://openalex.org/W4281789500', 'https://openalex.org/W3097930518', 'https://openalex.org/W4377864468', 'https://openalex.org/W2946200149', 'https://openalex.org/W4251733995', 'https://openalex.org/W4385570538', 'https://openalex.org/W4287121924', 'https://openalex.org/W3166396011', 'https://openalex.org/W3034999214', 'https://openalex.org/W4226348722', 'https://openalex.org/W3169320628', 'https://openalex.org/W2764284591', 'https://openalex.org/W4285483538', 'https://openalex.org/W4312779270', 'https://openalex.org/W2963609956', 'https://openalex.org/W4287761884', 'https://openalex.org/W4372348432', 'https://openalex.org/W3035202887', 'https://openalex.org/W4385245566', 'https://openalex.org/W4281736089', 'https://openalex.org/W4303519914', 'https://openalex.org/W2952716587', 'https://openalex.org/W3092028330', 'https://openalex.org/W4226182655', 'https://openalex.org/W2898519127', 'https://openalex.org/W2903739847', 'https://openalex.org/W2757091337', 'https://openalex.org/W4226033575', 'https://openalex.org/W4318718936', 'https://openalex.org/W2896457183', 'https://openalex.org/W4367061106', 'https://openalex.org/W2519091744']",2023-01-01
https://openalex.org/W4391089694,https://doi.org/10.1016/j.eswa.2024.123317,Speaker voice normalization for end-to-end speech translation,,"['https://openalex.org/W6780218876', 'https://openalex.org/W6800638409', 'https://openalex.org/W6800972147', 'https://openalex.org/W3209984917', 'https://openalex.org/W2973048981', 'https://openalex.org/W3113908264', 'https://openalex.org/W4226212120', 'https://openalex.org/W2466918907', 'https://openalex.org/W4221163209', 'https://openalex.org/W6845153935', 'https://openalex.org/W6795597983', 'https://openalex.org/W3209059054', 'https://openalex.org/W6776039324', 'https://openalex.org/W222053410', 'https://openalex.org/W2963250244', 'https://openalex.org/W6761144991', 'https://openalex.org/W2933138175', 'https://openalex.org/W6839738141', 'https://openalex.org/W6776312876', 'https://openalex.org/W3176711365', 'https://openalex.org/W6739901393', 'https://openalex.org/W6756627502', 'https://openalex.org/W6784050962', 'https://openalex.org/W6800268142', 'https://openalex.org/W2997436923', 'https://openalex.org/W3176382501', 'https://openalex.org/W6793534603', 'https://openalex.org/W4287890956', 'https://openalex.org/W6839510803', 'https://openalex.org/W3017454464', 'https://openalex.org/W6803403586', 'https://openalex.org/W2593831809', 'https://openalex.org/W3105669983', 'https://openalex.org/W2963532001', 'https://openalex.org/W3206375275', 'https://openalex.org/W4234465017', 'https://openalex.org/W2964172053', 'https://openalex.org/W2915722758', 'https://openalex.org/W4385570159', 'https://openalex.org/W4285215858', 'https://openalex.org/W2952167535', 'https://openalex.org/W4205616158', 'https://openalex.org/W4385571996', 'https://openalex.org/W4287854499', 'https://openalex.org/W3114896262', 'https://openalex.org/W3183859557', 'https://openalex.org/W2963779652', 'https://openalex.org/W4281789500', 'https://openalex.org/W3097301532', 'https://openalex.org/W2936848022', 'https://openalex.org/W2599674900', 'https://openalex.org/W3036601975']",2024-01-22
https://openalex.org/W4402716129,https://doi.org/10.1109/cvpr52733.2024.02580,AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation,,"['https://openalex.org/W2963216553', 'https://openalex.org/W2550821151', 'https://openalex.org/W3089472875', 'https://openalex.org/W3001434439', 'https://openalex.org/W6784447870', 'https://openalex.org/W4285077564', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6810419249', 'https://openalex.org/W4385570550', 'https://openalex.org/W6855650468', 'https://openalex.org/W2015394094', 'https://openalex.org/W2594690981', 'https://openalex.org/W4392669909', 'https://openalex.org/W6846720623', 'https://openalex.org/W4385478049', 'https://openalex.org/W4387251351', 'https://openalex.org/W2981767644', 'https://openalex.org/W6687566353', 'https://openalex.org/W3163793923', 'https://openalex.org/W4224319127', 'https://openalex.org/W3097030750', 'https://openalex.org/W6749489859', 'https://openalex.org/W6778823374', 'https://openalex.org/W4225746985', 'https://openalex.org/W3081492798', 'https://openalex.org/W4283818626', 'https://openalex.org/W3035016936', 'https://openalex.org/W2760656271', 'https://openalex.org/W3169369929', 'https://openalex.org/W3156404059', 'https://openalex.org/W4385822729', 'https://openalex.org/W3196509775', 'https://openalex.org/W3119308075', 'https://openalex.org/W3015698636', 'https://openalex.org/W6847363464', 'https://openalex.org/W3008125272', 'https://openalex.org/W3008549139', 'https://openalex.org/W6787407267', 'https://openalex.org/W3007068036', 'https://openalex.org/W6841035593', 'https://openalex.org/W4296070387', 'https://openalex.org/W2963609956', 'https://openalex.org/W4285189120', 'https://openalex.org/W6805710207', 'https://openalex.org/W3197199219', 'https://openalex.org/W3186090335', 'https://openalex.org/W4386072021', 'https://openalex.org/W4312095900', 'https://openalex.org/W6801536170', 'https://openalex.org/W6810168380', 'https://openalex.org/W6839936984', 'https://openalex.org/W4390873467', 'https://openalex.org/W4385571016', 'https://openalex.org/W6790356757', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2964161387', 'https://openalex.org/W6838789019', 'https://openalex.org/W4385570538', 'https://openalex.org/W2890952074', 'https://openalex.org/W3162293946', 'https://openalex.org/W4297841411', 'https://openalex.org/W4386071467', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W3213029956', 'https://openalex.org/W3209984917', 'https://openalex.org/W6802465204', 'https://openalex.org/W4221153524', 'https://openalex.org/W4385822683', 'https://openalex.org/W6857524042', 'https://openalex.org/W6856719735', 'https://openalex.org/W4392904292', 'https://openalex.org/W4307680525', 'https://openalex.org/W6847652939', 'https://openalex.org/W4376481237', 'https://openalex.org/W4390874021', 'https://openalex.org/W4297841641', 'https://openalex.org/W4386076005', 'https://openalex.org/W4385823403', 'https://openalex.org/W2551572271', 'https://openalex.org/W6754420807', 'https://openalex.org/W2808631503', 'https://openalex.org/W3197771105', 'https://openalex.org/W4289665794', 'https://openalex.org/W4385245566', 'https://openalex.org/W6783867762', 'https://openalex.org/W2972909277', 'https://openalex.org/W3024869864', 'https://openalex.org/W2962788625', 'https://openalex.org/W6752888775', 'https://openalex.org/W2046056978', 'https://openalex.org/W6796464841', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963532001', 'https://openalex.org/W6765779288', 'https://openalex.org/W3034552680', 'https://openalex.org/W2949662773', 'https://openalex.org/W6688816777', 'https://openalex.org/W6850334629', 'https://openalex.org/W4307286264', 'https://openalex.org/W3180391059', 'https://openalex.org/W3201519611', 'https://openalex.org/W3033411150', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963691546', 'https://openalex.org/W3205644108', 'https://openalex.org/W4392904805', 'https://openalex.org/W3104792420', 'https://openalex.org/W4323651091', 'https://openalex.org/W4392909068', 'https://openalex.org/W4285595742', 'https://openalex.org/W3092028330', 'https://openalex.org/W4308164026', 'https://openalex.org/W4394671563', 'https://openalex.org/W2219249508', 'https://openalex.org/W3093871477', 'https://openalex.org/W3101631197', 'https://openalex.org/W2808706139', 'https://openalex.org/W4385970143', 'https://openalex.org/W3173767661', 'https://openalex.org/W2891205112', 'https://openalex.org/W4281789500', 'https://openalex.org/W1593271688', 'https://openalex.org/W4287854499', 'https://openalex.org/W1538023239', 'https://openalex.org/W4301206121']",2024-06-16
https://openalex.org/W4389519423,https://doi.org/10.18653/v1/2023.emnlp-main.709,DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation,"While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).","['https://openalex.org/W2933138175', 'https://openalex.org/W4287083626', 'https://openalex.org/W2963736842', 'https://openalex.org/W3092028330', 'https://openalex.org/W2963532001', 'https://openalex.org/W3140429000', 'https://openalex.org/W3175871055', 'https://openalex.org/W4287854499', 'https://openalex.org/W2998702515', 'https://openalex.org/W3110257065', 'https://openalex.org/W2136545725', 'https://openalex.org/W4287079508', 'https://openalex.org/W3169320628', 'https://openalex.org/W3015698636', 'https://openalex.org/W3024786184', 'https://openalex.org/W3210177631', 'https://openalex.org/W4312933868', 'https://openalex.org/W3036601975', 'https://openalex.org/W4385571229', 'https://openalex.org/W3172148458', 'https://openalex.org/W4306802991', 'https://openalex.org/W4385572318', 'https://openalex.org/W4300425011', 'https://openalex.org/W4287329820', 'https://openalex.org/W3142316150', 'https://openalex.org/W3107826490', 'https://openalex.org/W2962784628', 'https://openalex.org/W2129069237', 'https://openalex.org/W3213018012', 'https://openalex.org/W4281789500', 'https://openalex.org/W2767206889', 'https://openalex.org/W3007068036', 'https://openalex.org/W3036167779', 'https://openalex.org/W4287072252', 'https://openalex.org/W3119308075', 'https://openalex.org/W4385245566', 'https://openalex.org/W4312051726', 'https://openalex.org/W2972495969', 'https://openalex.org/W3168053944', 'https://openalex.org/W2097203679', 'https://openalex.org/W4372191700', 'https://openalex.org/W4366460484']",2023-01-01
https://openalex.org/W4406461475,https://doi.org/10.1109/slt61566.2024.10832325,Self-Supervised Syllable Discovery Based on Speaker-Disentangled Hubert,,"['https://openalex.org/W3209059054', 'https://openalex.org/W4385823277', 'https://openalex.org/W4392904409', 'https://openalex.org/W4224875474', 'https://openalex.org/W6790356757', 'https://openalex.org/W3140429000', 'https://openalex.org/W4381786045', 'https://openalex.org/W4389524500', 'https://openalex.org/W4385823059', 'https://openalex.org/W4401610816', 'https://openalex.org/W4287854499', 'https://openalex.org/W6838789019', 'https://openalex.org/W4372348373', 'https://openalex.org/W4385822740', 'https://openalex.org/W6755207826', 'https://openalex.org/W6839738141', 'https://openalex.org/W4385245566', 'https://openalex.org/W3159481202', 'https://openalex.org/W2118612506', 'https://openalex.org/W1494198834', 'https://openalex.org/W6803547063', 'https://openalex.org/W130754613', 'https://openalex.org/W2726515241', 'https://openalex.org/W1836465849', 'https://openalex.org/W6755977528', 'https://openalex.org/W6757817989', 'https://openalex.org/W6867926528', 'https://openalex.org/W4394773771', 'https://openalex.org/W4281789500', 'https://openalex.org/W4301371414', 'https://openalex.org/W2908510526', 'https://openalex.org/W2896457183', 'https://openalex.org/W4398157616', 'https://openalex.org/W4283659485', 'https://openalex.org/W4394671563', 'https://openalex.org/W2899663614']",2024-12-02
https://openalex.org/W4410215894,https://doi.org/10.32622/ijrat.131202513,Survey On Monolingual Speech-to-Speech Translation,"Direct Speech-to-Speech (S2S) translation represents a significant goal in facilitating seamless cross- lingual communication, aiming to overcome the latency and error propagation issues inherent in traditional cascaded systems (ASRMT-TTS). However, the development of high- performing direct S2S models has been critically constrained by the scarcity of large-scale parallel S2S corpora. This survey details Translatotron 3, a pivotal direct S2S system introduced by Google Research in 2023. Translatotron 3 fundamentally shifts the paradigm by demonstrating, for the first time, the feasibility of training a high-quality, end-toend S2S model exclusively using readily available monolingual data resources: source/target speech and source/target text. Leveraging innovative techniques such as unsupervised utterance splitting, phoneme-based intermediate representations, speech-adapted back-translation, and a non auto regressive decoder for rapid inference, Translatotron 3 achieves strong translation quality and remarkable speaker voice preservation without requiring any parallel S2S examples. We critically review the technological context, dissect the model’s architecture and training methodology, analyze its reported performance benchmarks, and discuss its profound implications for advancing S2S translation, particularly for the vast number of low-resource languages previously underserved by data-hungry models.","['https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2936184970', 'https://openalex.org/W4410215894', 'https://openalex.org/W3035707016', 'https://openalex.org/W6810419249', 'https://openalex.org/W4281789500', 'https://openalex.org/W4307006458', 'https://openalex.org/W4223622550', 'https://openalex.org/W4280601369', 'https://openalex.org/W4307932596', 'https://openalex.org/W6798098866', 'https://openalex.org/W3193521535', 'https://openalex.org/W6846857581', 'https://openalex.org/W3169320628', 'https://openalex.org/W6745740328', 'https://openalex.org/W2765961751', 'https://openalex.org/W2284660317', 'https://openalex.org/W2798908575', 'https://openalex.org/W6755567139', 'https://openalex.org/W6798080464', 'https://openalex.org/W2605287558', 'https://openalex.org/W3036601975', 'https://openalex.org/W3205644108', 'https://openalex.org/W4381827575', 'https://openalex.org/W6744957266', 'https://openalex.org/W4378765378']",2025-03-30
https://openalex.org/W4385570538,https://doi.org/10.18653/v1/2023.acl-long.479,AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation,"Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, Zhou Zhao. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W4306175879', 'https://openalex.org/W3193411928', 'https://openalex.org/W4287079508', 'https://openalex.org/W4285345683', 'https://openalex.org/W3154451338', 'https://openalex.org/W2890952074', 'https://openalex.org/W4285483538', 'https://openalex.org/W2972495969', 'https://openalex.org/W3119308075', 'https://openalex.org/W4287854499', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330', 'https://openalex.org/W4297841864', 'https://openalex.org/W3097777922', 'https://openalex.org/W4303519914', 'https://openalex.org/W2933138175', 'https://openalex.org/W2891205112', 'https://openalex.org/W3175871055', 'https://openalex.org/W4281789500', 'https://openalex.org/W3016011581', 'https://openalex.org/W4296070387', 'https://openalex.org/W3206191467', 'https://openalex.org/W3140429000', 'https://openalex.org/W3015830103', 'https://openalex.org/W3054645415', 'https://openalex.org/W4385571229', 'https://openalex.org/W3169320628', 'https://openalex.org/W2963528589', 'https://openalex.org/W3007068036', 'https://openalex.org/W3096086473', 'https://openalex.org/W4221153524', 'https://openalex.org/W2219249508', 'https://openalex.org/W4221153068', 'https://openalex.org/W3107826490', 'https://openalex.org/W3186843219', 'https://openalex.org/W4226444650', 'https://openalex.org/W4307934980', 'https://openalex.org/W2995181338', 'https://openalex.org/W4385245566', 'https://openalex.org/W4312638101', 'https://openalex.org/W4385572318', 'https://openalex.org/W4367061106']",2023-01-01
https://openalex.org/W4385571990,https://doi.org/10.18653/v1/2023.findings-acl.442,AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment,"The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performance in terms of both objective and subjective metrics. Audio samples are available at https://alignsts.github.io.","['https://openalex.org/W4306175879', 'https://openalex.org/W2407685581', 'https://openalex.org/W4385245566', 'https://openalex.org/W2133665775', 'https://openalex.org/W4226332109', 'https://openalex.org/W2945478979', 'https://openalex.org/W2150933458', 'https://openalex.org/W3206191467', 'https://openalex.org/W2962793481', 'https://openalex.org/W4309395027', 'https://openalex.org/W3216401400', 'https://openalex.org/W2108382268', 'https://openalex.org/W2902070858', 'https://openalex.org/W2921017930', 'https://openalex.org/W3034794073', 'https://openalex.org/W4318718996', 'https://openalex.org/W4301371414', 'https://openalex.org/W2767052532', 'https://openalex.org/W4225939199', 'https://openalex.org/W3095074555', 'https://openalex.org/W3033411150', 'https://openalex.org/W1524333225', 'https://openalex.org/W854541894', 'https://openalex.org/W3169739675', 'https://openalex.org/W2532494225', 'https://openalex.org/W3092028330', 'https://openalex.org/W2884225676', 'https://openalex.org/W2005787925', 'https://openalex.org/W4285483538', 'https://openalex.org/W4285345683', 'https://openalex.org/W2963539064', 'https://openalex.org/W2963767194', 'https://openalex.org/W3036601975', 'https://openalex.org/W4295274059', 'https://openalex.org/W266116173', 'https://openalex.org/W3028988798', 'https://openalex.org/W2963799213', 'https://openalex.org/W4303519914', 'https://openalex.org/W4367061106', 'https://openalex.org/W4281789500', 'https://openalex.org/W2163922914', 'https://openalex.org/W2946555236', 'https://openalex.org/W2989926340', 'https://openalex.org/W4221153221', 'https://openalex.org/W3005991431', 'https://openalex.org/W3214606349']",2023-01-01
https://openalex.org/W4391244308,https://doi.org/10.48550/arxiv.2401.12992,TranSentence: Speech-to-speech Translation via Language-agnostic Sentence-level Speech Encoding without Language-parallel Data,"Although there has been significant advancement in the field of speech-to-speech translation, conventional models still require language-parallel speech data between the source and target languages for training. In this paper, we introduce TranSentence, a novel speech-to-speech translation without language-parallel speech data. To achieve this, we first adopt a language-agnostic sentence-level speech encoding that captures the semantic information of speech, irrespective of language. We then train our model to generate speech based on the encoded embedding obtained from a language-agnostic sentence-level speech encoder that is pre-trained with various languages. With this method, despite training exclusively on the target language's monolingual data, we can generate target language speech in the inference stage using language-agnostic speech embedding from the source language speech. Furthermore, we extend TranSentence to multilingual speech-to-speech translation. The experimental results demonstrate that TranSentence is superior to other models.","['https://openalex.org/W4287854499', 'https://openalex.org/W3180374548', 'https://openalex.org/W4385572318', 'https://openalex.org/W4372191700', 'https://openalex.org/W4378498579', 'https://openalex.org/W4372260139', 'https://openalex.org/W4287072252', 'https://openalex.org/W2972495969', 'https://openalex.org/W3140429000', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W4385570550', 'https://openalex.org/W3092028330', 'https://openalex.org/W2997520586', 'https://openalex.org/W3030437843', 'https://openalex.org/W4392903062', 'https://openalex.org/W3198429080', 'https://openalex.org/W3036601975', 'https://openalex.org/W4281789500', 'https://openalex.org/W4372260088', 'https://openalex.org/W3209059054', 'https://openalex.org/W3213029956']",2024-01-17
https://openalex.org/W4407156189,https://doi.org/10.2139/ssrn.5123533,Preserving Speaker Information in Direct Speech-to-Speech Translation with Non-Autoregressive Generation and Pre-Training,,"['https://openalex.org/W2990025607', 'https://openalex.org/W4200631896', 'https://openalex.org/W4387596303', 'https://openalex.org/W3169320628', 'https://openalex.org/W3092028330', 'https://openalex.org/W4307537044', 'https://openalex.org/W4378765378', 'https://openalex.org/W1494198834', 'https://openalex.org/W4386655580', 'https://openalex.org/W3205770165', 'https://openalex.org/W6745415975', 'https://openalex.org/W3035707016', 'https://openalex.org/W4225956675', 'https://openalex.org/W4287854499', 'https://openalex.org/W4391515560', 'https://openalex.org/W3209059054', 'https://openalex.org/W3024869864', 'https://openalex.org/W4385571229', 'https://openalex.org/W2972359262', 'https://openalex.org/W3033411150', 'https://openalex.org/W4226444650', 'https://openalex.org/W4372260053', 'https://openalex.org/W3097777922', 'https://openalex.org/W3119308075', 'https://openalex.org/W2962788625', 'https://openalex.org/W2972495969', 'https://openalex.org/W4386794795', 'https://openalex.org/W4225680573', 'https://openalex.org/W3007142233', 'https://openalex.org/W4385823048', 'https://openalex.org/W2896457183', 'https://openalex.org/W3167533889', 'https://openalex.org/W4296070387', 'https://openalex.org/W4392903062', 'https://openalex.org/W4281789500', 'https://openalex.org/W3175871055', 'https://openalex.org/W4385570550']",2025-01-01
https://openalex.org/W4379193822,https://doi.org/10.1016/j.inffus.2023.101869,A review of deep learning techniques for speech processing,,"['https://openalex.org/W2912581782', 'https://openalex.org/W3137249133', 'https://openalex.org/W2976159681', 'https://openalex.org/W4205130185', 'https://openalex.org/W2125838338', 'https://openalex.org/W2160815625', 'https://openalex.org/W1995562189', 'https://openalex.org/W6681513673', 'https://openalex.org/W6623517193', 'https://openalex.org/W6739901393', 'https://openalex.org/W2088632109', 'https://openalex.org/W4313178214', 'https://openalex.org/W2110164509', 'https://openalex.org/W6775102986', 'https://openalex.org/W6779480587', 'https://openalex.org/W2099124598', 'https://openalex.org/W6675661078', 'https://openalex.org/W6680522077', 'https://openalex.org/W6640010188', 'https://openalex.org/W6600721324', 'https://openalex.org/W6682354605', 'https://openalex.org/W6682859417', 'https://openalex.org/W6712389868', 'https://openalex.org/W2131774270', 'https://openalex.org/W2064675550', 'https://openalex.org/W6604396178', 'https://openalex.org/W6807617354', 'https://openalex.org/W6810597000', 'https://openalex.org/W6772959282', 'https://openalex.org/W6744661987', 'https://openalex.org/W6738686518', 'https://openalex.org/W6775998690', 'https://openalex.org/W6803217577', 'https://openalex.org/W6786748378', 'https://openalex.org/W2965002127', 'https://openalex.org/W6759942353', 'https://openalex.org/W2945057891', 'https://openalex.org/W6696843773', 'https://openalex.org/W3100777112', 'https://openalex.org/W6688985767', 'https://openalex.org/W6768288844', 'https://openalex.org/W2556345765', 'https://openalex.org/W4318200341', 'https://openalex.org/W6757555829', 'https://openalex.org/W6683129133', 'https://openalex.org/W4220705102', 'https://openalex.org/W6769971416', 'https://openalex.org/W6752581720', 'https://openalex.org/W4292969786', 'https://openalex.org/W6746700228', 'https://openalex.org/W4312611743', 'https://openalex.org/W2974308384', 'https://openalex.org/W3199024477', 'https://openalex.org/W2989622482', 'https://openalex.org/W6731370813', 'https://openalex.org/W6761206160', 'https://openalex.org/W4294406066', 'https://openalex.org/W6770389661', 'https://openalex.org/W3211408542', 'https://openalex.org/W6785003639', 'https://openalex.org/W6773702976', 'https://openalex.org/W6778883912', 'https://openalex.org/W4213019189', 'https://openalex.org/W3031696893', 'https://openalex.org/W1923211482', 'https://openalex.org/W3208624098', 'https://openalex.org/W6768009688', 'https://openalex.org/W6757079273', 'https://openalex.org/W2972389417', 'https://openalex.org/W6763701032', 'https://openalex.org/W6769627184', 'https://openalex.org/W6810007534', 'https://openalex.org/W6788335241', 'https://openalex.org/W2974390546', 'https://openalex.org/W6784470492', 'https://openalex.org/W4283364647', 'https://openalex.org/W6848673773', 'https://openalex.org/W6743440867', 'https://openalex.org/W6754299077', 'https://openalex.org/W6755930846', 'https://openalex.org/W6768003404', 'https://openalex.org/W6774280737', 'https://openalex.org/W6675365184', 'https://openalex.org/W2972837679', 'https://openalex.org/W6754456516', 'https://openalex.org/W6755879856', 'https://openalex.org/W6776305863', 'https://openalex.org/W6784657998', 'https://openalex.org/W6770506093', 'https://openalex.org/W2897353073', 'https://openalex.org/W6791631845', 'https://openalex.org/W3083423753', 'https://openalex.org/W3038342317', 'https://openalex.org/W6766665032', 'https://openalex.org/W6745628346', 'https://openalex.org/W6783872503', 'https://openalex.org/W2936055714', 'https://openalex.org/W6750691924', 'https://openalex.org/W6787822403', 'https://openalex.org/W2996906606', 'https://openalex.org/W2985331920', 'https://openalex.org/W6745537798', 'https://openalex.org/W6767098714', 'https://openalex.org/W6784342092', 'https://openalex.org/W6784455497', 'https://openalex.org/W6802368550', 'https://openalex.org/W6776488176', 'https://openalex.org/W6801630172', 'https://openalex.org/W6810367489', 'https://openalex.org/W6774849813', 'https://openalex.org/W6787396906', 'https://openalex.org/W4319663835', 'https://openalex.org/W6790799587', 'https://openalex.org/W4312982325', 'https://openalex.org/W6779823529', 'https://openalex.org/W6679045638', 'https://openalex.org/W6795261426', 'https://openalex.org/W6798721538', 'https://openalex.org/W6810499433', 'https://openalex.org/W6748257384', 'https://openalex.org/W6748010250', 'https://openalex.org/W6785269020', 'https://openalex.org/W6743234192', 'https://openalex.org/W6735236233', 'https://openalex.org/W6745415975', 'https://openalex.org/W6767878346', 'https://openalex.org/W6744765397', 'https://openalex.org/W6755054534', 'https://openalex.org/W6795170356', 'https://openalex.org/W6775421620', 'https://openalex.org/W3131643166', 'https://openalex.org/W6810185288', 'https://openalex.org/W3100270690', 'https://openalex.org/W6753968785', 'https://openalex.org/W6754496211', 'https://openalex.org/W6784435678', 'https://openalex.org/W6767815451', 'https://openalex.org/W6662018943', 'https://openalex.org/W6742911084', 'https://openalex.org/W6850090677', 'https://openalex.org/W6787699154', 'https://openalex.org/W3204696009', 'https://openalex.org/W6775452034', 'https://openalex.org/W6784637704', 'https://openalex.org/W6600443243', 'https://openalex.org/W2914913933', 'https://openalex.org/W3090098535', 'https://openalex.org/W6779092977', 'https://openalex.org/W3207924272', 'https://openalex.org/W6840776798', 'https://openalex.org/W6769593479', 'https://openalex.org/W3041561163', 'https://openalex.org/W6773243159', 'https://openalex.org/W2752796333', 'https://openalex.org/W2423557781', 'https://openalex.org/W6780218876', 'https://openalex.org/W6773553514', 'https://openalex.org/W6799935246', 'https://openalex.org/W6761522202', 'https://openalex.org/W3209059054', 'https://openalex.org/W6810673746', 'https://openalex.org/W3209984917', 'https://openalex.org/W6803394801', 'https://openalex.org/W6791978275', 'https://openalex.org/W6767771057', 'https://openalex.org/W3211278025', 'https://openalex.org/W6849880362', 'https://openalex.org/W3132072393', 'https://openalex.org/W6802483191', 'https://openalex.org/W6629717138', 'https://openalex.org/W6789826613', 'https://openalex.org/W6691509046', 'https://openalex.org/W6773909944', 'https://openalex.org/W6756319913', 'https://openalex.org/W6773475747', 'https://openalex.org/W6756426640', 'https://openalex.org/W6726109732', 'https://openalex.org/W6785229415', 'https://openalex.org/W6768848920', 'https://openalex.org/W6687566353', 'https://openalex.org/W6783867762', 'https://openalex.org/W6803547063', 'https://openalex.org/W2966441164', 'https://openalex.org/W2591927543', 'https://openalex.org/W6765987481', 'https://openalex.org/W6603838645', 'https://openalex.org/W6754224190', 'https://openalex.org/W6763832098', 'https://openalex.org/W6779337556', 'https://openalex.org/W6772340256', 'https://openalex.org/W6786041167', 'https://openalex.org/W6801738251', 'https://openalex.org/W6748409065', 'https://openalex.org/W3155441579', 'https://openalex.org/W6755811826', 'https://openalex.org/W6810319272', 'https://openalex.org/W6779871621', 'https://openalex.org/W3034949308', 'https://openalex.org/W6777694618', 'https://openalex.org/W6775324216', 'https://openalex.org/W6784809985', 'https://openalex.org/W6789577077', 'https://openalex.org/W6796464841', 'https://openalex.org/W6800393981', 'https://openalex.org/W6802142237', 'https://openalex.org/W6843673214', 'https://openalex.org/W6755592152', 'https://openalex.org/W6769767169', 'https://openalex.org/W6767111847', 'https://openalex.org/W6776594914', 'https://openalex.org/W6784902658', 'https://openalex.org/W6809981015', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W6755300632', 'https://openalex.org/W6794864843', 'https://openalex.org/W6849953009', 'https://openalex.org/W6810772336', 'https://openalex.org/W6839301689', 'https://openalex.org/W6790356757', 'https://openalex.org/W6842794329', 'https://openalex.org/W6787300339', 'https://openalex.org/W6801211481', 'https://openalex.org/W6800389019', 'https://openalex.org/W6804171790', 'https://openalex.org/W6754171061', 'https://openalex.org/W6769754352', 'https://openalex.org/W6774618169', 'https://openalex.org/W6735706088', 'https://openalex.org/W2991361823', 'https://openalex.org/W6771029501', 'https://openalex.org/W6839738141', 'https://openalex.org/W6798741528', 'https://openalex.org/W6790639658', 'https://openalex.org/W2996414377', 'https://openalex.org/W6761382815', 'https://openalex.org/W6791184523', 'https://openalex.org/W2946555236', 'https://openalex.org/W2975414524', 'https://openalex.org/W6782760101', 'https://openalex.org/W6840257003', 'https://openalex.org/W6771024825', 'https://openalex.org/W6779711495', 'https://openalex.org/W6790317203', 'https://openalex.org/W6769050887', 'https://openalex.org/W3216941316', 'https://openalex.org/W6774467145', 'https://openalex.org/W6770189186', 'https://openalex.org/W2966387353', 'https://openalex.org/W6760752336', 'https://openalex.org/W6771238005', 'https://openalex.org/W3168542456', 'https://openalex.org/W6777078688', 'https://openalex.org/W6785089734', 'https://openalex.org/W4319985616', 'https://openalex.org/W3203313352', 'https://openalex.org/W6846143095', 'https://openalex.org/W6762643587', 'https://openalex.org/W6840281264', 'https://openalex.org/W6757422803', 'https://openalex.org/W6757660528', 'https://openalex.org/W6776390925', 'https://openalex.org/W6788032840', 'https://openalex.org/W6810445039', 'https://openalex.org/W6775580082', 'https://openalex.org/W4225304461', 'https://openalex.org/W6781140794', 'https://openalex.org/W6854823327', 'https://openalex.org/W6774785168', 'https://openalex.org/W6726424462', 'https://openalex.org/W6686645966', 'https://openalex.org/W6712618806', 'https://openalex.org/W6769935359', 'https://openalex.org/W6776309249', 'https://openalex.org/W6770919351', 'https://openalex.org/W3123941558', 'https://openalex.org/W6788533573', 'https://openalex.org/W6784351597', 'https://openalex.org/W3156052130', 'https://openalex.org/W6687483927', 'https://openalex.org/W6776308974', 'https://openalex.org/W6759487677', 'https://openalex.org/W4205234379', 'https://openalex.org/W2081074144', 'https://openalex.org/W2159591770', 'https://openalex.org/W6801168043', 'https://openalex.org/W6810138040', 'https://openalex.org/W2997419692', 'https://openalex.org/W3178462146', 'https://openalex.org/W6775646649', 'https://openalex.org/W6678809451', 'https://openalex.org/W6767725219', 'https://openalex.org/W4289656378', 'https://openalex.org/W6802610516', 'https://openalex.org/W6769168142', 'https://openalex.org/W6784693623', 'https://openalex.org/W6802926664', 'https://openalex.org/W6761124745', 'https://openalex.org/W6789693907', 'https://openalex.org/W6780418161', 'https://openalex.org/W6800972147', 'https://openalex.org/W3092085609', 'https://openalex.org/W6781586211', 'https://openalex.org/W6770340004', 'https://openalex.org/W6841035593', 'https://openalex.org/W2973048981', 'https://openalex.org/W6839510803', 'https://openalex.org/W6713729801', 'https://openalex.org/W2128653836', 'https://openalex.org/W1995536493', 'https://openalex.org/W6682371319', 'https://openalex.org/W2016254085', 'https://openalex.org/W2121294245', 'https://openalex.org/W2070126272', 'https://openalex.org/W6790790445', 'https://openalex.org/W6785195226', 'https://openalex.org/W6791183285', 'https://openalex.org/W6802025286', 'https://openalex.org/W6787607767', 'https://openalex.org/W2788241093', 'https://openalex.org/W2969437910', 'https://openalex.org/W6767580577', 'https://openalex.org/W3190196596', 'https://openalex.org/W6762114000', 'https://openalex.org/W3032514799', 'https://openalex.org/W6773286268', 'https://openalex.org/W2044893557', 'https://openalex.org/W6713658392', 'https://openalex.org/W6726607834', 'https://openalex.org/W6744261651', 'https://openalex.org/W6773419339', 'https://openalex.org/W6751200590', 'https://openalex.org/W6800075084', 'https://openalex.org/W6767367760', 'https://openalex.org/W6802891684', 'https://openalex.org/W6803488396', 'https://openalex.org/W4210381452', 'https://openalex.org/W6795249795', 'https://openalex.org/W2144404214', 'https://openalex.org/W6630687167', 'https://openalex.org/W4205689591', 'https://openalex.org/W6633117090', 'https://openalex.org/W6675521023', 'https://openalex.org/W6731171211', 'https://openalex.org/W6776251105', 'https://openalex.org/W6775804823', 'https://openalex.org/W2944426940', 'https://openalex.org/W6801588699', 'https://openalex.org/W2143169494', 'https://openalex.org/W1653463454', 'https://openalex.org/W6688843265', 'https://openalex.org/W6754999120', 'https://openalex.org/W6754528422', 'https://openalex.org/W2734774145', 'https://openalex.org/W2952218014', 'https://openalex.org/W6784764134', 'https://openalex.org/W6791167661', 'https://openalex.org/W6774995033', 'https://openalex.org/W6768815455', 'https://openalex.org/W3185109982', 'https://openalex.org/W6756383738', 'https://openalex.org/W6755691404', 'https://openalex.org/W6669997747', 'https://openalex.org/W6672346363', 'https://openalex.org/W4283010118', 'https://openalex.org/W3163571828', 'https://openalex.org/W6788809509', 'https://openalex.org/W2582895315', 'https://openalex.org/W6753084572', 'https://openalex.org/W6762306309', 'https://openalex.org/W6776533818', 'https://openalex.org/W3186700381', 'https://openalex.org/W2029199293', 'https://openalex.org/W6734491695', 'https://openalex.org/W6755759972', 'https://openalex.org/W2015143272', 'https://openalex.org/W6743392952', 'https://openalex.org/W6732945439', 'https://openalex.org/W6775393185', 'https://openalex.org/W6769705265', 'https://openalex.org/W6781840038', 'https://openalex.org/W6752246543', 'https://openalex.org/W6788484017', 'https://openalex.org/W3136499730', 'https://openalex.org/W6767522477', 'https://openalex.org/W6761832646', 'https://openalex.org/W6757723671', 'https://openalex.org/W6810268148', 'https://openalex.org/W6810693232', 'https://openalex.org/W6761403421', 'https://openalex.org/W6795356967', 'https://openalex.org/W6767278437', 'https://openalex.org/W4321780088', 'https://openalex.org/W2946977471', 'https://openalex.org/W6785108712', 'https://openalex.org/W6810728751', 'https://openalex.org/W6721237847', 'https://openalex.org/W6801798994', 'https://openalex.org/W6838716384', 'https://openalex.org/W6751084923', 'https://openalex.org/W6810748662', 'https://openalex.org/W6784756737', 'https://openalex.org/W4285231507', 'https://openalex.org/W6775281820', 'https://openalex.org/W6801625237', 'https://openalex.org/W6810422493', 'https://openalex.org/W6845831171', 'https://openalex.org/W6810653126', 'https://openalex.org/W6793783828', 'https://openalex.org/W6769355297', 'https://openalex.org/W6775467270', 'https://openalex.org/W6795069909', 'https://openalex.org/W3213544594', 'https://openalex.org/W6810360497', 'https://openalex.org/W6759579507', 'https://openalex.org/W6788701349', 'https://openalex.org/W6796581206', 'https://openalex.org/W6777497023', 'https://openalex.org/W6780585982', 'https://openalex.org/W6794805428', 'https://openalex.org/W6770008262', 'https://openalex.org/W6792304734', 'https://openalex.org/W6846678071', 'https://openalex.org/W4372346804', 'https://openalex.org/W3102451458', 'https://openalex.org/W3099884890', 'https://openalex.org/W1828163288', 'https://openalex.org/W3196468212', 'https://openalex.org/W4301371414', 'https://openalex.org/W2884797218', 'https://openalex.org/W2752782242', 'https://openalex.org/W3165478005', 'https://openalex.org/W4283809320', 'https://openalex.org/W4384080510', 'https://openalex.org/W4285189120', 'https://openalex.org/W4205605040', 'https://openalex.org/W3030437843', 'https://openalex.org/W4372341629', 'https://openalex.org/W2996286887', 'https://openalex.org/W3123318516', 'https://openalex.org/W3195577433', 'https://openalex.org/W2584032004', 'https://openalex.org/W4283008665', 'https://openalex.org/W4318242507', 'https://openalex.org/W3025260599', 'https://openalex.org/W2915722758', 'https://openalex.org/W2914911817', 'https://openalex.org/W2972449503', 'https://openalex.org/W3097777922', 'https://openalex.org/W4238846128', 'https://openalex.org/W2144499799', 'https://openalex.org/W3197823486', 'https://openalex.org/W4281739498', 'https://openalex.org/W4296068589', 'https://openalex.org/W4289305009', 'https://openalex.org/W3198234802', 'https://openalex.org/W3196833881', 'https://openalex.org/W3197580070', 'https://openalex.org/W3176455679', 'https://openalex.org/W3152218910', 'https://openalex.org/W2988736778', 'https://openalex.org/W3175809709', 'https://openalex.org/W4297841818', 'https://openalex.org/W2962896155', 'https://openalex.org/W4367365521', 'https://openalex.org/W2963452667', 'https://openalex.org/W2730845691', 'https://openalex.org/W4382202681', 'https://openalex.org/W2972584841', 'https://openalex.org/W3097075707', 'https://openalex.org/W2600643276', 'https://openalex.org/W2970006822', 'https://openalex.org/W4285605725', 'https://openalex.org/W4300980117', 'https://openalex.org/W4320451749', 'https://openalex.org/W3197567540', 'https://openalex.org/W2972745527', 'https://openalex.org/W4318752004', 'https://openalex.org/W3140429000', 'https://openalex.org/W4327520938', 'https://openalex.org/W4312121834', 'https://openalex.org/W2973034126', 'https://openalex.org/W3196117288', 'https://openalex.org/W4395957972', 'https://openalex.org/W4372342420', 'https://openalex.org/W4285483692', 'https://openalex.org/W4372260139', 'https://openalex.org/W4287121455', 'https://openalex.org/W4375869120', 'https://openalex.org/W3197334236', 'https://openalex.org/W4372347392', 'https://openalex.org/W3035289074', 'https://openalex.org/W3096567388', 'https://openalex.org/W4296068781', 'https://openalex.org/W3141854550', 'https://openalex.org/W4245692952', 'https://openalex.org/W4253752031', 'https://openalex.org/W2919448963', 'https://openalex.org/W4323066695', 'https://openalex.org/W3101689408', 'https://openalex.org/W4297778191', 'https://openalex.org/W3097034112', 'https://openalex.org/W3036601975', 'https://openalex.org/W4281820413', 'https://openalex.org/W4281779489', 'https://openalex.org/W647272629', 'https://openalex.org/W4296068817', 'https://openalex.org/W2963082324', 'https://openalex.org/W3176382501', 'https://openalex.org/W4294619417', 'https://openalex.org/W4386434937', 'https://openalex.org/W3097892637', 'https://openalex.org/W4225566824', 'https://openalex.org/W3095936335', 'https://openalex.org/W599380687', 'https://openalex.org/W4324031729', 'https://openalex.org/W3197507772', 'https://openalex.org/W4323651091', 'https://openalex.org/W2918296821', 'https://openalex.org/W3095057960', 'https://openalex.org/W4287761884', 'https://openalex.org/W3033411150', 'https://openalex.org/W4286950013', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313484599', 'https://openalex.org/W3091928890', 'https://openalex.org/W4236965008', 'https://openalex.org/W4205285111', 'https://openalex.org/W4320086272', 'https://openalex.org/W3159740474', 'https://openalex.org/W4297841766', 'https://openalex.org/W3198769980', 'https://openalex.org/W4300978696', 'https://openalex.org/W3013020904', 'https://openalex.org/W2804945011', 'https://openalex.org/W3027637851', 'https://openalex.org/W2963840672', 'https://openalex.org/W4375869348', 'https://openalex.org/W1902237438', 'https://openalex.org/W4322718191', 'https://openalex.org/W4309395027', 'https://openalex.org/W1539670134', 'https://openalex.org/W2140409019', 'https://openalex.org/W3204602440', 'https://openalex.org/W4224000641', 'https://openalex.org/W4313679638', 'https://openalex.org/W3094665028', 'https://openalex.org/W3193590960', 'https://openalex.org/W3198213150', 'https://openalex.org/W4385573646', 'https://openalex.org/W2896457183', 'https://openalex.org/W4283215837', 'https://openalex.org/W4296070453', 'https://openalex.org/W3123097577', 'https://openalex.org/W2619368999', 'https://openalex.org/W4301368689', 'https://openalex.org/W3198777143', 'https://openalex.org/W4305038462', 'https://openalex.org/W2970066309', 'https://openalex.org/W3081565196', 'https://openalex.org/W2964559396', 'https://openalex.org/W4226162428', 'https://openalex.org/W4288089799', 'https://openalex.org/W4372262687', 'https://openalex.org/W3121150787', 'https://openalex.org/W4320013820', 'https://openalex.org/W3102342027', 'https://openalex.org/W4297733535', 'https://openalex.org/W2921052634', 'https://openalex.org/W3037469336', 'https://openalex.org/W3037217258', 'https://openalex.org/W4286795976', 'https://openalex.org/W3096408984', 'https://openalex.org/W4297808394', 'https://openalex.org/W3028017480', 'https://openalex.org/W4375868953', 'https://openalex.org/W3213029956', 'https://openalex.org/W3187990717', 'https://openalex.org/W3099330747', 'https://openalex.org/W2529559718', 'https://openalex.org/W4245804068', 'https://openalex.org/W3139918052', 'https://openalex.org/W4300539257', 'https://openalex.org/W2970597249', 'https://openalex.org/W4297817572', 'https://openalex.org/W2973143779', 'https://openalex.org/W3112034174', 'https://openalex.org/W3197331597', 'https://openalex.org/W3200245256', 'https://openalex.org/W3025528898', 'https://openalex.org/W3197343310', 'https://openalex.org/W2972659941', 'https://openalex.org/W2963033987', 'https://openalex.org/W3034625919', 'https://openalex.org/W2899901795', 'https://openalex.org/W4226063663', 'https://openalex.org/W3213604094', 'https://openalex.org/W3093579165', 'https://openalex.org/W2972943112', 'https://openalex.org/W4306175879', 'https://openalex.org/W3095883095', 'https://openalex.org/W4236097098', 'https://openalex.org/W3096323553', 'https://openalex.org/W4287888298', 'https://openalex.org/W2963609956', 'https://openalex.org/W3097906045', 'https://openalex.org/W4225746985', 'https://openalex.org/W3025581723', 'https://openalex.org/W3104216863', 'https://openalex.org/W4292779060', 'https://openalex.org/W2963066655', 'https://openalex.org/W2973157397', 'https://openalex.org/W3026041220', 'https://openalex.org/W2964171275', 'https://openalex.org/W3153675281', 'https://openalex.org/W2262249176', 'https://openalex.org/W3093933225', 'https://openalex.org/W4367359628', 'https://openalex.org/W3024869864', 'https://openalex.org/W1583837637', 'https://openalex.org/W3198608154', 'https://openalex.org/W2979476256', 'https://openalex.org/W2133564696', 'https://openalex.org/W4385567350', 'https://openalex.org/W4286905216', 'https://openalex.org/W3198257150', 'https://openalex.org/W2963040451', 'https://openalex.org/W2519091744', 'https://openalex.org/W2972889948', 'https://openalex.org/W4375868863', 'https://openalex.org/W3096442195', 'https://openalex.org/W2792764867', 'https://openalex.org/W4287641946', 'https://openalex.org/W4394658530', 'https://openalex.org/W4255556797', 'https://openalex.org/W3197842028', 'https://openalex.org/W4296959082', 'https://openalex.org/W4297683418', 'https://openalex.org/W3095790275', 'https://openalex.org/W2972909277', 'https://openalex.org/W2963975282', 'https://openalex.org/W4307079201', 'https://openalex.org/W4376490867', 'https://openalex.org/W3092424727', 'https://openalex.org/W4283704748', 'https://openalex.org/W2792760996', 'https://openalex.org/W3200287816', 'https://openalex.org/W2946200149', 'https://openalex.org/W3017465475', 'https://openalex.org/W2120615054', 'https://openalex.org/W3205644108', 'https://openalex.org/W3095999419', 'https://openalex.org/W3197763626', 'https://openalex.org/W2941814890', 'https://openalex.org/W4320459320', 'https://openalex.org/W3198035615', 'https://openalex.org/W4311000453', 'https://openalex.org/W2797583228', 'https://openalex.org/W4301606627', 'https://openalex.org/W3129651364', 'https://openalex.org/W3128910262', 'https://openalex.org/W2153849757', 'https://openalex.org/W3171689473', 'https://openalex.org/W4226278401', 'https://openalex.org/W3025498998', 'https://openalex.org/W2954386831', 'https://openalex.org/W3204679573', 'https://openalex.org/W3157070662', 'https://openalex.org/W854541894', 'https://openalex.org/W2963691546', 'https://openalex.org/W2103934944', 'https://openalex.org/W4288091954', 'https://openalex.org/W4394671563', 'https://openalex.org/W2952167535', 'https://openalex.org/W4224612669', 'https://openalex.org/W4312437435', 'https://openalex.org/W3024147341', 'https://openalex.org/W3097787598', 'https://openalex.org/W4226098957', 'https://openalex.org/W2963799213', 'https://openalex.org/W3105669983', 'https://openalex.org/W2973049979', 'https://openalex.org/W4296068981', 'https://openalex.org/W3036167779', 'https://openalex.org/W2963317665', 'https://openalex.org/W4255113413', 'https://openalex.org/W3197411683', 'https://openalex.org/W2917128112', 'https://openalex.org/W3097828251', 'https://openalex.org/W3096235116', 'https://openalex.org/W4362598673', 'https://openalex.org/W189595333', 'https://openalex.org/W4297841714', 'https://openalex.org/W4301062550', 'https://openalex.org/W3126648213', 'https://openalex.org/W3099078140', 'https://openalex.org/W3095173472', 'https://openalex.org/W3054645415', 'https://openalex.org/W2985287635', 'https://openalex.org/W4385822534', 'https://openalex.org/W2601450892', 'https://openalex.org/W4322718246', 'https://openalex.org/W3198836239', 'https://openalex.org/W3026874504', 'https://openalex.org/W2186373062', 'https://openalex.org/W2781626870', 'https://openalex.org/W3105031100', 'https://openalex.org/W2919752630', 'https://openalex.org/W4297841774', 'https://openalex.org/W4385245566', 'https://openalex.org/W2971753973', 'https://openalex.org/W4297841480']",2023-06-03
https://openalex.org/W4402301063,https://doi.org/10.1109/taslp.2024.3451951,ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations,"Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.","['https://openalex.org/W2963609956', 'https://openalex.org/W6778823374', 'https://openalex.org/W2964243274', 'https://openalex.org/W4391020683', 'https://openalex.org/W4385764360', 'https://openalex.org/W2972473628', 'https://openalex.org/W3095012670', 'https://openalex.org/W6805710207', 'https://openalex.org/W3197324626', 'https://openalex.org/W4296068816', 'https://openalex.org/W4372267432', 'https://openalex.org/W4385823466', 'https://openalex.org/W6752888775', 'https://openalex.org/W3015826515', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4225274946', 'https://openalex.org/W3206189675', 'https://openalex.org/W6803547063', 'https://openalex.org/W3207300132', 'https://openalex.org/W6848735303', 'https://openalex.org/W4385822745', 'https://openalex.org/W4226380987', 'https://openalex.org/W4381786045', 'https://openalex.org/W4390075359', 'https://openalex.org/W4252812408', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6851724922', 'https://openalex.org/W6862144568', 'https://openalex.org/W6858915148', 'https://openalex.org/W6748588790', 'https://openalex.org/W6750489868', 'https://openalex.org/W3196584150', 'https://openalex.org/W3161436426', 'https://openalex.org/W6811227718', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W3150572638', 'https://openalex.org/W3097297926', 'https://openalex.org/W6800393981', 'https://openalex.org/W4283640572', 'https://openalex.org/W6796464841', 'https://openalex.org/W2973084242', 'https://openalex.org/W4392114301', 'https://openalex.org/W2964002616', 'https://openalex.org/W3048217770', 'https://openalex.org/W3194464626', 'https://openalex.org/W6752124048', 'https://openalex.org/W3205533980', 'https://openalex.org/W4297841714', 'https://openalex.org/W3197763626', 'https://openalex.org/W4225096077', 'https://openalex.org/W4385329631', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226132755', 'https://openalex.org/W4382202703', 'https://openalex.org/W4281760581', 'https://openalex.org/W4385822479', 'https://openalex.org/W4392903365', 'https://openalex.org/W6853937136', 'https://openalex.org/W6850334629', 'https://openalex.org/W4389600306', 'https://openalex.org/W6783867762', 'https://openalex.org/W3196001064', 'https://openalex.org/W4225534571', 'https://openalex.org/W4296068817', 'https://openalex.org/W4367721746', 'https://openalex.org/W3197273793', 'https://openalex.org/W3095410713', 'https://openalex.org/W2084534958', 'https://openalex.org/W2972802841', 'https://openalex.org/W6917585676', 'https://openalex.org/W6783527727', 'https://openalex.org/W7062081054', 'https://openalex.org/W4225956675', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W2187089797', 'https://openalex.org/W4392538788', 'https://openalex.org/W4226424742', 'https://openalex.org/W4366460484', 'https://openalex.org/W4313679638', 'https://openalex.org/W3090254849', 'https://openalex.org/W4388927799', 'https://openalex.org/W4323651091']",2024-01-01
https://openalex.org/W4372341972,https://doi.org/10.1109/icassp49357.2023.10095547,Adversarial Data Augmentation Using VAE-GAN for Disordered Speech Recognition,"Automatic recognition of disordered speech remains a highly challenging task to date. The underlying neuro-motor conditions, often compounded with co-occurring physical disabilities, lead to the difficulty in collecting large quantities of impaired speech required for ASR system development. This paper presents novel variational auto-encoder generative adversarial network (VAE-GAN) based personalized disordered speech augmentation approaches that simultaneously learn to encode, generate and discriminate synthesized impaired speech. Separate latent features are derived to learn dysarthric speech characteristics and phoneme context representations. Self-supervised pre-trained Wav2vec 2.0 embedding features are also incorporated. Experiments conducted on the UASpeech corpus suggest the proposed adversarial data augmentation approach consistently outperformed the baseline speed perturbation and non-VAE GAN augmentation methods with trained hybrid TDNN and End-to-end Conformer systems. After LHUC speaker adaptation, the best system using VAE-GAN based augmentation produced an overall WER of 27.78% on the UASpeech test set of 16 dysarthric speakers, and the lowest published WER of 57.31% on the subset of speakers with ""Very Low"" intelligibility.","['https://openalex.org/W2401277329', 'https://openalex.org/W2396944218', 'https://openalex.org/W2250686550', 'https://openalex.org/W2888789389', 'https://openalex.org/W3097909406', 'https://openalex.org/W4221148457', 'https://openalex.org/W3095123370', 'https://openalex.org/W3197912841', 'https://openalex.org/W4206924127', 'https://openalex.org/W2168510624', 'https://openalex.org/W1989674786', 'https://openalex.org/W6675409298', 'https://openalex.org/W2407080277', 'https://openalex.org/W1979651826', 'https://openalex.org/W2696967604', 'https://openalex.org/W2883586237', 'https://openalex.org/W4221167580', 'https://openalex.org/W3197943112', 'https://openalex.org/W3082218567', 'https://openalex.org/W3207399576', 'https://openalex.org/W3034924009', 'https://openalex.org/W3161294170', 'https://openalex.org/W4221156109', 'https://openalex.org/W3198454118', 'https://openalex.org/W3097341038', 'https://openalex.org/W4296068414', 'https://openalex.org/W3174329270', 'https://openalex.org/W3198806843', 'https://openalex.org/W3163725792', 'https://openalex.org/W6640963894', 'https://openalex.org/W2752796333', 'https://openalex.org/W6687506355', 'https://openalex.org/W6780218876', 'https://openalex.org/W180052447', 'https://openalex.org/W4297841734', 'https://openalex.org/W2943237054', 'https://openalex.org/W2972956697', 'https://openalex.org/W2889419636', 'https://openalex.org/W4296068817', 'https://openalex.org/W3197858274', 'https://openalex.org/W3095930733', 'https://openalex.org/W3097969370', 'https://openalex.org/W2936771798', 'https://openalex.org/W4283461667', 'https://openalex.org/W2402146185', 'https://openalex.org/W2514741789', 'https://openalex.org/W6603616073', 'https://openalex.org/W2239847623', 'https://openalex.org/W2936861580', 'https://openalex.org/W3014690389', 'https://openalex.org/W6785297517', 'https://openalex.org/W3196511136', 'https://openalex.org/W4320013936', 'https://openalex.org/W3125118953', 'https://openalex.org/W2964167449', 'https://openalex.org/W2099621636', 'https://openalex.org/W4225753921', 'https://openalex.org/W3097720583', 'https://openalex.org/W2963799213', 'https://openalex.org/W1959608418', 'https://openalex.org/W3036601975', 'https://openalex.org/W2972516210']",2023-05-05
https://openalex.org/W4399729833,https://doi.org/10.1109/taslp.2024.3414342,QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning,,"['https://openalex.org/W3151309757', 'https://openalex.org/W6802715664', 'https://openalex.org/W6785521828', 'https://openalex.org/W6850266965', 'https://openalex.org/W3140429000', 'https://openalex.org/W4296068817', 'https://openalex.org/W2752796333', 'https://openalex.org/W3209059054', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2889028433', 'https://openalex.org/W3016223359', 'https://openalex.org/W3197324626', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962699523', 'https://openalex.org/W2970015022', 'https://openalex.org/W3015853838', 'https://openalex.org/W4390075359', 'https://openalex.org/W3096456328', 'https://openalex.org/W4221166168', 'https://openalex.org/W4281492411', 'https://openalex.org/W3035725276', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W3114632476', 'https://openalex.org/W4319862442', 'https://openalex.org/W4287889722', 'https://openalex.org/W3198039885', 'https://openalex.org/W3206189675', 'https://openalex.org/W4376456759', 'https://openalex.org/W4285144981', 'https://openalex.org/W4200047557', 'https://openalex.org/W4385488985', 'https://openalex.org/W6750852989', 'https://openalex.org/W3197858274', 'https://openalex.org/W3180355996', 'https://openalex.org/W4312388283', 'https://openalex.org/W2935711438', 'https://openalex.org/W4200219715', 'https://openalex.org/W4307323391', 'https://openalex.org/W3015434413', 'https://openalex.org/W6777028661', 'https://openalex.org/W6848735303', 'https://openalex.org/W4226132755', 'https://openalex.org/W2124509324', 'https://openalex.org/W3024869864', 'https://openalex.org/W6762931180', 'https://openalex.org/W6786494455', 'https://openalex.org/W3197273793', 'https://openalex.org/W6783867762', 'https://openalex.org/W6763832098', 'https://openalex.org/W3203407300', 'https://openalex.org/W6779337556', 'https://openalex.org/W6631190155', 'https://openalex.org/W6796464841', 'https://openalex.org/W6777694618', 'https://openalex.org/W2107860279', 'https://openalex.org/W6765779288', 'https://openalex.org/W2972478942']",2024-06-17
https://openalex.org/W4395010962,https://doi.org/10.3390/app14083509,P D N: A Priori Dictionary Network for Fashion Parsing,"The task of fashion parsing aims to assign pixel-level labels to clothing targets; thereby, parsing models are required to have good contextual recognition ability. However, the shapes of clothing components are complex, and the types are difficult to distinguish. Recent solutions focus on improving datasets and supplying abundant priori information, but the utilization of features by more efficient methods is rarely explored. In this paper, we propose a multi-scale fashion parsing model called the Priori Dictionary Network (PDN), which includes a priori attention module and a multi-scale backbone. The priori attention module extracts high dimensional features from our designed clothing average template as a priori information dictionary (priori dictionary, PD), and the PD is utilized to activate the feature maps of a CNN from a multi-scale attention mechanism. The backbone is derived from classical models, and five side paths are designed to leverage the richer features of local and global contextual representations. To measure the performance of our method, we evaluated the model on four public datasets, the CFPD, UTFR-SBD3, ModaNet and LIP, and the experimental results show that our model stands out from other State of the Art in all four datasets. This method can assist with the labeling problem of clothing datasets.","['https://openalex.org/W2964050021', 'https://openalex.org/W4385337277', 'https://openalex.org/W3035515747', 'https://openalex.org/W3094629174', 'https://openalex.org/W4292513190', 'https://openalex.org/W2156001867', 'https://openalex.org/W1998827347', 'https://openalex.org/W146395692', 'https://openalex.org/W2143085975', 'https://openalex.org/W1002857674', 'https://openalex.org/W2768139823', 'https://openalex.org/W2767370058', 'https://openalex.org/W2811481004', 'https://openalex.org/W2963978393', 'https://openalex.org/W3132190499', 'https://openalex.org/W2788024025', 'https://openalex.org/W4226030517', 'https://openalex.org/W2981959899', 'https://openalex.org/W2999338702', 'https://openalex.org/W2890319229', 'https://openalex.org/W2737106217', 'https://openalex.org/W2937863878', 'https://openalex.org/W2514518917', 'https://openalex.org/W3034509543', 'https://openalex.org/W4366999405', 'https://openalex.org/W3102853147', 'https://openalex.org/W4296068817', 'https://openalex.org/W2047809566', 'https://openalex.org/W2121339428', 'https://openalex.org/W2962974533', 'https://openalex.org/W2986945954', 'https://openalex.org/W2937423351', 'https://openalex.org/W2471768434', 'https://openalex.org/W2808620491', 'https://openalex.org/W1973255633', 'https://openalex.org/W2561228828', 'https://openalex.org/W2990371274', 'https://openalex.org/W2969640942', 'https://openalex.org/W2987738648', 'https://openalex.org/W2902930830', 'https://openalex.org/W3197396971', 'https://openalex.org/W3115879670']",2024-04-22
https://openalex.org/W4408100020,https://doi.org/10.1109/tip.2025.3540296,Multi-Concept Learning for Scene Graph Generation,"Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model's performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.","['https://openalex.org/W2963691377', 'https://openalex.org/W4389519483', 'https://openalex.org/W4226538672', 'https://openalex.org/W3036869132', 'https://openalex.org/W3207608362', 'https://openalex.org/W3115656231', 'https://openalex.org/W4295036294', 'https://openalex.org/W3209229003', 'https://openalex.org/W2513281263', 'https://openalex.org/W4390190127', 'https://openalex.org/W2963184176', 'https://openalex.org/W4214778243', 'https://openalex.org/W4225868495', 'https://openalex.org/W4313037583', 'https://openalex.org/W3191092153', 'https://openalex.org/W3196936439', 'https://openalex.org/W3181556077', 'https://openalex.org/W4312429751', 'https://openalex.org/W4385216542', 'https://openalex.org/W4386075638', 'https://openalex.org/W4304080485', 'https://openalex.org/W4403600844', 'https://openalex.org/W4214879921', 'https://openalex.org/W4386075932', 'https://openalex.org/W3035017890', 'https://openalex.org/W4312682661', 'https://openalex.org/W4225998785', 'https://openalex.org/W4312578903', 'https://openalex.org/W2963536419', 'https://openalex.org/W6779304510', 'https://openalex.org/W6800503170', 'https://openalex.org/W3155644662', 'https://openalex.org/W4386075828', 'https://openalex.org/W4312750374', 'https://openalex.org/W3200297852', 'https://openalex.org/W4312440658', 'https://openalex.org/W4312752917', 'https://openalex.org/W6720057410', 'https://openalex.org/W3035730922', 'https://openalex.org/W4319301005', 'https://openalex.org/W3157653192', 'https://openalex.org/W3118856670', 'https://openalex.org/W4221150632', 'https://openalex.org/W2250539671', 'https://openalex.org/W2886970679', 'https://openalex.org/W2963938081', 'https://openalex.org/W4385245566', 'https://openalex.org/W6758474948', 'https://openalex.org/W4296068817', 'https://openalex.org/W4386065342', 'https://openalex.org/W3217319719', 'https://openalex.org/W4386075801', 'https://openalex.org/W4283463548', 'https://openalex.org/W4312555984', 'https://openalex.org/W2277195237', 'https://openalex.org/W4288083516', 'https://openalex.org/W4293518226', 'https://openalex.org/W639708223', 'https://openalex.org/W4313161463', 'https://openalex.org/W3134735773', 'https://openalex.org/W4386072171', 'https://openalex.org/W4312563197', 'https://openalex.org/W4319300107', 'https://openalex.org/W4214693531', 'https://openalex.org/W3034538190', 'https://openalex.org/W4285287023', 'https://openalex.org/W3182902595', 'https://openalex.org/W4366352717', 'https://openalex.org/W4312744661', 'https://openalex.org/W3096609285', 'https://openalex.org/W3174865181', 'https://openalex.org/W3173181410', 'https://openalex.org/W3215287157', 'https://openalex.org/W6850625674']",2025-01-01
https://openalex.org/W4408351949,https://doi.org/10.1109/icassp49660.2025.10889794,Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation,,"['https://openalex.org/W6778883912', 'https://openalex.org/W6854866820', 'https://openalex.org/W6850334629', 'https://openalex.org/W6852421699', 'https://openalex.org/W4391833199', 'https://openalex.org/W4307323391', 'https://openalex.org/W6852581948', 'https://openalex.org/W6853515095', 'https://openalex.org/W6857409037', 'https://openalex.org/W6870085509', 'https://openalex.org/W4367721746', 'https://openalex.org/W6869585042', 'https://openalex.org/W4408355207', 'https://openalex.org/W4406461319', 'https://openalex.org/W4406495593', 'https://openalex.org/W4402111239', 'https://openalex.org/W6762931180', 'https://openalex.org/W4296068817', 'https://openalex.org/W3024869864', 'https://openalex.org/W2752796333', 'https://openalex.org/W2119913432', 'https://openalex.org/W2124509324', 'https://openalex.org/W6853096648', 'https://openalex.org/W4402111470', 'https://openalex.org/W6861507043', 'https://openalex.org/W6869301554', 'https://openalex.org/W6870225445', 'https://openalex.org/W4402111455', 'https://openalex.org/W6936129901', 'https://openalex.org/W6838843145', 'https://openalex.org/W6757817989', 'https://openalex.org/W6848735303']",2025-03-12
https://openalex.org/W4400111385,https://doi.org/10.1109/taslp.2024.3419418,SpeechX: Neural Codec Language Model as a Versatile Speech Transformer,,"['https://openalex.org/W6778883912', 'https://openalex.org/W4312933868', 'https://openalex.org/W4381786045', 'https://openalex.org/W6810334672', 'https://openalex.org/W6850204008', 'https://openalex.org/W4386072096', 'https://openalex.org/W4401042948', 'https://openalex.org/W6853998256', 'https://openalex.org/W6849105126', 'https://openalex.org/W6752888775', 'https://openalex.org/W3015826515', 'https://openalex.org/W3196584150', 'https://openalex.org/W6853888607', 'https://openalex.org/W6848735303', 'https://openalex.org/W4390075359', 'https://openalex.org/W6853771913', 'https://openalex.org/W6850334629', 'https://openalex.org/W6852870047', 'https://openalex.org/W2973062255', 'https://openalex.org/W2951130829', 'https://openalex.org/W3206706278', 'https://openalex.org/W4367597591', 'https://openalex.org/W4281820413', 'https://openalex.org/W2408688265', 'https://openalex.org/W3016185664', 'https://openalex.org/W2995181338', 'https://openalex.org/W6790978476', 'https://openalex.org/W6851724922', 'https://openalex.org/W4307323391', 'https://openalex.org/W4385245566', 'https://openalex.org/W6688816777', 'https://openalex.org/W3097777922', 'https://openalex.org/W3161480375', 'https://openalex.org/W1552314771', 'https://openalex.org/W4392908343', 'https://openalex.org/W3096408984', 'https://openalex.org/W3194338569', 'https://openalex.org/W6861001475', 'https://openalex.org/W6853165267', 'https://openalex.org/W6862144568', 'https://openalex.org/W4320458302', 'https://openalex.org/W4392538788', 'https://openalex.org/W4378945745', 'https://openalex.org/W4318351475', 'https://openalex.org/W4323651091', 'https://openalex.org/W2219249508', 'https://openalex.org/W4379924545', 'https://openalex.org/W4390962167', 'https://openalex.org/W4381827575', 'https://openalex.org/W4313679638']",2024-01-01
https://openalex.org/W4406461271,https://doi.org/10.1109/slt61566.2024.10832255,"Amphion: an Open-Source Audio, Music, and Speech Generation Toolkit",,"['https://openalex.org/W6778823374', 'https://openalex.org/W6851724922', 'https://openalex.org/W4402044753', 'https://openalex.org/W6796464841', 'https://openalex.org/W6848735303', 'https://openalex.org/W6849109464', 'https://openalex.org/W6851290578', 'https://openalex.org/W4406462037', 'https://openalex.org/W4405709542', 'https://openalex.org/W6870299770', 'https://openalex.org/W2519091744', 'https://openalex.org/W6748409065', 'https://openalex.org/W6783182287', 'https://openalex.org/W2963300588', 'https://openalex.org/W6767111847', 'https://openalex.org/W3096159803', 'https://openalex.org/W4372262501', 'https://openalex.org/W6838843145', 'https://openalex.org/W4377000449', 'https://openalex.org/W6785954764', 'https://openalex.org/W4307323391', 'https://openalex.org/W4392909842', 'https://openalex.org/W4402830263', 'https://openalex.org/W6862144568', 'https://openalex.org/W3198020407', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972359262', 'https://openalex.org/W3209059054', 'https://openalex.org/W4367359628', 'https://openalex.org/W3094550259', 'https://openalex.org/W4296068763', 'https://openalex.org/W4285345683', 'https://openalex.org/W4400480473', 'https://openalex.org/W4392538788', 'https://openalex.org/W4362598673', 'https://openalex.org/W3169905056', 'https://openalex.org/W4400376071', 'https://openalex.org/W3033411150', 'https://openalex.org/W4281736089', 'https://openalex.org/W4298580827', 'https://openalex.org/W3103104054', 'https://openalex.org/W4366460484', 'https://openalex.org/W4313679638', 'https://openalex.org/W3129651364', 'https://openalex.org/W2970006822', 'https://openalex.org/W4318752004']",2024-12-02
https://openalex.org/W4406461672,https://doi.org/10.1109/slt61566.2024.10832320,E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS,,"['https://openalex.org/W6763832098', 'https://openalex.org/W6777694618', 'https://openalex.org/W6783867762', 'https://openalex.org/W4391020683', 'https://openalex.org/W6748588790', 'https://openalex.org/W6752888775', 'https://openalex.org/W6848735303', 'https://openalex.org/W6856126247', 'https://openalex.org/W6861267420', 'https://openalex.org/W6864145738', 'https://openalex.org/W6869425021', 'https://openalex.org/W6856434366', 'https://openalex.org/W6851724922', 'https://openalex.org/W6862144568', 'https://openalex.org/W6779823529', 'https://openalex.org/W6786375611', 'https://openalex.org/W4252812408', 'https://openalex.org/W4392931276', 'https://openalex.org/W6846539466', 'https://openalex.org/W6795261426', 'https://openalex.org/W4391021721', 'https://openalex.org/W1901129140', 'https://openalex.org/W6869301554', 'https://openalex.org/W2964002616', 'https://openalex.org/W4385245566', 'https://openalex.org/W6752307458', 'https://openalex.org/W2747874407', 'https://openalex.org/W4392903704', 'https://openalex.org/W2995181338', 'https://openalex.org/W6838843145', 'https://openalex.org/W6840815571', 'https://openalex.org/W4402111215', 'https://openalex.org/W4391021797', 'https://openalex.org/W1494198834', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4313679638', 'https://openalex.org/W4391272929', 'https://openalex.org/W4399554695', 'https://openalex.org/W4399425629', 'https://openalex.org/W4394007232', 'https://openalex.org/W4288099666', 'https://openalex.org/W4387323811', 'https://openalex.org/W4400111385', 'https://openalex.org/W4392538788', 'https://openalex.org/W4366460484']",2024-12-02
https://openalex.org/W4406461488,https://doi.org/10.1109/slt61566.2024.10832364,Codec-Superb @ SLT 2024: A Lightweight Benchmark For Neural Audio Codec Models,,"['https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W6845479124', 'https://openalex.org/W6849105126', 'https://openalex.org/W6861548716', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6853188576', 'https://openalex.org/W4372270198', 'https://openalex.org/W6852581948', 'https://openalex.org/W6856515471', 'https://openalex.org/W6855885476', 'https://openalex.org/W6853515095', 'https://openalex.org/W6862144568', 'https://openalex.org/W6861935630', 'https://openalex.org/W4392909975', 'https://openalex.org/W4392903887', 'https://openalex.org/W4392903089', 'https://openalex.org/W4392902628', 'https://openalex.org/W4392931975', 'https://openalex.org/W4404740148', 'https://openalex.org/W4399875170', 'https://openalex.org/W6853165267', 'https://openalex.org/W4402111239', 'https://openalex.org/W4406496863', 'https://openalex.org/W6869393727', 'https://openalex.org/W4402111789', 'https://openalex.org/W6869755106', 'https://openalex.org/W4402670057', 'https://openalex.org/W1494198834', 'https://openalex.org/W2726515241', 'https://openalex.org/W1567520911', 'https://openalex.org/W3139878283', 'https://openalex.org/W2972584841', 'https://openalex.org/W3161223924', 'https://openalex.org/W2030931454', 'https://openalex.org/W2803193013', 'https://openalex.org/W6777776875', 'https://openalex.org/W2052666245', 'https://openalex.org/W4205689591', 'https://openalex.org/W6948682473', 'https://openalex.org/W6847363464', 'https://openalex.org/W3024869864', 'https://openalex.org/W6859908464', 'https://openalex.org/W4392903801', 'https://openalex.org/W4372266552', 'https://openalex.org/W1552314771', 'https://openalex.org/W6912804647', 'https://openalex.org/W2067295501', 'https://openalex.org/W6840200333', 'https://openalex.org/W3209059054', 'https://openalex.org/W6779823529', 'https://openalex.org/W4392019360', 'https://openalex.org/W3027008958', 'https://openalex.org/W3036167779', 'https://openalex.org/W4399759291', 'https://openalex.org/W4311000453', 'https://openalex.org/W4285483774', 'https://openalex.org/W4318351475', 'https://openalex.org/W4392538788', 'https://openalex.org/W2320832475', 'https://openalex.org/W4386384714', 'https://openalex.org/W4392019859', 'https://openalex.org/W4313679638', 'https://openalex.org/W4399911677', 'https://openalex.org/W4380551955', 'https://openalex.org/W4402669711', 'https://openalex.org/W4300980117', 'https://openalex.org/W4379259581', 'https://openalex.org/W4372279529', 'https://openalex.org/W4377010126', 'https://openalex.org/W4392903389']",2024-12-02
https://openalex.org/W4406461501,https://doi.org/10.1109/slt61566.2024.10832181,Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-To-Speech,,"['https://openalex.org/W4319985616', 'https://openalex.org/W4392904830', 'https://openalex.org/W4375869364', 'https://openalex.org/W4385822379', 'https://openalex.org/W4313316128', 'https://openalex.org/W4372262418', 'https://openalex.org/W4210777104', 'https://openalex.org/W4296069154', 'https://openalex.org/W6746238782', 'https://openalex.org/W3135644023', 'https://openalex.org/W3160329778', 'https://openalex.org/W4402115961', 'https://openalex.org/W4252812408', 'https://openalex.org/W6861856086', 'https://openalex.org/W6846539466', 'https://openalex.org/W6752307458', 'https://openalex.org/W2795781874', 'https://openalex.org/W3197934793', 'https://openalex.org/W2149628368', 'https://openalex.org/W2742542661', 'https://openalex.org/W6862144568', 'https://openalex.org/W4361994820', 'https://openalex.org/W6780218876', 'https://openalex.org/W4402111215', 'https://openalex.org/W6859908464', 'https://openalex.org/W4225302959', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6678809451', 'https://openalex.org/W2166637769', 'https://openalex.org/W6603931906', 'https://openalex.org/W4391454536', 'https://openalex.org/W3081192838', 'https://openalex.org/W4392908892', 'https://openalex.org/W6847363464', 'https://openalex.org/W3209984917', 'https://openalex.org/W6848735303', 'https://openalex.org/W4389600306', 'https://openalex.org/W4385245566', 'https://openalex.org/W6767111847', 'https://openalex.org/W4402112120', 'https://openalex.org/W2963755523', 'https://openalex.org/W4303647933', 'https://openalex.org/W4391801052', 'https://openalex.org/W2970006822', 'https://openalex.org/W4311000453', 'https://openalex.org/W97072897', 'https://openalex.org/W4392538788', 'https://openalex.org/W4313679638', 'https://openalex.org/W2770743791', 'https://openalex.org/W4402669711', 'https://openalex.org/W3036601975']",2024-12-02
https://openalex.org/W4406462026,https://doi.org/10.1109/slt61566.2024.10832178,TTSDS - Text-to-Speech Distribution Score,,"['https://openalex.org/W4307323391', 'https://openalex.org/W6861507043', 'https://openalex.org/W6869425021', 'https://openalex.org/W4402112533', 'https://openalex.org/W4296070211', 'https://openalex.org/W6859934604', 'https://openalex.org/W6853937136', 'https://openalex.org/W4385823488', 'https://openalex.org/W6765779288', 'https://openalex.org/W6781251213', 'https://openalex.org/W4382202703', 'https://openalex.org/W2923014074', 'https://openalex.org/W6762392948', 'https://openalex.org/W2964223283', 'https://openalex.org/W3197580070', 'https://openalex.org/W2185776260', 'https://openalex.org/W6629609042', 'https://openalex.org/W3011535310', 'https://openalex.org/W4393859250', 'https://openalex.org/W2093107730', 'https://openalex.org/W6802017037', 'https://openalex.org/W1552314771', 'https://openalex.org/W1574170747', 'https://openalex.org/W2962788625', 'https://openalex.org/W4372340947', 'https://openalex.org/W3209059054', 'https://openalex.org/W2471520273', 'https://openalex.org/W6780218876', 'https://openalex.org/W6847363464', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972478942', 'https://openalex.org/W6750442754', 'https://openalex.org/W6677973343', 'https://openalex.org/W4395958010', 'https://openalex.org/W4395962180', 'https://openalex.org/W3150572638', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972359262', 'https://openalex.org/W4385823191', 'https://openalex.org/W6917585676', 'https://openalex.org/W2052666245', 'https://openalex.org/W6809863597', 'https://openalex.org/W4225956675', 'https://openalex.org/W4296068974', 'https://openalex.org/W6862144568', 'https://openalex.org/W6859665146', 'https://openalex.org/W4402672020', 'https://openalex.org/W4301206121', 'https://openalex.org/W4389363808', 'https://openalex.org/W2795671189', 'https://openalex.org/W3203491020', 'https://openalex.org/W4391591803', 'https://openalex.org/W4392538788', 'https://openalex.org/W4390690192', 'https://openalex.org/W2943552823', 'https://openalex.org/W4399554695', 'https://openalex.org/W1492383498', 'https://openalex.org/W4380714711', 'https://openalex.org/W4221150649', 'https://openalex.org/W4311000453', 'https://openalex.org/W4287694050', 'https://openalex.org/W3036601975']",2024-12-02
https://openalex.org/W4402082073,https://doi.org/10.1007/978-3-031-70566-3_9,Effects of Training Strategies and the Amount of Speech Data on the Quality of Speech Synthesis,,"['https://openalex.org/W4312320922', 'https://openalex.org/W4385993834', 'https://openalex.org/W4386894134', 'https://openalex.org/W4392538788', 'https://openalex.org/W2948978827', 'https://openalex.org/W2107860279', 'https://openalex.org/W4312339389', 'https://openalex.org/W4295939982', 'https://openalex.org/W1964146567', 'https://openalex.org/W4240592325', 'https://openalex.org/W3198848374', 'https://openalex.org/W4378602476']",2024-01-01
https://openalex.org/W4406461977,https://doi.org/10.1109/slt61566.2024.10832229,SPMIS: An Investigation of Synthetic Spoken Misinformation Detection,,"['https://openalex.org/W4381679762', 'https://openalex.org/W6853188576', 'https://openalex.org/W4252812408', 'https://openalex.org/W6862144568', 'https://openalex.org/W4406461271', 'https://openalex.org/W4406461681', 'https://openalex.org/W3024920698', 'https://openalex.org/W4392872032', 'https://openalex.org/W2176804518', 'https://openalex.org/W2588445447', 'https://openalex.org/W6855853421', 'https://openalex.org/W6864895810', 'https://openalex.org/W2936802426', 'https://openalex.org/W3197358873', 'https://openalex.org/W4221138880', 'https://openalex.org/W6852755086', 'https://openalex.org/W4392904490', 'https://openalex.org/W6776929863', 'https://openalex.org/W1546425147', 'https://openalex.org/W6847255306', 'https://openalex.org/W6802523144', 'https://openalex.org/W4389519507', 'https://openalex.org/W4385570127', 'https://openalex.org/W2995181338', 'https://openalex.org/W6859665146', 'https://openalex.org/W6847363464', 'https://openalex.org/W6854866820', 'https://openalex.org/W6801088966', 'https://openalex.org/W3209984917', 'https://openalex.org/W2998702515', 'https://openalex.org/W6755207826', 'https://openalex.org/W2144211451', 'https://openalex.org/W4392538788', 'https://openalex.org/W4386302282', 'https://openalex.org/W3209641797', 'https://openalex.org/W4297816851', 'https://openalex.org/W3201773091', 'https://openalex.org/W4377010126', 'https://openalex.org/W4389363808', 'https://openalex.org/W4384918448', 'https://openalex.org/W4395065405', 'https://openalex.org/W2967606780', 'https://openalex.org/W4382201591', 'https://openalex.org/W4378506723']",2024-12-02
https://openalex.org/W4408933666,https://doi.org/10.1016/j.neucom.2025.130074,FastTalker: An unified framework for generating speech and conversational gestures from text,,"['https://openalex.org/W6763832098', 'https://openalex.org/W4391020683', 'https://openalex.org/W6796464841', 'https://openalex.org/W3154411171', 'https://openalex.org/W4200630629', 'https://openalex.org/W4386076250', 'https://openalex.org/W3083173864', 'https://openalex.org/W4312674262', 'https://openalex.org/W4386075984', 'https://openalex.org/W6798786167', 'https://openalex.org/W6778883912', 'https://openalex.org/W4385275735', 'https://openalex.org/W4230429791', 'https://openalex.org/W6781084731', 'https://openalex.org/W2978956737', 'https://openalex.org/W6746671576', 'https://openalex.org/W6859643701', 'https://openalex.org/W6863362160', 'https://openalex.org/W3206752670', 'https://openalex.org/W4213439605', 'https://openalex.org/W3180150746', 'https://openalex.org/W4392205891', 'https://openalex.org/W6773935006', 'https://openalex.org/W4312545227', 'https://openalex.org/W6683204974', 'https://openalex.org/W2962795401', 'https://openalex.org/W4304080460', 'https://openalex.org/W6611703090', 'https://openalex.org/W3204221554', 'https://openalex.org/W3025281718', 'https://openalex.org/W6762533536', 'https://openalex.org/W4318350985', 'https://openalex.org/W3198131199', 'https://openalex.org/W2155027007', 'https://openalex.org/W4393148441', 'https://openalex.org/W4385823163', 'https://openalex.org/W3033411150', 'https://openalex.org/W3122282950', 'https://openalex.org/W1522301498', 'https://openalex.org/W4392538788', 'https://openalex.org/W2951104886', 'https://openalex.org/W4221142137', 'https://openalex.org/W2769666294', 'https://openalex.org/W4366460484', 'https://openalex.org/W2896457183', 'https://openalex.org/W3125775899', 'https://openalex.org/W603671054', 'https://openalex.org/W4225956675', 'https://openalex.org/W4385764101', 'https://openalex.org/W4292779060', 'https://openalex.org/W4390572877', 'https://openalex.org/W2946200149', 'https://openalex.org/W4403322517']",2025-03-28
https://openalex.org/W4401667143,https://doi.org/10.54097/tk7x3833,disentanglement using pre-trained features,"This article proposes using pre trained features to address the challenge of detecting depression through speech. Traditional raw audio has shown low accuracy and insufficient generalization performance in depression detection. We use pre trained models that have been developed to extract features, which can be used to extract general feature representations from speech data. During the pre training process, we further decouple the speakers, introducing prior information and providing a better starting point for training downstream models. The results indicate that we achieved the best performance when using the extracted features from the CONTANTVEC pre- trained model with speaker decoupling improvement.","['https://openalex.org/W4220948941', 'https://openalex.org/W4225304582', 'https://openalex.org/W2994403800', 'https://openalex.org/W3003922336', 'https://openalex.org/W3197558301', 'https://openalex.org/W2088846517', 'https://openalex.org/W3036548303', 'https://openalex.org/W2054931962', 'https://openalex.org/W3161135402', 'https://openalex.org/W2394781531', 'https://openalex.org/W2889378348', 'https://openalex.org/W4225635674', 'https://openalex.org/W3204696009', 'https://openalex.org/W4225327688', 'https://openalex.org/W2936879941', 'https://openalex.org/W2003502731', 'https://openalex.org/W4224924082', 'https://openalex.org/W3024869864', 'https://openalex.org/W4395444071', 'https://openalex.org/W4392538788']",2024-08-06
https://openalex.org/W4402081863,https://doi.org/10.1007/978-3-031-70566-3_5,Zero-Shot vs. Few-Shot Multi-speaker TTS Using Pre-trained Czech SpeechT5 Model,,"['https://openalex.org/W3205644108', 'https://openalex.org/W2995929068', 'https://openalex.org/W3213029956', 'https://openalex.org/W4391021560', 'https://openalex.org/W6936129901', 'https://openalex.org/W4392538788', 'https://openalex.org/W1574170747', 'https://openalex.org/W4382603054', 'https://openalex.org/W4385823117', 'https://openalex.org/W4297841455', 'https://openalex.org/W2981852735', 'https://openalex.org/W4366460484', 'https://openalex.org/W2807627734', 'https://openalex.org/W3196771368', 'https://openalex.org/W6739901393', 'https://openalex.org/W3119308075', 'https://openalex.org/W4313679638', 'https://openalex.org/W4387323811', 'https://openalex.org/W2972359262']",2024-01-01
https://openalex.org/W4405193419,https://doi.org/10.5753/stil.2024.245424,EyetrackingMOS: Proposal for an online evaluation method for speech synthesis models,"Evaluating Text-To-Speech (TTS) systems is challenging, as the increasing quality of synthesis makes it difficult to discriminate models’ ability to reproduce prosodic attributes, especially for Brazilian Portuguese. Offline evaluation metrics do not capture our genuine reactions to audio stimuli. Therefore, we propose an online evaluation method using eye-tracking. Our experiments with 76 annotators show a reasonable correlation between EyetrackingMOS and MOS, as well as a reduction in the total evaluation time. We believe this metric provides precise and potentially fast information to complement existing evaluation methods.","['https://openalex.org/W3184629526', 'https://openalex.org/W7075679439', 'https://openalex.org/W6638482716', 'https://openalex.org/W3196584150', 'https://openalex.org/W4200631896', 'https://openalex.org/W4285169718', 'https://openalex.org/W4393859250', 'https://openalex.org/W2911827218', 'https://openalex.org/W2808706139', 'https://openalex.org/W4392538788', 'https://openalex.org/W2587284713', 'https://openalex.org/W4387849500', 'https://openalex.org/W2976040490', 'https://openalex.org/W349236604', 'https://openalex.org/W4372267700', 'https://openalex.org/W6778823374', 'https://openalex.org/W4280561221', 'https://openalex.org/W2160473997', 'https://openalex.org/W4372259881', 'https://openalex.org/W4366460484']",2024-11-17
https://openalex.org/W4406461435,https://doi.org/10.1109/slt61566.2024.10832250,DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset,,"['https://openalex.org/W6850266965', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W4385245566', 'https://openalex.org/W2903739847', 'https://openalex.org/W3150572638', 'https://openalex.org/W3096442195', 'https://openalex.org/W6763832098', 'https://openalex.org/W6778823374', 'https://openalex.org/W6777694618', 'https://openalex.org/W6854555942', 'https://openalex.org/W6846539466', 'https://openalex.org/W3198213150', 'https://openalex.org/W6851724922', 'https://openalex.org/W6853937136', 'https://openalex.org/W6860653961', 'https://openalex.org/W4304099317', 'https://openalex.org/W6849953009', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783182287', 'https://openalex.org/W6811008979', 'https://openalex.org/W6840257003', 'https://openalex.org/W4252812408', 'https://openalex.org/W4392931276', 'https://openalex.org/W6853075663', 'https://openalex.org/W2176804518', 'https://openalex.org/W4306867196', 'https://openalex.org/W4402112543', 'https://openalex.org/W4402111541', 'https://openalex.org/W4402111788', 'https://openalex.org/W4392930617', 'https://openalex.org/W2807325376', 'https://openalex.org/W3128666957', 'https://openalex.org/W4402116004', 'https://openalex.org/W4381198892', 'https://openalex.org/W6799089041', 'https://openalex.org/W4225649083', 'https://openalex.org/W4391046687', 'https://openalex.org/W6772349387', 'https://openalex.org/W6745289305', 'https://openalex.org/W4381708595', 'https://openalex.org/W1531799344', 'https://openalex.org/W3201773091', 'https://openalex.org/W3163596559', 'https://openalex.org/W2911286998', 'https://openalex.org/W6864895810', 'https://openalex.org/W6795261426', 'https://openalex.org/W6783867762', 'https://openalex.org/W4392904491', 'https://openalex.org/W6862144568', 'https://openalex.org/W4396542467', 'https://openalex.org/W6857102672', 'https://openalex.org/W4225956675', 'https://openalex.org/W2295519251', 'https://openalex.org/W2745644908', 'https://openalex.org/W4385822628', 'https://openalex.org/W3033411150', 'https://openalex.org/W3123097577', 'https://openalex.org/W4395065405', 'https://openalex.org/W4287120591', 'https://openalex.org/W4366460484', 'https://openalex.org/W2765486990', 'https://openalex.org/W4226246616', 'https://openalex.org/W4303647933', 'https://openalex.org/W4391271997', 'https://openalex.org/W3131786367', 'https://openalex.org/W4384648648', 'https://openalex.org/W4297841787', 'https://openalex.org/W4377864733', 'https://openalex.org/W3129651364', 'https://openalex.org/W4392538788', 'https://openalex.org/W4226213470', 'https://openalex.org/W4360890968', 'https://openalex.org/W4402351686', 'https://openalex.org/W4387322998']",2024-12-02
https://openalex.org/W4406461591,https://doi.org/10.1109/slt61566.2024.10832348,Improving Curriculum Learning For Target Speaker Extraction With Synthetic Speakers,,"['https://openalex.org/W2951130829', 'https://openalex.org/W2973062255', 'https://openalex.org/W1494198834', 'https://openalex.org/W6890361730', 'https://openalex.org/W6862144568', 'https://openalex.org/W6805710207', 'https://openalex.org/W6848735303', 'https://openalex.org/W4385823432', 'https://openalex.org/W4391021726', 'https://openalex.org/W4402112065', 'https://openalex.org/W3142849873', 'https://openalex.org/W3016232124', 'https://openalex.org/W4392931778', 'https://openalex.org/W4379058083', 'https://openalex.org/W4372269365', 'https://openalex.org/W3209984917', 'https://openalex.org/W6783867762', 'https://openalex.org/W3190394743', 'https://openalex.org/W6777776875', 'https://openalex.org/W2291877678', 'https://openalex.org/W3024869864', 'https://openalex.org/W2808631503', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313679638', 'https://openalex.org/W4392538788', 'https://openalex.org/W3027008958']",2024-12-02
https://openalex.org/W4409385411,https://doi.org/10.1016/j.neunet.2025.107432,CMDF-TTS: Text-to-speech method with limited target speaker corpus,,"['https://openalex.org/W6741832134', 'https://openalex.org/W6853444998', 'https://openalex.org/W6793018716', 'https://openalex.org/W2963426391', 'https://openalex.org/W6743252760', 'https://openalex.org/W6842838306', 'https://openalex.org/W6767966793', 'https://openalex.org/W6785546593', 'https://openalex.org/W6753797277', 'https://openalex.org/W6769900187', 'https://openalex.org/W6810869769', 'https://openalex.org/W6725978818', 'https://openalex.org/W2120847449', 'https://openalex.org/W4312388283', 'https://openalex.org/W6735913928', 'https://openalex.org/W6779823529', 'https://openalex.org/W6840314289', 'https://openalex.org/W6855736130', 'https://openalex.org/W6846143095', 'https://openalex.org/W6839301689', 'https://openalex.org/W6785653481', 'https://openalex.org/W6792476516', 'https://openalex.org/W6936129901', 'https://openalex.org/W6777168481', 'https://openalex.org/W6841663119', 'https://openalex.org/W4390075359', 'https://openalex.org/W6777694618', 'https://openalex.org/W6796464841', 'https://openalex.org/W6783182287', 'https://openalex.org/W4309182233', 'https://openalex.org/W6848461853', 'https://openalex.org/W6757079273', 'https://openalex.org/W6810302181', 'https://openalex.org/W3106690208', 'https://openalex.org/W6856500870', 'https://openalex.org/W2963140444', 'https://openalex.org/W6760754431', 'https://openalex.org/W6795261426', 'https://openalex.org/W2991327923', 'https://openalex.org/W6778823374', 'https://openalex.org/W6811003946', 'https://openalex.org/W6633117090', 'https://openalex.org/W6809884996', 'https://openalex.org/W4221144919', 'https://openalex.org/W6797157791', 'https://openalex.org/W6851724922', 'https://openalex.org/W6746700228', 'https://openalex.org/W6800720828', 'https://openalex.org/W6749555683', 'https://openalex.org/W6783713337', 'https://openalex.org/W6667372801', 'https://openalex.org/W4391020683', 'https://openalex.org/W2769810959', 'https://openalex.org/W6739901393', 'https://openalex.org/W4322629454', 'https://openalex.org/W4393187487', 'https://openalex.org/W4386170336', 'https://openalex.org/W6763501271', 'https://openalex.org/W4386075993', 'https://openalex.org/W6782342302', 'https://openalex.org/W2887511658', 'https://openalex.org/W3041133507', 'https://openalex.org/W4285306484', 'https://openalex.org/W1522301498', 'https://openalex.org/W4320013936', 'https://openalex.org/W4321231854', 'https://openalex.org/W4399425629', 'https://openalex.org/W4313447020', 'https://openalex.org/W2125389028', 'https://openalex.org/W2994143516', 'https://openalex.org/W2899575547', 'https://openalex.org/W1583837637', 'https://openalex.org/W4313679638', 'https://openalex.org/W4255113413', 'https://openalex.org/W4246270042', 'https://openalex.org/W2603947633', 'https://openalex.org/W4292419533', 'https://openalex.org/W4234842379', 'https://openalex.org/W2990138404', 'https://openalex.org/W2914911817', 'https://openalex.org/W4379924545', 'https://openalex.org/W4392538788', 'https://openalex.org/W4365806309', 'https://openalex.org/W4360890968', 'https://openalex.org/W4295521014', 'https://openalex.org/W4376548770', 'https://openalex.org/W3174758275', 'https://openalex.org/W4385245566']",2025-04-12
https://openalex.org/W4410184628,https://doi.org/10.2139/ssrn.5244226,A Speech Prediction Model Based on Codec Modeling and Transformer Decoding,,"['https://openalex.org/W2296153915', 'https://openalex.org/W1864555471', 'https://openalex.org/W4226265417', 'https://openalex.org/W3025035610', 'https://openalex.org/W2981687423', 'https://openalex.org/W2926827382', 'https://openalex.org/W4307323391', 'https://openalex.org/W4224875149', 'https://openalex.org/W6755257315', 'https://openalex.org/W2145103350', 'https://openalex.org/W2980292555', 'https://openalex.org/W4365205190', 'https://openalex.org/W2112464782', 'https://openalex.org/W4392538788', 'https://openalex.org/W6748409065', 'https://openalex.org/W4404102497', 'https://openalex.org/W6769276668', 'https://openalex.org/W6631190155', 'https://openalex.org/W82776184', 'https://openalex.org/W3092028330', 'https://openalex.org/W1561829009', 'https://openalex.org/W4297841545', 'https://openalex.org/W1550397807', 'https://openalex.org/W4296068992', 'https://openalex.org/W2799789537', 'https://openalex.org/W4389317961', 'https://openalex.org/W4393294847', 'https://openalex.org/W2898371500', 'https://openalex.org/W2321297794', 'https://openalex.org/W1964600646', 'https://openalex.org/W4387682234', 'https://openalex.org/W4283697979', 'https://openalex.org/W4307536105', 'https://openalex.org/W4379256228', 'https://openalex.org/W6784121926', 'https://openalex.org/W3178638614', 'https://openalex.org/W2793014896', 'https://openalex.org/W2556910157', 'https://openalex.org/W2898847420', 'https://openalex.org/W3002741552', 'https://openalex.org/W4225302959', 'https://openalex.org/W1552314771', 'https://openalex.org/W2132036212', 'https://openalex.org/W2802304149', 'https://openalex.org/W6796450881', 'https://openalex.org/W2889442120', 'https://openalex.org/W4280561221', 'https://openalex.org/W4280561867', 'https://openalex.org/W3207100935', 'https://openalex.org/W4385848954', 'https://openalex.org/W4224903235', 'https://openalex.org/W4387687749', 'https://openalex.org/W4386764852', 'https://openalex.org/W1955313418', 'https://openalex.org/W3215615641', 'https://openalex.org/W3141042598', 'https://openalex.org/W6769487070']",2025-01-01
https://openalex.org/W4411086595,https://doi.org/10.1109/taslpro.2025.3577324,Hierarchical Diffusion Model for Zero-Shot Singing Voice Synthesis With MIDI Priors,,"['https://openalex.org/W4390604258', 'https://openalex.org/W4392931276', 'https://openalex.org/W6862144568', 'https://openalex.org/W4396214748', 'https://openalex.org/W3196760057', 'https://openalex.org/W3158762648', 'https://openalex.org/W4385571362', 'https://openalex.org/W3187128306', 'https://openalex.org/W4403791262', 'https://openalex.org/W4396542467', 'https://openalex.org/W6858915148', 'https://openalex.org/W4402115961', 'https://openalex.org/W4406752483', 'https://openalex.org/W3207340675', 'https://openalex.org/W4406461759', 'https://openalex.org/W4393160753', 'https://openalex.org/W4404783513', 'https://openalex.org/W4367359628', 'https://openalex.org/W4380434618', 'https://openalex.org/W4385823264', 'https://openalex.org/W4226320669', 'https://openalex.org/W6763832098', 'https://openalex.org/W4392903036', 'https://openalex.org/W6851724922', 'https://openalex.org/W6771763809', 'https://openalex.org/W4387968164', 'https://openalex.org/W6795261426', 'https://openalex.org/W4375869198', 'https://openalex.org/W2990440871', 'https://openalex.org/W6803547063', 'https://openalex.org/W4387350832', 'https://openalex.org/W3016250102', 'https://openalex.org/W6810689344', 'https://openalex.org/W6849953009', 'https://openalex.org/W4372259784', 'https://openalex.org/W6795807602', 'https://openalex.org/W6779823529', 'https://openalex.org/W6786375611', 'https://openalex.org/W4390872297', 'https://openalex.org/W6838843145', 'https://openalex.org/W2973046048', 'https://openalex.org/W6802838302', 'https://openalex.org/W6783867762', 'https://openalex.org/W4408352903', 'https://openalex.org/W4389519221', 'https://openalex.org/W1901129140', 'https://openalex.org/W2519091744', 'https://openalex.org/W4388927799', 'https://openalex.org/W3110257065', 'https://openalex.org/W4392538788', 'https://openalex.org/W4281736089', 'https://openalex.org/W3000389243', 'https://openalex.org/W4366460484', 'https://openalex.org/W4286899907', 'https://openalex.org/W4225919501', 'https://openalex.org/W3190244907']",2025-01-01
https://openalex.org/W4406800520,https://doi.org/10.1007/s11432-024-4222-0,The rise and potential of large language model based agents: a survey,,"['https://openalex.org/W2124344619', 'https://openalex.org/W1537234190', 'https://openalex.org/W2397253692', 'https://openalex.org/W2070545326', 'https://openalex.org/W2029130131', 'https://openalex.org/W2107726111', 'https://openalex.org/W2058517091', 'https://openalex.org/W2165718398', 'https://openalex.org/W4363671832', 'https://openalex.org/W6853127500', 'https://openalex.org/W4226278401', 'https://openalex.org/W6838461927', 'https://openalex.org/W6856464235', 'https://openalex.org/W4360836968', 'https://openalex.org/W1504553351', 'https://openalex.org/W2144442672', 'https://openalex.org/W6675145293', 'https://openalex.org/W2255817990', 'https://openalex.org/W1993251552', 'https://openalex.org/W2111438441', 'https://openalex.org/W1540044141', 'https://openalex.org/W1968079292', 'https://openalex.org/W1519698894', 'https://openalex.org/W2097856935', 'https://openalex.org/W2045031658', 'https://openalex.org/W2118756286', 'https://openalex.org/W2131600418', 'https://openalex.org/W2257979135', 'https://openalex.org/W6637967152', 'https://openalex.org/W6750629867', 'https://openalex.org/W2891790128', 'https://openalex.org/W3121342653', 'https://openalex.org/W3181634442', 'https://openalex.org/W6676875888', 'https://openalex.org/W2578206533', 'https://openalex.org/W2788904251', 'https://openalex.org/W2923504512', 'https://openalex.org/W6768602481', 'https://openalex.org/W2803180393', 'https://openalex.org/W6849169497', 'https://openalex.org/W6778883912', 'https://openalex.org/W4386184788', 'https://openalex.org/W4226227340', 'https://openalex.org/W4304195432', 'https://openalex.org/W4320165837', 'https://openalex.org/W4366566341', 'https://openalex.org/W6852755768', 'https://openalex.org/W4221143046', 'https://openalex.org/W6838865847', 'https://openalex.org/W4221161695', 'https://openalex.org/W4281483047', 'https://openalex.org/W4378470461', 'https://openalex.org/W4353112996', 'https://openalex.org/W4310998175', 'https://openalex.org/W4376654477', 'https://openalex.org/W4322718421', 'https://openalex.org/W6800875267', 'https://openalex.org/W3205068155', 'https://openalex.org/W6847076894', 'https://openalex.org/W7064840731', 'https://openalex.org/W6854738257', 'https://openalex.org/W4378509427', 'https://openalex.org/W4378945542', 'https://openalex.org/W6809908440', 'https://openalex.org/W4366850747', 'https://openalex.org/W6852989508', 'https://openalex.org/W6850503672', 'https://openalex.org/W4378465306', 'https://openalex.org/W2001784601', 'https://openalex.org/W4378711594', 'https://openalex.org/W4321011818', 'https://openalex.org/W4366999541', 'https://openalex.org/W2336741951', 'https://openalex.org/W6810581500', 'https://openalex.org/W4377111802', 'https://openalex.org/W4383473935', 'https://openalex.org/W6683738474', 'https://openalex.org/W6604551671', 'https://openalex.org/W1971434212', 'https://openalex.org/W1502957213', 'https://openalex.org/W3044438666', 'https://openalex.org/W4385567216', 'https://openalex.org/W2792026792', 'https://openalex.org/W6809838524', 'https://openalex.org/W4309804142', 'https://openalex.org/W6803096969', 'https://openalex.org/W4384561707', 'https://openalex.org/W3034238904', 'https://openalex.org/W1969735301', 'https://openalex.org/W2013982304', 'https://openalex.org/W4367189927', 'https://openalex.org/W6855425347', 'https://openalex.org/W4377163995', 'https://openalex.org/W4378768661', 'https://openalex.org/W4385714639', 'https://openalex.org/W4378509422', 'https://openalex.org/W4385970259', 'https://openalex.org/W4379933140', 'https://openalex.org/W1997480541', 'https://openalex.org/W1986884366', 'https://openalex.org/W6851275496', 'https://openalex.org/W4386978002', 'https://openalex.org/W4391710008', 'https://openalex.org/W6811284106', 'https://openalex.org/W2026085771', 'https://openalex.org/W1974922297', 'https://openalex.org/W2004550196', 'https://openalex.org/W2614653677', 'https://openalex.org/W4224912544', 'https://openalex.org/W4378942217', 'https://openalex.org/W4392637287', 'https://openalex.org/W4372272282', 'https://openalex.org/W4378770584', 'https://openalex.org/W4377130677', 'https://openalex.org/W4389520747', 'https://openalex.org/W4221152848', 'https://openalex.org/W4225405251', 'https://openalex.org/W4378468533', 'https://openalex.org/W4378505261', 'https://openalex.org/W4285428875', 'https://openalex.org/W4327667441', 'https://openalex.org/W2097585998', 'https://openalex.org/W6748634344', 'https://openalex.org/W2896457183', 'https://openalex.org/W4312205996', 'https://openalex.org/W6848909144', 'https://openalex.org/W2094848746', 'https://openalex.org/W3156470785', 'https://openalex.org/W4310832511', 'https://openalex.org/W4313679638', 'https://openalex.org/W3177174258', 'https://openalex.org/W6843258702', 'https://openalex.org/W4364387438', 'https://openalex.org/W4323651091', 'https://openalex.org/W2116360511', 'https://openalex.org/W2144730066', 'https://openalex.org/W4378474033', 'https://openalex.org/W2626804490', 'https://openalex.org/W6783880405', 'https://openalex.org/W3034975599', 'https://openalex.org/W6766818547', 'https://openalex.org/W6775772702', 'https://openalex.org/W6771429369', 'https://openalex.org/W3129503315', 'https://openalex.org/W6852776751', 'https://openalex.org/W6856012975', 'https://openalex.org/W4322718246', 'https://openalex.org/W4382490555', 'https://openalex.org/W4318718936', 'https://openalex.org/W4376312115', 'https://openalex.org/W6852447913', 'https://openalex.org/W4375958083', 'https://openalex.org/W4380994269', 'https://openalex.org/W4380137126', 'https://openalex.org/W4367061106', 'https://openalex.org/W4311000453', 'https://openalex.org/W2946200149', 'https://openalex.org/W3209059054', 'https://openalex.org/W4392669753', 'https://openalex.org/W4321472132', 'https://openalex.org/W3213407400', 'https://openalex.org/W6776750061', 'https://openalex.org/W4379138252', 'https://openalex.org/W3116286104', 'https://openalex.org/W4385374425', 'https://openalex.org/W4322718191', 'https://openalex.org/W4308760226', 'https://openalex.org/W2962736495', 'https://openalex.org/W6854340891', 'https://openalex.org/W6604405615', 'https://openalex.org/W2296073425', 'https://openalex.org/W6798182279', 'https://openalex.org/W6851896007', 'https://openalex.org/W4380993239', 'https://openalex.org/W4365597205', 'https://openalex.org/W4323717348', 'https://openalex.org/W2103104224', 'https://openalex.org/W4207072548', 'https://openalex.org/W2810785043', 'https://openalex.org/W4319165647', 'https://openalex.org/W2799002257', 'https://openalex.org/W6786772999', 'https://openalex.org/W3189250028', 'https://openalex.org/W2987774896', 'https://openalex.org/W4292178068', 'https://openalex.org/W6761234640', 'https://openalex.org/W4318908031', 'https://openalex.org/W4296414573', 'https://openalex.org/W2982118014', 'https://openalex.org/W3115640848', 'https://openalex.org/W4379924706', 'https://openalex.org/W3066254485', 'https://openalex.org/W4306175879', 'https://openalex.org/W6846512445', 'https://openalex.org/W4319323461', 'https://openalex.org/W4378945292', 'https://openalex.org/W4285069854', 'https://openalex.org/W6786622191', 'https://openalex.org/W6846310961', 'https://openalex.org/W4221144910', 'https://openalex.org/W6811476558', 'https://openalex.org/W2997528589', 'https://openalex.org/W4311559655', 'https://openalex.org/W4385473486', 'https://openalex.org/W4283218993', 'https://openalex.org/W3173930728', 'https://openalex.org/W6777324918', 'https://openalex.org/W4318751977', 'https://openalex.org/W4388483618', 'https://openalex.org/W4385327683', 'https://openalex.org/W4385262076', 'https://openalex.org/W4377372292', 'https://openalex.org/W4283828996', 'https://openalex.org/W3141082077', 'https://openalex.org/W6853453387', 'https://openalex.org/W6787130917', 'https://openalex.org/W4385573990', 'https://openalex.org/W4389109049', 'https://openalex.org/W4382498654', 'https://openalex.org/W4327550249', 'https://openalex.org/W4226159083', 'https://openalex.org/W4385947658', 'https://openalex.org/W4308165003', 'https://openalex.org/W6780281623', 'https://openalex.org/W4318751399', 'https://openalex.org/W2497473826', 'https://openalex.org/W6683839571', 'https://openalex.org/W6852426770', 'https://openalex.org/W4361865652', 'https://openalex.org/W4396902788', 'https://openalex.org/W4385963839', 'https://openalex.org/W1993533232', 'https://openalex.org/W2911296969', 'https://openalex.org/W2766447205', 'https://openalex.org/W2625113742', 'https://openalex.org/W7019202113', 'https://openalex.org/W4385849309', 'https://openalex.org/W6793423792', 'https://openalex.org/W4321760789', 'https://openalex.org/W1949907236', 'https://openalex.org/W46490633', 'https://openalex.org/W2636355936', 'https://openalex.org/W4411517540', 'https://openalex.org/W4390832893', 'https://openalex.org/W6730098006', 'https://openalex.org/W6737850504', 'https://openalex.org/W4290771878', 'https://openalex.org/W4224861986', 'https://openalex.org/W2798753108', 'https://openalex.org/W2970393840', 'https://openalex.org/W3150595019', 'https://openalex.org/W4287887895', 'https://openalex.org/W4361807044', 'https://openalex.org/W4300230145', 'https://openalex.org/W4384613928', 'https://openalex.org/W4366851012', 'https://openalex.org/W6842239938', 'https://openalex.org/W4383604266', 'https://openalex.org/W4389520259', 'https://openalex.org/W4385681988', 'https://openalex.org/W2623357307', 'https://openalex.org/W3100551981', 'https://openalex.org/W4376988648', 'https://openalex.org/W4380993798', 'https://openalex.org/W3139433464', 'https://openalex.org/W6767670765', 'https://openalex.org/W3089860869', 'https://openalex.org/W6839226615', 'https://openalex.org/W4385681459', 'https://openalex.org/W2897690397', 'https://openalex.org/W4284966012', 'https://openalex.org/W3159807213', 'https://openalex.org/W4391944659', 'https://openalex.org/W3104266443', 'https://openalex.org/W4305033123', 'https://openalex.org/W4309663019', 'https://openalex.org/W2970894611', 'https://openalex.org/W2913781869', 'https://openalex.org/W6853822724', 'https://openalex.org/W2952204218', 'https://openalex.org/W4385681712', 'https://openalex.org/W3206525235', 'https://openalex.org/W4240912089', 'https://openalex.org/W4232406588', 'https://openalex.org/W4386644471', 'https://openalex.org/W4387799829', 'https://openalex.org/W3013499767', 'https://openalex.org/W3215698779', 'https://openalex.org/W4384072911', 'https://openalex.org/W4283263983', 'https://openalex.org/W4386384991', 'https://openalex.org/W4380551925', 'https://openalex.org/W4378470708', 'https://openalex.org/W6849906535', 'https://openalex.org/W1993584577', 'https://openalex.org/W4312091865', 'https://openalex.org/W6854260779', 'https://openalex.org/W4302445826', 'https://openalex.org/W4379919636', 'https://openalex.org/W2091786736', 'https://openalex.org/W6753101812', 'https://openalex.org/W2972550240', 'https://openalex.org/W2922424071', 'https://openalex.org/W4385374144', 'https://openalex.org/W4380763235', 'https://openalex.org/W4367365575', 'https://openalex.org/W1515781139', 'https://openalex.org/W2734203669', 'https://openalex.org/W2158076918', 'https://openalex.org/W2010524426', 'https://openalex.org/W2799899844', 'https://openalex.org/W4375870056', 'https://openalex.org/W4237556202', 'https://openalex.org/W2074812030', 'https://openalex.org/W2117014758', 'https://openalex.org/W2106731506', 'https://openalex.org/W6854015811', 'https://openalex.org/W4237262806', 'https://openalex.org/W1672610948', 'https://openalex.org/W4322626329', 'https://openalex.org/W4226112058', 'https://openalex.org/W4365211621', 'https://openalex.org/W3188785232', 'https://openalex.org/W2755689238', 'https://openalex.org/W4309217888', 'https://openalex.org/W4378465112', 'https://openalex.org/W4362631702', 'https://openalex.org/W3172205429', 'https://openalex.org/W2963919731', 'https://openalex.org/W4383895105', 'https://openalex.org/W4221159410', 'https://openalex.org/W6847753483', 'https://openalex.org/W4223908421', 'https://openalex.org/W4399455192', 'https://openalex.org/W6856086692', 'https://openalex.org/W4385714628', 'https://openalex.org/W6772715161', 'https://openalex.org/W3205272844', 'https://openalex.org/W4389519042', 'https://openalex.org/W4241563224', 'https://openalex.org/W4362655849', 'https://openalex.org/W4386185147', 'https://openalex.org/W4226414096', 'https://openalex.org/W4296413526', 'https://openalex.org/W6839548382', 'https://openalex.org/W6637162671', 'https://openalex.org/W2640329709', 'https://openalex.org/W6855939272', 'https://openalex.org/W4226284056', 'https://openalex.org/W2602856279', 'https://openalex.org/W2963859254', 'https://openalex.org/W3006647218', 'https://openalex.org/W4385574291', 'https://openalex.org/W6735677848', 'https://openalex.org/W4290858323', 'https://openalex.org/W2989807128', 'https://openalex.org/W2971970905', 'https://openalex.org/W2949128310', 'https://openalex.org/W4379958452', 'https://openalex.org/W3204619801', 'https://openalex.org/W3199419198', 'https://openalex.org/W4296841006', 'https://openalex.org/W6849590751', 'https://openalex.org/W4385573090', 'https://openalex.org/W4380353722', 'https://openalex.org/W3044324512', 'https://openalex.org/W3177190797', 'https://openalex.org/W4285266418', 'https://openalex.org/W6755310938', 'https://openalex.org/W2913266441', 'https://openalex.org/W3036286896', 'https://openalex.org/W4377371478', 'https://openalex.org/W2981852735', 'https://openalex.org/W4308015675', 'https://openalex.org/W3037831233', 'https://openalex.org/W3034115845', 'https://openalex.org/W2483215953', 'https://openalex.org/W2893425640', 'https://openalex.org/W4309674289', 'https://openalex.org/W4378501037', 'https://openalex.org/W3034383590', 'https://openalex.org/W4383987670', 'https://openalex.org/W4402722824', 'https://openalex.org/W6852476831', 'https://openalex.org/W4388843580', 'https://openalex.org/W2787887017', 'https://openalex.org/W6800751262', 'https://openalex.org/W4378474356', 'https://openalex.org/W3134756955', 'https://openalex.org/W4391047179', 'https://openalex.org/W4378718522', 'https://openalex.org/W3092516542', 'https://openalex.org/W4377864377', 'https://openalex.org/W4368304377', 'https://openalex.org/W1562983757', 'https://openalex.org/W2013116016', 'https://openalex.org/W2275530856', 'https://openalex.org/W2135150302', 'https://openalex.org/W6635852658', 'https://openalex.org/W6810313920', 'https://openalex.org/W4322832290', 'https://openalex.org/W4385430086', 'https://openalex.org/W4287026640', 'https://openalex.org/W3154565472', 'https://openalex.org/W4385473492', 'https://openalex.org/W4404826238', 'https://openalex.org/W4293253344', 'https://openalex.org/W4387993371', 'https://openalex.org/W3185212449', 'https://openalex.org/W4386114002', 'https://openalex.org/W4205185581', 'https://openalex.org/W2612560781', 'https://openalex.org/W1583837637', 'https://openalex.org/W2748789698', 'https://openalex.org/W1479925024', 'https://openalex.org/W4387835442', 'https://openalex.org/W4380715521', 'https://openalex.org/W4311991106', 'https://openalex.org/W4401042671', 'https://openalex.org/W4307079201', 'https://openalex.org/W3034655362', 'https://openalex.org/W2620949368', 'https://openalex.org/W4385473851', 'https://openalex.org/W4298857966', 'https://openalex.org/W4376652803', 'https://openalex.org/W4385261769', 'https://openalex.org/W4298174377', 'https://openalex.org/W4379251535', 'https://openalex.org/W3148330722', 'https://openalex.org/W4319453300', 'https://openalex.org/W4388744821', 'https://openalex.org/W3177813494', 'https://openalex.org/W4386081135', 'https://openalex.org/W2569616450', 'https://openalex.org/W2893662673', 'https://openalex.org/W3039636162', 'https://openalex.org/W2019641150', 'https://openalex.org/W4389523771', 'https://openalex.org/W4311408938', 'https://openalex.org/W4294435850', 'https://openalex.org/W4379919478', 'https://openalex.org/W4214717370', 'https://openalex.org/W4385682136', 'https://openalex.org/W4389519587', 'https://openalex.org/W3129717163', 'https://openalex.org/W4313483544', 'https://openalex.org/W4285429195', 'https://openalex.org/W4382132560', 'https://openalex.org/W4394645559', 'https://openalex.org/W4377865309', 'https://openalex.org/W4385970197', 'https://openalex.org/W4385468994', 'https://openalex.org/W4364384540', 'https://openalex.org/W1869215669', 'https://openalex.org/W108702333', 'https://openalex.org/W4386200967', 'https://openalex.org/W4283026156', 'https://openalex.org/W2042538315', 'https://openalex.org/W2955254504', 'https://openalex.org/W4231016000', 'https://openalex.org/W1598960279', 'https://openalex.org/W4401042726', 'https://openalex.org/W4384111857', 'https://openalex.org/W4386528753', 'https://openalex.org/W2935689779', 'https://openalex.org/W4308902180', 'https://openalex.org/W4321471914', 'https://openalex.org/W4402671548', 'https://openalex.org/W4390874280', 'https://openalex.org/W4309395891', 'https://openalex.org/W3139377883', 'https://openalex.org/W4286892945', 'https://openalex.org/W3111265093', 'https://openalex.org/W4306179311', 'https://openalex.org/W4385928960', 'https://openalex.org/W3103559770', 'https://openalex.org/W3195577433', 'https://openalex.org/W4389636360', 'https://openalex.org/W4307106501', 'https://openalex.org/W4365211632', 'https://openalex.org/W4366328015', 'https://openalex.org/W4385970117', 'https://openalex.org/W3011120880', 'https://openalex.org/W2318875203', 'https://openalex.org/W4287900772', 'https://openalex.org/W2769533150', 'https://openalex.org/W2797527950', 'https://openalex.org/W4385963592', 'https://openalex.org/W4385889839', 'https://openalex.org/W1937795883', 'https://openalex.org/W4312091380', 'https://openalex.org/W4362508231', 'https://openalex.org/W4404782219', 'https://openalex.org/W4287115670', 'https://openalex.org/W4301423176', 'https://openalex.org/W4384111914', 'https://openalex.org/W4292424355', 'https://openalex.org/W371619641', 'https://openalex.org/W4376167553', 'https://openalex.org/W4389518771', 'https://openalex.org/W4380374951', 'https://openalex.org/W1841352775', 'https://openalex.org/W4401416363', 'https://openalex.org/W4309088836', 'https://openalex.org/W2606347107', 'https://openalex.org/W3108144224', 'https://openalex.org/W4317670814', 'https://openalex.org/W4367061108', 'https://openalex.org/W4405812781', 'https://openalex.org/W3107615218', 'https://openalex.org/W4388488349', 'https://openalex.org/W4389665575', 'https://openalex.org/W4205807230', 'https://openalex.org/W3084525380', 'https://openalex.org/W2122410182', 'https://openalex.org/W4385652328', 'https://openalex.org/W4281557623', 'https://openalex.org/W2498253860', 'https://openalex.org/W4378464864', 'https://openalex.org/W4311642023', 'https://openalex.org/W4389667254', 'https://openalex.org/W4316829821', 'https://openalex.org/W4385430679', 'https://openalex.org/W4384812233', 'https://openalex.org/W4361020574', 'https://openalex.org/W4378189609', 'https://openalex.org/W4385734161', 'https://openalex.org/W2149524486', 'https://openalex.org/W2750779823', 'https://openalex.org/W4380714623', 'https://openalex.org/W4389520273', 'https://openalex.org/W2521824566', 'https://openalex.org/W4378768622', 'https://openalex.org/W3094833381', 'https://openalex.org/W4385968125', 'https://openalex.org/W4321593931', 'https://openalex.org/W4383112908', 'https://openalex.org/W4378711593', 'https://openalex.org/W4361866080']",2025-01-17
https://openalex.org/W4389524500,https://doi.org/10.18653/v1/2023.findings-emnlp.1055,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.","['https://openalex.org/W4385807416', 'https://openalex.org/W4394671563', 'https://openalex.org/W3166396011', 'https://openalex.org/W4367061106', 'https://openalex.org/W4381786045', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385572615', 'https://openalex.org/W4323572061', 'https://openalex.org/W4361866031', 'https://openalex.org/W3140429000', 'https://openalex.org/W4322718246', 'https://openalex.org/W3169320628', 'https://openalex.org/W4310924890', 'https://openalex.org/W4287120025', 'https://openalex.org/W2995181338', 'https://openalex.org/W4307680525', 'https://openalex.org/W4313679638', 'https://openalex.org/W3036601975', 'https://openalex.org/W4224308101', 'https://openalex.org/W3094502228', 'https://openalex.org/W1494198834', 'https://openalex.org/W3168867926', 'https://openalex.org/W4366330503', 'https://openalex.org/W4375958083', 'https://openalex.org/W4361229539', 'https://openalex.org/W3030437843', 'https://openalex.org/W4386384714', 'https://openalex.org/W4377297670', 'https://openalex.org/W4323651091', 'https://openalex.org/W4322718191']",2023-01-01
https://openalex.org/W4401070302,https://doi.org/10.1109/taslp.2024.3434425,"VioLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation",,"['https://openalex.org/W6739901393', 'https://openalex.org/W6755207826', 'https://openalex.org/W6778883912', 'https://openalex.org/W4386071687', 'https://openalex.org/W3034999214', 'https://openalex.org/W6769627184', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W4307323391', 'https://openalex.org/W6850334629', 'https://openalex.org/W1995562189', 'https://openalex.org/W2998386507', 'https://openalex.org/W2136545725', 'https://openalex.org/W3211278025', 'https://openalex.org/W4388017359', 'https://openalex.org/W2127141656', 'https://openalex.org/W2962824709', 'https://openalex.org/W3161873870', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W6778823374', 'https://openalex.org/W2997540646', 'https://openalex.org/W6679436768', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3205644108', 'https://openalex.org/W6795200824', 'https://openalex.org/W6847363464', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W4391021623', 'https://openalex.org/W6860320428', 'https://openalex.org/W2962784628', 'https://openalex.org/W3203407300', 'https://openalex.org/W3095410713', 'https://openalex.org/W6634186343', 'https://openalex.org/W1494198834', 'https://openalex.org/W6770506093', 'https://openalex.org/W2936078256', 'https://openalex.org/W2101105183', 'https://openalex.org/W3097206152', 'https://openalex.org/W3118974591', 'https://openalex.org/W3163573274', 'https://openalex.org/W4292779060', 'https://openalex.org/W2946200149', 'https://openalex.org/W4288089799', 'https://openalex.org/W4381827575', 'https://openalex.org/W4390528611', 'https://openalex.org/W1572989473', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323651091', 'https://openalex.org/W3161940574', 'https://openalex.org/W4385245566', 'https://openalex.org/W2130942839']",2024-01-01
https://openalex.org/W4406385636,https://doi.org/10.1038/s41586-024-08359-z,Joint speech and text machine translation for up to 100 languages,"Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.","['https://openalex.org/W2097203679', 'https://openalex.org/W1538023239', 'https://openalex.org/W2136545725', 'https://openalex.org/W2936184970', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6798080464', 'https://openalex.org/W4399365598', 'https://openalex.org/W6855524384', 'https://openalex.org/W2250342921', 'https://openalex.org/W2101105183', 'https://openalex.org/W3024869864', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385569960', 'https://openalex.org/W4385574194', 'https://openalex.org/W3039695075', 'https://openalex.org/W4281621399', 'https://openalex.org/W4319862635', 'https://openalex.org/W4323651091', 'https://openalex.org/W4311000453', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200631896', 'https://openalex.org/W6852909395', 'https://openalex.org/W4404781204', 'https://openalex.org/W4309129001', 'https://openalex.org/W4385571229', 'https://openalex.org/W4280617721', 'https://openalex.org/W4303648927', 'https://openalex.org/W4389518873', 'https://openalex.org/W4297841625', 'https://openalex.org/W4390810311', 'https://openalex.org/W4402670286', 'https://openalex.org/W2952328691', 'https://openalex.org/W3197577761', 'https://openalex.org/W4283809028', 'https://openalex.org/W3035070478', 'https://openalex.org/W4377865270', 'https://openalex.org/W4389524081', 'https://openalex.org/W4403431383', 'https://openalex.org/W3012624518', 'https://openalex.org/W4312391725', 'https://openalex.org/W4394778654', 'https://openalex.org/W2032374598', 'https://openalex.org/W4285131748', 'https://openalex.org/W4307416501', 'https://openalex.org/W3196509775', 'https://openalex.org/W3127012371', 'https://openalex.org/W3197771105', 'https://openalex.org/W4323066695', 'https://openalex.org/W2798389157', 'https://openalex.org/W2962735107', 'https://openalex.org/W2973088264', 'https://openalex.org/W4308756394', 'https://openalex.org/W4385572318', 'https://openalex.org/W4286359908', 'https://openalex.org/W6861295083', 'https://openalex.org/W4311731008', 'https://openalex.org/W4385570550', 'https://openalex.org/W3007068036', 'https://openalex.org/W3213029956', 'https://openalex.org/W4384648564', 'https://openalex.org/W4226033575', 'https://openalex.org/W3025165719', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W4385573923', 'https://openalex.org/W2899716505', 'https://openalex.org/W2964161387', 'https://openalex.org/W3096490862', 'https://openalex.org/W6739901393', 'https://openalex.org/W4283834483', 'https://openalex.org/W6778823374', 'https://openalex.org/W3194000401', 'https://openalex.org/W4311994673', 'https://openalex.org/W4385569956', 'https://openalex.org/W4385574250']",2025-01-15
https://openalex.org/W4401607735,https://doi.org/10.1109/taslp.2024.3444470,Textless Unit-to-Unit Training for Many-to-Many Multilingual Speech-to-Speech Translation,,"['https://openalex.org/W6640090968', 'https://openalex.org/W6687566353', 'https://openalex.org/W2526425061', 'https://openalex.org/W2766219058', 'https://openalex.org/W2962780374', 'https://openalex.org/W3205193540', 'https://openalex.org/W6810168380', 'https://openalex.org/W4224319127', 'https://openalex.org/W4386071467', 'https://openalex.org/W2105482032', 'https://openalex.org/W6727690538', 'https://openalex.org/W2964045208', 'https://openalex.org/W6778883912', 'https://openalex.org/W3001434439', 'https://openalex.org/W2963609956', 'https://openalex.org/W6752888775', 'https://openalex.org/W6778083308', 'https://openalex.org/W6790220310', 'https://openalex.org/W6805710207', 'https://openalex.org/W6848735303', 'https://openalex.org/W6811129797', 'https://openalex.org/W6850625674', 'https://openalex.org/W2972473628', 'https://openalex.org/W3095012670', 'https://openalex.org/W6850334629', 'https://openalex.org/W4385764360', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W4200300291', 'https://openalex.org/W6790356757', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385823403', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385822683', 'https://openalex.org/W6780218876', 'https://openalex.org/W3213029956', 'https://openalex.org/W4381786045', 'https://openalex.org/W2033436836', 'https://openalex.org/W2106440210', 'https://openalex.org/W6776472420', 'https://openalex.org/W4287854499', 'https://openalex.org/W3209984917', 'https://openalex.org/W3205644108', 'https://openalex.org/W2551572271', 'https://openalex.org/W3213322812', 'https://openalex.org/W6849622896', 'https://openalex.org/W4372260478', 'https://openalex.org/W4392909068', 'https://openalex.org/W4307680525', 'https://openalex.org/W4385574033', 'https://openalex.org/W2242221029', 'https://openalex.org/W1992475611', 'https://openalex.org/W1995562189', 'https://openalex.org/W3017474798', 'https://openalex.org/W2781918655', 'https://openalex.org/W4309041555', 'https://openalex.org/W3213544594', 'https://openalex.org/W4386133927', 'https://openalex.org/W2972495969', 'https://openalex.org/W3017535695', 'https://openalex.org/W6841035593', 'https://openalex.org/W3142316150', 'https://openalex.org/W4385245566', 'https://openalex.org/W6838789019', 'https://openalex.org/W4385570550', 'https://openalex.org/W3180374548', 'https://openalex.org/W4221153524', 'https://openalex.org/W4226033575', 'https://openalex.org/W4296070387', 'https://openalex.org/W2963216553', 'https://openalex.org/W6763832098', 'https://openalex.org/W6734815144', 'https://openalex.org/W3015826515', 'https://openalex.org/W3090474612', 'https://openalex.org/W3081416955', 'https://openalex.org/W6798575157', 'https://openalex.org/W6846288075', 'https://openalex.org/W4307323391', 'https://openalex.org/W4375868953', 'https://openalex.org/W4385573012', 'https://openalex.org/W4372349107', 'https://openalex.org/W3119308075', 'https://openalex.org/W3034999214', 'https://openalex.org/W2896457183', 'https://openalex.org/W2933138175', 'https://openalex.org/W6783867762', 'https://openalex.org/W3197771105', 'https://openalex.org/W4385822729', 'https://openalex.org/W6796464841', 'https://openalex.org/W6917585676', 'https://openalex.org/W6750200984', 'https://openalex.org/W4385572318', 'https://openalex.org/W2972802841', 'https://openalex.org/W3015698636', 'https://openalex.org/W6810701745', 'https://openalex.org/W3196509775', 'https://openalex.org/W6810259195', 'https://openalex.org/W3197324626', 'https://openalex.org/W6754420807', 'https://openalex.org/W4322718191', 'https://openalex.org/W4403635980', 'https://openalex.org/W4323651091', 'https://openalex.org/W2891205112', 'https://openalex.org/W4226399820', 'https://openalex.org/W3181257032', 'https://openalex.org/W4221155340', 'https://openalex.org/W4313679638', 'https://openalex.org/W2525778437', 'https://openalex.org/W4394671563', 'https://openalex.org/W1922655562']",2024-01-01
https://openalex.org/W4392903524,https://doi.org/10.1109/icassp48485.2024.10446203,Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding,"Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2963609956', 'https://openalex.org/W2591927543', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W6777694618', 'https://openalex.org/W3161296985', 'https://openalex.org/W6778883912', 'https://openalex.org/W4381786045', 'https://openalex.org/W4390075359', 'https://openalex.org/W4296068981', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3215615641', 'https://openalex.org/W6779823529', 'https://openalex.org/W6847363464', 'https://openalex.org/W4372260402', 'https://openalex.org/W2752782242', 'https://openalex.org/W4312069022', 'https://openalex.org/W4292779060', 'https://openalex.org/W3036601975', 'https://openalex.org/W3094002217', 'https://openalex.org/W2946200149', 'https://openalex.org/W4382603054', 'https://openalex.org/W3036167779', 'https://openalex.org/W2519091744', 'https://openalex.org/W4307323391', 'https://openalex.org/W1522301498', 'https://openalex.org/W4402301063', 'https://openalex.org/W4366460484', 'https://openalex.org/W3026874504', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323651091']",2024-03-18
https://openalex.org/W4402301096,https://doi.org/10.1109/taslp.2024.3453606,U-Style: Cascading U-Nets With Multi-Level Speaker and Style Modeling for Zero-Shot Voice Cloning,,"['https://openalex.org/W2963609956', 'https://openalex.org/W6745697700', 'https://openalex.org/W4297841574', 'https://openalex.org/W3162794600', 'https://openalex.org/W4313148337', 'https://openalex.org/W3200756692', 'https://openalex.org/W6748588790', 'https://openalex.org/W3015826515', 'https://openalex.org/W3163654539', 'https://openalex.org/W4372259784', 'https://openalex.org/W6805710207', 'https://openalex.org/W6795807602', 'https://openalex.org/W6846143095', 'https://openalex.org/W4283731195', 'https://openalex.org/W3163475957', 'https://openalex.org/W6848735303', 'https://openalex.org/W3094785744', 'https://openalex.org/W3022876224', 'https://openalex.org/W4372260402', 'https://openalex.org/W4385823160', 'https://openalex.org/W3015645837', 'https://openalex.org/W6795261426', 'https://openalex.org/W4294311176', 'https://openalex.org/W6840257003', 'https://openalex.org/W2972659941', 'https://openalex.org/W3196584150', 'https://openalex.org/W2939913319', 'https://openalex.org/W6724804524', 'https://openalex.org/W6851724922', 'https://openalex.org/W6854199242', 'https://openalex.org/W6749555683', 'https://openalex.org/W2904459034', 'https://openalex.org/W3135644023', 'https://openalex.org/W6760861152', 'https://openalex.org/W4283832640', 'https://openalex.org/W4226421465', 'https://openalex.org/W4386536205', 'https://openalex.org/W6639480849', 'https://openalex.org/W4372260509', 'https://openalex.org/W6803547063', 'https://openalex.org/W6755300632', 'https://openalex.org/W4395957972', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163573274', 'https://openalex.org/W6763832098', 'https://openalex.org/W6783867762', 'https://openalex.org/W6850334629', 'https://openalex.org/W4210433094', 'https://openalex.org/W4384615685', 'https://openalex.org/W4289383906', 'https://openalex.org/W4313679638', 'https://openalex.org/W2932022923', 'https://openalex.org/W4323651091', 'https://openalex.org/W4366460484', 'https://openalex.org/W2502312327']",2024-01-01
https://openalex.org/W4392931282,https://doi.org/10.1109/icassp48485.2024.10448495,High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models,"Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2963609956', 'https://openalex.org/W6734815144', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W6777694618', 'https://openalex.org/W3161296985', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6860417749', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W4390075359', 'https://openalex.org/W4296068981', 'https://openalex.org/W6851724922', 'https://openalex.org/W6853888607', 'https://openalex.org/W6779823529', 'https://openalex.org/W4226033575', 'https://openalex.org/W4392903524', 'https://openalex.org/W4254489317', 'https://openalex.org/W4392909624', 'https://openalex.org/W4319779739', 'https://openalex.org/W2752782242', 'https://openalex.org/W6631190155', 'https://openalex.org/W4372260402', 'https://openalex.org/W4312069022', 'https://openalex.org/W4402301063', 'https://openalex.org/W4366460484', 'https://openalex.org/W4323651091', 'https://openalex.org/W4382603054', 'https://openalex.org/W3036167779', 'https://openalex.org/W3036601975', 'https://openalex.org/W4313679638', 'https://openalex.org/W1522301498', 'https://openalex.org/W2519091744', 'https://openalex.org/W2946200149', 'https://openalex.org/W3026874504']",2024-03-18
https://openalex.org/W4396815655,https://doi.org/10.1109/satml59370.2024.00035,Data Redaction from Conditional Generative Models,"Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.","['https://openalex.org/W4312933868', 'https://openalex.org/W6790978476', 'https://openalex.org/W6809885388', 'https://openalex.org/W6849119191', 'https://openalex.org/W6783182287', 'https://openalex.org/W6838843145', 'https://openalex.org/W6850625674', 'https://openalex.org/W6810940779', 'https://openalex.org/W6802142950', 'https://openalex.org/W6846007759', 'https://openalex.org/W6845894799', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6747945526', 'https://openalex.org/W2982756474', 'https://openalex.org/W6783036129', 'https://openalex.org/W3100355250', 'https://openalex.org/W3184144760', 'https://openalex.org/W4385894687', 'https://openalex.org/W4220993274', 'https://openalex.org/W4386066536', 'https://openalex.org/W4378977315', 'https://openalex.org/W2966792645', 'https://openalex.org/W3106976604', 'https://openalex.org/W1488996941', 'https://openalex.org/W6769833289', 'https://openalex.org/W6775408278', 'https://openalex.org/W6780547051', 'https://openalex.org/W6791907524', 'https://openalex.org/W6790532531', 'https://openalex.org/W6791392103', 'https://openalex.org/W3154155772', 'https://openalex.org/W3193974325', 'https://openalex.org/W4387171800', 'https://openalex.org/W2961363059', 'https://openalex.org/W6779823529', 'https://openalex.org/W4312872987', 'https://openalex.org/W6841366371', 'https://openalex.org/W4386057725', 'https://openalex.org/W4385271134', 'https://openalex.org/W6846547581', 'https://openalex.org/W6784843700', 'https://openalex.org/W6789034737', 'https://openalex.org/W3177399056', 'https://openalex.org/W4390871724', 'https://openalex.org/W4394593019', 'https://openalex.org/W6851248805', 'https://openalex.org/W6852699082', 'https://openalex.org/W4390874260', 'https://openalex.org/W6840815571', 'https://openalex.org/W6678815747', 'https://openalex.org/W6600609147', 'https://openalex.org/W2398118205', 'https://openalex.org/W2963966654', 'https://openalex.org/W2937579788', 'https://openalex.org/W6847363464', 'https://openalex.org/W6631190155', 'https://openalex.org/W6718379498', 'https://openalex.org/W2183341477', 'https://openalex.org/W3107235539', 'https://openalex.org/W6917585676', 'https://openalex.org/W2972359262', 'https://openalex.org/W2141998673', 'https://openalex.org/W1565144712', 'https://openalex.org/W2125324924', 'https://openalex.org/W6855790366', 'https://openalex.org/W3035574324', 'https://openalex.org/W1522301498', 'https://openalex.org/W4311431699', 'https://openalex.org/W3035556513', 'https://openalex.org/W4289785045', 'https://openalex.org/W4287627508', 'https://openalex.org/W2783792814', 'https://openalex.org/W4306820534', 'https://openalex.org/W3013335600', 'https://openalex.org/W4322718191', 'https://openalex.org/W3203737321', 'https://openalex.org/W4402916987', 'https://openalex.org/W3040639636', 'https://openalex.org/W4288099666', 'https://openalex.org/W2125389028', 'https://openalex.org/W3086249591', 'https://openalex.org/W3036167779', 'https://openalex.org/W4224035735', 'https://openalex.org/W4385901090', 'https://openalex.org/W4323651091', 'https://openalex.org/W2963373786', 'https://openalex.org/W4305001257', 'https://openalex.org/W4226125322', 'https://openalex.org/W3122887115', 'https://openalex.org/W4377111763', 'https://openalex.org/W4317951215', 'https://openalex.org/W3135378441', 'https://openalex.org/W4311000453', 'https://openalex.org/W4313679638', 'https://openalex.org/W4320013936']",2024-04-09
https://openalex.org/W4405379347,https://doi.org/10.1051/itmconf/20246901003,Development of an intelligent virtual assistant for digitalization of Moroccan agriculture,"This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.","['https://openalex.org/W2037901597', 'https://openalex.org/W2907049732', 'https://openalex.org/W3003261680', 'https://openalex.org/W3006588924', 'https://openalex.org/W2806962830', 'https://openalex.org/W2788269531', 'https://openalex.org/W4387048877', 'https://openalex.org/W4323651091', 'https://openalex.org/W4372260088', 'https://openalex.org/W4382603054', 'https://openalex.org/W4389500133']",2024-01-01
https://openalex.org/W4385571610,https://doi.org/10.18653/v1/2023.iwslt-1.33,The Kyoto Speech-to-Speech Translation System for IWSLT 2023,"This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.","['https://openalex.org/W2962784628', 'https://openalex.org/W3092028330', 'https://openalex.org/W4385245566', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385570170', 'https://openalex.org/W4385573012', 'https://openalex.org/W2964227577', 'https://openalex.org/W3037217258', 'https://openalex.org/W4323651091', 'https://openalex.org/W4385823111', 'https://openalex.org/W4281569374', 'https://openalex.org/W2998386507', 'https://openalex.org/W3198533616', 'https://openalex.org/W2595715041', 'https://openalex.org/W2945700568', 'https://openalex.org/W3033411150', 'https://openalex.org/W3102811925']",2023-01-01
https://openalex.org/W4401597614,https://doi.org/10.1109/icasspw62465.2024.10626580,Low-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations,,"['https://openalex.org/W4391021724', 'https://openalex.org/W6796464841', 'https://openalex.org/W6778823374', 'https://openalex.org/W4385895691', 'https://openalex.org/W4372266915', 'https://openalex.org/W4375869015', 'https://openalex.org/W6851724922', 'https://openalex.org/W6850334629', 'https://openalex.org/W4385823074', 'https://openalex.org/W4385571362', 'https://openalex.org/W3158762648', 'https://openalex.org/W3170751106', 'https://openalex.org/W4385823416', 'https://openalex.org/W4297841449', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W4388979610', 'https://openalex.org/W6724804524', 'https://openalex.org/W6781550413', 'https://openalex.org/W6790220310', 'https://openalex.org/W6738560588', 'https://openalex.org/W3097777922', 'https://openalex.org/W1901129140', 'https://openalex.org/W2785678896', 'https://openalex.org/W6755977528', 'https://openalex.org/W4372347654', 'https://openalex.org/W2936774411', 'https://openalex.org/W6838910450', 'https://openalex.org/W6853844661', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963091184', 'https://openalex.org/W6631190155', 'https://openalex.org/W4391305764', 'https://openalex.org/W2622563070', 'https://openalex.org/W2502312327', 'https://openalex.org/W2899663614', 'https://openalex.org/W4366460484', 'https://openalex.org/W4323651091', 'https://openalex.org/W4288257146']",2024-04-14
https://openalex.org/W4402594291,https://doi.org/10.1109/icce-taiwan62264.2024.10674564,Using Large Language Model and Speech Synthesis Technology to Achieve Realistic Interactive Experiences in Smart City Applications,,"['https://openalex.org/W4210621764', 'https://openalex.org/W4312120796', 'https://openalex.org/W6855007681', 'https://openalex.org/W6850334629', 'https://openalex.org/W6860028411', 'https://openalex.org/W4390602555', 'https://openalex.org/W4401042136', 'https://openalex.org/W4323651091']",2024-07-09
https://openalex.org/W4411569810,https://doi.org/10.1007/s44443-025-00082-7,Emotion transfer in audio using mel-cepstral representation and CycleGANs,"Abstract The field of audio synthesis is currently confronted with two major challenges: to more effectively eliminate non-emotional influences in emotional feature extraction work, and to improve the emotional expression when reference audio is scarce. Therefore, an innovative audio deep feature decoupling and emotion adaptive fusion model, which combines Mel Frequency Cepstral Coefficients (MFCCs) with Cycle-consistent Generative Adversarial Networks (CycleGANs), is proposed in this paper. We designed a Deep Feature Decoupled Encoder Group (DFDEG), which is based on Gated Linear Units (GLU), Self-Attention, and Average Pooling. Meanwhile, we designed a feature fusion method called Emotion Adaptive Instance Normalization (Emo-AdaIN), which is based on AdaIN. By integrating the DFDEG, Emo-AdaIN, and CycleGANs, an unsupervised bidirectional multi-emotion transfer method within the MFCCs is successfully achieved. This method performs well in terms of emotion decoupling and transfer on unseen datasets: for different speakers, the transfer result’s Lowest Emotional Similarity (LES) is 94.56%, and Average Confidence Level (ACL) is 0.51. This demonstrates the generalization performance across different speakers and the robustness across different emotion granularity.","['https://openalex.org/W4313178214', 'https://openalex.org/W4394666973', 'https://openalex.org/W4298426053', 'https://openalex.org/W1924770834', 'https://openalex.org/W2567070169', 'https://openalex.org/W3096039514', 'https://openalex.org/W2194775991', 'https://openalex.org/W2940630528', 'https://openalex.org/W2797650215', 'https://openalex.org/W2963073614', 'https://openalex.org/W2116984840', 'https://openalex.org/W3034838124', 'https://openalex.org/W3170283647', 'https://openalex.org/W2964121744', 'https://openalex.org/W4392781941', 'https://openalex.org/W3150994787', 'https://openalex.org/W2963263347', 'https://openalex.org/W4319452539', 'https://openalex.org/W2763421725', 'https://openalex.org/W3168527213', 'https://openalex.org/W2125389028', 'https://openalex.org/W2982600914', 'https://openalex.org/W4200282610', 'https://openalex.org/W3192307151', 'https://openalex.org/W4319587015', 'https://openalex.org/W3174758275', 'https://openalex.org/W3085139254', 'https://openalex.org/W2502312327', 'https://openalex.org/W4385245566', 'https://openalex.org/W3034058691', 'https://openalex.org/W2963444790', 'https://openalex.org/W2963840672', 'https://openalex.org/W4323651091', 'https://openalex.org/W2768083292', 'https://openalex.org/W3095255070', 'https://openalex.org/W2605287558']",2025-06-23
https://openalex.org/W4415313017,https://doi.org/10.1016/j.specom.2025.103317,Direct speech-to-speech neural machine translation: A survey,,"['https://openalex.org/W2973088264', 'https://openalex.org/W3213029956', 'https://openalex.org/W2163922914', 'https://openalex.org/W4327656064', 'https://openalex.org/W3209984917', 'https://openalex.org/W4402112198', 'https://openalex.org/W1980861001', 'https://openalex.org/W2120847449', 'https://openalex.org/W3209059054', 'https://openalex.org/W3011411500', 'https://openalex.org/W3017535695', 'https://openalex.org/W4402538165', 'https://openalex.org/W3119378114', 'https://openalex.org/W4390604258', 'https://openalex.org/W2103091632', 'https://openalex.org/W3096709315', 'https://openalex.org/W4281492411', 'https://openalex.org/W4402669689', 'https://openalex.org/W2136545725', 'https://openalex.org/W2165698076', 'https://openalex.org/W2758950307', 'https://openalex.org/W4388017359', 'https://openalex.org/W3197411683', 'https://openalex.org/W2129120544', 'https://openalex.org/W2141998673', 'https://openalex.org/W3011462663', 'https://openalex.org/W2159591770', 'https://openalex.org/W4384211302', 'https://openalex.org/W4295857769', 'https://openalex.org/W4287322212', 'https://openalex.org/W3215615641', 'https://openalex.org/W2969521066', 'https://openalex.org/W4297808394', 'https://openalex.org/W2623399293', 'https://openalex.org/W4399317047', 'https://openalex.org/W4376548770', 'https://openalex.org/W4401306756', 'https://openalex.org/W4385245566', 'https://openalex.org/W4408354546', 'https://openalex.org/W4402533227', 'https://openalex.org/W3183859557', 'https://openalex.org/W4307323391', 'https://openalex.org/W4401607735', 'https://openalex.org/W2896457183', 'https://openalex.org/W4234842379', 'https://openalex.org/W3037469336', 'https://openalex.org/W4365799947', 'https://openalex.org/W4205807230', 'https://openalex.org/W4387596303', 'https://openalex.org/W3180374548', 'https://openalex.org/W3101118213', 'https://openalex.org/W3105214104', 'https://openalex.org/W4376632801', 'https://openalex.org/W2606555609', 'https://openalex.org/W2914584698', 'https://openalex.org/W3213018012', 'https://openalex.org/W2936695845', 'https://openalex.org/W4287854885', 'https://openalex.org/W2620949368', 'https://openalex.org/W4389500133', 'https://openalex.org/W1593271688', 'https://openalex.org/W4393027241', 'https://openalex.org/W1552314771', 'https://openalex.org/W2610930722', 'https://openalex.org/W2599674900', 'https://openalex.org/W2750779823', 'https://openalex.org/W3091928890', 'https://openalex.org/W4293171495', 'https://openalex.org/W2607303097', 'https://openalex.org/W4406163711', 'https://openalex.org/W4283026156', 'https://openalex.org/W4323651091', 'https://openalex.org/W2103934944', 'https://openalex.org/W4286903243', 'https://openalex.org/W4385569956', 'https://openalex.org/W3175871055', 'https://openalex.org/W2798389157', 'https://openalex.org/W128638292', 'https://openalex.org/W2915722758', 'https://openalex.org/W3036601975', 'https://openalex.org/W4384648564', 'https://openalex.org/W4389600306', 'https://openalex.org/W2597601064', 'https://openalex.org/W4226278401', 'https://openalex.org/W4255113413', 'https://openalex.org/W4221155340', 'https://openalex.org/W4409647763', 'https://openalex.org/W4238846128', 'https://openalex.org/W2963799213', 'https://openalex.org/W4311430515', 'https://openalex.org/W2747329762', 'https://openalex.org/W1538023239', 'https://openalex.org/W2939131199', 'https://openalex.org/W4287072252', 'https://openalex.org/W4285267877', 'https://openalex.org/W2915824643', 'https://openalex.org/W2113106066', 'https://openalex.org/W2501982834', 'https://openalex.org/W3141239769', 'https://openalex.org/W4412888943', 'https://openalex.org/W2097203679', 'https://openalex.org/W4288345582', 'https://openalex.org/W4404783772', 'https://openalex.org/W1631063262', 'https://openalex.org/W2419292002', 'https://openalex.org/W2625561335', 'https://openalex.org/W4402669744', 'https://openalex.org/W4378105483', 'https://openalex.org/W4399151597', 'https://openalex.org/W1583837637', 'https://openalex.org/W4379539302', 'https://openalex.org/W1821462560', 'https://openalex.org/W4292779060', 'https://openalex.org/W4381827575', 'https://openalex.org/W97072897', 'https://openalex.org/W4285306484']",2025-10-16
https://openalex.org/W4415321223,https://doi.org/10.1080/1448837x.2025.2568790,Multimodal fusion for enhancing English text translation: integrating multimedia information,,"['https://openalex.org/W4399376241', 'https://openalex.org/W4413053564', 'https://openalex.org/W3215763807', 'https://openalex.org/W4400771096', 'https://openalex.org/W4213388865', 'https://openalex.org/W3136823269', 'https://openalex.org/W3180506122', 'https://openalex.org/W4393407199', 'https://openalex.org/W4223941643', 'https://openalex.org/W4401377247', 'https://openalex.org/W4392812555', 'https://openalex.org/W4385780033', 'https://openalex.org/W4410582379', 'https://openalex.org/W3133473264', 'https://openalex.org/W4400086954', 'https://openalex.org/W2994772196', 'https://openalex.org/W3197400575', 'https://openalex.org/W3181440838', 'https://openalex.org/W4400300641', 'https://openalex.org/W4367041061', 'https://openalex.org/W3156804398', 'https://openalex.org/W4306655832', 'https://openalex.org/W3097393636', 'https://openalex.org/W4404503596', 'https://openalex.org/W4323651091', 'https://openalex.org/W4400300940']",2025-10-18
https://openalex.org/W4398152753,https://doi.org/10.1109/taslp.2024.3402088,InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt,"Expressive text-to-speech (TTS) aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">e.g.</i> , ""Sigh tone in full of sad mood with some helpless feeling"". Considering that there is no existing TTS corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named InstructTTS, which is novel in the sense of the following aspects: (1) We fully take advantage of self-supervised learning and cross-modal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt. Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of InstructTTS. Experimental results show that InstructTTS can synthesize high-fidelity and natural speech with style prompts controlling the speaking style. Synthesized samples are available online.","['https://openalex.org/W2963609956', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972699445', 'https://openalex.org/W2969658393', 'https://openalex.org/W6750489868', 'https://openalex.org/W6749555683', 'https://openalex.org/W6752888775', 'https://openalex.org/W4385822534', 'https://openalex.org/W6848735303', 'https://openalex.org/W6795807602', 'https://openalex.org/W6791353385', 'https://openalex.org/W4210913346', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W3180355996', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6802805937', 'https://openalex.org/W4367359628', 'https://openalex.org/W6852581948', 'https://openalex.org/W3197294703', 'https://openalex.org/W6777694618', 'https://openalex.org/W3196584150', 'https://openalex.org/W4313316128', 'https://openalex.org/W6846143095', 'https://openalex.org/W3197704090', 'https://openalex.org/W3198123658', 'https://openalex.org/W4375869257', 'https://openalex.org/W6795288823', 'https://openalex.org/W4312388283', 'https://openalex.org/W6810940779', 'https://openalex.org/W6800989748', 'https://openalex.org/W6838815585', 'https://openalex.org/W6846176957', 'https://openalex.org/W6783182287', 'https://openalex.org/W3198213150', 'https://openalex.org/W6849416043', 'https://openalex.org/W4312933868', 'https://openalex.org/W6679045638', 'https://openalex.org/W6796163713', 'https://openalex.org/W6798447524', 'https://openalex.org/W6766673545', 'https://openalex.org/W3156636935', 'https://openalex.org/W6844194202', 'https://openalex.org/W4312096566', 'https://openalex.org/W2157364932', 'https://openalex.org/W6752051073', 'https://openalex.org/W6779459370', 'https://openalex.org/W6796730497', 'https://openalex.org/W6783867762', 'https://openalex.org/W4226132755', 'https://openalex.org/W2963073614', 'https://openalex.org/W4385245566', 'https://openalex.org/W6840815571', 'https://openalex.org/W6838510122', 'https://openalex.org/W4381786045', 'https://openalex.org/W3198533616', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631190155', 'https://openalex.org/W4382202703', 'https://openalex.org/W6757817989', 'https://openalex.org/W2107860279', 'https://openalex.org/W2133665775', 'https://openalex.org/W2067295501', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W2058819080', 'https://openalex.org/W6810926057', 'https://openalex.org/W2007962718', 'https://openalex.org/W4388323056', 'https://openalex.org/W6780218876', 'https://openalex.org/W4297808394', 'https://openalex.org/W1522301498', 'https://openalex.org/W4313679638', 'https://openalex.org/W4281771798', 'https://openalex.org/W3174758275', 'https://openalex.org/W2908510526', 'https://openalex.org/W4372279529']",2024-01-01
https://openalex.org/W4399875170,https://doi.org/10.1109/taslp.2024.3417347,APCodec: A Neural Audio Codec With Parallel Amplitude and Phase Spectrum Encoding and Decoding,,"['https://openalex.org/W6633114069', 'https://openalex.org/W2130158387', 'https://openalex.org/W2159644785', 'https://openalex.org/W2168098717', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W6855885476', 'https://openalex.org/W6856524194', 'https://openalex.org/W4392903006', 'https://openalex.org/W2020883660', 'https://openalex.org/W6630442970', 'https://openalex.org/W1481955708', 'https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W2972519044', 'https://openalex.org/W3188073270', 'https://openalex.org/W4385822716', 'https://openalex.org/W4372260101', 'https://openalex.org/W4375869436', 'https://openalex.org/W1989395836', 'https://openalex.org/W2963182577', 'https://openalex.org/W2752796333', 'https://openalex.org/W2935711438', 'https://openalex.org/W2972354707', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W2131738223', 'https://openalex.org/W6783867762', 'https://openalex.org/W6852581948', 'https://openalex.org/W4385823076', 'https://openalex.org/W4375869380', 'https://openalex.org/W4385823460', 'https://openalex.org/W4372270198', 'https://openalex.org/W6853515095', 'https://openalex.org/W6780226713', 'https://openalex.org/W4386076493', 'https://openalex.org/W6755977528', 'https://openalex.org/W4372260247', 'https://openalex.org/W4377000449', 'https://openalex.org/W3197273793', 'https://openalex.org/W6767111847', 'https://openalex.org/W6772349387', 'https://openalex.org/W6757817989', 'https://openalex.org/W2067295501', 'https://openalex.org/W3037038648', 'https://openalex.org/W6859898589', 'https://openalex.org/W6771467084', 'https://openalex.org/W4296068763', 'https://openalex.org/W4205689591', 'https://openalex.org/W1553834069', 'https://openalex.org/W2998572311', 'https://openalex.org/W2964073500', 'https://openalex.org/W4405521053', 'https://openalex.org/W2899663614', 'https://openalex.org/W4313679638', 'https://openalex.org/W4372279529', 'https://openalex.org/W4402672068', 'https://openalex.org/W4386384714']",2024-01-01
https://openalex.org/W4405182531,https://doi.org/10.1145/3658644.3670285,SafeEar: Content Privacy-Preserving Audio Deepfake Detection,,"['https://openalex.org/W2270140468', 'https://openalex.org/W4312095886', 'https://openalex.org/W3024920698', 'https://openalex.org/W2005708641', 'https://openalex.org/W2120847449', 'https://openalex.org/W3209059054', 'https://openalex.org/W4390938875', 'https://openalex.org/W6604525367', 'https://openalex.org/W2903739847', 'https://openalex.org/W4388017405', 'https://openalex.org/W4388034436', 'https://openalex.org/W6600669965', 'https://openalex.org/W2152084093', 'https://openalex.org/W4281492411', 'https://openalex.org/W2471520273', 'https://openalex.org/W2047865878', 'https://openalex.org/W3128666957', 'https://openalex.org/W6602512906', 'https://openalex.org/W6812210286', 'https://openalex.org/W3098557217', 'https://openalex.org/W2738406145', 'https://openalex.org/W2141998673', 'https://openalex.org/W3163596559', 'https://openalex.org/W6607786901', 'https://openalex.org/W2733416080', 'https://openalex.org/W6639363673', 'https://openalex.org/W6600741150', 'https://openalex.org/W3198329097', 'https://openalex.org/W3026777299', 'https://openalex.org/W4376163495', 'https://openalex.org/W4388867345', 'https://openalex.org/W3198529186', 'https://openalex.org/W6817109343', 'https://openalex.org/W2984998244', 'https://openalex.org/W6600274734', 'https://openalex.org/W4375948231', 'https://openalex.org/W6602661299', 'https://openalex.org/W4388856757', 'https://openalex.org/W4385822614', 'https://openalex.org/W3138076532', 'https://openalex.org/W3127781933', 'https://openalex.org/W4387969008', 'https://openalex.org/W2774848319', 'https://openalex.org/W2747329762', 'https://openalex.org/W4221163854', 'https://openalex.org/W3131786367', 'https://openalex.org/W4372279529', 'https://openalex.org/W2560674852', 'https://openalex.org/W3174758275', 'https://openalex.org/W4233299962', 'https://openalex.org/W4313679638', 'https://openalex.org/W4315705950', 'https://openalex.org/W4307323391', 'https://openalex.org/W4205788663', 'https://openalex.org/W3158663310', 'https://openalex.org/W2592562038']",2024-12-02
https://openalex.org/W4392903006,https://doi.org/10.1109/icassp48485.2024.10448454,Fewer-Token Neural Speech Codec with Time-Invariant Codes,"Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.","['https://openalex.org/W6848735303', 'https://openalex.org/W4390075359', 'https://openalex.org/W6853188576', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W4385823076', 'https://openalex.org/W4385822716', 'https://openalex.org/W2775336875', 'https://openalex.org/W2519091744', 'https://openalex.org/W3163662330', 'https://openalex.org/W6640212811', 'https://openalex.org/W2935711438', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W6852581948', 'https://openalex.org/W2972359262', 'https://openalex.org/W1901129140', 'https://openalex.org/W6783867762', 'https://openalex.org/W6770838157', 'https://openalex.org/W6785090365', 'https://openalex.org/W3037038648', 'https://openalex.org/W2067295501', 'https://openalex.org/W3209059054', 'https://openalex.org/W4372279529', 'https://openalex.org/W3092028330', 'https://openalex.org/W3094002217', 'https://openalex.org/W4313679638', 'https://openalex.org/W2963799213', 'https://openalex.org/W4377010126', 'https://openalex.org/W2990124956', 'https://openalex.org/W4380551955']",2024-03-18
https://openalex.org/W4392902628,https://doi.org/10.1109/icassp48485.2024.10447532,LightCodec: A High Fidelity Neural Audio Codec with Low Computation Complexity,"The audio codec is one of the core modules in audio communication for real-time transmission. With the development of neural networks, end-to-end audio codecs have emerged and demonstrated effects beyond conventional codecs. However, current neural network-based codecs have the weakness of high computational complexity, and the performance of these methods decreases rapidly after decreasing the complexity, which is not conducive to deployment under low computational resources. In this paper, a low-complexity audio codec is proposed. To realize the low complexity of the model with high quality, a structure based on frequency band division is designed, which is implemented using a within bandacross band interaction (WBABI) module to learn the features across and within the subband. Further, we propose a new quantization-compensation module, which reduces the quantization error by 90%. The experimental results show that for audio with a sample rate of 24kHz, the model shows excellent performance at 3~6kbps compared to other codecs, and the complexity is only 0.8 Giga Multiply-Add Operations per Second(GMACs).","['https://openalex.org/W3215615641', 'https://openalex.org/W3188073270', 'https://openalex.org/W3163662330', 'https://openalex.org/W2972519044', 'https://openalex.org/W6783867762', 'https://openalex.org/W6767111847', 'https://openalex.org/W4297841527', 'https://openalex.org/W3015338123', 'https://openalex.org/W4297841531', 'https://openalex.org/W1901129140', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W4307323391', 'https://openalex.org/W4284972581', 'https://openalex.org/W4225287045', 'https://openalex.org/W6852581948', 'https://openalex.org/W4375869380', 'https://openalex.org/W3144035034', 'https://openalex.org/W2064675550', 'https://openalex.org/W6785090365', 'https://openalex.org/W2998572311', 'https://openalex.org/W2972359262', 'https://openalex.org/W3037038648', 'https://openalex.org/W2970006822', 'https://openalex.org/W4372279529', 'https://openalex.org/W2963799213', 'https://openalex.org/W3094002217', 'https://openalex.org/W4212774754', 'https://openalex.org/W3092028330']",2024-03-18
https://openalex.org/W4406461725,https://doi.org/10.1109/slt61566.2024.10832214,MDCTCodec: A Lightweight MDCT-Based Neural Audio Codec Towards High Sampling Rate and Low Bitrate Scenarios,,"['https://openalex.org/W2168098717', 'https://openalex.org/W6633114069', 'https://openalex.org/W6848735303', 'https://openalex.org/W6855885476', 'https://openalex.org/W4381786045', 'https://openalex.org/W4392931276', 'https://openalex.org/W6796464841', 'https://openalex.org/W6630442970', 'https://openalex.org/W1481955708', 'https://openalex.org/W2020883660', 'https://openalex.org/W2151626637', 'https://openalex.org/W6731445010', 'https://openalex.org/W3163662330', 'https://openalex.org/W3215615641', 'https://openalex.org/W2131738223', 'https://openalex.org/W4307323391', 'https://openalex.org/W6852581948', 'https://openalex.org/W6853515095', 'https://openalex.org/W4372270198', 'https://openalex.org/W4399875170', 'https://openalex.org/W4372260101', 'https://openalex.org/W3037038648', 'https://openalex.org/W6727697161', 'https://openalex.org/W3095095816', 'https://openalex.org/W4372260247', 'https://openalex.org/W6783867762', 'https://openalex.org/W6853165267', 'https://openalex.org/W4386076493', 'https://openalex.org/W6755977528', 'https://openalex.org/W6767111847', 'https://openalex.org/W6757817989', 'https://openalex.org/W3197273793', 'https://openalex.org/W6771467084', 'https://openalex.org/W4296068763', 'https://openalex.org/W4205689591', 'https://openalex.org/W4379259581', 'https://openalex.org/W2899663614', 'https://openalex.org/W2964073500', 'https://openalex.org/W4372279529', 'https://openalex.org/W4380551955', 'https://openalex.org/W2566983320', 'https://openalex.org/W1553834069', 'https://openalex.org/W4386384714', 'https://openalex.org/W2527729766', 'https://openalex.org/W4313679638', 'https://openalex.org/W2970006822']",2024-12-02
https://openalex.org/W4393241602,https://doi.org/10.1109/lsp.2024.3381910,Efficient Parallel Audio Generation Using Group Masked Language Modeling,,"['https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6778883912', 'https://openalex.org/W6850625674', 'https://openalex.org/W6783867762', 'https://openalex.org/W6783182287', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W4390075359', 'https://openalex.org/W6853188576', 'https://openalex.org/W6854494257', 'https://openalex.org/W6853096648', 'https://openalex.org/W6851724922', 'https://openalex.org/W4392909680', 'https://openalex.org/W6779823529', 'https://openalex.org/W6852581948', 'https://openalex.org/W6780218876', 'https://openalex.org/W2752796333', 'https://openalex.org/W4226033575', 'https://openalex.org/W3097777922', 'https://openalex.org/W4313021454', 'https://openalex.org/W6795952400', 'https://openalex.org/W4221166168', 'https://openalex.org/W6755207826', 'https://openalex.org/W4385245566', 'https://openalex.org/W2972359262', 'https://openalex.org/W3213029956', 'https://openalex.org/W6796464841', 'https://openalex.org/W3024869864', 'https://openalex.org/W6847363464', 'https://openalex.org/W3209984917', 'https://openalex.org/W4380136719', 'https://openalex.org/W4322718191', 'https://openalex.org/W4313679638', 'https://openalex.org/W2519091744', 'https://openalex.org/W4372279529', 'https://openalex.org/W4377010126']",2024-01-01
https://openalex.org/W4406495593,https://doi.org/10.1109/slt61566.2024.10832247,SoCodec: A Semantic-Ordered Multi-Stream Speech Codec For Efficient Language Model Based Text-to-Speech Synthesis,,"['https://openalex.org/W6854866820', 'https://openalex.org/W6848735303', 'https://openalex.org/W6852421699', 'https://openalex.org/W4391833199', 'https://openalex.org/W4307323391', 'https://openalex.org/W6852581948', 'https://openalex.org/W3209059054', 'https://openalex.org/W6853096648', 'https://openalex.org/W4392903006', 'https://openalex.org/W4402111239', 'https://openalex.org/W6862144568', 'https://openalex.org/W6855885476', 'https://openalex.org/W4404740148', 'https://openalex.org/W3198217962', 'https://openalex.org/W6683307983', 'https://openalex.org/W6790521310', 'https://openalex.org/W2124509324', 'https://openalex.org/W4406496863', 'https://openalex.org/W3143367551', 'https://openalex.org/W6854199242', 'https://openalex.org/W6838843145', 'https://openalex.org/W4402111455', 'https://openalex.org/W3203407300', 'https://openalex.org/W4313679638', 'https://openalex.org/W4376632512', 'https://openalex.org/W4372279529', 'https://openalex.org/W4386384714', 'https://openalex.org/W4384918448', 'https://openalex.org/W2950305991', 'https://openalex.org/W4384615685']",2024-12-02
https://openalex.org/W4404449734,https://doi.org/10.1016/j.procs.2024.10.307,Audio Compression Using Qubits and Quantum Neural Network,,"['https://openalex.org/W3026677205', 'https://openalex.org/W4393558743', 'https://openalex.org/W3007287940', 'https://openalex.org/W4245735304', 'https://openalex.org/W6800767477', 'https://openalex.org/W4307323391', 'https://openalex.org/W3009587941', 'https://openalex.org/W3120871371', 'https://openalex.org/W6771294960', 'https://openalex.org/W6763039619', 'https://openalex.org/W3216950988', 'https://openalex.org/W6856604417', 'https://openalex.org/W2982169647', 'https://openalex.org/W3092363263', 'https://openalex.org/W6852581948', 'https://openalex.org/W6870263063', 'https://openalex.org/W3004965358', 'https://openalex.org/W4396877837', 'https://openalex.org/W4210645695', 'https://openalex.org/W3123455941', 'https://openalex.org/W6798098866', 'https://openalex.org/W4400111385', 'https://openalex.org/W3091498161', 'https://openalex.org/W6795646078', 'https://openalex.org/W6912699698', 'https://openalex.org/W6748683284', 'https://openalex.org/W3010243192', 'https://openalex.org/W4300979811', 'https://openalex.org/W3103454714', 'https://openalex.org/W4229647796', 'https://openalex.org/W4402112643', 'https://openalex.org/W4288346708', 'https://openalex.org/W3197640319', 'https://openalex.org/W4300988299', 'https://openalex.org/W3099734242', 'https://openalex.org/W4386767079', 'https://openalex.org/W4372279529', 'https://openalex.org/W3198785558', 'https://openalex.org/W3215615641']",2024-01-01
https://openalex.org/W4392904056,https://doi.org/10.1109/icassp48485.2024.10445853,SPATIALCODEC: Neural Spatial Speech Coding,"In this work, we address the challenge of encoding speech captured by a microphone array using deep learning techniques with the aim of preserving and accurately reconstructing crucial spatial cues embedded in multi-channel recordings. We propose a neural spatial audio coding framework that achieves a high compression ratio, leveraging single-channel neural sub-band codec and SpatialCodec. Our approach encompasses two phases: (i) a neural sub-band codec is designed to encode the reference channel with low bit rates, and (ii), a SpatialCodec captures relative spatial information for accurate multi-channel reconstruction at the decoder end. In addition, we also propose novel evaluation metrics to assess the spatial cue preservation: (i) spatial similarity, which calculates cosine similarity on a spatially intuitive beamspace, and (ii), beamformed audio quality. Our system shows superior spatial performance compared with high bitrate baselines and black-box neural architecture. Demos are available at https://xzwy.github.io/SpatialCodecDemo. Codes and models are available at https://github.com/XZWY/SpatialCodec.","['https://openalex.org/W2151626637', 'https://openalex.org/W6696768431', 'https://openalex.org/W6630442970', 'https://openalex.org/W1481955708', 'https://openalex.org/W2117497587', 'https://openalex.org/W2486063739', 'https://openalex.org/W1634005169', 'https://openalex.org/W3215615641', 'https://openalex.org/W4377231659', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W6852581948', 'https://openalex.org/W2752796333', 'https://openalex.org/W6767111847', 'https://openalex.org/W6783867762', 'https://openalex.org/W3140429000', 'https://openalex.org/W6853096648', 'https://openalex.org/W6848735303', 'https://openalex.org/W316027304', 'https://openalex.org/W2141532435', 'https://openalex.org/W2089473323', 'https://openalex.org/W2091248202', 'https://openalex.org/W3127838239', 'https://openalex.org/W2974407961', 'https://openalex.org/W6754473786', 'https://openalex.org/W6810523761', 'https://openalex.org/W2807002113', 'https://openalex.org/W2113638573', 'https://openalex.org/W2763188033', 'https://openalex.org/W4281570978', 'https://openalex.org/W4385989146', 'https://openalex.org/W2963799213', 'https://openalex.org/W2889048668', 'https://openalex.org/W4380136719', 'https://openalex.org/W4225302959', 'https://openalex.org/W4297752165', 'https://openalex.org/W3101330598', 'https://openalex.org/W4372279529', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313679638', 'https://openalex.org/W2970006822', 'https://openalex.org/W2292235217']",2024-03-18
https://openalex.org/W4403182589,https://doi.org/10.1109/is262782.2024.10704165,Large-Scale Room Impulse Response Dataset Compression with Neural Audio Codecs,,"['https://openalex.org/W3029684239', 'https://openalex.org/W2918296821', 'https://openalex.org/W6630634255', 'https://openalex.org/W4302011018', 'https://openalex.org/W2071608040', 'https://openalex.org/W4389297803', 'https://openalex.org/W6630442970', 'https://openalex.org/W1481955708', 'https://openalex.org/W2963182577', 'https://openalex.org/W3215615641', 'https://openalex.org/W4225311785', 'https://openalex.org/W4225287045', 'https://openalex.org/W4402672068', 'https://openalex.org/W6853515095', 'https://openalex.org/W4392903887', 'https://openalex.org/W4399875170', 'https://openalex.org/W4392903006', 'https://openalex.org/W4386764631', 'https://openalex.org/W4385973788', 'https://openalex.org/W2555915854', 'https://openalex.org/W4401597862', 'https://openalex.org/W4327642293', 'https://openalex.org/W4391028422', 'https://openalex.org/W2002507711', 'https://openalex.org/W1500964790', 'https://openalex.org/W7030346516', 'https://openalex.org/W2168090960', 'https://openalex.org/W4238160257', 'https://openalex.org/W2119047110', 'https://openalex.org/W4294891437', 'https://openalex.org/W4320015822', 'https://openalex.org/W4251733995', 'https://openalex.org/W6772349387', 'https://openalex.org/W6770786944', 'https://openalex.org/W2791686384', 'https://openalex.org/W6798326965', 'https://openalex.org/W2609300858', 'https://openalex.org/W4403233744', 'https://openalex.org/W2988799144', 'https://openalex.org/W1510060937', 'https://openalex.org/W4372279529', 'https://openalex.org/W2964073500', 'https://openalex.org/W2998572311', 'https://openalex.org/W4386384714', 'https://openalex.org/W3182954816', 'https://openalex.org/W4307323391', 'https://openalex.org/W4380551955', 'https://openalex.org/W4404740148']",2024-09-30
https://openalex.org/W4405709311,https://doi.org/10.1109/iscslp63861.2024.10800201,A Hybrid DFSMN and Mamba Architecture for Low Bitrate Neural Speech Coding,,"['https://openalex.org/W2151626637', 'https://openalex.org/W2396073756', 'https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W3163662330', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W4225287045', 'https://openalex.org/W4375869380', 'https://openalex.org/W4372279529', 'https://openalex.org/W2147800946', 'https://openalex.org/W2064675550', 'https://openalex.org/W4385245566', 'https://openalex.org/W2289400088', 'https://openalex.org/W3209374680', 'https://openalex.org/W4389326242', 'https://openalex.org/W4391013663', 'https://openalex.org/W2208299922', 'https://openalex.org/W2963308316', 'https://openalex.org/W4393943345', 'https://openalex.org/W3092028330', 'https://openalex.org/W3120336970', 'https://openalex.org/W3015338123', 'https://openalex.org/W3037038648']",2024-11-07
https://openalex.org/W4407833650,https://doi.org/10.1063/5.0261161,High-Fidelity audio training kit with LM1875 for practicum learning media in the electronics engineering expertise vocational high school,,"['https://openalex.org/W4239295116', 'https://openalex.org/W4372279529', 'https://openalex.org/W2011721566', 'https://openalex.org/W3154036554']",2025-01-01
https://openalex.org/W4412825826,https://doi.org/10.1145/3711896.3736914,Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach,,"['https://openalex.org/W2089135543', 'https://openalex.org/W2107775979', 'https://openalex.org/W1998269045', 'https://openalex.org/W4390874241', 'https://openalex.org/W4313009682', 'https://openalex.org/W2115579991', 'https://openalex.org/W3034426027', 'https://openalex.org/W4385346076', 'https://openalex.org/W4386083005', 'https://openalex.org/W2101032778', 'https://openalex.org/W4312974539', 'https://openalex.org/W4379382445', 'https://openalex.org/W4308614496', 'https://openalex.org/W3138516171', 'https://openalex.org/W4312443924', 'https://openalex.org/W3003709258', 'https://openalex.org/W4249279051', 'https://openalex.org/W4309953041', 'https://openalex.org/W4386076121', 'https://openalex.org/W4381573216', 'https://openalex.org/W6811014117', 'https://openalex.org/W6600339963', 'https://openalex.org/W3138340468', 'https://openalex.org/W2967033144', 'https://openalex.org/W2133665775', 'https://openalex.org/W4361807013', 'https://openalex.org/W4372279529', 'https://openalex.org/W4387560383', 'https://openalex.org/W4312820606', 'https://openalex.org/W3215615641', 'https://openalex.org/W2528639018', 'https://openalex.org/W2962785568', 'https://openalex.org/W4390872799', 'https://openalex.org/W3080366021', 'https://openalex.org/W4285661751', 'https://openalex.org/W2747329762']",2025-08-01
https://openalex.org/W4414170608,https://doi.org/10.1109/icccworkshops67136.2025.11148169,A Multi-Scale Low-Bitrate Speech Codec for LEO Communication With Deep Feature Preservation Losses,,"['https://openalex.org/W4405461810', 'https://openalex.org/W4288062148', 'https://openalex.org/W1481955708', 'https://openalex.org/W3163827866', 'https://openalex.org/W2895598217', 'https://openalex.org/W2593414223', 'https://openalex.org/W3024869864', 'https://openalex.org/W3209059054', 'https://openalex.org/W2972359262', 'https://openalex.org/W2067295501', 'https://openalex.org/W4403345375', 'https://openalex.org/W2998572311', 'https://openalex.org/W3036601975', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372279529', 'https://openalex.org/W4205788663', 'https://openalex.org/W4380551955', 'https://openalex.org/W2963799213']",2025-08-10
https://openalex.org/W4414568523,https://doi.org/10.1016/j.csl.2025.101892,A speech prediction model based on codec modeling and transformer decoding,,"['https://openalex.org/W2296153915', 'https://openalex.org/W1864555471', 'https://openalex.org/W4296068407', 'https://openalex.org/W3148040514', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972943112', 'https://openalex.org/W4296069296', 'https://openalex.org/W2145103350', 'https://openalex.org/W2980292555', 'https://openalex.org/W4389474152', 'https://openalex.org/W2112464782', 'https://openalex.org/W4225287045', 'https://openalex.org/W4404102497', 'https://openalex.org/W2981283774', 'https://openalex.org/W82776184', 'https://openalex.org/W4297841545', 'https://openalex.org/W1550397807', 'https://openalex.org/W4296068992', 'https://openalex.org/W2799789537', 'https://openalex.org/W4393294847', 'https://openalex.org/W2898371500', 'https://openalex.org/W2321297794', 'https://openalex.org/W1964600646', 'https://openalex.org/W4313680291', 'https://openalex.org/W4372341905', 'https://openalex.org/W4385823134', 'https://openalex.org/W3161904430', 'https://openalex.org/W4205169642', 'https://openalex.org/W2963276790', 'https://openalex.org/W2556910157', 'https://openalex.org/W2963300588', 'https://openalex.org/W3015213852', 'https://openalex.org/W4225302959', 'https://openalex.org/W1552314771', 'https://openalex.org/W2132036212', 'https://openalex.org/W32436167', 'https://openalex.org/W2802304149', 'https://openalex.org/W3169418678', 'https://openalex.org/W4391020683', 'https://openalex.org/W2889442120', 'https://openalex.org/W4297841420', 'https://openalex.org/W3207100935', 'https://openalex.org/W4400111385', 'https://openalex.org/W4296069126', 'https://openalex.org/W4392902611', 'https://openalex.org/W4386764852', 'https://openalex.org/W1955313418', 'https://openalex.org/W3215615641', 'https://openalex.org/W3141042598', 'https://openalex.org/W2981905048', 'https://openalex.org/W3105183525', 'https://openalex.org/W4318351475', 'https://openalex.org/W2519091744', 'https://openalex.org/W4385245566', 'https://openalex.org/W4389317961', 'https://openalex.org/W3092028330', 'https://openalex.org/W2977305403', 'https://openalex.org/W1522301498', 'https://openalex.org/W4386875232', 'https://openalex.org/W3012382391', 'https://openalex.org/W1561829009', 'https://openalex.org/W4392019859', 'https://openalex.org/W4307323391', 'https://openalex.org/W4387682234', 'https://openalex.org/W4372279529']",2025-09-27
https://openalex.org/W4392902957,https://doi.org/10.1109/icassp48485.2024.10446663,Adapting Frechet Audio Distance for Generative Music Evaluation,"The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.","['https://openalex.org/W4367359628', 'https://openalex.org/W2526050071', 'https://openalex.org/W3034664137', 'https://openalex.org/W6766320909', 'https://openalex.org/W6855434424', 'https://openalex.org/W4221165315', 'https://openalex.org/W2939574508', 'https://openalex.org/W3006926732', 'https://openalex.org/W4392903801', 'https://openalex.org/W6846849257', 'https://openalex.org/W6853393314', 'https://openalex.org/W6790489232', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W3094550259', 'https://openalex.org/W4372266552', 'https://openalex.org/W4226343611', 'https://openalex.org/W4294534174', 'https://openalex.org/W4379251869', 'https://openalex.org/W4221152471', 'https://openalex.org/W4283382678', 'https://openalex.org/W3160506022', 'https://openalex.org/W4318752004', 'https://openalex.org/W4380136719', 'https://openalex.org/W4289106098', 'https://openalex.org/W2615063356', 'https://openalex.org/W4380551955', 'https://openalex.org/W4293575120', 'https://openalex.org/W4385473570', 'https://openalex.org/W4318351475', 'https://openalex.org/W2580221632', 'https://openalex.org/W4319452539', 'https://openalex.org/W4319989813', 'https://openalex.org/W4298310324']",2024-03-18
https://openalex.org/W4392904237,https://doi.org/10.1109/icassp48485.2024.10446088,STEMGEN: A Music Generation Model That Listens,"End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.","['https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W6849105126', 'https://openalex.org/W6853096648', 'https://openalex.org/W6854494257', 'https://openalex.org/W6849635556', 'https://openalex.org/W6839133533', 'https://openalex.org/W6849517043', 'https://openalex.org/W4388341054', 'https://openalex.org/W6776218486', 'https://openalex.org/W6848578254', 'https://openalex.org/W6848854281', 'https://openalex.org/W6853188576', 'https://openalex.org/W6838439425', 'https://openalex.org/W6840815571', 'https://openalex.org/W2998490864', 'https://openalex.org/W6850625674', 'https://openalex.org/W2972478942', 'https://openalex.org/W6610895800', 'https://openalex.org/W2039209917', 'https://openalex.org/W2898827701', 'https://openalex.org/W4372260308', 'https://openalex.org/W4224918587', 'https://openalex.org/W6803003856', 'https://openalex.org/W4224944643', 'https://openalex.org/W4372266552', 'https://openalex.org/W4287802874', 'https://openalex.org/W4282913091', 'https://openalex.org/W4318351475', 'https://openalex.org/W4318719023', 'https://openalex.org/W2519091744', 'https://openalex.org/W4288099666', 'https://openalex.org/W4322718191', 'https://openalex.org/W4319452539', 'https://openalex.org/W4380551955', 'https://openalex.org/W4380136719', 'https://openalex.org/W4383993968', 'https://openalex.org/W4319989813', 'https://openalex.org/W4377010126']",2024-03-18
https://openalex.org/W4393148499,https://doi.org/10.1609/aaai.v38i5.28299,V2Meow: Meowing to the Visual Beat via Video-to-Music Generation,"Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.","['https://openalex.org/W6793119350', 'https://openalex.org/W2619082050', 'https://openalex.org/W2912947616', 'https://openalex.org/W6736965957', 'https://openalex.org/W6781345847', 'https://openalex.org/W3193521535', 'https://openalex.org/W3207290297', 'https://openalex.org/W2772474126', 'https://openalex.org/W3015538334', 'https://openalex.org/W6780770960', 'https://openalex.org/W2593116425', 'https://openalex.org/W2946521317', 'https://openalex.org/W4293575120', 'https://openalex.org/W6601851198', 'https://openalex.org/W3046715528', 'https://openalex.org/W3016096605', 'https://openalex.org/W2966220025', 'https://openalex.org/W2885835225', 'https://openalex.org/W6687537536', 'https://openalex.org/W2792210438', 'https://openalex.org/W6780316881', 'https://openalex.org/W4361807266', 'https://openalex.org/W4283022926', 'https://openalex.org/W1522734439', 'https://openalex.org/W6798098866', 'https://openalex.org/W6746008334', 'https://openalex.org/W3034572008', 'https://openalex.org/W4385245566', 'https://openalex.org/W4318718996', 'https://openalex.org/W4287802874', 'https://openalex.org/W4319989813', 'https://openalex.org/W2963807156', 'https://openalex.org/W4318351475', 'https://openalex.org/W2963524571', 'https://openalex.org/W4283074862', 'https://openalex.org/W3037391061', 'https://openalex.org/W4300980117', 'https://openalex.org/W2606176153', 'https://openalex.org/W2963066677', 'https://openalex.org/W3111853169', 'https://openalex.org/W4386071656', 'https://openalex.org/W4300936792', 'https://openalex.org/W3046890131', 'https://openalex.org/W3108240585', 'https://openalex.org/W3006926732', 'https://openalex.org/W2913302158', 'https://openalex.org/W4318752004', 'https://openalex.org/W4307323391', 'https://openalex.org/W3204221554', 'https://openalex.org/W4381786045', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W2524365899', 'https://openalex.org/W4367860227', 'https://openalex.org/W3126721948', 'https://openalex.org/W2526050071', 'https://openalex.org/W3166396011', 'https://openalex.org/W4318719023', 'https://openalex.org/W4287372095', 'https://openalex.org/W4380551955', 'https://openalex.org/W2950547518', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963408210', 'https://openalex.org/W4318718630', 'https://openalex.org/W2997367363', 'https://openalex.org/W2964345931', 'https://openalex.org/W4312814772', 'https://openalex.org/W4367359628', 'https://openalex.org/W4383993968', 'https://openalex.org/W4287995946', 'https://openalex.org/W3206395542', 'https://openalex.org/W4312319752', 'https://openalex.org/W4214612132', 'https://openalex.org/W4380136719']",2024-03-24
https://openalex.org/W4392904158,https://doi.org/10.1109/icassp48485.2024.10447253,Maskmark: Robust Neuralwatermarking for Real and Synthetic Speech,"High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech. MaskMark embeds a secret key vector in audio via a multiplicative spectrogram mask, allowing the detection of watermarked speech segments even under substantial signal-processing or neural network-based transformations. Comparisons to a state-of-the-art baseline on natural and synthetic speech corpora and a human subjects evaluation demonstrate MaskMark's superior robustness in detecting watermarked speech while maintaining high perceptual transparency.","['https://openalex.org/W4381198892', 'https://openalex.org/W4313306150', 'https://openalex.org/W6838843145', 'https://openalex.org/W6802838302', 'https://openalex.org/W6796464841', 'https://openalex.org/W2963964246', 'https://openalex.org/W4382202512', 'https://openalex.org/W4200083849', 'https://openalex.org/W3097777922', 'https://openalex.org/W2126270135', 'https://openalex.org/W6688816777', 'https://openalex.org/W2696967604', 'https://openalex.org/W2972541922', 'https://openalex.org/W2555915854', 'https://openalex.org/W3097945073', 'https://openalex.org/W4307323391', 'https://openalex.org/W3093964892', 'https://openalex.org/W6757107679', 'https://openalex.org/W2086381917', 'https://openalex.org/W1552314771', 'https://openalex.org/W6852629737', 'https://openalex.org/W4307977691', 'https://openalex.org/W6853165267', 'https://openalex.org/W6783182287', 'https://openalex.org/W6783867762', 'https://openalex.org/W6853515095', 'https://openalex.org/W3197912330', 'https://openalex.org/W4281736089', 'https://openalex.org/W4380551955', 'https://openalex.org/W3169905056', 'https://openalex.org/W4286899907', 'https://openalex.org/W4379259581', 'https://openalex.org/W3129651364', 'https://openalex.org/W2219249508', 'https://openalex.org/W4379251513', 'https://openalex.org/W3092028330']",2024-03-18
https://openalex.org/W4392931975,https://doi.org/10.1109/icassp48485.2024.10446556,Generative De-Quantization for Neural Speech Codec Via Latent Diffusion,"End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W6845479124', 'https://openalex.org/W3215615641', 'https://openalex.org/W6853515095', 'https://openalex.org/W4307323391', 'https://openalex.org/W3110277971', 'https://openalex.org/W3186609711', 'https://openalex.org/W4284888249', 'https://openalex.org/W3161744562', 'https://openalex.org/W4372259964', 'https://openalex.org/W4372268681', 'https://openalex.org/W2775336875', 'https://openalex.org/W2935711438', 'https://openalex.org/W2972519044', 'https://openalex.org/W3140429000', 'https://openalex.org/W4375869380', 'https://openalex.org/W4372348514', 'https://openalex.org/W4372263438', 'https://openalex.org/W4312933868', 'https://openalex.org/W6779823529', 'https://openalex.org/W6783713337', 'https://openalex.org/W6793578827', 'https://openalex.org/W6849109464', 'https://openalex.org/W1494198834', 'https://openalex.org/W2401258970', 'https://openalex.org/W1552314771', 'https://openalex.org/W4385680913', 'https://openalex.org/W4313679638', 'https://openalex.org/W4380551955', 'https://openalex.org/W4287250916', 'https://openalex.org/W4212774754', 'https://openalex.org/W4300980117', 'https://openalex.org/W4318752004']",2024-03-18
https://openalex.org/W4402402508,https://doi.org/10.1109/icasspw62465.2024.10669909,Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio,,"['https://openalex.org/W3133695494', 'https://openalex.org/W6769017931', 'https://openalex.org/W2964075883', 'https://openalex.org/W3182900969', 'https://openalex.org/W3146622378', 'https://openalex.org/W2765813195', 'https://openalex.org/W6856165946', 'https://openalex.org/W6765778736', 'https://openalex.org/W6765910423', 'https://openalex.org/W2159561775', 'https://openalex.org/W6602337844', 'https://openalex.org/W6635859925', 'https://openalex.org/W1950706092', 'https://openalex.org/W4389450885', 'https://openalex.org/W2059652044', 'https://openalex.org/W6746228475', 'https://openalex.org/W6764574124', 'https://openalex.org/W6844040096', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W6712692208', 'https://openalex.org/W6785932716', 'https://openalex.org/W6732646663', 'https://openalex.org/W6809884996', 'https://openalex.org/W4387224530', 'https://openalex.org/W2945837706', 'https://openalex.org/W2964218314', 'https://openalex.org/W4386794615', 'https://openalex.org/W2981731882', 'https://openalex.org/W4286432986', 'https://openalex.org/W2952395326', 'https://openalex.org/W4300957348', 'https://openalex.org/W4380551955']",2024-04-14
https://openalex.org/W4401609391,https://doi.org/10.1109/icasspw62465.2024.10627667,A Fullband Neural Network for Audio Packet Loss Concealment,,"['https://openalex.org/W4296069296', 'https://openalex.org/W4401597532', 'https://openalex.org/W4297841545', 'https://openalex.org/W2172065531', 'https://openalex.org/W6838843145', 'https://openalex.org/W6853515095', 'https://openalex.org/W3162366763', 'https://openalex.org/W4392931003', 'https://openalex.org/W4281736089', 'https://openalex.org/W4380551955']",2024-04-14
https://openalex.org/W4408355354,https://doi.org/10.1109/icassp49660.2025.10890379,Speech Enhancement Using Continuous Embeddings of Neural Audio Codec,"Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.","['https://openalex.org/W2963045393', 'https://openalex.org/W2962843322', 'https://openalex.org/W3206809722', 'https://openalex.org/W3129077738', 'https://openalex.org/W2900381824', 'https://openalex.org/W2774389566', 'https://openalex.org/W2972436155', 'https://openalex.org/W3011982609', 'https://openalex.org/W3131332223', 'https://openalex.org/W4402111962', 'https://openalex.org/W4221144097', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W6848735303', 'https://openalex.org/W4402112643', 'https://openalex.org/W4400111385', 'https://openalex.org/W6856434366', 'https://openalex.org/W4402111399', 'https://openalex.org/W4402115964', 'https://openalex.org/W6779577414', 'https://openalex.org/W4392909571', 'https://openalex.org/W4225905067', 'https://openalex.org/W6936113694', 'https://openalex.org/W3097906045', 'https://openalex.org/W3161480375', 'https://openalex.org/W1552314771', 'https://openalex.org/W4281820413', 'https://openalex.org/W2890964092', 'https://openalex.org/W3163652268', 'https://openalex.org/W3167533889', 'https://openalex.org/W4296069339', 'https://openalex.org/W4297841790', 'https://openalex.org/W4384080510', 'https://openalex.org/W4406859288', 'https://openalex.org/W1556611829', 'https://openalex.org/W4313679638', 'https://openalex.org/W4387323811', 'https://openalex.org/W4380551955', 'https://openalex.org/W3034302232']",2025-03-12
https://openalex.org/W4392903014,https://doi.org/10.1109/icassp48485.2024.10446067,Personalized Neural Speech Codec,"In this paper, we propose a personalized neural speech codec, envisioning\nthat personalization can reduce the model complexity or improve perceptual\nspeech quality. Despite the common usage of speech codecs where only a single\ntalker is involved on each side of the communication, personalizing a codec for\nthe specific user has rarely been explored in the literature. First, we assume\nspeakers can be grouped into smaller subsets based on their perceptual\nsimilarity. Then, we also postulate that a group-specific codec can focus on\nthe group's speech characteristics to improve its perceptual quality and\ncomputational efficiency. To this end, we first develop a Siamese network that\nlearns the speaker embeddings from the LibriSpeech dataset, which are then\ngrouped into underlying speaker clusters. Finally, we retrain the LPCNet-based\nspeech codec baselines on each of the speaker clusters. Subjective listening\ntests show that the proposed personalization scheme introduces model\ncompression while maintaining speech quality. In other words, with the same\nmodel complexity, personalized codecs produce better speech quality.\n","['https://openalex.org/W2519091744', 'https://openalex.org/W2775336875', 'https://openalex.org/W2935711438', 'https://openalex.org/W2963091184', 'https://openalex.org/W6748409065', 'https://openalex.org/W2972519044', 'https://openalex.org/W4297841815', 'https://openalex.org/W4225290925', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6739901393', 'https://openalex.org/W4377231659', 'https://openalex.org/W6853515095', 'https://openalex.org/W4225860133', 'https://openalex.org/W4285258106', 'https://openalex.org/W3162424266', 'https://openalex.org/W3162538144', 'https://openalex.org/W4226185896', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127589108', 'https://openalex.org/W3080427797', 'https://openalex.org/W4293363567', 'https://openalex.org/W4380551955', 'https://openalex.org/W2978329087', 'https://openalex.org/W2187089797']",2024-03-18
https://openalex.org/W4403126617,https://doi.org/10.1109/iwaenc61483.2024.10694655,Evaluation of Objective Quality Models on Neural Audio Codecs,International audience,"['https://openalex.org/W1552314771', 'https://openalex.org/W3037038648', 'https://openalex.org/W2963091184', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W6853515095', 'https://openalex.org/W4296068974', 'https://openalex.org/W4221144124', 'https://openalex.org/W4297841766', 'https://openalex.org/W6800295808', 'https://openalex.org/W349236604', 'https://openalex.org/W4366684661', 'https://openalex.org/W3145029257', 'https://openalex.org/W3097206152', 'https://openalex.org/W2067295501', 'https://openalex.org/W4391421598', 'https://openalex.org/W2204452154', 'https://openalex.org/W1930928341', 'https://openalex.org/W2163181067', 'https://openalex.org/W6634931237', 'https://openalex.org/W6636045042', 'https://openalex.org/W3196475561', 'https://openalex.org/W3160077247', 'https://openalex.org/W4225956675', 'https://openalex.org/W4225302959', 'https://openalex.org/W3163113319', 'https://openalex.org/W1481955708', 'https://openalex.org/W4385680913', 'https://openalex.org/W4402112168', 'https://openalex.org/W4380551955', 'https://openalex.org/W652994534', 'https://openalex.org/W1606487971', 'https://openalex.org/W1588296165', 'https://openalex.org/W3197238235']",2024-09-09
https://openalex.org/W4405845644,https://doi.org/10.1109/isceic63613.2024.10810217,Improved Ultra-Low Bit Rate MELP Algorithm for BeiDou Satellite Communication,,"['https://openalex.org/W3161809154', 'https://openalex.org/W4285325517', 'https://openalex.org/W3215615641', 'https://openalex.org/W6783867762', 'https://openalex.org/W6853515095', 'https://openalex.org/W6802517614', 'https://openalex.org/W3180355996', 'https://openalex.org/W2462527566', 'https://openalex.org/W1552314771', 'https://openalex.org/W3161480375', 'https://openalex.org/W1728888090', 'https://openalex.org/W4372348879', 'https://openalex.org/W4312120714', 'https://openalex.org/W4224929672', 'https://openalex.org/W3206395542', 'https://openalex.org/W2990138404', 'https://openalex.org/W4380551955']",2024-11-08
https://openalex.org/W4406233769,https://doi.org/10.17163/ings.n33.2025.01,Determination of optimal formats for digital image compression,"Se concluye que independientemente de la herramienta que se utilice, es el formato de la imagen lo que influye en el tamaño final.&amp; The objective was to determine the influence of different image formats and tools used for compression on the final size of the images, to know which are the optimal formats for compression. The sample was made up of five digital image files with BMP extension, taken in different scenarios and at different times at the researcher's discretion. The technique used was the analysis of digital image files and as an instrument a double input matrix, where the conversions of BMP files to six different extensions of image files were registered, with four different tools for manipulation of image files. The experimental design was factorial, where the two factors were the image compression formats and tools and the dependent variable the final image file size. Factorial ANOVA statistical analysis was applied with a = 0.05. It was obtained that the format of smaller size was the JPG when using as tool the Illustrator and the one of greater size the one of greater extension the PSD also obtained with the Illustrator. The statistical analysis showed that the format factor significantly influences the final size of the images (p &lt; 0.05) and the tool factor does not show significant influence on the size of the images (p &gt; 0.05), nor is the interaction between the factors significant. It is concluded that regardless of the tool used, it is the image format that influences the final size.","['https://openalex.org/W4392672786', 'https://openalex.org/W2998600732', 'https://openalex.org/W2614571092', 'https://openalex.org/W2268076750', 'https://openalex.org/W1513626946', 'https://openalex.org/W2188519435', 'https://openalex.org/W2964122515', 'https://openalex.org/W2569234446', 'https://openalex.org/W3034008104', 'https://openalex.org/W2106648736', 'https://openalex.org/W2148685281', 'https://openalex.org/W2517064760', 'https://openalex.org/W2766934651', 'https://openalex.org/W2622645257', 'https://openalex.org/W2187490662', 'https://openalex.org/W2186446247', 'https://openalex.org/W2316556732', 'https://openalex.org/W2914628419', 'https://openalex.org/W3178376550', 'https://openalex.org/W4214692777', 'https://openalex.org/W4367721819', 'https://openalex.org/W4380551955']",2025-01-10
https://openalex.org/W4406461857,https://doi.org/10.1109/slt61566.2024.10832165,Data Efficient Reflow for Few Step Audio Generation,,"['https://openalex.org/W4381786045', 'https://openalex.org/W6845479124', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6849109464', 'https://openalex.org/W6861353174', 'https://openalex.org/W4252812408', 'https://openalex.org/W6859583170', 'https://openalex.org/W6809884996', 'https://openalex.org/W6864565647', 'https://openalex.org/W4402753769', 'https://openalex.org/W6857588802', 'https://openalex.org/W4206662443', 'https://openalex.org/W6838327568', 'https://openalex.org/W6846827642', 'https://openalex.org/W6843731886', 'https://openalex.org/W6856548676', 'https://openalex.org/W4392904491', 'https://openalex.org/W6853515095', 'https://openalex.org/W6853096648', 'https://openalex.org/W6846539466', 'https://openalex.org/W6857842755', 'https://openalex.org/W4386065690', 'https://openalex.org/W6840815571', 'https://openalex.org/W6810595431', 'https://openalex.org/W6757220786', 'https://openalex.org/W6846849257', 'https://openalex.org/W6769627184', 'https://openalex.org/W4380551955', 'https://openalex.org/W4387389604', 'https://openalex.org/W4221159371', 'https://openalex.org/W4308167501', 'https://openalex.org/W4390306858', 'https://openalex.org/W4394656839', 'https://openalex.org/W4281661987', 'https://openalex.org/W4388093137', 'https://openalex.org/W4297676498', 'https://openalex.org/W1583837637', 'https://openalex.org/W4288089799', 'https://openalex.org/W4391709184', 'https://openalex.org/W4303647933', 'https://openalex.org/W4312933868', 'https://openalex.org/W4288099666', 'https://openalex.org/W4372260310', 'https://openalex.org/W4386722101']",2024-12-02
https://openalex.org/W4412459428,https://doi.org/10.1016/j.specom.2025.103279,Towards robust heart failure detection in digital telephony environments by utilizing transformer-based codec inversion,,"['https://openalex.org/W2609527102', 'https://openalex.org/W6710796637', 'https://openalex.org/W3088922629', 'https://openalex.org/W2942897167', 'https://openalex.org/W3205690112', 'https://openalex.org/W6877558504', 'https://openalex.org/W6873369553', 'https://openalex.org/W6682881918', 'https://openalex.org/W2165558283', 'https://openalex.org/W6773037266', 'https://openalex.org/W3135466576', 'https://openalex.org/W3022396738', 'https://openalex.org/W3197125019', 'https://openalex.org/W2071738728', 'https://openalex.org/W6752283474', 'https://openalex.org/W2897150037', 'https://openalex.org/W2944209089', 'https://openalex.org/W3188283161', 'https://openalex.org/W2090777335', 'https://openalex.org/W6608225831', 'https://openalex.org/W4387580450', 'https://openalex.org/W6610415083', 'https://openalex.org/W6679331445', 'https://openalex.org/W6846958243', 'https://openalex.org/W2749421233', 'https://openalex.org/W2117245153', 'https://openalex.org/W2060718800', 'https://openalex.org/W2959813266', 'https://openalex.org/W2100787016', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197312245', 'https://openalex.org/W6872777128', 'https://openalex.org/W4385728310', 'https://openalex.org/W4395661434', 'https://openalex.org/W4391825007', 'https://openalex.org/W6842958304', 'https://openalex.org/W3014647115', 'https://openalex.org/W2995181338', 'https://openalex.org/W6674047566', 'https://openalex.org/W3094604100', 'https://openalex.org/W6742856688', 'https://openalex.org/W6873450539', 'https://openalex.org/W6853515095', 'https://openalex.org/W6790970297', 'https://openalex.org/W3169882120', 'https://openalex.org/W6874278102', 'https://openalex.org/W6878092324', 'https://openalex.org/W6856205631', 'https://openalex.org/W6872920942', 'https://openalex.org/W4306643859', 'https://openalex.org/W6750562983', 'https://openalex.org/W2191779130', 'https://openalex.org/W4205643594', 'https://openalex.org/W2121771908', 'https://openalex.org/W2766371122', 'https://openalex.org/W6795699504', 'https://openalex.org/W2905588048', 'https://openalex.org/W2937864292', 'https://openalex.org/W3033820982', 'https://openalex.org/W4360616662', 'https://openalex.org/W1924962846', 'https://openalex.org/W6691007031', 'https://openalex.org/W6629717138', 'https://openalex.org/W6756040250', 'https://openalex.org/W6675354045', 'https://openalex.org/W2533139507', 'https://openalex.org/W2745490897', 'https://openalex.org/W6784642771', 'https://openalex.org/W6843956233', 'https://openalex.org/W4390588437', 'https://openalex.org/W6877878634', 'https://openalex.org/W1996162794', 'https://openalex.org/W3204774355', 'https://openalex.org/W2999565548', 'https://openalex.org/W6791360606', 'https://openalex.org/W4392939451', 'https://openalex.org/W4389943413', 'https://openalex.org/W2018514196', 'https://openalex.org/W4210937888', 'https://openalex.org/W6864284187', 'https://openalex.org/W6872511270', 'https://openalex.org/W4399168695', 'https://openalex.org/W6803915641', 'https://openalex.org/W6852594348', 'https://openalex.org/W6803885467', 'https://openalex.org/W4214634056', 'https://openalex.org/W3120489340', 'https://openalex.org/W2017870635', 'https://openalex.org/W6755811826', 'https://openalex.org/W6739901393', 'https://openalex.org/W6751466015', 'https://openalex.org/W2807317037', 'https://openalex.org/W6800712280', 'https://openalex.org/W6876411249', 'https://openalex.org/W4408168195', 'https://openalex.org/W2809824582', 'https://openalex.org/W3197642003', 'https://openalex.org/W4403662039', 'https://openalex.org/W4231512336', 'https://openalex.org/W4402111621', 'https://openalex.org/W4403344204', 'https://openalex.org/W4407764388', 'https://openalex.org/W4310787750', 'https://openalex.org/W289000934', 'https://openalex.org/W4406880529', 'https://openalex.org/W2605311049', 'https://openalex.org/W2623902889', 'https://openalex.org/W4210531520', 'https://openalex.org/W4385245566', 'https://openalex.org/W4376548770', 'https://openalex.org/W1522301498', 'https://openalex.org/W2105961477', 'https://openalex.org/W4299800467', 'https://openalex.org/W3101840241', 'https://openalex.org/W4234842379', 'https://openalex.org/W2997591727', 'https://openalex.org/W4391094882', 'https://openalex.org/W3152218910', 'https://openalex.org/W3125384922', 'https://openalex.org/W3096468295', 'https://openalex.org/W4406794296', 'https://openalex.org/W3140429000', 'https://openalex.org/W2385755489', 'https://openalex.org/W4403883382', 'https://openalex.org/W3133739748', 'https://openalex.org/W3215191595', 'https://openalex.org/W4402111399', 'https://openalex.org/W4406032552', 'https://openalex.org/W2797142604', 'https://openalex.org/W3134002425', 'https://openalex.org/W3208743843', 'https://openalex.org/W4380551955', 'https://openalex.org/W4408347303', 'https://openalex.org/W3036601975', 'https://openalex.org/W2496661657']",2025-07-16
https://openalex.org/W4413329591,https://doi.org/10.1121/10.0038967,A scalable codec for bone-conducted speech based on generative and diffusion models,"In extremely noisy communication scenarios, the bone-conducted microphone (BCM) speech codec is often combined with speech bandwidth extension to improve the BCM speech quality. However, this tandem approach leads to a complex system architecture. To address the problem, a scalable codec for BCM speech based on generative and diffusion probabilistic models is proposed in this paper. Specifically, a specialized codec architecture is constructed to encode BCM speech while complementing its high-frequency components. Then, a key feature extraction block is presented to address the diminishing memory capacity in shallow layers as the network depth increases. Next, considering the potential lack of high-frequency detail information, an overall refinement block is introduced to refine the reconstructed speech signals. Finally, based on the U-Net architecture, a diffusion probability model is proposed to upsample the input audio signal from a bandwidth of 8 kHz to a high-resolution audio signal with a bandwidth of 20 kHz and a sampling rate of 48 kHz. The proposed method can simultaneously encode and improve BCM speech quality using a single network. It supports different bitrate settings without architectural changes or retraining and dynamically adjusts transmitted data based on changing network load. Simulation experiments demonstrate its feasibility.","['https://openalex.org/W2162743322', 'https://openalex.org/W2165291881', 'https://openalex.org/W4224923672', 'https://openalex.org/W3082563516', 'https://openalex.org/W2066359120', 'https://openalex.org/W2166344203', 'https://openalex.org/W2518315489', 'https://openalex.org/W4283215837', 'https://openalex.org/W4307411819', 'https://openalex.org/W1728888090', 'https://openalex.org/W3036167779', 'https://openalex.org/W4405022017', 'https://openalex.org/W6640963894', 'https://openalex.org/W3092028330', 'https://openalex.org/W3087665158', 'https://openalex.org/W2163605009', 'https://openalex.org/W4401887295', 'https://openalex.org/W6741681139', 'https://openalex.org/W2970006822', 'https://openalex.org/W4380551955', 'https://openalex.org/W3197334236', 'https://openalex.org/W4389164214', 'https://openalex.org/W2047233879', 'https://openalex.org/W4386794324', 'https://openalex.org/W6757817989', 'https://openalex.org/W171863622', 'https://openalex.org/W2769810959', 'https://openalex.org/W2147152002', 'https://openalex.org/W2080441134', 'https://openalex.org/W2588815553', 'https://openalex.org/W2947590261', 'https://openalex.org/W4377699820', 'https://openalex.org/W2129069237', 'https://openalex.org/W3110257065', 'https://openalex.org/W2095705004', 'https://openalex.org/W2141998673', 'https://openalex.org/W3095095816', 'https://openalex.org/W3043692043', 'https://openalex.org/W2161503095', 'https://openalex.org/W2752796333', 'https://openalex.org/W2131738223', 'https://openalex.org/W2968763213', 'https://openalex.org/W4410246158', 'https://openalex.org/W4297095639', 'https://openalex.org/W4311897894', 'https://openalex.org/W2400830530', 'https://openalex.org/W4307783450', 'https://openalex.org/W3215615641', 'https://openalex.org/W4224883138', 'https://openalex.org/W3011318140']",2025-08-01
https://openalex.org/W4387247604,https://doi.org/10.1109/taslp.2023.3320864,Disentangling Prosody Representations With Unsupervised Speech Reconstruction,"Human speech can be characterized by different components, including semantic content, speaker identity and prosodic information. Significant progress has been made in disentangling representations for semantic content and speaker identity in Automatic Speech Recognition (ASR) and speaker verification tasks respectively. However, it is still an open challenging research question to extract prosodic information because of the intrinsic association of different attributes, such as timbre and rhythm, and because of the need for supervised training schemes to achieve robust large-scale and speaker-independent ASR. The aim of this paper is to address the disentanglement of emotional prosody from speech based on unsupervised reconstruction. Specifically, we identify, design, implement and integrate three crucial components in our proposed speech reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech signals into discrete units for semantic content, (2) a pretrained speaker verification model to generate speaker identity embeddings, and (3) a trainable prosody encoder to learn prosody representations. We first pretrain the Prosody2Vec representations on unlabelled emotional speech corpora, then fine-tune the model on specific datasets to perform Speech Emotion Recognition (SER) and Emotional Voice Conversion (EVC) tasks. Both objective (weighted and unweighted accuracies) and subjective (mean opinion score) evaluations on the EVC task suggest that Prosody2Vec effectively captures general prosodic features that can be smoothly transferred to other emotional speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the prosody features learned by Prosody2Vec are complementary and beneficial for the performance of widely used speech pretraining models and surpass the state-of-the-art methods when combining Prosody2Vec with HuBERT representations. Some audio samples can be found on our demo website","['https://openalex.org/W3209059054', 'https://openalex.org/W2995181338', 'https://openalex.org/W2583542555', 'https://openalex.org/W3197580070', 'https://openalex.org/W4287887366', 'https://openalex.org/W3096723250', 'https://openalex.org/W4385574033', 'https://openalex.org/W2130821326', 'https://openalex.org/W2936774411', 'https://openalex.org/W4205742757', 'https://openalex.org/W6755207826', 'https://openalex.org/W6631190155', 'https://openalex.org/W3209984917', 'https://openalex.org/W6621543089', 'https://openalex.org/W2069924379', 'https://openalex.org/W6803547063', 'https://openalex.org/W4226487411', 'https://openalex.org/W2904459034', 'https://openalex.org/W2146334809', 'https://openalex.org/W2963199341', 'https://openalex.org/W4200635083', 'https://openalex.org/W6623517193', 'https://openalex.org/W2742542661', 'https://openalex.org/W6754420807', 'https://openalex.org/W2928165649', 'https://openalex.org/W2402146185', 'https://openalex.org/W2964243274', 'https://openalex.org/W2752782242', 'https://openalex.org/W2342475039', 'https://openalex.org/W6776390925', 'https://openalex.org/W6750489868', 'https://openalex.org/W3135547455', 'https://openalex.org/W2889374687', 'https://openalex.org/W2973181312', 'https://openalex.org/W2748654097', 'https://openalex.org/W2511640485', 'https://openalex.org/W2808631503', 'https://openalex.org/W3197993066', 'https://openalex.org/W3015241559', 'https://openalex.org/W3204457821', 'https://openalex.org/W4221147462', 'https://openalex.org/W6803378298', 'https://openalex.org/W6847363464', 'https://openalex.org/W4224918091', 'https://openalex.org/W2803193013', 'https://openalex.org/W3025680351', 'https://openalex.org/W2050681655', 'https://openalex.org/W2785978752', 'https://openalex.org/W3112594642', 'https://openalex.org/W2009375902', 'https://openalex.org/W3024869864', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015884429', 'https://openalex.org/W3160039712', 'https://openalex.org/W3204087964', 'https://openalex.org/W3205428167', 'https://openalex.org/W3175161143', 'https://openalex.org/W4312120641', 'https://openalex.org/W2795109282', 'https://openalex.org/W6810585344', 'https://openalex.org/W6746468907', 'https://openalex.org/W6795949861', 'https://openalex.org/W4313887688', 'https://openalex.org/W6762533536', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W6637108112', 'https://openalex.org/W6810007534', 'https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W2748702193', 'https://openalex.org/W6849896277', 'https://openalex.org/W4285251897', 'https://openalex.org/W4221162872', 'https://openalex.org/W3130293557', 'https://openalex.org/W2972498864', 'https://openalex.org/W2896457183', 'https://openalex.org/W648786980', 'https://openalex.org/W2187089797', 'https://openalex.org/W3211224152', 'https://openalex.org/W4311000453', 'https://openalex.org/W4301371414', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287073476', 'https://openalex.org/W4221145109', 'https://openalex.org/W2794490148', 'https://openalex.org/W2945478979', 'https://openalex.org/W2891205112', 'https://openalex.org/W4319988532', 'https://openalex.org/W2774085128', 'https://openalex.org/W4295731579', 'https://openalex.org/W4225939199', 'https://openalex.org/W854541894', 'https://openalex.org/W1686946872', 'https://openalex.org/W1522301498', 'https://openalex.org/W3034794073', 'https://openalex.org/W3112034174']",2023-10-02
https://openalex.org/W4412474972,https://doi.org/10.5753/jbcs.2025.5468,The evaluation of prosody in speech synthesis: a systematic review,"This paper presents a systematic review on the relationship between prosody and speech synthesis, focusing on the evaluation of prosodic parameters of synthesized speech. The relevance of the topic lies in the fact that the task of speech synthesis has not yet been resolved, therefore the information obtained in this review can contribute to knowledge and to the improvement of the methodologies used in evaluating the prosody of synthesized speech. To select studies, we used the Parsifal platform, including 100 studies published between 2020 and 2024, with the purpose of answering eight previously established research questions. The highlights of this systematic review are presented in the following. The main prosodic parameters considered in speech synthesis systems are fundamental frequency (F0), duration and intensity, with F0 standing out (95 studies). The metric most frequently used in studies belongs to the group of acoustic metrics --- F0-RMSE (Root mean-squared error evaluation of F0). Lower values of this metric indicate greater proximity between the F0 of synthesized speech and that of natural speech. The most used dataset was LJ Speech, a public domain speech dataset consisting of English audio clips of a single speaker reading short excerpts from seven non-fiction books, reinforcing that the predominant language was English --- 48 studies focus on English to evaluate speech description prosody, although there is a relevant number of studies in Mandarin Chinese (27 studies) and Japanese (15 studies).Most studies used models as a baseline to compare the performance of their methods or proposed new models in order to improve the prosody of synthesized speech. Each study presented different methods for this improvement, according to the objectives, such as learning prosodic features extracted from reference speech and adding auxiliary modules to existing model architectures. As highlighted baselines, there was recurrent use of Tacotron 2, which generates mel-spectrograms from text and then synthesize speech from the generated mel-spectrograms using a separately trained vocoder, and FastSpeech 2, which can extract explicit prosodic features to be directly used as entry into training.","['https://openalex.org/W4388692791', 'https://openalex.org/W4396696854', 'https://openalex.org/W3088892837', 'https://openalex.org/W3196964833', 'https://openalex.org/W4297841709', 'https://openalex.org/W3197541421', 'https://openalex.org/W4404781830', 'https://openalex.org/W6927786696', 'https://openalex.org/W3209383001', 'https://openalex.org/W6887974088', 'https://openalex.org/W1791080472', 'https://openalex.org/W4360770798', 'https://openalex.org/W3162948689', 'https://openalex.org/W2106564373', 'https://openalex.org/W3151450932', 'https://openalex.org/W4393859250', 'https://openalex.org/W4392902952', 'https://openalex.org/W3197287223', 'https://openalex.org/W4312095991', 'https://openalex.org/W3114420835', 'https://openalex.org/W4221140758', 'https://openalex.org/W4377233062', 'https://openalex.org/W3160844600', 'https://openalex.org/W4391021635', 'https://openalex.org/W4401808466', 'https://openalex.org/W4402263167', 'https://openalex.org/W3205579917', 'https://openalex.org/W4324027600', 'https://openalex.org/W4224932891', 'https://openalex.org/W2395804757', 'https://openalex.org/W6940163724', 'https://openalex.org/W4385570943', 'https://openalex.org/W4392903668', 'https://openalex.org/W4392904615', 'https://openalex.org/W4392909673', 'https://openalex.org/W4401733313', 'https://openalex.org/W4384615685', 'https://openalex.org/W4281983017', 'https://openalex.org/W52922326', 'https://openalex.org/W4296068812', 'https://openalex.org/W4206003764', 'https://openalex.org/W4226050434', 'https://openalex.org/W6923403701', 'https://openalex.org/W4372270801', 'https://openalex.org/W3150572638', 'https://openalex.org/W4224319466', 'https://openalex.org/W4372266971', 'https://openalex.org/W4385993857', 'https://openalex.org/W4392904093', 'https://openalex.org/W4226421465', 'https://openalex.org/W4402112175', 'https://openalex.org/W4380714711', 'https://openalex.org/W3183102188', 'https://openalex.org/W3168542456', 'https://openalex.org/W3162271107', 'https://openalex.org/W4297841650', 'https://openalex.org/W4401806189', 'https://openalex.org/W4385822381', 'https://openalex.org/W3047089639', 'https://openalex.org/W2061926458', 'https://openalex.org/W4405255149', 'https://openalex.org/W4318975479', 'https://openalex.org/W4388692852', 'https://openalex.org/W4388692895', 'https://openalex.org/W4382296240', 'https://openalex.org/W3196843885', 'https://openalex.org/W4214883111', 'https://openalex.org/W1973378890', 'https://openalex.org/W4385822575', 'https://openalex.org/W4396542467', 'https://openalex.org/W4385993782', 'https://openalex.org/W4400165212', 'https://openalex.org/W3203313352', 'https://openalex.org/W4297841883', 'https://openalex.org/W3032661621', 'https://openalex.org/W3097892637', 'https://openalex.org/W3130016944', 'https://openalex.org/W4286950013', 'https://openalex.org/W4402112245', 'https://openalex.org/W4232760665', 'https://openalex.org/W4308599760', 'https://openalex.org/W4404037498', 'https://openalex.org/W4225596771', 'https://openalex.org/W1570629387', 'https://openalex.org/W4385823466', 'https://openalex.org/W1544407768', 'https://openalex.org/W3011535310', 'https://openalex.org/W4221142789', 'https://openalex.org/W4226487411', 'https://openalex.org/W4224928640', 'https://openalex.org/W3045000780', 'https://openalex.org/W3041738652', 'https://openalex.org/W4385822941', 'https://openalex.org/W4372341043', 'https://openalex.org/W4404139479', 'https://openalex.org/W4295036296', 'https://openalex.org/W4323022342', 'https://openalex.org/W3207354624', 'https://openalex.org/W3095401840', 'https://openalex.org/W3166348604', 'https://openalex.org/W4403791830', 'https://openalex.org/W4386608861', 'https://openalex.org/W3193689627', 'https://openalex.org/W3097575868', 'https://openalex.org/W3197304925', 'https://openalex.org/W4383220533', 'https://openalex.org/W4388820352', 'https://openalex.org/W4385329631', 'https://openalex.org/W4391892546', 'https://openalex.org/W3198311967', 'https://openalex.org/W3130168980']",2025-07-09
https://openalex.org/W3168542456,https://doi.org/10.1109/taslp.2021.3076369,Expressive TTS Training With Frame and Style Reconstruction Loss,"We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn't require prosody annotations from training data. It doesn't attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.","['https://openalex.org/W2963522749', 'https://openalex.org/W2331128040', 'https://openalex.org/W2795109282', 'https://openalex.org/W2885800352', 'https://openalex.org/W6750489868', 'https://openalex.org/W2025617415', 'https://openalex.org/W6692550842', 'https://openalex.org/W6784688130', 'https://openalex.org/W3015440759', 'https://openalex.org/W3016021263', 'https://openalex.org/W3026278778', 'https://openalex.org/W6778385928', 'https://openalex.org/W6606697926', 'https://openalex.org/W6761673384', 'https://openalex.org/W6634507583', 'https://openalex.org/W2911340057', 'https://openalex.org/W2759925408', 'https://openalex.org/W3106690208', 'https://openalex.org/W4240592325', 'https://openalex.org/W2949973225', 'https://openalex.org/W2786387703', 'https://openalex.org/W3012016043', 'https://openalex.org/W3098557217', 'https://openalex.org/W2963945466', 'https://openalex.org/W6712034360', 'https://openalex.org/W6607761755', 'https://openalex.org/W6604530167', 'https://openalex.org/W2785978752', 'https://openalex.org/W2330979245', 'https://openalex.org/W2195693064', 'https://openalex.org/W1540083112', 'https://openalex.org/W3016027511', 'https://openalex.org/W6736356763', 'https://openalex.org/W2790783922', 'https://openalex.org/W3025182306', 'https://openalex.org/W2964243274', 'https://openalex.org/W2150658333', 'https://openalex.org/W3012404734', 'https://openalex.org/W2964138190', 'https://openalex.org/W6611766843', 'https://openalex.org/W6748409065', 'https://openalex.org/W2798951647', 'https://openalex.org/W2618530766', 'https://openalex.org/W1552314771', 'https://openalex.org/W6636045042', 'https://openalex.org/W2120847449', 'https://openalex.org/W6728214246', 'https://openalex.org/W2972443522', 'https://openalex.org/W3016050488', 'https://openalex.org/W2982037672', 'https://openalex.org/W6762643587', 'https://openalex.org/W2769810959', 'https://openalex.org/W6777567901', 'https://openalex.org/W2972394484', 'https://openalex.org/W3019993940', 'https://openalex.org/W2937154351', 'https://openalex.org/W6640963894', 'https://openalex.org/W6736010183', 'https://openalex.org/W2404839462', 'https://openalex.org/W6678929712', 'https://openalex.org/W2748654097', 'https://openalex.org/W2740504963', 'https://openalex.org/W2517513811', 'https://openalex.org/W6601608730', 'https://openalex.org/W6860575079', 'https://openalex.org/W2156146072', 'https://openalex.org/W6675380101', 'https://openalex.org/W2111284386', 'https://openalex.org/W6917585676', 'https://openalex.org/W1689711448', 'https://openalex.org/W2885005742', 'https://openalex.org/W6678292227', 'https://openalex.org/W2791322592', 'https://openalex.org/W6681857489', 'https://openalex.org/W6638667902', 'https://openalex.org/W2107860279', 'https://openalex.org/W3154443551', 'https://openalex.org/W2889028433', 'https://openalex.org/W2972702018', 'https://openalex.org/W2972595148', 'https://openalex.org/W3016151052', 'https://openalex.org/W6784342092', 'https://openalex.org/W2786868129', 'https://openalex.org/W2766272105', 'https://openalex.org/W2888932932', 'https://openalex.org/W2963087613', 'https://openalex.org/W2972789651', 'https://openalex.org/W1973378890', 'https://openalex.org/W2889064624', 'https://openalex.org/W2804664105', 'https://openalex.org/W2558531213', 'https://openalex.org/W2146334809', 'https://openalex.org/W1969386661', 'https://openalex.org/W3010916717', 'https://openalex.org/W3096830101', 'https://openalex.org/W3015841875', 'https://openalex.org/W2519091744', 'https://openalex.org/W2945544731', 'https://openalex.org/W2973013313', 'https://openalex.org/W4298580827', 'https://openalex.org/W188951208', 'https://openalex.org/W3094124570', 'https://openalex.org/W3096086473', 'https://openalex.org/W344150399', 'https://openalex.org/W2963272440', 'https://openalex.org/W3162271107', 'https://openalex.org/W4297800839', 'https://openalex.org/W2963782041', 'https://openalex.org/W3102739662', 'https://openalex.org/W2604184139', 'https://openalex.org/W2963927338', 'https://openalex.org/W2963174698', 'https://openalex.org/W4295731579', 'https://openalex.org/W2714549561', 'https://openalex.org/W1836465849', 'https://openalex.org/W113106864', 'https://openalex.org/W2127589467', 'https://openalex.org/W2395980997', 'https://openalex.org/W2952269766', 'https://openalex.org/W2964307104', 'https://openalex.org/W2145577652', 'https://openalex.org/W2529497146', 'https://openalex.org/W1581458799', 'https://openalex.org/W3094917204', 'https://openalex.org/W4390911378', 'https://openalex.org/W397522103', 'https://openalex.org/W2123237149', 'https://openalex.org/W2187089797', 'https://openalex.org/W2102003408', 'https://openalex.org/W2600829178', 'https://openalex.org/W2163605009', 'https://openalex.org/W4294619240', 'https://openalex.org/W251642908', 'https://openalex.org/W2941094131', 'https://openalex.org/W2794490148', 'https://openalex.org/W3163573274', 'https://openalex.org/W1606487971', 'https://openalex.org/W1959608418', 'https://openalex.org/W162654330', 'https://openalex.org/W39968598']",2021-01-01
https://openalex.org/W3160584619,https://doi.org/10.1109/icassp39728.2021.9413543,Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm,"We present a new approach to disentangle speaker voice and phone content by introducing new components to the VQ-VAE architecture for speech synthesis. The original VQ-VAE does not generalize well to unseen speakers or content. To alleviate this problem, we have incorporated a speaker encoder and speaker VQ codebook that learns global speaker characteristics entirely separate from the existing sub-phone codebooks. We also compare two training methods: self-supervised with global conditions and semi-supervised with speaker labels. Adding a speaker VQ component improves objective measures of speech synthesis quality (estimated MOS, speaker similarity, ASR-based intelligibility) and provides learned representations that are meaningful. Our speaker VQ codebook indices can be used in a simple speaker diarization task and perform slightly better than an x-vector baseline. Additionally, phones can be recognized from sub-phone VQ codebook indices in our semi-supervised VQ-VAE better than self-supervised with global conditions.","['https://openalex.org/W6778356106', 'https://openalex.org/W3094917204', 'https://openalex.org/W6637618735', 'https://openalex.org/W6772349387', 'https://openalex.org/W3024752295', 'https://openalex.org/W6601576264', 'https://openalex.org/W2889418727', 'https://openalex.org/W3037217258', 'https://openalex.org/W6728030952', 'https://openalex.org/W3015826515', 'https://openalex.org/W2963371159', 'https://openalex.org/W2972921407', 'https://openalex.org/W2972849140', 'https://openalex.org/W3024962219', 'https://openalex.org/W2972403660', 'https://openalex.org/W2150769028', 'https://openalex.org/W2890964092', 'https://openalex.org/W2752796333', 'https://openalex.org/W2527729766', 'https://openalex.org/W1635512741', 'https://openalex.org/W2963799213', 'https://openalex.org/W1731081199', 'https://openalex.org/W3008391559', 'https://openalex.org/W38194800', 'https://openalex.org/W2526425061', 'https://openalex.org/W2998572311', 'https://openalex.org/W3030987249']",2021-05-13
https://openalex.org/W4210560636,https://doi.org/10.1109/access.2022.3147670,NSVQ: Noise Substitution in Vector Quantization for Machine Learning,"Machine learning algorithms have been shown to be highly effective in solving optimization problems in a wide range of applications. Such algorithms typically use gradient descent with backpropagation and the chain rule. Hence, the backpropagation fails if intermediate gradients are zero for some functions in the computational graph, because it causes the gradients to collapse when multiplying with zero. Vector quantization is one of those challenging functions for machine learning algorithms, since it is a piece-wise constant function and its gradient is zero almost everywhere. A typical solution is to apply the straight through estimator which simply copies the gradients over the vector quantization function in the backpropagation. Other solutions are based on smooth or stochastic approximation. This study proposes a vector quantization technique called NSVQ, which approximates the vector quantization behavior by substituting a multiplicative noise so that it can be used for machine learning problems. Specifically, the vector quantization error is replaced by product of the original error and a normalized noise vector, the samples of which are drawn from a zero-mean, unit-variance normal distribution. We test our proposed NSVQ in three scenarios with various types of applications. Based on the experiments, the proposed NSVQ achieves more accuracy and faster convergence in comparison to the straight through estimator, exponential moving averages, and the MiniBatchKmeans approaches.","['https://openalex.org/W3135028703', 'https://openalex.org/W2968923792', 'https://openalex.org/W2963341071', 'https://openalex.org/W2160815625', 'https://openalex.org/W2112739286', 'https://openalex.org/W6679436768', 'https://openalex.org/W2105482032', 'https://openalex.org/W2097117768', 'https://openalex.org/W6684191040', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963446712', 'https://openalex.org/W1634005169', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W2935711438', 'https://openalex.org/W3215615641', 'https://openalex.org/W4200219715', 'https://openalex.org/W3197590976', 'https://openalex.org/W2972849140', 'https://openalex.org/W3096524539', 'https://openalex.org/W6776218486', 'https://openalex.org/W6753540710', 'https://openalex.org/W2972374322', 'https://openalex.org/W2982602185', 'https://openalex.org/W6690026940', 'https://openalex.org/W6727208969', 'https://openalex.org/W3100270690', 'https://openalex.org/W3094917204', 'https://openalex.org/W3004061291', 'https://openalex.org/W6741057705', 'https://openalex.org/W6754241195', 'https://openalex.org/W2889119508', 'https://openalex.org/W2999803881', 'https://openalex.org/W6734035190', 'https://openalex.org/W2119717200', 'https://openalex.org/W2552465432', 'https://openalex.org/W2972875719', 'https://openalex.org/W6694251005', 'https://openalex.org/W6639703010', 'https://openalex.org/W6767283756', 'https://openalex.org/W3192421036', 'https://openalex.org/W6628950865', 'https://openalex.org/W6753018729', 'https://openalex.org/W4244017338', 'https://openalex.org/W1494198834', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W2888465851', 'https://openalex.org/W2616492649', 'https://openalex.org/W2242818861', 'https://openalex.org/W1481935768', 'https://openalex.org/W2524428287', 'https://openalex.org/W4287802874', 'https://openalex.org/W4294567867', 'https://openalex.org/W4297659253']",2022-01-01
https://openalex.org/W3163338468,https://doi.org/10.1109/icassp39728.2021.9414499,End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE,"Explicit duration modeling is a key to achieving robust and efficient alignment in text-to-speech synthesis (TTS). We propose a new TTS framework using explicit duration modeling that incorporates duration as a discrete latent variable to TTS and enables joint optimization of whole modules from scratch. We formulate our method based on conditional VQ-VAE to handle discrete duration in a variational autoencoder and provide a theoretical explanation to justify our method. In our framework, a connectionist temporal classification (CTC) -based force aligner acts as the approximate posterior, and text-to-duration works as the prior in the variational autoencoder. We evaluated our proposed method with a listening test and compared it with other TTS methods based on soft-attention or explicit duration modeling. The results showed that our systems rated between soft-attention-based methods (Transformer-TTS, Tacotron2) and explicit duration modeling-based methods (Fastspeech).","['https://openalex.org/W3016160783', 'https://openalex.org/W6631190155', 'https://openalex.org/W3096442195', 'https://openalex.org/W6778823374', 'https://openalex.org/W6640963894', 'https://openalex.org/W2752796333', 'https://openalex.org/W6777694618', 'https://openalex.org/W175280642', 'https://openalex.org/W6753540710', 'https://openalex.org/W6773153034', 'https://openalex.org/W6777028661', 'https://openalex.org/W2973158936', 'https://openalex.org/W6623517193', 'https://openalex.org/W2127141656', 'https://openalex.org/W6679434410', 'https://openalex.org/W3015922793', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972702018', 'https://openalex.org/W6763832098', 'https://openalex.org/W2972746749', 'https://openalex.org/W2903739847', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964243274', 'https://openalex.org/W2935711438', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972849140', 'https://openalex.org/W6769328215', 'https://openalex.org/W2972374322', 'https://openalex.org/W6784520556', 'https://openalex.org/W3094917204', 'https://openalex.org/W2982602185', 'https://openalex.org/W3033411150', 'https://openalex.org/W2884607399', 'https://openalex.org/W2971753973', 'https://openalex.org/W2964121744', 'https://openalex.org/W3163338468', 'https://openalex.org/W3130016944', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015440759', 'https://openalex.org/W2133564696', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963799213', 'https://openalex.org/W3024605872', 'https://openalex.org/W3025793647', 'https://openalex.org/W1959608418', 'https://openalex.org/W854541894', 'https://openalex.org/W2519091744', 'https://openalex.org/W2970730223', 'https://openalex.org/W3026874504']",2021-05-13
https://openalex.org/W3160844600,https://doi.org/10.1109/icassp39728.2021.9414720,Improving Naturalness and Controllability of Sequence-to-Sequence Speech Synthesis by Learning Local Prosody Representations,"State-of-the-art neural text-to-speech (TTS) networks are trained with a large amount of speech data, which significantly improves the quality of synthetic speech compared with traditional approaches. However, the prosody and controllability of the generated speech is still insufficient, especially in tonal languages. Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence or words. In this study, we extended Tacotron2 with a pitch prediction task to capture discrete pitch-related representations. Specifically, the learned pitch-related suprasegmental information is fed simultaneously with traditional character features into the decoder to generate final Mel spectrogram. Experiments show that the proposed method can improve the quality of the generated speech (mean opinion score of 4.37 vs. 4.22). Moreover, we demonstrated that we can easily achieve word-level pitch control during generation by changing local pitch-related representations before passing them to the decoder network.","['https://openalex.org/W6762643587', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W2962691331', 'https://openalex.org/W3097892637', 'https://openalex.org/W2964138190', 'https://openalex.org/W3016021263', 'https://openalex.org/W2885800352', 'https://openalex.org/W2963091184', 'https://openalex.org/W3016007107', 'https://openalex.org/W2963945466', 'https://openalex.org/W2935711438', 'https://openalex.org/W3015440759', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972849140', 'https://openalex.org/W6736996214', 'https://openalex.org/W2964243274', 'https://openalex.org/W3094917204', 'https://openalex.org/W3015419784', 'https://openalex.org/W3010916717', 'https://openalex.org/W6690026940', 'https://openalex.org/W2091425152', 'https://openalex.org/W2952269766', 'https://openalex.org/W2963272440', 'https://openalex.org/W2963609956', 'https://openalex.org/W4295731579', 'https://openalex.org/W2963927338', 'https://openalex.org/W3086296097', 'https://openalex.org/W2242818861', 'https://openalex.org/W2794490148', 'https://openalex.org/W2945544731', 'https://openalex.org/W2519091744', 'https://openalex.org/W3125709657']",2021-05-13
https://openalex.org/W3196467321,https://doi.org/10.21437/interspeech.2021-852,TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions,,"['https://openalex.org/W2954882393', 'https://openalex.org/W2963975282', 'https://openalex.org/W3095389792', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963945466', 'https://openalex.org/W2964243274', 'https://openalex.org/W3094917204', 'https://openalex.org/W2972574864', 'https://openalex.org/W1995374299', 'https://openalex.org/W2970730223', 'https://openalex.org/W2144994235', 'https://openalex.org/W2963091184', 'https://openalex.org/W3095459301', 'https://openalex.org/W2963609956']",2021-08-27
https://openalex.org/W4392903361,https://doi.org/10.1109/icassp48485.2024.10446352,Stylespeech: Self-Supervised Style Enhancing with VQ-VAE-Based Pre-Training for Expressive Audiobook Speech Synthesis,"The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W6778823374', 'https://openalex.org/W6750489868', 'https://openalex.org/W4225326644', 'https://openalex.org/W6749555683', 'https://openalex.org/W4319586244', 'https://openalex.org/W2407139314', 'https://openalex.org/W2885800352', 'https://openalex.org/W6751878434', 'https://openalex.org/W3163339651', 'https://openalex.org/W4226230180', 'https://openalex.org/W4224928640', 'https://openalex.org/W4283689139', 'https://openalex.org/W2752796333', 'https://openalex.org/W6774314701', 'https://openalex.org/W3171153522', 'https://openalex.org/W2896457183', 'https://openalex.org/W3094917204', 'https://openalex.org/W3160584619', 'https://openalex.org/W3035323028', 'https://openalex.org/W2194775991', 'https://openalex.org/W6783867762', 'https://openalex.org/W4391156274', 'https://openalex.org/W4365799947', 'https://openalex.org/W2963799213', 'https://openalex.org/W3092028330']",2024-03-18
https://openalex.org/W3200243248,https://doi.org/10.5715/jnlp.28.881,When Creative AI Meets Conversational AI,,"['https://openalex.org/W2964110616', 'https://openalex.org/W3121826319', 'https://openalex.org/W3129050100', 'https://openalex.org/W6790703111', 'https://openalex.org/W3035316078', 'https://openalex.org/W3097514409', 'https://openalex.org/W3021182036', 'https://openalex.org/W3162723898', 'https://openalex.org/W6762931180', 'https://openalex.org/W6739901393', 'https://openalex.org/W7066234126', 'https://openalex.org/W3043786446', 'https://openalex.org/W2908412680', 'https://openalex.org/W3094917204', 'https://openalex.org/W2998563994', 'https://openalex.org/W2809621972', 'https://openalex.org/W2966792645', 'https://openalex.org/W3035430139', 'https://openalex.org/W3132890542', 'https://openalex.org/W2947590261', 'https://openalex.org/W2963403868', 'https://openalex.org/W2748262728']",2021-01-01
https://openalex.org/W3097112431,https://doi.org/10.1109/slt48900.2021.9383526,Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech,"Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.","['https://openalex.org/W2972366998', 'https://openalex.org/W2517513811', 'https://openalex.org/W6684200426', 'https://openalex.org/W6683855838', 'https://openalex.org/W2899361462', 'https://openalex.org/W6712034360', 'https://openalex.org/W2511640485', 'https://openalex.org/W2148846882', 'https://openalex.org/W2040587156', 'https://openalex.org/W2889544410', 'https://openalex.org/W3094917204', 'https://openalex.org/W6680071364', 'https://openalex.org/W6602155451', 'https://openalex.org/W2471520273', 'https://openalex.org/W3014201970', 'https://openalex.org/W2049686551', 'https://openalex.org/W6604530167', 'https://openalex.org/W2747914378', 'https://openalex.org/W2963522141', 'https://openalex.org/W2785978752', 'https://openalex.org/W2937829825', 'https://openalex.org/W6753049143', 'https://openalex.org/W6634507583', 'https://openalex.org/W2793479148', 'https://openalex.org/W3008297462', 'https://openalex.org/W6711854987', 'https://openalex.org/W6746801104', 'https://openalex.org/W2532494225', 'https://openalex.org/W2330979245', 'https://openalex.org/W2963539064', 'https://openalex.org/W6943714206', 'https://openalex.org/W7008786231', 'https://openalex.org/W2982602185', 'https://openalex.org/W6779669310', 'https://openalex.org/W2972951102', 'https://openalex.org/W2911340057', 'https://openalex.org/W3012498027', 'https://openalex.org/W2887511658', 'https://openalex.org/W2666408839', 'https://openalex.org/W2984809863', 'https://openalex.org/W2973138167', 'https://openalex.org/W2938833595', 'https://openalex.org/W3098557217', 'https://openalex.org/W2120605154', 'https://openalex.org/W2105160541', 'https://openalex.org/W2785608393', 'https://openalex.org/W2807668517', 'https://openalex.org/W2889064624', 'https://openalex.org/W2941094131', 'https://openalex.org/W2086796102', 'https://openalex.org/W3025182306', 'https://openalex.org/W3016151052', 'https://openalex.org/W3014255632', 'https://openalex.org/W6781784828', 'https://openalex.org/W6782294706', 'https://openalex.org/W2972699445', 'https://openalex.org/W3012970712', 'https://openalex.org/W3025680351', 'https://openalex.org/W2963252329', 'https://openalex.org/W2903365642', 'https://openalex.org/W3012404734', 'https://openalex.org/W3107061944', 'https://openalex.org/W2962896155', 'https://openalex.org/W2883743124', 'https://openalex.org/W3015805741', 'https://openalex.org/W2973135352', 'https://openalex.org/W2396025094', 'https://openalex.org/W2887894196', 'https://openalex.org/W52175521', 'https://openalex.org/W113106864', 'https://openalex.org/W2395980997', 'https://openalex.org/W3025878903', 'https://openalex.org/W3025044797', 'https://openalex.org/W2080119116', 'https://openalex.org/W2162295204', 'https://openalex.org/W1581458799', 'https://openalex.org/W3080698515', 'https://openalex.org/W3089767655', 'https://openalex.org/W2137376927', 'https://openalex.org/W3047107405', 'https://openalex.org/W2161736993', 'https://openalex.org/W2774848319', 'https://openalex.org/W3047769339', 'https://openalex.org/W2939189430', 'https://openalex.org/W2608338293', 'https://openalex.org/W2943230409', 'https://openalex.org/W2810914326']",2021-01-19
https://openalex.org/W3094421664,https://doi.org/10.48550/arxiv.2010.10727,Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm,"We present a new approach to disentangle speaker voice and phone content by introducing new components to the VQ-VAE architecture for speech synthesis. The original VQ-VAE does not generalize well to unseen speakers or content. To alleviate this problem, we have incorporated a speaker encoder and speaker VQ codebook that learns global speaker characteristics entirely separate from the existing sub-phone codebooks. We also compare two training methods: self-supervised with global conditions and semi-supervised with speaker labels. Adding a speaker VQ component improves objective measures of speech synthesis quality (estimated MOS, speaker similarity, ASR-based intelligibility) and provides learned representations that are meaningful. Our speaker VQ codebook indices can be used in a simple speaker diarization task and perform slightly better than an x-vector baseline. Additionally, phones can be recognized from sub-phone VQ codebook indices in our semi-supervised VQ-VAE better than self-supervised with global conditions.","['https://openalex.org/W1731081199', 'https://openalex.org/W3008391559', 'https://openalex.org/W38194800', 'https://openalex.org/W2527729766', 'https://openalex.org/W2150769028', 'https://openalex.org/W2526425061', 'https://openalex.org/W2972921407', 'https://openalex.org/W2890964092', 'https://openalex.org/W1635512741', 'https://openalex.org/W2963799213', 'https://openalex.org/W3037217258', 'https://openalex.org/W3094917204', 'https://openalex.org/W2972849140', 'https://openalex.org/W3024962219', 'https://openalex.org/W2889418727', 'https://openalex.org/W2963371159', 'https://openalex.org/W3024752295', 'https://openalex.org/W3030987249']",2020-10-21
https://openalex.org/W4402081904,https://doi.org/10.1007/978-3-031-70566-3_13,Generating High-Quality F0 Embeddings Using the Vector-Quantized Variational Autoencoder,,"['https://openalex.org/W2115098197', 'https://openalex.org/W3198217962', 'https://openalex.org/W4385574033', 'https://openalex.org/W2130086727', 'https://openalex.org/W3140429000', 'https://openalex.org/W3096323553', 'https://openalex.org/W2796738181', 'https://openalex.org/W4323896824', 'https://openalex.org/W2972359262', 'https://openalex.org/W3094917204']",2024-01-01
https://openalex.org/W4413221907,https://doi.org/10.1145/3749644,Enhanced Prosody Modeling and Character Voice Controlling for Audiobook Speech Synthesis,"Conventional speech synthesis techniques have made significant strides towards achieving human-like performance. However, the domain of audiobook speech synthesis still presents notable challenges. On one hand, the speech in audiobooks exhibits rich prosodic expressiveness, posing substantial difficulties in prosody modeling. On the other hand, the reader of audiobooks uses different voices to perform dialogues of different characters, which has been inadequately explored in existing speech synthesis methods. To address the first challenge, we integrate discourse-scale prosody modeling into the conventional autoencoder-based framework and introduce generative adversarial networks (GANs) for phoneme-level prosody code prediction. Regarding the second challenge, we further explore a character voice encoder based on the pretrained speaker verification model, integrating it into our proposed method. Experimental results validate that the proposed method enhances the prosodic expressiveness of synthesized audiobook speech. Moreover, it demonstrates the capacity to produce distinctive voices for different audiobook characters without compromising the naturalness of the synthesized speech.","['https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W3161296985', 'https://openalex.org/W4385823163', 'https://openalex.org/W4391020683', 'https://openalex.org/W3161113899', 'https://openalex.org/W2963341956', 'https://openalex.org/W3161732385', 'https://openalex.org/W3097795905', 'https://openalex.org/W3081488690', 'https://openalex.org/W3163339651', 'https://openalex.org/W3193700177', 'https://openalex.org/W3196027980', 'https://openalex.org/W4372341043', 'https://openalex.org/W4385329631', 'https://openalex.org/W4385993905', 'https://openalex.org/W4391021635', 'https://openalex.org/W4402112210', 'https://openalex.org/W4224928640', 'https://openalex.org/W3198123658', 'https://openalex.org/W4375869257', 'https://openalex.org/W4398152753', 'https://openalex.org/W4385822787', 'https://openalex.org/W4392903591', 'https://openalex.org/W3216941316', 'https://openalex.org/W3015338123', 'https://openalex.org/W4372346850', 'https://openalex.org/W2972951102', 'https://openalex.org/W4375868902', 'https://openalex.org/W3096086473', 'https://openalex.org/W3024869864', 'https://openalex.org/W2890964092', 'https://openalex.org/W3162746464', 'https://openalex.org/W3150572638', 'https://openalex.org/W2962691331', 'https://openalex.org/W4392904630', 'https://openalex.org/W3158374895', 'https://openalex.org/W3151450932', 'https://openalex.org/W3015440759', 'https://openalex.org/W3094917204', 'https://openalex.org/W4396542467', 'https://openalex.org/W2904459034', 'https://openalex.org/W2885800352', 'https://openalex.org/W3095545636', 'https://openalex.org/W4319586596', 'https://openalex.org/W4296068776', 'https://openalex.org/W3097777922', 'https://openalex.org/W2593414223', 'https://openalex.org/W4285218396', 'https://openalex.org/W4395961599', 'https://openalex.org/W2471520273', 'https://openalex.org/W2808631503', 'https://openalex.org/W4406794071', 'https://openalex.org/W4402112010']",2025-08-11
https://openalex.org/W4392903887,https://doi.org/10.1109/icassp48485.2024.10445966,Srcodec: Split-Residual Vector Quantization for Neural Speech Codec,"End-to-end neural speech coding achieves state-of-the-art performance by using residual vector quantization. However, it is a challenge to quantize the latent variables with as few bits as possible. In this paper, we propose SRCodec, a neural speech codec that relies on a fully convolutional encoder/decoder network with specifically proposed split-residual vector quantization. In particular, it divides the latent representation into two parts with the same dimensions. We utilize two different quantizers to quantize the low-dimensional features and the residual between the low- and high-dimensional features. Meanwhile, we propose a dual attention module in split-residual vector quantization to improve information sharing along both dimensions. Both subjective and objective evaluations demonstrate that the effectiveness of our proposed method can achieve a higher quality of reconstructed speech at 0.95 kbps than Lyra-v1 at 3 kbps and Encodec at 3 kbps.","['https://openalex.org/W2020883660', 'https://openalex.org/W1591492847', 'https://openalex.org/W2151626637', 'https://openalex.org/W2775336875', 'https://openalex.org/W3163662330', 'https://openalex.org/W4385822716', 'https://openalex.org/W3215615641', 'https://openalex.org/W4377231659', 'https://openalex.org/W4307323391', 'https://openalex.org/W4375869380', 'https://openalex.org/W2752796333', 'https://openalex.org/W2079337129', 'https://openalex.org/W2124509324', 'https://openalex.org/W2946822178', 'https://openalex.org/W3161744562', 'https://openalex.org/W4372268681', 'https://openalex.org/W6851660269', 'https://openalex.org/W3140429000', 'https://openalex.org/W2752782242', 'https://openalex.org/W3037038648', 'https://openalex.org/W3160077247', 'https://openalex.org/W2067295501', 'https://openalex.org/W6639363673', 'https://openalex.org/W4212774754', 'https://openalex.org/W4205788663', 'https://openalex.org/W4367190594', 'https://openalex.org/W2998572311']",2024-03-18
https://openalex.org/W4392903975,https://doi.org/10.1109/icassp48485.2024.10448332,NOLACE: Improving Low-Complexity Speech Codec Enhancement Through Adaptive Temporal Shaping,"Speech codec enhancement methods are designed to remove distortions added by speech codecs. While classical methods are very low in complexity and add zero delay, their effectiveness is rather limited. Compared to that, DNN-based methods deliver higher quality but they are typically high in complexity and/or require delay. The recently proposed Linear Adaptive Coding Enhancer (LACE) addresses this problem by combining DNNs with classical long-term/short-term post-filtering resulting in a causal low-complexity model. A short-coming of the LACE model is, however, that quality quickly saturates when the model size is scaled up. To mitigate this problem, we propose a novel adatpive temporal shaping module that adds high temporal resolution to the LACE model resulting in the Non-Linear Adaptive Coding Enhancer (NoLACE). We adapt NoLACE to enhance the Opus codec and show that NoLACE significantly outperforms both the Opus baseline and an enlarged LACE model at 6, 9 and 12 kb/s. We also show that LACE and NoLACE are well-behaved when used with an ASR system.","['https://openalex.org/W2131094339', 'https://openalex.org/W2809824582', 'https://openalex.org/W3096468295', 'https://openalex.org/W4226360164', 'https://openalex.org/W3016057201', 'https://openalex.org/W4225860133', 'https://openalex.org/W4386764386', 'https://openalex.org/W2972519044', 'https://openalex.org/W3163662330', 'https://openalex.org/W3215615641', 'https://openalex.org/W4284972581', 'https://openalex.org/W4375869380', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W6779192484', 'https://openalex.org/W6778625279', 'https://openalex.org/W2895654193', 'https://openalex.org/W6777664437', 'https://openalex.org/W6777781272', 'https://openalex.org/W2750167318', 'https://openalex.org/W3092791109', 'https://openalex.org/W3198020407', 'https://openalex.org/W3197273793', 'https://openalex.org/W4375868823', 'https://openalex.org/W2593414223', 'https://openalex.org/W3167533889', 'https://openalex.org/W1494198834']",2024-03-18
https://openalex.org/W4386764386,https://doi.org/10.1109/waspaa58266.2023.10248150,"Lace: A Light-Weight, Causal Model for Enhancing Coded Speech Through Adaptive Convolutions","Classical speech coding uses low-complexity postfilters with zero lookahead to enhance the quality of coded speech, but their effectiveness is limited by their simplicity. Deep Neural Networks (DNNs) can be much more effective, but require high complexity and model size, or added delay. We propose a DNN model that generates classical filter kernels on a per-frame basis with a model of just 300 K parameters and 100 MFLOPS complexity, which is a practical complexity for desktop or mobile device CPUs. The lack of added delay allows it to be integrated into the Opus codec, and we demonstrate that it enables effective wideband encoding for bitrates down to 6 kb/s.","['https://openalex.org/W4284972581', 'https://openalex.org/W3215615641', 'https://openalex.org/W6695676441', 'https://openalex.org/W4375869380', 'https://openalex.org/W3163662330', 'https://openalex.org/W2972519044', 'https://openalex.org/W2809824582', 'https://openalex.org/W2131094339', 'https://openalex.org/W6778625279', 'https://openalex.org/W6779192484', 'https://openalex.org/W3028785944', 'https://openalex.org/W2895654193', 'https://openalex.org/W3198020407', 'https://openalex.org/W3092791109', 'https://openalex.org/W6777781272', 'https://openalex.org/W2750167318', 'https://openalex.org/W6778672582', 'https://openalex.org/W6634531052', 'https://openalex.org/W6846849147', 'https://openalex.org/W6639363673', 'https://openalex.org/W4226360164', 'https://openalex.org/W6762390287', 'https://openalex.org/W4225860133', 'https://openalex.org/W3016057201', 'https://openalex.org/W4310630515', 'https://openalex.org/W3030374183', 'https://openalex.org/W1973746598', 'https://openalex.org/W4205788663', 'https://openalex.org/W2284050935', 'https://openalex.org/W3032479101', 'https://openalex.org/W3028954259', 'https://openalex.org/W3096468295', 'https://openalex.org/W3032558098']",2023-09-15
https://openalex.org/W4392903089,https://doi.org/10.1109/icassp48485.2024.10447744,SuperCodec: A Neural Speech Codec with Selective Back-Projection Network,"Neural speech coding is a rapidly developing topic, where state-of-the-art approaches now exhibit superior compression performance than conventional methods. Despite significant progress, existing methods still have limitations in preserving and reconstructing fine details for optimal reconstruction, especially at low bitrates. In this study, we introduce SuperCodec, a neural speech codec that achieves state-of-the-art performance at low bitrates. It employs a novel back projection method with selective feature fusion for augmented representation. Specifically, we propose to use Selective Up-sampling Back Projection (SUBP) and Selective Down-sampling Back Projection (SDBP) modules to replace the standard up- and down-sampling layers at the encoder and decoder, respectively. Experimental results show that our method outperforms the existing neural speech codecs operating at various bitrates. Specifically, our proposed method can achieve higher quality reconstructed speech at 1 kbps than Lyra V2 at 3.2 kbps and Encodec at 6 kbps.","['https://openalex.org/W6639363673', 'https://openalex.org/W1591492847', 'https://openalex.org/W2775336875', 'https://openalex.org/W2972519044', 'https://openalex.org/W3163662330', 'https://openalex.org/W3188073270', 'https://openalex.org/W4385822716', 'https://openalex.org/W3215615641', 'https://openalex.org/W4284972581', 'https://openalex.org/W4225287045', 'https://openalex.org/W4307323391', 'https://openalex.org/W4375869380', 'https://openalex.org/W4372259964', 'https://openalex.org/W2752796333', 'https://openalex.org/W6851660269', 'https://openalex.org/W2922509574', 'https://openalex.org/W4322102269', 'https://openalex.org/W6772349387', 'https://openalex.org/W2067295501', 'https://openalex.org/W3037038648', 'https://openalex.org/W3160077247', 'https://openalex.org/W4205788663', 'https://openalex.org/W4367190594', 'https://openalex.org/W2963799213', 'https://openalex.org/W2998572311']",2024-03-18
https://openalex.org/W4407949398,https://doi.org/10.1109/fie61694.2024.10893516,WIP: Building a Research Experience for Undergraduates in Quantum Machine Learning,,"['https://openalex.org/W3205963487', 'https://openalex.org/W3093163990', 'https://openalex.org/W2900822734', 'https://openalex.org/W3107965876', 'https://openalex.org/W6747870088', 'https://openalex.org/W6775250783', 'https://openalex.org/W2550492386', 'https://openalex.org/W4366449614', 'https://openalex.org/W2781738013', 'https://openalex.org/W2028406173', 'https://openalex.org/W1579786114', 'https://openalex.org/W1503961870', 'https://openalex.org/W4399423178', 'https://openalex.org/W1927793765', 'https://openalex.org/W4372266899', 'https://openalex.org/W2105946656', 'https://openalex.org/W4313018127', 'https://openalex.org/W3197634113', 'https://openalex.org/W4237378407', 'https://openalex.org/W4313170582', 'https://openalex.org/W2022554507', 'https://openalex.org/W2148371116', 'https://openalex.org/W2160566140', 'https://openalex.org/W4375869380', 'https://openalex.org/W4200442456', 'https://openalex.org/W4313159452', 'https://openalex.org/W4280626784', 'https://openalex.org/W4386590519', 'https://openalex.org/W4206973626', 'https://openalex.org/W4396853237', 'https://openalex.org/W4362694707', 'https://openalex.org/W4404037719', 'https://openalex.org/W6871158339', 'https://openalex.org/W4405522835', 'https://openalex.org/W3086716958']",2024-10-13
https://openalex.org/W4411232650,https://doi.org/10.1109/taslpro.2025.3579310,ERVQ: Enhanced Residual Vector Quantization With Intra-and-Inter-Codebook Optimization for Neural Audio Codecs,,"['https://openalex.org/W6633114069', 'https://openalex.org/W2159644785', 'https://openalex.org/W2168098717', 'https://openalex.org/W4381786045', 'https://openalex.org/W4406417959', 'https://openalex.org/W6851724922', 'https://openalex.org/W4387595589', 'https://openalex.org/W6862144568', 'https://openalex.org/W2752796333', 'https://openalex.org/W2935711438', 'https://openalex.org/W4200219715', 'https://openalex.org/W3197590976', 'https://openalex.org/W4224916783', 'https://openalex.org/W3215615641', 'https://openalex.org/W4372270198', 'https://openalex.org/W4375869380', 'https://openalex.org/W6873870642', 'https://openalex.org/W4403127074', 'https://openalex.org/W4392902628', 'https://openalex.org/W4402672068', 'https://openalex.org/W6776218486', 'https://openalex.org/W6853515095', 'https://openalex.org/W4307323391', 'https://openalex.org/W2133665775', 'https://openalex.org/W4399875170', 'https://openalex.org/W4392903389', 'https://openalex.org/W6762931180', 'https://openalex.org/W6802517614', 'https://openalex.org/W6852581948', 'https://openalex.org/W3186609711', 'https://openalex.org/W4385823076', 'https://openalex.org/W4284957875', 'https://openalex.org/W2963182577', 'https://openalex.org/W2775336875', 'https://openalex.org/W2972354707', 'https://openalex.org/W3161744562', 'https://openalex.org/W4225287045', 'https://openalex.org/W4390872598', 'https://openalex.org/W4386065342', 'https://openalex.org/W6936113694', 'https://openalex.org/W2972359262', 'https://openalex.org/W4205689591', 'https://openalex.org/W4296068763', 'https://openalex.org/W1728888090', 'https://openalex.org/W2067295501', 'https://openalex.org/W4283067311', 'https://openalex.org/W4385822407', 'https://openalex.org/W2962788625', 'https://openalex.org/W4406461271', 'https://openalex.org/W4225956675']",2025-01-01
https://openalex.org/W4403127074,https://doi.org/10.1109/iwaenc61483.2024.10694300,High-Fidelity Diffusion-Based Audio Codec,,"['https://openalex.org/W4284972581', 'https://openalex.org/W2935711438', 'https://openalex.org/W4307323391', 'https://openalex.org/W2775336875', 'https://openalex.org/W2963091184', 'https://openalex.org/W3163662330', 'https://openalex.org/W3215615641', 'https://openalex.org/W4386764371', 'https://openalex.org/W4372270198', 'https://openalex.org/W6852581948', 'https://openalex.org/W2785562966', 'https://openalex.org/W6679045638', 'https://openalex.org/W6765775151', 'https://openalex.org/W6779823529', 'https://openalex.org/W4281820413', 'https://openalex.org/W6786375611', 'https://openalex.org/W6782380777', 'https://openalex.org/W3144575004', 'https://openalex.org/W6796762324', 'https://openalex.org/W1901129140', 'https://openalex.org/W6634817459', 'https://openalex.org/W2760103357', 'https://openalex.org/W2143612128', 'https://openalex.org/W2178928294', 'https://openalex.org/W4375869380', 'https://openalex.org/W6853515095', 'https://openalex.org/W4392903887', 'https://openalex.org/W3084645150', 'https://openalex.org/W6766320909', 'https://openalex.org/W3037038648', 'https://openalex.org/W6744762798']",2024-09-09
https://openalex.org/W4408792343,https://doi.org/10.1109/tvt.2025.3547841,VoNTN: Real-Time and High-Quality Voice Services Over Narrowband Non-Terrestrial Networks,,"['https://openalex.org/W3179283102', 'https://openalex.org/W4390204450', 'https://openalex.org/W4390738482', 'https://openalex.org/W4312096437', 'https://openalex.org/W4388726103', 'https://openalex.org/W3128254715', 'https://openalex.org/W4385777644', 'https://openalex.org/W2973753807', 'https://openalex.org/W4389609958', 'https://openalex.org/W2125423668', 'https://openalex.org/W1548643265', 'https://openalex.org/W4313172087', 'https://openalex.org/W4375869380', 'https://openalex.org/W4284888249', 'https://openalex.org/W4285026506', 'https://openalex.org/W4392979815', 'https://openalex.org/W2632564668', 'https://openalex.org/W4285251554', 'https://openalex.org/W2963091184', 'https://openalex.org/W3209059054', 'https://openalex.org/W3019533076', 'https://openalex.org/W2105921478', 'https://openalex.org/W1552314771', 'https://openalex.org/W1728888090']",2025-03-24
https://openalex.org/W4410608878,https://doi.org/10.1007/978-3-031-88188-6_12,An Overview of AI Workload Optimization Techniques,,"['https://openalex.org/W4402718277', 'https://openalex.org/W4312122239', 'https://openalex.org/W4400329724', 'https://openalex.org/W3007848260', 'https://openalex.org/W6698039833', 'https://openalex.org/W3043273434', 'https://openalex.org/W4313433616', 'https://openalex.org/W3149731118', 'https://openalex.org/W2128022558', 'https://openalex.org/W4308090791', 'https://openalex.org/W4250981202', 'https://openalex.org/W2108157916', 'https://openalex.org/W2969766737', 'https://openalex.org/W2133156997', 'https://openalex.org/W4210258659', 'https://openalex.org/W3163275603', 'https://openalex.org/W3016163932', 'https://openalex.org/W3004127905', 'https://openalex.org/W2801748224', 'https://openalex.org/W3006732000', 'https://openalex.org/W6605361631', 'https://openalex.org/W3107241502', 'https://openalex.org/W4352977393', 'https://openalex.org/W3127355095', 'https://openalex.org/W4206336135', 'https://openalex.org/W4376130831', 'https://openalex.org/W6604447124', 'https://openalex.org/W4318603275', 'https://openalex.org/W2971002981', 'https://openalex.org/W4396686371', 'https://openalex.org/W2970959587', 'https://openalex.org/W4308090443', 'https://openalex.org/W4210357113', 'https://openalex.org/W3184299878', 'https://openalex.org/W3128096732', 'https://openalex.org/W3096230432', 'https://openalex.org/W3201352919', 'https://openalex.org/W3083136971', 'https://openalex.org/W4382203227', 'https://openalex.org/W3140854437', 'https://openalex.org/W4312790010', 'https://openalex.org/W3206210071', 'https://openalex.org/W6600763685', 'https://openalex.org/W3000315285', 'https://openalex.org/W3034536757', 'https://openalex.org/W3082020764', 'https://openalex.org/W4392265934', 'https://openalex.org/W6605551291', 'https://openalex.org/W6600728650', 'https://openalex.org/W6601630192', 'https://openalex.org/W3063051577', 'https://openalex.org/W6600424091', 'https://openalex.org/W3148573243', 'https://openalex.org/W2971860257', 'https://openalex.org/W3138303811', 'https://openalex.org/W2405578611', 'https://openalex.org/W3170887803', 'https://openalex.org/W6699519708', 'https://openalex.org/W6600213211', 'https://openalex.org/W2954996726', 'https://openalex.org/W2992308087', 'https://openalex.org/W6600281463', 'https://openalex.org/W3047806505', 'https://openalex.org/W3180181113', 'https://openalex.org/W2963545917', 'https://openalex.org/W3176923149', 'https://openalex.org/W3109835285', 'https://openalex.org/W4205952419', 'https://openalex.org/W4375869380', 'https://openalex.org/W6604190760', 'https://openalex.org/W2963711615', 'https://openalex.org/W2963940468', 'https://openalex.org/W4385484685', 'https://openalex.org/W2956015785', 'https://openalex.org/W3113682753', 'https://openalex.org/W3036586801', 'https://openalex.org/W6600302053', 'https://openalex.org/W3120991880', 'https://openalex.org/W2511770992', 'https://openalex.org/W6812577357', 'https://openalex.org/W6830698261', 'https://openalex.org/W6600135713', 'https://openalex.org/W3194730353', 'https://openalex.org/W6601144205', 'https://openalex.org/W2896158778', 'https://openalex.org/W3137147200', 'https://openalex.org/W3184606595', 'https://openalex.org/W6738979783', 'https://openalex.org/W2982479999', 'https://openalex.org/W6600062020', 'https://openalex.org/W4401726555', 'https://openalex.org/W6604472716', 'https://openalex.org/W3034368386', 'https://openalex.org/W3004127093', 'https://openalex.org/W6600778605', 'https://openalex.org/W2966284335', 'https://openalex.org/W4392205891', 'https://openalex.org/W4317727238', 'https://openalex.org/W3021654819', 'https://openalex.org/W3113149630', 'https://openalex.org/W6605396377', 'https://openalex.org/W3166319166', 'https://openalex.org/W6601700763', 'https://openalex.org/W4241316613', 'https://openalex.org/W4390873212', 'https://openalex.org/W3174529902', 'https://openalex.org/W6601241631', 'https://openalex.org/W2798725115', 'https://openalex.org/W4380874786', 'https://openalex.org/W3142170088', 'https://openalex.org/W2521267242', 'https://openalex.org/W3134447224', 'https://openalex.org/W4296126438', 'https://openalex.org/W3139657805', 'https://openalex.org/W4223904523', 'https://openalex.org/W4353091694', 'https://openalex.org/W2944614352', 'https://openalex.org/W4321636575', 'https://openalex.org/W1624756247', 'https://openalex.org/W1993005299', 'https://openalex.org/W2966231426', 'https://openalex.org/W3103594582', 'https://openalex.org/W3105888187', 'https://openalex.org/W3099432326', 'https://openalex.org/W3138154797', 'https://openalex.org/W3117961507', 'https://openalex.org/W3128451613']",2025-01-01
https://openalex.org/W4412798988,https://doi.org/10.1016/j.comnet.2025.111560,Performance evaluation of low-bitrate voice using spread spectrum techniques for satellite-based emergency communication,,"['https://openalex.org/W3120487332', 'https://openalex.org/W4406692749', 'https://openalex.org/W4400570481', 'https://openalex.org/W4406579668', 'https://openalex.org/W4402742528', 'https://openalex.org/W6868862106', 'https://openalex.org/W6637553805', 'https://openalex.org/W6873821451', 'https://openalex.org/W6870241108', 'https://openalex.org/W4401995251', 'https://openalex.org/W6870798867', 'https://openalex.org/W2918512233', 'https://openalex.org/W4400365747', 'https://openalex.org/W4394888157', 'https://openalex.org/W2068245410', 'https://openalex.org/W3089471195', 'https://openalex.org/W2798390329', 'https://openalex.org/W6860533218', 'https://openalex.org/W6679299603', 'https://openalex.org/W6855610131', 'https://openalex.org/W3022632306', 'https://openalex.org/W4388635302', 'https://openalex.org/W4375869380', 'https://openalex.org/W4391539913', 'https://openalex.org/W4392979815', 'https://openalex.org/W1966264494', 'https://openalex.org/W6870666441', 'https://openalex.org/W4402112436', 'https://openalex.org/W6681114804', 'https://openalex.org/W6761779080', 'https://openalex.org/W6800694852', 'https://openalex.org/W4389160192', 'https://openalex.org/W4402910007', 'https://openalex.org/W4366189228', 'https://openalex.org/W6846859373', 'https://openalex.org/W4386930030', 'https://openalex.org/W3012541894', 'https://openalex.org/W6633117090', 'https://openalex.org/W2091528887', 'https://openalex.org/W4308872147', 'https://openalex.org/W6662055143', 'https://openalex.org/W4243208617', 'https://openalex.org/W1809493103', 'https://openalex.org/W4244483815', 'https://openalex.org/W4401282119', 'https://openalex.org/W4411232650', 'https://openalex.org/W4400530980', 'https://openalex.org/W4399875170', 'https://openalex.org/W4308947955', 'https://openalex.org/W4240005120', 'https://openalex.org/W4403636076', 'https://openalex.org/W4404299109', 'https://openalex.org/W2131330825', 'https://openalex.org/W4390422058', 'https://openalex.org/W4255210669', 'https://openalex.org/W1552314771', 'https://openalex.org/W2940200110']",2025-07-28
https://openalex.org/W4404446528,https://doi.org/10.1007/978-981-96-0125-7_29,MSCACodec: A Low-Rate Neural Speech Codec With Multi-scale Residual Channel Attention,,"['https://openalex.org/W2775336875', 'https://openalex.org/W3163662330', 'https://openalex.org/W4385822716', 'https://openalex.org/W2935711438', 'https://openalex.org/W3215615641', 'https://openalex.org/W6739901393', 'https://openalex.org/W4375869380', 'https://openalex.org/W4381786045', 'https://openalex.org/W4385823076', 'https://openalex.org/W6601897980', 'https://openalex.org/W4221149546', 'https://openalex.org/W3094897602', 'https://openalex.org/W2962935966', 'https://openalex.org/W1494198834', 'https://openalex.org/W6602452458', 'https://openalex.org/W2299467264', 'https://openalex.org/W2067295501', 'https://openalex.org/W2121388655', 'https://openalex.org/W3037038648']",2024-11-12
https://openalex.org/W4405709688,https://doi.org/10.1109/iscslp63861.2024.10800135,A Differential Quantization Based End-to-End Neural Speech Codec,,"['https://openalex.org/W6639363673', 'https://openalex.org/W1481955708', 'https://openalex.org/W2097645910', 'https://openalex.org/W2151626637', 'https://openalex.org/W2064675550', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6852581948', 'https://openalex.org/W2752796333', 'https://openalex.org/W1849277567', 'https://openalex.org/W2124509324', 'https://openalex.org/W4375869380', 'https://openalex.org/W4385245566', 'https://openalex.org/W6782293762', 'https://openalex.org/W4392902628', 'https://openalex.org/W3180355996', 'https://openalex.org/W6783867762', 'https://openalex.org/W2963242190', 'https://openalex.org/W3198533616', 'https://openalex.org/W6636170946']",2024-11-07
https://openalex.org/W4405709717,https://doi.org/10.1109/iscslp63861.2024.10800720,A Dual-path Conformer-Based Network for Neural Speech Coding,,"['https://openalex.org/W6630236247', 'https://openalex.org/W6629271464', 'https://openalex.org/W3169418678', 'https://openalex.org/W2775336875', 'https://openalex.org/W3160652646', 'https://openalex.org/W3015780049', 'https://openalex.org/W3096468295', 'https://openalex.org/W2935711438', 'https://openalex.org/W3100270690', 'https://openalex.org/W3163662330', 'https://openalex.org/W6760712927', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W6853515095', 'https://openalex.org/W4375869380', 'https://openalex.org/W6852581948', 'https://openalex.org/W4225287045', 'https://openalex.org/W4377231659', 'https://openalex.org/W3097777922', 'https://openalex.org/W3015199127', 'https://openalex.org/W6638667902', 'https://openalex.org/W6640185926', 'https://openalex.org/W3120336970', 'https://openalex.org/W6802517614', 'https://openalex.org/W6783867762', 'https://openalex.org/W2593414223', 'https://openalex.org/W1728888090', 'https://openalex.org/W3161480375']",2024-11-07
https://openalex.org/W4408010809,https://doi.org/10.2139/ssrn.5151459,Performance Evaluation of Low-Bitrate Voice Using Spread Spectrum Techniques for Satellite-Based Emergency Communication,,"['https://openalex.org/W3120487332', 'https://openalex.org/W4406692749', 'https://openalex.org/W4400570481', 'https://openalex.org/W4402742528', 'https://openalex.org/W4399169295', 'https://openalex.org/W6637553805', 'https://openalex.org/W4404299109', 'https://openalex.org/W4401995251', 'https://openalex.org/W4400365747', 'https://openalex.org/W4394888157', 'https://openalex.org/W6667466450', 'https://openalex.org/W3089471195', 'https://openalex.org/W2798390329', 'https://openalex.org/W4390401813', 'https://openalex.org/W2131330825', 'https://openalex.org/W4388635302', 'https://openalex.org/W4360886119', 'https://openalex.org/W4391539913', 'https://openalex.org/W4392979815', 'https://openalex.org/W6641608186', 'https://openalex.org/W4402112436', 'https://openalex.org/W6681114804', 'https://openalex.org/W3196606841', 'https://openalex.org/W4220992882', 'https://openalex.org/W4389160192', 'https://openalex.org/W6638113460', 'https://openalex.org/W4386930030', 'https://openalex.org/W3012541894', 'https://openalex.org/W4308872147', 'https://openalex.org/W2045758595', 'https://openalex.org/W4375869380', 'https://openalex.org/W4401282119', 'https://openalex.org/W2145370087', 'https://openalex.org/W2918512233', 'https://openalex.org/W1966264494', 'https://openalex.org/W4403636076', 'https://openalex.org/W3022632306', 'https://openalex.org/W4399875170', 'https://openalex.org/W2940200110', 'https://openalex.org/W1691979803', 'https://openalex.org/W1809493103', 'https://openalex.org/W4366189228', 'https://openalex.org/W2068245410']",2025-01-01
https://openalex.org/W4408352558,https://doi.org/10.1109/icassp49660.2025.10889560,Attention Weighting and Conditional Entropy-driven Quantization Loss for Neural Audio Codecs,,"['https://openalex.org/W2130158387', 'https://openalex.org/W6633114069', 'https://openalex.org/W2168098717', 'https://openalex.org/W2159644785', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4372270198', 'https://openalex.org/W4392903089', 'https://openalex.org/W6853515095', 'https://openalex.org/W4392903389', 'https://openalex.org/W4375869380', 'https://openalex.org/W4392903887', 'https://openalex.org/W4386139314', 'https://openalex.org/W4386075800', 'https://openalex.org/W3214758449', 'https://openalex.org/W3110277971', 'https://openalex.org/W6739901393', 'https://openalex.org/W1995875735', 'https://openalex.org/W4402670057', 'https://openalex.org/W2972359262', 'https://openalex.org/W1567520911', 'https://openalex.org/W6750665317', 'https://openalex.org/W6777776875', 'https://openalex.org/W6744762798', 'https://openalex.org/W6948682473', 'https://openalex.org/W6639363673']",2025-03-12
https://openalex.org/W4409222133,https://doi.org/10.15622/ia.24.2.3,An AudioCodec Based on the Perceptual Equality between the Original and Restored Audio Signals,"A method for lossy audio data compression (AudioCodec) is presented. It allows for improving objective quality of the restored audio signal by 25% at a bitrate of 390 kbps and 55% at a bitrate of 64 kbps compared to the AAC MPEG-4 format. The proposed method of audio data compression is based on an advanced theory of lossy audio data compression (TLAC), which is also introduced in the article. The improvement in the objective quality of the reconstructed audio signal (according to the standardized PEAQ measure) is achieved because the TLAC overcomes issues in modern lossy audio data compression methods related to the use of psychoacoustic principles of human sound perception, including after overcoming the ""psychoacoustic compression limit"" of the audio signal (i.e. the moment in perceptual coding when the available bit budget is insufficient to encode all spectral components with the accuracy required from a psychoacoustic perspective). This allows for achieving perceptual equality between the original and reconstructed audio signals. As an analysis of the state of the art, solutions for both lossless and lossy audio data compression, as well as those using artificial intelligence, are considered. In all modern lossy audio data compression methods, the procedure for selecting the spectral components to be preserved, as well as the permissible quantization error, is carried out through a series of highly complex procedures collectively referred to as the ""psychoacoustic model of the lossy audio compression method"". In a strict sense, perceptual equality between the spectra of the original and restored signals has not been proven by any research group and, therefore, cannot be guaranteed by them. Independent experts regularly publish tests demonstrating that modern audio codecs have issues with certain audio signals. The article proposes an AudioCodec based on the perceptual equality between the original and restored audio signals, which is based on the new ideas of the theory of lossy audio compression (TLAC). These ideas guarantee the achievement of perceptual equality between the original and restored audio signals at different bitrates, therefore, the AudioCodec built on its basis is free from the above-mentioned issues and, as a result, significantly outperforms modern AudioCodecs in terms of the objective quality of the restored audio signal, as measured by PEAQ.","['https://openalex.org/W2585155455', 'https://openalex.org/W4375869380', 'https://openalex.org/W4392903897', 'https://openalex.org/W4221152438', 'https://openalex.org/W4297841815', 'https://openalex.org/W2963091184', 'https://openalex.org/W4225290925', 'https://openalex.org/W4372190822', 'https://openalex.org/W3215615641', 'https://openalex.org/W4392903389']",2025-04-01
https://openalex.org/W4413125390,https://doi.org/10.1109/lsp.2025.3596826,Token-Prediction-Based Post-Processing for Low-Bitrate Speech Coding,,"['https://openalex.org/W4399875170', 'https://openalex.org/W4375869380', 'https://openalex.org/W4403918744', 'https://openalex.org/W2126441393', 'https://openalex.org/W2131094339', 'https://openalex.org/W3097945073', 'https://openalex.org/W4395447416', 'https://openalex.org/W4410314146', 'https://openalex.org/W2809824582', 'https://openalex.org/W3016057201', 'https://openalex.org/W3015780049', 'https://openalex.org/W4225860133', 'https://openalex.org/W3188073270', 'https://openalex.org/W4408353418', 'https://openalex.org/W4252812408', 'https://openalex.org/W4406461725', 'https://openalex.org/W2972359262', 'https://openalex.org/W3196475561', 'https://openalex.org/W4225302959', 'https://openalex.org/W4225956675']",2025-01-01
https://openalex.org/W4415524614,https://doi.org/10.1109/mlsp62443.2025.11204264,State Prediction for Offline Reinforcement Learning via Sequence-to-Sequence Modeling,,"['https://openalex.org/W4392903533', 'https://openalex.org/W4392903325', 'https://openalex.org/W4404037589', 'https://openalex.org/W4387869694', 'https://openalex.org/W4404037585', 'https://openalex.org/W4404037473', 'https://openalex.org/W4404037281', 'https://openalex.org/W4404037515', 'https://openalex.org/W4385245566', 'https://openalex.org/W4375869380', 'https://openalex.org/W2964021598']",2025-08-31
https://openalex.org/W2117041980,https://doi.org/10.3115/1557690.1557736,Unsupervised learning of acoustic sub-word units,"Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.","['https://openalex.org/W4298166701', 'https://openalex.org/W3034729383', 'https://openalex.org/W2101536075', 'https://openalex.org/W1971081490', 'https://openalex.org/W1949782964', 'https://openalex.org/W1918599710']",2008-01-01
https://openalex.org/W2015876361,https://doi.org/10.1016/j.specom.2003.12.002,Automatic segmentation of continuous speech using minimum phase group delay functions,,"['https://openalex.org/W2121170145', 'https://openalex.org/W2011604126', 'https://openalex.org/W2167200294', 'https://openalex.org/W2121464381', 'https://openalex.org/W2182355246', 'https://openalex.org/W2179839788', 'https://openalex.org/W2152131029', 'https://openalex.org/W1965635292', 'https://openalex.org/W1996416483', 'https://openalex.org/W4230220154', 'https://openalex.org/W2192975138', 'https://openalex.org/W1949782964', 'https://openalex.org/W2108148744', 'https://openalex.org/W2172174902', 'https://openalex.org/W4285719527', 'https://openalex.org/W2074223539']",2003-12-30
https://openalex.org/W2167655920,https://doi.org/10.1109/89.985546,Automatic generation of subword units for speech recognition systems,"Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.","['https://openalex.org/W2128780426', 'https://openalex.org/W189838809', 'https://openalex.org/W2137919556', 'https://openalex.org/W2158598479', 'https://openalex.org/W1964917299', 'https://openalex.org/W1949782964', 'https://openalex.org/W1763005514', 'https://openalex.org/W17875541', 'https://openalex.org/W1865025591', 'https://openalex.org/W77847722', 'https://openalex.org/W2049633694', 'https://openalex.org/W2156530815', 'https://openalex.org/W2134237567', 'https://openalex.org/W2998215494', 'https://openalex.org/W2142401923', 'https://openalex.org/W1643320849', 'https://openalex.org/W1965201650', 'https://openalex.org/W2144639996', 'https://openalex.org/W1560013842', 'https://openalex.org/W2002089154', 'https://openalex.org/W4302339081', 'https://openalex.org/W2006969979', 'https://openalex.org/W2160645305']",2002-01-01
https://openalex.org/W2080723515,https://doi.org/10.1016/s0167-6393(99)00036-9,Maximum likelihood modelling of pronunciation variation,,"['https://openalex.org/W1987899306', 'https://openalex.org/W1976795180', 'https://openalex.org/W2137919556', 'https://openalex.org/W6629907700', 'https://openalex.org/W1932968309', 'https://openalex.org/W2123700259', 'https://openalex.org/W2157754466', 'https://openalex.org/W2158598479', 'https://openalex.org/W77847722', 'https://openalex.org/W2066452495', 'https://openalex.org/W2143980933', 'https://openalex.org/W2134383396', 'https://openalex.org/W1995872006', 'https://openalex.org/W2120234416', 'https://openalex.org/W2125142492', 'https://openalex.org/W1643320849', 'https://openalex.org/W2121155963', 'https://openalex.org/W2168513153', 'https://openalex.org/W1763005514', 'https://openalex.org/W2036102925', 'https://openalex.org/W1520302542', 'https://openalex.org/W2128780426', 'https://openalex.org/W189838809', 'https://openalex.org/W1949782964', 'https://openalex.org/W2162362731', 'https://openalex.org/W2160645305', 'https://openalex.org/W1501273589']",1999-11-01
https://openalex.org/W2909617156,https://doi.org/10.1016/j.specom.2019.01.003,Phoneme boundary detection from speech: A rule based approach,,"['https://openalex.org/W6718780072', 'https://openalex.org/W6712767976', 'https://openalex.org/W2135933868', 'https://openalex.org/W6674795352', 'https://openalex.org/W2086699924', 'https://openalex.org/W2075329984', 'https://openalex.org/W2054665642', 'https://openalex.org/W6676286652', 'https://openalex.org/W6638051570', 'https://openalex.org/W6601644675', 'https://openalex.org/W6605931127', 'https://openalex.org/W2174992124', 'https://openalex.org/W6671228137', 'https://openalex.org/W2091817122', 'https://openalex.org/W6738767006', 'https://openalex.org/W6640208575', 'https://openalex.org/W1501918702', 'https://openalex.org/W1659740212', 'https://openalex.org/W7027429494', 'https://openalex.org/W6677032769', 'https://openalex.org/W6604148789', 'https://openalex.org/W2168441989', 'https://openalex.org/W6677643112', 'https://openalex.org/W2192975138', 'https://openalex.org/W1525783482', 'https://openalex.org/W6638780785', 'https://openalex.org/W2077529289', 'https://openalex.org/W6713742277', 'https://openalex.org/W2084584260', 'https://openalex.org/W6634515307', 'https://openalex.org/W2059026503', 'https://openalex.org/W2127854316', 'https://openalex.org/W6679469922', 'https://openalex.org/W6628690291', 'https://openalex.org/W2315043753', 'https://openalex.org/W2179749845', 'https://openalex.org/W2033037901', 'https://openalex.org/W2147333232', 'https://openalex.org/W6677920165', 'https://openalex.org/W2165965469', 'https://openalex.org/W1965635292', 'https://openalex.org/W6677986390', 'https://openalex.org/W2045157414', 'https://openalex.org/W2112844139', 'https://openalex.org/W2102195139', 'https://openalex.org/W6677381445', 'https://openalex.org/W1986455287', 'https://openalex.org/W2145102879', 'https://openalex.org/W6682030320', 'https://openalex.org/W2106221537', 'https://openalex.org/W2034878035', 'https://openalex.org/W2089409887', 'https://openalex.org/W6713502316', 'https://openalex.org/W6712614097', 'https://openalex.org/W6682811720', 'https://openalex.org/W1964376316', 'https://openalex.org/W6640645036', 'https://openalex.org/W6606916877', 'https://openalex.org/W6675348038', 'https://openalex.org/W6640856679', 'https://openalex.org/W6643036409', 'https://openalex.org/W2169711598', 'https://openalex.org/W2514012605', 'https://openalex.org/W1975728937', 'https://openalex.org/W4230022764', 'https://openalex.org/W6640657561', 'https://openalex.org/W6679854563', 'https://openalex.org/W1967625181', 'https://openalex.org/W203145905', 'https://openalex.org/W2404169761', 'https://openalex.org/W2890915139', 'https://openalex.org/W2620638943', 'https://openalex.org/W2149016536', 'https://openalex.org/W3127686677', 'https://openalex.org/W2460352032', 'https://openalex.org/W1971081490', 'https://openalex.org/W2113587986', 'https://openalex.org/W168484277', 'https://openalex.org/W2914584698', 'https://openalex.org/W2119155265', 'https://openalex.org/W2130414229', 'https://openalex.org/W1544407768', 'https://openalex.org/W2612972698', 'https://openalex.org/W4210849719', 'https://openalex.org/W1949782964', 'https://openalex.org/W148046306', 'https://openalex.org/W1570629387', 'https://openalex.org/W2107615612', 'https://openalex.org/W3147165232', 'https://openalex.org/W2398444131', 'https://openalex.org/W2408737549', 'https://openalex.org/W1578585021', 'https://openalex.org/W2115459718', 'https://openalex.org/W2083904075', 'https://openalex.org/W1510007267', 'https://openalex.org/W2119315826', 'https://openalex.org/W2098363562', 'https://openalex.org/W4214644094', 'https://openalex.org/W1536972981', 'https://openalex.org/W1839659014', 'https://openalex.org/W1918599710', 'https://openalex.org/W3151288016', 'https://openalex.org/W2153904410', 'https://openalex.org/W2127141656', 'https://openalex.org/W1481105745', 'https://openalex.org/W3140706748', 'https://openalex.org/W1792616884', 'https://openalex.org/W40598367', 'https://openalex.org/W2132820034', 'https://openalex.org/W4249047487', 'https://openalex.org/W4205284213', 'https://openalex.org/W2101536075', 'https://openalex.org/W2915722758', 'https://openalex.org/W1959097584', 'https://openalex.org/W2399033287', 'https://openalex.org/W100975862', 'https://openalex.org/W1950396994', 'https://openalex.org/W2118774185']",2019-01-09
https://openalex.org/W3039958863,https://doi.org/10.1109/iccsea49143.2020.9132848,Speech Recognition to Build Context: A Survey,"In era Computer evolution many problems can be solved using computer vision and signal processing. These domains are typically Digitized in binary files like Images, Audio, and Videos. The translation, recognition and synthesis are required while understating the meaning of the binary content. The recognition process is also having many problems in case of audio processing. The missing context is the major reason in pattern-based matching. This is due to unclear or low-quality input, as well as training model on different frequencies but by using context some of the accuracy may improve. Context finding from binary files is a challenge as it works in temporal and space domain. Binary data like images contain special information, while audio files contain temporal information. Video files have both time and space domains. Updating context in the temporal domain, to find proper context from the audio corpus, speech recognition is applied. Over the time period, there are different models adapted like Hidden Markov Model (HMM), Rule Based models with fuzzy support, pattern-based models including machine learning techniques K-nearest neighbor, Support Vector Machine, also latest techniques like Artificial Neural Network (ANN). These technologies are typically included in Automatic Speech Recognition (ASR). ASR uses Language resources with any one of the above models. Here, an in-depth survey on ASR and available APIs. Technologies used to build APIs also discussed.","['https://openalex.org/W2171850596', 'https://openalex.org/W2007321142', 'https://openalex.org/W4236088841', 'https://openalex.org/W2165880886', 'https://openalex.org/W2073705768', 'https://openalex.org/W6636885848', 'https://openalex.org/W6648629197', 'https://openalex.org/W2019724452', 'https://openalex.org/W2064675550', 'https://openalex.org/W2096503735', 'https://openalex.org/W2962824709', 'https://openalex.org/W2104981903', 'https://openalex.org/W6681199919', 'https://openalex.org/W1949782964', 'https://openalex.org/W2142635246', 'https://openalex.org/W2147627917', 'https://openalex.org/W6630067450', 'https://openalex.org/W2132549764', 'https://openalex.org/W2063106010', 'https://openalex.org/W2160815625', 'https://openalex.org/W2589857635', 'https://openalex.org/W1993482042', 'https://openalex.org/W2184045248', 'https://openalex.org/W2258185185', 'https://openalex.org/W2116952749', 'https://openalex.org/W2990683251', 'https://openalex.org/W2734883232', 'https://openalex.org/W2143980933', 'https://openalex.org/W2981022124', 'https://openalex.org/W1503402065', 'https://openalex.org/W1012287703', 'https://openalex.org/W1637570796']",2020-03-01
https://openalex.org/W2125142492,https://doi.org/10.1109/icassp.1990.115888,Lexicon-building methods for an acoustic sub-word based speech recognizer,"The use of an acoustic subword unit (ASWU)-based speech recognition system for the recognition of isolated words is discussed. Some methods are proposed for generating the deterministic and the statistical types of word lexicon. It is shown that the use of a modified k-means algorithm on the likelihoods derived through the Viterbi algorithm provides the best deterministic-type of word lexicon. However, the ASWU-based speech recognizer leads to better performance with the statistical type of word lexicon than with the deterministic type. Improving the design of the word lexicon makes it possible to narrow the gap in the recognition performances of the whole word unit (WWU)-based and the ASWU-based speech recognizers considerably. Further improvements are expected by designing the word lexicon better.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2128780426', 'https://openalex.org/W2143980933', 'https://openalex.org/W1950396994', 'https://openalex.org/W2134383396', 'https://openalex.org/W2125838338', 'https://openalex.org/W2048648518', 'https://openalex.org/W1966812932', 'https://openalex.org/W1921966169', 'https://openalex.org/W1837375349', 'https://openalex.org/W6640657561', 'https://openalex.org/W1611176049', 'https://openalex.org/W6640828828', 'https://openalex.org/W2169951677', 'https://openalex.org/W2085893426', 'https://openalex.org/W2110507941', 'https://openalex.org/W1949782964', 'https://openalex.org/W1957665339', 'https://openalex.org/W1994859265', 'https://openalex.org/W4239340178']",2002-12-04
https://openalex.org/W2128780426,https://doi.org/10.1109/icassp.1989.266375,An improved sub-word based speech recognizer,"The authors describe a system for speaker-dependent speech recognition based on acoustic subword units. Several strategies for automatic generation of an acoustic lexicon are outlined. Preliminary tests have been performed on a small vocabulary. In these tests, the proposed system showed results comparable to those of whole-word-based systems.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W1611176049', 'https://openalex.org/W2134383396', 'https://openalex.org/W1950396994', 'https://openalex.org/W2585529150', 'https://openalex.org/W2584782119', 'https://openalex.org/W2945307304', 'https://openalex.org/W6636593334', 'https://openalex.org/W1921966169', 'https://openalex.org/W2105594594', 'https://openalex.org/W6640657561', 'https://openalex.org/W2156446297', 'https://openalex.org/W2057833190', 'https://openalex.org/W6640828828', 'https://openalex.org/W2110507941', 'https://openalex.org/W4239340178', 'https://openalex.org/W1628850721', 'https://openalex.org/W2112529238', 'https://openalex.org/W1949782964', 'https://openalex.org/W1957665339']",2003-01-13
https://openalex.org/W2099878906,https://doi.org/10.1109/ccece.1999.808034,Automatic detection of acoustic sub-word boundaries for single digit recognition,This paper investigates the use of a spectral variation function to automatically detect acoustic sub-word boundaries in single digits. The developed algorithm generates sub-words for single-digit recognition system using an RBF neural network.,"['https://openalex.org/W6640657561', 'https://openalex.org/W2078376347', 'https://openalex.org/W2144003766', 'https://openalex.org/W2121551440', 'https://openalex.org/W2025209169', 'https://openalex.org/W2048205846', 'https://openalex.org/W2152131029', 'https://openalex.org/W1989337816', 'https://openalex.org/W1560013842', 'https://openalex.org/W1949782964', 'https://openalex.org/W2035224173']",2003-01-20
https://openalex.org/W2609772913,https://doi.org/10.1016/j.specom.2017.04.003,Automatic syllable segmentation algorithm of Chinese speech based on MF-DFA,,"['https://openalex.org/W2006391062', 'https://openalex.org/W2385327289', 'https://openalex.org/W6604712344', 'https://openalex.org/W2384978429', 'https://openalex.org/W2192975138', 'https://openalex.org/W6632346793', 'https://openalex.org/W2071602085', 'https://openalex.org/W2286739582', 'https://openalex.org/W2345980057', 'https://openalex.org/W1965635292', 'https://openalex.org/W1986410788', 'https://openalex.org/W2253740514', 'https://openalex.org/W2034878035', 'https://openalex.org/W2017821362', 'https://openalex.org/W6675820916', 'https://openalex.org/W6683767290', 'https://openalex.org/W6635877648', 'https://openalex.org/W2015876361', 'https://openalex.org/W4230220154', 'https://openalex.org/W2262496412', 'https://openalex.org/W6685028937', 'https://openalex.org/W2356914971', 'https://openalex.org/W2392585165', 'https://openalex.org/W6640657561', 'https://openalex.org/W2197851527', 'https://openalex.org/W2053360453', 'https://openalex.org/W6668861157', 'https://openalex.org/W1538482441', 'https://openalex.org/W2161368496', 'https://openalex.org/W2107105119', 'https://openalex.org/W116355242', 'https://openalex.org/W2171631590', 'https://openalex.org/W1949782964', 'https://openalex.org/W2073885837', 'https://openalex.org/W2074223539', 'https://openalex.org/W3143835353', 'https://openalex.org/W2371348422', 'https://openalex.org/W1598371594']",2017-04-26
https://openalex.org/W2158598479,https://doi.org/10.1109/asru.1997.659006,Combined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units,"A major challenge in speech recognition is creating a lexicon which is robust to inter and intra speaker variations. This is even more so in speech recognisers based on non linguistic units, e.g., acoustic subword units (ASWUs), since no standard pronunciation dictionaries are available. Thus the baseforms describing the vocabulary words in terms of the recognition units need to be generated from training data. We propose an algorithm for ASWU based speech recognition which performs a combined optimisation of the baseforms and the subword models. The resulting system has been tested on the DARPA Resource Management task, and is shown to perform comparably to a baseline phoneme based system.","['https://openalex.org/W1949782964', 'https://openalex.org/W2134383396', 'https://openalex.org/W2128780426', 'https://openalex.org/W2143980933', 'https://openalex.org/W6674137845', 'https://openalex.org/W2137919556', 'https://openalex.org/W2157754466', 'https://openalex.org/W1950396994', 'https://openalex.org/W189838809', 'https://openalex.org/W2095776451']",2002-11-22
https://openalex.org/W2107105119,https://doi.org/10.1109/icslp.1996.607750,On the robust automatic segmentation of spontaneous speech,"The results from applying an improved algorithm in the task of automatic segmentationof spontaneoustelephonequality speechare presented, and compared to the results from those resulting from superimposing white noise. Three segmentation algorithms are compared which are all based on variants of the Spectral Variation Function. Experimental results are obtained on the OGI multi-language telephonespeechcorpus (OGI TS). We show that the use of the auditory forward andbackwardmaskingeffects prior to the SVFcomputation increases the robustness of the algorithm to white noise. When the average signal-to-noise ratio (SNR) is decreased to 10dB the peak ratio (defined as the ratio of the number of peaks measured at the target over the original SNRs) is increased by 16%, 12%, and 11% for the MFC (Mel-Frequency Cepstra), RASTA (RelAtive SpecTrAl processing), and the FBDYN (Forward-Backward auditory masking DYNamic cepstra) SVF segmentation algorithms, respectively.","['https://openalex.org/W1994396704', 'https://openalex.org/W6603625576', 'https://openalex.org/W1950396994', 'https://openalex.org/W6640657561', 'https://openalex.org/W2100643000', 'https://openalex.org/W6607538539', 'https://openalex.org/W6603539429', 'https://openalex.org/W2134445571', 'https://openalex.org/W638114908', 'https://openalex.org/W87246703', 'https://openalex.org/W185189022', 'https://openalex.org/W1949782964', 'https://openalex.org/W89602566']",2002-12-24
https://openalex.org/W2033431959,https://doi.org/10.1109/89.222877,Transform representation of the spectra of acoustic speech segments with applications. I. General approach and application to speech recognition,"An approach to modeling and capturing the time-varying structure of the spectral envelope of speech is reported. Acoustic subword decomposition and the Karhunen-Loeve transform (KLT) are used to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the KLT with acoustic subword modeling provides concise representation of both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition are presented. The performance of the recognition algorithm based on this approach compares favorably with that of other techniques.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W1540061949', 'https://openalex.org/W2105839915', 'https://openalex.org/W6689278025', 'https://openalex.org/W2110507941', 'https://openalex.org/W2127344093', 'https://openalex.org/W6681983562', 'https://openalex.org/W1536990986', 'https://openalex.org/W3021042054', 'https://openalex.org/W1984749204', 'https://openalex.org/W1817831125', 'https://openalex.org/W2101858445', 'https://openalex.org/W6640657561', 'https://openalex.org/W2046316417', 'https://openalex.org/W2158358098', 'https://openalex.org/W2137089646', 'https://openalex.org/W2001496355', 'https://openalex.org/W2144418681', 'https://openalex.org/W2102794086', 'https://openalex.org/W2065625684', 'https://openalex.org/W2112446559', 'https://openalex.org/W2001850558', 'https://openalex.org/W2234114278', 'https://openalex.org/W605447104', 'https://openalex.org/W1949782964', 'https://openalex.org/W2149916200']",1993-04-01
https://openalex.org/W2107988889,https://doi.org/10.1109/spcom.2004.1458347,Pronunciation modeling for speech technology,"Written text is based on an orthographic representation of words, i.e. linear sequences of letters. Modern speech technology (automatic speech recognition and text-to-speech synthesis) is based on phonetic units representing realization of sounds. A mapping between the orthographic form and phonetic forms representing the pronunciation is thus required. This may be obtained by creating pronunciation lexica and/or rule-based systems for grapheme-to-phoneme conversion. Traditionally, this mapping has been obtained manually, based on phonetic and linguistic knowledge. This approach has a number of drawbacks: i) the pronunciations represent typical pronunciations and will have a limited capacity for describing pronunciation variation due to speaking style and dialectical/accent variations; ii) if multiple pronunciation variants are included, it does not indicate which variants are more significant for the specific application; iii) the description is based on phonetic-knowledge and does not take into account that the units used in speech technology may deviate from the phonetic interpretation; and iv) the description is limited to units with a linguistic interpretation. The paper will present and discuss methods for modeling pronunciation and pronunciation variation specifically for applications in speech technology.","['https://openalex.org/W172350449', 'https://openalex.org/W2158598479', 'https://openalex.org/W2519424249', 'https://openalex.org/W2164098806', 'https://openalex.org/W2128780426', 'https://openalex.org/W2080723515', 'https://openalex.org/W189838809', 'https://openalex.org/W2127611282', 'https://openalex.org/W1992883985', 'https://openalex.org/W1949782964', 'https://openalex.org/W1493946344', 'https://openalex.org/W108399510']",2005-06-28
https://openalex.org/W2755708162,https://doi.org/10.1088/1757-899x/231/1/012042,Voice activity detection based on deep neural networks and Viterbi,"Voice Activity Detection (VAD) is important in speech processing. In the applications, the systems usually need to separate speech/non-speech parts, so that only the speech part can be dealt with. How to improve the performances of VAD in different noisy environments is an important issue in speech processing. Deep Neural network, which proves its efficiency in speech recognition, has been widely used in recent years. This paper studies the present typical VAD algorithms, and presents a new VAD algorithm based on deep neural networks and Viterbi algorithm. The result demonstrates the effectiveness of the deep neural network with Viterbi used in VAD. In addition, it shows the flexibility and the real-time performance of the algorithms.","['https://openalex.org/W2143448557', 'https://openalex.org/W1950396994', 'https://openalex.org/W1949782964', 'https://openalex.org/W1851092852', 'https://openalex.org/W2401273849', 'https://openalex.org/W1498436455', 'https://openalex.org/W2125838338', 'https://openalex.org/W2085245374', 'https://openalex.org/W2396495723', 'https://openalex.org/W1966804589', 'https://openalex.org/W24109054', 'https://openalex.org/W1945906288', 'https://openalex.org/W2361576795', 'https://openalex.org/W2606429533', 'https://openalex.org/W2069501481', 'https://openalex.org/W115144749', 'https://openalex.org/W1547574500', 'https://openalex.org/W1550198362', 'https://openalex.org/W2356325859', 'https://openalex.org/W2626699878']",2017-09-01
https://openalex.org/W2324819908,https://doi.org/10.1080/03772063.2015.1075914,Labelling of Hindi Speech,"The goal of this paper is to obtain segmented and labelled speech at syllable level and also that the reasonable number of syllables may suffice the need for travel domain applications. A base-line group delay-based segmentation technique is applied on spoken speech sentences to generate labelled database at syllable level. The system is validated against 50 manually segmented speech utterances. The segmentation accuracy was evaluated by performing time-error analysis. It is observed that 63.07% syllables have time-error less than 30 ms. It is observed that vowels are more accurately segmented as compared to fricatives. The confidence interval is found to be 0.1147 ms for confidence level of 95%. This paper also presents implementation of algorithm for identifying syllables based on linguistic rules for Hindi words. After survey of the relevant literature, a set of rules are identified and implemented as a simple easy-to-implement algorithm. The text segmentation algorithm is tested on 2400 distinct words and algorithm performs with 99.5% accuracy for segmentation of written text.","['https://openalex.org/W80577151', 'https://openalex.org/W1846195874', 'https://openalex.org/W2006391062', 'https://openalex.org/W2033037901', 'https://openalex.org/W1949782964', 'https://openalex.org/W1965635292', 'https://openalex.org/W2179839788', 'https://openalex.org/W2108148744', 'https://openalex.org/W1985725485', 'https://openalex.org/W2115459718', 'https://openalex.org/W168484277', 'https://openalex.org/W2597684388', 'https://openalex.org/W1875231349', 'https://openalex.org/W2075978670', 'https://openalex.org/W2102195139', 'https://openalex.org/W2015876361']",2015-09-07
https://openalex.org/W2099714852,https://doi.org/10.1109/89.496216,Speech analysis and segmentation by parametric filtering,"A new set of digital signal processing techniques for detecting changes in a speech signal is considered. The overall approach is called parametric filtering, and it yields several promising new diagnostics for speech analysis and segmentation including in particular, the demodulated lag-one autocorrelation /spl gamma//sub /spl theta//(/spl eta/), the time-correlation analysis plot, and the /spl gamma//sub /spl theta//(/spl eta/)-based distortion measures. Initial experiments described in this paper establish the potential significance of the parametric filtering method and these new diagnostics for speech analysis and segmentation.","['https://openalex.org/W2158560939', 'https://openalex.org/W1987700346', 'https://openalex.org/W6631188892', 'https://openalex.org/W2111170213', 'https://openalex.org/W2155776568', 'https://openalex.org/W1886451772', 'https://openalex.org/W2146567324', 'https://openalex.org/W1575977464', 'https://openalex.org/W2156542523', 'https://openalex.org/W2125449350', 'https://openalex.org/W2112911062', 'https://openalex.org/W2100836879', 'https://openalex.org/W2099714852', 'https://openalex.org/W1550223443', 'https://openalex.org/W2119487894', 'https://openalex.org/W6640657561', 'https://openalex.org/W2131127032', 'https://openalex.org/W6602153350', 'https://openalex.org/W2165865510', 'https://openalex.org/W2104687512', 'https://openalex.org/W2112446559', 'https://openalex.org/W2169951677', 'https://openalex.org/W1997184885', 'https://openalex.org/W2164240509', 'https://openalex.org/W1963871118', 'https://openalex.org/W1950396994', 'https://openalex.org/W2132606418', 'https://openalex.org/W2192975138', 'https://openalex.org/W4248979535', 'https://openalex.org/W1978248232', 'https://openalex.org/W1520302738', 'https://openalex.org/W1597732836', 'https://openalex.org/W107054539', 'https://openalex.org/W1501095260', 'https://openalex.org/W4239340178', 'https://openalex.org/W2113301133', 'https://openalex.org/W1493163583', 'https://openalex.org/W126774805', 'https://openalex.org/W1560013842', 'https://openalex.org/W1949782964', 'https://openalex.org/W2737005980', 'https://openalex.org/W52899050', 'https://openalex.org/W3094114204', 'https://openalex.org/W1528176576', 'https://openalex.org/W2163899311']",1996-05-01
https://openalex.org/W2162595709,https://doi.org/10.1109/acssc.1992.269127,A HMM-based approach for segmenting continuous speech,"Several algorithms used for automatically segmenting an input speech signal are reviewed. It is shown that they either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. Another approach to automatically segmentating continuous speech is presented. To verify this approach, experimental results from a database of 30 speakers whose speech has been recorded over the public switched telephone network are presented. The results benchmark the algorithm against a state-of-the-art approach and show a 4* reduction in the error rate of the recognition system.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2192975138', 'https://openalex.org/W1950396994', 'https://openalex.org/W2056457557', 'https://openalex.org/W2099959844', 'https://openalex.org/W1989634675', 'https://openalex.org/W6680297918', 'https://openalex.org/W6640657561', 'https://openalex.org/W2073693905', 'https://openalex.org/W2136419841', 'https://openalex.org/W1976349544', 'https://openalex.org/W1949782964']",2003-01-02
https://openalex.org/W2027790409,https://doi.org/10.1016/s0893-6080(00)00030-7,A neural network for 500 word vocabulary word spotting using non-uniform units,,"['https://openalex.org/W2033431959', 'https://openalex.org/W2001496355', 'https://openalex.org/W1883901056', 'https://openalex.org/W1828833942', 'https://openalex.org/W2125142492', 'https://openalex.org/W2137347187', 'https://openalex.org/W2115040572', 'https://openalex.org/W6676214501', 'https://openalex.org/W2117671523', 'https://openalex.org/W1949782964', 'https://openalex.org/W153863737', 'https://openalex.org/W2097473479', 'https://openalex.org/W2104992468']",2000-07-01
https://openalex.org/W1947057191,https://doi.org/10.1109/icassp.1994.389357,A new method for segmenting continuous speech,"Speech recognition systems are increasingly utilized in various applications like telephone services where a user places a call by uttering the digits or the name of the person. One of the main problems in this application is the segmentation of the input utterance into speech and nonspeech portions. Current approaches typically suffer from two problems. They either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. The authors present another approach to automatically segment continuous speech and create speaker dependent models. To verify the hypothesis, they use a database of 30 speakers whose speech has been recorded over the public switched telephone network. With this database, they benchmark their algorithm against a state of the art approach and show a 4/spl times/ reduction in the error rate of the recognition system.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2056457557', 'https://openalex.org/W2099959844', 'https://openalex.org/W1989634675', 'https://openalex.org/W2192975138', 'https://openalex.org/W1950396994', 'https://openalex.org/W6680297918', 'https://openalex.org/W6640657561', 'https://openalex.org/W2073693905', 'https://openalex.org/W1976349544', 'https://openalex.org/W2136419841', 'https://openalex.org/W1949782964']",2002-12-17
https://openalex.org/W2004740791,https://doi.org/10.1109/ijcnn.2013.6706863,Speaker recognition based on SOINN and incremental learning Gaussian mixture model,"Gaussian Mixture Models has been widely used in speaker recognition during the last decades. To deal with the dynamic growth of datasets, initial clustering problem and achieving the results of clustering effectively on incremental data, an incremental adaptation method called incremental learning Gaussian mixture model (IGMM) is proposed in this paper. It was applied to speaker recognition system based on Self Organization Incremental Learning Neural Network (SOINN) and improved EM algorithm. SOINN is a Neural Network which can reach a suitable mixture number and appropriate initial cluster for each model. First, the initial training is conducted by SOINN and EM algorithm only need a limited amount of data. Then, the model would adapt to the data available in each session to enrich itself incrementally and recursively. Experiments were taken on the 1st speech separation challenge database. The results show that IGMM outperforms GMM and classical Bayesian adaptation in most of the cases.","['https://openalex.org/W2061761368', 'https://openalex.org/W6688068256', 'https://openalex.org/W2150159007', 'https://openalex.org/W7048738093', 'https://openalex.org/W2098471762', 'https://openalex.org/W1949782964', 'https://openalex.org/W2165880886', 'https://openalex.org/W2148154194', 'https://openalex.org/W2117605913', 'https://openalex.org/W2136044422', 'https://openalex.org/W2041823554', 'https://openalex.org/W172790201', 'https://openalex.org/W2100969003', 'https://openalex.org/W2152402526', 'https://openalex.org/W2147147599', 'https://openalex.org/W2146871184', 'https://openalex.org/W1545632541', 'https://openalex.org/W2049633694', 'https://openalex.org/W3129711340', 'https://openalex.org/W2198735866']",2013-08-01
https://openalex.org/W1540061949,https://doi.org/10.1109/icassp.1989.266374,Characterization of spectral transitions with applications to acoustic sub-word segmentation and automatic speech recognition,"A mathematical model has been developed for tracking spectral transitions within the spectral envelope of a speech signal. This technique incorporates linguistic knowledge into a mathematical framework to determine time-varying acoustic-phonetic features and describe formant transitions. The proposed model is quite robust and is capable of extracting not only rapid spectral movement, but also smoother spectral transitions that occur in vowel and sonorant sequences. This basic approach has been previously used to extract steady-state acoustic-phonetic features across spectrally homogeneous regions and to perform speaker dependent recognition in which quite successful results were attained in clean as well as noisy speech. It has now been augmented to capture the dynamics of spectral acoustic-phonetic features.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2112446559', 'https://openalex.org/W2102794086', 'https://openalex.org/W2144418681', 'https://openalex.org/W2001496355', 'https://openalex.org/W6689278025', 'https://openalex.org/W2146567324', 'https://openalex.org/W1949782964', 'https://openalex.org/W2110507941', 'https://openalex.org/W2127344093', 'https://openalex.org/W2105839915', 'https://openalex.org/W2234114278', 'https://openalex.org/W1564660545']",2003-01-13
https://openalex.org/W2025688404,https://doi.org/10.1016/0885-2308(89)90013-2,On the use of some robust modeling techniques for speech recognition,,"['https://openalex.org/W2163929346', 'https://openalex.org/W6681084769', 'https://openalex.org/W6638501295', 'https://openalex.org/W6640292850', 'https://openalex.org/W6630041935', 'https://openalex.org/W2112197391', 'https://openalex.org/W2064218608', 'https://openalex.org/W2171850596', 'https://openalex.org/W2105594594', 'https://openalex.org/W2142775654', 'https://openalex.org/W6639312680', 'https://openalex.org/W6635057086', 'https://openalex.org/W1950396994', 'https://openalex.org/W6640657561', 'https://openalex.org/W1584928905', 'https://openalex.org/W1890491863', 'https://openalex.org/W1990005915', 'https://openalex.org/W2142384583', 'https://openalex.org/W1927229584', 'https://openalex.org/W2170967986', 'https://openalex.org/W1949782964', 'https://openalex.org/W1825672924', 'https://openalex.org/W1581253957']",1989-01-01
https://openalex.org/W165254982,,The ICSI 2007 Language Recognition System,"In this paper, we describe the ICSI 2007 language recognition system. The system constitutes a variant of the classic PPRLM (parallel phone recognizer followed by language modeling) approach. We used a combination of frame-by-frame multilayer perceptron (MLP) phone classifiers for English, Arabic, and Mandarin and one open loop hidden Markov Model (HMM) phone recognizer (trained on English data). The maximum likelihood language modeling is substituted by support-vectormachines (SVMs) as a more powerful, discriminative classifi cation method. Rank normalization is used as a normalization method superior to mean-variance normalization. Results are presented on the NIST 2005 language recognition evaluation (LRE05) set and a test set taken from the LRE07 training corpus. The average NIST cost of the system on the LRE05 set is 0.0886.","['https://openalex.org/W1576520375', 'https://openalex.org/W2097426397', 'https://openalex.org/W2395544453', 'https://openalex.org/W2125142492', 'https://openalex.org/W1989674379', 'https://openalex.org/W2104637105', 'https://openalex.org/W1771922646', 'https://openalex.org/W1719251338', 'https://openalex.org/W2078953162', 'https://openalex.org/W2157476997', 'https://openalex.org/W2147533531', 'https://openalex.org/W1949782964', 'https://openalex.org/W2141985727', 'https://openalex.org/W2172287020', 'https://openalex.org/W2147986626']",2008-01-01
https://openalex.org/W1997183752,,A digital neural network approach to speech recognition,"recognition based on sub-word component&amp;quot;. A digital neural network is the fundamental processing strategy in beth methods. The first design is based on the &amp;apos;Separate Segmentation &amp;amp; Labelling &amp;apos; (SS&amp;amp;L) approach. The spectral data of the input utterance is first segmented into phoneme-like units which are then time normalised by linear time normalisation. The neural network labels the time-normalised phoneme-like segments./8.36 % recognition accuracy is achieved for the phoneme-like unit. In the second design, no time no-malisation is required. After segmentation, recognition is performed by classifying the data in a window as it is slid one frame at a time, from the start to the end of of each phoneme-like segment in the utterance. 73.97 % recognition accuracy for the phoneme-like unit is achieved in this application. The parameters of the neural net have been optimised for maximum recognition performance. A segmentation strategy using the sum of the difference in filterbank channel energy over successive spectra produced 80.27 % correct segmentation of isolated utterances into phoneme-like units. A linguistic processor based on that of Kashyap &amp;amp; Mittal [84] enables 93.11 % and 93.49 % word recognition accuracy to be achieved for the SS&amp;amp;L and &amp;apos;Sliding Window &amp;apos; recognisers respectively. The linguistic processor has been redesigned to make it portable so that it can be easily applied to any phoneme based isolated word speech recogruser, To my Parents, brothers and sisters, Aunt Afifa, Unc1e Ghazanfer and Cousin Fahad. ACKNOWLEDGEMENT I would like to express heartfelt thanks to my Supervisor, Dr. T.J. Stonham for his guidance, help, and encouragement throughout this","['https://openalex.org/W1550797049', 'https://openalex.org/W2110816506', 'https://openalex.org/W1965635292', 'https://openalex.org/W2055249553', 'https://openalex.org/W1976882641', 'https://openalex.org/W2479014774', 'https://openalex.org/W2032474878', 'https://openalex.org/W1990005915', 'https://openalex.org/W2101927907', 'https://openalex.org/W2128437483', 'https://openalex.org/W1626971540', 'https://openalex.org/W1677840610', 'https://openalex.org/W1980329685', 'https://openalex.org/W2161259239', 'https://openalex.org/W1977986499', 'https://openalex.org/W2105594594', 'https://openalex.org/W2151111139', 'https://openalex.org/W1996746519', 'https://openalex.org/W1507699566', 'https://openalex.org/W2171850596', 'https://openalex.org/W1837611419', 'https://openalex.org/W2086312927', 'https://openalex.org/W2027406879', 'https://openalex.org/W2112446559', 'https://openalex.org/W1982597206', 'https://openalex.org/W1820275796', 'https://openalex.org/W1628850721', 'https://openalex.org/W1918143823', 'https://openalex.org/W2071663716', 'https://openalex.org/W1888232360', 'https://openalex.org/W2003764568', 'https://openalex.org/W1949782964']",1989-01-01
https://openalex.org/W2125222687,,Query-by-Example Spoken Document Retrieval : The Star Challenge 2008,"In this paper, we give an update of recent research activities in HLT department of I2R in query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the Star Challenge 2008, which was organized by A*STAR, Singapore. It is suggested that low-level feature-based approach, which does not rely on error-prone speech transcripts, is a promising solution to query-by-example multilingual spoken document retrieval.","['https://openalex.org/W2117041980', 'https://openalex.org/W2136583886', 'https://openalex.org/W14090282', 'https://openalex.org/W1514089653', 'https://openalex.org/W2140277151', 'https://openalex.org/W2161723943', 'https://openalex.org/W2106284094', 'https://openalex.org/W2142934499', 'https://openalex.org/W1949782964', 'https://openalex.org/W2172287020', 'https://openalex.org/W49437105', 'https://openalex.org/W2146229144']",2009-10-04
https://openalex.org/W2248362992,https://doi.org/10.47749/t/unicamp.1999.183837,Segmentação automatica e treinamento discriminativo aplicados a um sistema de reconhecimento de digitos conectados,,"['https://openalex.org/W2134383396', 'https://openalex.org/W2138104661', 'https://openalex.org/W2048239603', 'https://openalex.org/W2116658213', 'https://openalex.org/W1950396994', 'https://openalex.org/W4239340178', 'https://openalex.org/W2104687512', 'https://openalex.org/W252202574', 'https://openalex.org/W2171244697', 'https://openalex.org/W1547605749', 'https://openalex.org/W2064218608', 'https://openalex.org/W2036102925', 'https://openalex.org/W1560013842', 'https://openalex.org/W56033721', 'https://openalex.org/W1949782964', 'https://openalex.org/W1988378063', 'https://openalex.org/W1833981795', 'https://openalex.org/W2111751470', 'https://openalex.org/W2270719270', 'https://openalex.org/W1989226853', 'https://openalex.org/W2133155955', 'https://openalex.org/W2099714852', 'https://openalex.org/W2163165400', 'https://openalex.org/W1920769845', 'https://openalex.org/W2009111612', 'https://openalex.org/W1886451772', 'https://openalex.org/W2192975138', 'https://openalex.org/W2135665758', 'https://openalex.org/W1565453839', 'https://openalex.org/W2140337865', 'https://openalex.org/W1194891887', 'https://openalex.org/W29808378', 'https://openalex.org/W2151129782', 'https://openalex.org/W2110804267', 'https://openalex.org/W2584852661', 'https://openalex.org/W2167307239', 'https://openalex.org/W2035789797', 'https://openalex.org/W202828674', 'https://openalex.org/W1585396714', 'https://openalex.org/W2311840975']",1999-12-17
https://openalex.org/W3110206221,https://doi.org/10.48550/arxiv.2011.14062,Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks,"Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery. Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.","['https://openalex.org/W3095049654', 'https://openalex.org/W2786608204', 'https://openalex.org/W3095433102', 'https://openalex.org/W2078769636', 'https://openalex.org/W2913062184', 'https://openalex.org/W2963620343', 'https://openalex.org/W2972794572', 'https://openalex.org/W2964169922', 'https://openalex.org/W2748394715', 'https://openalex.org/W2100736366', 'https://openalex.org/W2009388533', 'https://openalex.org/W2127589108', 'https://openalex.org/W151377110', 'https://openalex.org/W2057007397', 'https://openalex.org/W1957665339', 'https://openalex.org/W4289564011', 'https://openalex.org/W2963775347', 'https://openalex.org/W2118841860', 'https://openalex.org/W2190506272', 'https://openalex.org/W3091905774', 'https://openalex.org/W1949782964', 'https://openalex.org/W2889270419']",2020-11-28
https://openalex.org/W1488766199,https://doi.org/10.1109/icassp.1989.266429,A sub-word based speaker independent speech recognizer using a two-pass segmentation scheme,"An isolated-word speech recognizer based on acoustically defined subwords is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the subword boundaries are estimated. The second pass divides each subword into three subsegments, thus matching the states in the three-state HMMs used to model the subwords. On the basis of both the segmental and subsegmental information, the acoustic lexicon and the HMMs are created. Used in a speaker-independent mode the recognizer is slightly inferior to a corresponding whole-word-based recognizer.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2134383396', 'https://openalex.org/W6640828828', 'https://openalex.org/W2105594594', 'https://openalex.org/W2048648518', 'https://openalex.org/W2009422037', 'https://openalex.org/W1950396994', 'https://openalex.org/W1949782964', 'https://openalex.org/W1957665339']",2003-01-13
https://openalex.org/W1492135023,https://doi.org/10.5281/zenodo.1062766,Edge Detection With The Parametric Filtering Method (Comparison With Canny Method),"In this paper, a new method of image edge-detection and characterization is presented. ""Parametric Filtering method"" uses a judicious defined filter, which preserves the signal correlation structure as input in the autocorrelation of the output. This leads, showing the evolution of the image correlation structure as well as various distortion measures which quantify the deviation between two zones of the signal (the two Hamming signals) for the protection of an image edge.","['https://openalex.org/W2044535354', 'https://openalex.org/W2169951677', 'https://openalex.org/W1597732836', 'https://openalex.org/W2164240509', 'https://openalex.org/W654432019', 'https://openalex.org/W2125449350', 'https://openalex.org/W1963871118', 'https://openalex.org/W1949782964', 'https://openalex.org/W1950396994', 'https://openalex.org/W1493163583', 'https://openalex.org/W2111170213']",2007-10-28
https://openalex.org/W1546983583,https://doi.org/10.1007/bfb0018410,Speech recognition for knowledge based computer systems,,"['https://openalex.org/W1975598412', 'https://openalex.org/W4300402905', 'https://openalex.org/W1949782964', 'https://openalex.org/W2026554312', 'https://openalex.org/W1605336157', 'https://openalex.org/W3207342693']",2005-11-22
https://openalex.org/W2187576911,,Speech Segments with Applications-I: General Approach and Application to Speech Recognition,"Absmcr- We present in this series of two papers a new approach for modeling and capturing the time-varying structure of the spectral envelope of speech. In this approach, we use an acoustic subword decomposition and the Karhunen-Loeve transform (UT) to extract and efficiently represent the highly correlated structure of the spectral envelope. Integration of the UT with acoustic subword modeling is a novel approach that concisely represents both steady-state and dynamic features of the spectra in a unified framework that very effectively captures acoustic-phonetic patterns. The organization of these two papers is as follows: the first paper, Part I presents the physiological and perceptual basis for the approach, the frame-based and acoustic-subword-based spectral representation, and applications to speaker-dependent recognition. The performance of the recognition algorithm based on this approach compares favorably to other existing techniques. Part I1 will present a frequency-domain coding technique by analysidsynthesis. This application of the new method produces good quality speech at low bit rates.","['https://openalex.org/W2105839915', 'https://openalex.org/W2001850558', 'https://openalex.org/W2137089646', 'https://openalex.org/W2127344093', 'https://openalex.org/W1817831125', 'https://openalex.org/W1949782964', 'https://openalex.org/W2065625684', 'https://openalex.org/W2149916200', 'https://openalex.org/W2234114278', 'https://openalex.org/W2144418681', 'https://openalex.org/W2001496355', 'https://openalex.org/W2101858445', 'https://openalex.org/W1984749204', 'https://openalex.org/W2046316417', 'https://openalex.org/W1536990986', 'https://openalex.org/W2112446559', 'https://openalex.org/W605447104', 'https://openalex.org/W1540061949', 'https://openalex.org/W2110507941']",1993-01-01
https://openalex.org/W2188541342,,USING A TWO-PASS SEGMENTATION SCHEME.,"An isolated word speech recognizer based on acoustically defined sub-words is described. In the training stage the speech is decomposed by an automatic two-pass segmentation algorithm. In the first pass the sub-word boundaries are estimated. The second pass divides each sub- word into three subsegments, thus matching the statcs in the 3-state HMMs used to model the sub-words. Based on both the segmental and subsegmental information, the acoustic lexicon and the HMMs are mated. Used in a speaker independent mode the recognizer is slightly inferior to a corresponding whole-word based recognizer.","['https://openalex.org/W1957665339', 'https://openalex.org/W2032474878', 'https://openalex.org/W2134383396', 'https://openalex.org/W1949782964']",1989-01-01
https://openalex.org/W2289055988,,Détection de contour par la méthode de filtrage paramétrique,"Resume: La detection de contours n’est pas un objectif en soi, elle est la premiere etape dans un systeme de vision, pour cela des varietes de detecteurs de contours ont ete proposees, ils different dans leurs proprietes mathematiques et algorithmiques, et le type de contours qu’ils sont capables d’extraire. On propose une nouvelle methode de detection de contours, qui s »appelle filtrage parametrique, elle ouvre de nouvelles tres prometteuses dans l’analyse de contour et la segmentation, incluant en particulier, le signal d’autocorrelation de premier ordre ) (η","['https://openalex.org/W2111170213', 'https://openalex.org/W654432019', 'https://openalex.org/W2294416960', 'https://openalex.org/W1597732836', 'https://openalex.org/W2044535354', 'https://openalex.org/W2169951677', 'https://openalex.org/W1480865305', 'https://openalex.org/W1950396994', 'https://openalex.org/W1963871118', 'https://openalex.org/W2112911062', 'https://openalex.org/W1493163583', 'https://openalex.org/W2099714852', 'https://openalex.org/W2125449350', 'https://openalex.org/W1949782964', 'https://openalex.org/W2164240509']",2005-01-01
https://openalex.org/W4406461596,https://doi.org/10.1109/slt61566.2024.10832139,A Simple HMM with Self-Supervised Representations for Phone Segmentation,,"['https://openalex.org/W3096656254', 'https://openalex.org/W3197974236', 'https://openalex.org/W3204915839', 'https://openalex.org/W3209993061', 'https://openalex.org/W6866724454', 'https://openalex.org/W4294068504', 'https://openalex.org/W3198134274', 'https://openalex.org/W4313182775', 'https://openalex.org/W2146567324', 'https://openalex.org/W4372259842', 'https://openalex.org/W2161562001', 'https://openalex.org/W3127686677', 'https://openalex.org/W4319862440', 'https://openalex.org/W89602566', 'https://openalex.org/W2150783950', 'https://openalex.org/W1949782964', 'https://openalex.org/W1950396994', 'https://openalex.org/W2054665642', 'https://openalex.org/W2116697043', 'https://openalex.org/W1558402681', 'https://openalex.org/W2059823263', 'https://openalex.org/W2017814685', 'https://openalex.org/W2586482422', 'https://openalex.org/W2164463707', 'https://openalex.org/W1610605641', 'https://openalex.org/W2121997342', 'https://openalex.org/W2087863531', 'https://openalex.org/W130754613', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226380987', 'https://openalex.org/W2117041980', 'https://openalex.org/W6675022971', 'https://openalex.org/W2468716020', 'https://openalex.org/W2964169922', 'https://openalex.org/W4297841405', 'https://openalex.org/W4391021372', 'https://openalex.org/W2594908799', 'https://openalex.org/W3098643042', 'https://openalex.org/W2100768664', 'https://openalex.org/W3036601975']",2024-12-02
https://openalex.org/W4392903114,https://doi.org/10.1109/icassp48485.2024.10447265,MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies,International audience,"['https://openalex.org/W2982753834', 'https://openalex.org/W2546879872', 'https://openalex.org/W6809885388', 'https://openalex.org/W4312933868', 'https://openalex.org/W6849109464', 'https://openalex.org/W6849416043', 'https://openalex.org/W6849105126', 'https://openalex.org/W6853096648', 'https://openalex.org/W6849635556', 'https://openalex.org/W4386076098', 'https://openalex.org/W6849141701', 'https://openalex.org/W6745136726', 'https://openalex.org/W4372260310', 'https://openalex.org/W6640963894', 'https://openalex.org/W6783867762', 'https://openalex.org/W4226442948', 'https://openalex.org/W6766673545', 'https://openalex.org/W2194775991', 'https://openalex.org/W6779823529', 'https://openalex.org/W6783182287', 'https://openalex.org/W6849517043', 'https://openalex.org/W6844204944', 'https://openalex.org/W2526050071', 'https://openalex.org/W3094550259', 'https://openalex.org/W4319989813', 'https://openalex.org/W4224035735', 'https://openalex.org/W4318719586', 'https://openalex.org/W2965373594', 'https://openalex.org/W4318351475', 'https://openalex.org/W4318718996', 'https://openalex.org/W4380136719']",2024-03-18
https://openalex.org/W4399743538,https://doi.org/10.1145/3672554,AI-Based Affective Music Generation Systems: A Review of Methods and Challenges,"Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancements in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of controllable AI-AMG systems. The main building blocks of an AI-AMG system are discussed and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.","['https://openalex.org/W2569781171', 'https://openalex.org/W4379534434', 'https://openalex.org/W3170115573', 'https://openalex.org/W2592535880', 'https://openalex.org/W6811335593', 'https://openalex.org/W2775487773', 'https://openalex.org/W3014731244', 'https://openalex.org/W4392903114', 'https://openalex.org/W2123106875', 'https://openalex.org/W2986357843', 'https://openalex.org/W2774685635', 'https://openalex.org/W4221159010', 'https://openalex.org/W2788156829', 'https://openalex.org/W1924770834', 'https://openalex.org/W2062003142', 'https://openalex.org/W6853096648', 'https://openalex.org/W2182269766', 'https://openalex.org/W2967787724', 'https://openalex.org/W2996025994', 'https://openalex.org/W2973163080', 'https://openalex.org/W3203175858', 'https://openalex.org/W2921773252', 'https://openalex.org/W2155891975', 'https://openalex.org/W2110485445', 'https://openalex.org/W6864798992', 'https://openalex.org/W3094138986', 'https://openalex.org/W4306679606', 'https://openalex.org/W4246278590', 'https://openalex.org/W1544168675', 'https://openalex.org/W4296449909', 'https://openalex.org/W2128284591', 'https://openalex.org/W4206519129', 'https://openalex.org/W2744457411', 'https://openalex.org/W2758804652', 'https://openalex.org/W2064675550', 'https://openalex.org/W3175663427', 'https://openalex.org/W3116287406', 'https://openalex.org/W2096410791', 'https://openalex.org/W4205568416', 'https://openalex.org/W2955070661', 'https://openalex.org/W3099378280', 'https://openalex.org/W4376607936', 'https://openalex.org/W4379924943', 'https://openalex.org/W4377140494', 'https://openalex.org/W2543209890', 'https://openalex.org/W4313294746', 'https://openalex.org/W2124239333', 'https://openalex.org/W1590874898', 'https://openalex.org/W2902733006', 'https://openalex.org/W4392271377', 'https://openalex.org/W6779467793', 'https://openalex.org/W2141885966', 'https://openalex.org/W4281699226', 'https://openalex.org/W2792351129', 'https://openalex.org/W3157090171', 'https://openalex.org/W2739226128', 'https://openalex.org/W4388717448', 'https://openalex.org/W3215474158', 'https://openalex.org/W3114948430', 'https://openalex.org/W4225253266', 'https://openalex.org/W1966004642', 'https://openalex.org/W4308860117', 'https://openalex.org/W4285331791', 'https://openalex.org/W2067342401', 'https://openalex.org/W2981967511', 'https://openalex.org/W2903350023', 'https://openalex.org/W2071589047', 'https://openalex.org/W2149628368', 'https://openalex.org/W2102998034', 'https://openalex.org/W4318718630', 'https://openalex.org/W35583034', 'https://openalex.org/W2725695910', 'https://openalex.org/W2346300282', 'https://openalex.org/W2621854987', 'https://openalex.org/W3086848615', 'https://openalex.org/W2142996485', 'https://openalex.org/W1976092872', 'https://openalex.org/W2945411714', 'https://openalex.org/W2093623179', 'https://openalex.org/W4225492824', 'https://openalex.org/W3087146618', 'https://openalex.org/W3161056088', 'https://openalex.org/W6739901393', 'https://openalex.org/W2166067393', 'https://openalex.org/W3158589047', 'https://openalex.org/W2031828195', 'https://openalex.org/W2613994972', 'https://openalex.org/W2112665542', 'https://openalex.org/W2508950605', 'https://openalex.org/W1967352593', 'https://openalex.org/W2171848217', 'https://openalex.org/W2949950073', 'https://openalex.org/W4225504856', 'https://openalex.org/W2166957649', 'https://openalex.org/W4380136719', 'https://openalex.org/W4299451408', 'https://openalex.org/W4311415873', 'https://openalex.org/W4394906120', 'https://openalex.org/W2183285237', 'https://openalex.org/W3099425575', 'https://openalex.org/W3036013631', 'https://openalex.org/W4385245566', 'https://openalex.org/W4226103472', 'https://openalex.org/W4312108389', 'https://openalex.org/W4236094338', 'https://openalex.org/W2982303213', 'https://openalex.org/W3134753491', 'https://openalex.org/W3098928157', 'https://openalex.org/W4287198655', 'https://openalex.org/W4391308108', 'https://openalex.org/W4377819589', 'https://openalex.org/W4320013936']",2024-06-17
https://openalex.org/W4392903177,https://doi.org/10.1109/icassp48485.2024.10447246,Audiosr: Versatile Audio Super-Resolution at Scale,"Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can act as a plug-and-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https://audioldm.github.io/audiosr.","['https://openalex.org/W6802017037', 'https://openalex.org/W2100285470', 'https://openalex.org/W3125868443', 'https://openalex.org/W3197334236', 'https://openalex.org/W4221155904', 'https://openalex.org/W3095802501', 'https://openalex.org/W3213952100', 'https://openalex.org/W4283215837', 'https://openalex.org/W6849109464', 'https://openalex.org/W6853096648', 'https://openalex.org/W6778823374', 'https://openalex.org/W4312933868', 'https://openalex.org/W4394625831', 'https://openalex.org/W6809884996', 'https://openalex.org/W4396877837', 'https://openalex.org/W6783713337', 'https://openalex.org/W6783867762', 'https://openalex.org/W3198234802', 'https://openalex.org/W6855434424', 'https://openalex.org/W6697040288', 'https://openalex.org/W4400033239', 'https://openalex.org/W6849105126', 'https://openalex.org/W2052666245', 'https://openalex.org/W4318351475', 'https://openalex.org/W4385473570', 'https://openalex.org/W3001377302', 'https://openalex.org/W4212774754', 'https://openalex.org/W4380136719', 'https://openalex.org/W3203491020']",2024-03-18
https://openalex.org/W4392909390,https://doi.org/10.1109/icassp48485.2024.10447027,Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning,"Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.","['https://openalex.org/W6853096648', 'https://openalex.org/W6776218486', 'https://openalex.org/W6849517043', 'https://openalex.org/W6849105126', 'https://openalex.org/W3201326582', 'https://openalex.org/W6798955355', 'https://openalex.org/W6855932524', 'https://openalex.org/W6851948999', 'https://openalex.org/W6853249747', 'https://openalex.org/W4386071707', 'https://openalex.org/W6850625674', 'https://openalex.org/W6854866820', 'https://openalex.org/W4292794290', 'https://openalex.org/W6678969435', 'https://openalex.org/W6798597329', 'https://openalex.org/W2939574508', 'https://openalex.org/W4372260310', 'https://openalex.org/W4284898017', 'https://openalex.org/W6791353385', 'https://openalex.org/W6853393314', 'https://openalex.org/W6811051256', 'https://openalex.org/W6763945542', 'https://openalex.org/W6898505805', 'https://openalex.org/W6678262379', 'https://openalex.org/W6682631176', 'https://openalex.org/W6761205521', 'https://openalex.org/W6852489829', 'https://openalex.org/W4380136719', 'https://openalex.org/W4379251869', 'https://openalex.org/W4361229539', 'https://openalex.org/W4322718191', 'https://openalex.org/W4318351475', 'https://openalex.org/W4385474092', 'https://openalex.org/W2950060770', 'https://openalex.org/W4226358929', 'https://openalex.org/W2936695845', 'https://openalex.org/W4367628410', 'https://openalex.org/W3180663620', 'https://openalex.org/W4384918448', 'https://openalex.org/W3185739472', 'https://openalex.org/W4377130946', 'https://openalex.org/W4287802874', 'https://openalex.org/W2154652894']",2024-03-18
https://openalex.org/W4393147998,https://doi.org/10.1609/aaai.v38i7.28486,Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation,"We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.","['https://openalex.org/W2904913586', 'https://openalex.org/W6803988721', 'https://openalex.org/W3113172960', 'https://openalex.org/W3110013267', 'https://openalex.org/W6776074830', 'https://openalex.org/W6736965957', 'https://openalex.org/W4224951336', 'https://openalex.org/W3111551570', 'https://openalex.org/W6810561959', 'https://openalex.org/W4226373173', 'https://openalex.org/W2593116425', 'https://openalex.org/W4225620083', 'https://openalex.org/W6779823529', 'https://openalex.org/W6634713522', 'https://openalex.org/W6779753537', 'https://openalex.org/W6810340843', 'https://openalex.org/W4226125322', 'https://openalex.org/W4283818626', 'https://openalex.org/W4226317937', 'https://openalex.org/W4312052147', 'https://openalex.org/W6761417287', 'https://openalex.org/W4308613617', 'https://openalex.org/W2887256099', 'https://openalex.org/W6600741150', 'https://openalex.org/W6803340982', 'https://openalex.org/W3036167779', 'https://openalex.org/W4300980117', 'https://openalex.org/W4384918448', 'https://openalex.org/W4312048190', 'https://openalex.org/W24089286', 'https://openalex.org/W4313679638', 'https://openalex.org/W3015371781', 'https://openalex.org/W4226282829', 'https://openalex.org/W2963821905', 'https://openalex.org/W4377865046', 'https://openalex.org/W4303441850', 'https://openalex.org/W1522734439', 'https://openalex.org/W4386075767', 'https://openalex.org/W4386076280', 'https://openalex.org/W3165922361', 'https://openalex.org/W4312282373', 'https://openalex.org/W4322718191', 'https://openalex.org/W4292779060', 'https://openalex.org/W4281485151', 'https://openalex.org/W3166396011', 'https://openalex.org/W4380136719', 'https://openalex.org/W2964218959', 'https://openalex.org/W4213286388', 'https://openalex.org/W1578285471', 'https://openalex.org/W4284898017', 'https://openalex.org/W3203982370', 'https://openalex.org/W4377865102', 'https://openalex.org/W4309957684', 'https://openalex.org/W4312933868', 'https://openalex.org/W4372348103', 'https://openalex.org/W4385848606', 'https://openalex.org/W4283388932', 'https://openalex.org/W4312633146', 'https://openalex.org/W3036125056', 'https://openalex.org/W2951611190', 'https://openalex.org/W4224035735', 'https://openalex.org/W4303440777', 'https://openalex.org/W4298185919', 'https://openalex.org/W3157199281', 'https://openalex.org/W2963066677', 'https://openalex.org/W4287551327', 'https://openalex.org/W2902437806', 'https://openalex.org/W4312655926']",2024-03-24
https://openalex.org/W4401110409,https://doi.org/10.1109/cai59869.2024.00146,JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models,,"['https://openalex.org/W4387969125', 'https://openalex.org/W4225492824', 'https://openalex.org/W3139532015', 'https://openalex.org/W6786183582', 'https://openalex.org/W4221151907', 'https://openalex.org/W2981959899', 'https://openalex.org/W6810311916', 'https://openalex.org/W4312933868', 'https://openalex.org/W6779823529', 'https://openalex.org/W3215615641', 'https://openalex.org/W6838639034', 'https://openalex.org/W4385245566', 'https://openalex.org/W4381786045', 'https://openalex.org/W2972478942', 'https://openalex.org/W2026653933', 'https://openalex.org/W4372266552', 'https://openalex.org/W2593116425', 'https://openalex.org/W4319989813', 'https://openalex.org/W4318718996', 'https://openalex.org/W2746068898', 'https://openalex.org/W3036167779', 'https://openalex.org/W3115879670', 'https://openalex.org/W4380136719', 'https://openalex.org/W4281485151', 'https://openalex.org/W4307323391', 'https://openalex.org/W4287802874', 'https://openalex.org/W4318752004', 'https://openalex.org/W4288099666', 'https://openalex.org/W2908510526', 'https://openalex.org/W4318351475', 'https://openalex.org/W3121370741', 'https://openalex.org/W2519091744', 'https://openalex.org/W3104749918', 'https://openalex.org/W4307079201', 'https://openalex.org/W4300980117']",2024-06-25
https://openalex.org/W4407755370,https://doi.org/10.1016/j.neucom.2025.129645,"A comprehensive overview of Generative AI (GAI): Technologies, applications, and challenges",,"['https://openalex.org/W4220863497', 'https://openalex.org/W6741094427', 'https://openalex.org/W3157651462', 'https://openalex.org/W6778215572', 'https://openalex.org/W6653417934', 'https://openalex.org/W6793010855', 'https://openalex.org/W6751451819', 'https://openalex.org/W6752206335', 'https://openalex.org/W2991305957', 'https://openalex.org/W2904628589', 'https://openalex.org/W4281872770', 'https://openalex.org/W6731566855', 'https://openalex.org/W1994100902', 'https://openalex.org/W4311887664', 'https://openalex.org/W6775628819', 'https://openalex.org/W4312933868', 'https://openalex.org/W2998563994', 'https://openalex.org/W3042185737', 'https://openalex.org/W6801313230', 'https://openalex.org/W6868278908', 'https://openalex.org/W4396685186', 'https://openalex.org/W6674571003', 'https://openalex.org/W2919115771', 'https://openalex.org/W3011574394', 'https://openalex.org/W6739901393', 'https://openalex.org/W6778883912', 'https://openalex.org/W6769627184', 'https://openalex.org/W6759579507', 'https://openalex.org/W3185341429', 'https://openalex.org/W6847076894', 'https://openalex.org/W6809646742', 'https://openalex.org/W6838865847', 'https://openalex.org/W6853465110', 'https://openalex.org/W6846648116', 'https://openalex.org/W6850764995', 'https://openalex.org/W6851275496', 'https://openalex.org/W6635750829', 'https://openalex.org/W6849732303', 'https://openalex.org/W2909678128', 'https://openalex.org/W2964018924', 'https://openalex.org/W6796533464', 'https://openalex.org/W2963109634', 'https://openalex.org/W4390195594', 'https://openalex.org/W4396612944', 'https://openalex.org/W2556388456', 'https://openalex.org/W3217340782', 'https://openalex.org/W6767521871', 'https://openalex.org/W4402716120', 'https://openalex.org/W4402754020', 'https://openalex.org/W6751737857', 'https://openalex.org/W6770847983', 'https://openalex.org/W6779823529', 'https://openalex.org/W6639824700', 'https://openalex.org/W6741832134', 'https://openalex.org/W6752378368', 'https://openalex.org/W6621378261', 'https://openalex.org/W2962770929', 'https://openalex.org/W4386076458', 'https://openalex.org/W2962793481', 'https://openalex.org/W6861027578', 'https://openalex.org/W6639118987', 'https://openalex.org/W2752796333', 'https://openalex.org/W6752910514', 'https://openalex.org/W6765775151', 'https://openalex.org/W6778946027', 'https://openalex.org/W6795986329', 'https://openalex.org/W6795288823', 'https://openalex.org/W3212516020', 'https://openalex.org/W4402753769', 'https://openalex.org/W4393148714', 'https://openalex.org/W2423557781', 'https://openalex.org/W6790978476', 'https://openalex.org/W6791353385', 'https://openalex.org/W3199400376', 'https://openalex.org/W6841064443', 'https://openalex.org/W4402769626', 'https://openalex.org/W6846948161', 'https://openalex.org/W6791295117', 'https://openalex.org/W6726983635', 'https://openalex.org/W2963092440', 'https://openalex.org/W2964033924', 'https://openalex.org/W2964318715', 'https://openalex.org/W2963168844', 'https://openalex.org/W2963917969', 'https://openalex.org/W4386065764', 'https://openalex.org/W4402667906', 'https://openalex.org/W4402727736', 'https://openalex.org/W6767111847', 'https://openalex.org/W6769767169', 'https://openalex.org/W6749351710', 'https://openalex.org/W6775577943', 'https://openalex.org/W6748409065', 'https://openalex.org/W6853096648', 'https://openalex.org/W6755592152', 'https://openalex.org/W6771024825', 'https://openalex.org/W4367359628', 'https://openalex.org/W6856779307', 'https://openalex.org/W6851724922', 'https://openalex.org/W4398226295', 'https://openalex.org/W4402702908', 'https://openalex.org/W6851592950', 'https://openalex.org/W4393178509', 'https://openalex.org/W6804184823', 'https://openalex.org/W4386071707', 'https://openalex.org/W4402671885', 'https://openalex.org/W3194676777', 'https://openalex.org/W2912924812', 'https://openalex.org/W2964223283', 'https://openalex.org/W6676297131', 'https://openalex.org/W1958236864', 'https://openalex.org/W1834627138', 'https://openalex.org/W2471768434', 'https://openalex.org/W3034521057', 'https://openalex.org/W3034600949', 'https://openalex.org/W6810695432', 'https://openalex.org/W4402716163', 'https://openalex.org/W2340897893', 'https://openalex.org/W6767264202', 'https://openalex.org/W6728588997', 'https://openalex.org/W2886641317', 'https://openalex.org/W4286611269', 'https://openalex.org/W6853264062', 'https://openalex.org/W6600757733', 'https://openalex.org/W2425121537', 'https://openalex.org/W2963916161', 'https://openalex.org/W2984008963', 'https://openalex.org/W3204588463', 'https://openalex.org/W4402753903', 'https://openalex.org/W6734260513', 'https://openalex.org/W2972359262', 'https://openalex.org/W6734491695', 'https://openalex.org/W6785212809', 'https://openalex.org/W6762392948', 'https://openalex.org/W4384071683', 'https://openalex.org/W6852870998', 'https://openalex.org/W6853859227', 'https://openalex.org/W6838639034', 'https://openalex.org/W6840180455', 'https://openalex.org/W6853927091', 'https://openalex.org/W4390889801', 'https://openalex.org/W6857999865', 'https://openalex.org/W6810577046', 'https://openalex.org/W4391020683', 'https://openalex.org/W4291711121', 'https://openalex.org/W6839198299', 'https://openalex.org/W6778325684', 'https://openalex.org/W6757789324', 'https://openalex.org/W3161207330', 'https://openalex.org/W6850706490', 'https://openalex.org/W6631455383', 'https://openalex.org/W6838940880', 'https://openalex.org/W2981915309', 'https://openalex.org/W6810633050', 'https://openalex.org/W3167290312', 'https://openalex.org/W4304892053', 'https://openalex.org/W2664267452', 'https://openalex.org/W4386076098', 'https://openalex.org/W6849887644', 'https://openalex.org/W4360847209', 'https://openalex.org/W4323655724', 'https://openalex.org/W2898418766', 'https://openalex.org/W2989512989', 'https://openalex.org/W6838297376', 'https://openalex.org/W4367146875', 'https://openalex.org/W6839328737', 'https://openalex.org/W6603527449', 'https://openalex.org/W4390873054', 'https://openalex.org/W4313472219', 'https://openalex.org/W3168867926', 'https://openalex.org/W2735575534', 'https://openalex.org/W3171654528', 'https://openalex.org/W4387322912', 'https://openalex.org/W3099950029', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964268978', 'https://openalex.org/W4298157202', 'https://openalex.org/W4400717319', 'https://openalex.org/W2161826494', 'https://openalex.org/W3208015123', 'https://openalex.org/W4288089799', 'https://openalex.org/W4387356416', 'https://openalex.org/W3133428285', 'https://openalex.org/W4323572061', 'https://openalex.org/W4376653732', 'https://openalex.org/W4292779060', 'https://openalex.org/W4244955044', 'https://openalex.org/W4281690148', 'https://openalex.org/W4362508231', 'https://openalex.org/W3161457214', 'https://openalex.org/W2726515241', 'https://openalex.org/W3105013723', 'https://openalex.org/W4378189609', 'https://openalex.org/W4401043132', 'https://openalex.org/W4380558379', 'https://openalex.org/W3134642945', 'https://openalex.org/W4401203364', 'https://openalex.org/W4389989145', 'https://openalex.org/W4391164179', 'https://openalex.org/W1840435438', 'https://openalex.org/W4327810433', 'https://openalex.org/W4221143046', 'https://openalex.org/W3191781868', 'https://openalex.org/W4387596573', 'https://openalex.org/W4308759654', 'https://openalex.org/W2960404386', 'https://openalex.org/W4376548770', 'https://openalex.org/W2519091744', 'https://openalex.org/W2923014074', 'https://openalex.org/W4226177592', 'https://openalex.org/W4391709184', 'https://openalex.org/W4365601249', 'https://openalex.org/W4307079201', 'https://openalex.org/W4405439871', 'https://openalex.org/W4320165837', 'https://openalex.org/W4210327135', 'https://openalex.org/W4386044080', 'https://openalex.org/W4402684121', 'https://openalex.org/W4379919062', 'https://openalex.org/W2794325560', 'https://openalex.org/W2914120296', 'https://openalex.org/W4308244910', 'https://openalex.org/W4226125322', 'https://openalex.org/W3121904249', 'https://openalex.org/W4309117899', 'https://openalex.org/W2998108143', 'https://openalex.org/W1959608418', 'https://openalex.org/W4381253553', 'https://openalex.org/W4366330503', 'https://openalex.org/W4401042535', 'https://openalex.org/W2013426435', 'https://openalex.org/W3008823916', 'https://openalex.org/W2887051120', 'https://openalex.org/W4384918941', 'https://openalex.org/W4318752004', 'https://openalex.org/W4361866080', 'https://openalex.org/W4386755570', 'https://openalex.org/W4380136719', 'https://openalex.org/W3121370741', 'https://openalex.org/W3150807214', 'https://openalex.org/W2964332173', 'https://openalex.org/W4297798428', 'https://openalex.org/W4389520749', 'https://openalex.org/W4320013936', 'https://openalex.org/W4389524329', 'https://openalex.org/W4320005767', 'https://openalex.org/W4281485151', 'https://openalex.org/W4283768109', 'https://openalex.org/W4386071957', 'https://openalex.org/W967544008', 'https://openalex.org/W4379539660', 'https://openalex.org/W3192307151', 'https://openalex.org/W2524365899', 'https://openalex.org/W3034999214', 'https://openalex.org/W4289761690', 'https://openalex.org/W4389519042', 'https://openalex.org/W4385849309', 'https://openalex.org/W4312053697', 'https://openalex.org/W4405916988', 'https://openalex.org/W2890894339', 'https://openalex.org/W4294643831', 'https://openalex.org/W4379958452', 'https://openalex.org/W4384807774', 'https://openalex.org/W4396821296', 'https://openalex.org/W2097333193', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963226019', 'https://openalex.org/W1811254738', 'https://openalex.org/W4380136478', 'https://openalex.org/W3172642864', 'https://openalex.org/W3209532394', 'https://openalex.org/W4318718996', 'https://openalex.org/W2794857359', 'https://openalex.org/W4388444668', 'https://openalex.org/W4318718936', 'https://openalex.org/W2963339397', 'https://openalex.org/W4382137667', 'https://openalex.org/W4401587065', 'https://openalex.org/W3016399951', 'https://openalex.org/W1779483307', 'https://openalex.org/W4318351475', 'https://openalex.org/W4384915881', 'https://openalex.org/W3034238904', 'https://openalex.org/W1797268635', 'https://openalex.org/W2951523806', 'https://openalex.org/W4393335604', 'https://openalex.org/W2943552823', 'https://openalex.org/W4377372007', 'https://openalex.org/W4385682098', 'https://openalex.org/W1583912456', 'https://openalex.org/W4376312115', 'https://openalex.org/W4221152111', 'https://openalex.org/W3088409176', 'https://openalex.org/W4229596002', 'https://openalex.org/W4402660110', 'https://openalex.org/W4300980117', 'https://openalex.org/W2952267213', 'https://openalex.org/W4289305009', 'https://openalex.org/W4402684046', 'https://openalex.org/W4390041933', 'https://openalex.org/W4309088836', 'https://openalex.org/W4307653761', 'https://openalex.org/W4385292374', 'https://openalex.org/W4310428868', 'https://openalex.org/W4281632497', 'https://openalex.org/W3173617765', 'https://openalex.org/W3129651364', 'https://openalex.org/W4304194220', 'https://openalex.org/W3162926177', 'https://openalex.org/W4386271714', 'https://openalex.org/W2970006822', 'https://openalex.org/W3118608800', 'https://openalex.org/W2990138404', 'https://openalex.org/W2746068898', 'https://openalex.org/W4297772798', 'https://openalex.org/W4380994495', 'https://openalex.org/W4281657280', 'https://openalex.org/W4310695675', 'https://openalex.org/W4402727496', 'https://openalex.org/W4402671633', 'https://openalex.org/W4377130677', 'https://openalex.org/W4321052096', 'https://openalex.org/W4353115070', 'https://openalex.org/W4401042689', 'https://openalex.org/W4376653782', 'https://openalex.org/W4287802874', 'https://openalex.org/W4362656036', 'https://openalex.org/W4224912544', 'https://openalex.org/W4287752039', 'https://openalex.org/W4389519118', 'https://openalex.org/W4386908092', 'https://openalex.org/W4382361534', 'https://openalex.org/W4404311544', 'https://openalex.org/W3135190223', 'https://openalex.org/W4249502209', 'https://openalex.org/W2910577860', 'https://openalex.org/W4393147120', 'https://openalex.org/W4255556797', 'https://openalex.org/W4298185919', 'https://openalex.org/W4399400954', 'https://openalex.org/W24089286', 'https://openalex.org/W3177813494', 'https://openalex.org/W4316116392', 'https://openalex.org/W4295289379', 'https://openalex.org/W4402670276', 'https://openalex.org/W3041416104', 'https://openalex.org/W3103559770', 'https://openalex.org/W4385572001', 'https://openalex.org/W2959300817', 'https://openalex.org/W2808631503', 'https://openalex.org/W4404658388', 'https://openalex.org/W4389520756', 'https://openalex.org/W4322718191', 'https://openalex.org/W4385292963', 'https://openalex.org/W4381110793', 'https://openalex.org/W4404782209', 'https://openalex.org/W4384389674', 'https://openalex.org/W2996520109', 'https://openalex.org/W4323706279', 'https://openalex.org/W3036167779', 'https://openalex.org/W3110257065', 'https://openalex.org/W2938704169', 'https://openalex.org/W4402671743', 'https://openalex.org/W4281679115', 'https://openalex.org/W4386977707', 'https://openalex.org/W4281483047', 'https://openalex.org/W3209488220', 'https://openalex.org/W3201320181', 'https://openalex.org/W4286987939', 'https://openalex.org/W2963799213', 'https://openalex.org/W4293411878', 'https://openalex.org/W4399809828', 'https://openalex.org/W4389984066', 'https://openalex.org/W2584032004', 'https://openalex.org/W2949099979', 'https://openalex.org/W4321392342', 'https://openalex.org/W4385473486', 'https://openalex.org/W4281557260', 'https://openalex.org/W4234842379', 'https://openalex.org/W2963015836', 'https://openalex.org/W4288283348', 'https://openalex.org/W4289600032', 'https://openalex.org/W3014675721', 'https://openalex.org/W4385573003', 'https://openalex.org/W4381586491']",2025-02-19
https://openalex.org/W4393138539,https://doi.org/10.1109/icassp48485.2024.10446869,Investigating Personalization Methods in Text to Music Generation,"In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music concepts more easily than melody. The code, dataset, and example material of this study are open to the research community <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6849109464', 'https://openalex.org/W6853096648', 'https://openalex.org/W4386072096', 'https://openalex.org/W6841755765', 'https://openalex.org/W4386076425', 'https://openalex.org/W4312933868', 'https://openalex.org/W2989265781', 'https://openalex.org/W4390872341', 'https://openalex.org/W6779823529', 'https://openalex.org/W2593116425', 'https://openalex.org/W2972478942', 'https://openalex.org/W4372260310', 'https://openalex.org/W3015829441', 'https://openalex.org/W6632244161', 'https://openalex.org/W6763954463', 'https://openalex.org/W2137637927', 'https://openalex.org/W4385271055', 'https://openalex.org/W4385535331', 'https://openalex.org/W4385262478', 'https://openalex.org/W4318752004', 'https://openalex.org/W1538887594', 'https://openalex.org/W4212774754', 'https://openalex.org/W4380136719']",2024-03-18
https://openalex.org/W4392908898,https://doi.org/10.1109/icassp48485.2024.10446400,Bass Accompaniment Generation Via Latent Diffusion,"The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task. We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length. At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem. To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling. For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space. We train our model on a dataset of pairs of mixes and matching bass stems. Quantitative experiments demonstrate that, given an input mix, the proposed system can generate basslines with user-specified timbres. Our controllable conditional audio generation framework represents a significant step forward in creating generative AI tools to assist musicians in music production.","['https://openalex.org/W6730558285', 'https://openalex.org/W6760601182', 'https://openalex.org/W4221148975', 'https://openalex.org/W6776218486', 'https://openalex.org/W6679045638', 'https://openalex.org/W6779823529', 'https://openalex.org/W6783713337', 'https://openalex.org/W4312933868', 'https://openalex.org/W6640963894', 'https://openalex.org/W2519091744', 'https://openalex.org/W6732429163', 'https://openalex.org/W6853096648', 'https://openalex.org/W6755257315', 'https://openalex.org/W6758675244', 'https://openalex.org/W6841472891', 'https://openalex.org/W3088329082', 'https://openalex.org/W6783182287', 'https://openalex.org/W6782760101', 'https://openalex.org/W6855355973', 'https://openalex.org/W6771763809', 'https://openalex.org/W6803922201', 'https://openalex.org/W6795986329', 'https://openalex.org/W1901129140', 'https://openalex.org/W6739901393', 'https://openalex.org/W4385570645', 'https://openalex.org/W4389977645', 'https://openalex.org/W4312349930', 'https://openalex.org/W6840815571', 'https://openalex.org/W6838639034', 'https://openalex.org/W4394625831', 'https://openalex.org/W6631190155', 'https://openalex.org/W6757817989', 'https://openalex.org/W6809884996', 'https://openalex.org/W6763945542', 'https://openalex.org/W3094550259', 'https://openalex.org/W4287802874', 'https://openalex.org/W4309117899', 'https://openalex.org/W4288099666', 'https://openalex.org/W4401110409', 'https://openalex.org/W4319452539', 'https://openalex.org/W4281485151', 'https://openalex.org/W4318351475', 'https://openalex.org/W4380136719']",2024-03-18
https://openalex.org/W4402727350,https://doi.org/10.1109/cvpr52733.2024.02533,MELFuSION: Synthesizing Music from Image and Language Cues Using Diffusion Models,,"['https://openalex.org/W4297841924', 'https://openalex.org/W3036080639', 'https://openalex.org/W6746363961', 'https://openalex.org/W4372266552', 'https://openalex.org/W2093341630', 'https://openalex.org/W6797230115', 'https://openalex.org/W3108240585', 'https://openalex.org/W2593116425', 'https://openalex.org/W2111907603', 'https://openalex.org/W2526050071', 'https://openalex.org/W4285605725', 'https://openalex.org/W4304099317', 'https://openalex.org/W6783867762', 'https://openalex.org/W3094550259', 'https://openalex.org/W3108047681', 'https://openalex.org/W2948556824', 'https://openalex.org/W6795261426', 'https://openalex.org/W6791353385', 'https://openalex.org/W4312933868', 'https://openalex.org/W1901129140', 'https://openalex.org/W4372348103', 'https://openalex.org/W2126286273', 'https://openalex.org/W4385245566', 'https://openalex.org/W2013573600', 'https://openalex.org/W4367359628', 'https://openalex.org/W6743059557', 'https://openalex.org/W2005527477', 'https://openalex.org/W2612690371', 'https://openalex.org/W4287802874', 'https://openalex.org/W4318752004', 'https://openalex.org/W4300980117', 'https://openalex.org/W4319989813', 'https://openalex.org/W2746068898', 'https://openalex.org/W2747464893', 'https://openalex.org/W4376312115', 'https://openalex.org/W4383993968', 'https://openalex.org/W4318718630', 'https://openalex.org/W4293575120', 'https://openalex.org/W4380136719', 'https://openalex.org/W4318351475', 'https://openalex.org/W4226213470', 'https://openalex.org/W4378499140', 'https://openalex.org/W4287120591', 'https://openalex.org/W4287111106', 'https://openalex.org/W4307079201', 'https://openalex.org/W2990138404', 'https://openalex.org/W3121370741', 'https://openalex.org/W3094502228', 'https://openalex.org/W4367365521', 'https://openalex.org/W4293342670', 'https://openalex.org/W4280542470']",2024-06-16
https://openalex.org/W4390905523,https://doi.org/10.1109/tkde.2024.3354960,Managing Metaverse Data Tsunami: Actionable Insights,"In the metaverse the physical space and the virtual space co-exist, and interact simultaneously. While the physical space is virtually enhanced with information, the virtual space is continuously refreshed with real-time, real-world information. To allow users to process and manipulate information seamlessly between the real and digital spaces, novel technologies must be developed. These include smart interfaces, new augmented realities, and efficient data storage, management, and dissemination techniques. In this paper, we first discuss some promising co-space applications. These applications offer opportunities that neither of the spaces can realize on its own. Then, we further discuss several emerging technologies that empower the construction of metaverse. After that, we discuss comprehensively the data centric challenges. Finally, we discuss and envision what are likely to be required from the database and system perspectives.","['https://openalex.org/W2999832078', 'https://openalex.org/W6772630726', 'https://openalex.org/W2988359971', 'https://openalex.org/W1965569186', 'https://openalex.org/W4378676985', 'https://openalex.org/W4304812148', 'https://openalex.org/W4381745568', 'https://openalex.org/W4285298463', 'https://openalex.org/W4284966012', 'https://openalex.org/W2796626119', 'https://openalex.org/W3163707479', 'https://openalex.org/W4200069166', 'https://openalex.org/W4320018409', 'https://openalex.org/W4304202992', 'https://openalex.org/W3161970973', 'https://openalex.org/W2986445670', 'https://openalex.org/W3021395787', 'https://openalex.org/W3007362259', 'https://openalex.org/W4385346108', 'https://openalex.org/W4295857769', 'https://openalex.org/W4400118952', 'https://openalex.org/W4385624010', 'https://openalex.org/W4390828922', 'https://openalex.org/W4312933868', 'https://openalex.org/W6844305113', 'https://openalex.org/W6849105126', 'https://openalex.org/W6853096648', 'https://openalex.org/W6843813467', 'https://openalex.org/W4386065887', 'https://openalex.org/W4393178509', 'https://openalex.org/W6852827418', 'https://openalex.org/W3154818219', 'https://openalex.org/W4362496217', 'https://openalex.org/W4396220739', 'https://openalex.org/W4293339592', 'https://openalex.org/W4301185598', 'https://openalex.org/W3162886728', 'https://openalex.org/W4211263987', 'https://openalex.org/W3015482591', 'https://openalex.org/W3213366125', 'https://openalex.org/W4283738729', 'https://openalex.org/W3202199074', 'https://openalex.org/W4327927453', 'https://openalex.org/W3194918904', 'https://openalex.org/W4285815732', 'https://openalex.org/W4226225860', 'https://openalex.org/W4320717855', 'https://openalex.org/W3210522271', 'https://openalex.org/W4293261694', 'https://openalex.org/W3153670037', 'https://openalex.org/W4384009751', 'https://openalex.org/W4293353521', 'https://openalex.org/W4321021837', 'https://openalex.org/W4320063157', 'https://openalex.org/W4224059887', 'https://openalex.org/W3129114628', 'https://openalex.org/W3003334694', 'https://openalex.org/W4308763268', 'https://openalex.org/W4381747623', 'https://openalex.org/W4220991980', 'https://openalex.org/W4364321750', 'https://openalex.org/W4376853918', 'https://openalex.org/W4313121243', 'https://openalex.org/W4319066461', 'https://openalex.org/W4283743615', 'https://openalex.org/W3208948435', 'https://openalex.org/W4205934968', 'https://openalex.org/W4361200075', 'https://openalex.org/W4304142358', 'https://openalex.org/W4379382461', 'https://openalex.org/W4225749806', 'https://openalex.org/W3156976925', 'https://openalex.org/W2916302242', 'https://openalex.org/W4286432951', 'https://openalex.org/W2132615914', 'https://openalex.org/W2074374448', 'https://openalex.org/W2154721480', 'https://openalex.org/W2023349988', 'https://openalex.org/W4294891500', 'https://openalex.org/W4309484364', 'https://openalex.org/W2102682774', 'https://openalex.org/W2146018070', 'https://openalex.org/W2094962446', 'https://openalex.org/W6635661358', 'https://openalex.org/W2132884486', 'https://openalex.org/W2168202182', 'https://openalex.org/W1590548082', 'https://openalex.org/W1992673035', 'https://openalex.org/W2889140904', 'https://openalex.org/W2260616469', 'https://openalex.org/W6809847419', 'https://openalex.org/W2104805832', 'https://openalex.org/W2098907704', 'https://openalex.org/W2059153561', 'https://openalex.org/W2109787180', 'https://openalex.org/W2023749213', 'https://openalex.org/W4313004354', 'https://openalex.org/W6839763862', 'https://openalex.org/W4206320562', 'https://openalex.org/W3197640018', 'https://openalex.org/W3049595782', 'https://openalex.org/W4380433128', 'https://openalex.org/W1873763122', 'https://openalex.org/W4381328606', 'https://openalex.org/W3094542121', 'https://openalex.org/W6676230027', 'https://openalex.org/W2117268753', 'https://openalex.org/W2105247179', 'https://openalex.org/W6677231548', 'https://openalex.org/W2010801412', 'https://openalex.org/W2044288632', 'https://openalex.org/W2098931293', 'https://openalex.org/W4253102189', 'https://openalex.org/W1488414089', 'https://openalex.org/W3012510853', 'https://openalex.org/W2173213060', 'https://openalex.org/W2171668176', 'https://openalex.org/W2019183416', 'https://openalex.org/W1816580784', 'https://openalex.org/W4381946597', 'https://openalex.org/W4282576498', 'https://openalex.org/W2884688818', 'https://openalex.org/W1999610418', 'https://openalex.org/W2798304533', 'https://openalex.org/W4243028477', 'https://openalex.org/W1851853373', 'https://openalex.org/W6601248513', 'https://openalex.org/W2889230848', 'https://openalex.org/W6757188888', 'https://openalex.org/W6761088107', 'https://openalex.org/W6775201933', 'https://openalex.org/W2155713641', 'https://openalex.org/W2060883486', 'https://openalex.org/W2164638025', 'https://openalex.org/W2134141434', 'https://openalex.org/W2145943173', 'https://openalex.org/W4288310158', 'https://openalex.org/W2963638209', 'https://openalex.org/W3139769037', 'https://openalex.org/W4288076018', 'https://openalex.org/W6846281143', 'https://openalex.org/W2525739395', 'https://openalex.org/W3176739743', 'https://openalex.org/W3175954176', 'https://openalex.org/W3173587790', 'https://openalex.org/W6799852636', 'https://openalex.org/W4367046983', 'https://openalex.org/W6761121591', 'https://openalex.org/W3109585842', 'https://openalex.org/W6838957029', 'https://openalex.org/W6838825371', 'https://openalex.org/W4312280420', 'https://openalex.org/W6800751262', 'https://openalex.org/W6778883912', 'https://openalex.org/W6809750266', 'https://openalex.org/W6849990444', 'https://openalex.org/W3205786327', 'https://openalex.org/W4390872335', 'https://openalex.org/W4312777269', 'https://openalex.org/W4385573410', 'https://openalex.org/W30266919', 'https://openalex.org/W3010543330', 'https://openalex.org/W4298187450', 'https://openalex.org/W4307787507', 'https://openalex.org/W3195577433', 'https://openalex.org/W4320086121', 'https://openalex.org/W4376167340', 'https://openalex.org/W2928790192', 'https://openalex.org/W3175646727', 'https://openalex.org/W4301466060', 'https://openalex.org/W3000334388', 'https://openalex.org/W2115503987', 'https://openalex.org/W4281786751', 'https://openalex.org/W3125162816', 'https://openalex.org/W2928138944', 'https://openalex.org/W1595752182', 'https://openalex.org/W4311001906', 'https://openalex.org/W3178212073', 'https://openalex.org/W4292779060', 'https://openalex.org/W4380136719', 'https://openalex.org/W4281663660', 'https://openalex.org/W2107800601', 'https://openalex.org/W3159350105', 'https://openalex.org/W4323366671', 'https://openalex.org/W2930508541', 'https://openalex.org/W4309442291', 'https://openalex.org/W4221160934', 'https://openalex.org/W1575210522', 'https://openalex.org/W4361806024', 'https://openalex.org/W4318351475', 'https://openalex.org/W2109979674', 'https://openalex.org/W4367061106', 'https://openalex.org/W2161569281', 'https://openalex.org/W4298185919', 'https://openalex.org/W2905507405', 'https://openalex.org/W4385474113', 'https://openalex.org/W4249046202', 'https://openalex.org/W4295801271', 'https://openalex.org/W4223945632', 'https://openalex.org/W3101860164', 'https://openalex.org/W4294106961', 'https://openalex.org/W4220855261']",2024-01-16
https://openalex.org/W4391584535,https://doi.org/10.1109/aiiip61647.2023.00030,Harmonizing AI-Generated Music: Integrating Symbolic and Audio Models for Text-to-Music Generation,"The evolution of AI-generated music through input text has seen remarkable advancements in both symbolic and audio music generation. Despite this progress, the synergy between these two domains remains underexplored. Consequently, we introduce a novel method for text-to-music generation, capitalizing on the precise control over specific musical attributes provided by symbolic music models and the ability of audio music models to generate music coherent with the contextual meaning of input text. This method enhances the alignment between the generated music and the input text. Specifically, the proposed method initiates by generating symbolic music from input text, which is then transformed into audio music. Ultimately, music conditioned on the input text and the transformed audio music is generated. The experiments demonstrate that the proposed method produces music more aligned with the input text compared to individual models. Moreover, the method proves particularly effective in generating music lasting between 30 to 74 seconds, and shows consistency improvement on individual models under variable input text lengths.","['https://openalex.org/W4286250570', 'https://openalex.org/W2250539671', 'https://openalex.org/W6682691769', 'https://openalex.org/W6769627184', 'https://openalex.org/W4372260310', 'https://openalex.org/W1965419851', 'https://openalex.org/W4377140494', 'https://openalex.org/W6739901393', 'https://openalex.org/W3213549365', 'https://openalex.org/W4376607936', 'https://openalex.org/W4379251846', 'https://openalex.org/W4320342491', 'https://openalex.org/W4380136719', 'https://openalex.org/W4288089799', 'https://openalex.org/W2896457183', 'https://openalex.org/W1959608418', 'https://openalex.org/W4307323391', 'https://openalex.org/W4293575120', 'https://openalex.org/W4294170691', 'https://openalex.org/W4318351475', 'https://openalex.org/W4309801653', 'https://openalex.org/W4318718630', 'https://openalex.org/W4385245566']",2023-10-27
https://openalex.org/W4404037650,https://doi.org/10.1109/mlsp58920.2024.10734721,Foleygen: Visually-Guided Audio Generation,,"['https://openalex.org/W6849109464', 'https://openalex.org/W6845479124', 'https://openalex.org/W4396877837', 'https://openalex.org/W6853096648', 'https://openalex.org/W2964345931', 'https://openalex.org/W2912947616', 'https://openalex.org/W2963807156', 'https://openalex.org/W3046890131', 'https://openalex.org/W6802805937', 'https://openalex.org/W4372348103', 'https://openalex.org/W6854445938', 'https://openalex.org/W4393160294', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381786045', 'https://openalex.org/W6791353385', 'https://openalex.org/W4386071707', 'https://openalex.org/W6810265253', 'https://openalex.org/W4385245566', 'https://openalex.org/W3015371781', 'https://openalex.org/W6838322825', 'https://openalex.org/W6840815571', 'https://openalex.org/W6757220786', 'https://openalex.org/W2526050071', 'https://openalex.org/W3205475937', 'https://openalex.org/W4380136719', 'https://openalex.org/W4288099666', 'https://openalex.org/W2560662850', 'https://openalex.org/W4383045354', 'https://openalex.org/W4221167396', 'https://openalex.org/W4281758439']",2024-09-22
https://openalex.org/W4381886802,https://doi.org/10.2139/ssrn.4490102,Artificial Intelligence Techniques for Pop Music Creation: A Real Music Production Perspective,,"['https://openalex.org/W4327810158', 'https://openalex.org/W2553169610', 'https://openalex.org/W2185636028', 'https://openalex.org/W2792210438', 'https://openalex.org/W2969348315', 'https://openalex.org/W2963575853', 'https://openalex.org/W2978471471', 'https://openalex.org/W2579406683', 'https://openalex.org/W3190244907', 'https://openalex.org/W4226199750', 'https://openalex.org/W2746068898', 'https://openalex.org/W2772474126', 'https://openalex.org/W3093276169', 'https://openalex.org/W3099425575', 'https://openalex.org/W2295159313', 'https://openalex.org/W2969457396', 'https://openalex.org/W2995670387', 'https://openalex.org/W3125394132', 'https://openalex.org/W2919624000', 'https://openalex.org/W2905204858', 'https://openalex.org/W3207290297', 'https://openalex.org/W3016250102', 'https://openalex.org/W2475687244', 'https://openalex.org/W3169635929', 'https://openalex.org/W4380136719', 'https://openalex.org/W1556624199', 'https://openalex.org/W3034228289', 'https://openalex.org/W3112317145', 'https://openalex.org/W3154236293', 'https://openalex.org/W3096437652', 'https://openalex.org/W4318351475', 'https://openalex.org/W2753868141', 'https://openalex.org/W2910577860', 'https://openalex.org/W2804809828', 'https://openalex.org/W2752134738', 'https://openalex.org/W2954957095', 'https://openalex.org/W3163031268', 'https://openalex.org/W3201026571', 'https://openalex.org/W3137883189', 'https://openalex.org/W3015499232', 'https://openalex.org/W3175663427', 'https://openalex.org/W4221148975', 'https://openalex.org/W4302076919', 'https://openalex.org/W3112789166', 'https://openalex.org/W2328988907', 'https://openalex.org/W3049247973', 'https://openalex.org/W2963568710', 'https://openalex.org/W3049272330', 'https://openalex.org/W2904399960', 'https://openalex.org/W4225723749', 'https://openalex.org/W2794841650', 'https://openalex.org/W4225716783', 'https://openalex.org/W2953100410', 'https://openalex.org/W4285751616', 'https://openalex.org/W3003673875', 'https://openalex.org/W3105368110', 'https://openalex.org/W2989708046', 'https://openalex.org/W4287688462', 'https://openalex.org/W4385567278']",2023-01-01
https://openalex.org/W4393158405,https://doi.org/10.1609/aaai.v38i1.27810,Music Style Transfer with Time-Varying Inversion of Diffusion Models,"With the development of diffusion models, text-guided image style transfer has demonstrated great controllable and high-quality results. However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music style transfer approach that effectively captures musical attributes using minimal data. We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we utilize a bias-reduced stylization technique to get stable results. Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies. Samples and code are available at https://lsfhuihuiff.github.io/MusicTI/.","['https://openalex.org/W4308860663', 'https://openalex.org/W3196700074', 'https://openalex.org/W2892104732', 'https://openalex.org/W6800706383', 'https://openalex.org/W6846530565', 'https://openalex.org/W3127854286', 'https://openalex.org/W3080857286', 'https://openalex.org/W4289785095', 'https://openalex.org/W4321855146', 'https://openalex.org/W6745564987', 'https://openalex.org/W6811173682', 'https://openalex.org/W4293575120', 'https://openalex.org/W6601851198', 'https://openalex.org/W4318718996', 'https://openalex.org/W2901739194', 'https://openalex.org/W6784002356', 'https://openalex.org/W2902578955', 'https://openalex.org/W6752096078', 'https://openalex.org/W4226317937', 'https://openalex.org/W4368304597', 'https://openalex.org/W4312083936', 'https://openalex.org/W4372263311', 'https://openalex.org/W4378505187', 'https://openalex.org/W6846971355', 'https://openalex.org/W2893613496', 'https://openalex.org/W4385346076', 'https://openalex.org/W4318902838', 'https://openalex.org/W4318752004', 'https://openalex.org/W3196783587', 'https://openalex.org/W4385535331', 'https://openalex.org/W4318351475', 'https://openalex.org/W4319989813', 'https://openalex.org/W2902922910', 'https://openalex.org/W3121370741', 'https://openalex.org/W2952405182', 'https://openalex.org/W2963233633', 'https://openalex.org/W3090569444', 'https://openalex.org/W4312933868', 'https://openalex.org/W4372348948', 'https://openalex.org/W4381786045', 'https://openalex.org/W4380136719', 'https://openalex.org/W2963889406', 'https://openalex.org/W4377719641', 'https://openalex.org/W4360892045', 'https://openalex.org/W4389334989', 'https://openalex.org/W3166396011', 'https://openalex.org/W4386071613', 'https://openalex.org/W4361230700', 'https://openalex.org/W4367365521', 'https://openalex.org/W4318718630', 'https://openalex.org/W2962775980', 'https://openalex.org/W4226278401', 'https://openalex.org/W2797595207', 'https://openalex.org/W4385271055', 'https://openalex.org/W4327811186']",2024-03-24
https://openalex.org/W4385644302,https://doi.org/10.1145/3616855.3635700,GEMRec: Towards Generative Model Recommendation,"Recommender Systems are built to retrieve relevant items to satisfy users' information needs. The candidate corpus usually consists of a finite set of items that are ready to be served, such as videos, products, or articles. With recent advances in Generative AI such as GPT and Diffusion models, a new form of recommendation task is yet to be explored where items are to be created by generative models with personalized prompts. Taking image generation as an example, with a single prompt from the user and access to a generative model, it is possible to generate hundreds of new images in a few minutes. How shall we attain personalization in the presence of ""infinite"" items? In this preliminary study, we propose a two-stage framework, namely Prompt-Model Retrieval and Generated Item Ranking, to approach this new task formulation. We release GEMRec-18K, a prompt-model interaction dataset with 18K images generated by 200 publicly-available generative models paired with a diverse set of 90 textual prompts. Our findings demonstrate the promise of generative model recommendation as a novel personalization problem and the limitations of existing evaluation metrics. We highlight future directions for the RecSys community to advance towards generative recommender systems. Our code and dataset are available at https://github.com/MAPS-research/GEMRec.","['https://openalex.org/W4312933868', 'https://openalex.org/W4386072096', 'https://openalex.org/W4390873054', 'https://openalex.org/W4283388932', 'https://openalex.org/W4402671806', 'https://openalex.org/W4320831569', 'https://openalex.org/W4379260519', 'https://openalex.org/W2140310134', 'https://openalex.org/W3168867926', 'https://openalex.org/W4363672019', 'https://openalex.org/W2187089797', 'https://openalex.org/W4386076215', 'https://openalex.org/W4385535562', 'https://openalex.org/W3166396011', 'https://openalex.org/W4385570204', 'https://openalex.org/W2913754224', 'https://openalex.org/W4289785095', 'https://openalex.org/W4380136719', 'https://openalex.org/W4321855132', 'https://openalex.org/W4327810158', 'https://openalex.org/W4368303308']",2024-03-04
https://openalex.org/W4392904047,https://doi.org/10.1109/icassp48485.2024.10447122,Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models,"Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.","['https://openalex.org/W2752796333', 'https://openalex.org/W6765775151', 'https://openalex.org/W6779823529', 'https://openalex.org/W6786375611', 'https://openalex.org/W4396877837', 'https://openalex.org/W6769627184', 'https://openalex.org/W4225314222', 'https://openalex.org/W4372266552', 'https://openalex.org/W4372260310', 'https://openalex.org/W4401023775', 'https://openalex.org/W6840815571', 'https://openalex.org/W6774212381', 'https://openalex.org/W2998490864', 'https://openalex.org/W6763945542', 'https://openalex.org/W6838327568', 'https://openalex.org/W2972478942', 'https://openalex.org/W2964058413', 'https://openalex.org/W4225293699', 'https://openalex.org/W4362598673', 'https://openalex.org/W4318351475', 'https://openalex.org/W4318719023', 'https://openalex.org/W4281661987', 'https://openalex.org/W4383047490', 'https://openalex.org/W4380136719', 'https://openalex.org/W4383993968', 'https://openalex.org/W3036167779', 'https://openalex.org/W4392903555', 'https://openalex.org/W2963799213', 'https://openalex.org/W2959300817', 'https://openalex.org/W4319452539', 'https://openalex.org/W4288089799']",2024-03-18
https://openalex.org/W4393228744,https://doi.org/10.21428/e4baedd9.8fa181e9,AI for Musical Discovery,"What role should generative AI technology play in music? Long before recent advances, similar questions have been pondered without definitive answers. We argue that the true potential of generative AI lies in cultivating musical discovery, expanding our individual and collective musical horizons. We outline a vision for systems that nurture human creativity, learning, and community. To contend with the richness of music in such contexts, we believe machines will need a kind of musical common sense comprising structural, emotional, and sociocultural factors. Such capabilities characterize human intuitive musicality, but go beyond what current techniques or datasets address. We discuss possible models and strategies for developing new discovery-focused musical tools, drawing on past and ongoing work in our research group ranging from the individual to the community scale. We present this article as an invitation to collectively explore the exciting frontier of AI for musical discovery.","['https://openalex.org/W4318351475', 'https://openalex.org/W4242654914', 'https://openalex.org/W4353061330', 'https://openalex.org/W1556219185', 'https://openalex.org/W1985945240', 'https://openalex.org/W6702669607', 'https://openalex.org/W2000192964', 'https://openalex.org/W2483215953', 'https://openalex.org/W3195577433', 'https://openalex.org/W2788481061', 'https://openalex.org/W1986163437', 'https://openalex.org/W4309117899', 'https://openalex.org/W4226367199', 'https://openalex.org/W2034671641', 'https://openalex.org/W4380136719', 'https://openalex.org/W1702768895', 'https://openalex.org/W2073302931', 'https://openalex.org/W2580221632', 'https://openalex.org/W4287802874', 'https://openalex.org/W4372266552', 'https://openalex.org/W6771763809', 'https://openalex.org/W2052265952', 'https://openalex.org/W2770119437', 'https://openalex.org/W2593116425', 'https://openalex.org/W4386557677', 'https://openalex.org/W3209211564', 'https://openalex.org/W6839332779', 'https://openalex.org/W1507054833', 'https://openalex.org/W2145666791', 'https://openalex.org/W2919624000', 'https://openalex.org/W2023001347', 'https://openalex.org/W6607490660', 'https://openalex.org/W1591470100', 'https://openalex.org/W1504734958', 'https://openalex.org/W6629311719', 'https://openalex.org/W2963305465', 'https://openalex.org/W4366597099', 'https://openalex.org/W6857045109', 'https://openalex.org/W2584032004', 'https://openalex.org/W2327437958', 'https://openalex.org/W2519091744', 'https://openalex.org/W3034228289', 'https://openalex.org/W4230277160', 'https://openalex.org/W2093650095', 'https://openalex.org/W2277018427', 'https://openalex.org/W4226161442', 'https://openalex.org/W2023413025', 'https://openalex.org/W2912128212', 'https://openalex.org/W6647908716', 'https://openalex.org/W3155157105', 'https://openalex.org/W4399395169', 'https://openalex.org/W6752989324', 'https://openalex.org/W6988143973', 'https://openalex.org/W1996499497', 'https://openalex.org/W2070662532', 'https://openalex.org/W164195485', 'https://openalex.org/W2043821646', 'https://openalex.org/W4245652239', 'https://openalex.org/W4388685253', 'https://openalex.org/W4372260310', 'https://openalex.org/W6758973661', 'https://openalex.org/W6851234123', 'https://openalex.org/W6659054917', 'https://openalex.org/W6755406732', 'https://openalex.org/W6800751262', 'https://openalex.org/W6669324098', 'https://openalex.org/W6701573534', 'https://openalex.org/W6838439425', 'https://openalex.org/W6647072268', 'https://openalex.org/W6673896139', 'https://openalex.org/W6646771963', 'https://openalex.org/W6868884132', 'https://openalex.org/W616239202', 'https://openalex.org/W4387427717', 'https://openalex.org/W1559968280', 'https://openalex.org/W2017114040', 'https://openalex.org/W3005713856', 'https://openalex.org/W1479828534', 'https://openalex.org/W2884822436', 'https://openalex.org/W4312319752', 'https://openalex.org/W1599265522', 'https://openalex.org/W3159324329', 'https://openalex.org/W4281926257', 'https://openalex.org/W2950018712', 'https://openalex.org/W1490212016', 'https://openalex.org/W189033833', 'https://openalex.org/W1512123845', 'https://openalex.org/W4402110003', 'https://openalex.org/W4298174969', 'https://openalex.org/W4387872955', 'https://openalex.org/W296357383']",2024-03-27
https://openalex.org/W4404135183,https://doi.org/10.1016/j.chbah.2024.100101,How voice and helpfulness shape perceptions in human–agent teams,"Voice assistants are increasingly prevalent, from personal devices to team environments. This study explores how voice type and contribution quality influence human–agent team performance and perceptions of anthropomorphism, animacy, intelligence, and trustworthiness. By manipulating both, we reveal mechanisms of perception and clarify ambiguity in previous work. Our results show that the human resemblance of a voice assistant’s voice negatively interacts with the helpfulness of an agent’s contribution to flip its effect on perceived anthropomorphism and perceived animacy. This means human teammates interpret the agent’s contributions differently depending on its voice. Our study found no significant effect of voice on perceived intelligence, trustworthiness, or team performance. We find differences in these measures are caused by manipulating the helpfulness of an agent. These findings suggest that function matters more than form when designing agents for high-performing human–agent teams, but controlling perceptions of anthropomorphism and animacy can be unpredictable even with high human resemblance.","['https://openalex.org/W4283641801', 'https://openalex.org/W4214810114', 'https://openalex.org/W2029308871', 'https://openalex.org/W2164725482', 'https://openalex.org/W2032568497', 'https://openalex.org/W3135619344', 'https://openalex.org/W6637623960', 'https://openalex.org/W6680920554', 'https://openalex.org/W2920996501', 'https://openalex.org/W2986334266', 'https://openalex.org/W6803368495', 'https://openalex.org/W6853096648', 'https://openalex.org/W6748398305', 'https://openalex.org/W2734963796', 'https://openalex.org/W2005401218', 'https://openalex.org/W2499526086', 'https://openalex.org/W1991044014', 'https://openalex.org/W3207331896', 'https://openalex.org/W2010158189', 'https://openalex.org/W2574287525', 'https://openalex.org/W2115040353', 'https://openalex.org/W2155246433', 'https://openalex.org/W3200832617', 'https://openalex.org/W3216249220', 'https://openalex.org/W4367550478', 'https://openalex.org/W4231399809', 'https://openalex.org/W3006245018', 'https://openalex.org/W2135907989', 'https://openalex.org/W6654516771', 'https://openalex.org/W3127356397', 'https://openalex.org/W2024320470', 'https://openalex.org/W2225036064', 'https://openalex.org/W2062585762', 'https://openalex.org/W3036668652', 'https://openalex.org/W2026272086', 'https://openalex.org/W6679396994', 'https://openalex.org/W2972387638', 'https://openalex.org/W4243342770', 'https://openalex.org/W2058624789', 'https://openalex.org/W2141373701', 'https://openalex.org/W1975950487', 'https://openalex.org/W2136407476', 'https://openalex.org/W2770836081', 'https://openalex.org/W2620350838', 'https://openalex.org/W2035546865', 'https://openalex.org/W3034161046', 'https://openalex.org/W2620920482', 'https://openalex.org/W1991015565', 'https://openalex.org/W4362588460', 'https://openalex.org/W6804085326', 'https://openalex.org/W2314033003', 'https://openalex.org/W2063052894', 'https://openalex.org/W3123303846', 'https://openalex.org/W3163271954', 'https://openalex.org/W2039376592', 'https://openalex.org/W4205928526', 'https://openalex.org/W3216894215', 'https://openalex.org/W2164171543', 'https://openalex.org/W2517471910', 'https://openalex.org/W3159367349', 'https://openalex.org/W3187653187', 'https://openalex.org/W4212899454', 'https://openalex.org/W2274175710', 'https://openalex.org/W2912159753', 'https://openalex.org/W6760868126', 'https://openalex.org/W2991175618', 'https://openalex.org/W2557592112', 'https://openalex.org/W6842428671', 'https://openalex.org/W3009578469', 'https://openalex.org/W2942157335', 'https://openalex.org/W3120142301', 'https://openalex.org/W4380136719', 'https://openalex.org/W4295124108', 'https://openalex.org/W4241818697', 'https://openalex.org/W2926006265', 'https://openalex.org/W2017180264', 'https://openalex.org/W4382239349', 'https://openalex.org/W3205692592']",2024-08-01
https://openalex.org/W4392902987,https://doi.org/10.1109/icassp48485.2024.10448249,Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation,"Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.","['https://openalex.org/W6855691466', 'https://openalex.org/W3092850823', 'https://openalex.org/W2772474126', 'https://openalex.org/W4372259826', 'https://openalex.org/W6760601182', 'https://openalex.org/W6765759998', 'https://openalex.org/W6849105126', 'https://openalex.org/W6849109464', 'https://openalex.org/W6853096648', 'https://openalex.org/W6852824296', 'https://openalex.org/W6849635556', 'https://openalex.org/W6852971826', 'https://openalex.org/W6852872555', 'https://openalex.org/W6749351710', 'https://openalex.org/W2963253162', 'https://openalex.org/W3048062783', 'https://openalex.org/W6782288222', 'https://openalex.org/W6810995942', 'https://openalex.org/W6793578827', 'https://openalex.org/W6751598888', 'https://openalex.org/W2963408210', 'https://openalex.org/W3173187964', 'https://openalex.org/W4385245566', 'https://openalex.org/W6844035133', 'https://openalex.org/W6744627333', 'https://openalex.org/W4221148975', 'https://openalex.org/W2805697608', 'https://openalex.org/W4378499140', 'https://openalex.org/W4287250916', 'https://openalex.org/W4378942405', 'https://openalex.org/W4298182583', 'https://openalex.org/W4380136719', 'https://openalex.org/W4377140494', 'https://openalex.org/W4318351475', 'https://openalex.org/W4318752004', 'https://openalex.org/W4319989813', 'https://openalex.org/W4385328213']",2024-03-18
https://openalex.org/W4392909509,https://doi.org/10.1109/icassp48485.2024.10447950,GPT-4 Driven Cinematic Music Generation Through Text Processing,"This paper presents Herrmann-1 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> , a multimodal framework to generate background music tailored to movie scenes, by integrating state-of-the-art vision, language, music, and speech processing models. Our pipeline begins by extracting visual and speech information from a movie scene, performing emotional analysis on it, and converting these into descriptive texts. Then, GPT-4 translates these high-level descriptions into low-level music conditions. Finally, these text-based music conditions guide a text-to-music model to generate music that resonates with input movie scenes. Comprehensive objective and subjective evaluations attest to the high synthesis quality, congruence, and superiority of our pipeline.","['https://openalex.org/W4393148499', 'https://openalex.org/W4390873340', 'https://openalex.org/W3207290297', 'https://openalex.org/W4280599323', 'https://openalex.org/W2079557775', 'https://openalex.org/W6791353385', 'https://openalex.org/W6790019176', 'https://openalex.org/W4214663214', 'https://openalex.org/W6777481484', 'https://openalex.org/W3107128832', 'https://openalex.org/W6850204008', 'https://openalex.org/W6811013733', 'https://openalex.org/W6849177959', 'https://openalex.org/W6811072154', 'https://openalex.org/W2909791946', 'https://openalex.org/W6804336870', 'https://openalex.org/W2997377425', 'https://openalex.org/W6847363464', 'https://openalex.org/W3015783745', 'https://openalex.org/W6849739172', 'https://openalex.org/W6849105126', 'https://openalex.org/W6849635556', 'https://openalex.org/W6853533739', 'https://openalex.org/W6853096648', 'https://openalex.org/W6789403026', 'https://openalex.org/W6757220786', 'https://openalex.org/W2526050071', 'https://openalex.org/W6763362620', 'https://openalex.org/W2948574791', 'https://openalex.org/W3166396011', 'https://openalex.org/W4380136719', 'https://openalex.org/W4289106098', 'https://openalex.org/W4319989813', 'https://openalex.org/W4320458302', 'https://openalex.org/W4287777632', 'https://openalex.org/W4320342491', 'https://openalex.org/W2992252352', 'https://openalex.org/W4318351475', 'https://openalex.org/W4226182655', 'https://openalex.org/W4311000453', 'https://openalex.org/W4379251846', 'https://openalex.org/W4318718936', 'https://openalex.org/W4229042118', 'https://openalex.org/W3126337491', 'https://openalex.org/W4287372095']",2024-03-18
https://openalex.org/W4389518683,https://doi.org/10.18653/v1/2023.findings-emnlp.898,Sound of Story: Multi-modal Storytelling with Audio,"Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of the story. Therefore, we propose to extend story understanding and telling areas by establishing a new component called background sound which is story context-based audio without any linguistic information. For this purpose, we introduce a new dataset, called Sound of Story (SoS), which has paired image and text sequences with corresponding sound or background music for a story. To the best of our knowledge, this is the largest well-curated dataset for storytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6 images per story and 984 hours of speech-decoupled audio such as background music and other sounds. As benchmark tasks for storytelling with sound and the dataset, we propose retrieval tasks between modalities, and audio generation tasks from image-text sequences, introducing strong baselines for them. We believe the proposed dataset and tasks may shed light on the multi-modal understanding of storytelling in terms of sound.","['https://openalex.org/W3010277541', 'https://openalex.org/W4386071687', 'https://openalex.org/W4214819138', 'https://openalex.org/W4323570346', 'https://openalex.org/W4398958419', 'https://openalex.org/W2038484192', 'https://openalex.org/W4296406182', 'https://openalex.org/W3103662468', 'https://openalex.org/W4286980120', 'https://openalex.org/W2078238240', 'https://openalex.org/W4315570819', 'https://openalex.org/W4309501214', 'https://openalex.org/W3196974791', 'https://openalex.org/W2963305465', 'https://openalex.org/W4307933152', 'https://openalex.org/W4312933868', 'https://openalex.org/W2137343183', 'https://openalex.org/W4319300251', 'https://openalex.org/W4320831569', 'https://openalex.org/W46948191', 'https://openalex.org/W2963890755', 'https://openalex.org/W3044283505', 'https://openalex.org/W2526050071', 'https://openalex.org/W2963963993', 'https://openalex.org/W4372260330', 'https://openalex.org/W2047170290', 'https://openalex.org/W2129069237', 'https://openalex.org/W2963096510', 'https://openalex.org/W4284898017', 'https://openalex.org/W2948211236', 'https://openalex.org/W4300980117', 'https://openalex.org/W4313887138', 'https://openalex.org/W2883891001', 'https://openalex.org/W3035552787', 'https://openalex.org/W2178339699', 'https://openalex.org/W4307008216', 'https://openalex.org/W4372266552', 'https://openalex.org/W3128334185', 'https://openalex.org/W4377865169', 'https://openalex.org/W4380136719', 'https://openalex.org/W2962968152', 'https://openalex.org/W4318718630', 'https://openalex.org/W3107806026', 'https://openalex.org/W4324107482', 'https://openalex.org/W3037859269', 'https://openalex.org/W4318351475', 'https://openalex.org/W2965458216', 'https://openalex.org/W2466175319', 'https://openalex.org/W3188174792', 'https://openalex.org/W4285265603', 'https://openalex.org/W2593116425', 'https://openalex.org/W4319300260', 'https://openalex.org/W3166396011', 'https://openalex.org/W2425121537', 'https://openalex.org/W3133614184', 'https://openalex.org/W4375869190', 'https://openalex.org/W4289538158', 'https://openalex.org/W4381786045', 'https://openalex.org/W2086384421', 'https://openalex.org/W4285606530']",2023-01-01
https://openalex.org/W4392904268,https://doi.org/10.1109/icassp48485.2024.10448078,Ainur: Harmonizing Speed and Quality in Deep Music Generation Through Lyrics-Audio Embeddings,"In the domain of music generation, prevailing methods focus on text-to-music tasks, predominantly relying on diffusion models. However, they fail to achieve good vocal quality in synthetic music compositions.To tackle this critical challenge, we present Ainur, a hierarchical diffusion model that concentrates on the lyrics-to-music generation task. Through its use of multimodal Lyrics-Audio Spectrogram Pre-training (CLASP) embeddings, Ainur distinguishes itself from past approaches by specifically enhancing the vocal quality of synthetically produced music. Notably, Ainur's training and testing processes are highly efficient, requiring only a single GPU. According to experimental results, Ainur meets or exceeds the quality of other state-of-the-art models like MusicGen, MusicLM, and AudioLDM2 in both objective and subjective evaluations. Additionally, Ainur offers near real-time inference speed, which facilitate its use in practical, real-world applications.","['https://openalex.org/W4300980117', 'https://openalex.org/W3094502228', 'https://openalex.org/W4381786045', 'https://openalex.org/W4221159371', 'https://openalex.org/W3006926732', 'https://openalex.org/W4367359628', 'https://openalex.org/W3166396011', 'https://openalex.org/W4385775276', 'https://openalex.org/W4318902838', 'https://openalex.org/W4288089799', 'https://openalex.org/W4318718630', 'https://openalex.org/W4287802874', 'https://openalex.org/W4372266552', 'https://openalex.org/W4318752004', 'https://openalex.org/W2526050071', 'https://openalex.org/W2979826702', 'https://openalex.org/W4318351475', 'https://openalex.org/W4285483774', 'https://openalex.org/W4380136719']",2024-03-18
https://openalex.org/W4393145938,https://doi.org/10.1609/aaai.v38i21.30558,Deep Learning for Style Transfer and Experimentation with Audio Effects and Music Creation,"Recent advancements in deep learning have the potential to transform the process of writing and creating music. Models that have the potential to capture and analyze higher-level representations of music and audio can serve to change the field of digital signal processing. In this statement, I propose a set of Music+AI methods that serves to assist with the writing of and melodies, modelling and transferring of timbres, applying a wide variety of audio effects, including research into experimental audio effects, and production of audio samples using style transfers. Writing and producing music is a tedious task that is notably difficult to become proficient in, as many tools to create music both cost sums money and require long-term commitments to study. An all-encompassing framework for music processing would make the process much more accessible and simple and would allow for human art to work alongside technology to advance.","['https://openalex.org/W4380136719', 'https://openalex.org/W4287802874', 'https://openalex.org/W2963889406', 'https://openalex.org/W4307977691', 'https://openalex.org/W3092316096', 'https://openalex.org/W4308538393', 'https://openalex.org/W4402112510', 'https://openalex.org/W2896457183']",2024-03-24
https://openalex.org/W4399530928,https://doi.org/10.1109/nice61972.2024.10549580,Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input,"Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.","['https://openalex.org/W2016574277', 'https://openalex.org/W2968243907', 'https://openalex.org/W4283753774', 'https://openalex.org/W2757430550', 'https://openalex.org/W3035212231', 'https://openalex.org/W4386076494', 'https://openalex.org/W4312280868', 'https://openalex.org/W3009453333', 'https://openalex.org/W4229060330', 'https://openalex.org/W4293756458', 'https://openalex.org/W6967576201', 'https://openalex.org/W4387300055', 'https://openalex.org/W4390993582', 'https://openalex.org/W4402915744', 'https://openalex.org/W2745933219', 'https://openalex.org/W2425121537', 'https://openalex.org/W2567239141', 'https://openalex.org/W3095334530', 'https://openalex.org/W3196480172', 'https://openalex.org/W6755494875', 'https://openalex.org/W6856129569', 'https://openalex.org/W3188021906', 'https://openalex.org/W3034469298', 'https://openalex.org/W6855339026', 'https://openalex.org/W3114635118', 'https://openalex.org/W6783508657', 'https://openalex.org/W6679045638', 'https://openalex.org/W6779823529', 'https://openalex.org/W4387195417', 'https://openalex.org/W4360884927', 'https://openalex.org/W6853096648', 'https://openalex.org/W4390874263', 'https://openalex.org/W4390873550', 'https://openalex.org/W6809885388', 'https://openalex.org/W4312933868', 'https://openalex.org/W4386065764', 'https://openalex.org/W6856700853', 'https://openalex.org/W6845281891', 'https://openalex.org/W3176481196', 'https://openalex.org/W4312614039', 'https://openalex.org/W4386076661', 'https://openalex.org/W4390872862', 'https://openalex.org/W3191528047', 'https://openalex.org/W4308235769', 'https://openalex.org/W4292787316', 'https://openalex.org/W4390873758', 'https://openalex.org/W2998281665', 'https://openalex.org/W4386076324', 'https://openalex.org/W6792408638', 'https://openalex.org/W6758151150', 'https://openalex.org/W4304891868', 'https://openalex.org/W6851907563', 'https://openalex.org/W3147919432', 'https://openalex.org/W4390872297', 'https://openalex.org/W6838639034', 'https://openalex.org/W6840815571', 'https://openalex.org/W6788990321', 'https://openalex.org/W6748816842', 'https://openalex.org/W6755977528', 'https://openalex.org/W6674330103', 'https://openalex.org/W2147800946', 'https://openalex.org/W6766978945', 'https://openalex.org/W6631190155', 'https://openalex.org/W6757817989', 'https://openalex.org/W206948248', 'https://openalex.org/W3122190729', 'https://openalex.org/W2899663614', 'https://openalex.org/W4224035735', 'https://openalex.org/W1522301498', 'https://openalex.org/W3036167779', 'https://openalex.org/W4387156659', 'https://openalex.org/W4380136719', 'https://openalex.org/W4295312788', 'https://openalex.org/W2786672974', 'https://openalex.org/W4288099666', 'https://openalex.org/W4365456854', 'https://openalex.org/W2095705004', 'https://openalex.org/W4385681544', 'https://openalex.org/W2910135751', 'https://openalex.org/W4401416682', 'https://openalex.org/W2914304175', 'https://openalex.org/W2908510526', 'https://openalex.org/W3141758171', 'https://openalex.org/W4281485151', 'https://openalex.org/W4303440777']",2024-04-23
https://openalex.org/W4404520044,https://doi.org/10.1145/3680530.3695456,KnitworkVR: Dual-reality Experience through Distributed Sensor-Actuator Networks in the Living Knitwork Pavilion,,"['https://openalex.org/W2319158947', 'https://openalex.org/W4380136719', 'https://openalex.org/W2739873567', 'https://openalex.org/W4210337797', 'https://openalex.org/W2160971146', 'https://openalex.org/W3162308512', 'https://openalex.org/W4236806994', 'https://openalex.org/W4386609419', 'https://openalex.org/W3045454034', 'https://openalex.org/W3169642793', 'https://openalex.org/W4283165056']",2024-11-19
https://openalex.org/W4408787663,https://doi.org/10.1080/10447318.2025.2478265,"A Meta-Methodology for User Evaluation of Artificial Intelligence Generated Music; Using the Analytical Hierarchy Process, Likert and Emotional State Estimations",,"['https://openalex.org/W2022460558', 'https://openalex.org/W4309214211', 'https://openalex.org/W1979720880', 'https://openalex.org/W4221139477', 'https://openalex.org/W2775487773', 'https://openalex.org/W2767186077', 'https://openalex.org/W38457614', 'https://openalex.org/W4306317440', 'https://openalex.org/W2903214502', 'https://openalex.org/W4286250570', 'https://openalex.org/W4400768137', 'https://openalex.org/W4401787828', 'https://openalex.org/W4380136719', 'https://openalex.org/W3023498982', 'https://openalex.org/W2990356913', 'https://openalex.org/W3161647083', 'https://openalex.org/W4221045209', 'https://openalex.org/W4407082126', 'https://openalex.org/W2044035095', 'https://openalex.org/W6781423933', 'https://openalex.org/W4406733248', 'https://openalex.org/W2042266440', 'https://openalex.org/W2769503652', 'https://openalex.org/W4362558555', 'https://openalex.org/W6774160373', 'https://openalex.org/W4386337496', 'https://openalex.org/W2889370398', 'https://openalex.org/W2790511979', 'https://openalex.org/W2560316200', 'https://openalex.org/W2898148140', 'https://openalex.org/W3096414595', 'https://openalex.org/W2885403181', 'https://openalex.org/W4281855428', 'https://openalex.org/W3188889501', 'https://openalex.org/W3145867403', 'https://openalex.org/W3200417041', 'https://openalex.org/W4386569931', 'https://openalex.org/W3015625764', 'https://openalex.org/W2147223282', 'https://openalex.org/W6685926591', 'https://openalex.org/W6636113898', 'https://openalex.org/W3198870284', 'https://openalex.org/W2972478942', 'https://openalex.org/W3200241438', 'https://openalex.org/W2036980203', 'https://openalex.org/W3008438970', 'https://openalex.org/W4289600570', 'https://openalex.org/W6719310969', 'https://openalex.org/W4384517588', 'https://openalex.org/W4317493180', 'https://openalex.org/W2616399473', 'https://openalex.org/W4224314831', 'https://openalex.org/W2160473997', 'https://openalex.org/W6997531736', 'https://openalex.org/W1975955223', 'https://openalex.org/W2894993706', 'https://openalex.org/W2028443633', 'https://openalex.org/W2980594093', 'https://openalex.org/W2344064156', 'https://openalex.org/W2752731013', 'https://openalex.org/W2091254174', 'https://openalex.org/W4353072309', 'https://openalex.org/W1972200139', 'https://openalex.org/W2294683276', 'https://openalex.org/W3198074494', 'https://openalex.org/W2963313917', 'https://openalex.org/W2898827701', 'https://openalex.org/W4243095255', 'https://openalex.org/W2183300284', 'https://openalex.org/W3099425575', 'https://openalex.org/W2783539613', 'https://openalex.org/W1595126664', 'https://openalex.org/W3206173303', 'https://openalex.org/W4401359619', 'https://openalex.org/W1601919122', 'https://openalex.org/W3143835353', 'https://openalex.org/W4235789871']",2025-03-24
https://openalex.org/W4412507263,https://doi.org/10.1016/j.inffus.2025.103517,Foundation models and Transformers for anomaly detection: A survey,,"['https://openalex.org/W2084512860', 'https://openalex.org/W2278186031', 'https://openalex.org/W4205471456', 'https://openalex.org/W3184778778', 'https://openalex.org/W4390875033', 'https://openalex.org/W6805504937', 'https://openalex.org/W2919115771', 'https://openalex.org/W6654536944', 'https://openalex.org/W3146620971', 'https://openalex.org/W3169651898', 'https://openalex.org/W4394625793', 'https://openalex.org/W4390873025', 'https://openalex.org/W4214694907', 'https://openalex.org/W6785596320', 'https://openalex.org/W4386071848', 'https://openalex.org/W6810601475', 'https://openalex.org/W4402716375', 'https://openalex.org/W6809993782', 'https://openalex.org/W6853299294', 'https://openalex.org/W4379470918', 'https://openalex.org/W4390886362', 'https://openalex.org/W3214920362', 'https://openalex.org/W4205388843', 'https://openalex.org/W4391614403', 'https://openalex.org/W4409507112', 'https://openalex.org/W3196890755', 'https://openalex.org/W3184357402', 'https://openalex.org/W6864640105', 'https://openalex.org/W6847423516', 'https://openalex.org/W4321093365', 'https://openalex.org/W4396243431', 'https://openalex.org/W4316660910', 'https://openalex.org/W4377145718', 'https://openalex.org/W6852900310', 'https://openalex.org/W4288886033', 'https://openalex.org/W4281478450', 'https://openalex.org/W4288046542', 'https://openalex.org/W4200597473', 'https://openalex.org/W4391264797', 'https://openalex.org/W4388440196', 'https://openalex.org/W4386113288', 'https://openalex.org/W6800244179', 'https://openalex.org/W6843642380', 'https://openalex.org/W4386076578', 'https://openalex.org/W6793825583', 'https://openalex.org/W4287009635', 'https://openalex.org/W4317669908', 'https://openalex.org/W6775494149', 'https://openalex.org/W4225370003', 'https://openalex.org/W4322628180', 'https://openalex.org/W4392379948', 'https://openalex.org/W4392693771', 'https://openalex.org/W4382360125', 'https://openalex.org/W6872329898', 'https://openalex.org/W6871717491', 'https://openalex.org/W4390558448', 'https://openalex.org/W6857763825', 'https://openalex.org/W4385195017', 'https://openalex.org/W4283331368', 'https://openalex.org/W6809723540', 'https://openalex.org/W4390880461', 'https://openalex.org/W4388714230', 'https://openalex.org/W6853671139', 'https://openalex.org/W4388622902', 'https://openalex.org/W4387415072', 'https://openalex.org/W4292793943', 'https://openalex.org/W4385805191', 'https://openalex.org/W6853882915', 'https://openalex.org/W6863343937', 'https://openalex.org/W6873610565', 'https://openalex.org/W4386065608', 'https://openalex.org/W4386071635', 'https://openalex.org/W2987228832', 'https://openalex.org/W4313327905', 'https://openalex.org/W6850081247', 'https://openalex.org/W4320913379', 'https://openalex.org/W3190318906', 'https://openalex.org/W6858770149', 'https://openalex.org/W6847375115', 'https://openalex.org/W4402183627', 'https://openalex.org/W4385805156', 'https://openalex.org/W6847886332', 'https://openalex.org/W4391048112', 'https://openalex.org/W4385486148', 'https://openalex.org/W4313555484', 'https://openalex.org/W4386632296', 'https://openalex.org/W4388580593', 'https://openalex.org/W4403791158', 'https://openalex.org/W4394625837', 'https://openalex.org/W6872371936', 'https://openalex.org/W4403780904', 'https://openalex.org/W4405093242', 'https://openalex.org/W4383695414', 'https://openalex.org/W4402733578', 'https://openalex.org/W4402753378', 'https://openalex.org/W6869952721', 'https://openalex.org/W4385801084', 'https://openalex.org/W4285043061', 'https://openalex.org/W6802062233', 'https://openalex.org/W6840256930', 'https://openalex.org/W6872597200', 'https://openalex.org/W6769627184', 'https://openalex.org/W3085139254', 'https://openalex.org/W4206706211', 'https://openalex.org/W6639204139', 'https://openalex.org/W2097117768', 'https://openalex.org/W6842900917', 'https://openalex.org/W6802647179', 'https://openalex.org/W6703852671', 'https://openalex.org/W6743674261', 'https://openalex.org/W6855673971', 'https://openalex.org/W4401024146', 'https://openalex.org/W4403601156', 'https://openalex.org/W4409356433', 'https://openalex.org/W6600103761', 'https://openalex.org/W3176309086', 'https://openalex.org/W2057910578', 'https://openalex.org/W4221161778', 'https://openalex.org/W6779018880', 'https://openalex.org/W3168984673', 'https://openalex.org/W4214493665', 'https://openalex.org/W6844479167', 'https://openalex.org/W2005876975', 'https://openalex.org/W4388915719', 'https://openalex.org/W6811013733', 'https://openalex.org/W4386071707', 'https://openalex.org/W3198377975', 'https://openalex.org/W6810254323', 'https://openalex.org/W4312889062', 'https://openalex.org/W4386071547', 'https://openalex.org/W3217340782', 'https://openalex.org/W2963811535', 'https://openalex.org/W6800895557', 'https://openalex.org/W6748382702', 'https://openalex.org/W4390872136', 'https://openalex.org/W4402703029', 'https://openalex.org/W3126337491', 'https://openalex.org/W3131500599', 'https://openalex.org/W4312530853', 'https://openalex.org/W4400070502', 'https://openalex.org/W4403565644', 'https://openalex.org/W4221140369', 'https://openalex.org/W4367365477', 'https://openalex.org/W4205376232', 'https://openalex.org/W4366850747', 'https://openalex.org/W4312788538', 'https://openalex.org/W3134652006', 'https://openalex.org/W3118895125', 'https://openalex.org/W4406735535', 'https://openalex.org/W4385473486', 'https://openalex.org/W4281803202', 'https://openalex.org/W4382322597', 'https://openalex.org/W2559927751', 'https://openalex.org/W3195577433', 'https://openalex.org/W4308623278', 'https://openalex.org/W4386351457', 'https://openalex.org/W3034648032', 'https://openalex.org/W2978971541', 'https://openalex.org/W4286910290', 'https://openalex.org/W4309875327', 'https://openalex.org/W3129576130', 'https://openalex.org/W4225323055', 'https://openalex.org/W4382240180', 'https://openalex.org/W4320711995', 'https://openalex.org/W4404612908', 'https://openalex.org/W4312420092', 'https://openalex.org/W3216908833', 'https://openalex.org/W3159663321', 'https://openalex.org/W4386065775', 'https://openalex.org/W2884585870', 'https://openalex.org/W4310286673', 'https://openalex.org/W4206174637', 'https://openalex.org/W4388482029', 'https://openalex.org/W1583912456', 'https://openalex.org/W2969262604', 'https://openalex.org/W4399204185', 'https://openalex.org/W3205270560', 'https://openalex.org/W3091346723', 'https://openalex.org/W4380353763', 'https://openalex.org/W3159481202', 'https://openalex.org/W4386597062', 'https://openalex.org/W4365460740', 'https://openalex.org/W2962974533', 'https://openalex.org/W4313156423', 'https://openalex.org/W1901129140', 'https://openalex.org/W4411244487', 'https://openalex.org/W4292779060', 'https://openalex.org/W4386699337', 'https://openalex.org/W3207639163', 'https://openalex.org/W4382317531', 'https://openalex.org/W3166166117', 'https://openalex.org/W3147387781', 'https://openalex.org/W4393158476', 'https://openalex.org/W4388482628', 'https://openalex.org/W4380994256', 'https://openalex.org/W4353115070', 'https://openalex.org/W4399929809', 'https://openalex.org/W4402727956', 'https://openalex.org/W2910068345', 'https://openalex.org/W4391168247', 'https://openalex.org/W2183341477', 'https://openalex.org/W4391940723', 'https://openalex.org/W2896457183', 'https://openalex.org/W3166396011', 'https://openalex.org/W4301039631', 'https://openalex.org/W4393153999', 'https://openalex.org/W4221140561', 'https://openalex.org/W2963420686', 'https://openalex.org/W4366493107', 'https://openalex.org/W4365395996', 'https://openalex.org/W3138516171', 'https://openalex.org/W4292264187', 'https://openalex.org/W4413967215', 'https://openalex.org/W4405955451', 'https://openalex.org/W4312480274', 'https://openalex.org/W1583837637', 'https://openalex.org/W3094502228', 'https://openalex.org/W1686810756', 'https://openalex.org/W4404658388', 'https://openalex.org/W4386065385', 'https://openalex.org/W4385970122', 'https://openalex.org/W3169077988', 'https://openalex.org/W4231109964', 'https://openalex.org/W4234842379', 'https://openalex.org/W4387928922', 'https://openalex.org/W3170874841', 'https://openalex.org/W4389712919', 'https://openalex.org/W3148722492', 'https://openalex.org/W4394593160', 'https://openalex.org/W4306313147', 'https://openalex.org/W4214634256', 'https://openalex.org/W4385245566', 'https://openalex.org/W4287025617', 'https://openalex.org/W4404852513', 'https://openalex.org/W4362515116', 'https://openalex.org/W4308021623', 'https://openalex.org/W4287753132', 'https://openalex.org/W4380136719', 'https://openalex.org/W4288089799', 'https://openalex.org/W4388761943', 'https://openalex.org/W4362509536', 'https://openalex.org/W4388482448', 'https://openalex.org/W4388093086', 'https://openalex.org/W4402733597', 'https://openalex.org/W4255840386', 'https://openalex.org/W4306820534', 'https://openalex.org/W4380994575', 'https://openalex.org/W4253695868', 'https://openalex.org/W4391421044', 'https://openalex.org/W3211490618', 'https://openalex.org/W4312933868', 'https://openalex.org/W4386113989', 'https://openalex.org/W4286588141', 'https://openalex.org/W2775293244', 'https://openalex.org/W4226278401', 'https://openalex.org/W4281643792', 'https://openalex.org/W4285191490', 'https://openalex.org/W4407800509', 'https://openalex.org/W3159648608', 'https://openalex.org/W4388093160', 'https://openalex.org/W4255556797', 'https://openalex.org/W3212044949', 'https://openalex.org/W3179869055', 'https://openalex.org/W2601530120', 'https://openalex.org/W4394785917', 'https://openalex.org/W1821462560', 'https://openalex.org/W4318718936', 'https://openalex.org/W4405127308', 'https://openalex.org/W4309864938', 'https://openalex.org/W4387294588', 'https://openalex.org/W4366208220']",2025-07-21
https://openalex.org/W4412995570,https://doi.org/10.3390/info16080656,From Tools to Creators: A Review on the Development and Application of Artificial Intelligence Music Generation,"Artificial intelligence (AI) has emerged as a significant driving force in the development of technology and industry. It is also integrated with music as music AI in music generation and analysis. It originated from early algorithmic composition techniques in the mid-20th century. Recent advancements in machine learning and neural networks have enabled innovative music generation and exploration. This article surveys the development history and technical route of music AI, analyzes the current status and limitations of music artificial intelligence across various areas, including music generation and composition, rehabilitation and treatment, as well as education and learning. It reveals that music AI has become a promising creator in the field of music generation. The influence of music AI on the music industry and the challenges it encounters are explored. Additionally, an emotional music generation system driven by multimodal signals is proposed. Although music artificial intelligence technology still needs to be further improved, with the continuous breakthroughs in technology, it will have a more profound impact on all areas of music.","['https://openalex.org/W4399939019', 'https://openalex.org/W4241118706', 'https://openalex.org/W2982753834', 'https://openalex.org/W6811189141', 'https://openalex.org/W4387053528', 'https://openalex.org/W3108590809', 'https://openalex.org/W4405180710', 'https://openalex.org/W4391516278', 'https://openalex.org/W2775487773', 'https://openalex.org/W6785053800', 'https://openalex.org/W3034228289', 'https://openalex.org/W2130225697', 'https://openalex.org/W2091863045', 'https://openalex.org/W3044117879', 'https://openalex.org/W1582059369', 'https://openalex.org/W2151245740', 'https://openalex.org/W2514141612', 'https://openalex.org/W975403785', 'https://openalex.org/W2142996485', 'https://openalex.org/W2919620454', 'https://openalex.org/W6605965525', 'https://openalex.org/W2035447461', 'https://openalex.org/W2621854987', 'https://openalex.org/W2980033274', 'https://openalex.org/W2067621398', 'https://openalex.org/W2174443198', 'https://openalex.org/W2137619888', 'https://openalex.org/W4309674289', 'https://openalex.org/W2560316200', 'https://openalex.org/W2992790584', 'https://openalex.org/W2772474126', 'https://openalex.org/W6950401703', 'https://openalex.org/W2792210438', 'https://openalex.org/W2975076716', 'https://openalex.org/W6853096648', 'https://openalex.org/W4399868806', 'https://openalex.org/W2056551148', 'https://openalex.org/W2566969886', 'https://openalex.org/W2326610521', 'https://openalex.org/W2013606862', 'https://openalex.org/W2010769244', 'https://openalex.org/W6652584369', 'https://openalex.org/W6773807743', 'https://openalex.org/W4391924849', 'https://openalex.org/W2051505710', 'https://openalex.org/W3025787433', 'https://openalex.org/W2105217850', 'https://openalex.org/W1659842140', 'https://openalex.org/W2017679785', 'https://openalex.org/W6605505536', 'https://openalex.org/W2069143585', 'https://openalex.org/W2064675550', 'https://openalex.org/W2951111322', 'https://openalex.org/W6739901393', 'https://openalex.org/W3038344852', 'https://openalex.org/W3015625764', 'https://openalex.org/W4391660208', 'https://openalex.org/W2129069237', 'https://openalex.org/W3163211956', 'https://openalex.org/W3094242209', 'https://openalex.org/W4413496470', 'https://openalex.org/W2040611779', 'https://openalex.org/W2996102005', 'https://openalex.org/W3132280322', 'https://openalex.org/W3166775656', 'https://openalex.org/W6795650314', 'https://openalex.org/W2911752852', 'https://openalex.org/W3003571748', 'https://openalex.org/W3130788903', 'https://openalex.org/W3007959348', 'https://openalex.org/W3160114885', 'https://openalex.org/W3132322741', 'https://openalex.org/W3164797975', 'https://openalex.org/W2078773720', 'https://openalex.org/W4293657302', 'https://openalex.org/W3215743154', 'https://openalex.org/W4220786059', 'https://openalex.org/W6878364695', 'https://openalex.org/W4406951622', 'https://openalex.org/W4390703501', 'https://openalex.org/W4206484422', 'https://openalex.org/W6858526008', 'https://openalex.org/W959181621', 'https://openalex.org/W6676713131', 'https://openalex.org/W3014731244', 'https://openalex.org/W3145867403', 'https://openalex.org/W2921595788', 'https://openalex.org/W4391774550', 'https://openalex.org/W2968915073', 'https://openalex.org/W3208960754', 'https://openalex.org/W3000676613', 'https://openalex.org/W4283208072', 'https://openalex.org/W4376607936', 'https://openalex.org/W4404238545', 'https://openalex.org/W4293426679', 'https://openalex.org/W4412119716', 'https://openalex.org/W4408039472', 'https://openalex.org/W4405700988', 'https://openalex.org/W4396929191', 'https://openalex.org/W4401682885', 'https://openalex.org/W6603143895', 'https://openalex.org/W3020487153', 'https://openalex.org/W4251591997', 'https://openalex.org/W2963350032', 'https://openalex.org/W4403063382', 'https://openalex.org/W4389904109', 'https://openalex.org/W3164748250', 'https://openalex.org/W4283326183', 'https://openalex.org/W4380136719', 'https://openalex.org/W134527144', 'https://openalex.org/W3096784947', 'https://openalex.org/W4388793704', 'https://openalex.org/W4408669860', 'https://openalex.org/W149236175', 'https://openalex.org/W3099425575']",2025-07-31
https://openalex.org/W4413440756,https://doi.org/10.1177/27523543251371153,Scoring Synthetic Media: Library Music in the Viral “A.I. Film” Trend,"Of the many new forms of synthetic media, one trend to garner especially sustained attention in the popular imagination has been the recent slew of “A.I. Films.” Despite the cinematic qualities that this moniker invokes, these media artifacts are typically short-form digital creations—short films, fanciful trailers for non-existent movies, etc.—that circulate online, share distinct aims, and, quite often, achieve considerable virality. But what of sound and music in this idiosyncratic subset of digital media, and to what extent might they also be the product of algorithmic co-creation, as the term A.I. film implies? This article investigates screen music's status as part of the most pervasive popular perceptions concerning A.I. in filmmaking, thus challenging the longstanding visual bias of much existing literature on synthetic media. It considers screen scoring in several relevant contexts concerning the ethics and aesthetics of A.I., raising issues of authorship, labor, and deceptiveness. It also brings the growing corpus of literature on A.I. from screen studies into dialogue with film music scholarship, especially the nascent debates on library music in which the soundtracks of so-called “A.I. films” are so often enmeshed. Ultimately, this article seeks to identify the dominant scoring idiom for these short-form creations and interrogate its role in consolidating their cinematic aspirations, while also questioning the more pernicious ways that certain scoring tendencies in this trend have perpetuated the obfuscation/erasure of labor by vulnerable communities of artists and composers.","['https://openalex.org/W3207314914', 'https://openalex.org/W3096104285', 'https://openalex.org/W1967315420', 'https://openalex.org/W2507975203', 'https://openalex.org/W2074888916', 'https://openalex.org/W2621882367', 'https://openalex.org/W2561257126', 'https://openalex.org/W3113346131', 'https://openalex.org/W4380136719', 'https://openalex.org/W6636061441', 'https://openalex.org/W2884368341', 'https://openalex.org/W3081382259', 'https://openalex.org/W3018902465', 'https://openalex.org/W3085942006', 'https://openalex.org/W4321435165', 'https://openalex.org/W6618280353', 'https://openalex.org/W4285199862', 'https://openalex.org/W4235309863', 'https://openalex.org/W3184762085', 'https://openalex.org/W4388750062', 'https://openalex.org/W4365388135', 'https://openalex.org/W4384695616', 'https://openalex.org/W2296903270', 'https://openalex.org/W4248009353', 'https://openalex.org/W2147459640', 'https://openalex.org/W4253058889', 'https://openalex.org/W4387876671', 'https://openalex.org/W37059553', 'https://openalex.org/W2202866613', 'https://openalex.org/W4206572068', 'https://openalex.org/W92796414']",2025-08-21
https://openalex.org/W4392909680,https://doi.org/10.1109/icassp48485.2024.10447392,Stack-and-Delay: A New Codebook Pattern for Music Generation,"Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.","['https://openalex.org/W6849635556', 'https://openalex.org/W6852824296', 'https://openalex.org/W6853096648', 'https://openalex.org/W4401110409', 'https://openalex.org/W6854494257', 'https://openalex.org/W6795288823', 'https://openalex.org/W4313021454', 'https://openalex.org/W6850625674', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6769627184', 'https://openalex.org/W6848735303', 'https://openalex.org/W6853188576', 'https://openalex.org/W6845479124', 'https://openalex.org/W6849105126', 'https://openalex.org/W2526050071', 'https://openalex.org/W4288089799', 'https://openalex.org/W4313679638', 'https://openalex.org/W4319989813', 'https://openalex.org/W3162926177', 'https://openalex.org/W4383993968', 'https://openalex.org/W4378499140', 'https://openalex.org/W4318351475', 'https://openalex.org/W4322718191', 'https://openalex.org/W4380136719', 'https://openalex.org/W4377010126']",2024-03-18
https://openalex.org/W4399418461,https://doi.org/10.1145/3652583.3657586,Mapping the Audio Landscape for Innovative Music Sample Generation,"This paper introduces the Generative Sample Map (GESAM), a novel two-stage unsupervised learning framework capable of generating high-quality and expressive audio samples for music production. Recent generative approaches based on language models rely on text prompts as conditions. However, fine nuances in musical audio samples can hardly be described in the modality of text. For addressing this shortcoming, we propose to learn a highly descriptive latent 2D audio map by a Variational Autoencoder (VAE) which is then utilized for conditioning a Transformer model. We demonstrate the Transformer model's ability to achieve high generation quality and compare its performance against two baseline models. By selecting points on the map that compresses the manifold of the audio training set into 2D, we enable a more natural interaction with the model. We showcase this capability through an interactive demo interface, which is accessible on our website https://limchr.github.io/gesam/","['https://openalex.org/W4318351475', 'https://openalex.org/W4297677272', 'https://openalex.org/W2775487773', 'https://openalex.org/W4309117899', 'https://openalex.org/W3160235471', 'https://openalex.org/W3123097577', 'https://openalex.org/W4380136719', 'https://openalex.org/W4372260516', 'https://openalex.org/W4287271111', 'https://openalex.org/W4301206121', 'https://openalex.org/W4303519914', 'https://openalex.org/W3099378280', 'https://openalex.org/W4311415873', 'https://openalex.org/W4300980117', 'https://openalex.org/W4372341951', 'https://openalex.org/W3136272958', 'https://openalex.org/W3015287975', 'https://openalex.org/W4205137627', 'https://openalex.org/W3093209529', 'https://openalex.org/W2519091744', 'https://openalex.org/W2948211236', 'https://openalex.org/W4372263311', 'https://openalex.org/W4306177919', 'https://openalex.org/W4367359628', 'https://openalex.org/W4297632254', 'https://openalex.org/W4390784372', 'https://openalex.org/W4383646190', 'https://openalex.org/W3099425575', 'https://openalex.org/W2913668833']",2024-05-30
https://openalex.org/W4388761715,https://doi.org/10.17760/d20581905,Learning latent representations for controllable combinational creativity and game design,"Latent variable models have been increasingly applied for performing a variety of creative applications, primarily in the domains of visual art and music. Such models learn continuous latent representations of data which are then utilized for generating novel artifacts via sampling and interpolation, as well as for performing various other creative tasks. However, despite a growing body of work surrounding procedural content generation via machine learning (PCGML), the use of deep latent models for similar applications in games remains underexplored. While defining and using a possibility space of an individual game is a well-established practice in automated game design and procedural content generation, learning possibility spaces such that they span a set of one or more given games is uncommon, and in general, the use of generative models to enable a broader range of creative applications has not been as widely adopted for game design. Thus, in this thesis, we study how deep latent variable models can be leveraged for various game design applications, in two broad directions. First, we investigate the use of learned latent spaces for developing controllable combinational creativity systems, focusing specifically on game blending. Combinational creativity is the branch of creativity that focuses on producing novel artifacts by recombining properties of existing ones. Game blending is a combinational creativity process referring to recombining the levels and/or mechanics of two or more games to generate a new game and has been proposed as a means of capturing the process by which designers often create new games by combining ideas from existing ones. In this part, we focus on using variational autoencoders (VAEs) for building systems for performing such game blending, building up to a novel combinational creativity framework that defines and generates blends as linear combinations of learned latent design spaces. Second, we focus on using learned latent representations to enable game and level design applications more broadly. This section thus focuses on using models trained on one or more games to enable creative ML applications and affordances for game design, similar to those seen in visual art and music. We refer to these using the umbrella term Game Design via Creative ML or GDCML. More specifically, this part of the thesis demonstrates the use of supervised methods and evolutionary algorithms to enable a range of game design applications in the form of level editing, level search and optimization, level layout generation and style transfer.--Author's abstract","['https://openalex.org/W2901107321', 'https://openalex.org/W2796897898', 'https://openalex.org/W1576660662', 'https://openalex.org/W4205094101', 'https://openalex.org/W3209736102', 'https://openalex.org/W2897177022', 'https://openalex.org/W3014204396', 'https://openalex.org/W1924770834', 'https://openalex.org/W2327509600', 'https://openalex.org/W2556225777', 'https://openalex.org/W3164413033', 'https://openalex.org/W4200317285', 'https://openalex.org/W2108598243', 'https://openalex.org/W2982125965', 'https://openalex.org/W2772474126', 'https://openalex.org/W1889013705', 'https://openalex.org/W1541825479', 'https://openalex.org/W3111551570', 'https://openalex.org/W1536258620', 'https://openalex.org/W2893995718', 'https://openalex.org/W2161208721', 'https://openalex.org/W2798964688', 'https://openalex.org/W2412515132', 'https://openalex.org/W2960352216', 'https://openalex.org/W3000654067', 'https://openalex.org/W2893089956', 'https://openalex.org/W2893403588', 'https://openalex.org/W2726805909', 'https://openalex.org/W2892144052', 'https://openalex.org/W2887335294', 'https://openalex.org/W2560316200', 'https://openalex.org/W2295991281', 'https://openalex.org/W6893603746', 'https://openalex.org/W2138537392', 'https://openalex.org/W2194775991', 'https://openalex.org/W2100495367', 'https://openalex.org/W2603777577', 'https://openalex.org/W4313017248', 'https://openalex.org/W2888412837', 'https://openalex.org/W2904367110', 'https://openalex.org/W2995664936', 'https://openalex.org/W2163605009', 'https://openalex.org/W2202109488', 'https://openalex.org/W2136111243', 'https://openalex.org/W2099746672', 'https://openalex.org/W2101456123', 'https://openalex.org/W2944678459', 'https://openalex.org/W1551571670', 'https://openalex.org/W2899771611', 'https://openalex.org/W2462548332', 'https://openalex.org/W1974150877', 'https://openalex.org/W3135367836', 'https://openalex.org/W2957258179', 'https://openalex.org/W2794719876', 'https://openalex.org/W2792210438', 'https://openalex.org/W4226317937', 'https://openalex.org/W2911959244', 'https://openalex.org/W3081769760', 'https://openalex.org/W3174255807', 'https://openalex.org/W3129951600', 'https://openalex.org/W3178878427', 'https://openalex.org/W4367146923', 'https://openalex.org/W3084486398', 'https://openalex.org/W3008938201', 'https://openalex.org/W3092863540', 'https://openalex.org/W3014144626', 'https://openalex.org/W3142822945', 'https://openalex.org/W2228991311', 'https://openalex.org/W2103823214', 'https://openalex.org/W2108806781', 'https://openalex.org/W2729456299', 'https://openalex.org/W2546680972', 'https://openalex.org/W2767134165', 'https://openalex.org/W3036216308', 'https://openalex.org/W2188365844', 'https://openalex.org/W2902069995', 'https://openalex.org/W2290393232', 'https://openalex.org/W2735218925', 'https://openalex.org/W3164135765', 'https://openalex.org/W2586544230', 'https://openalex.org/W4312598802', 'https://openalex.org/W2083252561', 'https://openalex.org/W2975116180', 'https://openalex.org/W66306528', 'https://openalex.org/W2295130376', 'https://openalex.org/W2025768430', 'https://openalex.org/W2798854895', 'https://openalex.org/W1498730170', 'https://openalex.org/W3087208910', 'https://openalex.org/W6732528621', 'https://openalex.org/W2892295629', 'https://openalex.org/W2605287558', 'https://openalex.org/W2519091744', 'https://openalex.org/W4312282373', 'https://openalex.org/W3175662569', 'https://openalex.org/W2752134738', 'https://openalex.org/W2028208918', 'https://openalex.org/W4301489422', 'https://openalex.org/W2952716587', 'https://openalex.org/W3009830825', 'https://openalex.org/W4312933868', 'https://openalex.org/W4236250754', 'https://openalex.org/W2950669295', 'https://openalex.org/W2750384547', 'https://openalex.org/W4226348722', 'https://openalex.org/W4380136719', 'https://openalex.org/W2001923012', 'https://openalex.org/W4287802874', 'https://openalex.org/W4306681766', 'https://openalex.org/W3083116736', 'https://openalex.org/W2116804945', 'https://openalex.org/W3166396011', 'https://openalex.org/W2990300161', 'https://openalex.org/W2962750014', 'https://openalex.org/W1823742419', 'https://openalex.org/W3206122975', 'https://openalex.org/W2964201809', 'https://openalex.org/W2963575853', 'https://openalex.org/W2964332173', 'https://openalex.org/W2982700443', 'https://openalex.org/W3106433390', 'https://openalex.org/W2949869682', 'https://openalex.org/W4289492459', 'https://openalex.org/W2902888275', 'https://openalex.org/W2953100410', 'https://openalex.org/W2920929112', 'https://openalex.org/W2963525668', 'https://openalex.org/W2579414847', 'https://openalex.org/W2963771763', 'https://openalex.org/W4390187339', 'https://openalex.org/W2964167449', 'https://openalex.org/W2889205371', 'https://openalex.org/W2804078698', 'https://openalex.org/W2910577860', 'https://openalex.org/W758372786', 'https://openalex.org/W2556467266', 'https://openalex.org/W2302243225', 'https://openalex.org/W2746068898', 'https://openalex.org/W3038300925', 'https://openalex.org/W1985364607', 'https://openalex.org/W4214813851', 'https://openalex.org/W4293771587', 'https://openalex.org/W1909320841', 'https://openalex.org/W4206130186', 'https://openalex.org/W2898422183', 'https://openalex.org/W4287642507', 'https://openalex.org/W3094314045', 'https://openalex.org/W2187089797', 'https://openalex.org/W2122838682', 'https://openalex.org/W2962793481', 'https://openalex.org/W2101234009', 'https://openalex.org/W1959608418', 'https://openalex.org/W4287864310', 'https://openalex.org/W2474605630', 'https://openalex.org/W3094223921', 'https://openalex.org/W2289278205', 'https://openalex.org/W3118608800', 'https://openalex.org/W4298077424', 'https://openalex.org/W2742165006', 'https://openalex.org/W1686810756', 'https://openalex.org/W4287113536', 'https://openalex.org/W2963567641', 'https://openalex.org/W3094114563', 'https://openalex.org/W2896966958', 'https://openalex.org/W3180355996', 'https://openalex.org/W2964074081', 'https://openalex.org/W2901601067', 'https://openalex.org/W2964193438', 'https://openalex.org/W4306759102', 'https://openalex.org/W4239072543', 'https://openalex.org/W3088030554', 'https://openalex.org/W2963690854', 'https://openalex.org/W4297749385', 'https://openalex.org/W4224035735', 'https://openalex.org/W3129576130', 'https://openalex.org/W3007399319', 'https://openalex.org/W2803131472', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963073614', 'https://openalex.org/W4237611433', 'https://openalex.org/W2786672974', 'https://openalex.org/W2962212541']",2023-01-01
https://openalex.org/W4391093895,https://doi.org/10.1109/bigdata59044.2023.10386748,Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation,"Image to music generation is a new task that has the potential to enhance the creative process in the fields of film, television, and game production. Due to the significant differences between images and music in terms of their modalities, there are considerable challenges in generating music from images. We introduce an image to music generation framework which can simultaneously maintain musicality and conform to the semantic content of the original image. It consists two paths, focusing on both musical element enhancement and image detail preservation. Our experiments show that the dual-path network does outperform our previous single-path model. Furthermore, our model demonstrates its ability to create music pieces of great diversity. We've set various catagorized musical terms for CLIP to match therefore enables the model to have more choices. Some generated music samples can be found in https://andyliu2008.github.io/image2music/","['https://openalex.org/W4286250570', 'https://openalex.org/W2110851725', 'https://openalex.org/W3136806279', 'https://openalex.org/W6849105126', 'https://openalex.org/W6841982715', 'https://openalex.org/W6853096648', 'https://openalex.org/W6809885388', 'https://openalex.org/W2772474126', 'https://openalex.org/W2775487773', 'https://openalex.org/W6749351710', 'https://openalex.org/W6730558285', 'https://openalex.org/W2950864153', 'https://openalex.org/W2962968152', 'https://openalex.org/W2266728343', 'https://openalex.org/W2737041163', 'https://openalex.org/W6854031379', 'https://openalex.org/W6712426025', 'https://openalex.org/W2964260444', 'https://openalex.org/W6682691769', 'https://openalex.org/W2250539671', 'https://openalex.org/W6755207826', 'https://openalex.org/W2194775991', 'https://openalex.org/W6637373629', 'https://openalex.org/W1686810756', 'https://openalex.org/W4318351475', 'https://openalex.org/W4294170691', 'https://openalex.org/W2399507222', 'https://openalex.org/W2896457183', 'https://openalex.org/W4224035735', 'https://openalex.org/W3099425575', 'https://openalex.org/W4293575120', 'https://openalex.org/W4380136719']",2023-12-15
https://openalex.org/W4392902968,https://doi.org/10.1109/icassp48485.2024.10446643,Generating Stereophonic Music with Single-Stage Language Models,"The recent success of audio language models (LMs) has revolutionized the field of neural music generation. Among all audio LM approaches, MusicGen has demonstrated the success of a single-stage LMs based music generation framework, without needing to train multiple LMs. Despite its promising performance in generating monophonic (mono) music, directly generating stereophonic (stereo) music following the previous framework has resulted in perceptible quality degradation. In this paper, we first discuss the difficulty of directly encoding stereo music with neural codec, and then provide a stable and practical solution based on a dual encoding approach. To utilize the dually encoded tokens in single-stage LMs, we also propose two forms of token sequence patterns. An extensive evaluation has been conducted using various aspects of stereo music audios to examine the performance of stereo neural codec approaches and the generation quality of single-stage LMs. Finally, our experimental results suggest that (i) our proposed dual encoding approach for neural codec is significantly better than the typical joint encoding approach in terms of reconstruction quality, and (ii) the stereo single-stage LMs trained with our proposed token sequence patterns substantially improved the perceptual quality of the state-of-the-art music generation model (i.e. MusicGen) in subjective tests.","['https://openalex.org/W4240826525', 'https://openalex.org/W6776218486', 'https://openalex.org/W6849517043', 'https://openalex.org/W6849635556', 'https://openalex.org/W6849105126', 'https://openalex.org/W6852824296', 'https://openalex.org/W6853096648', 'https://openalex.org/W6748409065', 'https://openalex.org/W4381786045', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W3215615641', 'https://openalex.org/W4244017338', 'https://openalex.org/W2131738223', 'https://openalex.org/W4307323391', 'https://openalex.org/W2460742184', 'https://openalex.org/W2963992487', 'https://openalex.org/W6636170946', 'https://openalex.org/W3037038648', 'https://openalex.org/W4298310324', 'https://openalex.org/W4380136719', 'https://openalex.org/W4319989813', 'https://openalex.org/W4285528093', 'https://openalex.org/W4378499140', 'https://openalex.org/W4318351475', 'https://openalex.org/W4212774754', 'https://openalex.org/W4287802874']",2024-03-18
https://openalex.org/W4394625798,https://doi.org/10.1109/wacv57701.2024.00702,Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm,"It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a 'live' demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements.","['https://openalex.org/W2619697695', 'https://openalex.org/W6729831399', 'https://openalex.org/W6606783348', 'https://openalex.org/W2559085405', 'https://openalex.org/W3108332675', 'https://openalex.org/W3046890131', 'https://openalex.org/W6941176339', 'https://openalex.org/W2811167645', 'https://openalex.org/W3213726885', 'https://openalex.org/W6776218486', 'https://openalex.org/W3207290297', 'https://openalex.org/W2772474126', 'https://openalex.org/W6746569791', 'https://openalex.org/W2010771919', 'https://openalex.org/W6615672858', 'https://openalex.org/W3108240585', 'https://openalex.org/W3017343282', 'https://openalex.org/W2963218389', 'https://openalex.org/W6762661407', 'https://openalex.org/W2962795401', 'https://openalex.org/W2098125043', 'https://openalex.org/W6729977899', 'https://openalex.org/W6755182157', 'https://openalex.org/W6760601182', 'https://openalex.org/W3092879656', 'https://openalex.org/W2103570990', 'https://openalex.org/W2888202177', 'https://openalex.org/W2914217321', 'https://openalex.org/W3016096605', 'https://openalex.org/W2997367363', 'https://openalex.org/W2497597915', 'https://openalex.org/W3204221554', 'https://openalex.org/W2743147997', 'https://openalex.org/W2979157532', 'https://openalex.org/W2963408210', 'https://openalex.org/W2964345931', 'https://openalex.org/W2511428026', 'https://openalex.org/W6749351710', 'https://openalex.org/W2780124704', 'https://openalex.org/W3139813613', 'https://openalex.org/W4393148499', 'https://openalex.org/W6780316881', 'https://openalex.org/W6787185846', 'https://openalex.org/W6803751523', 'https://openalex.org/W2738406145', 'https://openalex.org/W2964109005', 'https://openalex.org/W6770639321', 'https://openalex.org/W2759263543', 'https://openalex.org/W3093257790', 'https://openalex.org/W6683248184', 'https://openalex.org/W2981851635', 'https://openalex.org/W2962865004', 'https://openalex.org/W3019952993', 'https://openalex.org/W2963807156', 'https://openalex.org/W4312814772', 'https://openalex.org/W6838582276', 'https://openalex.org/W4287802874', 'https://openalex.org/W398859631', 'https://openalex.org/W3109114891', 'https://openalex.org/W2950547518', 'https://openalex.org/W569478347', 'https://openalex.org/W4380136719', 'https://openalex.org/W4319989813', 'https://openalex.org/W3102619627', 'https://openalex.org/W3111853169', 'https://openalex.org/W4212774754', 'https://openalex.org/W4293771587', 'https://openalex.org/W4318351475']",2024-01-03
https://openalex.org/W4396606329,https://doi.org/10.1162/comj_a_00674,Raging with the Machine in the Uncanny Valley: Human–AI Cocreativity in the Eurovision-Themed AI Song Contest,"Abstract We report here the processes involved in creating our entry in the 2020 AI Song Contest, “Beautiful the World”; the technical innovations from the project; and the decision-making that divided tasks between human and machine in a way that ensured that the final creation was AI-inspired but human-created, starting from generated melodies, lyrics, and timbres. Key innovations include the use of lyric stress patterns as queries to a stress-based melody index to a database of generated melodies, and the creation of a novel instrument timbre with differential digital signal processing, trained on Australian animal calls. We reflect on how human–AI cocreativity occurred during the process and how it may develop in the future.","['https://openalex.org/W6730621950', 'https://openalex.org/W2321639984', 'https://openalex.org/W2034843522', 'https://openalex.org/W7027389853', 'https://openalex.org/W6731689491', 'https://openalex.org/W6788162885', 'https://openalex.org/W6778883912', 'https://openalex.org/W4200035946', 'https://openalex.org/W4247396748', 'https://openalex.org/W6756584662', 'https://openalex.org/W2567219640', 'https://openalex.org/W6853096648', 'https://openalex.org/W6687655553', 'https://openalex.org/W6776218486', 'https://openalex.org/W2043674606', 'https://openalex.org/W1970224177', 'https://openalex.org/W6771763809', 'https://openalex.org/W2103498773', 'https://openalex.org/W2180429496', 'https://openalex.org/W3127760579', 'https://openalex.org/W3037149862', 'https://openalex.org/W6630534141', 'https://openalex.org/W3092135915', 'https://openalex.org/W4377140494', 'https://openalex.org/W2147223282', 'https://openalex.org/W6739869923', 'https://openalex.org/W6688273814', 'https://openalex.org/W6748253163', 'https://openalex.org/W6631190155', 'https://openalex.org/W4233449897', 'https://openalex.org/W6753487251', 'https://openalex.org/W6732429163', 'https://openalex.org/W3215474158', 'https://openalex.org/W6713876852', 'https://openalex.org/W6766978945', 'https://openalex.org/W1946974084', 'https://openalex.org/W2170238612', 'https://openalex.org/W2036343254', 'https://openalex.org/W6629668480', 'https://openalex.org/W2477695133', 'https://openalex.org/W1999629274', 'https://openalex.org/W2942446308', 'https://openalex.org/W6769243733', 'https://openalex.org/W6732528621', 'https://openalex.org/W2901601067', 'https://openalex.org/W4380136719', 'https://openalex.org/W2208374197', 'https://openalex.org/W3117855249', 'https://openalex.org/W2574771818', 'https://openalex.org/W3000389243', 'https://openalex.org/W2691348169', 'https://openalex.org/W1512123845', 'https://openalex.org/W2408435475', 'https://openalex.org/W2560662714', 'https://openalex.org/W4295312788', 'https://openalex.org/W2196701451', 'https://openalex.org/W1522301498', 'https://openalex.org/W4292779060', 'https://openalex.org/W2584032004', 'https://openalex.org/W2579414847', 'https://openalex.org/W4287802874', 'https://openalex.org/W2979826702', 'https://openalex.org/W2811228919', 'https://openalex.org/W4289287927', 'https://openalex.org/W2962866891']",2023-01-01
https://openalex.org/W4398199707,https://doi.org/10.5594/jmi.2024/wren8857,AI Assistants in Media Production and Management: A Survey of Workflow Optimizations for Enhancing Creativity,"This paper delves into AI's transformative role in media production and management, examining its application in media asset management, video editing, audio production, and music composition. It investigates how AI, through technologies like semantic embedding and large language models, significantly impacts creative processes, workflow optimization, and ideation. AI aids in automating mundane tasks, enhancing contextual searches, and providing recommended editorial choices, thus allowing creators to concentrate on more complex creative tasks. The survey highlights the use of AI in content management, semantic media search, transcript-based video editing, sound design, and chord symbol auto-completion, illustrating AI's role as a collaborative partner that enhances human ingenuity. The paper underscores the symbiotic relationship between AI and creators, emphasizing the potential for AI to usher in a new era of innovative media content creation and management, positioning AI as a central component of the modern media landscape.","['https://openalex.org/W4385817711', 'https://openalex.org/W4321513965', 'https://openalex.org/W4397036001', 'https://openalex.org/W2991118492', 'https://openalex.org/W6791353385', 'https://openalex.org/W4386065512', 'https://openalex.org/W6859465070', 'https://openalex.org/W4390041933', 'https://openalex.org/W6852952677', 'https://openalex.org/W6854866820', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W6850625674', 'https://openalex.org/W3212886388', 'https://openalex.org/W6847363464', 'https://openalex.org/W6850218400', 'https://openalex.org/W6852909395', 'https://openalex.org/W2962843322', 'https://openalex.org/W3024003175', 'https://openalex.org/W4225515706', 'https://openalex.org/W4300980117', 'https://openalex.org/W6854102746', 'https://openalex.org/W6755406732', 'https://openalex.org/W6730558285', 'https://openalex.org/W6853096648', 'https://openalex.org/W6776218486', 'https://openalex.org/W4312933868', 'https://openalex.org/W4385988359', 'https://openalex.org/W4317748910', 'https://openalex.org/W4380136719', 'https://openalex.org/W4322718191', 'https://openalex.org/W4402753899', 'https://openalex.org/W4287802874', 'https://openalex.org/W4380559123', 'https://openalex.org/W4323066695', 'https://openalex.org/W4380993674', 'https://openalex.org/W4378105483', 'https://openalex.org/W4292779060', 'https://openalex.org/W4384918448', 'https://openalex.org/W4226278401']",2024-05-15
https://openalex.org/W4399260034,https://doi.org/10.14569/ijacsa.2024.01505101,Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms,"Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.","['https://openalex.org/W4380136719', 'https://openalex.org/W2592737436', 'https://openalex.org/W2088807535', 'https://openalex.org/W2601110281', 'https://openalex.org/W2561662441', 'https://openalex.org/W3104096215', 'https://openalex.org/W2772281385', 'https://openalex.org/W4378499140', 'https://openalex.org/W4384662684', 'https://openalex.org/W2963575853', 'https://openalex.org/W4385293770', 'https://openalex.org/W2148730083', 'https://openalex.org/W4391462885', 'https://openalex.org/W2151245740', 'https://openalex.org/W2293504909', 'https://openalex.org/W4320013936', 'https://openalex.org/W2031265013', 'https://openalex.org/W4390919369']",2024-01-01
https://openalex.org/W4399426794,https://doi.org/10.1109/icict60155.2024.10544569,AudioPalette: Generating Soundtracks through Feature Extraction from Images,"This research study aims to create a streamlined way for generating a musical soundtrack from an image. The proposed pipeline employs extracting necessary features from an image, such as the context in terms of text captioning and the sentiment portrayed by the image. Using these extracted features, Meta's music generation model, MusicGen, generates a soundtrack. This study also introduces a new ""pace"" model, which was trained on a manually annotated dataset of around 8,000 images. Based on a conducted survey, the pace model proved to be a necessary and impactful addition to the pipeline. This model extracts the ""pace"" or ""tempo"" for a soundtrack from the image. We discuss the methods used along with the underlying architectures and demonstrate their effectiveness in creating a suitable auditory experience. Another implemented functionality is the generation of a slideshow with music, with the input being a set of similar images. The experiment highlights potential applications in multimedia, immersive storytelling, and more.","['https://openalex.org/W6850204008', 'https://openalex.org/W6703879901', 'https://openalex.org/W1972889870', 'https://openalex.org/W1525099241', 'https://openalex.org/W6849177959', 'https://openalex.org/W2099813784', 'https://openalex.org/W6853096648', 'https://openalex.org/W4307323391', 'https://openalex.org/W6849105126', 'https://openalex.org/W6841982715', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226033575', 'https://openalex.org/W4381786045', 'https://openalex.org/W6845479124', 'https://openalex.org/W2169896877', 'https://openalex.org/W6755257315', 'https://openalex.org/W4293575120', 'https://openalex.org/W4380136719', 'https://openalex.org/W4297817572', 'https://openalex.org/W4318351475', 'https://openalex.org/W4300980117', 'https://openalex.org/W2338312508', 'https://openalex.org/W4318718936', 'https://openalex.org/W4320458302']",2024-04-24
https://openalex.org/W4406457796,https://doi.org/10.1109/bigdata62323.2024.10825824,Interpreting Graphic Notation with MusicLDM: An AI Improvisation of Cornelius Cardew’s Treatise,,"['https://openalex.org/W2519091744', 'https://openalex.org/W6732429163', 'https://openalex.org/W6755257315', 'https://openalex.org/W6776218486', 'https://openalex.org/W6849105126', 'https://openalex.org/W6853096648', 'https://openalex.org/W4401043564', 'https://openalex.org/W6849517043', 'https://openalex.org/W6783182287', 'https://openalex.org/W4392903114', 'https://openalex.org/W6755182157', 'https://openalex.org/W6839133533', 'https://openalex.org/W4390929260', 'https://openalex.org/W4312918978', 'https://openalex.org/W4398226295', 'https://openalex.org/W6860268860', 'https://openalex.org/W6854652134', 'https://openalex.org/W6849938875', 'https://openalex.org/W6858340110', 'https://openalex.org/W4385245566', 'https://openalex.org/W6855718144', 'https://openalex.org/W6779823529', 'https://openalex.org/W6679045638', 'https://openalex.org/W4372260310', 'https://openalex.org/W3036167779', 'https://openalex.org/W4396877837', 'https://openalex.org/W4297817572', 'https://openalex.org/W4391159170', 'https://openalex.org/W2950547518', 'https://openalex.org/W4388276176', 'https://openalex.org/W4380136719', 'https://openalex.org/W4318351475', 'https://openalex.org/W4282913091', 'https://openalex.org/W395933519', 'https://openalex.org/W398859631', 'https://openalex.org/W2584032004', 'https://openalex.org/W4287802874', 'https://openalex.org/W3129651364', 'https://openalex.org/W2129069237']",2024-12-15
https://openalex.org/W4406458896,https://doi.org/10.1109/bigdata62323.2024.10825473,AI Assisted Workflow for Set-list Preparation with Loops for Live Musicians,,"['https://openalex.org/W4286250570', 'https://openalex.org/W6855835023', 'https://openalex.org/W6685276911', 'https://openalex.org/W1985044054', 'https://openalex.org/W6804644920', 'https://openalex.org/W2795371128', 'https://openalex.org/W6858251482', 'https://openalex.org/W4402670276', 'https://openalex.org/W6853096648', 'https://openalex.org/W4380136719', 'https://openalex.org/W4386228790', 'https://openalex.org/W4293258764', 'https://openalex.org/W4294990489', 'https://openalex.org/W4387838399']",2024-12-15
https://openalex.org/W4406495765,https://doi.org/10.1109/bigdata62323.2024.10825438,Decomposing Audio into Timbral Features with Convolutional Neural Networks,,"['https://openalex.org/W6849105126', 'https://openalex.org/W6853096648', 'https://openalex.org/W6776218486', 'https://openalex.org/W4391586430', 'https://openalex.org/W2964218314', 'https://openalex.org/W2794150026', 'https://openalex.org/W4206995848', 'https://openalex.org/W6730401039', 'https://openalex.org/W6745878906', 'https://openalex.org/W2963232038', 'https://openalex.org/W2059652044', 'https://openalex.org/W3023104975', 'https://openalex.org/W4206782543', 'https://openalex.org/W6732646663', 'https://openalex.org/W4287802874', 'https://openalex.org/W2580221632', 'https://openalex.org/W2767858146', 'https://openalex.org/W2559688696', 'https://openalex.org/W4380136719', 'https://openalex.org/W4318351475']",2024-12-15
https://openalex.org/W4408345698,https://doi.org/10.1109/icassp49660.2025.10887745,Benchmarking Music Generation Models and Metrics via Human Preference Studies,"Recent advancements have brought generated music closer to human-created compositions, yet evaluating these models remains challenging. While human preference is the gold standard for assessing quality, translating these subjective judgments into objective metrics, particularly for text-audio alignment and music quality, has proven difficult. In this work, we generate 6k songs using 12 state-of-the-art models and conduct a survey of 15k pairwise audio comparisons with 2.5k human participants to evaluate the correlation between human preferences and widely used metrics. To the best of our knowledge, this work is the first to rank current state-of-the-art music generation models and metrics based on human preference. To further the field of subjective metric evaluation, we provide open access to our dataset of generated music and human evaluations.","['https://openalex.org/W6757220786', 'https://openalex.org/W2526050071', 'https://openalex.org/W6849105126', 'https://openalex.org/W4372266552', 'https://openalex.org/W4392903801', 'https://openalex.org/W4372260310', 'https://openalex.org/W6842893850', 'https://openalex.org/W4392902957', 'https://openalex.org/W6849635556', 'https://openalex.org/W6763945542', 'https://openalex.org/W6732646663', 'https://openalex.org/W6678969435', 'https://openalex.org/W6853096648', 'https://openalex.org/W4396877837', 'https://openalex.org/W4401043564', 'https://openalex.org/W6861353174', 'https://openalex.org/W6862631004', 'https://openalex.org/W4324129606', 'https://openalex.org/W6640689915', 'https://openalex.org/W2763110165', 'https://openalex.org/W3094550259', 'https://openalex.org/W4307323391', 'https://openalex.org/W4392616695', 'https://openalex.org/W4294534174', 'https://openalex.org/W2944941081', 'https://openalex.org/W4391709184', 'https://openalex.org/W2580221632', 'https://openalex.org/W4289106098', 'https://openalex.org/W4380136719', 'https://openalex.org/W1948584817', 'https://openalex.org/W4319989813', 'https://openalex.org/W4318351475']",2025-03-12
https://openalex.org/W4409870537,https://doi.org/10.1145/3706599.3719686,Mozualization: Crafting Music and Visual Representation with Multimodal AI,,"['https://openalex.org/W4318351475', 'https://openalex.org/W4403746926', 'https://openalex.org/W4281759470', 'https://openalex.org/W2752134738', 'https://openalex.org/W4402727350', 'https://openalex.org/W4380136719', 'https://openalex.org/W2148154194', 'https://openalex.org/W2772474126', 'https://openalex.org/W4232657190', 'https://openalex.org/W4384936261', 'https://openalex.org/W2902511764', 'https://openalex.org/W4392647990', 'https://openalex.org/W4378499140', 'https://openalex.org/W4226182655', 'https://openalex.org/W2191779130', 'https://openalex.org/W2000715026', 'https://openalex.org/W4386855906', 'https://openalex.org/W2067621398', 'https://openalex.org/W4399627159', 'https://openalex.org/W2142179950', 'https://openalex.org/W2746068898', 'https://openalex.org/W4225631565', 'https://openalex.org/W4391766578']",2025-04-25
https://openalex.org/W4411353851,https://doi.org/10.1016/j.aej.2025.05.053,MusDiff: A multimodal-guided framework for music generation,"Music generation has become a key area in artificial intelligence, achieving significant progress in recent years. However, current research focuses primarily on general music tasks, with limited support for ethnic music. Moreover, the lack of multimodal guidance, such as text and image inputs, restricts generative models in understanding complex semantics and producing high-quality music. To address these limitations, we propose MusDiff, a multimodal music generation framework that combines text and image inputs to enhance music quality and cross-modal consistency. MusDiff is based on a diffusion model architecture, integrating IP-Adapter and KAN (Kolmogorov–Arnold Network) optimizations to improve feature fusion and modality alignment. Additionally, we introduce a new multimodal dataset, MusiTextImg, which includes diverse music categories, such as ethnic and modern styles, with annotations for text, image, and music modalities. We also extend the MusicCaps dataset by adding matched image pairs to text descriptions, further supporting multimodal research. Experimental results demonstrate that MusDiff outperforms existing methods on benchmark datasets (MusiTextImg and MusicCaps), excelling in realism, detail fidelity, and multimodal alignment. MusDiff not only sets a new performance standard for multimodal music generation but also opens new research directions in the field of multimodal generation.","['https://openalex.org/W6803270871', 'https://openalex.org/W6810530438', 'https://openalex.org/W6787780790', 'https://openalex.org/W4404639702', 'https://openalex.org/W6855611815', 'https://openalex.org/W3131876117', 'https://openalex.org/W2968379763', 'https://openalex.org/W6805303941', 'https://openalex.org/W4286250570', 'https://openalex.org/W4404341926', 'https://openalex.org/W6869734375', 'https://openalex.org/W6856713528', 'https://openalex.org/W6801317528', 'https://openalex.org/W6768610422', 'https://openalex.org/W4402715807', 'https://openalex.org/W6865644415', 'https://openalex.org/W4399431730', 'https://openalex.org/W3211570179', 'https://openalex.org/W4401021017', 'https://openalex.org/W4399039621', 'https://openalex.org/W6864728182', 'https://openalex.org/W4398226295', 'https://openalex.org/W6852824296', 'https://openalex.org/W4402727778', 'https://openalex.org/W4406559612', 'https://openalex.org/W6853096648', 'https://openalex.org/W6852975727', 'https://openalex.org/W6857333368', 'https://openalex.org/W4402727350', 'https://openalex.org/W6861509483', 'https://openalex.org/W4403780831', 'https://openalex.org/W4385848774', 'https://openalex.org/W4399666089', 'https://openalex.org/W4386499520', 'https://openalex.org/W4319989813', 'https://openalex.org/W4391248518', 'https://openalex.org/W4380136719', 'https://openalex.org/W4378499140', 'https://openalex.org/W4391462885', 'https://openalex.org/W4318351475', 'https://openalex.org/W4377372281', 'https://openalex.org/W4318718630', 'https://openalex.org/W4206174637', 'https://openalex.org/W4205616158', 'https://openalex.org/W4394906120', 'https://openalex.org/W4391159170', 'https://openalex.org/W4401023668', 'https://openalex.org/W4396821501']",2025-06-16
https://openalex.org/W4411488433,https://doi.org/10.1016/b978-0-443-34717-7.00006-4,AI and Web3 for sustainable digital economy,,"['https://openalex.org/W4312646739', 'https://openalex.org/W6759018101', 'https://openalex.org/W3037921969', 'https://openalex.org/W4390065202', 'https://openalex.org/W4302734258', 'https://openalex.org/W6793263649', 'https://openalex.org/W4294559022', 'https://openalex.org/W4324144553', 'https://openalex.org/W4205974944', 'https://openalex.org/W4382195294', 'https://openalex.org/W3015482591', 'https://openalex.org/W4300905078', 'https://openalex.org/W4324134927', 'https://openalex.org/W6852286946', 'https://openalex.org/W4283738729', 'https://openalex.org/W4308192654', 'https://openalex.org/W3194918904', 'https://openalex.org/W6790978476', 'https://openalex.org/W6800631459', 'https://openalex.org/W6728484576', 'https://openalex.org/W6763175294', 'https://openalex.org/W2798657499', 'https://openalex.org/W6744467996', 'https://openalex.org/W6760894593', 'https://openalex.org/W6752346538', 'https://openalex.org/W4388820042', 'https://openalex.org/W2999832078', 'https://openalex.org/W6853382001', 'https://openalex.org/W6845598993', 'https://openalex.org/W6854181681', 'https://openalex.org/W6840193529', 'https://openalex.org/W4284966012', 'https://openalex.org/W6750698406', 'https://openalex.org/W6855971675', 'https://openalex.org/W6810595431', 'https://openalex.org/W6846200007', 'https://openalex.org/W6793754391', 'https://openalex.org/W6851420574', 'https://openalex.org/W4293339592', 'https://openalex.org/W6808837589', 'https://openalex.org/W3213366125', 'https://openalex.org/W3202199074', 'https://openalex.org/W6850917995', 'https://openalex.org/W4285815732', 'https://openalex.org/W4226225860', 'https://openalex.org/W4320717855', 'https://openalex.org/W3210522271', 'https://openalex.org/W6841899086', 'https://openalex.org/W3153670037', 'https://openalex.org/W4311411365', 'https://openalex.org/W4293353521', 'https://openalex.org/W4321021837', 'https://openalex.org/W6850640699', 'https://openalex.org/W4224059887', 'https://openalex.org/W3129114628', 'https://openalex.org/W6773006575', 'https://openalex.org/W4385489426', 'https://openalex.org/W4223945632', 'https://openalex.org/W4393178509', 'https://openalex.org/W4388642299', 'https://openalex.org/W4311001906', 'https://openalex.org/W4385627162', 'https://openalex.org/W4285820179', 'https://openalex.org/W4252775465', 'https://openalex.org/W4400491135', 'https://openalex.org/W4375930419', 'https://openalex.org/W3201060641', 'https://openalex.org/W4241476509', 'https://openalex.org/W2796626119', 'https://openalex.org/W4381785319', 'https://openalex.org/W4298185919', 'https://openalex.org/W4376167340', 'https://openalex.org/W4298187450', 'https://openalex.org/W4361806024', 'https://openalex.org/W4380136719', 'https://openalex.org/W4212937970', 'https://openalex.org/W4401725485', 'https://openalex.org/W4211263987', 'https://openalex.org/W4233278670', 'https://openalex.org/W3106673115', 'https://openalex.org/W4318351475', 'https://openalex.org/W4378647672', 'https://openalex.org/W4213069590', 'https://openalex.org/W4362500576']",2025-01-01
https://openalex.org/W4412495440,https://doi.org/10.1007/s00146-025-02471-y,Actionable tacit knowledge in second-wave ubiquitous music frameworks,,"['https://openalex.org/W3185693483', 'https://openalex.org/W4391352207', 'https://openalex.org/W2047299263', 'https://openalex.org/W2985654598', 'https://openalex.org/W2982753834', 'https://openalex.org/W2299069134', 'https://openalex.org/W4380136719', 'https://openalex.org/W1995039359', 'https://openalex.org/W4388821834', 'https://openalex.org/W4396509245', 'https://openalex.org/W2002676726', 'https://openalex.org/W2131018663', 'https://openalex.org/W3093393275', 'https://openalex.org/W2418670069', 'https://openalex.org/W4389393001', 'https://openalex.org/W4248685047', 'https://openalex.org/W4362606454', 'https://openalex.org/W4395470991', 'https://openalex.org/W4285044316', 'https://openalex.org/W3208435704', 'https://openalex.org/W4234756972', 'https://openalex.org/W2035508503', 'https://openalex.org/W4294243596', 'https://openalex.org/W4402671241', 'https://openalex.org/W2908516207', 'https://openalex.org/W2074308058']",2025-07-20
https://openalex.org/W4412705948,https://doi.org/10.30935/ojcmt/16669,"Trailer Reimagined: An Innovative, Llm-DRiven, Expressive Automated Movie Summary framework (TRAILDREAMS)","This paper introduces TRAILDREAMS, a framework that uses a large language model (LLM) to automate the production of movie trailers. The purpose of LLM is to select key visual sequences and impactful dialogues, and to help TRAILDREAMS to generate audio elements such as music and voiceovers. The goal is to produce engaging and visually appealing trailers efficiently. In comparative evaluations, TRAILDREAMS surpasses current state-of-the-art trailer generation methods in viewer ratings. However, it still falls short when compared to real, human-crafted trailers. While TRAILDREAMS demonstrates significant promise and marks an advancement in automated creative processes, further improvements are necessary to bridge the quality gap with traditional trailers.","['https://openalex.org/W2967615747', 'https://openalex.org/W4405361836', 'https://openalex.org/W4229847516', 'https://openalex.org/W3015783745', 'https://openalex.org/W4380136719', 'https://openalex.org/W4381435513', 'https://openalex.org/W4380769213', 'https://openalex.org/W4390970405', 'https://openalex.org/W2801673049', 'https://openalex.org/W4285104223', 'https://openalex.org/W2064469569', 'https://openalex.org/W6697996091', 'https://openalex.org/W6633724138', 'https://openalex.org/W4390572799', 'https://openalex.org/W2737677090', 'https://openalex.org/W1538606284', 'https://openalex.org/W4307083829', 'https://openalex.org/W2060666149', 'https://openalex.org/W1810943226', 'https://openalex.org/W2970641574', 'https://openalex.org/W4372260250', 'https://openalex.org/W2194187530', 'https://openalex.org/W2025367812', 'https://openalex.org/W2766348671', 'https://openalex.org/W6614503671', 'https://openalex.org/W2755889034', 'https://openalex.org/W4253034184', 'https://openalex.org/W4285058679', 'https://openalex.org/W2267033539', 'https://openalex.org/W2972359262', 'https://openalex.org/W2076771430', 'https://openalex.org/W2781922022', 'https://openalex.org/W4387968628']",2025-07-28
https://openalex.org/W4413265676,https://doi.org/10.3390/app15158504,Enhancing MusicGen with Prompt Tuning,"Generative AI has been gaining attention across various creative domains. In particular, MusicGen stands out as a representative approach capable of generating music based on text or audio inputs. However, it has limitations in producing high-quality outputs for specific genres and fully reflecting user intentions. This paper proposes a prompt tuning technique that effectively adjusts the output quality of MusicGen without modifying its original parameters and optimizes its ability to generate music tailored to specific genres and styles. Experiments were conducted to compare the performance of the traditional MusicGen with the proposed method and evaluate the quality of generated music using the Contrastive Language-Audio Pretraining (CLAP) and Kullback–Leibler Divergence (KLD) scoring approaches. The results demonstrated that the proposed method significantly improved the output quality and musical coherence, particularly for specific genres and styles. Compared with the traditional model, the CLAP score was increased by 0.1270, and the KLD score was increased by 0.00403 on average. The effectiveness of prompt tuning in optimizing the performance of MusicGen validated the proposed method and highlighted its potential for advancing generative AI-based music generation tools.","['https://openalex.org/W6853096648', 'https://openalex.org/W4392904237', 'https://openalex.org/W6852824296', 'https://openalex.org/W4401043564', 'https://openalex.org/W4392903114', 'https://openalex.org/W4376131672', 'https://openalex.org/W3173837831', 'https://openalex.org/W2406222150', 'https://openalex.org/W2936774411', 'https://openalex.org/W2772474126', 'https://openalex.org/W2792210438', 'https://openalex.org/W4205991051', 'https://openalex.org/W4386187806', 'https://openalex.org/W4285247752', 'https://openalex.org/W3174770825', 'https://openalex.org/W4404144711', 'https://openalex.org/W4408924515', 'https://openalex.org/W4409526155', 'https://openalex.org/W4372266552', 'https://openalex.org/W1965555277', 'https://openalex.org/W2170142018', 'https://openalex.org/W2133824856', 'https://openalex.org/W2124988752', 'https://openalex.org/W4378499140', 'https://openalex.org/W4380136719']",2025-07-31
https://openalex.org/W4413362604,https://doi.org/10.3389/fcomp.2025.1575168,Neural audio instruments: epistemological and phenomenological perspectives on musical embodiment of deep learning,"Neural Audio is a category of deep learning pipelines which output audio signals directly, in real-time scenarios of action-sound interactions. In this work, we examine how neural audio-based artificial intelligence, when embedded in digital musical instruments (DMIs), shapes embodied musical interaction. While DMIs have long struggled to match the physical immediacy of acoustic instruments, neural audio methods can magnify this challenge, requiring data collection, model training and deep theoretical knowledge that appear to push musicians toward symbolic or conceptual modes of engagement. Paradoxically, these same methods can also foster more embodied practices, by introducing opaque yet expressive behaviors that free performers from rigid technical models and encourage discovery through tactile, real-time experimentation. Drawing on established perspectives in DMI embodiment literature, as well as emerging neural-audio-focused efforts within the community, we highlight two seemingly conflicting aspects of these instruments: on one side, they inherit many “disembodying” traits known from DMIs; on the other, they open pathways reminiscent of acoustic phenomenology and soma, potentially restoring the close physical interplay often missed in digital performance.","['https://openalex.org/W6849105126', 'https://openalex.org/W6852486833', 'https://openalex.org/W2103960658', 'https://openalex.org/W2065331554', 'https://openalex.org/W6850898621', 'https://openalex.org/W2775487773', 'https://openalex.org/W2099603485', 'https://openalex.org/W2560769978', 'https://openalex.org/W2002506522', 'https://openalex.org/W4309117899', 'https://openalex.org/W2974783070', 'https://openalex.org/W6712707749', 'https://openalex.org/W2137847051', 'https://openalex.org/W2141740050', 'https://openalex.org/W3014731244', 'https://openalex.org/W4289287927', 'https://openalex.org/W4283733664', 'https://openalex.org/W1501493860', 'https://openalex.org/W4287124627', 'https://openalex.org/W3090251239', 'https://openalex.org/W6853096648', 'https://openalex.org/W4400066905', 'https://openalex.org/W6713309424', 'https://openalex.org/W6893083318', 'https://openalex.org/W3097078247', 'https://openalex.org/W2496309272', 'https://openalex.org/W6853227284', 'https://openalex.org/W4256740660', 'https://openalex.org/W6758675244', 'https://openalex.org/W6771763809', 'https://openalex.org/W6866518913', 'https://openalex.org/W6752996073', 'https://openalex.org/W2147176915', 'https://openalex.org/W2139879587', 'https://openalex.org/W2103498773', 'https://openalex.org/W6778762215', 'https://openalex.org/W2056497635', 'https://openalex.org/W4235296576', 'https://openalex.org/W2169655147', 'https://openalex.org/W1999940579', 'https://openalex.org/W2120847449', 'https://openalex.org/W2481497441', 'https://openalex.org/W2152466978', 'https://openalex.org/W2921749038', 'https://openalex.org/W6770046961', 'https://openalex.org/W4390745914', 'https://openalex.org/W4281667003', 'https://openalex.org/W2758804652', 'https://openalex.org/W4234889190', 'https://openalex.org/W3213222354', 'https://openalex.org/W2766099953', 'https://openalex.org/W2963889406', 'https://openalex.org/W2134079826', 'https://openalex.org/W13122474', 'https://openalex.org/W4366547572', 'https://openalex.org/W6735556979', 'https://openalex.org/W2157505989', 'https://openalex.org/W6688561468', 'https://openalex.org/W4311313962', 'https://openalex.org/W6604999789', 'https://openalex.org/W1549329517', 'https://openalex.org/W6862187309', 'https://openalex.org/W2146939832', 'https://openalex.org/W2044330353', 'https://openalex.org/W4389301599', 'https://openalex.org/W2109917277', 'https://openalex.org/W1988082944', 'https://openalex.org/W6845479124', 'https://openalex.org/W6767111847', 'https://openalex.org/W2037878919', 'https://openalex.org/W1521141615', 'https://openalex.org/W2270987086', 'https://openalex.org/W2151295100', 'https://openalex.org/W4402952717', 'https://openalex.org/W1993958841', 'https://openalex.org/W4283012987', 'https://openalex.org/W2162505594', 'https://openalex.org/W2996941927', 'https://openalex.org/W4388561185', 'https://openalex.org/W2517412762', 'https://openalex.org/W2117376728', 'https://openalex.org/W4376279272', 'https://openalex.org/W2742502647', 'https://openalex.org/W6727219811', 'https://openalex.org/W6732269906', 'https://openalex.org/W2584032004', 'https://openalex.org/W4299517288', 'https://openalex.org/W6782741650', 'https://openalex.org/W2048073916', 'https://openalex.org/W4283075402', 'https://openalex.org/W6754638547', 'https://openalex.org/W4396832161', 'https://openalex.org/W4253816439', 'https://openalex.org/W4402967819', 'https://openalex.org/W2044170912', 'https://openalex.org/W2801274389', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963636093', 'https://openalex.org/W4294619240', 'https://openalex.org/W6680407149', 'https://openalex.org/W4283265157', 'https://openalex.org/W6853795530', 'https://openalex.org/W4283029647', 'https://openalex.org/W6749892750', 'https://openalex.org/W1965487093', 'https://openalex.org/W2799577142', 'https://openalex.org/W2560528076', 'https://openalex.org/W2563502399', 'https://openalex.org/W6949337704', 'https://openalex.org/W6870578325', 'https://openalex.org/W4206254914', 'https://openalex.org/W2126197241', 'https://openalex.org/W6635057767', 'https://openalex.org/W4283032880', 'https://openalex.org/W3122300013', 'https://openalex.org/W4378498754', 'https://openalex.org/W2890231126', 'https://openalex.org/W4213107228', 'https://openalex.org/W6892976010', 'https://openalex.org/W4287199918', 'https://openalex.org/W2072785492', 'https://openalex.org/W2587573685', 'https://openalex.org/W2325375068', 'https://openalex.org/W2090284173', 'https://openalex.org/W3000751478', 'https://openalex.org/W2052134646', 'https://openalex.org/W6743002019', 'https://openalex.org/W2800653071', 'https://openalex.org/W2895593755', 'https://openalex.org/W6712784185', 'https://openalex.org/W6639204139', 'https://openalex.org/W2769533150', 'https://openalex.org/W2913133519', 'https://openalex.org/W2911816400', 'https://openalex.org/W2910577860', 'https://openalex.org/W591106111', 'https://openalex.org/W2317293732', 'https://openalex.org/W4400434880', 'https://openalex.org/W2765825931', 'https://openalex.org/W1442392778', 'https://openalex.org/W3099425575', 'https://openalex.org/W2135895405', 'https://openalex.org/W3000389243', 'https://openalex.org/W2399672709', 'https://openalex.org/W1539694548', 'https://openalex.org/W58926815', 'https://openalex.org/W2889532686', 'https://openalex.org/W3006561225', 'https://openalex.org/W4247519706', 'https://openalex.org/W2401498750', 'https://openalex.org/W148163061', 'https://openalex.org/W1584375480', 'https://openalex.org/W2989708046', 'https://openalex.org/W3032175222', 'https://openalex.org/W4392486316', 'https://openalex.org/W122259971', 'https://openalex.org/W4327892058', 'https://openalex.org/W2154393540', 'https://openalex.org/W4300980117', 'https://openalex.org/W840886904', 'https://openalex.org/W2477810735', 'https://openalex.org/W2577327720', 'https://openalex.org/W2122410182', 'https://openalex.org/W2334930053', 'https://openalex.org/W1539609515', 'https://openalex.org/W4318351475', 'https://openalex.org/W1903683453', 'https://openalex.org/W2746068898', 'https://openalex.org/W3003668765', 'https://openalex.org/W4378474086', 'https://openalex.org/W4380136719']",2025-08-20
https://openalex.org/W4413967603,https://doi.org/10.1109/icmlcn64995.2025.11140271,Generative AI for Short Sound Message Transmission in the Internet of Things,,"['https://openalex.org/W3032636897', 'https://openalex.org/W2071608040', 'https://openalex.org/W2588026839', 'https://openalex.org/W4379211989', 'https://openalex.org/W2100458749', 'https://openalex.org/W2161913347', 'https://openalex.org/W4223636626', 'https://openalex.org/W2009059240', 'https://openalex.org/W3211336390', 'https://openalex.org/W4322503920', 'https://openalex.org/W4400113230', 'https://openalex.org/W4372267124', 'https://openalex.org/W3212947338', 'https://openalex.org/W2973049979', 'https://openalex.org/W2191779130', 'https://openalex.org/W2962866891', 'https://openalex.org/W2178339699', 'https://openalex.org/W2112333831', 'https://openalex.org/W2087406010', 'https://openalex.org/W4281826977', 'https://openalex.org/W3162472933', 'https://openalex.org/W3010512579', 'https://openalex.org/W3082878506', 'https://openalex.org/W2963964246', 'https://openalex.org/W4383989372', 'https://openalex.org/W2950151997', 'https://openalex.org/W4380136719', 'https://openalex.org/W3130402721']",2025-05-26
https://openalex.org/W4414140450,https://doi.org/10.1109/icmew68306.2025.11152154,AI Music Artist Toolkit (AIMAT) - A Modular Environment for Experimenting with AI in Music,,"['https://openalex.org/W4404892063', 'https://openalex.org/W4234703843', 'https://openalex.org/W4402518153', 'https://openalex.org/W3092879656', 'https://openalex.org/W4385764036', 'https://openalex.org/W4407571725', 'https://openalex.org/W2911664431', 'https://openalex.org/W4225281045', 'https://openalex.org/W4318752004', 'https://openalex.org/W4380993674', 'https://openalex.org/W4380136719', 'https://openalex.org/W4318351475', 'https://openalex.org/W4287802874', 'https://openalex.org/W2971458685', 'https://openalex.org/W4225716783', 'https://openalex.org/W4292419534', 'https://openalex.org/W4388718054', 'https://openalex.org/W4383993968']",2025-06-30
https://openalex.org/W4414140891,https://doi.org/10.1109/icmew68306.2025.11152218,"M6(GPT)3: Generating Multitrack Modifiable Multi-Minute MIDI Music from Text using Genetic Algorithms, Probabilistic Methods and GPT Models in any Progression and Time Signature",,"['https://openalex.org/W3092879656', 'https://openalex.org/W4372259826', 'https://openalex.org/W1965906514', 'https://openalex.org/W2621868418', 'https://openalex.org/W1590874898', 'https://openalex.org/W1966910303', 'https://openalex.org/W4246278590', 'https://openalex.org/W3114948430', 'https://openalex.org/W2772474126', 'https://openalex.org/W4396819664', 'https://openalex.org/W398859631', 'https://openalex.org/W4287802874', 'https://openalex.org/W3127633281', 'https://openalex.org/W2338312508', 'https://openalex.org/W4221148975', 'https://openalex.org/W4380136719', 'https://openalex.org/W4379251846', 'https://openalex.org/W149236175', 'https://openalex.org/W3049272330', 'https://openalex.org/W4318351475', 'https://openalex.org/W4399418334']",2025-06-30
https://openalex.org/W4414877730,https://doi.org/10.1007/s11432-024-4564-6,SoulSearch: applying heuristic optimization to enhance text-to-image generation with personalized human-LMM collaboration,,"['https://openalex.org/W2090022222', 'https://openalex.org/W4205622332', 'https://openalex.org/W2152195021', 'https://openalex.org/W4383301640', 'https://openalex.org/W2046609661', 'https://openalex.org/W2317744239', 'https://openalex.org/W2123066915', 'https://openalex.org/W2797319902', 'https://openalex.org/W2011272021', 'https://openalex.org/W4285319657', 'https://openalex.org/W2050927287', 'https://openalex.org/W4283172359', 'https://openalex.org/W2132498545', 'https://openalex.org/W2005383742', 'https://openalex.org/W4361229539', 'https://openalex.org/W4298185919', 'https://openalex.org/W4385774961', 'https://openalex.org/W2914304175', 'https://openalex.org/W4410512518', 'https://openalex.org/W2552104267', 'https://openalex.org/W4205387711', 'https://openalex.org/W4383174046', 'https://openalex.org/W4409735071', 'https://openalex.org/W2769533150', 'https://openalex.org/W4383472654', 'https://openalex.org/W4378498678', 'https://openalex.org/W4409720128', 'https://openalex.org/W4322718191', 'https://openalex.org/W4297630849', 'https://openalex.org/W4387156669', 'https://openalex.org/W4302011534', 'https://openalex.org/W4366850747', 'https://openalex.org/W2914783728', 'https://openalex.org/W4315817451', 'https://openalex.org/W2100291730', 'https://openalex.org/W2783539613', 'https://openalex.org/W4323717348', 'https://openalex.org/W4386655647', 'https://openalex.org/W2530938137', 'https://openalex.org/W4380136719', 'https://openalex.org/W3040472729', 'https://openalex.org/W2768343520']",2025-09-28
https://openalex.org/W4415350508,https://doi.org/10.1145/3746278.3759380,Precision Emotion Regulation for Mood Disorders: A Micro-Expression-Based Closed-Loop Music Therapy System,,"['https://openalex.org/W2980031356', 'https://openalex.org/W1927641621', 'https://openalex.org/W4399178463', 'https://openalex.org/W4244432079', 'https://openalex.org/W2073395356', 'https://openalex.org/W4399258801', 'https://openalex.org/W4298087588', 'https://openalex.org/W2194775991', 'https://openalex.org/W2950978907', 'https://openalex.org/W4281898042', 'https://openalex.org/W4242769062', 'https://openalex.org/W4411338008', 'https://openalex.org/W1979494212', 'https://openalex.org/W4405713210', 'https://openalex.org/W4396547429', 'https://openalex.org/W4394952467', 'https://openalex.org/W4372260310', 'https://openalex.org/W2006426145', 'https://openalex.org/W2072038603', 'https://openalex.org/W4251046910', 'https://openalex.org/W4380136719']",2025-10-20
https://openalex.org/W4415445931,https://doi.org/10.1016/j.eswa.2025.130059,BandCondiNet: Parallel transformers-based conditional popular music generation with multi-view features,,"['https://openalex.org/W4226103472', 'https://openalex.org/W4286250570', 'https://openalex.org/W2758804652', 'https://openalex.org/W4377140494', 'https://openalex.org/W3211570179', 'https://openalex.org/W4392647990', 'https://openalex.org/W4392271377', 'https://openalex.org/W3216006394', 'https://openalex.org/W4389371446', 'https://openalex.org/W4377719641', 'https://openalex.org/W4383646190', 'https://openalex.org/W2809931103', 'https://openalex.org/W4385245566', 'https://openalex.org/W4307008216', 'https://openalex.org/W4214540501', 'https://openalex.org/W4380136719', 'https://openalex.org/W4404520762', 'https://openalex.org/W2963799213', 'https://openalex.org/W4288391450', 'https://openalex.org/W3043861921', 'https://openalex.org/W4242489199', 'https://openalex.org/W4383265516', 'https://openalex.org/W4412719254', 'https://openalex.org/W2475687244', 'https://openalex.org/W2914911817', 'https://openalex.org/W2606555609', 'https://openalex.org/W3183859557']",2025-10-23
https://openalex.org/W2963425185,https://doi.org/10.21437/interspeech.2018-2341,Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech,"In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.The proposed model can be viewed as a speech version of Word2Vec [1].Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training.Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text.The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.","['https://openalex.org/W1494198834', 'https://openalex.org/W2132631284', 'https://openalex.org/W2103318667', 'https://openalex.org/W2187089797', 'https://openalex.org/W2556930864', 'https://openalex.org/W2133564696', 'https://openalex.org/W2611470828', 'https://openalex.org/W2139501017', 'https://openalex.org/W1854884267', 'https://openalex.org/W2964116568', 'https://openalex.org/W2951559648', 'https://openalex.org/W2962904995', 'https://openalex.org/W2493916176', 'https://openalex.org/W2962925243', 'https://openalex.org/W2251012068', 'https://openalex.org/W2059652594', 'https://openalex.org/W4294170691', 'https://openalex.org/W1860935423', 'https://openalex.org/W2130942839', 'https://openalex.org/W2137735870', 'https://openalex.org/W2176085882', 'https://openalex.org/W4300822525', 'https://openalex.org/W2142625445', 'https://openalex.org/W2296681920', 'https://openalex.org/W2468716020', 'https://openalex.org/W2252211741', 'https://openalex.org/W2170682101', 'https://openalex.org/W2190506272', 'https://openalex.org/W2951216052', 'https://openalex.org/W2137010615', 'https://openalex.org/W2296283641', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963366649', 'https://openalex.org/W2767224889', 'https://openalex.org/W2251253014', 'https://openalex.org/W2963296402', 'https://openalex.org/W2157331557', 'https://openalex.org/W2962753610', 'https://openalex.org/W2080100102', 'https://openalex.org/W2891197287', 'https://openalex.org/W2250539671', 'https://openalex.org/W2963571336', 'https://openalex.org/W2899771611', 'https://openalex.org/W2550241133']",2018-08-28
https://openalex.org/W2898630520,https://doi.org/10.17863/cam.35236,Confidence Estimation and Deletion Prediction Using Bidirectional Recurrent Neural Networks,"The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.","['https://openalex.org/W1631260214', 'https://openalex.org/W2143577772', 'https://openalex.org/W1577202350', 'https://openalex.org/W2406451296', 'https://openalex.org/W2132030416', 'https://openalex.org/W2110073835', 'https://openalex.org/W2407922671', 'https://openalex.org/W2889080286', 'https://openalex.org/W2519224033', 'https://openalex.org/W2729190387', 'https://openalex.org/W2129334286', 'https://openalex.org/W2128135494', 'https://openalex.org/W2131774270', 'https://openalex.org/W1976526581', 'https://openalex.org/W2085628288', 'https://openalex.org/W2889162046', 'https://openalex.org/W34303869', 'https://openalex.org/W2963217176', 'https://openalex.org/W2166810516', 'https://openalex.org/W2783549597', 'https://openalex.org/W1795658042', 'https://openalex.org/W2144920829', 'https://openalex.org/W1524333225', 'https://openalex.org/W2407420912', 'https://openalex.org/W55333121', 'https://openalex.org/W2952566282', 'https://openalex.org/W2169806449', 'https://openalex.org/W2671812860', 'https://openalex.org/W2280141299', 'https://openalex.org/W2129999749', 'https://openalex.org/W2271840356', 'https://openalex.org/W2791556425', 'https://openalex.org/W2046566693', 'https://openalex.org/W2622203030']",2018-12-21
https://openalex.org/W1994606281,https://doi.org/10.1109/icassp.2013.6639084,Multilingual training of deep neural networks,"We investigate multilingual modeling in the context of a deep neural network (DNN) - hidden Markov model (HMM) hybrid, where the DNN outputs are used as the HMM state likelihoods. By viewing neural networks as a cascade of feature extractors followed by a logistic regression classifier, we hypothesise that the hidden layers, which act as feature extractors, will be transferable between languages. As a corollary, we propose that training the hidden layers on multiple languages makes them more suitable for such cross-lingual transfer. We experimentally confirm these hypotheses on the GlobalPhone corpus using seven languages from three different language families: Germanic, Romance, and Slavic. The experiments demonstrate substantial improvements over a monolingual DNN-HMM hybrid baseline, and hint at avenues of further exploration.","['https://openalex.org/W2407897255', 'https://openalex.org/W2131042651', 'https://openalex.org/W2090764203', 'https://openalex.org/W811578723', 'https://openalex.org/W2165712214', 'https://openalex.org/W2120209245', 'https://openalex.org/W2136922672', 'https://openalex.org/W4231109964', 'https://openalex.org/W2163922914', 'https://openalex.org/W2169189000', 'https://openalex.org/W2127982613', 'https://openalex.org/W2110871230', 'https://openalex.org/W1993882792', 'https://openalex.org/W2147768505', 'https://openalex.org/W6602682705', 'https://openalex.org/W6631362777', 'https://openalex.org/W198385923', 'https://openalex.org/W2152175008', 'https://openalex.org/W2033436836', 'https://openalex.org/W6601939441', 'https://openalex.org/W1991180839', 'https://openalex.org/W73572011', 'https://openalex.org/W2111306781', 'https://openalex.org/W6674389704', 'https://openalex.org/W6638728282', 'https://openalex.org/W1980850109', 'https://openalex.org/W2123798005', 'https://openalex.org/W1981706894', 'https://openalex.org/W1524333225', 'https://openalex.org/W1846073453', 'https://openalex.org/W2072128103', 'https://openalex.org/W1553004968', 'https://openalex.org/W66627554', 'https://openalex.org/W2096672142', 'https://openalex.org/W4285719527', 'https://openalex.org/W47568227']",2013-05-01
https://openalex.org/W2127141656,https://doi.org/10.1145/1143844.1143891,Connectionist temporal classification,"Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.","['https://openalex.org/W2110871230', 'https://openalex.org/W2147880316', 'https://openalex.org/W4285719527', 'https://openalex.org/W2125838338', 'https://openalex.org/W1554663460', 'https://openalex.org/W1553004968', 'https://openalex.org/W2131774270', 'https://openalex.org/W1542537254', 'https://openalex.org/W1566256432', 'https://openalex.org/W2130984546', 'https://openalex.org/W183625566', 'https://openalex.org/W1823409095', 'https://openalex.org/W2079735306', 'https://openalex.org/W2064675550', 'https://openalex.org/W2150355110']",2006-01-01
https://openalex.org/W3005680577,https://doi.org/10.48550/arxiv.2002.05709,A Simple Framework for Contrastive Learning of Visual Representations,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","['https://openalex.org/W2757910899', 'https://openalex.org/W2949117887', 'https://openalex.org/W2136922672', 'https://openalex.org/W2097117768', 'https://openalex.org/W2063971957', 'https://openalex.org/W2962824366', 'https://openalex.org/W2117539524', 'https://openalex.org/W1846799578', 'https://openalex.org/W12634471', 'https://openalex.org/W2194775991', 'https://openalex.org/W2970241862', 'https://openalex.org/W1686810756', 'https://openalex.org/W2951004968', 'https://openalex.org/W2326925005', 'https://openalex.org/W2138621090', 'https://openalex.org/W2518108298', 'https://openalex.org/W2944828972', 'https://openalex.org/W2017814585', 'https://openalex.org/W2155541015', 'https://openalex.org/W343636949', 'https://openalex.org/W2798991696', 'https://openalex.org/W2155904486', 'https://openalex.org/W2099471712', 'https://openalex.org/W2640408555', 'https://openalex.org/W2950577311', 'https://openalex.org/W2321533354', 'https://openalex.org/W2953327099', 'https://openalex.org/W2990500698', 'https://openalex.org/W2942203175', 'https://openalex.org/W780950768', 'https://openalex.org/W2964420626', 'https://openalex.org/W2951873722', 'https://openalex.org/W3001197829', 'https://openalex.org/W1993309459', 'https://openalex.org/W2187089797', 'https://openalex.org/W2949736877', 'https://openalex.org/W2533598788', 'https://openalex.org/W2962369866', 'https://openalex.org/W2163605009', 'https://openalex.org/W2994536315', 'https://openalex.org/W2971155163', 'https://openalex.org/W1977295328', 'https://openalex.org/W2804935296', 'https://openalex.org/W2148349024', 'https://openalex.org/W2987283559', 'https://openalex.org/W2746314669', 'https://openalex.org/W2913939497', 'https://openalex.org/W2995489995', 'https://openalex.org/W2962742544', 'https://openalex.org/W2949194345', 'https://openalex.org/W2842511635', 'https://openalex.org/W2555897561', 'https://openalex.org/W2949517790', 'https://openalex.org/W2031489346', 'https://openalex.org/W2047643928', 'https://openalex.org/W2622263826', 'https://openalex.org/W2979579363', 'https://openalex.org/W3099206234', 'https://openalex.org/W3118608800', 'https://openalex.org/W2998388430']",2020-02-13
https://openalex.org/W2970049541,,Cross-lingual Language Model Pretraining,"Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",[],2019-01-22
https://openalex.org/W3095410713,https://doi.org/10.21437/interspeech.2020-2826,MLS: A Large-Scale Multilingual Dataset for Speech Research,"This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.","['https://openalex.org/W2953190524', 'https://openalex.org/W4297818305', 'https://openalex.org/W2087347434', 'https://openalex.org/W2127141656', 'https://openalex.org/W4385245566', 'https://openalex.org/W2134800885', 'https://openalex.org/W2786234940', 'https://openalex.org/W2996159613', 'https://openalex.org/W3030437843', 'https://openalex.org/W3198270883', 'https://openalex.org/W2972630480', 'https://openalex.org/W2936774411', 'https://openalex.org/W3093502935', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096104971', 'https://openalex.org/W2937197076', 'https://openalex.org/W2520160253', 'https://openalex.org/W2991213871', 'https://openalex.org/W2995181338', 'https://openalex.org/W2975381464', 'https://openalex.org/W3001899777', 'https://openalex.org/W2781384251', 'https://openalex.org/W2963979492', 'https://openalex.org/W2626778328', 'https://openalex.org/W2146502635', 'https://openalex.org/W2972359262', 'https://openalex.org/W2087064593']",2020-10-25
https://openalex.org/W2124509324,https://doi.org/10.1109/tpami.2010.57,Product Quantization for Nearest Neighbor Search,"This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.","['https://openalex.org/W1672197616', 'https://openalex.org/W2124222502', 'https://openalex.org/W2024668293', 'https://openalex.org/W6632397832', 'https://openalex.org/W2162006472', 'https://openalex.org/W1627400044', 'https://openalex.org/W2171790913', 'https://openalex.org/W4251572646', 'https://openalex.org/W6795644987', 'https://openalex.org/W2099907898', 'https://openalex.org/W2099253838', 'https://openalex.org/W2128017662', 'https://openalex.org/W2145607950', 'https://openalex.org/W2154956324', 'https://openalex.org/W1566135517', 'https://openalex.org/W6697214482', 'https://openalex.org/W1556531089', 'https://openalex.org/W2020308406', 'https://openalex.org/W2148809531', 'https://openalex.org/W2151103935', 'https://openalex.org/W4254197176', 'https://openalex.org/W2131846894', 'https://openalex.org/W1972378554', 'https://openalex.org/W2543932557', 'https://openalex.org/W2149991777', 'https://openalex.org/W196542726', 'https://openalex.org/W2141362318', 'https://openalex.org/W1541459201', 'https://openalex.org/W2293597654', 'https://openalex.org/W3160851792', 'https://openalex.org/W2008526771', 'https://openalex.org/W1502916507', 'https://openalex.org/W2158658988', 'https://openalex.org/W630138847', 'https://openalex.org/W1491105865']",2010-03-19
https://openalex.org/W2996383576,,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.,"['https://openalex.org/W2346964103', 'https://openalex.org/W2518108298', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962901777', 'https://openalex.org/W10548402', 'https://openalex.org/W2842511635', 'https://openalex.org/W2547875792', 'https://openalex.org/W2520160253', 'https://openalex.org/W2962784628', 'https://openalex.org/W3011411500', 'https://openalex.org/W3127686677', 'https://openalex.org/W2947591107', 'https://openalex.org/W2124509324', 'https://openalex.org/W2941814890', 'https://openalex.org/W2963425185', 'https://openalex.org/W2965373594', 'https://openalex.org/W2936295285', 'https://openalex.org/W2926827382', 'https://openalex.org/W2973049979', 'https://openalex.org/W2153579005', 'https://openalex.org/W2889282842', 'https://openalex.org/W2936774411', 'https://openalex.org/W2940180244', 'https://openalex.org/W2987741655', 'https://openalex.org/W2963382687', 'https://openalex.org/W2933138175', 'https://openalex.org/W2786459654', 'https://openalex.org/W1494198834', 'https://openalex.org/W2794209590', 'https://openalex.org/W1885680957', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963620343', 'https://openalex.org/W2141440284']",2020-04-30
https://openalex.org/W2292087804,,Speech recognition and keyword spotting for low-resource languages : Babel project research at CUED,"Recently there has been increased interest in Automatic Speech Recognition (ASR) and Key Word Spotting (KWS) systems for low resource languages. One of the driving forces for this research di-rection is the IARPA Babel project. This paper describes some of the research funded by this project at Cambridge University, as part of the Lorelei team co-ordinated by IBM. A range of topics are dis-cussed including: deep neural network based acoustic models; data augmentation; and zero acoustic model resource systems. Perfor-mance for all approaches is evaluated using the Limited (approx-imately 10 hours) and/or Full (approximately 80 hours) language packs distributed by IARPA. Both KWS and ASR performance fig-ures are given. Though absolute performance varies from language to language, and keyword list, the approaches described show con-sistent trends over the languages investigated to date. Using com-parable systems over the five Option Period 1 languages indicates a strong correlation between ASR performance and KWS perfor-mance.","['https://openalex.org/W2099621636', 'https://openalex.org/W2028956843', 'https://openalex.org/W2165921245', 'https://openalex.org/W2039285212', 'https://openalex.org/W2165712214', 'https://openalex.org/W1553004968', 'https://openalex.org/W1989674786', 'https://openalex.org/W2103933358', 'https://openalex.org/W1992912377', 'https://openalex.org/W2184045248', 'https://openalex.org/W1604771987', 'https://openalex.org/W2150907703', 'https://openalex.org/W23025778', 'https://openalex.org/W1975113979', 'https://openalex.org/W2160306971', 'https://openalex.org/W2096140469', 'https://openalex.org/W2106554350', 'https://openalex.org/W2139453310', 'https://openalex.org/W1993952617', 'https://openalex.org/W1994606281', 'https://openalex.org/W1904457459', 'https://openalex.org/W2023155336', 'https://openalex.org/W2169272853', 'https://openalex.org/W2442329935', 'https://openalex.org/W1975550806', 'https://openalex.org/W2103635001', 'https://openalex.org/W114193738', 'https://openalex.org/W2184343439', 'https://openalex.org/W1571931074', 'https://openalex.org/W2002342963', 'https://openalex.org/W2030694415', 'https://openalex.org/W2129334286', 'https://openalex.org/W2125610823', 'https://openalex.org/W2286443923', 'https://openalex.org/W2164505566', 'https://openalex.org/W2090764203', 'https://openalex.org/W2407897255', 'https://openalex.org/W2114016253']",2014-05-14
https://openalex.org/W3099782249,,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,,[],2020-06-20
https://openalex.org/W2981991061,https://doi.org/10.48550/arxiv.1910.09932,Improving Transformer-based Speech Recognition Using Unsupervised Pre-training,"Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3%, exceeding the best end-to-end model by over 0.2% absolute CER. With more pre-training data, we can further reduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.","['https://openalex.org/W2973157397', 'https://openalex.org/W2842511635', 'https://openalex.org/W2327501763', 'https://openalex.org/W2972894903', 'https://openalex.org/W2972818416', 'https://openalex.org/W2514741789', 'https://openalex.org/W2965373594', 'https://openalex.org/W2962893195', 'https://openalex.org/W2963939538', 'https://openalex.org/W2963341956', 'https://openalex.org/W1526236009', 'https://openalex.org/W2963242190', 'https://openalex.org/W2913718171', 'https://openalex.org/W2963850025', 'https://openalex.org/W2972943112', 'https://openalex.org/W1507177964', 'https://openalex.org/W2526425061', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963403868', 'https://openalex.org/W2922709902', 'https://openalex.org/W343636949', 'https://openalex.org/W2963236196', 'https://openalex.org/W2950813464', 'https://openalex.org/W2168961642', 'https://openalex.org/W2133856945', 'https://openalex.org/W2973049979', 'https://openalex.org/W2515753980', 'https://openalex.org/W2900898015', 'https://openalex.org/W2892009249', 'https://openalex.org/W2962739339']",2019-10-22
https://openalex.org/W2025198378,https://doi.org/10.1109/icassp.2013.6639081,Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers,"In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.","['https://openalex.org/W2123798005', 'https://openalex.org/W1968147312', 'https://openalex.org/W2151841219', 'https://openalex.org/W2033436836', 'https://openalex.org/W2130414229', 'https://openalex.org/W6678242812', 'https://openalex.org/W2120209245', 'https://openalex.org/W2160815625', 'https://openalex.org/W2913340405', 'https://openalex.org/W2407897255', 'https://openalex.org/W2131042651', 'https://openalex.org/W2013205774', 'https://openalex.org/W2407441242', 'https://openalex.org/W2127982613', 'https://openalex.org/W2117130368', 'https://openalex.org/W2394932179', 'https://openalex.org/W2062164080', 'https://openalex.org/W2147768505', 'https://openalex.org/W2076794394', 'https://openalex.org/W2296748324', 'https://openalex.org/W1993882792', 'https://openalex.org/W2160306971', 'https://openalex.org/W1987238397', 'https://openalex.org/W2403195671', 'https://openalex.org/W319941341', 'https://openalex.org/W2144792281', 'https://openalex.org/W2914746235', 'https://openalex.org/W2120480077', 'https://openalex.org/W2950789693', 'https://openalex.org/W2253807446', 'https://openalex.org/W2184045248', 'https://openalex.org/W217970951']",2013-05-01
https://openalex.org/W2896457183,https://doi.org/10.5281/zenodo.12561108,EMBI,"Requirements are an integral part of industry operation and projects. Not only do requirements dictate industrial operations, but they are used in legally binding contracts between supplier and purchaser. Some companies even have requirements as their core business. Most requirements are found in textual documents, this brings a couple of challenges such as ambiguity, scalability, maintenance, and finding relevant and related requirements. Having the requirements in a machine-readable format would be a solution to these challenges, however, existing requirements need to be transformed into machine-readable requirements using NLP technology. Using state-of-the-art NLP methods based on end-to-end neural modelling on such documents is not trivial because the language is technical and domain-specific and training data is not available. In this paper, we focus on one step in that direction, namely scope detection of textual requirements using weak supervision and a simple classifier based on BERT general domain word embeddings and show that using openly available data, it is possible to get promising results on domain-specific requirements documents.","['https://openalex.org/W2131462252', 'https://openalex.org/W2963310665', 'https://openalex.org/W3104033643', 'https://openalex.org/W2158108973', 'https://openalex.org/W2949433733', 'https://openalex.org/W2025768430', 'https://openalex.org/W2963748441', 'https://openalex.org/W2149933564', 'https://openalex.org/W1840435438', 'https://openalex.org/W2153579005', 'https://openalex.org/W2251939518', 'https://openalex.org/W2270070752', 'https://openalex.org/W2413794162', 'https://openalex.org/W2880875857', 'https://openalex.org/W2130903752', 'https://openalex.org/W2963026768', 'https://openalex.org/W1599016936', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963804993', 'https://openalex.org/W2886490473', 'https://openalex.org/W2130158090', 'https://openalex.org/W1486649854', 'https://openalex.org/W2951714314', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963563735', 'https://openalex.org/W2897076808', 'https://openalex.org/W2158139315', 'https://openalex.org/W2507974895', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963564796', 'https://openalex.org/W2117130368', 'https://openalex.org/W2806120502', 'https://openalex.org/W2962808855', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962718483', 'https://openalex.org/W2963846996', 'https://openalex.org/W2551396370', 'https://openalex.org/W2963918774', 'https://openalex.org/W2888329843', 'https://openalex.org/W2962739339', 'https://openalex.org/W2121227244', 'https://openalex.org/W2144578941', 'https://openalex.org/W2963644595', 'https://openalex.org/W2610858497', 'https://openalex.org/W2784823820', 'https://openalex.org/W2525778437', 'https://openalex.org/W3098057198', 'https://openalex.org/W2250539671', 'https://openalex.org/W2963403868', 'https://openalex.org/W2396767181', 'https://openalex.org/W131533222', 'https://openalex.org/W2891602716', 'https://openalex.org/W2963159690', 'https://openalex.org/W2170973209', 'https://openalex.org/W2131744502']",2024-06-27
https://openalex.org/W2141440284,,Scalable Modified Kneser-Ney Language Model Estimation,"We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0 % of the wall time taken by SRILM. The code is open source as part of KenLM. 1","['https://openalex.org/W16967297', 'https://openalex.org/W2461884057', 'https://openalex.org/W2087735403', 'https://openalex.org/W2171457693', 'https://openalex.org/W2141151634', 'https://openalex.org/W2124807415', 'https://openalex.org/W2106540279', 'https://openalex.org/W2134800885', 'https://openalex.org/W2075136957', 'https://openalex.org/W1685426458', 'https://openalex.org/W2173213060', 'https://openalex.org/W67447785', 'https://openalex.org/W2158195707', 'https://openalex.org/W127782651', 'https://openalex.org/W22168010', 'https://openalex.org/W1631260214', 'https://openalex.org/W2089395510', 'https://openalex.org/W2109664771', 'https://openalex.org/W1934041838', 'https://openalex.org/W2250947781', 'https://openalex.org/W2180952760', 'https://openalex.org/W3148153833', 'https://openalex.org/W2122429665', 'https://openalex.org/W2020191321', 'https://openalex.org/W2134237567', 'https://openalex.org/W2159755860']",2013-08-01
https://openalex.org/W2696253854,https://doi.org/10.1109/icassp.2017.7953074,Stimulated training for automatic speech recognition and keyword search in limited resource conditions,"Training neural network acoustic models on limited quantities of data is a challenging task. A number of techniques have been proposed to improve generalisation. This paper investigates one such technique called stimulated training. It enables standard criteria such as cross-entropy to enforce spatial constraints on activations originating from different units. Having different regions being active depending on the input unit may help network to discriminate better and as a consequence yield lower error rates. This paper investigates stimulated training for automatic speech recognition of a number of languages representing different families, alphabets, phone sets and vocabulary sizes. In particular, it looks at ensembles of stimulated networks to ensure that improved generalisation will withstand system combination effects. In order to assess stimulated training beyond 1-best transcription accuracy, this paper looks at keyword search as a proxy for assessing quality of lattices. Experiments are conducted on IARPA Babel program languages including the surprise language of OpenKWS 2016 competition.","['https://openalex.org/W6713762819', 'https://openalex.org/W6675409298', 'https://openalex.org/W6640036494', 'https://openalex.org/W6696315177', 'https://openalex.org/W2508220198', 'https://openalex.org/W2160815625', 'https://openalex.org/W2005708641', 'https://openalex.org/W2131342762', 'https://openalex.org/W2172097686', 'https://openalex.org/W6713507411', 'https://openalex.org/W2085628288', 'https://openalex.org/W2136922672', 'https://openalex.org/W2090861223', 'https://openalex.org/W1545083717', 'https://openalex.org/W6669599735', 'https://openalex.org/W1994606281', 'https://openalex.org/W2913340405', 'https://openalex.org/W6697040452', 'https://openalex.org/W1506752962', 'https://openalex.org/W2124558353', 'https://openalex.org/W6637061625', 'https://openalex.org/W6713813099', 'https://openalex.org/W2401969231', 'https://openalex.org/W2288977568', 'https://openalex.org/W2401430117', 'https://openalex.org/W2099621636', 'https://openalex.org/W2407420912', 'https://openalex.org/W2187089797', 'https://openalex.org/W2076250379', 'https://openalex.org/W3216781622', 'https://openalex.org/W1666984270', 'https://openalex.org/W3209042722', 'https://openalex.org/W1904365287', 'https://openalex.org/W2291975472', 'https://openalex.org/W2407080277', 'https://openalex.org/W1979651826', 'https://openalex.org/W2184045248', 'https://openalex.org/W1600722501']",2017-03-01
https://openalex.org/W2958953787,https://doi.org/10.48550/arxiv.1907.05019,Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,"We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair. We set a milestone towards this goal by building a single massively multilingual NMT model handling 103 languages trained on over 25 billion examples. Our system demonstrates effective transfer learning ability, significantly improving translation quality of low-resource languages, while keeping high-resource language translation quality on-par with competitive bilingual baselines. We provide in-depth analysis of various aspects of model building that are crucial to achieving quality and practicality in universal NMT. While we prototype a high-quality universal translation system, our extensive empirical analysis exposes issues that need to be further addressed, and we suggest directions for future research.","['https://openalex.org/W2952339051', 'https://openalex.org/W2797328513', 'https://openalex.org/W2936887742', 'https://openalex.org/W2550821151', 'https://openalex.org/W2552124255', 'https://openalex.org/W1594170634', 'https://openalex.org/W2101105183', 'https://openalex.org/W2417358553', 'https://openalex.org/W2962784628', 'https://openalex.org/W2145680191', 'https://openalex.org/W2561274697', 'https://openalex.org/W2933350699', 'https://openalex.org/W2557283755', 'https://openalex.org/W2908336025', 'https://openalex.org/W2963982496', 'https://openalex.org/W2887920589', 'https://openalex.org/W2525778437', 'https://openalex.org/W2921280978', 'https://openalex.org/W2512924740', 'https://openalex.org/W2251743902', 'https://openalex.org/W2117130368', 'https://openalex.org/W2964045208', 'https://openalex.org/W2809290718', 'https://openalex.org/W2963403868', 'https://openalex.org/W2785847164', 'https://openalex.org/W2932484476', 'https://openalex.org/W2597452529', 'https://openalex.org/W2512848817', 'https://openalex.org/W2760656271', 'https://openalex.org/W1753482797', 'https://openalex.org/W2922466325', 'https://openalex.org/W2983826605', 'https://openalex.org/W2185720331', 'https://openalex.org/W1738019091', 'https://openalex.org/W2788388592', 'https://openalex.org/W22168010', 'https://openalex.org/W2794365787', 'https://openalex.org/W2250875036', 'https://openalex.org/W2963250244', 'https://openalex.org/W2952036846', 'https://openalex.org/W2919188216', 'https://openalex.org/W2555745756', 'https://openalex.org/W2928941594', 'https://openalex.org/W2560647685', 'https://openalex.org/W2565989828', 'https://openalex.org/W2531207078', 'https://openalex.org/W2949454572', 'https://openalex.org/W2903193068', 'https://openalex.org/W2311921240', 'https://openalex.org/W2896691342', 'https://openalex.org/W2794363191', 'https://openalex.org/W2296073425', 'https://openalex.org/W2923622379', 'https://openalex.org/W2795900505', 'https://openalex.org/W2110518760', 'https://openalex.org/W2229833550', 'https://openalex.org/W2237537322', 'https://openalex.org/W630532510', 'https://openalex.org/W2903188467', 'https://openalex.org/W2948223045', 'https://openalex.org/W2091432990', 'https://openalex.org/W2946740027', 'https://openalex.org/W2657631929', 'https://openalex.org/W2952468927', 'https://openalex.org/W3035219538', 'https://openalex.org/W1924762813', 'https://openalex.org/W1857884451', 'https://openalex.org/W2962830144', 'https://openalex.org/W2537667581', 'https://openalex.org/W2426267443', 'https://openalex.org/W2626792426', 'https://openalex.org/W2963993537', 'https://openalex.org/W2572474373', 'https://openalex.org/W2766182427', 'https://openalex.org/W2613253298', 'https://openalex.org/W2401823607', 'https://openalex.org/W2184135559', 'https://openalex.org/W2994475016', 'https://openalex.org/W2963842982', 'https://openalex.org/W2095705004', 'https://openalex.org/W2950135462', 'https://openalex.org/W2963991316', 'https://openalex.org/W1533861849', 'https://openalex.org/W2199580741', 'https://openalex.org/W2624871570', 'https://openalex.org/W652269744', 'https://openalex.org/W2919290281', 'https://openalex.org/W2250342921', 'https://openalex.org/W2891089320', 'https://openalex.org/W2963216553', 'https://openalex.org/W2606722458', 'https://openalex.org/W2913340405', 'https://openalex.org/W2913946806', 'https://openalex.org/W2742079690', 'https://openalex.org/W2798362442', 'https://openalex.org/W2963351145', 'https://openalex.org/W2157331557', 'https://openalex.org/W2804145368', 'https://openalex.org/W2806311723', 'https://openalex.org/W2788190072', 'https://openalex.org/W2962982474', 'https://openalex.org/W2798926775', 'https://openalex.org/W2893749619', 'https://openalex.org/W2952650870', 'https://openalex.org/W2152477898', 'https://openalex.org/W2101096097', 'https://openalex.org/W2767206889', 'https://openalex.org/W2907121943', 'https://openalex.org/W2775461895', 'https://openalex.org/W2972909479', 'https://openalex.org/W2963633299', 'https://openalex.org/W2962724315', 'https://openalex.org/W2807535859', 'https://openalex.org/W1825672851', 'https://openalex.org/W1991564165', 'https://openalex.org/W2604763608', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964007535', 'https://openalex.org/W2951882776', 'https://openalex.org/W2765961751', 'https://openalex.org/W2888541716', 'https://openalex.org/W2610245951', 'https://openalex.org/W2952626150', 'https://openalex.org/W2991040477', 'https://openalex.org/W2964327384', 'https://openalex.org/W2952518244', 'https://openalex.org/W2788800397', 'https://openalex.org/W2784231336', 'https://openalex.org/W2741787148', 'https://openalex.org/W2555428947', 'https://openalex.org/W2123301721', 'https://openalex.org/W2078861931', 'https://openalex.org/W2613904329', 'https://openalex.org/W2339995566', 'https://openalex.org/W99485931', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964308564', 'https://openalex.org/W2120501001', 'https://openalex.org/W1602635394', 'https://openalex.org/W2952444318', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963887123', 'https://openalex.org/W1572401739', 'https://openalex.org/W1915251500', 'https://openalex.org/W2948697567', 'https://openalex.org/W2953212265', 'https://openalex.org/W2891924676', 'https://openalex.org/W2220350356', 'https://openalex.org/W2963341956', 'https://openalex.org/W2748679025', 'https://openalex.org/W2463008967']",2019-07-11
https://openalex.org/W2671812860,https://doi.org/10.1109/icassp.2017.7953259,The 2016 BBN Georgian telephone speech keyword spotting system,"In this paper we describe the 2016 BBN conversational telephone speech keyword spotting system; the culmination of four years of research and development under the IARPA Babel program. The system was constructed in response to the NIST Open Keyword Search (OpenKWS) evaluation of 2016. We present our technological breakthroughs in building top-performing keyword spotting processing systems for new languages, in the face of limited transcribed speech, noisy conditions, and limited system build time of one week.","['https://openalex.org/W2052274902', 'https://openalex.org/W1904457459', 'https://openalex.org/W1990294375', 'https://openalex.org/W2533125211', 'https://openalex.org/W2346259642', 'https://openalex.org/W6680912004', 'https://openalex.org/W6629930100', 'https://openalex.org/W6677408996', 'https://openalex.org/W2963217176', 'https://openalex.org/W6713435378', 'https://openalex.org/W2514741789', 'https://openalex.org/W2112739286', 'https://openalex.org/W6714027799', 'https://openalex.org/W2398829910', 'https://openalex.org/W6631362777', 'https://openalex.org/W2116930768', 'https://openalex.org/W2021167964', 'https://openalex.org/W2397152345', 'https://openalex.org/W2000654171', 'https://openalex.org/W6686045668', 'https://openalex.org/W2515737026', 'https://openalex.org/W1501286448', 'https://openalex.org/W2514805646', 'https://openalex.org/W2031804986', 'https://openalex.org/W2026369565', 'https://openalex.org/W2110006374', 'https://openalex.org/W6712271494', 'https://openalex.org/W2404324365', 'https://openalex.org/W2689696018', 'https://openalex.org/W1565198967', 'https://openalex.org/W2594610113', 'https://openalex.org/W2116261113', 'https://openalex.org/W2407922671', 'https://openalex.org/W2181607856', 'https://openalex.org/W2405416873', 'https://openalex.org/W1499864241', 'https://openalex.org/W2140571699', 'https://openalex.org/W1524333225', 'https://openalex.org/W2396672644']",2017-03-01
https://openalex.org/W2972943112,https://doi.org/10.21437/interspeech.2019-1473,An Unsupervised Autoregressive Model for Speech Representation Learning,"This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations.In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks.In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data.Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches.Further analysis shows that different levels of speech information are captured by our model at different layers.In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.","['https://openalex.org/W1522301498', 'https://openalex.org/W2962850167', 'https://openalex.org/W179875071', 'https://openalex.org/W2963571336', 'https://openalex.org/W2525778437', 'https://openalex.org/W2951585248', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2997574889', 'https://openalex.org/W343636949', 'https://openalex.org/W2599837529', 'https://openalex.org/W2550241133', 'https://openalex.org/W2758785877', 'https://openalex.org/W2150769028', 'https://openalex.org/W2888329843', 'https://openalex.org/W2519091744', 'https://openalex.org/W219040644', 'https://openalex.org/W2963425185', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963317665', 'https://openalex.org/W2963609956', 'https://openalex.org/W1686946872', 'https://openalex.org/W2190506272', 'https://openalex.org/W3125709657', 'https://openalex.org/W2024490156', 'https://openalex.org/W2194775991', 'https://openalex.org/W2896457183', 'https://openalex.org/W2827410935', 'https://openalex.org/W2395899413']",2019-09-13
https://openalex.org/W2964121744,https://doi.org/10.4230/lipics.dna.28.4,,"Designing complex, dynamic yet multi-functional materials and devices is challenging because the design spaces for these materials have numerous interdependent and often conflicting constraints. Taking inspiration from advances in artificial intelligence and their applications in material discovery, we propose a computational method for designing metamorphic DNA-co-polymerized hydrogel structures. The method consists of a coarse-grained simulation and a deep learning-guided optimization system for exploring the immense design space of these structures. Here, we develop a simple numeric simulation of DNA-co-polymerized hydrogel shape change and seek to find designs for structured hydrogels that can fold into the shapes of different Arabic numerals in different actuation states. We train a convolutional neural network to classify and score the geometric outputs of the coarse-grained simulation to provide autonomous feedback for design optimization. We then construct a genetic algorithm that generates and selects large batches of material designs that compete with one another to evolve and converge on optimal objective-matching designs. We show that we are able to explore the large design space and learn important parameters and traits. We identify vital relationships between the material scale size and the range of shape change that can be achieved by individual domains and we elucidate trade-offs between different design parameters. Finally, we discover material designs capable of transforming into multiple different digits in different actuation states.",[],2022-01-01
https://openalex.org/W2141820854,https://doi.org/10.1109/tasl.2009.2021723,Automatic Speech Recognition for Under-Resourced Languages: Application to Vietnamese Language,"This paper presents our work in automatic speech recognition (ASR) in the context of under-resourced languages with application to Vietnamese. Different techniques for bootstrapping acoustic models are presented. First, we present the use of acoustic-phonetic unit distances and the potential of crosslingual acoustic modeling for under-resourced languages. Experimental results on Vietnamese showed that with only a few hours of target language speech data, crosslingual context independent modeling worked better than crosslingual context dependent modeling. However, it was outperformed by the latter one, when more speech data were available. We concluded, therefore, that in both cases, crosslingual systems are better than monolingual baseline systems. The proposal of grapheme-based acoustic modeling, which avoids building a phonetic dictionary, is also investigated in our work. Finally, since the use of sub-word units (morphemes, syllables, characters, etc.) can reduce the high out-of-vocabulary rate and improve the lack of text resources in statistical language modeling for under-resourced languages, we propose several methods to decompose, normalize and combine word and sub-word lattices generated from different ASR systems. The proposed lattice combination scheme results in a relative syllable error rate reduction of 6.6% over the sentence MAP baseline method for a Vietnamese ASR task.","['https://openalex.org/W2094391517', 'https://openalex.org/W3150637114', 'https://openalex.org/W2158069733', 'https://openalex.org/W1886381962', 'https://openalex.org/W6607325517', 'https://openalex.org/W2105354660', 'https://openalex.org/W1510837697', 'https://openalex.org/W2153159136', 'https://openalex.org/W1846073453', 'https://openalex.org/W2132534451', 'https://openalex.org/W2106101143', 'https://openalex.org/W6600435476', 'https://openalex.org/W2033436836', 'https://openalex.org/W2154820175', 'https://openalex.org/W6636811518', 'https://openalex.org/W2594610113', 'https://openalex.org/W2141404479', 'https://openalex.org/W6601644286', 'https://openalex.org/W4299344082', 'https://openalex.org/W2403983345', 'https://openalex.org/W6713186479', 'https://openalex.org/W6712677759', 'https://openalex.org/W1532513216', 'https://openalex.org/W2401271578', 'https://openalex.org/W1520886244', 'https://openalex.org/W6605248007', 'https://openalex.org/W2139820289', 'https://openalex.org/W6713005659', 'https://openalex.org/W6684184871', 'https://openalex.org/W6679576089', 'https://openalex.org/W2135379785', 'https://openalex.org/W4388277788', 'https://openalex.org/W6675239441', 'https://openalex.org/W6606379130', 'https://openalex.org/W8908565', 'https://openalex.org/W6678034947', 'https://openalex.org/W6629517120', 'https://openalex.org/W813000', 'https://openalex.org/W58893626', 'https://openalex.org/W2138436078', 'https://openalex.org/W6600942878', 'https://openalex.org/W2142401923', 'https://openalex.org/W6600595526', 'https://openalex.org/W6602059946', 'https://openalex.org/W55691059', 'https://openalex.org/W156297382', 'https://openalex.org/W128364799', 'https://openalex.org/W88081813', 'https://openalex.org/W2165355580', 'https://openalex.org/W1631260214', 'https://openalex.org/W1984672765', 'https://openalex.org/W2129848961', 'https://openalex.org/W2101281673', 'https://openalex.org/W2401033099', 'https://openalex.org/W10572049', 'https://openalex.org/W144374866', 'https://openalex.org/W23024349', 'https://openalex.org/W40323203', 'https://openalex.org/W2587033535', 'https://openalex.org/W1525736366', 'https://openalex.org/W1547854313', 'https://openalex.org/W2406358862', 'https://openalex.org/W60702959', 'https://openalex.org/W67810553', 'https://openalex.org/W2399730253', 'https://openalex.org/W2401933876', 'https://openalex.org/W14331692', 'https://openalex.org/W2276283915', 'https://openalex.org/W2918013879', 'https://openalex.org/W2573273887', 'https://openalex.org/W50642703', 'https://openalex.org/W1494693235', 'https://openalex.org/W180168658', 'https://openalex.org/W2398235957']",2009-04-28
https://openalex.org/W2842511635,,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.","['https://openalex.org/W2412320034', 'https://openalex.org/W2187089797', 'https://openalex.org/W2160660844', 'https://openalex.org/W2152808281', 'https://openalex.org/W2964127152', 'https://openalex.org/W2163605009', 'https://openalex.org/W2302255633', 'https://openalex.org/W2014902591', 'https://openalex.org/W343636949', 'https://openalex.org/W2157331557', 'https://openalex.org/W2950726992', 'https://openalex.org/W1494198834', 'https://openalex.org/W2106053110', 'https://openalex.org/W2160815625', 'https://openalex.org/W2606347107', 'https://openalex.org/W1522301498', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963403868', 'https://openalex.org/W2127958135', 'https://openalex.org/W3099206234', 'https://openalex.org/W219040644', 'https://openalex.org/W2964043796', 'https://openalex.org/W2326925005', 'https://openalex.org/W1895577753', 'https://openalex.org/W2259472270', 'https://openalex.org/W2786036274', 'https://openalex.org/W2952186591', 'https://openalex.org/W1836465849', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962824366', 'https://openalex.org/W3037932933', 'https://openalex.org/W2114524997', 'https://openalex.org/W1681397005', 'https://openalex.org/W2130942839', 'https://openalex.org/W2070246124', 'https://openalex.org/W2037034710', 'https://openalex.org/W2950577311', 'https://openalex.org/W2112129677', 'https://openalex.org/W2119885245', 'https://openalex.org/W2950797609', 'https://openalex.org/W2963762683', 'https://openalex.org/W2146444479', 'https://openalex.org/W2152790380', 'https://openalex.org/W2131744502', 'https://openalex.org/W3189092450', 'https://openalex.org/W1566289585', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949536664', 'https://openalex.org/W2157364932', 'https://openalex.org/W2117539524']",2018-07-10
https://openalex.org/W2891616026,https://doi.org/10.1109/icassp.2018.8462180,An End-to-End Language-Tracking Speech Recognizer for Mixed-Language Speech,"End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity to build a monolithic multilingual ASR system with a language-independent neural network architecture. In our previous work, we proposed a monolithic neural network architecture that can recognize multiple languages, and showed its effectiveness compared with conventional language-dependent models. However, the model is not guaranteed to properly handle switches in language within an utterance, thus lacking the flexibility to recognize mixed-language speech such as code-switching. In this paper, we extend our model to enable dynamic tracking of the language within an utterance, and propose a training procedure that takes advantage of a newly created mixed-language speech corpus. Experimental results show that the extended model outperforms both language-dependent models and our previous model without suffering from performance degradation that could be associated with language switching.","['https://openalex.org/W2786835190', 'https://openalex.org/W6725219515', 'https://openalex.org/W6601563604', 'https://openalex.org/W1526236009', 'https://openalex.org/W2296073425', 'https://openalex.org/W2085628288', 'https://openalex.org/W6732447497', 'https://openalex.org/W2627092829', 'https://openalex.org/W2096140469', 'https://openalex.org/W2091746061', 'https://openalex.org/W1982774639', 'https://openalex.org/W1970890968', 'https://openalex.org/W6728910023', 'https://openalex.org/W6687566353', 'https://openalex.org/W6683008568', 'https://openalex.org/W2545177271', 'https://openalex.org/W37526647', 'https://openalex.org/W2193413348', 'https://openalex.org/W2512355221', 'https://openalex.org/W2577366047', 'https://openalex.org/W2154304575']",2018-04-01
https://openalex.org/W2811079561,https://doi.org/10.48550/arxiv.1806.10474,The challenge of realistic music generation: modelling raw audio at scale,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.","['https://openalex.org/W2963799213', 'https://openalex.org/W2963241221', 'https://openalex.org/W2963373786', 'https://openalex.org/W2474920236', 'https://openalex.org/W1665214252', 'https://openalex.org/W2770298516', 'https://openalex.org/W2752134738', 'https://openalex.org/W2210838531', 'https://openalex.org/W2587284713', 'https://openalex.org/W2795370625', 'https://openalex.org/W648143168', 'https://openalex.org/W2792210438', 'https://openalex.org/W2953046278', 'https://openalex.org/W2990244669', 'https://openalex.org/W2963042606', 'https://openalex.org/W2267126114', 'https://openalex.org/W2964121744', 'https://openalex.org/W2950299304', 'https://openalex.org/W2964307104', 'https://openalex.org/W2787214294', 'https://openalex.org/W2949382160', 'https://openalex.org/W2086161653', 'https://openalex.org/W2963840672', 'https://openalex.org/W2964122153', 'https://openalex.org/W2242818861', 'https://openalex.org/W2951535099', 'https://openalex.org/W2950067852', 'https://openalex.org/W2949899814', 'https://openalex.org/W2963790827', 'https://openalex.org/W2118776487', 'https://openalex.org/W2099257174', 'https://openalex.org/W1909320841', 'https://openalex.org/W2553374874', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963981733', 'https://openalex.org/W2962968839', 'https://openalex.org/W3122518304', 'https://openalex.org/W2793273050', 'https://openalex.org/W2770703741', 'https://openalex.org/W2786254735', 'https://openalex.org/W2278108219', 'https://openalex.org/W2787439980', 'https://openalex.org/W2769810959', 'https://openalex.org/W2951004968', 'https://openalex.org/W2952276042', 'https://openalex.org/W2963292439', 'https://openalex.org/W2099471712']",2018-06-26
https://openalex.org/W3032816972,,CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data,,"['https://openalex.org/W2963341956', 'https://openalex.org/W3082274269', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963979492', 'https://openalex.org/W2950133940', 'https://openalex.org/W2134800885', 'https://openalex.org/W2626778328', 'https://openalex.org/W2948902769', 'https://openalex.org/W2250539671', 'https://openalex.org/W2891555348', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963626623', 'https://openalex.org/W2250653840']",2019-11-01
https://openalex.org/W2971840980,https://doi.org/10.21437/interspeech.2019-2858,Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model,"Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages.They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models.This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages.Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model.The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).","['https://openalex.org/W2964303773', 'https://openalex.org/W2786835190', 'https://openalex.org/W2953212265', 'https://openalex.org/W2769437540', 'https://openalex.org/W2001458616', 'https://openalex.org/W2928941594', 'https://openalex.org/W2913244614', 'https://openalex.org/W2025401819', 'https://openalex.org/W1828163288', 'https://openalex.org/W2291975472', 'https://openalex.org/W2962361184', 'https://openalex.org/W2962760690', 'https://openalex.org/W2134054733', 'https://openalex.org/W2894835365', 'https://openalex.org/W2972347614', 'https://openalex.org/W2276408190', 'https://openalex.org/W2293009711', 'https://openalex.org/W2094147890', 'https://openalex.org/W2963414781', 'https://openalex.org/W1855892484', 'https://openalex.org/W2962893195', 'https://openalex.org/W2970925270', 'https://openalex.org/W2633884958', 'https://openalex.org/W2963211188', 'https://openalex.org/W2914699162', 'https://openalex.org/W2964002616', 'https://openalex.org/W2127982613', 'https://openalex.org/W2716988359', 'https://openalex.org/W2533125211', 'https://openalex.org/W2114016253', 'https://openalex.org/W1994606281', 'https://openalex.org/W1978660892', 'https://openalex.org/W1975550806']",2019-09-13
https://openalex.org/W1524956127,https://doi.org/10.1109/icassp.2015.7178862,Semi-supervised training in low-resource ASR and KWS,"In particular for ""low resource"" Keyword Search (KWS) and Speech-to-Text (STT) tasks, more untranscribed test data may be available than training data. Several approaches have been proposed to make this data useful during system development, even when initial systems have Word Error Rates (WER) above 70%. In this paper, we present a set of experiments on low-resource languages in telephony speech quality in Assamese, Bengali, Lao, Haitian, Zulu, and Tamil, demonstrating the impact that such techniques can have, in particular learning robust bottle-neck features on the test data. In the case of Tamil, when significantly more test data than training data is available, we integrated semi-supervised training and speaker adaptation on the test data, and achieved significant additional improvements in STT and KWS.","['https://openalex.org/W2093728366', 'https://openalex.org/W2090755665', 'https://openalex.org/W6608128620', 'https://openalex.org/W2136780519', 'https://openalex.org/W4205948809', 'https://openalex.org/W2134756243', 'https://openalex.org/W6669996083', 'https://openalex.org/W2087395764', 'https://openalex.org/W2028148926', 'https://openalex.org/W6618759765', 'https://openalex.org/W1970088388', 'https://openalex.org/W6679097437', 'https://openalex.org/W2155273149', 'https://openalex.org/W6713061419', 'https://openalex.org/W2117671523', 'https://openalex.org/W6629208684', 'https://openalex.org/W2594610113', 'https://openalex.org/W6675409298', 'https://openalex.org/W6609412284', 'https://openalex.org/W6604666349', 'https://openalex.org/W1987563958', 'https://openalex.org/W1983927863', 'https://openalex.org/W1965842648', 'https://openalex.org/W2085628288', 'https://openalex.org/W6670225552', 'https://openalex.org/W6650992335', 'https://openalex.org/W6682889407', 'https://openalex.org/W6615969787', 'https://openalex.org/W6672625215', 'https://openalex.org/W6696761078', 'https://openalex.org/W1490911310', 'https://openalex.org/W2129070077', 'https://openalex.org/W3037950864', 'https://openalex.org/W567546468', 'https://openalex.org/W247110447', 'https://openalex.org/W609062392', 'https://openalex.org/W2402401665', 'https://openalex.org/W201532657', 'https://openalex.org/W2099621636', 'https://openalex.org/W2078396654', 'https://openalex.org/W2156387975', 'https://openalex.org/W2079623482', 'https://openalex.org/W2089917322', 'https://openalex.org/W114193738', 'https://openalex.org/W2002342963']",2015-04-01
https://openalex.org/W2988736778,https://doi.org/10.48550/arxiv.1911.03912,Effectiveness of self-supervised pre-training for speech recognition,"We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.","['https://openalex.org/W2933138175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972943112', 'https://openalex.org/W1994606281', 'https://openalex.org/W2970119519', 'https://openalex.org/W2964110616', 'https://openalex.org/W2025482506', 'https://openalex.org/W2025198378', 'https://openalex.org/W2114347655', 'https://openalex.org/W2932675979', 'https://openalex.org/W2153579005', 'https://openalex.org/W3103005696', 'https://openalex.org/W2963807318', 'https://openalex.org/W3207342693', 'https://openalex.org/W2962907457', 'https://openalex.org/W2127141656', 'https://openalex.org/W2586148577', 'https://openalex.org/W2941814890', 'https://openalex.org/W2940322076', 'https://openalex.org/W2296681920', 'https://openalex.org/W2965373594', 'https://openalex.org/W2953190524', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W2998649947', 'https://openalex.org/W1545920196', 'https://openalex.org/W3015522062', 'https://openalex.org/W2995181338', 'https://openalex.org/W2787779284', 'https://openalex.org/W2963425185', 'https://openalex.org/W2055408826', 'https://openalex.org/W1978660892', 'https://openalex.org/W2291975472', 'https://openalex.org/W2059652594', 'https://openalex.org/W30845872', 'https://openalex.org/W2973049979', 'https://openalex.org/W2163922914', 'https://openalex.org/W2667408400', 'https://openalex.org/W2964115348', 'https://openalex.org/W2514608284', 'https://openalex.org/W2566587499', 'https://openalex.org/W2106440210', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963382687', 'https://openalex.org/W2936774411', 'https://openalex.org/W1524333225', 'https://openalex.org/W2981857663', 'https://openalex.org/W2964001192', 'https://openalex.org/W2948012107', 'https://openalex.org/W2963799213', 'https://openalex.org/W2172097686', 'https://openalex.org/W2557283755', 'https://openalex.org/W2124558353', 'https://openalex.org/W3127686677', 'https://openalex.org/W2520160253', 'https://openalex.org/W2608612081']",2019-11-10
https://openalex.org/W1978660892,https://doi.org/10.1109/icassp.2013.6639348,Multilingual acoustic models using distributed deep neural networks,"Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain from jointly training the languages on all data comes at an increased training time of roughly four weeks, compared to two weeks (monolingual) and one week (crosslingual).","['https://openalex.org/W2127982613', 'https://openalex.org/W2013205774', 'https://openalex.org/W2399153411', 'https://openalex.org/W2131042651', 'https://openalex.org/W2407897255', 'https://openalex.org/W6605800014', 'https://openalex.org/W6678242812', 'https://openalex.org/W6684859321', 'https://openalex.org/W2120209245', 'https://openalex.org/W6601939441', 'https://openalex.org/W2407441242', 'https://openalex.org/W6656619859', 'https://openalex.org/W2130414229', 'https://openalex.org/W2123798005', 'https://openalex.org/W2014869452', 'https://openalex.org/W1998582365', 'https://openalex.org/W2136922672', 'https://openalex.org/W6777926273', 'https://openalex.org/W6682034417', 'https://openalex.org/W2394932179', 'https://openalex.org/W2913340405', 'https://openalex.org/W1734538896', 'https://openalex.org/W6681435938', 'https://openalex.org/W2091432990', 'https://openalex.org/W2160815625', 'https://openalex.org/W2296748324', 'https://openalex.org/W2147768505', 'https://openalex.org/W2162888803', 'https://openalex.org/W6679564466', 'https://openalex.org/W14991580', 'https://openalex.org/W3104240813', 'https://openalex.org/W319941341', 'https://openalex.org/W2119032565', 'https://openalex.org/W2144792281', 'https://openalex.org/W2133013156', 'https://openalex.org/W3028642772', 'https://openalex.org/W2025198378', 'https://openalex.org/W47568227', 'https://openalex.org/W2253807446', 'https://openalex.org/W2168231600', 'https://openalex.org/W2120480077', 'https://openalex.org/W2914746235', 'https://openalex.org/W142063520', 'https://openalex.org/W2146502635', 'https://openalex.org/W2950789693']",2013-05-01
https://openalex.org/W2964309797,https://doi.org/10.1109/icassp.2018.8461972,Multilingual Speech Recognition with a Single End-to-End Model,"Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.","['https://openalex.org/W6649207683', 'https://openalex.org/W2025198378', 'https://openalex.org/W6675791022', 'https://openalex.org/W2633221078', 'https://openalex.org/W6656670161', 'https://openalex.org/W1975550806', 'https://openalex.org/W2327501763', 'https://openalex.org/W6743440867', 'https://openalex.org/W1489125746', 'https://openalex.org/W2033436836', 'https://openalex.org/W2577366047', 'https://openalex.org/W73572011', 'https://openalex.org/W6679469922', 'https://openalex.org/W1968147312', 'https://openalex.org/W6679081875', 'https://openalex.org/W6678250534', 'https://openalex.org/W1605308203', 'https://openalex.org/W6644703941', 'https://openalex.org/W2091746061', 'https://openalex.org/W2064675550', 'https://openalex.org/W2913340405', 'https://openalex.org/W2131774270', 'https://openalex.org/W6676315081', 'https://openalex.org/W2138992227', 'https://openalex.org/W2168231600', 'https://openalex.org/W2750499125', 'https://openalex.org/W2127982613', 'https://openalex.org/W2130414229', 'https://openalex.org/W1978660892', 'https://openalex.org/W1994606281', 'https://openalex.org/W2025401819', 'https://openalex.org/W2123798005', 'https://openalex.org/W2106440210', 'https://openalex.org/W2108677974']",2018-04-01
https://openalex.org/W2987283559,https://doi.org/10.48550/arxiv.1911.05722,Momentum Contrast for Unsupervised Visual Representation Learning,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.","['https://openalex.org/W2131846894', 'https://openalex.org/W2998388430', 'https://openalex.org/W2987741655', 'https://openalex.org/W2842511635', 'https://openalex.org/W2933502442', 'https://openalex.org/W1903029394', 'https://openalex.org/W2622263826', 'https://openalex.org/W2549139847', 'https://openalex.org/W2883725317', 'https://openalex.org/W2962742544', 'https://openalex.org/W2147800946', 'https://openalex.org/W2944828972', 'https://openalex.org/W1976921161', 'https://openalex.org/W2326925005', 'https://openalex.org/W1861492603', 'https://openalex.org/W1836465849', 'https://openalex.org/W2970241862', 'https://openalex.org/W2963341956', 'https://openalex.org/W2194775991', 'https://openalex.org/W2031489346', 'https://openalex.org/W2887997457', 'https://openalex.org/W2798991696', 'https://openalex.org/W343636949', 'https://openalex.org/W2799269579', 'https://openalex.org/W2025768430', 'https://openalex.org/W2952865063', 'https://openalex.org/W2963016543', 'https://openalex.org/W2948672349', 'https://openalex.org/W2613718673', 'https://openalex.org/W2184852195', 'https://openalex.org/W2575671312', 'https://openalex.org/W2152790380', 'https://openalex.org/W2991391304', 'https://openalex.org/W3005680577', 'https://openalex.org/W2302255633', 'https://openalex.org/W3009561768', 'https://openalex.org/W2108598243', 'https://openalex.org/W2963265008', 'https://openalex.org/W2102605133', 'https://openalex.org/W2990873191', 'https://openalex.org/W2953139137', 'https://openalex.org/W2558661413', 'https://openalex.org/W2565639579', 'https://openalex.org/W219040644', 'https://openalex.org/W2148349024', 'https://openalex.org/W2962835968', 'https://openalex.org/W1536680647', 'https://openalex.org/W2099471712', 'https://openalex.org/W2785694322', 'https://openalex.org/W2913939497', 'https://openalex.org/W2144794286', 'https://openalex.org/W2963684275', 'https://openalex.org/W2949517790', 'https://openalex.org/W2948012107', 'https://openalex.org/W2138621090', 'https://openalex.org/W2250384498', 'https://openalex.org/W2342877626', 'https://openalex.org/W2321533354']",2019-11-13
https://openalex.org/W2547875792,https://doi.org/10.48550/arxiv.1611.01144,Categorical Reparameterization with Gumbel-Softmax,"Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",[],2016-11-03
https://openalex.org/W2894835365,https://doi.org/10.1109/slt.2018.8639655,"Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling","Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.","['https://openalex.org/W2038810952', 'https://openalex.org/W6739324457', 'https://openalex.org/W2096140469', 'https://openalex.org/W2586412610', 'https://openalex.org/W6746041127', 'https://openalex.org/W2094147890', 'https://openalex.org/W2743818757', 'https://openalex.org/W2963292011', 'https://openalex.org/W2786835190', 'https://openalex.org/W2964309797', 'https://openalex.org/W2766219058', 'https://openalex.org/W2964089206', 'https://openalex.org/W6720877245', 'https://openalex.org/W6623517193', 'https://openalex.org/W2064675550', 'https://openalex.org/W2157331557', 'https://openalex.org/W950853366', 'https://openalex.org/W2962780374', 'https://openalex.org/W6675365184', 'https://openalex.org/W2697044473', 'https://openalex.org/W2327501763', 'https://openalex.org/W6679434410', 'https://openalex.org/W2091746061', 'https://openalex.org/W6679436768', 'https://openalex.org/W6747158283', 'https://openalex.org/W6739366949', 'https://openalex.org/W2131774270', 'https://openalex.org/W6631362777', 'https://openalex.org/W854541894', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964272710', 'https://openalex.org/W1686810756', 'https://openalex.org/W2130942839', 'https://openalex.org/W2474824677', 'https://openalex.org/W2133564696', 'https://openalex.org/W2102113734', 'https://openalex.org/W2633221078', 'https://openalex.org/W2769437540', 'https://openalex.org/W2627092829', 'https://openalex.org/W4294619417', 'https://openalex.org/W2963403664', 'https://openalex.org/W1524333225']",2018-12-01
https://openalex.org/W2983040767,https://doi.org/10.18653/v1/2020.acl-main.747,Unsupervised Cross-lingual Representation Learning at Scale,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.","['https://openalex.org/W2963626623', 'https://openalex.org/W2981852735', 'https://openalex.org/W2250539671', 'https://openalex.org/W2948384082', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963748441', 'https://openalex.org/W2980404057', 'https://openalex.org/W2550821151', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963667932', 'https://openalex.org/W2971863715', 'https://openalex.org/W2915429162', 'https://openalex.org/W2905933322', 'https://openalex.org/W2963979492', 'https://openalex.org/W2970854433', 'https://openalex.org/W2962369866', 'https://openalex.org/W2915128308', 'https://openalex.org/W2296283641', 'https://openalex.org/W2989539713', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963631907', 'https://openalex.org/W2251939518', 'https://openalex.org/W3035547806', 'https://openalex.org/W2126725946', 'https://openalex.org/W2971411841', 'https://openalex.org/W2963403868', 'https://openalex.org/W2914120296', 'https://openalex.org/W2880875857', 'https://openalex.org/W2259472270', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963323070', 'https://openalex.org/W2948017315', 'https://openalex.org/W1840435438', 'https://openalex.org/W2153579005', 'https://openalex.org/W2965373594', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956']",2020-01-01
https://openalex.org/W1846073453,https://doi.org/10.1109/icassp.2000.859138,Towards language independent acoustic modeling,"We describe procedures and experimental results using speech from diverse source languages to build an ASR system for a single target language. This work is intended to improve ASR in languages for which large amounts of training data are not available. We have developed both knowledge-based and automatic methods to map phonetic units from the source languages to the target language. We employed HMM adaptation techniques and discriminative model combination to combine acoustic models from the individual source languages for recognition of speech in the target language. Experiments are described in which Czech Broadcast News is transcribed using acoustic models trained from small amounts of Czech read speech augmented by English, Spanish, Russian, and Mandarin acoustic models.","['https://openalex.org/W1526294009', 'https://openalex.org/W4244017338', 'https://openalex.org/W2171074980', 'https://openalex.org/W2170101319', 'https://openalex.org/W2105354660', 'https://openalex.org/W2132534451', 'https://openalex.org/W6628027437', 'https://openalex.org/W2160041458', 'https://openalex.org/W47948440', 'https://openalex.org/W73572011', 'https://openalex.org/W2105852393', 'https://openalex.org/W1846073453', 'https://openalex.org/W88081813', 'https://openalex.org/W1522821972', 'https://openalex.org/W2913399920', 'https://openalex.org/W1236994949']",2002-11-07
https://openalex.org/W2123798005,https://doi.org/10.1109/icassp.2010.5495646,Multilingual acoustic modeling for speech recognition based on subspace Gaussian Mixture Models,"Although research has previously been done on multilingual speech recognition, it has been found to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of âuniversal phone setâ that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a âSubspace Gaussian Mixture Modelâ where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the total parameter space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data.","['https://openalex.org/W2090861223', 'https://openalex.org/W2166637769', 'https://openalex.org/W6631419337', 'https://openalex.org/W6679469920', 'https://openalex.org/W6714284179', 'https://openalex.org/W2130414229', 'https://openalex.org/W6680151373', 'https://openalex.org/W2151402979', 'https://openalex.org/W6890491316', 'https://openalex.org/W2047497400', 'https://openalex.org/W2130086010', 'https://openalex.org/W2407897255', 'https://openalex.org/W2729906263', 'https://openalex.org/W2134260474', 'https://openalex.org/W1526995323', 'https://openalex.org/W179430498']",2010-03-01
https://openalex.org/W2963027641,https://doi.org/10.1109/icassp.2019.8682918,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,"This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.","['https://openalex.org/W6756040250', 'https://openalex.org/W6637373629', 'https://openalex.org/W6631362777', 'https://openalex.org/W6779469704', 'https://openalex.org/W6629717138', 'https://openalex.org/W2962780374', 'https://openalex.org/W6621543089', 'https://openalex.org/W2064675550', 'https://openalex.org/W2799316004', 'https://openalex.org/W1970890968', 'https://openalex.org/W2564058731', 'https://openalex.org/W2106440210', 'https://openalex.org/W2963336460', 'https://openalex.org/W2786835190', 'https://openalex.org/W2964309797', 'https://openalex.org/W2962704885', 'https://openalex.org/W6746574493', 'https://openalex.org/W2963362078', 'https://openalex.org/W6728030952', 'https://openalex.org/W2766219058', 'https://openalex.org/W2127141656', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963292011', 'https://openalex.org/W6696449567', 'https://openalex.org/W2799800213', 'https://openalex.org/W6746041127', 'https://openalex.org/W6755235688', 'https://openalex.org/W6623517193', 'https://openalex.org/W2891816510', 'https://openalex.org/W6679434410', 'https://openalex.org/W6640059789', 'https://openalex.org/W6729503108', 'https://openalex.org/W2888779557', 'https://openalex.org/W6753186555', 'https://openalex.org/W2758310181', 'https://openalex.org/W2327501763', 'https://openalex.org/W6758673196', 'https://openalex.org/W2555428947', 'https://openalex.org/W3034729383', 'https://openalex.org/W2912937645', 'https://openalex.org/W1686810756', 'https://openalex.org/W2963240019', 'https://openalex.org/W1494198834', 'https://openalex.org/W2292087804', 'https://openalex.org/W1915251500', 'https://openalex.org/W6908809', 'https://openalex.org/W2883586237', 'https://openalex.org/W2526425061', 'https://openalex.org/W2133564696', 'https://openalex.org/W648786980', 'https://openalex.org/W2964308564', 'https://openalex.org/W2894835365', 'https://openalex.org/W2899771611', 'https://openalex.org/W854541894', 'https://openalex.org/W2769437540', 'https://openalex.org/W1524333225']",2019-04-16
https://openalex.org/W2730658205,https://doi.org/10.1016/j.csl.2017.06.008,Multilingually trained bottleneck features in spoken language recognition,,"['https://openalex.org/W6683168674', 'https://openalex.org/W2913340405', 'https://openalex.org/W6606257714', 'https://openalex.org/W2150769028', 'https://openalex.org/W6675772068', 'https://openalex.org/W2406215990', 'https://openalex.org/W2175575774', 'https://openalex.org/W6713813839', 'https://openalex.org/W2034304679', 'https://openalex.org/W162588823', 'https://openalex.org/W2012897754', 'https://openalex.org/W2137075158', 'https://openalex.org/W2400962240', 'https://openalex.org/W2098196171', 'https://openalex.org/W6666276816', 'https://openalex.org/W2398569491', 'https://openalex.org/W2397450537', 'https://openalex.org/W2122538988', 'https://openalex.org/W6606260724', 'https://openalex.org/W2371039490', 'https://openalex.org/W6664241336', 'https://openalex.org/W2340176088', 'https://openalex.org/W2185814970', 'https://openalex.org/W6714073180', 'https://openalex.org/W6639916541', 'https://openalex.org/W6630162575', 'https://openalex.org/W6602682705', 'https://openalex.org/W2033436836', 'https://openalex.org/W2020057315', 'https://openalex.org/W6677604902', 'https://openalex.org/W6681893342', 'https://openalex.org/W2076954782', 'https://openalex.org/W2072349636', 'https://openalex.org/W1970890968', 'https://openalex.org/W6680764333', 'https://openalex.org/W2259317772', 'https://openalex.org/W6674998567', 'https://openalex.org/W2604292070', 'https://openalex.org/W2136711599', 'https://openalex.org/W2597684388', 'https://openalex.org/W2408548175', 'https://openalex.org/W2098859361', 'https://openalex.org/W2154820175', 'https://openalex.org/W2915722758', 'https://openalex.org/W2146194791', 'https://openalex.org/W154262204', 'https://openalex.org/W1581253957', 'https://openalex.org/W66627554', 'https://openalex.org/W1501286448', 'https://openalex.org/W2406392101', 'https://openalex.org/W2105948329', 'https://openalex.org/W2916018751', 'https://openalex.org/W2119603452', 'https://openalex.org/W2805889152', 'https://openalex.org/W2063913958', 'https://openalex.org/W2345662995', 'https://openalex.org/W2056119007', 'https://openalex.org/W153983408', 'https://openalex.org/W2601106850', 'https://openalex.org/W1909308924']",2017-07-05
https://openalex.org/W2941814890,https://doi.org/10.48550/arxiv.1904.11660,Transformers with convolutional context for ASR,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","['https://openalex.org/W2802023636', 'https://openalex.org/W2102113734', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964089206', 'https://openalex.org/W2799800213', 'https://openalex.org/W2782451907', 'https://openalex.org/W2520160253', 'https://openalex.org/W6908809', 'https://openalex.org/W2963742216', 'https://openalex.org/W2127141656', 'https://openalex.org/W2005708641', 'https://openalex.org/W2184045248', 'https://openalex.org/W2962778134', 'https://openalex.org/W2327501763', 'https://openalex.org/W2911291251', 'https://openalex.org/W179875071', 'https://openalex.org/W2963250244', 'https://openalex.org/W1922655562', 'https://openalex.org/W854541894', 'https://openalex.org/W2963403868', 'https://openalex.org/W2904818793', 'https://openalex.org/W2157331557', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374']",2019-04-26
https://openalex.org/W2933138175,https://doi.org/10.18653/v1/n19-4009,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling","Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019.","['https://openalex.org/W2633911929', 'https://openalex.org/W2964190861', 'https://openalex.org/W2519314406', 'https://openalex.org/W4293718192', 'https://openalex.org/W2973088264', 'https://openalex.org/W2612675303', 'https://openalex.org/W4385245566', 'https://openalex.org/W2888482885', 'https://openalex.org/W2963112338', 'https://openalex.org/W2792376130', 'https://openalex.org/W2889518897', 'https://openalex.org/W2963366552', 'https://openalex.org/W2964258094', 'https://openalex.org/W2613904329', 'https://openalex.org/W2963500743', 'https://openalex.org/W4297747548', 'https://openalex.org/W2889326796', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962805889', 'https://openalex.org/W2963631907', 'https://openalex.org/W2963475460', 'https://openalex.org/W2898875342', 'https://openalex.org/W2963418779', 'https://openalex.org/W2952339051', 'https://openalex.org/W2963034893', 'https://openalex.org/W2963773505', 'https://openalex.org/W2963096510', 'https://openalex.org/W1544827683', 'https://openalex.org/W2963970792', 'https://openalex.org/W2797328513', 'https://openalex.org/W2914855263', 'https://openalex.org/W2890914727', 'https://openalex.org/W2962974452', 'https://openalex.org/W2606974598', 'https://openalex.org/W2950513705', 'https://openalex.org/W2963212250', 'https://openalex.org/W3082674894', 'https://openalex.org/W2567070169', 'https://openalex.org/W4293569541', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963263347', 'https://openalex.org/W2963532001', 'https://openalex.org/W2804704270', 'https://openalex.org/W2964265128', 'https://openalex.org/W2154652894', 'https://openalex.org/W2767989436', 'https://openalex.org/W2259472270', 'https://openalex.org/W2785047343', 'https://openalex.org/W2962784628', 'https://openalex.org/W2763421725', 'https://openalex.org/W2798931235', 'https://openalex.org/W2807964741', 'https://openalex.org/W2949555952', 'https://openalex.org/W2963929190', 'https://openalex.org/W2963807318', 'https://openalex.org/W4297788867', 'https://openalex.org/W2795285343', 'https://openalex.org/W2963925437', 'https://openalex.org/W2951672049', 'https://openalex.org/W2951559648', 'https://openalex.org/W2964343359', 'https://openalex.org/W1938755728', 'https://openalex.org/W1902237438', 'https://openalex.org/W2778814079', 'https://openalex.org/W2949615363', 'https://openalex.org/W4297801368']",2019-01-01
https://openalex.org/W2001619934,https://doi.org/10.1111/j.1469-1809.1936.tb02137.x,THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS,"The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.",['https://openalex.org/W2027987815'],1936-09-01
https://openalex.org/W2124629003,https://doi.org/10.1109/icassp.1998.675351,Maximum likelihood modeling with Gaussian distributions for classification,"Maximum likelihood (ML) modeling of multiclass data for classification often suffers from the following problems: (a) data insufficiency implying overtrained or unreliable models, (b) large storage requirement, (c) large computational requirement and/or (d) the ML is not discriminating between classes. Sharing parameters across classes (or constraining the parameters) clearly tends to alleviate the first three problems. We show that in some cases it can also lead to better discrimination (as evidenced by reduced misclassification error). The parameters considered are the means and variances of the Gaussians and linear transformations of the feature space (or equivalently the Gaussian means). Some constraints on the parameters are shown to lead to linear discrimination analysis (a well-known result) while others are shown to lead to optimal feature spaces (a relatively new result). Applications of some of these ideas to the speech recognition problem are also given.","['https://openalex.org/W2146871184', 'https://openalex.org/W2070545637', 'https://openalex.org/W7048738093', 'https://openalex.org/W2170285963', 'https://openalex.org/W2145336848', 'https://openalex.org/W2002342963', 'https://openalex.org/W2049633694', 'https://openalex.org/W1538424569', 'https://openalex.org/W3129711340', 'https://openalex.org/W2106554350']",2002-11-27
https://openalex.org/W2106554350,https://doi.org/10.1109/89.759034,Semi-tied covariance matrices for hidden Markov models,"There is normally a simple choice made in the form of the covariance matrix to be used with continuous-density HMMs. Either a diagonal covariance matrix is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modeled. Unfortunately when using full or block-diagonal covariance matrices there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of covariance matrix which allows a few ""full"" covariance matrices to be shared over many distributions, whilst each distribution maintains its own ""diagonal"" covariance matrix. In contrast to other schemes which have hypothesized a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of covariance matrix is evaluated on a large-vocabulary speech-recognition task. In initial experiments the performance of the standard system was achieved using approximately half the number of parameters. Moreover, a 10% reduction in word error rate compared to a standard system can be achieved with less than a 1% increase in the number of parameters and little increase in recognition time.","['https://openalex.org/W2037740282', 'https://openalex.org/W2014089984', 'https://openalex.org/W2146871184', 'https://openalex.org/W1980800561', 'https://openalex.org/W2070545637', 'https://openalex.org/W2081323593', 'https://openalex.org/W2125838338', 'https://openalex.org/W7048738093', 'https://openalex.org/W2148154194', 'https://openalex.org/W6633351333', 'https://openalex.org/W2019368538', 'https://openalex.org/W142684478', 'https://openalex.org/W2158069733', 'https://openalex.org/W2002342963', 'https://openalex.org/W2049633694', 'https://openalex.org/W2135346934', 'https://openalex.org/W4298379700', 'https://openalex.org/W31925794', 'https://openalex.org/W3129711340', 'https://openalex.org/W2411442511', 'https://openalex.org/W2772658608', 'https://openalex.org/W1557500035']",1999-05-01
https://openalex.org/W1599512239,https://doi.org/10.1109/icslp.1996.607807,A compact model for speaker-adaptive training,"We formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 5K vocabulary tasks respectively.","['https://openalex.org/W2124052683', 'https://openalex.org/W2121681768', 'https://openalex.org/W2086699924', 'https://openalex.org/W2398991154', 'https://openalex.org/W2167152389', 'https://openalex.org/W2099914327', 'https://openalex.org/W2140567543', 'https://openalex.org/W7048738093', 'https://openalex.org/W2149007717', 'https://openalex.org/W4285719527', 'https://openalex.org/W2049633694', 'https://openalex.org/W1601032495']",2002-12-24
https://openalex.org/W2002342963,https://doi.org/10.1006/csla.1998.0043,Maximum likelihood linear transformations for HMM-based speech recognition,,[],1998-04-01
https://openalex.org/W2019042707,https://doi.org/10.1016/j.csl.2011.04.002,A basis representation of constrained MLLR transforms for robust adaptation,,"['https://openalex.org/W2142635246', 'https://openalex.org/W6629029613', 'https://openalex.org/W2121981798', 'https://openalex.org/W2002342963', 'https://openalex.org/W2037740282', 'https://openalex.org/W6680151373', 'https://openalex.org/W6640365096', 'https://openalex.org/W6680130450', 'https://openalex.org/W2158289097', 'https://openalex.org/W6635814114', 'https://openalex.org/W6604513159', 'https://openalex.org/W2168175751', 'https://openalex.org/W2098841537', 'https://openalex.org/W6681160539', 'https://openalex.org/W73056077', 'https://openalex.org/W6644042397', 'https://openalex.org/W59017505', 'https://openalex.org/W2134260474', 'https://openalex.org/W2135762233', 'https://openalex.org/W112154549', 'https://openalex.org/W2142980177', 'https://openalex.org/W3149745985', 'https://openalex.org/W1932968309', 'https://openalex.org/W1974218343', 'https://openalex.org/W1602232401', 'https://openalex.org/W1485038873']",2011-05-20
https://openalex.org/W2078769636,https://doi.org/10.1016/j.csl.2013.05.002,Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery,,"['https://openalex.org/W6608294043', 'https://openalex.org/W6683874881', 'https://openalex.org/W2087863531', 'https://openalex.org/W6674847979', 'https://openalex.org/W6684527036', 'https://openalex.org/W6681627878', 'https://openalex.org/W6652900202', 'https://openalex.org/W6679182551', 'https://openalex.org/W6601311673', 'https://openalex.org/W1975113979', 'https://openalex.org/W6633159005', 'https://openalex.org/W2106284094', 'https://openalex.org/W6603381559', 'https://openalex.org/W2099424804', 'https://openalex.org/W149529473', 'https://openalex.org/W6677452305', 'https://openalex.org/W1507177964', 'https://openalex.org/W6610841746', 'https://openalex.org/W6713082324', 'https://openalex.org/W6631419337', 'https://openalex.org/W6670629611', 'https://openalex.org/W6684865868', 'https://openalex.org/W2143203634', 'https://openalex.org/W30845872', 'https://openalex.org/W308497914', 'https://openalex.org/W2399869768', 'https://openalex.org/W203607283', 'https://openalex.org/W2105778889', 'https://openalex.org/W82886505', 'https://openalex.org/W2167845555', 'https://openalex.org/W2009388533', 'https://openalex.org/W2153635508', 'https://openalex.org/W2079460648', 'https://openalex.org/W2164922523', 'https://openalex.org/W2118841860', 'https://openalex.org/W1552962923', 'https://openalex.org/W2099415988', 'https://openalex.org/W3120421331', 'https://openalex.org/W4285719527', 'https://openalex.org/W51277926', 'https://openalex.org/W202343879', 'https://openalex.org/W2130180273', 'https://openalex.org/W2162042658', 'https://openalex.org/W2168347480', 'https://openalex.org/W1526995323']",2013-05-20
https://openalex.org/W2509930204,https://doi.org/10.21437/interspeech.2016-988,Supervised Learning of Acoustic Models in a Zero Resource Setting to Improve DPGMM Clustering,,"['https://openalex.org/W2345811097', 'https://openalex.org/W2064210461', 'https://openalex.org/W2117041980', 'https://openalex.org/W2963620343', 'https://openalex.org/W2119187236', 'https://openalex.org/W1524333225', 'https://openalex.org/W2118841860', 'https://openalex.org/W66167291', 'https://openalex.org/W2395899413', 'https://openalex.org/W2126203737', 'https://openalex.org/W2001619934', 'https://openalex.org/W2002342963', 'https://openalex.org/W2399576818', 'https://openalex.org/W2160719354', 'https://openalex.org/W2402014506', 'https://openalex.org/W2128032727']",2016-08-29
https://openalex.org/W2586754519,https://doi.org/10.1109/slt.2016.7846245,Iterative training of a DPGMM-HMM acoustic unit recognizer in a zero resource scenario,"In this paper we propose a framework for building a full-fledged acoustic unit recognizer in a zero resource setting, i.e., without any provided labels. For that, we combine an iterative Dirichlet process Gaussian mixture model (DPGMM) clustering framework with a standard pipeline for supervised GMM-HMM acoustic model (AM) and n-gram language model (LM) training, enhanced by a scheme for iterative model re-training. We use the DPGMM to cluster feature vectors into a dynamically sized set of acoustic units. The frame based class labels serve as transcriptions of the audio data and are used as input to the AM and LM training pipeline. We show that iterative unsupervised model re-training of this DPGMM-HMM acoustic unit recognizer improves performance according to an ABX sound class discriminability task based evaluation. Our results show that the learned models generalize well and that sound class discriminability benefits from contextual information introduced by the language model. Our systems are competitive with supervisedly trained phone recognizers, and can beat the baseline set by DPGMM clustering.","['https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W6677734967', 'https://openalex.org/W6602705600', 'https://openalex.org/W6712553779', 'https://openalex.org/W6704305767', 'https://openalex.org/W2509930204', 'https://openalex.org/W2963684067', 'https://openalex.org/W2137826140', 'https://openalex.org/W2402014506', 'https://openalex.org/W2056786202', 'https://openalex.org/W1975113979', 'https://openalex.org/W2401464865', 'https://openalex.org/W2078769636', 'https://openalex.org/W2064210461', 'https://openalex.org/W6675022971', 'https://openalex.org/W6973666849', 'https://openalex.org/W6712444837', 'https://openalex.org/W4234482113', 'https://openalex.org/W6678947187', 'https://openalex.org/W6636811518', 'https://openalex.org/W6631362777', 'https://openalex.org/W2146950091', 'https://openalex.org/W2113641473', 'https://openalex.org/W2128032727', 'https://openalex.org/W2345811097', 'https://openalex.org/W2395899413', 'https://openalex.org/W2399576818', 'https://openalex.org/W2963620343', 'https://openalex.org/W66167291', 'https://openalex.org/W1631260214', 'https://openalex.org/W2786608204', 'https://openalex.org/W1524333225', 'https://openalex.org/W2119187236', 'https://openalex.org/W3148201686', 'https://openalex.org/W2100768664']",2016-12-01
https://openalex.org/W2113641473,https://doi.org/10.1109/18.87000,The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression,"Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2095306947', 'https://openalex.org/W4247267886', 'https://openalex.org/W1975965284', 'https://openalex.org/W2161628678', 'https://openalex.org/W2152851525', 'https://openalex.org/W2046419776', 'https://openalex.org/W2911940095', 'https://openalex.org/W1974717499', 'https://openalex.org/W2107745473', 'https://openalex.org/W2129652681', 'https://openalex.org/W2119047110', 'https://openalex.org/W2122962290', 'https://openalex.org/W2146368895', 'https://openalex.org/W2171763016', 'https://openalex.org/W2007108779', 'https://openalex.org/W4285719527', 'https://openalex.org/W2126163471', 'https://openalex.org/W2045740840', 'https://openalex.org/W45615632', 'https://openalex.org/W2611071497', 'https://openalex.org/W17078302', 'https://openalex.org/W1519253855']",1991-07-01
https://openalex.org/W1631260214,https://doi.org/10.21437/icslp.2002-303,SRILM - an extensible language modeling toolkit,,"['https://openalex.org/W2121227244', 'https://openalex.org/W1582730572', 'https://openalex.org/W1528470941', 'https://openalex.org/W1551413707', 'https://openalex.org/W1549285799', 'https://openalex.org/W2594610113', 'https://openalex.org/W2075401516', 'https://openalex.org/W2100506586', 'https://openalex.org/W2084084380', 'https://openalex.org/W2162132066', 'https://openalex.org/W2139884663', 'https://openalex.org/W2124246394', 'https://openalex.org/W2127836646', 'https://openalex.org/W32217939', 'https://openalex.org/W1797288984', 'https://openalex.org/W2158195707', 'https://openalex.org/W1536904578', 'https://openalex.org/W2097978681', 'https://openalex.org/W1904457459']",2002-09-16
https://openalex.org/W2395899413,https://doi.org/10.21437/interspeech.2013-441,Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline,"We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.","['https://openalex.org/W2407151108', 'https://openalex.org/W2232131953', 'https://openalex.org/W2980286501', 'https://openalex.org/W2137075158', 'https://openalex.org/W2090861223', 'https://openalex.org/W4285719527', 'https://openalex.org/W2096765209', 'https://openalex.org/W2986535481', 'https://openalex.org/W4247807440', 'https://openalex.org/W2054139811', 'https://openalex.org/W168991039', 'https://openalex.org/W2406820985', 'https://openalex.org/W2089177488', 'https://openalex.org/W2025482506', 'https://openalex.org/W2160719354', 'https://openalex.org/W156237177', 'https://openalex.org/W48303286', 'https://openalex.org/W2400113920', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W282666689', 'https://openalex.org/W1480485976']",2013-08-25
https://openalex.org/W2396043527,https://doi.org/10.21437/interspeech.2015-639,Discovering discrete subword units with binarized autoencoders and hidden-Markov-model encoders,,"['https://openalex.org/W1545920196', 'https://openalex.org/W2099415988', 'https://openalex.org/W2055408826', 'https://openalex.org/W2913932916', 'https://openalex.org/W2049633694', 'https://openalex.org/W2117041980', 'https://openalex.org/W2231075402', 'https://openalex.org/W2170353620', 'https://openalex.org/W1964917299', 'https://openalex.org/W2100768664', 'https://openalex.org/W2020607164', 'https://openalex.org/W2401464865', 'https://openalex.org/W2100495367']",2015-09-06
https://openalex.org/W2404799143,https://doi.org/10.21437/interspeech.2015-640,A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling,"We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.","['https://openalex.org/W2619993508', 'https://openalex.org/W2608958052', 'https://openalex.org/W2138621090', 'https://openalex.org/W1997460147', 'https://openalex.org/W3127686677', 'https://openalex.org/W1545920196', 'https://openalex.org/W1985371235', 'https://openalex.org/W2251545788', 'https://openalex.org/W2079623482', 'https://openalex.org/W2164770604', 'https://openalex.org/W1993755070', 'https://openalex.org/W2171590421', 'https://openalex.org/W1972159947', 'https://openalex.org/W148837159', 'https://openalex.org/W2048741136', 'https://openalex.org/W2052697931', 'https://openalex.org/W2127589108', 'https://openalex.org/W2054948443', 'https://openalex.org/W1606347560', 'https://openalex.org/W6908809', 'https://openalex.org/W904206136', 'https://openalex.org/W2114347655', 'https://openalex.org/W2057007397', 'https://openalex.org/W21006490']",2015-09-06
https://openalex.org/W2128032727,,Parallel Sampling of DP Mixture Models using Sub-Cluster Splits,"We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.","['https://openalex.org/W2112796928', 'https://openalex.org/W2138309709', 'https://openalex.org/W2053405531', 'https://openalex.org/W2127498532', 'https://openalex.org/W3120740533', 'https://openalex.org/W2158266063', 'https://openalex.org/W2155762504', 'https://openalex.org/W1971807270', 'https://openalex.org/W3145738572', 'https://openalex.org/W2467379829', 'https://openalex.org/W2062882942', 'https://openalex.org/W2100231460', 'https://openalex.org/W2062373184', 'https://openalex.org/W2163229341', 'https://openalex.org/W2170886255', 'https://openalex.org/W2118036030', 'https://openalex.org/W2125858773', 'https://openalex.org/W29489373', 'https://openalex.org/W2072169887', 'https://openalex.org/W2105767795', 'https://openalex.org/W2069429561', 'https://openalex.org/W1833498382', 'https://openalex.org/W2080972498', 'https://openalex.org/W2162021827', 'https://openalex.org/W2101998432', 'https://openalex.org/W1551893515', 'https://openalex.org/W2110717342', 'https://openalex.org/W1500803570', 'https://openalex.org/W2132827946', 'https://openalex.org/W2091797506', 'https://openalex.org/W2082630584']",2013-12-05
https://openalex.org/W1796128977,https://doi.org/10.21437/interspeech.2015-644,A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge,"The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.","['https://openalex.org/W2044138293', 'https://openalex.org/W2107878631', 'https://openalex.org/W2112688413', 'https://openalex.org/W1606347560', 'https://openalex.org/W2002318609', 'https://openalex.org/W2786608204', 'https://openalex.org/W2057007397', 'https://openalex.org/W2026369565', 'https://openalex.org/W66167291', 'https://openalex.org/W2146502635', 'https://openalex.org/W2100495367', 'https://openalex.org/W2395899413', 'https://openalex.org/W2017257315', 'https://openalex.org/W2145094598', 'https://openalex.org/W2148154194', 'https://openalex.org/W2407151108', 'https://openalex.org/W2052697931', 'https://openalex.org/W3028642772', 'https://openalex.org/W2145410271', 'https://openalex.org/W2020607164', 'https://openalex.org/W4285719527', 'https://openalex.org/W2025768430', 'https://openalex.org/W1524333225', 'https://openalex.org/W1545920196', 'https://openalex.org/W2406349064']",2015-09-06
https://openalex.org/W1524333225,,The Kaldi Speech Recognition Toolkit,"Abstract—We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users. I.","['https://openalex.org/W38163879', 'https://openalex.org/W1582482241', 'https://openalex.org/W1981706894', 'https://openalex.org/W2045644151', 'https://openalex.org/W2122028591', 'https://openalex.org/W2002342963', 'https://openalex.org/W2046932483', 'https://openalex.org/W31925794', 'https://openalex.org/W2106554350', 'https://openalex.org/W2148428486', 'https://openalex.org/W2124629003', 'https://openalex.org/W2157421955', 'https://openalex.org/W1491238342', 'https://openalex.org/W2146871184', 'https://openalex.org/W173010698']",2011-01-01
https://openalex.org/W2399576818,https://doi.org/10.21437/interspeech.2015-642,Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study,"We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.","['https://openalex.org/W2406349064', 'https://openalex.org/W2119187236', 'https://openalex.org/W2106284094', 'https://openalex.org/W2114347655', 'https://openalex.org/W2049142189', 'https://openalex.org/W2125534887', 'https://openalex.org/W2127498532', 'https://openalex.org/W2151967501', 'https://openalex.org/W66167291', 'https://openalex.org/W2125247927', 'https://openalex.org/W2395899413', 'https://openalex.org/W2079460648', 'https://openalex.org/W1967924372', 'https://openalex.org/W2020607164', 'https://openalex.org/W2213520355', 'https://openalex.org/W2399363370', 'https://openalex.org/W2126203737', 'https://openalex.org/W2406820985', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W1997505733', 'https://openalex.org/W2170659185', 'https://openalex.org/W2078769636', 'https://openalex.org/W2052697931', 'https://openalex.org/W2171019095', 'https://openalex.org/W2065136108', 'https://openalex.org/W1503398984', 'https://openalex.org/W2168319451', 'https://openalex.org/W2044138293', 'https://openalex.org/W2080972498', 'https://openalex.org/W1975728937', 'https://openalex.org/W2162021827', 'https://openalex.org/W1833498382', 'https://openalex.org/W1545920196', 'https://openalex.org/W2154085905', 'https://openalex.org/W2128032727', 'https://openalex.org/W2110589736']",2015-09-06
https://openalex.org/W2345811097,https://doi.org/10.1016/j.procs.2016.04.032,Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario,"In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.","['https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W1502957213', 'https://openalex.org/W1997505733', 'https://openalex.org/W2399576818', 'https://openalex.org/W2001619934', 'https://openalex.org/W2064210461', 'https://openalex.org/W2402014506', 'https://openalex.org/W1981276685', 'https://openalex.org/W2346964103', 'https://openalex.org/W2395899413', 'https://openalex.org/W2294798173', 'https://openalex.org/W2071128523', 'https://openalex.org/W66167291', 'https://openalex.org/W2119187236', 'https://openalex.org/W2786608204', 'https://openalex.org/W2128032727']",2016-01-01
https://openalex.org/W2547039119,,Development of Indonesian Large Vocabulary Continuous Speech Recognition System within A-STAR Project.,"The paper outlines the development of a large vocabulary continuous speech recognition (LVCSR) system for the Indonesian language within the Asian speech translation (A-STAR) project. An overview of the A-STAR project and Indonesian language characteristics will be briefly described. We then focus on a discussion of the development of Indonesian LVCSR, including data resources issues, acoustic modeling, language modeling, the lexicon, and accuracy of recognition. There are three types of Indonesian data resources: daily news, telephone application, and BTEC tasks, which are used in this project. They are available in both text and speech forms. The Indonesian speech recognition engine was trained using the clean speech of both daily news and telephone application tasks. The optimum performance achieved on the BTEC task was 92.47 % word accuracy.","['https://openalex.org/W22876683', 'https://openalex.org/W1584533718', 'https://openalex.org/W1502737068', 'https://openalex.org/W206967138']",2008-01-01
https://openalex.org/W2963691546,https://doi.org/10.48550/arxiv.1710.07654,Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence\n Learning,"We present Deep Voice 3, a fully-convolutional attention-based neural\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\nspeech synthesis systems in naturalness while training ten times faster. We\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\nthan eight hundred hours of audio from over two thousand speakers. In addition,\nwe identify common error modes of attention-based speech synthesis networks,\ndemonstrate how to mitigate them, and compare several different waveform\nsynthesis methods. We also describe how to scale inference to ten million\nqueries per day on one single-GPU server.\n",[],2017-10-20
https://openalex.org/W2899518769,https://doi.org/10.48550/arxiv.1811.02775,Improved Audio Embeddings by Adjacency-Based Clustering with Applications in Spoken Term Detection,"Embedding audio signal segments into vectors with fixed dimensionality is attractive because all following processing will be easier and more efficient, for example modeling, classifying or indexing. Audio Word2Vec previously proposed was shown to be able to represent audio segments for spoken words as such vectors carrying information about the phonetic structures of the signal segments. However, each linguistic unit (word, syllable, phoneme in text form) corresponds to unlimited number of audio segments with vector representations inevitably spread over the embedding space, which causes some confusion. It is therefore desired to better cluster the audio embeddings such that those corresponding to the same linguistic unit can be more compactly distributed. In this paper, inspired by Siamese networks, we propose some approaches to achieve the above goal. This includes identifying positive and negative pairs from unlabeled data for Siamese style training, disentangling acoustic factors such as speaker characteristics from the audio embedding, handling unbalanced data distribution, and having the embedding processes learn from the adjacency relationships among data points. All these can be done in an unsupervised way. Improved performance was obtained in preliminary experiments on the LibriSpeech data set, including clustering characteristics analysis and applications of spoken term detection.","['https://openalex.org/W2190506272', 'https://openalex.org/W2807071886', 'https://openalex.org/W1494198834', 'https://openalex.org/W2295088914', 'https://openalex.org/W2951216052', 'https://openalex.org/W2951828005', 'https://openalex.org/W2884305338', 'https://openalex.org/W2949844463', 'https://openalex.org/W2030422732', 'https://openalex.org/W2550241133', 'https://openalex.org/W2950635152', 'https://openalex.org/W2751947065', 'https://openalex.org/W2401725913', 'https://openalex.org/W2025482506', 'https://openalex.org/W2794739275', 'https://openalex.org/W2888845873', 'https://openalex.org/W1577418252', 'https://openalex.org/W2963571336', 'https://openalex.org/W1610356397']",2018-11-07
https://openalex.org/W2963609956,https://doi.org/10.21437/interspeech.2017-1452,Tacotron: Towards End-to-End Speech Synthesis,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.Building these components often requires extensive domain expertise and may contain brittle design choices.In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.Given <text, audio> pairs, the model can be trained completely from scratch with random initialization.We present several key techniques to make the sequence-tosequence framework perform well for this challenging task.Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.","['https://openalex.org/W1869752048', 'https://openalex.org/W2327501763', 'https://openalex.org/W2130942839', 'https://openalex.org/W2525778437', 'https://openalex.org/W2584032004', 'https://openalex.org/W2133564696', 'https://openalex.org/W4295276571', 'https://openalex.org/W2507771204', 'https://openalex.org/W2519091744', 'https://openalex.org/W1924770834', 'https://openalex.org/W648786980', 'https://openalex.org/W4394643672', 'https://openalex.org/W2591927543', 'https://openalex.org/W2194775991', 'https://openalex.org/W2271840356', 'https://openalex.org/W1563460361', 'https://openalex.org/W2901997113', 'https://openalex.org/W4298261015', 'https://openalex.org/W2099057450', 'https://openalex.org/W1836465849', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W2515943672', 'https://openalex.org/W2129142580', 'https://openalex.org/W2531207078']",2017-08-16
https://openalex.org/W2550241133,https://doi.org/10.48550/arxiv.1611.02550,Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches,"Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a ""Siamese network"" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.","['https://openalex.org/W1499864241', 'https://openalex.org/W1942713348', 'https://openalex.org/W2511733680', 'https://openalex.org/W2096298528', 'https://openalex.org/W2190506272', 'https://openalex.org/W2131033001', 'https://openalex.org/W1942035323', 'https://openalex.org/W2291770225', 'https://openalex.org/W2187089797', 'https://openalex.org/W2059652594', 'https://openalex.org/W753012316', 'https://openalex.org/W1967924372', 'https://openalex.org/W2166637769', 'https://openalex.org/W2114478143', 'https://openalex.org/W1522301498', 'https://openalex.org/W1509905243', 'https://openalex.org/W6908809', 'https://openalex.org/W1545920196', 'https://openalex.org/W2117459613', 'https://openalex.org/W2143612262', 'https://openalex.org/W2407151108', 'https://openalex.org/W1577418252', 'https://openalex.org/W2294995564', 'https://openalex.org/W2405601855']",2016-11-08
https://openalex.org/W1522301498,https://doi.org/10.48550/arxiv.1412.6980,Adam: A Method for Stochastic Optimization,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","['https://openalex.org/W1984541135', 'https://openalex.org/W6908809', 'https://openalex.org/W2100495367', 'https://openalex.org/W1810943226', 'https://openalex.org/W35527955', 'https://openalex.org/W2143612262', 'https://openalex.org/W104184427', 'https://openalex.org/W1904365287', 'https://openalex.org/W2113459411', 'https://openalex.org/W2950351588', 'https://openalex.org/W59018853', 'https://openalex.org/W2146502635', 'https://openalex.org/W2160815625', 'https://openalex.org/W3176583243', 'https://openalex.org/W2156779765', 'https://openalex.org/W2148825261', 'https://openalex.org/W2951004968', 'https://openalex.org/W1598497354', 'https://openalex.org/W1491468723', 'https://openalex.org/W2086161653']",2014-12-22
https://openalex.org/W2792995953,https://doi.org/10.48550/arxiv.1710.07654,Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.","['https://openalex.org/W2049686551', 'https://openalex.org/W2962965405', 'https://openalex.org/W2157331557', 'https://openalex.org/W2120847449', 'https://openalex.org/W2117418893', 'https://openalex.org/W2951064684', 'https://openalex.org/W854541894', 'https://openalex.org/W1563460361', 'https://openalex.org/W2736900972', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964308564', 'https://openalex.org/W2160473997', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963712897', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963970792', 'https://openalex.org/W2901997113', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963685250', 'https://openalex.org/W2591927543', 'https://openalex.org/W2165143604', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964265128', 'https://openalex.org/W2584032004', 'https://openalex.org/W2507771204', 'https://openalex.org/W1570629387', 'https://openalex.org/W1494198834']",2017-10-20
https://openalex.org/W2020607164,https://doi.org/10.1109/icassp.2014.6855085,An auto-encoder based approach to unsupervised learning of subword units,In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.,"['https://openalex.org/W6789826613', 'https://openalex.org/W2100495367', 'https://openalex.org/W7011707065', 'https://openalex.org/W6676071220', 'https://openalex.org/W2035424729', 'https://openalex.org/W2117041980', 'https://openalex.org/W2123237149', 'https://openalex.org/W6677919164', 'https://openalex.org/W6675022971', 'https://openalex.org/W2167655920', 'https://openalex.org/W1964917299', 'https://openalex.org/W6681096077', 'https://openalex.org/W2099415988', 'https://openalex.org/W2401464865', 'https://openalex.org/W2107789863', 'https://openalex.org/W2100768664', 'https://openalex.org/W2619993508', 'https://openalex.org/W3127686677', 'https://openalex.org/W2145094598', 'https://openalex.org/W2118858186', 'https://openalex.org/W2072128103', 'https://openalex.org/W2997574889', 'https://openalex.org/W2010800472']",2014-05-01
https://openalex.org/W2767754137,https://doi.org/10.1109/icassp.2018.8461684,Unsupervised Learning of Semantic Audio Representations,"Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.","['https://openalex.org/W6676071220', 'https://openalex.org/W2561826558', 'https://openalex.org/W6631829595', 'https://openalex.org/W6973666849', 'https://openalex.org/W1967924372', 'https://openalex.org/W6691725507', 'https://openalex.org/W6663645435', 'https://openalex.org/W6632653590', 'https://openalex.org/W2516890051', 'https://openalex.org/W2096733369', 'https://openalex.org/W6738531280', 'https://openalex.org/W219040644', 'https://openalex.org/W2136655611', 'https://openalex.org/W6701655646', 'https://openalex.org/W1975517671', 'https://openalex.org/W6675751002', 'https://openalex.org/W2526050071', 'https://openalex.org/W2963775347', 'https://openalex.org/W2190506272', 'https://openalex.org/W2963420272', 'https://openalex.org/W1520997877', 'https://openalex.org/W343636949', 'https://openalex.org/W6729977899', 'https://openalex.org/W6729831399', 'https://openalex.org/W2844632039', 'https://openalex.org/W2964001192', 'https://openalex.org/W1545920196', 'https://openalex.org/W2618269622', 'https://openalex.org/W2846592055', 'https://openalex.org/W3104866538', 'https://openalex.org/W2326925005', 'https://openalex.org/W2107789863', 'https://openalex.org/W2342547278', 'https://openalex.org/W2052697931', 'https://openalex.org/W2250874882', 'https://openalex.org/W2786608204', 'https://openalex.org/W2106053110', 'https://openalex.org/W4293665662', 'https://openalex.org/W1531663008', 'https://openalex.org/W2591013610', 'https://openalex.org/W2556930864']",2018-04-01
https://openalex.org/W2963618559,,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,"We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.",[],2017-01-01
https://openalex.org/W2962974898,https://doi.org/10.1109/wacv.2018.00082,Decoupled Learning for Conditional Adversarial Networks,"Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network. This paper gives the first attempt to relax the need of manual balancing by proposing the concept of decoupled learning, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses. In existing works, the encoding-decoding nets and GANs are integrated by sharing weights on the generator/decoder, thus the two losses are backpropagated to the generator/decoder simultaneously, where a weighting factor is needed to balance the interaction between the two losses. The decoupled learning avoids the interaction and thus removes the requirement of the weighting factor, essentially improving the generalization capacity of the designed model to different applications. The decoupled learning framework could be easily adapted to most existing encoding-decoding-based generative networks and achieve competitive performance without the need of weight adjustment. Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called normalized relative discriminative score (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.","['https://openalex.org/W6717255582', 'https://openalex.org/W2533598788', 'https://openalex.org/W2963420272', 'https://openalex.org/W6713645886', 'https://openalex.org/W2963073614', 'https://openalex.org/W2053757129', 'https://openalex.org/W6718379498', 'https://openalex.org/W6734382031', 'https://openalex.org/W2962793481', 'https://openalex.org/W6726794401', 'https://openalex.org/W2962947361', 'https://openalex.org/W2405756170', 'https://openalex.org/W2592232824', 'https://openalex.org/W4293568373', 'https://openalex.org/W2952288113', 'https://openalex.org/W4297752781', 'https://openalex.org/W2554314924', 'https://openalex.org/W2620770761', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963470893', 'https://openalex.org/W2593414223', 'https://openalex.org/W189587150', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963464195', 'https://openalex.org/W2577946330', 'https://openalex.org/W2519536754', 'https://openalex.org/W4311415873', 'https://openalex.org/W4293320219', 'https://openalex.org/W2964167449', 'https://openalex.org/W2548275288', 'https://openalex.org/W2950776302', 'https://openalex.org/W2173520492', 'https://openalex.org/W2963373786', 'https://openalex.org/W4320013936', 'https://openalex.org/W1797268635', 'https://openalex.org/W2202109488']",2018-03-01
https://openalex.org/W2532494225,https://doi.org/10.1109/apsipa.2016.7820786,Voice conversion from non-parallel corpora using variational auto-encoder,"We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.","['https://openalex.org/W2512087624', 'https://openalex.org/W2290946177', 'https://openalex.org/W2169579015', 'https://openalex.org/W2067175291', 'https://openalex.org/W2161476805', 'https://openalex.org/W2407281753', 'https://openalex.org/W6631184489', 'https://openalex.org/W6675944832', 'https://openalex.org/W6640963894', 'https://openalex.org/W2121387787', 'https://openalex.org/W2019849101', 'https://openalex.org/W2157412983', 'https://openalex.org/W2156477760', 'https://openalex.org/W2290463584', 'https://openalex.org/W6696767757', 'https://openalex.org/W1517202054', 'https://openalex.org/W2120605154', 'https://openalex.org/W1977362459', 'https://openalex.org/W2473388484', 'https://openalex.org/W2049686551', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W1665214252', 'https://openalex.org/W2108501770', 'https://openalex.org/W2032130465', 'https://openalex.org/W2964121744', 'https://openalex.org/W2294351487', 'https://openalex.org/W1520370180', 'https://openalex.org/W2949416428', 'https://openalex.org/W1522301498', 'https://openalex.org/W1959608418']",2016-12-01
https://openalex.org/W4320013936,https://doi.org/10.1007/978-3-658-40442-0_9,Generative Adversarial Nets,,[],2023-01-01
https://openalex.org/W2964243274,https://doi.org/10.1109/icassp.2018.8461368,Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.","['https://openalex.org/W2964301388', 'https://openalex.org/W2507771204', 'https://openalex.org/W6738277540', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963609956', 'https://openalex.org/W6679436768', 'https://openalex.org/W2120847449', 'https://openalex.org/W2749651610', 'https://openalex.org/W6756197946', 'https://openalex.org/W2148154194', 'https://openalex.org/W6638667902', 'https://openalex.org/W2131774270', 'https://openalex.org/W2769810959', 'https://openalex.org/W2154920538', 'https://openalex.org/W6635953567', 'https://openalex.org/W6675380101', 'https://openalex.org/W6631190155', 'https://openalex.org/W2129142580', 'https://openalex.org/W2111284386', 'https://openalex.org/W2150658333', 'https://openalex.org/W6734815144', 'https://openalex.org/W1570629387', 'https://openalex.org/W2064675550', 'https://openalex.org/W6679434410', 'https://openalex.org/W6623517193', 'https://openalex.org/W6634817459', 'https://openalex.org/W6714142977', 'https://openalex.org/W6674330103', 'https://openalex.org/W2095705004', 'https://openalex.org/W1836465849', 'https://openalex.org/W2591927543', 'https://openalex.org/W4293714597', 'https://openalex.org/W2519091744', 'https://openalex.org/W2619368999', 'https://openalex.org/W854541894', 'https://openalex.org/W2102003408', 'https://openalex.org/W1579853615', 'https://openalex.org/W2766812927', 'https://openalex.org/W2901997113', 'https://openalex.org/W4294619240', 'https://openalex.org/W1599623585', 'https://openalex.org/W1522301498', 'https://openalex.org/W2133564696', 'https://openalex.org/W2130942839']",2018-04-01
https://openalex.org/W2347098582,https://doi.org/10.1016/j.procs.2016.04.033,Variational Inference for Acoustic Unit Discovery,"Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.","['https://openalex.org/W2170353620', 'https://openalex.org/W1796128977', 'https://openalex.org/W6959456255', 'https://openalex.org/W1967687583', 'https://openalex.org/W2127498532', 'https://openalex.org/W1635512741', 'https://openalex.org/W6624852173', 'https://openalex.org/W2040891197', 'https://openalex.org/W2154099718', 'https://openalex.org/W1506806321', 'https://openalex.org/W2619993508', 'https://openalex.org/W2120636621', 'https://openalex.org/W1516111018', 'https://openalex.org/W2100768664']",2016-01-01
https://openalex.org/W4394670483,https://doi.org/10.48550/arxiv.1701.07875,Wasserstein GAN,"We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.",[],2017-01-26
https://openalex.org/W2598638573,https://doi.org/10.21437/ssw.2016-33,Merlin: An Open Source Neural Network Speech Synthesis System,"We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.","['https://openalex.org/W2402539796', 'https://openalex.org/W2100167690', 'https://openalex.org/W2129142580', 'https://openalex.org/W2399853303', 'https://openalex.org/W1924770834', 'https://openalex.org/W2406990556', 'https://openalex.org/W2049686551', 'https://openalex.org/W1990505856', 'https://openalex.org/W2402103843', 'https://openalex.org/W2045158511', 'https://openalex.org/W2111284386', 'https://openalex.org/W2134973740', 'https://openalex.org/W1888924187', 'https://openalex.org/W1571950845', 'https://openalex.org/W2397670047', 'https://openalex.org/W2102003408', 'https://openalex.org/W3123963976', 'https://openalex.org/W1613141907', 'https://openalex.org/W133102907', 'https://openalex.org/W2964060510', 'https://openalex.org/W1576227399', 'https://openalex.org/W2294797155', 'https://openalex.org/W2806733747', 'https://openalex.org/W2079735306', 'https://openalex.org/W1499332833', 'https://openalex.org/W2239904444', 'https://openalex.org/W1502723613', 'https://openalex.org/W1120805016', 'https://openalex.org/W2020024436', 'https://openalex.org/W2170980774', 'https://openalex.org/W31154030', 'https://openalex.org/W2395700867', 'https://openalex.org/W1544516254', 'https://openalex.org/W2404881427', 'https://openalex.org/W2043003570', 'https://openalex.org/W2471520273', 'https://openalex.org/W1543299179', 'https://openalex.org/W2136374105']",2016-09-13
https://openalex.org/W2099471712,https://doi.org/10.3156/jsoft.29.5_177_2,GAN（Generative Adversarial Nets）,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","['https://openalex.org/W2163605009', 'https://openalex.org/W1813659000', 'https://openalex.org/W2163176424', 'https://openalex.org/W3118608800', 'https://openalex.org/W1994618660', 'https://openalex.org/W1990838964', 'https://openalex.org/W2156387975', 'https://openalex.org/W189596042', 'https://openalex.org/W1606347560', 'https://openalex.org/W152055444', 'https://openalex.org/W2184045248', 'https://openalex.org/W2134842679', 'https://openalex.org/W2122262818', 'https://openalex.org/W2072128103', 'https://openalex.org/W2546302380', 'https://openalex.org/W2950320139', 'https://openalex.org/W2025768430', 'https://openalex.org/W2294059674', 'https://openalex.org/W1496559305', 'https://openalex.org/W1909320841', 'https://openalex.org/W1904365287', 'https://openalex.org/W2136922672', 'https://openalex.org/W2951446714', 'https://openalex.org/W1993845689', 'https://openalex.org/W2098617596', 'https://openalex.org/W2152790380', 'https://openalex.org/W2097268041', 'https://openalex.org/W2126398289', 'https://openalex.org/W1959608418', 'https://openalex.org/W2112796928', 'https://openalex.org/W1872489089', 'https://openalex.org/W2106439909', 'https://openalex.org/W2116825644', 'https://openalex.org/W2964153729']",2017-10-15
https://openalex.org/W2963830550,https://doi.org/10.21437/interspeech.2018-1830,Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations,"Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker.In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals.An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation.The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance.The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator.A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained.Conventional voice conversion metrics are reported.We also show that the speaker information has been properly reduced from the latent representations.","['https://openalex.org/W2502312327', 'https://openalex.org/W2962974898', 'https://openalex.org/W2964135678', 'https://openalex.org/W2950776302', 'https://openalex.org/W2476548250', 'https://openalex.org/W2621350877', 'https://openalex.org/W2120605154', 'https://openalex.org/W2157412983', 'https://openalex.org/W2758785877', 'https://openalex.org/W1523372075', 'https://openalex.org/W2962896155', 'https://openalex.org/W4295521014', 'https://openalex.org/W2396025094', 'https://openalex.org/W2127520494', 'https://openalex.org/W2532494225', 'https://openalex.org/W4298426053', 'https://openalex.org/W2547364378', 'https://openalex.org/W1509691205', 'https://openalex.org/W1959608418', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963073614', 'https://openalex.org/W2057609679', 'https://openalex.org/W2156142001', 'https://openalex.org/W2086796102', 'https://openalex.org/W2963808252', 'https://openalex.org/W2056852181', 'https://openalex.org/W2148846882', 'https://openalex.org/W2651834199', 'https://openalex.org/W2608207374', 'https://openalex.org/W2774848319', 'https://openalex.org/W1987992317', 'https://openalex.org/W2105160541', 'https://openalex.org/W2619034550']",2018-08-28
https://openalex.org/W2962736743,https://doi.org/10.1109/slt.2016.7846310,Discriminative acoustic word embeddings: Tecurrent neural network-based approaches,"Acoustic word embeddings - fixed-dimensional vector representations of variable-length spoken word segments - have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a ""Siamese network"" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.","['https://openalex.org/W6622239757', 'https://openalex.org/W2095705004', 'https://openalex.org/W2166637769', 'https://openalex.org/W2171590421', 'https://openalex.org/W6637242042', 'https://openalex.org/W6631190155', 'https://openalex.org/W6600284362', 'https://openalex.org/W6681435938', 'https://openalex.org/W6761030284', 'https://openalex.org/W2511733680', 'https://openalex.org/W2296681920', 'https://openalex.org/W2059652594', 'https://openalex.org/W1577418252', 'https://openalex.org/W2190506272', 'https://openalex.org/W2963571336', 'https://openalex.org/W2294995564', 'https://openalex.org/W1496120315', 'https://openalex.org/W2131033001', 'https://openalex.org/W1997505733', 'https://openalex.org/W6623517193', 'https://openalex.org/W2097207027', 'https://openalex.org/W6629930100', 'https://openalex.org/W2077450519', 'https://openalex.org/W2799046698', 'https://openalex.org/W6640708543', 'https://openalex.org/W2117459613', 'https://openalex.org/W2096298528', 'https://openalex.org/W2114478143', 'https://openalex.org/W6640777149', 'https://openalex.org/W1545920196', 'https://openalex.org/W2407151108', 'https://openalex.org/W2064675550', 'https://openalex.org/W1967924372', 'https://openalex.org/W2143612262', 'https://openalex.org/W6640212811', 'https://openalex.org/W2964121744', 'https://openalex.org/W1942035323', 'https://openalex.org/W2172609755', 'https://openalex.org/W6908809', 'https://openalex.org/W753012316', 'https://openalex.org/W2187089797', 'https://openalex.org/W1522301498', 'https://openalex.org/W1924770834', 'https://openalex.org/W1509905243', 'https://openalex.org/W2291770225', 'https://openalex.org/W854541894', 'https://openalex.org/W1942713348', 'https://openalex.org/W2405601855', 'https://openalex.org/W1665214252', 'https://openalex.org/W1499864241', 'https://openalex.org/W2146502635', 'https://openalex.org/W2936995161']",2016-12-01
https://openalex.org/W2608207374,https://doi.org/10.48550/arxiv.1703.10135,Tacotron: Towards End-to-End Speech Synthesis,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.","['https://openalex.org/W2963069010', 'https://openalex.org/W2507771204', 'https://openalex.org/W2949117887', 'https://openalex.org/W2525778437', 'https://openalex.org/W2428702538', 'https://openalex.org/W2591927543', 'https://openalex.org/W2099057450', 'https://openalex.org/W2194775991', 'https://openalex.org/W2546744831', 'https://openalex.org/W2271840356', 'https://openalex.org/W2949382160', 'https://openalex.org/W2953331651', 'https://openalex.org/W2129142580', 'https://openalex.org/W1924770834', 'https://openalex.org/W2950304420', 'https://openalex.org/W2964308564', 'https://openalex.org/W1563460361', 'https://openalex.org/W2531207078', 'https://openalex.org/W2515943672', 'https://openalex.org/W2964121744']",2017-03-29
https://openalex.org/W2476548250,https://doi.org/10.1109/cvpr.2016.207,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,"Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.","['https://openalex.org/W2150081556', 'https://openalex.org/W2044451987', 'https://openalex.org/W6605475217', 'https://openalex.org/W1979606177', 'https://openalex.org/W1950594372', 'https://openalex.org/W6677651945', 'https://openalex.org/W2109586214', 'https://openalex.org/W2097117768', 'https://openalex.org/W2111454493', 'https://openalex.org/W2112024783', 'https://openalex.org/W4136762', 'https://openalex.org/W2172174689', 'https://openalex.org/W2118963448', 'https://openalex.org/W6638300599', 'https://openalex.org/W2129276048', 'https://openalex.org/W2154579312', 'https://openalex.org/W6684191040', 'https://openalex.org/W1981990039', 'https://openalex.org/W2121927366', 'https://openalex.org/W6680935700', 'https://openalex.org/W1791560514', 'https://openalex.org/W2087436818', 'https://openalex.org/W2016482162', 'https://openalex.org/W6683781035', 'https://openalex.org/W4235713725', 'https://openalex.org/W2165939075', 'https://openalex.org/W6624640001', 'https://openalex.org/W2103047572', 'https://openalex.org/W2103844245', 'https://openalex.org/W2534320940', 'https://openalex.org/W2058182006', 'https://openalex.org/W6677645113', 'https://openalex.org/W2126653386', 'https://openalex.org/W1977581467', 'https://openalex.org/W1983364832', 'https://openalex.org/W6605397647', 'https://openalex.org/W6602211262', 'https://openalex.org/W1949096787', 'https://openalex.org/W2117865218', 'https://openalex.org/W6639309558', 'https://openalex.org/W1849277567', 'https://openalex.org/W2146782367', 'https://openalex.org/W2164551808', 'https://openalex.org/W6600294690', 'https://openalex.org/W6680022851', 'https://openalex.org/W2121058967', 'https://openalex.org/W2164598857', 'https://openalex.org/W1522734439', 'https://openalex.org/W6678025836', 'https://openalex.org/W2599944722', 'https://openalex.org/W2118103795', 'https://openalex.org/W1906770428', 'https://openalex.org/W1885185971', 'https://openalex.org/W2141200610', 'https://openalex.org/W2250093075', 'https://openalex.org/W2117539524', 'https://openalex.org/W1686810756', 'https://openalex.org/W3104720471', 'https://openalex.org/W2120824855', 'https://openalex.org/W2160558632', 'https://openalex.org/W1903029394', 'https://openalex.org/W7682646', 'https://openalex.org/W935139217', 'https://openalex.org/W18046889', 'https://openalex.org/W2163605009', 'https://openalex.org/W134193804', 'https://openalex.org/W1811400895', 'https://openalex.org/W54257720', 'https://openalex.org/W135113724', 'https://openalex.org/W1591116419', 'https://openalex.org/W2134672310']",2016-06-01
https://openalex.org/W2548275288,https://doi.org/10.48550/arxiv.1610.09585,Conditional Image Synthesis With Auxiliary Classifier GANs,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.","['https://openalex.org/W2535388113', 'https://openalex.org/W2626610348', 'https://openalex.org/W2949416428', 'https://openalex.org/W1909320841', 'https://openalex.org/W2117539524', 'https://openalex.org/W2949605076', 'https://openalex.org/W2529989861', 'https://openalex.org/W2951004968', 'https://openalex.org/W2423557781', 'https://openalex.org/W2963226019', 'https://openalex.org/W2516038988', 'https://openalex.org/W2952742172', 'https://openalex.org/W2173520492', 'https://openalex.org/W1738019091', 'https://openalex.org/W2530372461', 'https://openalex.org/W2418098761', 'https://openalex.org/W2151035455', 'https://openalex.org/W2412510955', 'https://openalex.org/W2431962807', 'https://openalex.org/W2953318193', 'https://openalex.org/W2125389028', 'https://openalex.org/W299440670', 'https://openalex.org/W2409550820', 'https://openalex.org/W2133665775', 'https://openalex.org/W2951523806', 'https://openalex.org/W2466606639', 'https://openalex.org/W2178928294', 'https://openalex.org/W2950179405', 'https://openalex.org/W1901129140', 'https://openalex.org/W2949888546']",2016-10-30
https://openalex.org/W2963799213,https://doi.org/10.48550/arxiv.1711.00937,Neural Discrete Representation Learning,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",[],2017-11-02
https://openalex.org/W2758785877,https://doi.org/10.48550/arxiv.1709.07902,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,"We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.","['https://openalex.org/W1909320841', 'https://openalex.org/W2950067852', 'https://openalex.org/W2735642330', 'https://openalex.org/W2411541852', 'https://openalex.org/W2464234964', 'https://openalex.org/W2173520492', 'https://openalex.org/W2181607856', 'https://openalex.org/W2202109488', 'https://openalex.org/W1522301498', 'https://openalex.org/W2005708641', 'https://openalex.org/W2964084166', 'https://openalex.org/W1691728462', 'https://openalex.org/W1524333225', 'https://openalex.org/W2510842514', 'https://openalex.org/W2396566817', 'https://openalex.org/W1516111018', 'https://openalex.org/W2737108017', 'https://openalex.org/W2951004968', 'https://openalex.org/W2024490156', 'https://openalex.org/W2953318193', 'https://openalex.org/W2293634267', 'https://openalex.org/W2412589713', 'https://openalex.org/W2753738274', 'https://openalex.org/W2213952365', 'https://openalex.org/W1933340426', 'https://openalex.org/W2605762339']",2017-09-22
https://openalex.org/W2059652594,https://doi.org/10.1109/asru.2013.6707765,Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,"Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.","['https://openalex.org/W1579848672', 'https://openalex.org/W2170580867', 'https://openalex.org/W137541753', 'https://openalex.org/W2395342389', 'https://openalex.org/W1974540032', 'https://openalex.org/W2111732304', 'https://openalex.org/W2126203737', 'https://openalex.org/W2799046698', 'https://openalex.org/W2110889574', 'https://openalex.org/W2097527229', 'https://openalex.org/W2147717514', 'https://openalex.org/W1583581687', 'https://openalex.org/W6683338658', 'https://openalex.org/W2053186076', 'https://openalex.org/W6683161245', 'https://openalex.org/W2097308346', 'https://openalex.org/W6675747103', 'https://openalex.org/W1984857732', 'https://openalex.org/W2097207027', 'https://openalex.org/W2023952145', 'https://openalex.org/W2407964052', 'https://openalex.org/W1978741356', 'https://openalex.org/W6634500723', 'https://openalex.org/W2114347655', 'https://openalex.org/W2397535009', 'https://openalex.org/W6714100551', 'https://openalex.org/W3148981562', 'https://openalex.org/W2117459613', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W2997701990', 'https://openalex.org/W2569190383', 'https://openalex.org/W2407151108', 'https://openalex.org/W2158139921', 'https://openalex.org/W2104290444', 'https://openalex.org/W2025482506', 'https://openalex.org/W2157444450', 'https://openalex.org/W2151660570', 'https://openalex.org/W1576613474', 'https://openalex.org/W2164365067']",2013-12-01
https://openalex.org/W2963571336,https://doi.org/10.21437/interspeech.2016-82,Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder,"The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry.This paper proposes a parallel version, the Audio Word2Vec.It offers the vector representations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD).In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements.We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Autoencoder (SA).SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence.The two RNNs are jointly trained by minimizing the reconstruction error.Denoising Sequence-to-sequence Autoencoder (DSA) is further proposed offering more robust learning.","['https://openalex.org/W4294170691', 'https://openalex.org/W2059652594', 'https://openalex.org/W1689711448', 'https://openalex.org/W2100649405', 'https://openalex.org/W1494198834', 'https://openalex.org/W2293634267', 'https://openalex.org/W2030422732', 'https://openalex.org/W2128160875', 'https://openalex.org/W2295088914', 'https://openalex.org/W1496120315', 'https://openalex.org/W1606347560', 'https://openalex.org/W2107878631', 'https://openalex.org/W4302400662', 'https://openalex.org/W2025768430', 'https://openalex.org/W2401725913', 'https://openalex.org/W2181347294', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131744502', 'https://openalex.org/W2157331557', 'https://openalex.org/W111477576', 'https://openalex.org/W1577418252', 'https://openalex.org/W2296681920', 'https://openalex.org/W139960808', 'https://openalex.org/W1924770834', 'https://openalex.org/W2105016867', 'https://openalex.org/W2116435618', 'https://openalex.org/W1501669607', 'https://openalex.org/W4213009331', 'https://openalex.org/W2018970719', 'https://openalex.org/W2115613106', 'https://openalex.org/W2100495367', 'https://openalex.org/W2997574889', 'https://openalex.org/W1614298861', 'https://openalex.org/W1810943226', 'https://openalex.org/W2147107577', 'https://openalex.org/W2190506272']",2016-08-29
https://openalex.org/W2963341956,https://doi.org/10.18653/v1/n19-1423,,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.","['https://openalex.org/W3098057198', 'https://openalex.org/W2888329843', 'https://openalex.org/W2897076808', 'https://openalex.org/W2551396370', 'https://openalex.org/W2117130368', 'https://openalex.org/W2880875857', 'https://openalex.org/W2413794162', 'https://openalex.org/W2025768430', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963159690', 'https://openalex.org/W2963748441', 'https://openalex.org/W2978670439', 'https://openalex.org/W3104033643', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963088785', 'https://openalex.org/W2507974895', 'https://openalex.org/W2158108973', 'https://openalex.org/W2130903752', 'https://openalex.org/W2891602716', 'https://openalex.org/W2963403868', 'https://openalex.org/W2149933564', 'https://openalex.org/W2131744502', 'https://openalex.org/W2158139315', 'https://openalex.org/W2131462252', 'https://openalex.org/W2270070752', 'https://openalex.org/W2951714314', 'https://openalex.org/W2963339397', 'https://openalex.org/W2784823820', 'https://openalex.org/W2250539671', 'https://openalex.org/W2144578941', 'https://openalex.org/W2121227244', 'https://openalex.org/W1566289585', 'https://openalex.org/W2396767181', 'https://openalex.org/W2962718483', 'https://openalex.org/W1599016936', 'https://openalex.org/W2963564796', 'https://openalex.org/W2962808855', 'https://openalex.org/W2963644595', 'https://openalex.org/W1840435438', 'https://openalex.org/W2963563735', 'https://openalex.org/W2108598243', 'https://openalex.org/W131533222', 'https://openalex.org/W1486649854', 'https://openalex.org/W2963846996', 'https://openalex.org/W2462831000', 'https://openalex.org/W2170973209', 'https://openalex.org/W2153579005', 'https://openalex.org/W2251939518', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963918774', 'https://openalex.org/W2610858497']",2019-01-01
https://openalex.org/W2972447203,https://doi.org/10.1162/tacl_a_00283,<i>Tabula</i>Nearly<i>Rasa:</i>Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text,"Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our “near tabula rasa” RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.","['https://openalex.org/W2085663684', 'https://openalex.org/W2905532018', 'https://openalex.org/W2029948425', 'https://openalex.org/W2018982897', 'https://openalex.org/W2166391802', 'https://openalex.org/W4241976975', 'https://openalex.org/W354650940', 'https://openalex.org/W2110485445', 'https://openalex.org/W1988126949', 'https://openalex.org/W2255581062', 'https://openalex.org/W2883158411', 'https://openalex.org/W2480148714', 'https://openalex.org/W4210984920', 'https://openalex.org/W2126377586', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963430224', 'https://openalex.org/W1971624066', 'https://openalex.org/W3140196993', 'https://openalex.org/W2292919134', 'https://openalex.org/W2295297373', 'https://openalex.org/W2020944885', 'https://openalex.org/W2531882892', 'https://openalex.org/W2531207078', 'https://openalex.org/W2549835527', 'https://openalex.org/W2104752510', 'https://openalex.org/W2507974895', 'https://openalex.org/W4252588748', 'https://openalex.org/W2148917251', 'https://openalex.org/W1522263329', 'https://openalex.org/W2510248383', 'https://openalex.org/W2781528640', 'https://openalex.org/W3105148948', 'https://openalex.org/W591353820', 'https://openalex.org/W2953318277', 'https://openalex.org/W2767360697', 'https://openalex.org/W2759181158', 'https://openalex.org/W2888958984', 'https://openalex.org/W2884554299', 'https://openalex.org/W2168488947', 'https://openalex.org/W196214544', 'https://openalex.org/W2963751529', 'https://openalex.org/W1938755728', 'https://openalex.org/W2949888546', 'https://openalex.org/W2900065283', 'https://openalex.org/W1614298861', 'https://openalex.org/W1527142120', 'https://openalex.org/W2561296568', 'https://openalex.org/W1722351164', 'https://openalex.org/W2905623858', 'https://openalex.org/W2788924045', 'https://openalex.org/W1810943226', 'https://openalex.org/W1579929927', 'https://openalex.org/W2559655401', 'https://openalex.org/W1550933260', 'https://openalex.org/W608050949', 'https://openalex.org/W2964072233', 'https://openalex.org/W2515741950', 'https://openalex.org/W2606347107', 'https://openalex.org/W2038542953', 'https://openalex.org/W2346280181', 'https://openalex.org/W2792376130', 'https://openalex.org/W2963025830', 'https://openalex.org/W2803214681', 'https://openalex.org/W2964289395', 'https://openalex.org/W2108425242', 'https://openalex.org/W2141599568', 'https://openalex.org/W2605717780', 'https://openalex.org/W2964159778', 'https://openalex.org/W2563574619', 'https://openalex.org/W2124059530', 'https://openalex.org/W2963208801', 'https://openalex.org/W2889506631', 'https://openalex.org/W331019419', 'https://openalex.org/W2167723982', 'https://openalex.org/W2911267749', 'https://openalex.org/W4387627350', 'https://openalex.org/W2579599705', 'https://openalex.org/W2799124508', 'https://openalex.org/W2176796957', 'https://openalex.org/W653912306', 'https://openalex.org/W4299895104']",2019-09-11
https://openalex.org/W2252211741,https://doi.org/10.18653/v1/d15-1036,Evaluation methods for unsupervised word embeddings,"We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.We present new evaluation techniques that directly compare embeddings with respect to specific queries.These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.","['https://openalex.org/W2158899491', 'https://openalex.org/W2112184938', 'https://openalex.org/W2130451992', 'https://openalex.org/W1849368448', 'https://openalex.org/W1988686126', 'https://openalex.org/W1615991656', 'https://openalex.org/W2158139315', 'https://openalex.org/W2251803266', 'https://openalex.org/W2041836310', 'https://openalex.org/W2153579005', 'https://openalex.org/W1499253590', 'https://openalex.org/W2125031621', 'https://openalex.org/W4294170691', 'https://openalex.org/W2123442489', 'https://openalex.org/W4386506836', 'https://openalex.org/W2147946282', 'https://openalex.org/W2296076036', 'https://openalex.org/W2250539671', 'https://openalex.org/W2141599568', 'https://openalex.org/W2113459411', 'https://openalex.org/W2251703179', 'https://openalex.org/W2952230511', 'https://openalex.org/W1576954243', 'https://openalex.org/W2081580037', 'https://openalex.org/W2053921957', 'https://openalex.org/W2118585731', 'https://openalex.org/W2251066368', 'https://openalex.org/W2057399676', 'https://openalex.org/W2251253014', 'https://openalex.org/W2159426623', 'https://openalex.org/W2159495802', 'https://openalex.org/W2035265474']",2015-01-01
https://openalex.org/W3016011332,https://doi.org/10.1109/icassp40776.2020.9054438,Generative Pre-Training for Speech with Autoregressive Predictive Coding,"Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.","['https://openalex.org/W2889028433', 'https://openalex.org/W2943845043', 'https://openalex.org/W6761563299', 'https://openalex.org/W2963026768', 'https://openalex.org/W6752888775', 'https://openalex.org/W6764498146', 'https://openalex.org/W2973217961', 'https://openalex.org/W2973034126', 'https://openalex.org/W6745117592', 'https://openalex.org/W179875071', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964199361', 'https://openalex.org/W6748148878', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2194775991', 'https://openalex.org/W6755977528', 'https://openalex.org/W4300558631', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W6898505805', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973048981', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963317665', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2884305338', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2024490156', 'https://openalex.org/W6766673545', 'https://openalex.org/W6755207826', 'https://openalex.org/W2899134946', 'https://openalex.org/W6748215858', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963432880', 'https://openalex.org/W2785350307', 'https://openalex.org/W2941814890', 'https://openalex.org/W2808706139', 'https://openalex.org/W854541894', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963045354', 'https://openalex.org/W2758785877', 'https://openalex.org/W2101105183', 'https://openalex.org/W2952127920', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W2899663614', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964121744']",2020-04-09
https://openalex.org/W2137735870,,Distributional Semantics in Technicolor,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.","['https://openalex.org/W2128053456', 'https://openalex.org/W2053921957', 'https://openalex.org/W2130463046', 'https://openalex.org/W2066134726', 'https://openalex.org/W1566135517', 'https://openalex.org/W2162915993', 'https://openalex.org/W2250196260', 'https://openalex.org/W2151103935', 'https://openalex.org/W2141282920', 'https://openalex.org/W1974689608', 'https://openalex.org/W1664311846', 'https://openalex.org/W1983578042', 'https://openalex.org/W155596317', 'https://openalex.org/W2124386111', 'https://openalex.org/W1897761818', 'https://openalex.org/W2128017662', 'https://openalex.org/W1662133657', 'https://openalex.org/W2104978738', 'https://openalex.org/W2145023731', 'https://openalex.org/W2103652203', 'https://openalex.org/W1771464967', 'https://openalex.org/W115307656', 'https://openalex.org/W2125186487', 'https://openalex.org/W1608604164', 'https://openalex.org/W1981617416', 'https://openalex.org/W2252218513', 'https://openalex.org/W2177113878', 'https://openalex.org/W149643677', 'https://openalex.org/W1625255723', 'https://openalex.org/W2050830825', 'https://openalex.org/W2126274282', 'https://openalex.org/W2036718463', 'https://openalex.org/W2148596671', 'https://openalex.org/W2128870637', 'https://openalex.org/W2131846894', 'https://openalex.org/W2066941820']",2012-07-08
https://openalex.org/W3003875258,https://doi.org/10.1109/icassp40776.2020.9053541,Unsupervised Pre-Training of Bidirectional Speech Encoders via Masked Reconstruction,"We propose an approach for pre-training speech representations via a masked reconstruction loss. Our pre-trained encoder networks are bidirectional and can therefore be used directly in typical bidirectional speech recognition models. The pre-trained networks can then be fine-tuned on a smaller amount of supervised data for speech recognition. Experiments with this approach on the LibriSpeech and Wall Street Journal corpora show promising results. We find that the main factors that lead to speech recognition improvements are: masking segments of sufficient width in both time and frequency, pre-training on a much larger amount of unlabeled data than the labeled data, and domain adaptation when the unlabeled and labeled data come from different domains. The gain from pre-training is additive to that of supervised data augmentation.","['https://openalex.org/W2963211739', 'https://openalex.org/W2087006792', 'https://openalex.org/W1519855838', 'https://openalex.org/W6755207826', 'https://openalex.org/W6750615492', 'https://openalex.org/W2936774411', 'https://openalex.org/W2940544976', 'https://openalex.org/W3100270690', 'https://openalex.org/W2468716020', 'https://openalex.org/W6729977899', 'https://openalex.org/W6745117592', 'https://openalex.org/W2963317665', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W2064675550', 'https://openalex.org/W6631943919', 'https://openalex.org/W2973157397', 'https://openalex.org/W1513862252', 'https://openalex.org/W6674330103', 'https://openalex.org/W2972943112', 'https://openalex.org/W2160815625', 'https://openalex.org/W6748634344', 'https://openalex.org/W2116064496', 'https://openalex.org/W2979476256', 'https://openalex.org/W6754278344', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W4297808394', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973026522', 'https://openalex.org/W2923014074', 'https://openalex.org/W2996383576', 'https://openalex.org/W1665214252', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963310665', 'https://openalex.org/W2556930864', 'https://openalex.org/W1533861849', 'https://openalex.org/W3125709657', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963618559', 'https://openalex.org/W1522301498', 'https://openalex.org/W2887997457', 'https://openalex.org/W1524333225', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W3098643042', 'https://openalex.org/W2095705004']",2020-04-09
https://openalex.org/W2809981375,,The Emergence of Grammaticality in Connectionist Networks,,[],2009-04-12
https://openalex.org/W2142625445,https://doi.org/10.1145/2339530.2339751,Large-scale learning of word relatedness with constraints,"Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.","['https://openalex.org/W2155870214', 'https://openalex.org/W1880262756', 'https://openalex.org/W2136930489', 'https://openalex.org/W1593239840', 'https://openalex.org/W2147152072', 'https://openalex.org/W4235505822', 'https://openalex.org/W1993509040', 'https://openalex.org/W3216404684', 'https://openalex.org/W3001753394', 'https://openalex.org/W2787894218', 'https://openalex.org/W6678578999', 'https://openalex.org/W588052932', 'https://openalex.org/W2120084270', 'https://openalex.org/W2026487812', 'https://openalex.org/W1994616650', 'https://openalex.org/W1557757161', 'https://openalex.org/W203276351', 'https://openalex.org/W1974595223', 'https://openalex.org/W2059975159', 'https://openalex.org/W1974406477', 'https://openalex.org/W2058602429', 'https://openalex.org/W2117065474', 'https://openalex.org/W2053921957', 'https://openalex.org/W1992914835', 'https://openalex.org/W2099938389', 'https://openalex.org/W1558797106', 'https://openalex.org/W1480376833', 'https://openalex.org/W2038721957', 'https://openalex.org/W2125972432', 'https://openalex.org/W2166490638', 'https://openalex.org/W1521908097', 'https://openalex.org/W1970381522']",2012-08-12
https://openalex.org/W2103318667,https://doi.org/10.1080/01690969108406936,Contextual correlates of semantic similarity,"Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.","['https://openalex.org/W13823885', 'https://openalex.org/W2100322854', 'https://openalex.org/W2079194046', 'https://openalex.org/W2437277295', 'https://openalex.org/W2045553479', 'https://openalex.org/W4300703805', 'https://openalex.org/W4238944320', 'https://openalex.org/W4298351405', 'https://openalex.org/W1966153816', 'https://openalex.org/W2033716723', 'https://openalex.org/W4210560736', 'https://openalex.org/W1971220772', 'https://openalex.org/W2036895580', 'https://openalex.org/W1634667895', 'https://openalex.org/W2114826854', 'https://openalex.org/W2020159140', 'https://openalex.org/W1570047706', 'https://openalex.org/W1989415743', 'https://openalex.org/W2026442395', 'https://openalex.org/W4236556464', 'https://openalex.org/W2014735492', 'https://openalex.org/W2064332540', 'https://openalex.org/W2914326967', 'https://openalex.org/W2080100102', 'https://openalex.org/W1751175273', 'https://openalex.org/W1536719366', 'https://openalex.org/W2321575885', 'https://openalex.org/W2746118309', 'https://openalex.org/W4298028598', 'https://openalex.org/W1507401974', 'https://openalex.org/W4301847896', 'https://openalex.org/W1966680968', 'https://openalex.org/W2091384152', 'https://openalex.org/W2071094139', 'https://openalex.org/W1483126227', 'https://openalex.org/W2109334311', 'https://openalex.org/W1513522035', 'https://openalex.org/W2082766935', 'https://openalex.org/W2017580301', 'https://openalex.org/W1547269118', 'https://openalex.org/W4205256201', 'https://openalex.org/W3023045483']",1991-01-01
https://openalex.org/W3093096176,https://doi.org/10.21437/interspeech.2020-2743,The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units,"We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.",[],2020-10-25
https://openalex.org/W2176085882,https://doi.org/10.3115/v1/d14-1034,An Unsupervised Model for Instance Level Subcategorization Acquisition,"Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .","['https://openalex.org/W2111024941', 'https://openalex.org/W1996430422', 'https://openalex.org/W2116410915', 'https://openalex.org/W2171116649', 'https://openalex.org/W2097606805', 'https://openalex.org/W1611318214', 'https://openalex.org/W2084774044', 'https://openalex.org/W1601035521', 'https://openalex.org/W1982827383', 'https://openalex.org/W2039217078', 'https://openalex.org/W32345482', 'https://openalex.org/W2068250380', 'https://openalex.org/W2757452186', 'https://openalex.org/W2607879133', 'https://openalex.org/W2099896133', 'https://openalex.org/W2250837944', 'https://openalex.org/W2121798597', 'https://openalex.org/W1511986666', 'https://openalex.org/W2119056510', 'https://openalex.org/W1662133657', 'https://openalex.org/W2250213987', 'https://openalex.org/W2029184000', 'https://openalex.org/W2129227538', 'https://openalex.org/W1508977358', 'https://openalex.org/W2153274216', 'https://openalex.org/W2005181355', 'https://openalex.org/W1585222170', 'https://openalex.org/W2081228205', 'https://openalex.org/W2250526231', 'https://openalex.org/W2053921957', 'https://openalex.org/W1945172732', 'https://openalex.org/W2144321756', 'https://openalex.org/W1818857488', 'https://openalex.org/W2107928532', 'https://openalex.org/W2083342361', 'https://openalex.org/W2251314312', 'https://openalex.org/W1490737687', 'https://openalex.org/W1855822899', 'https://openalex.org/W2135674549', 'https://openalex.org/W2076482515', 'https://openalex.org/W2121435899', 'https://openalex.org/W2074064986', 'https://openalex.org/W2088198454', 'https://openalex.org/W2078546664', 'https://openalex.org/W1972805353', 'https://openalex.org/W1502719479', 'https://openalex.org/W2138615112', 'https://openalex.org/W2130178369', 'https://openalex.org/W1564452174', 'https://openalex.org/W1909733559', 'https://openalex.org/W2130717590', 'https://openalex.org/W14067211', 'https://openalex.org/W1586407311', 'https://openalex.org/W1626542014', 'https://openalex.org/W81190909', 'https://openalex.org/W2052474702']",2014-01-01
https://openalex.org/W2741692265,https://doi.org/10.18653/v1/e17-2020,Comparing Character-level Neural Language Models Using a Lexical Decision Task,International audience,"['https://openalex.org/W2014307400', 'https://openalex.org/W2168979204', 'https://openalex.org/W2037387434', 'https://openalex.org/W2963251942', 'https://openalex.org/W2141599568', 'https://openalex.org/W2740606810', 'https://openalex.org/W2531882892', 'https://openalex.org/W4254816979', 'https://openalex.org/W1548013675', 'https://openalex.org/W4362220304', 'https://openalex.org/W2549835527', 'https://openalex.org/W303185113', 'https://openalex.org/W1989462718', 'https://openalex.org/W2138723686', 'https://openalex.org/W2110485445', 'https://openalex.org/W1836465849', 'https://openalex.org/W1566289585', 'https://openalex.org/W2098192529', 'https://openalex.org/W1522301498', 'https://openalex.org/W2064675550', 'https://openalex.org/W2185277953', 'https://openalex.org/W1938755728', 'https://openalex.org/W2951559648', 'https://openalex.org/W2964121744', 'https://openalex.org/W2885588803', 'https://openalex.org/W2020220682', 'https://openalex.org/W1917177419', 'https://openalex.org/W2962776659', 'https://openalex.org/W2515741950', 'https://openalex.org/W1990948551']",2017-01-01
https://openalex.org/W3102342027,https://doi.org/10.18653/v1/2020.findings-emnlp.106,Learning Robust and Multilingual Speech Representations,"Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages.","['https://openalex.org/W3101648800', 'https://openalex.org/W2136922672', 'https://openalex.org/W2184045248', 'https://openalex.org/W2152790380', 'https://openalex.org/W2948012107', 'https://openalex.org/W2166637769', 'https://openalex.org/W4288107125', 'https://openalex.org/W2110798204', 'https://openalex.org/W2066873261', 'https://openalex.org/W2971155163', 'https://openalex.org/W2401271873', 'https://openalex.org/W2996383576', 'https://openalex.org/W4300906742', 'https://openalex.org/W2997574889', 'https://openalex.org/W2024490156', 'https://openalex.org/W4294170691', 'https://openalex.org/W2842511635', 'https://openalex.org/W2962739339', 'https://openalex.org/W2953469440', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W3123318516', 'https://openalex.org/W2094544353', 'https://openalex.org/W2963446712', 'https://openalex.org/W2119569779', 'https://openalex.org/W4297808394', 'https://openalex.org/W2970241862', 'https://openalex.org/W2963341956', 'https://openalex.org/W4300047444', 'https://openalex.org/W2963620343', 'https://openalex.org/W2979476256', 'https://openalex.org/W2135346645', 'https://openalex.org/W2572097499', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962824366', 'https://openalex.org/W2166243357', 'https://openalex.org/W2949759968', 'https://openalex.org/W2520160253', 'https://openalex.org/W2944828972', 'https://openalex.org/W4289259401', 'https://openalex.org/W3189092450', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W2153579005', 'https://openalex.org/W3016181583', 'https://openalex.org/W2593116425', 'https://openalex.org/W3015522062', 'https://openalex.org/W2108384452', 'https://openalex.org/W2145094598', 'https://openalex.org/W3127686677', 'https://openalex.org/W2901739041', 'https://openalex.org/W2971840980', 'https://openalex.org/W2991213871', 'https://openalex.org/W2896457183', 'https://openalex.org/W2146444479', 'https://openalex.org/W2193413348', 'https://openalex.org/W2150178435', 'https://openalex.org/W2524544624', 'https://openalex.org/W2007645738', 'https://openalex.org/W1530454533']",2020-01-01
https://openalex.org/W2014307400,https://doi.org/10.3758/brm.42.3.627,Wuggy: A multilingual pseudoword generator,,"['https://openalex.org/W2168979204', 'https://openalex.org/W2186780112', 'https://openalex.org/W2140255971', 'https://openalex.org/W1980082163', 'https://openalex.org/W1986788796', 'https://openalex.org/W2073556933', 'https://openalex.org/W2095845433', 'https://openalex.org/W2106558942', 'https://openalex.org/W4247667741', 'https://openalex.org/W2144710514', 'https://openalex.org/W2073441004', 'https://openalex.org/W2135314857', 'https://openalex.org/W2061625509', 'https://openalex.org/W2020220682', 'https://openalex.org/W4290377501', 'https://openalex.org/W2132193240']",2010-08-01
https://openalex.org/W2965373594,https://doi.org/10.4230/oasics.ldk.2021.22,Towards Learning Terminological Concept Systems from Multilingual Natural Language Text,"Terminological Concept Systems (TCS) provide a means of organizing, structuring and representing domain-specific multilingual information and are important to ensure terminological consistency in many tasks, such as translation and cross-border communication. While several approaches to (semi-)automatic term extraction exist, learning their interrelations is vastly underexplored. We propose an automated method to extract terms and relations across natural languages and specialized domains. To this end, we adapt pretrained multilingual neural language models, which we evaluate on term extraction standard datasets with best performing results and a combination of relation extraction standard datasets with competitive results. Code and dataset are publicly available.","['https://openalex.org/W2170973209', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963807318', 'https://openalex.org/W1599016936', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963748441', 'https://openalex.org/W2805206884', 'https://openalex.org/W2937297214', 'https://openalex.org/W2963846996', 'https://openalex.org/W2525127255', 'https://openalex.org/W2950501607', 'https://openalex.org/W2963112338', 'https://openalex.org/W2947813521', 'https://openalex.org/W2945785363', 'https://openalex.org/W2930786691', 'https://openalex.org/W2945290257', 'https://openalex.org/W2920812691', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963403868', 'https://openalex.org/W2948629866', 'https://openalex.org/W2962753370', 'https://openalex.org/W2914526845', 'https://openalex.org/W2963341956', 'https://openalex.org/W2978670439', 'https://openalex.org/W2793353489', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963310665', 'https://openalex.org/W2396767181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2130158090', 'https://openalex.org/W1566289585', 'https://openalex.org/W2899771611', 'https://openalex.org/W131533222', 'https://openalex.org/W2963756346', 'https://openalex.org/W2251939518', 'https://openalex.org/W2970597249', 'https://openalex.org/W2990704537', 'https://openalex.org/W2963026768', 'https://openalex.org/W2914120296', 'https://openalex.org/W2938830017', 'https://openalex.org/W2945260553', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963323070', 'https://openalex.org/W2784121710', 'https://openalex.org/W1840435438']",2021-01-01
https://openalex.org/W2080100102,https://doi.org/10.1145/365628.365657,Contextual correlates of synonymy,"article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","['https://openalex.org/W2882319491', 'https://openalex.org/W2015271197', 'https://openalex.org/W2087404238', 'https://openalex.org/W2033716723', 'https://openalex.org/W1738233868']",1965-10-01
https://openalex.org/W2346964103,https://doi.org/10.1016/j.procs.2016.04.031,The Zero Resource Speech Challenge 2015: Proposed Approaches and Results,"This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.","['https://openalex.org/W2025482506', 'https://openalex.org/W2346964103', 'https://openalex.org/W2400668844', 'https://openalex.org/W2396043527', 'https://openalex.org/W2402366697', 'https://openalex.org/W2399576818', 'https://openalex.org/W2407614114', 'https://openalex.org/W2398490608', 'https://openalex.org/W1796128977', 'https://openalex.org/W2404799143', 'https://openalex.org/W2044138293', 'https://openalex.org/W2238331496', 'https://openalex.org/W2247128061', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W2251025892', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117126688', 'https://openalex.org/W2052697931', 'https://openalex.org/W2011845089', 'https://openalex.org/W2786608204']",2016-01-01
https://openalex.org/W2251012068,,Better Word Representations with Recursive Neural Networks for Morphology,"Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.","['https://openalex.org/W2040711288', 'https://openalex.org/W2167419393', 'https://openalex.org/W2004763266', 'https://openalex.org/W1889268436', 'https://openalex.org/W2103305545', 'https://openalex.org/W1792316426', 'https://openalex.org/W1970381522', 'https://openalex.org/W2053921957', 'https://openalex.org/W2131462252', 'https://openalex.org/W36903255', 'https://openalex.org/W2137807925', 'https://openalex.org/W2103318667', 'https://openalex.org/W1423339008', 'https://openalex.org/W2164973920', 'https://openalex.org/W2025032307', 'https://openalex.org/W2140275137', 'https://openalex.org/W2080100102', 'https://openalex.org/W2171928131', 'https://openalex.org/W2081580037', 'https://openalex.org/W2571532437', 'https://openalex.org/W2164019165', 'https://openalex.org/W2018789714', 'https://openalex.org/W98255950', 'https://openalex.org/W2128634885', 'https://openalex.org/W2091812280', 'https://openalex.org/W2158139315', 'https://openalex.org/W2150539551', 'https://openalex.org/W2104518905', 'https://openalex.org/W71795751', 'https://openalex.org/W2158899491', 'https://openalex.org/W2053306448', 'https://openalex.org/W22861983', 'https://openalex.org/W179875071', 'https://openalex.org/W2141599568', 'https://openalex.org/W2132339004', 'https://openalex.org/W2117130368', 'https://openalex.org/W1999965501']",2013-08-01
https://openalex.org/W2251025892,,Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems,International audience,"['https://openalex.org/W2074546930', 'https://openalex.org/W2029948425', 'https://openalex.org/W2084734691', 'https://openalex.org/W2403207842', 'https://openalex.org/W1548450879', 'https://openalex.org/W2119187236', 'https://openalex.org/W2952343510', 'https://openalex.org/W2126377586', 'https://openalex.org/W2251671292', 'https://openalex.org/W2057007397', 'https://openalex.org/W2114347655', 'https://openalex.org/W2212197906', 'https://openalex.org/W2049142189', 'https://openalex.org/W2055408826', 'https://openalex.org/W2107959623', 'https://openalex.org/W2112107221', 'https://openalex.org/W2100768664', 'https://openalex.org/W2083087666', 'https://openalex.org/W66167291', 'https://openalex.org/W2072240081', 'https://openalex.org/W2117126688', 'https://openalex.org/W2050850095', 'https://openalex.org/W2025482506', 'https://openalex.org/W2126449874', 'https://openalex.org/W2166270474']",2014-05-01
https://openalex.org/W2549835527,https://doi.org/10.1162/tacl_a_00115,Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies,"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.","['https://openalex.org/W4245765565', 'https://openalex.org/W2087807962', 'https://openalex.org/W2147571305', 'https://openalex.org/W2110485445', 'https://openalex.org/W2151834591', 'https://openalex.org/W2190736972', 'https://openalex.org/W1988126949', 'https://openalex.org/W2121029939', 'https://openalex.org/W2064675550', 'https://openalex.org/W2301095666', 'https://openalex.org/W1970526427', 'https://openalex.org/W1972085588', 'https://openalex.org/W2116723809', 'https://openalex.org/W1965511524', 'https://openalex.org/W2100808592', 'https://openalex.org/W2082041499', 'https://openalex.org/W1951216520', 'https://openalex.org/W595069947', 'https://openalex.org/W4231809946', 'https://openalex.org/W2914746235', 'https://openalex.org/W2259472270', 'https://openalex.org/W2963069010', 'https://openalex.org/W2251529809', 'https://openalex.org/W2963447120', 'https://openalex.org/W2013833248', 'https://openalex.org/W2292919134', 'https://openalex.org/W2157331557', 'https://openalex.org/W1602017060', 'https://openalex.org/W2024988999', 'https://openalex.org/W2600110521', 'https://openalex.org/W2160073299', 'https://openalex.org/W4285719527', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964159778', 'https://openalex.org/W2402268235', 'https://openalex.org/W2157889740', 'https://openalex.org/W2170716495', 'https://openalex.org/W1595256356', 'https://openalex.org/W2134991647', 'https://openalex.org/W2105214622', 'https://openalex.org/W2087946919', 'https://openalex.org/W2118170533', 'https://openalex.org/W1732222442', 'https://openalex.org/W2951714314', 'https://openalex.org/W2963073938', 'https://openalex.org/W2964308564', 'https://openalex.org/W1574036494', 'https://openalex.org/W179875071', 'https://openalex.org/W2974832207']",2016-12-01
https://openalex.org/W2593779438,,ABX-discriminability measures and applications,"This thesis constitutes an indirect contribution to the problem of modeling phonetic category acquisition in infancy. Some specific computational models of phonetic category acquisition have been proposed, but they were never tested extensively nor compared quantitatively to see whether they were really able to account for a sizable portion of the available empirical observations. In this thesis, we introduce ABX-Discriminability Measures and we develop a methodology based on these measures that allows to perform such a systematic evaluation. We demonstrate the interest of our framework by applying it to the evaluation of models for two related problems: phonetic category processing at birth and in adulthood. The next step, applying our framework to models of phonetic category acquisition, is left for future work.The interest of ABX-Discriminability Measures is not restricted to the particular problem of evaluating models of phonetic category processing in humans. We argue that their interest generalizes to the study of other signals than speech and other category structures than phonetic categories, as well as to other research fields than cognitive science, like low-resource engineering, data mining and artificial intelligence for example. To make this point, we study the properties of these measures in a general abstract framework and we detail the rationale for three broad family of potential applications: evaluating systems operating without explicit supervision in their ability to represent a category structure; providing simple computational models of behavior in discrimination tasks; providing descriptive measurements for representations of categorical data.","['https://openalex.org/W2404799143', 'https://openalex.org/W2127394101', 'https://openalex.org/W2073279303', 'https://openalex.org/W2062663442', 'https://openalex.org/W2032476212', 'https://openalex.org/W2396043527', 'https://openalex.org/W1974777011', 'https://openalex.org/W2153006233', 'https://openalex.org/W3187397227', 'https://openalex.org/W2397686546', 'https://openalex.org/W2399576818', 'https://openalex.org/W2085085647', 'https://openalex.org/W2072059109', 'https://openalex.org/W3034729383', 'https://openalex.org/W1587073199', 'https://openalex.org/W2078993594', 'https://openalex.org/W2036502752', 'https://openalex.org/W2100768664', 'https://openalex.org/W2400549570', 'https://openalex.org/W1977979148', 'https://openalex.org/W1982801854', 'https://openalex.org/W2095458199', 'https://openalex.org/W2091432990', 'https://openalex.org/W2015977525', 'https://openalex.org/W2018069071', 'https://openalex.org/W2115614142', 'https://openalex.org/W2164203346', 'https://openalex.org/W2270030679', 'https://openalex.org/W2093111935', 'https://openalex.org/W2145410271', 'https://openalex.org/W2406349064', 'https://openalex.org/W2022465033', 'https://openalex.org/W2068116204', 'https://openalex.org/W1976526581', 'https://openalex.org/W2150261429', 'https://openalex.org/W2142209090', 'https://openalex.org/W1945005118', 'https://openalex.org/W2119821739', 'https://openalex.org/W2167499516', 'https://openalex.org/W2086989311', 'https://openalex.org/W1980704774', 'https://openalex.org/W1924689489', 'https://openalex.org/W1964201439', 'https://openalex.org/W2406820985', 'https://openalex.org/W2040913319', 'https://openalex.org/W1995710564', 'https://openalex.org/W2168548156', 'https://openalex.org/W2107274740', 'https://openalex.org/W2108145097', 'https://openalex.org/W2010692105', 'https://openalex.org/W1537881105', 'https://openalex.org/W1558306184', 'https://openalex.org/W1968801326', 'https://openalex.org/W2135563147', 'https://openalex.org/W2090861223', 'https://openalex.org/W2017133403', 'https://openalex.org/W2168614750', 'https://openalex.org/W2013020033', 'https://openalex.org/W2152483743', 'https://openalex.org/W2787234017', 'https://openalex.org/W8787889', 'https://openalex.org/W2171061694', 'https://openalex.org/W1524333225', 'https://openalex.org/W2786608204', 'https://openalex.org/W2889643527', 'https://openalex.org/W2102040782', 'https://openalex.org/W2149159134', 'https://openalex.org/W2026408911', 'https://openalex.org/W2002276939', 'https://openalex.org/W2092655206', 'https://openalex.org/W2009150118', 'https://openalex.org/W2346964103', 'https://openalex.org/W2007498854', 'https://openalex.org/W1972950801', 'https://openalex.org/W2075854640', 'https://openalex.org/W2126958229', 'https://openalex.org/W1532704504', 'https://openalex.org/W3151807103', 'https://openalex.org/W1796128977', 'https://openalex.org/W1564039035', 'https://openalex.org/W2395899413', 'https://openalex.org/W1967924372', 'https://openalex.org/W2060349248', 'https://openalex.org/W2052697931', 'https://openalex.org/W2160815625', 'https://openalex.org/W1980044217', 'https://openalex.org/W2250874882', 'https://openalex.org/W2407151108', 'https://openalex.org/W2150021272', 'https://openalex.org/W2075371579', 'https://openalex.org/W2144640755', 'https://openalex.org/W2096736596', 'https://openalex.org/W2397058949', 'https://openalex.org/W2139102046', 'https://openalex.org/W1784695092', 'https://openalex.org/W2153767712', 'https://openalex.org/W2041394569', 'https://openalex.org/W2031282998', 'https://openalex.org/W2129877081', 'https://openalex.org/W1990444693', 'https://openalex.org/W2108858705', 'https://openalex.org/W66627554', 'https://openalex.org/W1994492508', 'https://openalex.org/W1554944419', 'https://openalex.org/W2053872598', 'https://openalex.org/W2011238950', 'https://openalex.org/W2005311247', 'https://openalex.org/W1604599697', 'https://openalex.org/W1972168487', 'https://openalex.org/W2345968833', 'https://openalex.org/W607518649', 'https://openalex.org/W2109572214', 'https://openalex.org/W2002681612', 'https://openalex.org/W2113153226', 'https://openalex.org/W2133687321', 'https://openalex.org/W2023163512', 'https://openalex.org/W2401464865', 'https://openalex.org/W1575566382', 'https://openalex.org/W2076493153', 'https://openalex.org/W2093231248', 'https://openalex.org/W153534061', 'https://openalex.org/W2121947440', 'https://openalex.org/W2010725167', 'https://openalex.org/W2345811097', 'https://openalex.org/W2167057640', 'https://openalex.org/W2407869992', 'https://openalex.org/W281094599', 'https://openalex.org/W2136653392', 'https://openalex.org/W1987841215', 'https://openalex.org/W2126345904', 'https://openalex.org/W1901616594', 'https://openalex.org/W2410344739', 'https://openalex.org/W2103869314', 'https://openalex.org/W2011701921', 'https://openalex.org/W2037976268', 'https://openalex.org/W2114719288', 'https://openalex.org/W2056730193', 'https://openalex.org/W2055120733', 'https://openalex.org/W2074314503', 'https://openalex.org/W2142633459', 'https://openalex.org/W2101509422', 'https://openalex.org/W1992468098', 'https://openalex.org/W2086022490', 'https://openalex.org/W1981241922', 'https://openalex.org/W1974776535', 'https://openalex.org/W2024490156', 'https://openalex.org/W2052005497', 'https://openalex.org/W2020344439', 'https://openalex.org/W2014512781', 'https://openalex.org/W2764410184', 'https://openalex.org/W2063303346', 'https://openalex.org/W2110627398', 'https://openalex.org/W598767079', 'https://openalex.org/W1545406001', 'https://openalex.org/W2049068198', 'https://openalex.org/W643591744']",2016-09-29
https://openalex.org/W2132631284,,Verb similarity on the taxonomy of WordNet,"In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity","['https://openalex.org/W2003240077', 'https://openalex.org/W1950936596', 'https://openalex.org/W2117753247', 'https://openalex.org/W2038721957', 'https://openalex.org/W2140887277', 'https://openalex.org/W1971220772', 'https://openalex.org/W1573498319', 'https://openalex.org/W2136480620', 'https://openalex.org/W1567365482', 'https://openalex.org/W2117805756', 'https://openalex.org/W4285719527', 'https://openalex.org/W2123489126', 'https://openalex.org/W1983578042', 'https://openalex.org/W2100935296', 'https://openalex.org/W3088601333', 'https://openalex.org/W2149671658', 'https://openalex.org/W2087739686', 'https://openalex.org/W2534712034', 'https://openalex.org/W136846643', 'https://openalex.org/W2962689487', 'https://openalex.org/W2080100102', 'https://openalex.org/W2081580037', 'https://openalex.org/W2160482687', 'https://openalex.org/W2117149238']",2006-01-01
https://openalex.org/W2026487812,https://doi.org/10.1145/1963405.1963455,A word at a time,"Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as ""war"" and ""peace"" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.","['https://openalex.org/W2918757710', 'https://openalex.org/W6637101025', 'https://openalex.org/W2127128140', 'https://openalex.org/W1963726077', 'https://openalex.org/W2091735588', 'https://openalex.org/W116902681', 'https://openalex.org/W2136930489', 'https://openalex.org/W2040546864', 'https://openalex.org/W1593239840', 'https://openalex.org/W2147152072', 'https://openalex.org/W1964365134', 'https://openalex.org/W2108280221', 'https://openalex.org/W4235505822', 'https://openalex.org/W3216404684', 'https://openalex.org/W2081798681', 'https://openalex.org/W2120779048', 'https://openalex.org/W2097766966', 'https://openalex.org/W2021314079', 'https://openalex.org/W2061986359', 'https://openalex.org/W2150739536', 'https://openalex.org/W2120084270', 'https://openalex.org/W2057714964', 'https://openalex.org/W2127536142', 'https://openalex.org/W3003709066', 'https://openalex.org/W2161443453', 'https://openalex.org/W2166008115', 'https://openalex.org/W2117065474', 'https://openalex.org/W2058602429', 'https://openalex.org/W2170907470', 'https://openalex.org/W2165299010', 'https://openalex.org/W2147880780', 'https://openalex.org/W1965495241', 'https://openalex.org/W1601068082', 'https://openalex.org/W2113889316', 'https://openalex.org/W2100935296', 'https://openalex.org/W2168621303', 'https://openalex.org/W2053921957', 'https://openalex.org/W1970381522', 'https://openalex.org/W1782572861', 'https://openalex.org/W1659833910', 'https://openalex.org/W2038721957', 'https://openalex.org/W158057341', 'https://openalex.org/W1647729745', 'https://openalex.org/W1984558542', 'https://openalex.org/W1521908097', 'https://openalex.org/W1660390307', 'https://openalex.org/W1646006088', 'https://openalex.org/W1992914835', 'https://openalex.org/W2022122406']",2011-03-28
https://openalex.org/W1494198834,https://doi.org/10.1109/icassp.2015.7178964,Librispeech: An ASR corpus based on public domain audio books,"This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.","['https://openalex.org/W2113641473', 'https://openalex.org/W2090755665', 'https://openalex.org/W2148154194', 'https://openalex.org/W2106554350', 'https://openalex.org/W2087064593', 'https://openalex.org/W1647671624', 'https://openalex.org/W1517939602', 'https://openalex.org/W2037740282', 'https://openalex.org/W1599512239', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603477829', 'https://openalex.org/W2024490156', 'https://openalex.org/W2164107060', 'https://openalex.org/W6712802073', 'https://openalex.org/W6677973343', 'https://openalex.org/W6636811518', 'https://openalex.org/W6738902873', 'https://openalex.org/W2097927681', 'https://openalex.org/W2026369565', 'https://openalex.org/W2330075180', 'https://openalex.org/W2950186769', 'https://openalex.org/W2125234026', 'https://openalex.org/W1524333225', 'https://openalex.org/W1934041838', 'https://openalex.org/W1631260214', 'https://openalex.org/W2397159106', 'https://openalex.org/W2620757702', 'https://openalex.org/W85707815', 'https://openalex.org/W2916535084']",2015-04-01
https://openalex.org/W1854884267,https://doi.org/10.1162/coli_a_00237,SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation,"We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.","['https://openalex.org/W2170682101', 'https://openalex.org/W2036931463', 'https://openalex.org/W2251874715', 'https://openalex.org/W2251803266', 'https://openalex.org/W2128870637', 'https://openalex.org/W2117865617', 'https://openalex.org/W2108530510', 'https://openalex.org/W2132339004', 'https://openalex.org/W21497345', 'https://openalex.org/W4251372957', 'https://openalex.org/W1979606348', 'https://openalex.org/W2136930489', 'https://openalex.org/W2123982464', 'https://openalex.org/W2117130368', 'https://openalex.org/W2067438047', 'https://openalex.org/W2000746239', 'https://openalex.org/W2482300836', 'https://openalex.org/W2078841894', 'https://openalex.org/W1984251878', 'https://openalex.org/W1788602', 'https://openalex.org/W2171802951', 'https://openalex.org/W2032964561', 'https://openalex.org/W2286410738', 'https://openalex.org/W2251117789', 'https://openalex.org/W2250676463', 'https://openalex.org/W1983578042', 'https://openalex.org/W2143413399', 'https://openalex.org/W2296076036', 'https://openalex.org/W2027267056', 'https://openalex.org/W2086039194', 'https://openalex.org/W2165979968', 'https://openalex.org/W2436001372', 'https://openalex.org/W2035726644', 'https://openalex.org/W1985953330', 'https://openalex.org/W2142120379', 'https://openalex.org/W2171836785', 'https://openalex.org/W2111258243', 'https://openalex.org/W1970476061', 'https://openalex.org/W1554804307', 'https://openalex.org/W2080834271', 'https://openalex.org/W2080100102', 'https://openalex.org/W2127002961', 'https://openalex.org/W1662133657', 'https://openalex.org/W2059975159', 'https://openalex.org/W1582344906', 'https://openalex.org/W2136480620', 'https://openalex.org/W2117805756', 'https://openalex.org/W2140406733', 'https://openalex.org/W1614298861', 'https://openalex.org/W181737412', 'https://openalex.org/W2158139315', 'https://openalex.org/W2135341569', 'https://openalex.org/W1541481035', 'https://openalex.org/W1565863475', 'https://openalex.org/W2251012068', 'https://openalex.org/W2098352331', 'https://openalex.org/W2164973920', 'https://openalex.org/W2129271949', 'https://openalex.org/W2290013849', 'https://openalex.org/W2137735870', 'https://openalex.org/W4285719527', 'https://openalex.org/W96809255', 'https://openalex.org/W2080902366', 'https://openalex.org/W3099386342', 'https://openalex.org/W2250742840', 'https://openalex.org/W191422183', 'https://openalex.org/W2506188197', 'https://openalex.org/W2126530744', 'https://openalex.org/W2154531419', 'https://openalex.org/W2150102617', 'https://openalex.org/W2251771443', 'https://openalex.org/W1868671693', 'https://openalex.org/W2164019165', 'https://openalex.org/W2250536421', 'https://openalex.org/W1566139570', 'https://openalex.org/W2143017621', 'https://openalex.org/W2153579005']",2015-12-01
https://openalex.org/W2889947987,https://doi.org/10.18653/v1/w18-5412,Can LSTM Learn to Capture Agreement? The Case of Basque,"Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system. Analyzing experimental results from two syntactic prediction tasks – verb number prediction and suffix recovery – we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English. Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence. We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.","['https://openalex.org/W2768794963', 'https://openalex.org/W2549835527', 'https://openalex.org/W2963571341', 'https://openalex.org/W2963430224', 'https://openalex.org/W2962776659', 'https://openalex.org/W2964121744', 'https://openalex.org/W2552110825', 'https://openalex.org/W782259221', 'https://openalex.org/W2949952998', 'https://openalex.org/W1810943226', 'https://openalex.org/W4210984920', 'https://openalex.org/W2064675550', 'https://openalex.org/W2126807068', 'https://openalex.org/W1522301498', 'https://openalex.org/W2563574619', 'https://openalex.org/W2826721128', 'https://openalex.org/W2963751529', 'https://openalex.org/W2259472270', 'https://openalex.org/W2134036914', 'https://openalex.org/W2963748792', 'https://openalex.org/W583659850', 'https://openalex.org/W2157331557', 'https://openalex.org/W1924770834', 'https://openalex.org/W2515741950', 'https://openalex.org/W2963951265', 'https://openalex.org/W1951216520', 'https://openalex.org/W4300427683', 'https://openalex.org/W2786167576', 'https://openalex.org/W2606089314', 'https://openalex.org/W2301095666', 'https://openalex.org/W1601694074']",2018-01-01
https://openalex.org/W2170682101,https://doi.org/10.3115/1620754.1620758,A study on similarity and relatedness using distributional and WordNet-based approaches,"This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.","['https://openalex.org/W2120779048', 'https://openalex.org/W1782572861', 'https://openalex.org/W2951798058', 'https://openalex.org/W4233775226', 'https://openalex.org/W192694488', 'https://openalex.org/W2166971953', 'https://openalex.org/W2096223431', 'https://openalex.org/W2161443453', 'https://openalex.org/W2100935296', 'https://openalex.org/W2136930489', 'https://openalex.org/W4255386104', 'https://openalex.org/W2103318667', 'https://openalex.org/W1959533457', 'https://openalex.org/W2136480620', 'https://openalex.org/W1567365482', 'https://openalex.org/W2170344111', 'https://openalex.org/W2166776180', 'https://openalex.org/W2055518963', 'https://openalex.org/W2534712034', 'https://openalex.org/W2149393279', 'https://openalex.org/W2149801387', 'https://openalex.org/W2117805756', 'https://openalex.org/W1517377188', 'https://openalex.org/W4300121351', 'https://openalex.org/W158057341', 'https://openalex.org/W1573498319', 'https://openalex.org/W2135207619', 'https://openalex.org/W4255198209', 'https://openalex.org/W1647729745', 'https://openalex.org/W1596967103', 'https://openalex.org/W2053921957', 'https://openalex.org/W2042160362', 'https://openalex.org/W4256347525', 'https://openalex.org/W2950225692', 'https://openalex.org/W2080100102', 'https://openalex.org/W2005181355']",2009-01-01
https://openalex.org/W2996728628,https://doi.org/10.1162/tacl_a_00321,BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources),"We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.","['https://openalex.org/W2888922637', 'https://openalex.org/W2963025830', 'https://openalex.org/W1986120881', 'https://openalex.org/W2963403868', 'https://openalex.org/W2167723982', 'https://openalex.org/W2995181640', 'https://openalex.org/W2163730805', 'https://openalex.org/W2923014074', 'https://openalex.org/W2750779823', 'https://openalex.org/W2962961857', 'https://openalex.org/W2973957133', 'https://openalex.org/W2953369973', 'https://openalex.org/W2787560479', 'https://openalex.org/W2911109671', 'https://openalex.org/W2963494889', 'https://openalex.org/W3037191812', 'https://openalex.org/W2918996109', 'https://openalex.org/W2963026768', 'https://openalex.org/W1582487387', 'https://openalex.org/W2064675550', 'https://openalex.org/W2158195707', 'https://openalex.org/W2770232188', 'https://openalex.org/W4365799947', 'https://openalex.org/W2978670439', 'https://openalex.org/W2972351548', 'https://openalex.org/W2759181158', 'https://openalex.org/W2170716495', 'https://openalex.org/W4252209867', 'https://openalex.org/W2964117978', 'https://openalex.org/W179875071', 'https://openalex.org/W2730712696', 'https://openalex.org/W1502957213', 'https://openalex.org/W2549835527', 'https://openalex.org/W2971044268', 'https://openalex.org/W2531882892', 'https://openalex.org/W2994665957', 'https://openalex.org/W4388123003', 'https://openalex.org/W2964204621', 'https://openalex.org/W2891399254', 'https://openalex.org/W1984471812', 'https://openalex.org/W1974795422', 'https://openalex.org/W2963341956', 'https://openalex.org/W2599674900', 'https://openalex.org/W2612690371', 'https://openalex.org/W4245765565', 'https://openalex.org/W2864832950', 'https://openalex.org/W2563574619', 'https://openalex.org/W2124669395', 'https://openalex.org/W2515741950', 'https://openalex.org/W2141440284', 'https://openalex.org/W2972752636', 'https://openalex.org/W2981852735', 'https://openalex.org/W2902967615', 'https://openalex.org/W1586060904', 'https://openalex.org/W2962926715', 'https://openalex.org/W2251930319', 'https://openalex.org/W2024988999', 'https://openalex.org/W2990704537']",2020-07-29
https://openalex.org/W2910243263,https://doi.org/10.48550/arxiv.1901.05287,Assessing BERT's Syntactic Abilities,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","['https://openalex.org/W2963341956', 'https://openalex.org/W2963751529', 'https://openalex.org/W2888539709', 'https://openalex.org/W2888922637', 'https://openalex.org/W2962911926', 'https://openalex.org/W2963403868']",2019-01-16
https://openalex.org/W2119717200,https://doi.org/10.1007/bf00992696,Simple statistical gradient-following algorithms for connectionist reinforcement learning,,"['https://openalex.org/W1505136099', 'https://openalex.org/W2021801581', 'https://openalex.org/W7024637854', 'https://openalex.org/W2091565802', 'https://openalex.org/W2102673654', 'https://openalex.org/W6636530465', 'https://openalex.org/W6820729035', 'https://openalex.org/W6634074277', 'https://openalex.org/W2080759927', 'https://openalex.org/W6632924624', 'https://openalex.org/W2043968544', 'https://openalex.org/W6632269551', 'https://openalex.org/W2068896964', 'https://openalex.org/W6682302528', 'https://openalex.org/W4300402905', 'https://openalex.org/W1569296262', 'https://openalex.org/W6780394890', 'https://openalex.org/W2160575156', 'https://openalex.org/W2101130101', 'https://openalex.org/W1573503290', 'https://openalex.org/W2166082022', 'https://openalex.org/W1993411524', 'https://openalex.org/W2766736793', 'https://openalex.org/W2565808444', 'https://openalex.org/W1547224907', 'https://openalex.org/W1610678877', 'https://openalex.org/W354832773', 'https://openalex.org/W2949173247', 'https://openalex.org/W2009796613', 'https://openalex.org/W3011120880', 'https://openalex.org/W2152475379', 'https://openalex.org/W2100677568', 'https://openalex.org/W105751747', 'https://openalex.org/W569156507', 'https://openalex.org/W1569320505', 'https://openalex.org/W1968743014', 'https://openalex.org/W1538558539', 'https://openalex.org/W1652505363', 'https://openalex.org/W3207342693', 'https://openalex.org/W3121926921', 'https://openalex.org/W2796556654']",1992-05-01
https://openalex.org/W2888911345,https://doi.org/10.21437/interspeech.2018-2148,Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery,"The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model. It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task. However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization. This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions. Experiments on TIMIT and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.","['https://openalex.org/W2750248772', 'https://openalex.org/W2142384583', 'https://openalex.org/W2468716020', 'https://openalex.org/W2753738274', 'https://openalex.org/W3127686677', 'https://openalex.org/W2347098582', 'https://openalex.org/W2127498532', 'https://openalex.org/W1833498382']",2018-08-28
https://openalex.org/W2750248772,https://doi.org/10.21437/interspeech.2017-1160,Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery,"Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs.","['https://openalex.org/W1796128977', 'https://openalex.org/W2468716020', 'https://openalex.org/W2159283619', 'https://openalex.org/W2078769636', 'https://openalex.org/W2347098582', 'https://openalex.org/W2962695963', 'https://openalex.org/W2117041980', 'https://openalex.org/W1971081490', 'https://openalex.org/W2142384583', 'https://openalex.org/W2556467266', 'https://openalex.org/W2100768664', 'https://openalex.org/W2077804127', 'https://openalex.org/W2949416428']",2017-08-16
https://openalex.org/W2119885577,https://doi.org/10.1145/3147.3165,Random sampling with a reservoir,"We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O ( n (1 + log( N/n ))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.","['https://openalex.org/W2152248683', 'https://openalex.org/W2113046370', 'https://openalex.org/W2068625010', 'https://openalex.org/W2002722920', 'https://openalex.org/W2751862591', 'https://openalex.org/W2752853835', 'https://openalex.org/W2043405945', 'https://openalex.org/W4245266864']",1985-03-01
https://openalex.org/W1902027874,https://doi.org/10.1038/44565,Learning the parts of objects by non-negative matrix factorization,,"['https://openalex.org/W1976645892', 'https://openalex.org/W2080370442', 'https://openalex.org/W2144730066', 'https://openalex.org/W2156406284', 'https://openalex.org/W4241541897', 'https://openalex.org/W2138451337', 'https://openalex.org/W2085927826', 'https://openalex.org/W2145889472', 'https://openalex.org/W2056857971', 'https://openalex.org/W2006200729', 'https://openalex.org/W1993845689', 'https://openalex.org/W1983578042', 'https://openalex.org/W1996355918', 'https://openalex.org/W2108384452', 'https://openalex.org/W2035782554', 'https://openalex.org/W2069629287', 'https://openalex.org/W2170608748', 'https://openalex.org/W2088909704', 'https://openalex.org/W2049633694', 'https://openalex.org/W2148365208', 'https://openalex.org/W2325227998', 'https://openalex.org/W2126375903', 'https://openalex.org/W1536929369', 'https://openalex.org/W1532372219', 'https://openalex.org/W4285719527', 'https://openalex.org/W1956559956']",1999-10-01
https://openalex.org/W2123237149,https://doi.org/10.21437/interspeech.2011-91,Improved bottleneck features using pretrained deep neural networks,"Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems. Conventionally, bottleneck features are extracted from a multi-layer perceptron (MLP) trained to predict context-independent monophone states. The MLP typically has three hidden layers and is trained using the backpropagation algorithm. In this paper, we propose two improvements to the training of bottleneck features motivated by recent advances in the use of deep neural networks (DNNs) for speech recognition. First, we show how the use of unsupervised pretraining of a DNN enhances the network’s discriminative power and improves the bottleneck features it generates. Second, we show that a neural network trained to predict context-dependent senone targets produces better bottleneck features than one trained to predict monophone states. Bottleneck features trained using the proposed methods produced a 16% relative reduction in sentence error rate over conventional bottleneck features on a large vocabulary business search task.","['https://openalex.org/W2159948109', 'https://openalex.org/W190289757', 'https://openalex.org/W2169434751', 'https://openalex.org/W2147768505', 'https://openalex.org/W2606321545', 'https://openalex.org/W2116064496', 'https://openalex.org/W2100495367', 'https://openalex.org/W2012897754', 'https://openalex.org/W2099832642', 'https://openalex.org/W98857008', 'https://openalex.org/W1993882792', 'https://openalex.org/W217970951', 'https://openalex.org/W2167458787', 'https://openalex.org/W2141778357']",2011-08-27
https://openalex.org/W3100270690,https://doi.org/10.1109/taslp.2019.2938863,Unsupervised Speech Representation Learning Using WaveNet Autoencoders,"We consider the task of unsupervised extraction of meaningful latent\nrepresentations of speech by applying autoencoding neural networks to speech\nwaveforms. The goal is to learn a representation able to capture high level\nsemantic content from the signal, e.g.\\ phoneme identities, while being\ninvariant to confounding low level details in the signal such as the underlying\npitch contour or background noise. Since the learned representation is tuned to\ncontain only phonetic content, we resort to using a high capacity WaveNet\ndecoder to infer information discarded by the encoder from previous samples.\nMoreover, the behavior of autoencoder models depends on the kind of constraint\nthat is applied to the latent representation. We compare three variants: a\nsimple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder\n(VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of\nlearned representations in terms of speaker independence, the ability to\npredict phonetic content, and the ability to accurately reconstruct individual\nspectrogram frames. Moreover, for discrete encodings extracted using the\nVQ-VAE, we measure the ease of mapping them to phonemes. We introduce a\nregularization scheme that forces the representations to focus on the phonetic\ncontent of the utterance and report performance comparable with the top entries\nin the ZeroSpeech 2017 unsupervised acoustic unit discovery task.\n","['https://openalex.org/W2888911345', 'https://openalex.org/W2750248772', 'https://openalex.org/W2180849752', 'https://openalex.org/W2347098582', 'https://openalex.org/W2963425185', 'https://openalex.org/W6745388339', 'https://openalex.org/W2114347655', 'https://openalex.org/W6744627333', 'https://openalex.org/W2468716020', 'https://openalex.org/W6640963894', 'https://openalex.org/W6751433836', 'https://openalex.org/W2057007397', 'https://openalex.org/W1840435438', 'https://openalex.org/W6742080785', 'https://openalex.org/W6678292227', 'https://openalex.org/W1970890968', 'https://openalex.org/W2145889472', 'https://openalex.org/W1902027874', 'https://openalex.org/W6857890821', 'https://openalex.org/W2963918774', 'https://openalex.org/W2787223168', 'https://openalex.org/W2964243274', 'https://openalex.org/W2785415724', 'https://openalex.org/W2061883090', 'https://openalex.org/W2108598243', 'https://openalex.org/W6736723571', 'https://openalex.org/W6712560600', 'https://openalex.org/W6637618735', 'https://openalex.org/W6745117592', 'https://openalex.org/W6682132143', 'https://openalex.org/W6764882207', 'https://openalex.org/W6712553779', 'https://openalex.org/W6675022971', 'https://openalex.org/W2025768430', 'https://openalex.org/W2100495367', 'https://openalex.org/W2963620343', 'https://openalex.org/W6679718588', 'https://openalex.org/W1498436455', 'https://openalex.org/W2132037657', 'https://openalex.org/W2059652044', 'https://openalex.org/W6637061625', 'https://openalex.org/W2408093180', 'https://openalex.org/W2095705004', 'https://openalex.org/W6714142977', 'https://openalex.org/W2786902352', 'https://openalex.org/W2345811097', 'https://openalex.org/W6713745070', 'https://openalex.org/W6712444837', 'https://openalex.org/W6631362777', 'https://openalex.org/W6755490972', 'https://openalex.org/W2086161653', 'https://openalex.org/W6631190155', 'https://openalex.org/W6751097180', 'https://openalex.org/W1849277567', 'https://openalex.org/W6729906282', 'https://openalex.org/W6734194636', 'https://openalex.org/W6742082877', 'https://openalex.org/W2963804033', 'https://openalex.org/W2120209245', 'https://openalex.org/W1993660824', 'https://openalex.org/W1494198834', 'https://openalex.org/W2752796333', 'https://openalex.org/W6761568071', 'https://openalex.org/W2097117768', 'https://openalex.org/W2618530766', 'https://openalex.org/W6727690538', 'https://openalex.org/W6679434410', 'https://openalex.org/W2962824709', 'https://openalex.org/W2143612262', 'https://openalex.org/W2963223306', 'https://openalex.org/W2740747242', 'https://openalex.org/W6755300632', 'https://openalex.org/W6749825310', 'https://openalex.org/W2146444479', 'https://openalex.org/W6731535438', 'https://openalex.org/W6690026940', 'https://openalex.org/W6733471323', 'https://openalex.org/W6731370813']",2019-09-03
https://openalex.org/W2752796333,,Neural Discrete Representation Learning.,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of posterior collapse -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",[],2017-11-02
https://openalex.org/W1970890968,https://doi.org/10.1109/slt.2012.6424246,The language-independent bottleneck features,"In this paper we present novel language-independent bottleneck (BN) feature extraction framework. In our experiments we have used Multilingual Artificial Neural Network (ANN), where each language is modelled by separate output layer, while all the hidden layers jointly model the variability of all the source languages. The key idea is that the entire ANN is trained on all the languages simultaneously, thus the BN-features are not biased towards any of the languages. Exactly for this reason, the final BN-features are considered as language independent. In the experiments with GlobalPhone database, we show that Multilingual BN-features consistently outperform Monolingual BN-features. Also, cross-lingual generalization is evaluated, where we train on 5 source languages and test on 3 other languages. The results show that the ANN can produce very good BN-features even for unseen languages, in some cases even better than if we trained the ANN on the target language only.","['https://openalex.org/W6713204639', 'https://openalex.org/W2143355529', 'https://openalex.org/W6604916345', 'https://openalex.org/W2072349636', 'https://openalex.org/W6615085950', 'https://openalex.org/W1991180839', 'https://openalex.org/W6602682705', 'https://openalex.org/W6738263903', 'https://openalex.org/W6679081875', 'https://openalex.org/W2090764203', 'https://openalex.org/W2012897754', 'https://openalex.org/W2165712214', 'https://openalex.org/W1501286448', 'https://openalex.org/W6635896687', 'https://openalex.org/W2105354660', 'https://openalex.org/W1507177964', 'https://openalex.org/W1604771987', 'https://openalex.org/W2127982613', 'https://openalex.org/W121155700', 'https://openalex.org/W2402698866', 'https://openalex.org/W2618180021', 'https://openalex.org/W66627554']",2012-12-01
https://openalex.org/W2145889472,https://doi.org/10.1038/381607a0,Emergence of simple-cell receptive field properties by learning a sparse code for natural images,,"['https://openalex.org/W2117731089', 'https://openalex.org/W2135587681', 'https://openalex.org/W1914401667', 'https://openalex.org/W1966261464', 'https://openalex.org/W2167034998', 'https://openalex.org/W4232132600', 'https://openalex.org/W2077086611', 'https://openalex.org/W2120838001', 'https://openalex.org/W2085927826', 'https://openalex.org/W2169326908', 'https://openalex.org/W2045628405', 'https://openalex.org/W1984856371', 'https://openalex.org/W2041455057', 'https://openalex.org/W4240035482', 'https://openalex.org/W2122925692', 'https://openalex.org/W4233548532', 'https://openalex.org/W2000405398', 'https://openalex.org/W4250460708', 'https://openalex.org/W2106884367', 'https://openalex.org/W2127003262', 'https://openalex.org/W2108384452', 'https://openalex.org/W2042912927', 'https://openalex.org/W1993845689', 'https://openalex.org/W2911607583', 'https://openalex.org/W1994530392', 'https://openalex.org/W4235390321']",1996-06-01
https://openalex.org/W2086161653,https://doi.org/10.1137/0330046,Acceleration of Stochastic Approximation by Averaging,A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.,"['https://openalex.org/W2004585872', 'https://openalex.org/W2081961678', 'https://openalex.org/W2010552022', 'https://openalex.org/W2009797711', 'https://openalex.org/W2235056388', 'https://openalex.org/W1994616650', 'https://openalex.org/W2070709745', 'https://openalex.org/W2060471940', 'https://openalex.org/W2086996405', 'https://openalex.org/W2056698052', 'https://openalex.org/W1485035304', 'https://openalex.org/W105751747', 'https://openalex.org/W1586848174', 'https://openalex.org/W3139633029', 'https://openalex.org/W2336878578', 'https://openalex.org/W1498711961', 'https://openalex.org/W3045718248', 'https://openalex.org/W1569320505', 'https://openalex.org/W1540723801', 'https://openalex.org/W4205555171', 'https://openalex.org/W4233456539', 'https://openalex.org/W4206670532']",1992-07-01
https://openalex.org/W2602076750,https://doi.org/10.48550/arxiv.1703.07370,"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models","Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \emph{unbiased} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.",[],2017-03-21
https://openalex.org/W1583776211,https://doi.org/10.48550/arxiv.1305.2982,Estimating or Propagating Gradients Through Stochastic Neurons,"Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we ""back-propagate"" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.","['https://openalex.org/W1613249581', 'https://openalex.org/W2124289529', 'https://openalex.org/W2025768430', 'https://openalex.org/W2072128103', 'https://openalex.org/W2294059674', 'https://openalex.org/W2156387975', 'https://openalex.org/W2136922672', 'https://openalex.org/W2099257174', 'https://openalex.org/W60493759', 'https://openalex.org/W2041176801', 'https://openalex.org/W1665214252', 'https://openalex.org/W1498436455', 'https://openalex.org/W2913932916', 'https://openalex.org/W1904365287', 'https://openalex.org/W2163605009']",2013-05-14
https://openalex.org/W2464234964,https://doi.org/10.48550/arxiv.1603.06277,Composing graphical models with neural networks for structured representations and fast inference,"We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.","['https://openalex.org/W1959608418', 'https://openalex.org/W3146320432', 'https://openalex.org/W1909320841', 'https://openalex.org/W1503398984', 'https://openalex.org/W2155894447', 'https://openalex.org/W1850742715', 'https://openalex.org/W42107399', 'https://openalex.org/W2950067852', 'https://openalex.org/W2115979064', 'https://openalex.org/W2109957730', 'https://openalex.org/W2095898861', 'https://openalex.org/W2271522117', 'https://openalex.org/W1573082642', 'https://openalex.org/W2176035349', 'https://openalex.org/W2161523118', 'https://openalex.org/W2963173382', 'https://openalex.org/W2160815625', 'https://openalex.org/W2166851633']",2016-03-20
https://openalex.org/W2962695963,,Composing graphical models with neural networks for structured representations and fast inference,"We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.",[],2016-03-20
https://openalex.org/W1686946872,https://doi.org/10.5281/zenodo.4505123,embo - Empirical Bottleneck,"A Python implementation of the Information Bottleneck analysis framework [Tishby, Pereira, Bialek 2001], especially geared towards the analysis of concrete, finite-size data sets. See on PyPI","['https://openalex.org/W2148986322', 'https://openalex.org/W2099111195', 'https://openalex.org/W2103723258', 'https://openalex.org/W2127314673']",2020-02-03
https://openalex.org/W3049315473,https://doi.org/10.1109/icfhr2020.2020.00023,The “ScribbleLens” Dutch Historical Handwriting Corpus,"Historical handwritten documents guard an important part of human knowledge only at the reach of a few scholars and experts. Recent developments in machine learning have the potential of rendering this information accessible to a larger audience. Data-driven approaches to automatic manuscript recognition require large amounts of transcribed scans to work. To this end, we introduce a new handwritten corpus based on 400-year-old, cursive, early modern Dutch documents such as ship journals and daily logbooks. This is a 1000 page collection, segmented into lines, to facilitate fully-, weakly- and un-supervised research and with textual transcriptions on 20% of the pages. Other annotations such as handwriting slant, year of origin, complexity, and writer identity have been manually added. With over 80 writers this corpus is significantly larger and more varied than other existing historical data sets such as Spanish RODRIGO. We provide train/test splits, experimental results from an automatic transcription baseline and tools to facilitate its use in deep learning research. The manuscripts span over 150 years of significant journeys by captains and traders from the Vereenigde Oost-indische Company (VOC) such as Tasman, Brouwer and Van Neck, making this resource also valuable to historians and the paleography community.","['https://openalex.org/W6748668392', 'https://openalex.org/W2140130847', 'https://openalex.org/W2906299545', 'https://openalex.org/W6761655912', 'https://openalex.org/W2100037535', 'https://openalex.org/W6607694189', 'https://openalex.org/W2142330437', 'https://openalex.org/W2162395775', 'https://openalex.org/W2142069714', 'https://openalex.org/W2573601984', 'https://openalex.org/W3016492459', 'https://openalex.org/W6739901393', 'https://openalex.org/W2783389926', 'https://openalex.org/W6774456908', 'https://openalex.org/W6679844677', 'https://openalex.org/W2933708308', 'https://openalex.org/W4287992115', 'https://openalex.org/W2786298913', 'https://openalex.org/W2963403868', 'https://openalex.org/W3008499099', 'https://openalex.org/W2327809500', 'https://openalex.org/W2134429390', 'https://openalex.org/W4385245566', 'https://openalex.org/W189028356', 'https://openalex.org/W2995898313']",2020-09-01
https://openalex.org/W3136591341,https://doi.org/10.48550/arxiv.1703.07370,"REBAR: Low-variance, unbiased gradient estimates for discrete latent\n variable models","Learning in models with discrete latent variables is challenging due to high\nvariance gradient estimators. Generally, approaches have relied on control\nvariates to reduce the variance of the REINFORCE estimator. Recent work (Jang\net al. 2016, Maddison et al. 2016) has taken a different approach, introducing\na continuous relaxation of discrete variables to produce low-variance, but\nbiased, gradient estimates. In this work, we combine the two approaches through\na novel control variate that produces low-variance, \\emph{unbiased} gradient\nestimates. Then, we introduce a modification to the continuous relaxation and\nshow that the tightness of the relaxation can be adapted online, removing it as\na hyperparameter. We show state-of-the-art variance reduction on several\nbenchmark generative modeling tasks, generally leading to faster convergence to\na better final log-likelihood.\n",[],2017-03-21
https://openalex.org/W3008499099,,Unsupervised Neural Segmentation and Clustering for Unit Discovery in Sequential Data,International audience,"['https://openalex.org/W2146444479', 'https://openalex.org/W2962695963', 'https://openalex.org/W2162833336', 'https://openalex.org/W2888911345', 'https://openalex.org/W2193413348', 'https://openalex.org/W2100768664', 'https://openalex.org/W1959608418']",2019-12-13
https://openalex.org/W2193413348,https://doi.org/10.48550/arxiv.1512.02595,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.","['https://openalex.org/W1993882792', 'https://openalex.org/W1553004968', 'https://openalex.org/W2064675550', 'https://openalex.org/W2112739286', 'https://openalex.org/W2117671523', 'https://openalex.org/W1835666251', 'https://openalex.org/W2102113734', 'https://openalex.org/W2288645994', 'https://openalex.org/W1922655562', 'https://openalex.org/W2163680580', 'https://openalex.org/W1517386993', 'https://openalex.org/W2109726413', 'https://openalex.org/W2253807446', 'https://openalex.org/W2296748324', 'https://openalex.org/W2949190276', 'https://openalex.org/W2134557905', 'https://openalex.org/W1586532344', 'https://openalex.org/W1494198834', 'https://openalex.org/W2951781666', 'https://openalex.org/W2963920996', 'https://openalex.org/W2162390675', 'https://openalex.org/W2950179405', 'https://openalex.org/W2168231600', 'https://openalex.org/W2289394825', 'https://openalex.org/W104184427', 'https://openalex.org/W2962949994', 'https://openalex.org/W2293634267', 'https://openalex.org/W1836465849', 'https://openalex.org/W1855892484', 'https://openalex.org/W2127141656', 'https://openalex.org/W2157331557', 'https://openalex.org/W2066824132', 'https://openalex.org/W2394932179', 'https://openalex.org/W2147768505', 'https://openalex.org/W2293858598', 'https://openalex.org/W2963211739', 'https://openalex.org/W2109886035', 'https://openalex.org/W2184045248', 'https://openalex.org/W1710082047', 'https://openalex.org/W1667652561', 'https://openalex.org/W2076014259', 'https://openalex.org/W1600744878', 'https://openalex.org/W2138243089', 'https://openalex.org/W2155273149', 'https://openalex.org/W2169272853', 'https://openalex.org/W1581407678', 'https://openalex.org/W2120432001', 'https://openalex.org/W2296073425', 'https://openalex.org/W2169189000', 'https://openalex.org/W2143612262', 'https://openalex.org/W2618530766']",2015-12-08
https://openalex.org/W2804145368,https://doi.org/10.48550/arxiv.1805.11063,Theory and Experiments on Vector Quantized Autoencoders,"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.","['https://openalex.org/W2952264928', 'https://openalex.org/W1821462560', 'https://openalex.org/W1522301498', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963434219', 'https://openalex.org/W2119717200', 'https://openalex.org/W2547875792', 'https://openalex.org/W2949888546', 'https://openalex.org/W2789543585', 'https://openalex.org/W2594961016', 'https://openalex.org/W2964308564', 'https://openalex.org/W2008225289', 'https://openalex.org/W2963629403', 'https://openalex.org/W2049633694', 'https://openalex.org/W2142838865', 'https://openalex.org/W2423557781', 'https://openalex.org/W2785779000', 'https://openalex.org/W2072634211', 'https://openalex.org/W2964122153', 'https://openalex.org/W2767002724', 'https://openalex.org/W2119072456', 'https://openalex.org/W2950237263', 'https://openalex.org/W2463507112', 'https://openalex.org/W2148554573', 'https://openalex.org/W2626778328', 'https://openalex.org/W2950300355', 'https://openalex.org/W2127218421']",2018-05-28
https://openalex.org/W2949117887,https://doi.org/10.57702/o9raffed,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.","['https://openalex.org/W2127230474', 'https://openalex.org/W104184427', 'https://openalex.org/W2019866374', 'https://openalex.org/W2084894614', 'https://openalex.org/W2123649031', 'https://openalex.org/W2152424459', 'https://openalex.org/W2168231600', 'https://openalex.org/W2146502635', 'https://openalex.org/W2952020226', 'https://openalex.org/W2034368206', 'https://openalex.org/W1762484328', 'https://openalex.org/W2125930537', 'https://openalex.org/W2095705004', 'https://openalex.org/W2950179405', 'https://openalex.org/W1677182931', 'https://openalex.org/W2112796928']",2024-01-01
https://openalex.org/W2100768664,,A Nonparametric Bayesian Approach to Acoustic Model Discovery,"We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1","['https://openalex.org/W2099415988', 'https://openalex.org/W2080972498', 'https://openalex.org/W53570656', 'https://openalex.org/W2117041980', 'https://openalex.org/W2120636621', 'https://openalex.org/W2154093685', 'https://openalex.org/W2077804127', 'https://openalex.org/W2401464865', 'https://openalex.org/W2121997342', 'https://openalex.org/W2026858810', 'https://openalex.org/W1957665339', 'https://openalex.org/W1990005915', 'https://openalex.org/W2083904075', 'https://openalex.org/W3127686677', 'https://openalex.org/W2045656233', 'https://openalex.org/W2126377586', 'https://openalex.org/W2148154194', 'https://openalex.org/W2111732304', 'https://openalex.org/W2171752983', 'https://openalex.org/W2126203737', 'https://openalex.org/W3104490327', 'https://openalex.org/W2403642609']",2012-07-08
https://openalex.org/W2979454998,,The information bottleneck method,"We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y. That is, we squeeze the information that X provides about Y through a ‘bottleneck ’ formed by a limited set of codewords ˜X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, ˜x) emerges from the joint statistics of X and Y. This approach yields an exact set of self consistent equations for the coding rules X → ˜ X and ˜ X → Y. Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere. 1 1","['https://openalex.org/W2103723258', 'https://openalex.org/W2148986322', 'https://openalex.org/W2127314673', 'https://openalex.org/W2099111195', 'https://openalex.org/W3193262203']",2000-04-24
https://openalex.org/W2267126114,https://doi.org/10.48550/arxiv.1601.06759,Pixel Recurrent Neural Networks,"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.","['https://openalex.org/W2157002241', 'https://openalex.org/W2170942820', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963857374', 'https://openalex.org/W2097268041', 'https://openalex.org/W2952366348', 'https://openalex.org/W2962741254', 'https://openalex.org/W2962897886', 'https://openalex.org/W2064675550', 'https://openalex.org/W2083380015', 'https://openalex.org/W855255571', 'https://openalex.org/W189596042', 'https://openalex.org/W1810943226', 'https://openalex.org/W2170111110', 'https://openalex.org/W1906598733', 'https://openalex.org/W2148464528', 'https://openalex.org/W2112796928', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964084166', 'https://openalex.org/W1959608418', 'https://openalex.org/W2167348665', 'https://openalex.org/W2135181320', 'https://openalex.org/W1583912456', 'https://openalex.org/W196214544', 'https://openalex.org/W3118608800', 'https://openalex.org/W1753482797', 'https://openalex.org/W1866230956', 'https://openalex.org/W115742922', 'https://openalex.org/W2096192494', 'https://openalex.org/W1771459135', 'https://openalex.org/W2097039814', 'https://openalex.org/W2962736171']",2016-01-25
https://openalex.org/W2887927938,https://doi.org/10.48550/arxiv.1808.01048,Variational Information Bottleneck on Vector Quantized Autoencoders,"In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE can be derived from the variational deterministic information bottleneck (VDIB) principle. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as an approximation to the variational information bottleneck(VIB) principle.","['https://openalex.org/W1686946872', 'https://openalex.org/W2557579533', 'https://openalex.org/W2797548502', 'https://openalex.org/W1959608418', 'https://openalex.org/W2964184826']",2018-08-02
https://openalex.org/W1959608418,https://doi.org/10.48550/arxiv.1312.6114,Auto-Encoding Variational Bayes,"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.","['https://openalex.org/W2103633133', 'https://openalex.org/W2059448777', 'https://openalex.org/W2963173382', 'https://openalex.org/W2145094598', 'https://openalex.org/W3104819538', 'https://openalex.org/W2080829915', 'https://openalex.org/W2405601855', 'https://openalex.org/W2171490498', 'https://openalex.org/W2097268041', 'https://openalex.org/W2119196781', 'https://openalex.org/W177847060', 'https://openalex.org/W2951493172', 'https://openalex.org/W2166851633', 'https://openalex.org/W2135346645', 'https://openalex.org/W2163922914']",2013-12-20
https://openalex.org/W2293634267,https://doi.org/10.21437/interspeech.2014-80,Long short-term memory recurrent neural network architectures for large scale acoustic modeling,"Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.","['https://openalex.org/W2121029939', 'https://openalex.org/W2168231600', 'https://openalex.org/W2296748324', 'https://openalex.org/W2070725064', 'https://openalex.org/W2136848157', 'https://openalex.org/W2107878631', 'https://openalex.org/W2147768505', 'https://openalex.org/W1499864241', 'https://openalex.org/W811578723', 'https://openalex.org/W2159505618', 'https://openalex.org/W2158373110', 'https://openalex.org/W1993882792', 'https://openalex.org/W4285719527', 'https://openalex.org/W2079735306', 'https://openalex.org/W2143612262', 'https://openalex.org/W2133676762', 'https://openalex.org/W179875071', 'https://openalex.org/W2402268235', 'https://openalex.org/W2184045248', 'https://openalex.org/W2064675550', 'https://openalex.org/W2005708641', 'https://openalex.org/W2122585011']",2014-09-14
https://openalex.org/W2073459066,https://doi.org/10.5555/1283383.1283494,k-means++: the advantages of careful seeding,"The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.","['https://openalex.org/W2028761382', 'https://openalex.org/W1998325344', 'https://openalex.org/W2091283109', 'https://openalex.org/W2045964207', 'https://openalex.org/W2033387072', 'https://openalex.org/W1501500081', 'https://openalex.org/W1970866964', 'https://openalex.org/W1529871791', 'https://openalex.org/W2110105238', 'https://openalex.org/W2151242668', 'https://openalex.org/W1998739300', 'https://openalex.org/W2091684877', 'https://openalex.org/W2004791924', 'https://openalex.org/W1489597983', 'https://openalex.org/W2123297508', 'https://openalex.org/W2199495299', 'https://openalex.org/W2058295780', 'https://openalex.org/W1998905999', 'https://openalex.org/W2150593711', 'https://openalex.org/W2050761777', 'https://openalex.org/W2118858274']",2007-01-07
https://openalex.org/W2557579533,https://doi.org/10.48550/arxiv.1612.00410,Deep Variational Information Bottleneck,"We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method ""Deep Variational Information Bottleneck"", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.","['https://openalex.org/W2564029303', 'https://openalex.org/W2963105951', 'https://openalex.org/W2964184826', 'https://openalex.org/W2963467071', 'https://openalex.org/W2963955657', 'https://openalex.org/W2164411961', 'https://openalex.org/W2165629668', 'https://openalex.org/W2953047670', 'https://openalex.org/W1508769776', 'https://openalex.org/W2128957129', 'https://openalex.org/W2962730405', 'https://openalex.org/W2168614937', 'https://openalex.org/W2530846021', 'https://openalex.org/W2515654213', 'https://openalex.org/W2108598243', 'https://openalex.org/W2170503197', 'https://openalex.org/W3102616566', 'https://openalex.org/W115285041', 'https://openalex.org/W2086161653', 'https://openalex.org/W2753738274', 'https://openalex.org/W2962750142', 'https://openalex.org/W2180612164', 'https://openalex.org/W2230740169', 'https://openalex.org/W2516574342', 'https://openalex.org/W1959608418', 'https://openalex.org/W2243397390', 'https://openalex.org/W2123469175', 'https://openalex.org/W2964153729', 'https://openalex.org/W2274287116', 'https://openalex.org/W2683470288', 'https://openalex.org/W1932198206', 'https://openalex.org/W2247148075', 'https://openalex.org/W3140968660', 'https://openalex.org/W2963542245', 'https://openalex.org/W2979454998', 'https://openalex.org/W2963207607', 'https://openalex.org/W1533861849', 'https://openalex.org/W2964121744']",2016-12-01
https://openalex.org/W2953318193,,Pixel Recurrent Neural Networks,"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.","['https://openalex.org/W2949314557', 'https://openalex.org/W2096192494', 'https://openalex.org/W2951269467', 'https://openalex.org/W1810943226', 'https://openalex.org/W1850742715', 'https://openalex.org/W2135181320', 'https://openalex.org/W1753482797', 'https://openalex.org/W1771459135', 'https://openalex.org/W2170111110', 'https://openalex.org/W2157002241', 'https://openalex.org/W2129069237', 'https://openalex.org/W1583912456', 'https://openalex.org/W2953250761', 'https://openalex.org/W2083380015', 'https://openalex.org/W2112796928', 'https://openalex.org/W2952838738', 'https://openalex.org/W189596042', 'https://openalex.org/W1909320841', 'https://openalex.org/W2097268041', 'https://openalex.org/W2117539524', 'https://openalex.org/W2194775991', 'https://openalex.org/W2099057450', 'https://openalex.org/W2148464528', 'https://openalex.org/W115742922', 'https://openalex.org/W2951004968', 'https://openalex.org/W3118608800', 'https://openalex.org/W196214544', 'https://openalex.org/W1906598733', 'https://openalex.org/W2064675550']",2016-01-25
https://openalex.org/W2947590261,https://doi.org/10.48550/arxiv.1906.00446,Generating Diverse High-Fidelity Images with VQ-VAE-2,"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.","['https://openalex.org/W2962750131', 'https://openalex.org/W2945027741', 'https://openalex.org/W299440670', 'https://openalex.org/W2963373786', 'https://openalex.org/W2097039814', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962942158', 'https://openalex.org/W2135181320', 'https://openalex.org/W2963981733', 'https://openalex.org/W2962793481', 'https://openalex.org/W1909320841', 'https://openalex.org/W2409550820', 'https://openalex.org/W2267126114', 'https://openalex.org/W2523714292', 'https://openalex.org/W2904367110', 'https://openalex.org/W2897371726', 'https://openalex.org/W2893749619', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963292439', 'https://openalex.org/W2805984778', 'https://openalex.org/W2626778328', 'https://openalex.org/W2778792233', 'https://openalex.org/W2782980316', 'https://openalex.org/W1583912456', 'https://openalex.org/W2553897675', 'https://openalex.org/W2902630600', 'https://openalex.org/W2937274663', 'https://openalex.org/W2963799213', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963139417', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963082441', 'https://openalex.org/W2922386270', 'https://openalex.org/W2963857374', 'https://openalex.org/W2140196014']",2019-06-02
https://openalex.org/W1836465849,https://doi.org/10.48550/arxiv.1502.03167,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.","['https://openalex.org/W1563686443', 'https://openalex.org/W2084894614', 'https://openalex.org/W1915968771', 'https://openalex.org/W2034368206', 'https://openalex.org/W2146502635', 'https://openalex.org/W2914484425', 'https://openalex.org/W2123649031', 'https://openalex.org/W2127230474', 'https://openalex.org/W104184427', 'https://openalex.org/W1762484328', 'https://openalex.org/W1815076433', 'https://openalex.org/W1814328102', 'https://openalex.org/W2952020226', 'https://openalex.org/W2152424459', 'https://openalex.org/W2963504252', 'https://openalex.org/W1533861849', 'https://openalex.org/W2950179405', 'https://openalex.org/W2168231600', 'https://openalex.org/W1665214252', 'https://openalex.org/W2095705004', 'https://openalex.org/W1677182931', 'https://openalex.org/W2112796928', 'https://openalex.org/W2134583763']",2015-02-11
https://openalex.org/W4284888249,https://doi.org/10.21437/interspeech.2022-10988,Ultra-Low-Bitrate Speech Coding with Pretrained Transformers,"Speech coding facilitates the transmission of speech over lowbandwidth networks with minimal distortion.Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches.While this new generation of codecs is capable of synthesizing highfidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently.We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias.As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder.Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of 600 bps that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate.Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.","['https://openalex.org/W3036601975', 'https://openalex.org/W2129438793', 'https://openalex.org/W3160077247', 'https://openalex.org/W2970006822', 'https://openalex.org/W3097777922', 'https://openalex.org/W4226033575', 'https://openalex.org/W4205788663', 'https://openalex.org/W2131738223', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963799213', 'https://openalex.org/W2115098197', 'https://openalex.org/W2292235217', 'https://openalex.org/W4394668580', 'https://openalex.org/W3206495532', 'https://openalex.org/W4385245566', 'https://openalex.org/W3140429000', 'https://openalex.org/W2114925438', 'https://openalex.org/W3016098186', 'https://openalex.org/W2775336875', 'https://openalex.org/W4320013936', 'https://openalex.org/W3037038648', 'https://openalex.org/W1481955708', 'https://openalex.org/W2963208781', 'https://openalex.org/W3092028330', 'https://openalex.org/W1567186732', 'https://openalex.org/W2998572311', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963091184', 'https://openalex.org/W1996957914', 'https://openalex.org/W3007965881', 'https://openalex.org/W2108532241', 'https://openalex.org/W2935711438', 'https://openalex.org/W3093579165', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226380987']",2022-09-16
https://openalex.org/W3202278141,https://doi.org/10.1109/icassp43922.2022.9746395,Generalization Ability of MOS Prediction Networks,"Automatic methods to predict listener opinions of synthesized speech remain elusive since listeners, systems being evaluated, characteristics of the speech, and even the instructions given and the rating scale all vary from test to test. While automatic predictors for metrics such as mean opinion score (MOS) can achieve high prediction accuracy on samples from the same test, they typically fail to generalize well to new listening test contexts. In this paper, using a variety of networks for MOS prediction including MOSNet and self-supervised speech models such as wav2vec2, we investigate their performance on data from different listening tests in both zero-shot and fine-tuned settings. We find that wav2vec2 models fine-tuned for MOS prediction have good generalization capability to out-of-domain data even for the most challenging case of utterance-level predictions in the zero-shot setting, and that fine-tuning to in-domain data can improve predictions. We also observe that unseen systems are especially challenging for MOS prediction models.","['https://openalex.org/W3095410713', 'https://openalex.org/W6603931906', 'https://openalex.org/W2166637769', 'https://openalex.org/W4395699666', 'https://openalex.org/W4395699642', 'https://openalex.org/W4395958010', 'https://openalex.org/W4395958177', 'https://openalex.org/W2473388484', 'https://openalex.org/W2515028311', 'https://openalex.org/W2963035245', 'https://openalex.org/W3082130377', 'https://openalex.org/W3083776549', 'https://openalex.org/W2962780374', 'https://openalex.org/W2995181338', 'https://openalex.org/W3209059054', 'https://openalex.org/W1494198834', 'https://openalex.org/W6780218876', 'https://openalex.org/W3161558238', 'https://openalex.org/W6771467084', 'https://openalex.org/W3024752295', 'https://openalex.org/W6865107258', 'https://openalex.org/W3198270377', 'https://openalex.org/W2972394484', 'https://openalex.org/W3196225973', 'https://openalex.org/W3016160783', 'https://openalex.org/W2936802426', 'https://openalex.org/W3026777299', 'https://openalex.org/W2963522141', 'https://openalex.org/W6865530009', 'https://openalex.org/W3197580070', 'https://openalex.org/W6712208827', 'https://openalex.org/W2915960560', 'https://openalex.org/W3036601975', 'https://openalex.org/W2917688842', 'https://openalex.org/W3030437843', 'https://openalex.org/W2967606780', 'https://openalex.org/W3099782249', 'https://openalex.org/W2394921947', 'https://openalex.org/W97072897', 'https://openalex.org/W3169320628', 'https://openalex.org/W2917438849', 'https://openalex.org/W2796495654', 'https://openalex.org/W2917245127', 'https://openalex.org/W2119929864']",2022-04-27
https://openalex.org/W3097777922,https://doi.org/10.21437/interspeech.2020-3015,Conformer: Convolution-augmented Transformer for Speech Recognition,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.","['https://openalex.org/W1964175594', 'https://openalex.org/W2952809536', 'https://openalex.org/W2963542740', 'https://openalex.org/W3015537910', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015194534', 'https://openalex.org/W4287812455', 'https://openalex.org/W2112739286', 'https://openalex.org/W4385245566', 'https://openalex.org/W4309845474', 'https://openalex.org/W3016010032', 'https://openalex.org/W2962760690', 'https://openalex.org/W4295253143', 'https://openalex.org/W1828163288', 'https://openalex.org/W2095705004', 'https://openalex.org/W2948981900', 'https://openalex.org/W2963414781', 'https://openalex.org/W2981413347', 'https://openalex.org/W2892009249', 'https://openalex.org/W3015995734', 'https://openalex.org/W1522301498', 'https://openalex.org/W2899423466', 'https://openalex.org/W2936774411', 'https://openalex.org/W4320930577', 'https://openalex.org/W3095173472', 'https://openalex.org/W2928941594', 'https://openalex.org/W2567070169', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973215447', 'https://openalex.org/W1995562189', 'https://openalex.org/W2981857663', 'https://openalex.org/W2979636403', 'https://openalex.org/W2964110616']",2020-10-25
https://openalex.org/W1481955708,https://doi.org/10.1109/icassp.2015.7179063,Overview of the EVS codec architecture,S.5698-5702,"['https://openalex.org/W1519690927', 'https://openalex.org/W6631472606', 'https://openalex.org/W1594363840', 'https://openalex.org/W1548002231', 'https://openalex.org/W1600971048', 'https://openalex.org/W1518739398', 'https://openalex.org/W1600887652', 'https://openalex.org/W1502717739', 'https://openalex.org/W1608376827', 'https://openalex.org/W1566451502', 'https://openalex.org/W1577541836', 'https://openalex.org/W1532961188', 'https://openalex.org/W1518181241', 'https://openalex.org/W6635645706', 'https://openalex.org/W2165291881', 'https://openalex.org/W2108179702', 'https://openalex.org/W2151549934', 'https://openalex.org/W6631257896', 'https://openalex.org/W2151825755', 'https://openalex.org/W1596775060', 'https://openalex.org/W1590793133', 'https://openalex.org/W1523644733']",2015-04-01
https://openalex.org/W1597121597,https://doi.org/10.1108/rr-08-2013-0197,LibriVox: Free Public Domain Audiobooks,,[],2014-01-14
https://openalex.org/W3037038648,https://doi.org/10.1109/qomex48832.2020.9123150,ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric,"The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020","['https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W2963300588', 'https://openalex.org/W1976188834', 'https://openalex.org/W2922332774', 'https://openalex.org/W2111964411', 'https://openalex.org/W6638872167', 'https://openalex.org/W2469918093', 'https://openalex.org/W2623662528', 'https://openalex.org/W2972359262', 'https://openalex.org/W2153635508', 'https://openalex.org/W2108708552', 'https://openalex.org/W6636045042', 'https://openalex.org/W1496883935', 'https://openalex.org/W6636170946', 'https://openalex.org/W6754761153', 'https://openalex.org/W1552314771', 'https://openalex.org/W2040151471', 'https://openalex.org/W2990738305', 'https://openalex.org/W1606487971', 'https://openalex.org/W1849775568', 'https://openalex.org/W1607435270', 'https://openalex.org/W2892330131', 'https://openalex.org/W2103934944', 'https://openalex.org/W2987307811']",2020-05-01
https://openalex.org/W2972354707,https://doi.org/10.21437/interspeech.2019-1816,Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding,"Speech codecs learn compact representations of speech signals to facilitate data transmission.Many recent deep neural network (DNN) based end-to-end speech codecs achieve low bitrates and high perceptual quality at the cost of model complexity.We propose a cross-module residual learning (CMRL) pipeline as a module carrier with each module reconstructing the residual from its preceding modules.CMRL differs from other DNN-based speech codecs, in that rather than modeling speech compression problem in a single large neural network, it optimizes a series of less-complicated modules in a two-phase training scheme.The proposed method shows better objective performance than AMR-WB and the state-of-the-art DNNbased speech codec with a similar network architecture.As an end-to-end model, it takes raw PCM signals as an input, but is also compatible with linear predictive coding (LPC), showing better subjective quality at high bitrates than AMR-WB and OPUS.The gain is achieved by using only 0.9 million trainable parameters, a significantly less complex architecture than the other DNN-based codecs in the literature.","['https://openalex.org/W2775336875', 'https://openalex.org/W1556611829', 'https://openalex.org/W4297752165', 'https://openalex.org/W1703292843', 'https://openalex.org/W2309400744', 'https://openalex.org/W2889329491', 'https://openalex.org/W2584032004', 'https://openalex.org/W2194775991', 'https://openalex.org/W2013078344', 'https://openalex.org/W2891355459', 'https://openalex.org/W2020883660', 'https://openalex.org/W2151626637', 'https://openalex.org/W2002182716', 'https://openalex.org/W2165291881', 'https://openalex.org/W2168013545', 'https://openalex.org/W1552314771', 'https://openalex.org/W1522301498', 'https://openalex.org/W1512294521', 'https://openalex.org/W2060108852', 'https://openalex.org/W2597747080', 'https://openalex.org/W2935711438', 'https://openalex.org/W2037034710', 'https://openalex.org/W2266701264', 'https://openalex.org/W2129652681', 'https://openalex.org/W2732044853', 'https://openalex.org/W2963182577', 'https://openalex.org/W1553834069', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963208781', 'https://openalex.org/W2476548250']",2019-09-13
https://openalex.org/W3186609711,https://doi.org/10.1109/waspaa52581.2021.9632723,Harp-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding,"We propose a novel autoencoder architecture that improves the architectural scalability of general-purpose neural audio coding models. An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.","['https://openalex.org/W2963091184', 'https://openalex.org/W2924551963', 'https://openalex.org/W3110277971', 'https://openalex.org/W6764117752', 'https://openalex.org/W6639824700', 'https://openalex.org/W6751512325', 'https://openalex.org/W2100495367', 'https://openalex.org/W6741057705', 'https://openalex.org/W6746914816', 'https://openalex.org/W2476548250', 'https://openalex.org/W6639363673', 'https://openalex.org/W2165291881', 'https://openalex.org/W6773743766', 'https://openalex.org/W2963182577', 'https://openalex.org/W2775336875', 'https://openalex.org/W2935711438', 'https://openalex.org/W2105921478', 'https://openalex.org/W2732044853', 'https://openalex.org/W3015268401', 'https://openalex.org/W2963452667', 'https://openalex.org/W4205788663', 'https://openalex.org/W2949382160', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964164354', 'https://openalex.org/W1901129140', 'https://openalex.org/W2972354707', 'https://openalex.org/W2972519044', 'https://openalex.org/W1885680957', 'https://openalex.org/W2774707525']",2021-10-17
https://openalex.org/W2129913307,https://doi.org/10.1109/icc.1990.117117,Speech coding based on a multi-layer neural network,"The authors present a speech-compression scheme based on a three-layer perceptron in which the number of units in the hidden layer is reduced. Input and output layers have the same number of units in order to achieve identity mapping. Speech coding is realized by scalar or vector quantization of hidden-layer outputs. By analyzing the weighting coefficients, it can be shown that speech coding based on a three-layer neural network is speaker-independent. Transform coding is automatically based on back propagation. The relation between compression ratio and SNR (signal-to-noise ratio) is investigated. The bit allocation and optimum number of hidden-layer units necessary to realize a specific bit rate are given. According to the analysis of weighting coefficients, speech coding based on a neural network is transform coding similar to Karhunen-Loeve transformation. The characteristics of a five-layer neural network are examined. It is shown that since the five-layer neural network can realize nonlinear mapping, it is more effective than the three-layer network.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W4253020087', 'https://openalex.org/W2046432185']",1990-01-01
https://openalex.org/W2935711438,https://doi.org/10.1109/icassp.2019.8683277,Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder,"In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.","['https://openalex.org/W2165291881', 'https://openalex.org/W2752796333', 'https://openalex.org/W6640963894', 'https://openalex.org/W1494198834', 'https://openalex.org/W6696768431', 'https://openalex.org/W2108532241', 'https://openalex.org/W6752888775', 'https://openalex.org/W6738494155', 'https://openalex.org/W6735849998', 'https://openalex.org/W2963182577', 'https://openalex.org/W2309400744', 'https://openalex.org/W2632564668', 'https://openalex.org/W2552465432', 'https://openalex.org/W6734035190', 'https://openalex.org/W2775336875', 'https://openalex.org/W6755135894', 'https://openalex.org/W2808706139', 'https://openalex.org/W2950237263', 'https://openalex.org/W2963799213', 'https://openalex.org/W1959608418', 'https://openalex.org/W2616159609', 'https://openalex.org/W4294567867', 'https://openalex.org/W2604231067', 'https://openalex.org/W2892620417', 'https://openalex.org/W2951004968', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963449488', 'https://openalex.org/W2962676454', 'https://openalex.org/W3124456579', 'https://openalex.org/W2292235217']",2019-04-16
https://openalex.org/W4385245566,https://doi.org/10.4230/lipics.itp.2023.19,MizAR 60 for Mizar 50,"As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",[],2023-01-01
https://openalex.org/W2998572311,https://doi.org/10.7488/ds/2645,CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",[],2019-01-01
https://openalex.org/W2516890051,https://doi.org/10.21437/interspeech.2016-811,Joint Learning of Speaker and Phonetic Similarities with Siamese Networks,,"['https://openalex.org/W2052697931', 'https://openalex.org/W1532499126', 'https://openalex.org/W2127589108', 'https://openalex.org/W2088247287', 'https://openalex.org/W1494198834', 'https://openalex.org/W2400549570', 'https://openalex.org/W2963620343', 'https://openalex.org/W2183016404', 'https://openalex.org/W2395899413', 'https://openalex.org/W2128160875']",2016-08-28
https://openalex.org/W2780786457,https://doi.org/10.1587/transinf.2017edp7175,Learning Supervised Feature Transformations on Zero Resources for Improved Acoustic Unit Discovery,"In this work we utilize feature transformations that are common in supervised learning without having prior supervision, with the goal to improve Dirichlet process Gaussian mixture model (DPGMM) based acoustic unit discovery. The motivation of using such transformations is to create feature vectors that are more suitable for clustering. The need of labels for these methods makes it difficult to use them in a zero resource setting. To overcome this issue we utilize a first iteration of DPGMM clustering to generate frame based class labels for the target data. The labels serve as basis for learning linear discriminant analysis (LDA), maximum likelihood linear transform (MLLT) and feature-space maximum likelihood linear regression (fMLLR) based feature transformations. The novelty of our approach is the way how we use a traditional acoustic model training pipeline for supervised learning to estimate feature transformations in a zero resource scenario. We show that the learned transformations greatly support the DPGMM sampler in finding better clusters, according to the performance of the DPGMM posteriorgrams on the ABX sound class discriminability task. We also introduce a method for combining posteriorgram outputs of multiple clusterings and demonstrate that such combinations can further improve sound class discriminability.","['https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W2119187236', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396043527', 'https://openalex.org/W2404799143', 'https://openalex.org/W1997505733', 'https://openalex.org/W2399576818', 'https://openalex.org/W2001619934', 'https://openalex.org/W2124629003', 'https://openalex.org/W2106554350', 'https://openalex.org/W4302557958', 'https://openalex.org/W2002342963', 'https://openalex.org/W2064210461', 'https://openalex.org/W2402014506', 'https://openalex.org/W2128032727', 'https://openalex.org/W2145410271', 'https://openalex.org/W2044138293', 'https://openalex.org/W2395899413', 'https://openalex.org/W2294798173', 'https://openalex.org/W2071128523', 'https://openalex.org/W2963620343', 'https://openalex.org/W1524333225', 'https://openalex.org/W66167291']",2017-12-31
https://openalex.org/W2963112338,,Mixed Precision Training,"Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.",[],2017-10-10
https://openalex.org/W2010188467,https://doi.org/10.1016/j.specom.2012.05.001,Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions,,"['https://openalex.org/W2603386934', 'https://openalex.org/W1992613273', 'https://openalex.org/W2069732686', 'https://openalex.org/W1602670218', 'https://openalex.org/W2098363562', 'https://openalex.org/W2621111985', 'https://openalex.org/W2051882640', 'https://openalex.org/W2117325844', 'https://openalex.org/W2074546930', 'https://openalex.org/W2029948425', 'https://openalex.org/W2071402670', 'https://openalex.org/W6765586083', 'https://openalex.org/W2512925645', 'https://openalex.org/W1976234022', 'https://openalex.org/W2166391802', 'https://openalex.org/W1965307478', 'https://openalex.org/W6678256346', 'https://openalex.org/W153534061', 'https://openalex.org/W2000812031', 'https://openalex.org/W2157982733', 'https://openalex.org/W2084734691', 'https://openalex.org/W2095458199', 'https://openalex.org/W1779834323', 'https://openalex.org/W2049633694', 'https://openalex.org/W1555009096', 'https://openalex.org/W2400864884', 'https://openalex.org/W2106112361', 'https://openalex.org/W2011238950', 'https://openalex.org/W2110485445', 'https://openalex.org/W6610251635', 'https://openalex.org/W2174992124', 'https://openalex.org/W2083904075', 'https://openalex.org/W52412328', 'https://openalex.org/W2146163948', 'https://openalex.org/W6681871837', 'https://openalex.org/W2052262800', 'https://openalex.org/W2129705892', 'https://openalex.org/W2002002768', 'https://openalex.org/W2070157293', 'https://openalex.org/W2096563020', 'https://openalex.org/W2096128727', 'https://openalex.org/W2070696251', 'https://openalex.org/W2020344439', 'https://openalex.org/W2106876504', 'https://openalex.org/W2039208728', 'https://openalex.org/W2129177073', 'https://openalex.org/W7056100395', 'https://openalex.org/W135984148', 'https://openalex.org/W2070862086', 'https://openalex.org/W1584739173', 'https://openalex.org/W2078845431', 'https://openalex.org/W1761091820', 'https://openalex.org/W1980096805', 'https://openalex.org/W1990517717', 'https://openalex.org/W2050824494', 'https://openalex.org/W281094599', 'https://openalex.org/W2410344739', 'https://openalex.org/W2020944885', 'https://openalex.org/W1984717236', 'https://openalex.org/W2063525438', 'https://openalex.org/W2103091632', 'https://openalex.org/W2110614779', 'https://openalex.org/W1902027874', 'https://openalex.org/W2109018653', 'https://openalex.org/W2150423550', 'https://openalex.org/W6678914141', 'https://openalex.org/W1995991622', 'https://openalex.org/W6605048925', 'https://openalex.org/W316006058', 'https://openalex.org/W2104752510', 'https://openalex.org/W2135704565', 'https://openalex.org/W1548450879', 'https://openalex.org/W2172262067', 'https://openalex.org/W2169991335', 'https://openalex.org/W6638997730', 'https://openalex.org/W2110221456', 'https://openalex.org/W1992927229', 'https://openalex.org/W2965857144', 'https://openalex.org/W2089458547', 'https://openalex.org/W2135280482', 'https://openalex.org/W2156899532', 'https://openalex.org/W6674546347', 'https://openalex.org/W2152518525', 'https://openalex.org/W2160119185', 'https://openalex.org/W2118841860', 'https://openalex.org/W2140277151', 'https://openalex.org/W2056133372', 'https://openalex.org/W2013588070', 'https://openalex.org/W1991274470', 'https://openalex.org/W2164269361', 'https://openalex.org/W2167245505', 'https://openalex.org/W2064090909', 'https://openalex.org/W2405649931', 'https://openalex.org/W124411215', 'https://openalex.org/W1558402681', 'https://openalex.org/W2132921748', 'https://openalex.org/W2159265825', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W2132396358', 'https://openalex.org/W1589615572', 'https://openalex.org/W2059168261', 'https://openalex.org/W2165159843', 'https://openalex.org/W1977811986', 'https://openalex.org/W1628323995', 'https://openalex.org/W2018139470', 'https://openalex.org/W2053000838', 'https://openalex.org/W2036964623', 'https://openalex.org/W2136549906', 'https://openalex.org/W1606268232', 'https://openalex.org/W2123543395', 'https://openalex.org/W1575001262', 'https://openalex.org/W1548542558', 'https://openalex.org/W1994146479', 'https://openalex.org/W171897918', 'https://openalex.org/W2142111485', 'https://openalex.org/W1991083510', 'https://openalex.org/W2169711598', 'https://openalex.org/W6629667201', 'https://openalex.org/W2069481850', 'https://openalex.org/W2105000456', 'https://openalex.org/W2032476212', 'https://openalex.org/W2108582985', 'https://openalex.org/W2119876241', 'https://openalex.org/W2153767712', 'https://openalex.org/W189630165', 'https://openalex.org/W2161952424', 'https://openalex.org/W1496597695', 'https://openalex.org/W2114777034', 'https://openalex.org/W2045419663', 'https://openalex.org/W2170062498', 'https://openalex.org/W2089883580', 'https://openalex.org/W2101509422', 'https://openalex.org/W2120717621', 'https://openalex.org/W2040300040', 'https://openalex.org/W4285719527', 'https://openalex.org/W2952343510', 'https://openalex.org/W1490760466', 'https://openalex.org/W1980491396', 'https://openalex.org/W2962598389', 'https://openalex.org/W2048239613', 'https://openalex.org/W4246559809', 'https://openalex.org/W123135133', 'https://openalex.org/W2006725023', 'https://openalex.org/W125225111', 'https://openalex.org/W2097863906', 'https://openalex.org/W2151484683', 'https://openalex.org/W2123333253', 'https://openalex.org/W1494996288', 'https://openalex.org/W2147682057', 'https://openalex.org/W2014013793', 'https://openalex.org/W1583743095', 'https://openalex.org/W2980877534', 'https://openalex.org/W2117331204', 'https://openalex.org/W2728435982', 'https://openalex.org/W288075909', 'https://openalex.org/W1854837444', 'https://openalex.org/W2939803422', 'https://openalex.org/W2127218421']",2012-05-26
https://openalex.org/W2789543585,https://doi.org/10.48550/arxiv.1803.03382,Fast Decoding in Sequence Models using Discrete Latent Variables,"Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.","['https://openalex.org/W1863218161', 'https://openalex.org/W2602076750', 'https://openalex.org/W2952165242', 'https://openalex.org/W2547875792', 'https://openalex.org/W2157331557', 'https://openalex.org/W382779172', 'https://openalex.org/W2613904329', 'https://openalex.org/W2545625743', 'https://openalex.org/W2951004968', 'https://openalex.org/W2594538354', 'https://openalex.org/W1652807540', 'https://openalex.org/W2064675550', 'https://openalex.org/W1753482797', 'https://openalex.org/W2953333557', 'https://openalex.org/W2913932916', 'https://openalex.org/W2278112683', 'https://openalex.org/W2100495367', 'https://openalex.org/W2119717200', 'https://openalex.org/W2145094598', 'https://openalex.org/W2626778328', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963069010', 'https://openalex.org/W2962997665', 'https://openalex.org/W1779483307', 'https://openalex.org/W2767002724', 'https://openalex.org/W2789649201', 'https://openalex.org/W2963187627', 'https://openalex.org/W189596042', 'https://openalex.org/W2962790997', 'https://openalex.org/W2785779000', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963045354', 'https://openalex.org/W2952264928', 'https://openalex.org/W2540404261']",2018-03-09
https://openalex.org/W1778492285,https://doi.org/10.1162/tacl_a_00146,Unsupervised Lexicon Discovery from Acoustic Input,"We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.","['https://openalex.org/W1964917299', 'https://openalex.org/W2074546930', 'https://openalex.org/W2162505970', 'https://openalex.org/W2108207895', 'https://openalex.org/W2148154194', 'https://openalex.org/W2069429561', 'https://openalex.org/W2130518563', 'https://openalex.org/W2111732304', 'https://openalex.org/W2101711363', 'https://openalex.org/W2042143122', 'https://openalex.org/W2126377586', 'https://openalex.org/W4251556668', 'https://openalex.org/W1990005915', 'https://openalex.org/W2142390309', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W2026858810', 'https://openalex.org/W1969608442', 'https://openalex.org/W2030118973', 'https://openalex.org/W4301623764', 'https://openalex.org/W2111668269', 'https://openalex.org/W2154093685', 'https://openalex.org/W2164151151', 'https://openalex.org/W2126203737', 'https://openalex.org/W1992613273', 'https://openalex.org/W2087309226', 'https://openalex.org/W2114347655', 'https://openalex.org/W2107038463', 'https://openalex.org/W1548450879', 'https://openalex.org/W3036063182', 'https://openalex.org/W2408712009', 'https://openalex.org/W2083195487', 'https://openalex.org/W2140991203', 'https://openalex.org/W2022201897', 'https://openalex.org/W2025482506', 'https://openalex.org/W2053306448', 'https://openalex.org/W2126953647', 'https://openalex.org/W2952343510', 'https://openalex.org/W2116422968', 'https://openalex.org/W2170353620', 'https://openalex.org/W4237961478', 'https://openalex.org/W2100768664', 'https://openalex.org/W2070554026', 'https://openalex.org/W2033413759', 'https://openalex.org/W2117126688', 'https://openalex.org/W2086891622', 'https://openalex.org/W2130042265', 'https://openalex.org/W2188924248', 'https://openalex.org/W1650210997', 'https://openalex.org/W30845872', 'https://openalex.org/W2226889031', 'https://openalex.org/W1779834323', 'https://openalex.org/W2126449874', 'https://openalex.org/W2163245285', 'https://openalex.org/W1525748279', 'https://openalex.org/W2157381218', 'https://openalex.org/W1530250655', 'https://openalex.org/W1843673427', 'https://openalex.org/W1583697620', 'https://openalex.org/W1978470410']",2015-12-01
https://openalex.org/W2911249026,https://doi.org/10.32470/ccn.2018.1240-0,Neural network vs. HMM speech recognition systems as models of human cross-linguistic phonetic perception,,"['https://openalex.org/W1524333225', 'https://openalex.org/W2145339207', 'https://openalex.org/W2024490156', 'https://openalex.org/W1994492508', 'https://openalex.org/W2145410271', 'https://openalex.org/W2062956221', 'https://openalex.org/W2533523411', 'https://openalex.org/W2040913319', 'https://openalex.org/W3034729383', 'https://openalex.org/W2395899413', 'https://openalex.org/W2058616551', 'https://openalex.org/W66627554', 'https://openalex.org/W2026408911', 'https://openalex.org/W1677182931']",2018-01-01
https://openalex.org/W2949382160,,WaveNet: A Generative Model for Raw Audio,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.","['https://openalex.org/W2963175699', 'https://openalex.org/W181056519', 'https://openalex.org/W1610060839', 'https://openalex.org/W2139576349', 'https://openalex.org/W2507771204', 'https://openalex.org/W2964301388', 'https://openalex.org/W2408093180', 'https://openalex.org/W2293049663', 'https://openalex.org/W1923697677', 'https://openalex.org/W1979449467', 'https://openalex.org/W1527535554', 'https://openalex.org/W2259472270', 'https://openalex.org/W1542280630', 'https://openalex.org/W2953037075', 'https://openalex.org/W3035139526', 'https://openalex.org/W2064675550', 'https://openalex.org/W2112197391', 'https://openalex.org/W2150658333', 'https://openalex.org/W2133035145', 'https://openalex.org/W2395849284', 'https://openalex.org/W2471520273', 'https://openalex.org/W2000513720', 'https://openalex.org/W2063105057', 'https://openalex.org/W1579853615', 'https://openalex.org/W1560013842', 'https://openalex.org/W2285182995', 'https://openalex.org/W2423557781', 'https://openalex.org/W2428180336', 'https://openalex.org/W2953318193', 'https://openalex.org/W2294797155', 'https://openalex.org/W2338186431', 'https://openalex.org/W1877553482', 'https://openalex.org/W2949650786', 'https://openalex.org/W2527729766', 'https://openalex.org/W2286929393', 'https://openalex.org/W2067329295', 'https://openalex.org/W2102003408', 'https://openalex.org/W2953250761', 'https://openalex.org/W2049686551', 'https://openalex.org/W2337335398', 'https://openalex.org/W2798983173', 'https://openalex.org/W1594494252', 'https://openalex.org/W3142087749', 'https://openalex.org/W233059834', 'https://openalex.org/W2395578248']",2016-09-12
https://openalex.org/W2890983311,https://doi.org/10.1109/icassp.2018.8462431,Fftnet: A Real-Time Speaker-Dependent Neural Vocoder,"We introduce FFTNet, a deep learning approach synthesizing audio waveforms. Our approach builds on the recent WaveNet project, which showed that it was possible to synthesize a natural sounding audio waveform directly from a deep convolutional neural network. FFTNet offers two improvements over WaveNet. First it is substantially faster, allowing for real-time synthesis of audio waveforms. Second, when used as a vocoder, the resulting speech sounds more natural, as measured via a ""mean opinion score"" test.","['https://openalex.org/W2328165521', 'https://openalex.org/W6792025129', 'https://openalex.org/W2145892079', 'https://openalex.org/W2769810959', 'https://openalex.org/W6950332740', 'https://openalex.org/W2061171222', 'https://openalex.org/W2423557781', 'https://openalex.org/W4253928870', 'https://openalex.org/W6756197946', 'https://openalex.org/W2251498782', 'https://openalex.org/W6736356763', 'https://openalex.org/W2746474733', 'https://openalex.org/W6740049204', 'https://openalex.org/W1510007267', 'https://openalex.org/W2749651610', 'https://openalex.org/W6603838645', 'https://openalex.org/W2141708418', 'https://openalex.org/W2737697117', 'https://openalex.org/W4230964063', 'https://openalex.org/W2395718496', 'https://openalex.org/W6712900185', 'https://openalex.org/W2604184139', 'https://openalex.org/W573259127', 'https://openalex.org/W2901997113', 'https://openalex.org/W1495679096', 'https://openalex.org/W2402356521', 'https://openalex.org/W95152782', 'https://openalex.org/W3142087749', 'https://openalex.org/W2606176153', 'https://openalex.org/W4298642009', 'https://openalex.org/W1522301498', 'https://openalex.org/W2591927543', 'https://openalex.org/W4294619240', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963103134']",2018-04-01
https://openalex.org/W2954386831,https://doi.org/10.21437/interspeech.2019-1424,Towards Achieving Robust Universal Neural Vocoding,"This paper explores the potential universality of neural vocoders.We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages.This vocoder is shown to be capable of generating speech of consistently good quality (98% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality.When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75%.These results are shown to be consistent across languages, regardless of them being seen during training (e.g.English or Japanese) or unseen (e.g.","['https://openalex.org/W2888169323', 'https://openalex.org/W2519091744', 'https://openalex.org/W2409959948', 'https://openalex.org/W2963975282', 'https://openalex.org/W2889064624', 'https://openalex.org/W2963300588', 'https://openalex.org/W2395578248', 'https://openalex.org/W30609886', 'https://openalex.org/W1997402792', 'https://openalex.org/W2066999516', 'https://openalex.org/W2916985722', 'https://openalex.org/W1544516254', 'https://openalex.org/W2401318443', 'https://openalex.org/W4294619240', 'https://openalex.org/W2786868129', 'https://openalex.org/W2471520273', 'https://openalex.org/W2572097499', 'https://openalex.org/W2963411216', 'https://openalex.org/W2120634960', 'https://openalex.org/W2963654953', 'https://openalex.org/W2889329491', 'https://openalex.org/W2964243274', 'https://openalex.org/W2120847449', 'https://openalex.org/W2757519008', 'https://openalex.org/W2808706139', 'https://openalex.org/W2889092828', 'https://openalex.org/W2191779130', 'https://openalex.org/W2400063444', 'https://openalex.org/W169745891', 'https://openalex.org/W2762101354', 'https://openalex.org/W4298580827', 'https://openalex.org/W2890983311', 'https://openalex.org/W2963522141']",2019-09-13
https://openalex.org/W2242818861,https://doi.org/10.48550/arxiv.1308.3432,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,"Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we ""back-propagate"" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.","['https://openalex.org/W1498436455', 'https://openalex.org/W1904365287', 'https://openalex.org/W2951163624', 'https://openalex.org/W2913932916', 'https://openalex.org/W2124289529', 'https://openalex.org/W3037950864', 'https://openalex.org/W2025768430', 'https://openalex.org/W2156718681', 'https://openalex.org/W2099257174', 'https://openalex.org/W60493759', 'https://openalex.org/W2041176801']",2013-08-15
https://openalex.org/W2971775690,https://doi.org/10.1126/sciadv.aaw2594,"Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche",Human languages encode similar average information rates (~39 bits/s) despite their remarkable differences.,"['https://openalex.org/W1903378096', 'https://openalex.org/W1916508083', 'https://openalex.org/W2000196122', 'https://openalex.org/W2793163934', 'https://openalex.org/W2498513106', 'https://openalex.org/W2129955048', 'https://openalex.org/W2140188190', 'https://openalex.org/W2186124548', 'https://openalex.org/W2026356512', 'https://openalex.org/W2113413162', 'https://openalex.org/W2157736532', 'https://openalex.org/W2121464381', 'https://openalex.org/W2037534576', 'https://openalex.org/W2056486423', 'https://openalex.org/W2119427866', 'https://openalex.org/W2789751179', 'https://openalex.org/W2001902690', 'https://openalex.org/W2756649375', 'https://openalex.org/W1985486109', 'https://openalex.org/W4241468802', 'https://openalex.org/W2801080079', 'https://openalex.org/W2140300418', 'https://openalex.org/W1969218003', 'https://openalex.org/W2567079951', 'https://openalex.org/W1483830027', 'https://openalex.org/W2305947592', 'https://openalex.org/W2074712099', 'https://openalex.org/W2584469054', 'https://openalex.org/W2621665188', 'https://openalex.org/W2002286769', 'https://openalex.org/W2002945352', 'https://openalex.org/W2326736831', 'https://openalex.org/W2055249791', 'https://openalex.org/W2079659544', 'https://openalex.org/W2560001118', 'https://openalex.org/W2092203759', 'https://openalex.org/W2174700846', 'https://openalex.org/W2337026175', 'https://openalex.org/W2557345304', 'https://openalex.org/W2476160527', 'https://openalex.org/W2096526749', 'https://openalex.org/W2079696017', 'https://openalex.org/W1497663926', 'https://openalex.org/W1981457167', 'https://openalex.org/W6686294705', 'https://openalex.org/W1997817740', 'https://openalex.org/W2013452503', 'https://openalex.org/W2805866554', 'https://openalex.org/W2049515993', 'https://openalex.org/W2477963042', 'https://openalex.org/W2396545434', 'https://openalex.org/W2073441004', 'https://openalex.org/W1593509093', 'https://openalex.org/W2252252584', 'https://openalex.org/W2070528687', 'https://openalex.org/W2797472986', 'https://openalex.org/W4299517958', 'https://openalex.org/W2185736755', 'https://openalex.org/W2206193224', 'https://openalex.org/W2865837075']",2019-09-04
https://openalex.org/W3018535504,https://doi.org/10.48550/arxiv.2004.10120,Vector Quantized Contrastive Predictive Coding for Template-based Music Generation,"In this work, we propose a flexible method for generating variations of discrete sequences in which tokens can be grouped into basic units, like sentences in a text or bars in music. More precisely, given a template sequence, we aim at producing novel sequences sharing perceptible similarities with the original template without relying on any annotation; so our problem of generating variations is intimately linked to the problem of learning relevant high-level representations without supervision. Our contribution is two-fold: First, we propose a self-supervised encoding technique, named Vector Quantized Contrastive Predictive Coding which allows to learn a meaningful assignment of the basic units over a discrete set of codes, together with mechanisms allowing to control the information content of these learnt discrete representations. Secondly, we show how these compressed representations can be used to generate variations of a template sequence by using an appropriate attention pattern in the Transformer architecture. We illustrate our approach on the corpus of J.S. Bach chorales where we discuss the musical meaning of the learnt discrete codes and show that our proposed method allows to generate coherent and high-quality variations of a given template.","['https://openalex.org/W2971524460', 'https://openalex.org/W3098518646', 'https://openalex.org/W2947590261', 'https://openalex.org/W2938704169', 'https://openalex.org/W2970401203', 'https://openalex.org/W3000514857', 'https://openalex.org/W2124509324', 'https://openalex.org/W2982753834', 'https://openalex.org/W3032697904', 'https://openalex.org/W2778792233', 'https://openalex.org/W2991421901', 'https://openalex.org/W2789541106', 'https://openalex.org/W2964016415', 'https://openalex.org/W2152790380', 'https://openalex.org/W2995416527', 'https://openalex.org/W2944828972', 'https://openalex.org/W2940744433', 'https://openalex.org/W2946567085', 'https://openalex.org/W3005680577', 'https://openalex.org/W2591710685', 'https://openalex.org/W2979476256', 'https://openalex.org/W2911109671', 'https://openalex.org/W2769811909', 'https://openalex.org/W2963350250', 'https://openalex.org/W2901638613', 'https://openalex.org/W2963253162', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964122153', 'https://openalex.org/W2962212541', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963575853', 'https://openalex.org/W2959020461', 'https://openalex.org/W2963799213', 'https://openalex.org/W2343635552', 'https://openalex.org/W2919624000', 'https://openalex.org/W2792210438', 'https://openalex.org/W2922386270']",2020-04-21
https://openalex.org/W2101105183,https://doi.org/10.3115/1073083.1073135,BLEU,"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.","['https://openalex.org/W3037252522', 'https://openalex.org/W2001810881', 'https://openalex.org/W2732923061']",2001-01-01
https://openalex.org/W2963909453,,,"We introduce the Multi30K dataset to stimulate multilingual multimodal research.Recent advances in image description have been demonstrated on Englishlanguage datasets almost exclusively, but image description should not be limited to English.This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) German descriptions crowdsourced independently of the original English descriptions.We describe the data and outline how it can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks.","['https://openalex.org/W2119775030', 'https://openalex.org/W4241645538', 'https://openalex.org/W114341944', 'https://openalex.org/W2964308564', 'https://openalex.org/W2152263452', 'https://openalex.org/W1895577753', 'https://openalex.org/W1514535095', 'https://openalex.org/W2252200119', 'https://openalex.org/W1889081078', 'https://openalex.org/W2735359189', 'https://openalex.org/W2143449221', 'https://openalex.org/W2950178297', 'https://openalex.org/W3005285208', 'https://openalex.org/W2509282593', 'https://openalex.org/W68733909', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2293344577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2247931231', 'https://openalex.org/W2282219577', 'https://openalex.org/W2185175083']",
https://openalex.org/W2191779130,https://doi.org/10.25080/majora-7b98e3ed-003,librosa: Audio and Music Signal Analysis in Python,"This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.","['https://openalex.org/W6675354045', 'https://openalex.org/W6691839725', 'https://openalex.org/W2146292423', 'https://openalex.org/W1916910924', 'https://openalex.org/W2103869314', 'https://openalex.org/W6603616073', 'https://openalex.org/W1975929202', 'https://openalex.org/W2125324924', 'https://openalex.org/W129413713', 'https://openalex.org/W2915624731', 'https://openalex.org/W2397818963', 'https://openalex.org/W78743328', 'https://openalex.org/W1902027874', 'https://openalex.org/W1561135842', 'https://openalex.org/W2016885049', 'https://openalex.org/W6697202309', 'https://openalex.org/W2385545', 'https://openalex.org/W2285540944', 'https://openalex.org/W2053347927', 'https://openalex.org/W2294459443', 'https://openalex.org/W2052872069', 'https://openalex.org/W2101234009', 'https://openalex.org/W4232336823', 'https://openalex.org/W4245436919', 'https://openalex.org/W2254715784', 'https://openalex.org/W2407685581', 'https://openalex.org/W3005347330', 'https://openalex.org/W2011301426', 'https://openalex.org/W2016381774']",2015-01-01
https://openalex.org/W2327501763,https://doi.org/10.1109/icassp.2016.7472621,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition","We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.","['https://openalex.org/W6679434410', 'https://openalex.org/W6635078382', 'https://openalex.org/W6623517193', 'https://openalex.org/W6630875275', 'https://openalex.org/W6676562027', 'https://openalex.org/W2064675550', 'https://openalex.org/W2005708641', 'https://openalex.org/W6674758992', 'https://openalex.org/W6680587008', 'https://openalex.org/W2112796928', 'https://openalex.org/W6679429981', 'https://openalex.org/W6629052376', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963211739', 'https://openalex.org/W6696982659', 'https://openalex.org/W6679436768', 'https://openalex.org/W2399086392', 'https://openalex.org/W6675365184', 'https://openalex.org/W2157331557', 'https://openalex.org/W2143612262', 'https://openalex.org/W6621543089', 'https://openalex.org/W6635768407', 'https://openalex.org/W6631362777', 'https://openalex.org/W6640090968', 'https://openalex.org/W6684859321', 'https://openalex.org/W1533416326', 'https://openalex.org/W2293858598', 'https://openalex.org/W2963920996', 'https://openalex.org/W1922655562', 'https://openalex.org/W854541894', 'https://openalex.org/W2109886035', 'https://openalex.org/W2130942839', 'https://openalex.org/W1600744878', 'https://openalex.org/W1484868577', 'https://openalex.org/W1586532344', 'https://openalex.org/W2168231600', 'https://openalex.org/W1514535095', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2131342762', 'https://openalex.org/W2293009711', 'https://openalex.org/W2099257174', 'https://openalex.org/W2102113734', 'https://openalex.org/W2504108613', 'https://openalex.org/W648786980', 'https://openalex.org/W2138660131', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962826786']",2016-03-01
https://openalex.org/W2123301721,,METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments,"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalize concept of unigram match ing between th e mach inepro uce translation an h uman-pro uce reference translations. Unigrams can be match e base on th eir surface forms, stemme forms, an meanings; furth ermore, METEOR can be easily exten e to inclu e more a vance match ing strategies. Once all generalize unigram match es between th e two strings h ave been foun , METEOR computes a score for th is match ing using a combination of unigram-precision, unigram-recall, an a measure of fragmentation th at is esigne to irectly capture h ow well-or ere th e match e wor s in th e mach ine translation are in relation to th e reference. We evaluate METEOR by measuring th e correlation between th e metric scores an h uman judgments of translation quality. We compute th e Pearson R correlation value between its scores an human quality assessments of th e LDC TIDES 2003 Arabic-to-English and Chinese-to-English atasets. We perform segment-bysegment correlation, an show that METEOR gets an R correlation value of 0.347 on the Arabic data an 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.","['https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118021410', 'https://openalex.org/W1588719663', 'https://openalex.org/W1489409710', 'https://openalex.org/W3197906698']",2005-06-01
https://openalex.org/W1956340063,https://doi.org/10.1109/cvpr.2015.7299087,CIDEr: Consensus-based image description evaluation,"Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.","['https://openalex.org/W2104974755', 'https://openalex.org/W2049705550', 'https://openalex.org/W6898505805', 'https://openalex.org/W6676497082', 'https://openalex.org/W6675323542', 'https://openalex.org/W6700749866', 'https://openalex.org/W2110933980', 'https://openalex.org/W2024932032', 'https://openalex.org/W6677994088', 'https://openalex.org/W2294130536', 'https://openalex.org/W6640617836', 'https://openalex.org/W2149489787', 'https://openalex.org/W6681184217', 'https://openalex.org/W2128856065', 'https://openalex.org/W6686583229', 'https://openalex.org/W6639694449', 'https://openalex.org/W2168356304', 'https://openalex.org/W6634846276', 'https://openalex.org/W6625044600', 'https://openalex.org/W68733909', 'https://openalex.org/W6639809013', 'https://openalex.org/W2121927366', 'https://openalex.org/W1489525520', 'https://openalex.org/W6683512859', 'https://openalex.org/W2066931620', 'https://openalex.org/W6677969093', 'https://openalex.org/W6691532579', 'https://openalex.org/W6639432524', 'https://openalex.org/W2133459682', 'https://openalex.org/W6676297131', 'https://openalex.org/W2067816745', 'https://openalex.org/W90646044', 'https://openalex.org/W6678262379', 'https://openalex.org/W2154652894', 'https://openalex.org/W6676647902', 'https://openalex.org/W2251583212', 'https://openalex.org/W2066134726', 'https://openalex.org/W1996418862', 'https://openalex.org/W6631516269', 'https://openalex.org/W6641064462', 'https://openalex.org/W6637306801', 'https://openalex.org/W6638478770', 'https://openalex.org/W6680127537', 'https://openalex.org/W6639118148', 'https://openalex.org/W2169852119', 'https://openalex.org/W6639657675', 'https://openalex.org/W6639102338', 'https://openalex.org/W2134270519', 'https://openalex.org/W2109586012', 'https://openalex.org/W2951342632', 'https://openalex.org/W2143449221', 'https://openalex.org/W2101105183', 'https://openalex.org/W956551720', 'https://openalex.org/W2953276893', 'https://openalex.org/W1584193343', 'https://openalex.org/W1905882502', 'https://openalex.org/W1687846465', 'https://openalex.org/W1889081078', 'https://openalex.org/W1830239953', 'https://openalex.org/W2186094539', 'https://openalex.org/W2119775030', 'https://openalex.org/W2108598243', 'https://openalex.org/W1861492603', 'https://openalex.org/W2102065370', 'https://openalex.org/W2123301721', 'https://openalex.org/W1897761818', 'https://openalex.org/W2037227137', 'https://openalex.org/W2159243025', 'https://openalex.org/W1858383477', 'https://openalex.org/W1947481528', 'https://openalex.org/W2321228595', 'https://openalex.org/W2112912048', 'https://openalex.org/W2251442176', 'https://openalex.org/W1956340063', 'https://openalex.org/W1527575280', 'https://openalex.org/W1895577753', 'https://openalex.org/W2951183276', 'https://openalex.org/W3143107425', 'https://openalex.org/W2122180654']",2015-06-01
https://openalex.org/W2951444698,https://doi.org/10.1109/access.2019.2922617,End-to-End Speech Recognition Sequence Training With Reinforcement Learning,"End-to-end sequence modeling has become a popular choice for automatic speech recognition (ASR) because of the simpler pipeline compared to the conventional system and its excellent performance. However, there are several drawbacks in the end-to-end ASR model training where the current time-step prediction on the target side are conditioned with the ground truth transcription and speech features. In the inference stage, the condition is different because the model does not have any access to the target sequence ground-truth, thus any mistakes might be accumulated and degrade the decoding result over time. Another issue is raised because of the discrepancy between training and evaluation objective. In the training stage, maximum likelihood estimation criterion is used as the objective function. However, the ASR systems quality is evaluated based on the word error rate via Levenshtein distance. Therefore, we present an alternative for optimizing end-to-end ASR model with one of the reinforcement learning method called policy gradient. The model trained the proposed approach has several advantages: (1) the model simulates the inference stage by free sampling process and uses its own sample as the input, and; (2) optimize the model with a reward function correlated with the ASR evaluation metric (e.g., negative Levenshtein distance). Based on the result from our experiment, our proposed method significantly improve the model performance compared to a model trained only with teacher forcing and maximum likelihood objective function.","['https://openalex.org/W6630875275', 'https://openalex.org/W6640185926', 'https://openalex.org/W6736996214', 'https://openalex.org/W1895577753', 'https://openalex.org/W6745628346', 'https://openalex.org/W6745177358', 'https://openalex.org/W2016589492', 'https://openalex.org/W2119717200', 'https://openalex.org/W32403112', 'https://openalex.org/W2526425061', 'https://openalex.org/W2129142580', 'https://openalex.org/W6631190155', 'https://openalex.org/W1977655452', 'https://openalex.org/W3211848854', 'https://openalex.org/W2963167310', 'https://openalex.org/W6677929280', 'https://openalex.org/W2145339207', 'https://openalex.org/W2963929190', 'https://openalex.org/W6898505805', 'https://openalex.org/W2024490156', 'https://openalex.org/W4205130185', 'https://openalex.org/W6679436768', 'https://openalex.org/W2327501763', 'https://openalex.org/W6676023451', 'https://openalex.org/W2963144852', 'https://openalex.org/W2144499799', 'https://openalex.org/W2064675550', 'https://openalex.org/W2160815625', 'https://openalex.org/W2962826786', 'https://openalex.org/W2892124901', 'https://openalex.org/W6679434410', 'https://openalex.org/W6631362777', 'https://openalex.org/W2076063813', 'https://openalex.org/W6685322675', 'https://openalex.org/W2963463964', 'https://openalex.org/W6738974909', 'https://openalex.org/W6683794156', 'https://openalex.org/W2257979135', 'https://openalex.org/W1515851193', 'https://openalex.org/W2163068732', 'https://openalex.org/W2962765220', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962759037', 'https://openalex.org/W1921523184', 'https://openalex.org/W2130942839', 'https://openalex.org/W1916559533', 'https://openalex.org/W2525778437', 'https://openalex.org/W2142416747', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963609956', 'https://openalex.org/W2101105183', 'https://openalex.org/W1522301498', 'https://openalex.org/W2108682071', 'https://openalex.org/W2952264928', 'https://openalex.org/W1514535095', 'https://openalex.org/W2176263492']",2019-01-01
https://openalex.org/W2170659185,https://doi.org/10.1109/icassp.2011.5947338,Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection,"In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.","['https://openalex.org/W1591300715', 'https://openalex.org/W6628911050', 'https://openalex.org/W2103360771', 'https://openalex.org/W6601311673', 'https://openalex.org/W6647712556', 'https://openalex.org/W2079460648', 'https://openalex.org/W1606268232', 'https://openalex.org/W2171019095', 'https://openalex.org/W2117041980', 'https://openalex.org/W6679327398', 'https://openalex.org/W6632102281', 'https://openalex.org/W2168175751', 'https://openalex.org/W2130408113', 'https://openalex.org/W1583620810', 'https://openalex.org/W1482605500', 'https://openalex.org/W2187637362', 'https://openalex.org/W1988540447', 'https://openalex.org/W1556474518', 'https://openalex.org/W2168561756', 'https://openalex.org/W30845872', 'https://openalex.org/W1534656595']",2011-05-01
https://openalex.org/W2079460648,https://doi.org/10.1109/icassp.2010.5495637,Towards multi-speaker unsupervised speech pattern discovery,"In this paper, we explore the use of a Gaussian posteriorgram based representation for unsupervised discovery of speech patterns. Compared with our previous work, the new approach provides significant improvement towards speaker independence. The framework consists of three main procedures: a Gaussian posteriorgram generation procedure which learns an unsupervised Gaussian mixture model and labels each speech frame with a Gaussian posteriorgram representation; a segmental dynamic time warping procedure which locates pairs of similar sequences of Gaussian posteriorgram vectors; and a graph clustering procedure which groups similar sequences into clusters. We demonstrate the viability of using the posteriorgram approach to handle many talkers by finding clusters of words in the TIMIT corpus.","['https://openalex.org/W2126203737', 'https://openalex.org/W2114510609', 'https://openalex.org/W2107076535', 'https://openalex.org/W2089458547', 'https://openalex.org/W2114347655', 'https://openalex.org/W2099415988', 'https://openalex.org/W159433267']",2010-01-01
https://openalex.org/W2057007397,https://doi.org/10.1109/asru.2011.6163965,Efficient spoken term discovery using randomized algorithms,"Spoken term discovery is the task of automatically identifying words and phrases in speech data by searching for long repeated acoustic patterns. Initial solutions relied on exhaustive dynamic time warping-based searches across the entire similarity matrix, a method whose scalability is ultimately limited by the O(n <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> ) nature of the search space. Recent strategies have attempted to improve search efficiency by using either unsupervised or mismatched-language acoustic models to reduce the complexity of the feature representation. Taking a completely different approach, this paper investigates the use of randomized algorithms that operate directly on the raw acoustic features to produce sparse approximate similarity matrices in O(n) space and O(n log n) time. We demonstrate these techniques facilitate spoken term discovery performance capable of outperforming a model-based strategy in the zero resource setting.","['https://openalex.org/W2112038498', 'https://openalex.org/W2116343079', 'https://openalex.org/W6680730250', 'https://openalex.org/W2113932204', 'https://openalex.org/W2397535009', 'https://openalex.org/W2170580867', 'https://openalex.org/W6795644987', 'https://openalex.org/W2122056984', 'https://openalex.org/W6714100551', 'https://openalex.org/W2152565070', 'https://openalex.org/W2401464865', 'https://openalex.org/W2079460648', 'https://openalex.org/W2147717514', 'https://openalex.org/W1606268232', 'https://openalex.org/W4230940751', 'https://openalex.org/W2108120477', 'https://openalex.org/W2114347655', 'https://openalex.org/W308497914', 'https://openalex.org/W2166343746', 'https://openalex.org/W30845872', 'https://openalex.org/W2096834449', 'https://openalex.org/W1539935047', 'https://openalex.org/W6602705600', 'https://openalex.org/W2112107221', 'https://openalex.org/W2140427797', 'https://openalex.org/W3160851792', 'https://openalex.org/W66167291', 'https://openalex.org/W2012833704', 'https://openalex.org/W2407151108']",2011-12-01
https://openalex.org/W2398490608,https://doi.org/10.21437/interspeech.2015-645,Unsupervised word discovery from speech using automatic segmentation into syllable-like units,,"['https://openalex.org/W2252172689', 'https://openalex.org/W2164356097', 'https://openalex.org/W2400622388', 'https://openalex.org/W2056486423', 'https://openalex.org/W2055408826', 'https://openalex.org/W1993287287', 'https://openalex.org/W1991274470', 'https://openalex.org/W2020944885', 'https://openalex.org/W2053952913', 'https://openalex.org/W1985163701', 'https://openalex.org/W2029948425', 'https://openalex.org/W2004833594', 'https://openalex.org/W2044138293', 'https://openalex.org/W2049142189', 'https://openalex.org/W2078769636', 'https://openalex.org/W2071665560', 'https://openalex.org/W2059824090', 'https://openalex.org/W28194048', 'https://openalex.org/W2251025892', 'https://openalex.org/W2114777034', 'https://openalex.org/W2067539019', 'https://openalex.org/W2133148365', 'https://openalex.org/W1965635292', 'https://openalex.org/W2026764577', 'https://openalex.org/W1774234760', 'https://openalex.org/W2126203737']",2015-09-06
https://openalex.org/W4206865574,,Image2speech: Automatically generating audio descriptions of images,,[],2017-01-01
https://openalex.org/W2120847449,https://doi.org/10.1109/tassp.1984.1164317,Signal estimation from modified short-time Fourier transform,"In this paper, we present an algorithm to estimate a signal from its modified short-time Fourier transform (STFT). This algorithm is computationally simple and is obtained by minimizing the mean squared error between the STFT of the estimated signal and the modified STFT. Using this algorithm, we also develop an iterative algorithm to estimate a signal from its modified STFT magnitude. The iterative algorithm is shown to decrease, in each iteration, the mean squared error between the STFT magnitude of the estimated signal and the modified STFT magnitude. The major computation involved in the iterative algorithm is the discrete Fourier transform (DFT) computation, and the algorithm appears to be real-time implementable with current hardware technology. The algorithm developed in this paper has been applied to the time-scale modification of speech. The resulting system generates very high-quality speech, and appears to be better in performance than any existing method.","['https://openalex.org/W2138196899', 'https://openalex.org/W6628955290', 'https://openalex.org/W2051049794', 'https://openalex.org/W2080145918', 'https://openalex.org/W2133541109', 'https://openalex.org/W2168274102', 'https://openalex.org/W2020997493', 'https://openalex.org/W1968939597', 'https://openalex.org/W1989344925', 'https://openalex.org/W1973119332', 'https://openalex.org/W2069501481', 'https://openalex.org/W2071609006', 'https://openalex.org/W1484412996', 'https://openalex.org/W3094114204']",1984-04-01
https://openalex.org/W2964115348,,"Linguistic unit discovery from multimodal inputs in unwritten languages: Summary of the ""Speaking Rosetta"" JSALT 2017 Workshop",We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.,"['https://openalex.org/W2117041980', 'https://openalex.org/W2963620343', 'https://openalex.org/W2395052932', 'https://openalex.org/W2057007397', 'https://openalex.org/W1833498382', 'https://openalex.org/W2962862718', 'https://openalex.org/W2325972120', 'https://openalex.org/W2114347655', 'https://openalex.org/W4206864474', 'https://openalex.org/W2164505566', 'https://openalex.org/W2762715843', 'https://openalex.org/W2079460648', 'https://openalex.org/W2251025892', 'https://openalex.org/W1861492603', 'https://openalex.org/W2464234964', 'https://openalex.org/W2406349064', 'https://openalex.org/W1970890968', 'https://openalex.org/W2345799635', 'https://openalex.org/W2949328740', 'https://openalex.org/W1520968739', 'https://openalex.org/W2406324447', 'https://openalex.org/W1855892484', 'https://openalex.org/W2963819008', 'https://openalex.org/W2556930864', 'https://openalex.org/W2347098582', 'https://openalex.org/W2586148577', 'https://openalex.org/W4206865574', 'https://openalex.org/W2962832640', 'https://openalex.org/W3217769081']",2018-04-15
https://openalex.org/W3096216486,https://doi.org/10.21437/interspeech.2020-3033,Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge,"In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019.The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels.In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers.The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate.Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook.Additionally, we also explored several regularization methods to improve performance even further.","['https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963799213', 'https://openalex.org/W4288107125', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W2191779130', 'https://openalex.org/W2789543585', 'https://openalex.org/W2547039119', 'https://openalex.org/W2972374322', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973', 'https://openalex.org/W2972867623', 'https://openalex.org/W2940544976', 'https://openalex.org/W2906111771', 'https://openalex.org/W2025768430', 'https://openalex.org/W4295312788']",2020-10-25
https://openalex.org/W2347145335,https://doi.org/10.1016/j.procs.2016.04.023,Breaking the Unwritten Language Barrier: The BULB Project,International audience,"['https://openalex.org/W4388277788', 'https://openalex.org/W2397643736', 'https://openalex.org/W1712048714', 'https://openalex.org/W2132534451', 'https://openalex.org/W2101281673', 'https://openalex.org/W6652311901', 'https://openalex.org/W2156985047', 'https://openalex.org/W191292882', 'https://openalex.org/W2063655091', 'https://openalex.org/W141348503', 'https://openalex.org/W2095907708', 'https://openalex.org/W2530395991', 'https://openalex.org/W2122228338', 'https://openalex.org/W2140991203', 'https://openalex.org/W2601422447', 'https://openalex.org/W2117126688', 'https://openalex.org/W1969608442', 'https://openalex.org/W2166270474', 'https://openalex.org/W1885617648', 'https://openalex.org/W1993799394', 'https://openalex.org/W2345799635', 'https://openalex.org/W289413641', 'https://openalex.org/W2092052114', 'https://openalex.org/W6610006008', 'https://openalex.org/W2162465526', 'https://openalex.org/W192980855', 'https://openalex.org/W65599759', 'https://openalex.org/W3040954139', 'https://openalex.org/W2160131530', 'https://openalex.org/W2251146352', 'https://openalex.org/W1577146168', 'https://openalex.org/W2014840417', 'https://openalex.org/W2055535974', 'https://openalex.org/W2006969979', 'https://openalex.org/W1689157505', 'https://openalex.org/W182534140', 'https://openalex.org/W2251001376']",2016-01-01
https://openalex.org/W3009205145,https://doi.org/10.1109/iccv.2019.00769,Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck,"Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem ""skip-modal generation"" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.","['https://openalex.org/W6756197946', 'https://openalex.org/W6746700228', 'https://openalex.org/W1963549852', 'https://openalex.org/W2607579284', 'https://openalex.org/W1494198834', 'https://openalex.org/W6748573829', 'https://openalex.org/W6620707391', 'https://openalex.org/W6713645886', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963267809', 'https://openalex.org/W6757422803', 'https://openalex.org/W6691096134', 'https://openalex.org/W2745461083', 'https://openalex.org/W6687566353', 'https://openalex.org/W6753850902', 'https://openalex.org/W6631190155', 'https://openalex.org/W6735204497', 'https://openalex.org/W2277195237', 'https://openalex.org/W6640963894', 'https://openalex.org/W6747270024', 'https://openalex.org/W6750489868', 'https://openalex.org/W6732742072', 'https://openalex.org/W6745992979', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963727906', 'https://openalex.org/W2963807156', 'https://openalex.org/W2964024144', 'https://openalex.org/W6698228248', 'https://openalex.org/W2963966654', 'https://openalex.org/W6630875275', 'https://openalex.org/W2108598243', 'https://openalex.org/W6639480849', 'https://openalex.org/W2097117768', 'https://openalex.org/W2040510368', 'https://openalex.org/W6765987481', 'https://openalex.org/W2120847449', 'https://openalex.org/W2194775991', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963073614', 'https://openalex.org/W2591927543', 'https://openalex.org/W6639432524', 'https://openalex.org/W6739366716', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963767194', 'https://openalex.org/W6739901393', 'https://openalex.org/W1895577753', 'https://openalex.org/W6639082767', 'https://openalex.org/W6745569068', 'https://openalex.org/W6730095352', 'https://openalex.org/W6637108112', 'https://openalex.org/W2613718673', 'https://openalex.org/W2963712897', 'https://openalex.org/W2619368999', 'https://openalex.org/W4298857617', 'https://openalex.org/W2901997113', 'https://openalex.org/W2777302760', 'https://openalex.org/W2781384251', 'https://openalex.org/W2885013662', 'https://openalex.org/W2963826681', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964121744', 'https://openalex.org/W2405756170', 'https://openalex.org/W2962739369', 'https://openalex.org/W2949382160', 'https://openalex.org/W1522301498', 'https://openalex.org/W2184045248', 'https://openalex.org/W1686946872', 'https://openalex.org/W4298174729', 'https://openalex.org/W2964243274', 'https://openalex.org/W2626778328', 'https://openalex.org/W2766812927', 'https://openalex.org/W4297772798', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964341837', 'https://openalex.org/W1869752048', 'https://openalex.org/W2963534259', 'https://openalex.org/W2906797124', 'https://openalex.org/W2768959015', 'https://openalex.org/W2962964479', 'https://openalex.org/W2963125871', 'https://openalex.org/W2951939904', 'https://openalex.org/W2953022248', 'https://openalex.org/W4297818305', 'https://openalex.org/W639708223', 'https://openalex.org/W2949999304', 'https://openalex.org/W1514535095', 'https://openalex.org/W2302086703', 'https://openalex.org/W2979454998', 'https://openalex.org/W2099471712', 'https://openalex.org/W1882958252', 'https://openalex.org/W4320013936', 'https://openalex.org/W1889081078', 'https://openalex.org/W2564591810', 'https://openalex.org/W4385245566', 'https://openalex.org/W4293398859', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963330667', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963069010']",2019-10-01
https://openalex.org/W2122538988,https://doi.org/10.1002/aic.690370209,Nonlinear principal component analysis using autoassociative neural networks,"Abstract Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well‐known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time‐dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.","['https://openalex.org/W2042492924', 'https://openalex.org/W2078626246', 'https://openalex.org/W6604803494', 'https://openalex.org/W2103496339', 'https://openalex.org/W2030328665', 'https://openalex.org/W2158863190', 'https://openalex.org/W1487182516', 'https://openalex.org/W145476170', 'https://openalex.org/W1992301222', 'https://openalex.org/W1571748899', 'https://openalex.org/W2097533491', 'https://openalex.org/W2023104721', 'https://openalex.org/W2432567885', 'https://openalex.org/W4300402905', 'https://openalex.org/W2131329059', 'https://openalex.org/W2147912439', 'https://openalex.org/W1573503290', 'https://openalex.org/W2009642296', 'https://openalex.org/W3121926921', 'https://openalex.org/W1507849272', 'https://openalex.org/W3121126077', 'https://openalex.org/W2766736793', 'https://openalex.org/W2134273960', 'https://openalex.org/W4254171776', 'https://openalex.org/W3017143921', 'https://openalex.org/W138825259', 'https://openalex.org/W4285259144', 'https://openalex.org/W1965324089', 'https://openalex.org/W3016210511', 'https://openalex.org/W117096852']",1991-02-01
https://openalex.org/W2194775991,https://doi.org/10.1109/cvpr.2016.90,Deep Residual Learning for Image Recognition,"Actualmente diversas investigaciones se han enfocado en analizar a partir de videos de alta velocidad, características de las descargas eléctricas atmosféricas con el fin de adquirir mejor comprensión del fenómeno, que conlleva entre otros aspectos el desarrollo de sistemas de protección robustos. La mayoría de las investigaciones han requerido de un observador que ante el suceso del evento provea un disparo manual a la cámara permitiendo almacenar la información visual del fenómeno. Por tanto, este trabajo se orientó en proponer una metodología para la detección de las descargas utilizando dos implementaciones basadas en procesamiento de señales y visión computacional, con el propósito que el sistema autónomamente sea el que suministre el disparo, apartando al observador de la realización de esta tarea. El sistema de detección basado en técnicas de procesamiento de imágenes requirió la adecuación de métodos de segmentación, representación, descripción y clasificación. El algoritmo de reconocimiento con visión computacional se implementó mediante la red neuronal convolucional EfficientNetB4. Fuera de línea, las técnicas basadas en procesamiento de imágenes suministraron una precisión del 81.81%, mientras que haciendo uso de visión computacional la precisión fue de 71.63%. Con el objeto de evaluar el desempeño en tiempo real, las técnicas de procesamiento se adaptaron en un ordenador de placa reducida correspondiente a la Raspberry Pi 3 modelo B+ obteniéndose una precisión de 86.95%. Adicionalmente, se evaluó la característica de multiplicidad la cual corresponde al número de descargas subsecuentes presentes en el canal de la descarga logrando una precisión de 66.66%. (Texto tomado de la fuente)","['https://openalex.org/W6629368666', 'https://openalex.org/W4212915314', 'https://openalex.org/W2117812871', 'https://openalex.org/W6620707391', 'https://openalex.org/W6682359208', 'https://openalex.org/W2147238549', 'https://openalex.org/W6680555419', 'https://openalex.org/W6637551013', 'https://openalex.org/W2161388792', 'https://openalex.org/W1903029394', 'https://openalex.org/W6637242042', 'https://openalex.org/W4388297464', 'https://openalex.org/W2107878631', 'https://openalex.org/W6787972765', 'https://openalex.org/W2147800946', 'https://openalex.org/W6684191040', 'https://openalex.org/W4238404964', 'https://openalex.org/W6639102338', 'https://openalex.org/W1932847118', 'https://openalex.org/W6676338569', 'https://openalex.org/W6637373629', 'https://openalex.org/W1677182931', 'https://openalex.org/W6640036494', 'https://openalex.org/W2064675550', 'https://openalex.org/W6638667902', 'https://openalex.org/W2124509324', 'https://openalex.org/W1984309565', 'https://openalex.org/W1976921161', 'https://openalex.org/W1997542937', 'https://openalex.org/W1536680647', 'https://openalex.org/W2031489346', 'https://openalex.org/W6631943919', 'https://openalex.org/W2102605133', 'https://openalex.org/W6688059459', 'https://openalex.org/W2964103341', 'https://openalex.org/W4231990273', 'https://openalex.org/W4242212377', 'https://openalex.org/W6626481562', 'https://openalex.org/W2159979951', 'https://openalex.org/W2097117768', 'https://openalex.org/W2139398462', 'https://openalex.org/W3118608800', 'https://openalex.org/W2025357764', 'https://openalex.org/W2963542991', 'https://openalex.org/W1932624639', 'https://openalex.org/W1861492603', 'https://openalex.org/W4285719527', 'https://openalex.org/W2950094539', 'https://openalex.org/W1799366690', 'https://openalex.org/W1690739335', 'https://openalex.org/W1665214252', 'https://openalex.org/W2168894214', 'https://openalex.org/W1811843574', 'https://openalex.org/W2962835968', 'https://openalex.org/W1533861849', 'https://openalex.org/W2613718673', 'https://openalex.org/W2022532533', 'https://openalex.org/W2952009708', 'https://openalex.org/W1904365287', 'https://openalex.org/W3037950864', 'https://openalex.org/W2328425223', 'https://openalex.org/W1530872699', 'https://openalex.org/W194249466', 'https://openalex.org/W2155893237', 'https://openalex.org/W2963504252', 'https://openalex.org/W2914484425', 'https://openalex.org/W1836465849', 'https://openalex.org/W2206858481', 'https://openalex.org/W2964118293', 'https://openalex.org/W2179352600', 'https://openalex.org/W1686810756', 'https://openalex.org/W2066941820', 'https://openalex.org/W2117539524', 'https://openalex.org/W2950621961', 'https://openalex.org/W1554663460', 'https://openalex.org/W2951603627', 'https://openalex.org/W3038058348', 'https://openalex.org/W2163605009', 'https://openalex.org/W639708223', 'https://openalex.org/W2125930537', 'https://openalex.org/W2152424459']",2016-06-01
https://openalex.org/W2154652894,,ROUGE: A Package for Automatic Evaluation of Summaries,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1","['https://openalex.org/W2101105183', 'https://openalex.org/W2963010813', 'https://openalex.org/W2112174218', 'https://openalex.org/W2752885492', 'https://openalex.org/W3145128584', 'https://openalex.org/W2108325777', 'https://openalex.org/W2097879961', 'https://openalex.org/W1493638625', 'https://openalex.org/W2150824314', 'https://openalex.org/W2088781183', 'https://openalex.org/W3151369355', 'https://openalex.org/W2117010802']",2004-07-25
https://openalex.org/W3114436296,https://doi.org/10.18653/v1/2021.acl-long.411,Text-Free Image-to-Speech Synthesis Using Learned Segmental Units,"In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.","['https://openalex.org/W2979476256', 'https://openalex.org/W1499360075', 'https://openalex.org/W2939710050', 'https://openalex.org/W2963799213', 'https://openalex.org/W2988907666', 'https://openalex.org/W2963618559', 'https://openalex.org/W2963568578', 'https://openalex.org/W2926827382', 'https://openalex.org/W2964001192', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962706528', 'https://openalex.org/W2995969307', 'https://openalex.org/W2120605154', 'https://openalex.org/W2953114965', 'https://openalex.org/W2963300588', 'https://openalex.org/W1861492603', 'https://openalex.org/W2788277448', 'https://openalex.org/W2108598243', 'https://openalex.org/W2736900972', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995480165', 'https://openalex.org/W2962862718', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963360726', 'https://openalex.org/W2971709506', 'https://openalex.org/W2996287690', 'https://openalex.org/W2947591107', 'https://openalex.org/W2995680346', 'https://openalex.org/W2575842049', 'https://openalex.org/W2950178297', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W2940544976', 'https://openalex.org/W3039910566', 'https://openalex.org/W2963534259', 'https://openalex.org/W2970351109', 'https://openalex.org/W2962691331', 'https://openalex.org/W2608338293', 'https://openalex.org/W2964243274', 'https://openalex.org/W2532494225', 'https://openalex.org/W2963830550', 'https://openalex.org/W2586148577', 'https://openalex.org/W2154652894', 'https://openalex.org/W2013996527', 'https://openalex.org/W3125709657', 'https://openalex.org/W854541894', 'https://openalex.org/W2988736778', 'https://openalex.org/W3007068036', 'https://openalex.org/W2758849341', 'https://openalex.org/W2805122419', 'https://openalex.org/W2963417023', 'https://openalex.org/W2964069186', 'https://openalex.org/W3003875258', 'https://openalex.org/W2962850167', 'https://openalex.org/W2927673779', 'https://openalex.org/W3045485643', 'https://openalex.org/W2527729766', 'https://openalex.org/W3093845497', 'https://openalex.org/W2787779284', 'https://openalex.org/W2995404354', 'https://openalex.org/W2963804033', 'https://openalex.org/W2963902314', 'https://openalex.org/W2106053110', 'https://openalex.org/W3009205145', 'https://openalex.org/W2506483933', 'https://openalex.org/W2120847449', 'https://openalex.org/W1956340063', 'https://openalex.org/W1514535095', 'https://openalex.org/W2795151422', 'https://openalex.org/W2194775991', 'https://openalex.org/W385555557', 'https://openalex.org/W2996383576', 'https://openalex.org/W2101105183', 'https://openalex.org/W3105148948', 'https://openalex.org/W2736876693', 'https://openalex.org/W2745461083', 'https://openalex.org/W2792995953', 'https://openalex.org/W2156142001', 'https://openalex.org/W2963084599', 'https://openalex.org/W2884607399', 'https://openalex.org/W2989358187', 'https://openalex.org/W2796495654', 'https://openalex.org/W2920166246', 'https://openalex.org/W2937090315', 'https://openalex.org/W2119775030', 'https://openalex.org/W2171361956', 'https://openalex.org/W2134670479', 'https://openalex.org/W1895577753', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963096510', 'https://openalex.org/W2964249784', 'https://openalex.org/W2133459682', 'https://openalex.org/W2556930864', 'https://openalex.org/W2950133079']",2021-01-01
https://openalex.org/W2556930864,,Unsupervised learning of spoken language with visual context,"Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.","['https://openalex.org/W2100768664', 'https://openalex.org/W1905882502', 'https://openalex.org/W2314664620', 'https://openalex.org/W2134670479', 'https://openalex.org/W1969608442', 'https://openalex.org/W2112912048', 'https://openalex.org/W1931639407', 'https://openalex.org/W66167291', 'https://openalex.org/W2009388533', 'https://openalex.org/W2102605133', 'https://openalex.org/W2114347655', 'https://openalex.org/W2062955551', 'https://openalex.org/W1895577753', 'https://openalex.org/W2137471889', 'https://openalex.org/W2119775030', 'https://openalex.org/W2055408826', 'https://openalex.org/W2119187236', 'https://openalex.org/W1524333225', 'https://openalex.org/W2123024445', 'https://openalex.org/W2187089797', 'https://openalex.org/W1778492285', 'https://openalex.org/W2962862718', 'https://openalex.org/W2149557440', 'https://openalex.org/W3127686677', 'https://openalex.org/W2126203737', 'https://openalex.org/W30845872', 'https://openalex.org/W1686810756']",2016-01-01
https://openalex.org/W2119775030,,Collecting Image Annotations Using Amazon's Mechanical Turk,"Crowd-sourcing approaches such as Amazon’s Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images. 1","['https://openalex.org/W2151913182', 'https://openalex.org/W2143280940', 'https://openalex.org/W2124821580', 'https://openalex.org/W2125201090', 'https://openalex.org/W1970381522', 'https://openalex.org/W2151401338', 'https://openalex.org/W2149489787', 'https://openalex.org/W2143539737', 'https://openalex.org/W64798685']",2010-06-06
https://openalex.org/W3093427098,https://doi.org/10.48550/arxiv.2010.05967,The Zero Resource Speech Challenge 2020: Discovering discrete subword and word units,"We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.","['https://openalex.org/W3097485645', 'https://openalex.org/W2347098582', 'https://openalex.org/W3097692357', 'https://openalex.org/W2940544976', 'https://openalex.org/W2572097499', 'https://openalex.org/W2296607128', 'https://openalex.org/W3095361818', 'https://openalex.org/W3024040651', 'https://openalex.org/W3044386551', 'https://openalex.org/W3096216486', 'https://openalex.org/W2126377586', 'https://openalex.org/W4288107125', 'https://openalex.org/W2996383576', 'https://openalex.org/W3096262326', 'https://openalex.org/W1524333225', 'https://openalex.org/W4300047444', 'https://openalex.org/W2284628133', 'https://openalex.org/W2346964103', 'https://openalex.org/W2598638573', 'https://openalex.org/W2962693497', 'https://openalex.org/W2057007397', 'https://openalex.org/W2962699523', 'https://openalex.org/W2547039119', 'https://openalex.org/W3096359985', 'https://openalex.org/W3097056138', 'https://openalex.org/W2979476256', 'https://openalex.org/W3097159218', 'https://openalex.org/W3084014658']",2020-10-12
https://openalex.org/W2126377586,https://doi.org/10.1016/j.cognition.2009.03.008,A Bayesian framework for word segmentation: Exploring the effects of context,,"['https://openalex.org/W6609746230', 'https://openalex.org/W6982500881', 'https://openalex.org/W2154164802', 'https://openalex.org/W1975992139', 'https://openalex.org/W2115197214', 'https://openalex.org/W6602703493', 'https://openalex.org/W1971245457', 'https://openalex.org/W6791388132', 'https://openalex.org/W2089484716', 'https://openalex.org/W4231510805', 'https://openalex.org/W2132827946', 'https://openalex.org/W2074546930', 'https://openalex.org/W2029948425', 'https://openalex.org/W2071402670', 'https://openalex.org/W1997236510', 'https://openalex.org/W2064699871', 'https://openalex.org/W2166391802', 'https://openalex.org/W1526685788', 'https://openalex.org/W6679859123', 'https://openalex.org/W2117621558', 'https://openalex.org/W6727943367', 'https://openalex.org/W2126736494', 'https://openalex.org/W2025653016', 'https://openalex.org/W6606895322', 'https://openalex.org/W2110485445', 'https://openalex.org/W2091797506', 'https://openalex.org/W6682816277', 'https://openalex.org/W2069429561', 'https://openalex.org/W6677149852', 'https://openalex.org/W1500689260', 'https://openalex.org/W6677207036', 'https://openalex.org/W2786088222', 'https://openalex.org/W2130518563', 'https://openalex.org/W6677975208', 'https://openalex.org/W2020999234', 'https://openalex.org/W2101711363', 'https://openalex.org/W2111668269', 'https://openalex.org/W2172709584', 'https://openalex.org/W2122228338', 'https://openalex.org/W2159399018', 'https://openalex.org/W2604174198', 'https://openalex.org/W1990486914', 'https://openalex.org/W2882319491', 'https://openalex.org/W4251556668', 'https://openalex.org/W2138309709', 'https://openalex.org/W2117126688', 'https://openalex.org/W2157149948', 'https://openalex.org/W2105438133', 'https://openalex.org/W2061863127', 'https://openalex.org/W2059824090', 'https://openalex.org/W1934041838', 'https://openalex.org/W2049738347', 'https://openalex.org/W2157381218', 'https://openalex.org/W2065392216', 'https://openalex.org/W6679162469', 'https://openalex.org/W1995991622', 'https://openalex.org/W2135631383', 'https://openalex.org/W1993768374', 'https://openalex.org/W2056760934', 'https://openalex.org/W2111971553', 'https://openalex.org/W2118727822', 'https://openalex.org/W2149368642', 'https://openalex.org/W2080972498', 'https://openalex.org/W2003208764', 'https://openalex.org/W2149356016', 'https://openalex.org/W6678007500', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W6638762797', 'https://openalex.org/W2154167968', 'https://openalex.org/W2136549906', 'https://openalex.org/W6683603713', 'https://openalex.org/W2142111485', 'https://openalex.org/W1991083510', 'https://openalex.org/W1994960488', 'https://openalex.org/W2161952424', 'https://openalex.org/W2121416338', 'https://openalex.org/W2016429292', 'https://openalex.org/W2147336195', 'https://openalex.org/W263845233', 'https://openalex.org/W1967465043', 'https://openalex.org/W2992985668', 'https://openalex.org/W1880262756', 'https://openalex.org/W167685538', 'https://openalex.org/W4232383088', 'https://openalex.org/W2111568643', 'https://openalex.org/W2154756108', 'https://openalex.org/W2123157758', 'https://openalex.org/W4237780050', 'https://openalex.org/W2130416410', 'https://openalex.org/W2122739034', 'https://openalex.org/W1985898923', 'https://openalex.org/W2045656233', 'https://openalex.org/W3136512150', 'https://openalex.org/W2528091958', 'https://openalex.org/W1487404635', 'https://openalex.org/W1982647060', 'https://openalex.org/W1483307070', 'https://openalex.org/W2009086942', 'https://openalex.org/W2005902041', 'https://openalex.org/W2070554026', 'https://openalex.org/W1779834323', 'https://openalex.org/W2132957691', 'https://openalex.org/W2117786207', 'https://openalex.org/W3150904112', 'https://openalex.org/W2131963099', 'https://openalex.org/W2540180329', 'https://openalex.org/W1663973292', 'https://openalex.org/W2703151831', 'https://openalex.org/W2158195707', 'https://openalex.org/W2760669695', 'https://openalex.org/W3147653952', 'https://openalex.org/W2120636621', 'https://openalex.org/W2158266063', 'https://openalex.org/W2420792277', 'https://openalex.org/W2004411856', 'https://openalex.org/W1842705192']",2009-05-06
https://openalex.org/W130754613,https://doi.org/10.21437/interspeech.2009-538,An improved speech segmentation quality measure: the r-value,"Phone segmentation in ASR is usually performed indirectly by Viterbi decoding of HMM output. Direct approaches also exist, e.g., blind speech segmentation algorithms. In either case, performance of automatic speech segmentation algorithms is often measured using automated evaluation algorithms and used to optimize a segmentation system’s performance. However, evaluation approaches reported in literature were found to be lacking. Also, we have determined that increases in phone boundary location detection rates are often due to increased over-segmentation levels and not to algorithmic improvements, i.e., by simply adding random boundaries a better hit-rate can be achieved when using current quality measures. Since established measures were found to be insensitive to this type of random boundary insertion, a new R-value quality measure is introduced that indicates how close a segmentation algorithm’s performance is to an ideal point of operation.","['https://openalex.org/W1720037957', 'https://openalex.org/W2174992124', 'https://openalex.org/W1994737544', 'https://openalex.org/W2171700247', 'https://openalex.org/W2098363562', 'https://openalex.org/W1589615572', 'https://openalex.org/W2069732686', 'https://openalex.org/W59175527', 'https://openalex.org/W4285719527', 'https://openalex.org/W2107105119', 'https://openalex.org/W2083904075', 'https://openalex.org/W2116697043', 'https://openalex.org/W2114259387']",2009-09-06
https://openalex.org/W2145410271,https://doi.org/10.1016/j.specom.2004.09.001,The Buckeye corpus of conversational speech: labeling conventions and a test of transcriber reliability,,"['https://openalex.org/W2065513936', 'https://openalex.org/W1970028954', 'https://openalex.org/W2095439836', 'https://openalex.org/W2018315802', 'https://openalex.org/W2053154970', 'https://openalex.org/W2132133133', 'https://openalex.org/W6632486585', 'https://openalex.org/W4235898698', 'https://openalex.org/W2131195080', 'https://openalex.org/W6608194208', 'https://openalex.org/W1997930093', 'https://openalex.org/W1635512741', 'https://openalex.org/W4256717825', 'https://openalex.org/W4231003508', 'https://openalex.org/W2053622538', 'https://openalex.org/W35020344', 'https://openalex.org/W1486114846', 'https://openalex.org/W2071572975', 'https://openalex.org/W123448396', 'https://openalex.org/W2069585723', 'https://openalex.org/W1569410387', 'https://openalex.org/W2134491472', 'https://openalex.org/W2164098806', 'https://openalex.org/W2064565514', 'https://openalex.org/W2172066118', 'https://openalex.org/W1720037957', 'https://openalex.org/W196788897', 'https://openalex.org/W2409402797', 'https://openalex.org/W1514711353', 'https://openalex.org/W163332037', 'https://openalex.org/W2029418021', 'https://openalex.org/W1543392156', 'https://openalex.org/W87722466', 'https://openalex.org/W1996079964', 'https://openalex.org/W2060689695', 'https://openalex.org/W2334956147', 'https://openalex.org/W629806996', 'https://openalex.org/W2051970456', 'https://openalex.org/W1520106435', 'https://openalex.org/W2050200483', 'https://openalex.org/W3127686677']",2004-10-21
https://openalex.org/W2478415332,,Improving Phoneme segmentation with Recurrent Neural Networks.,"Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.","['https://openalex.org/W2069732686', 'https://openalex.org/W2095705004', 'https://openalex.org/W2083904075', 'https://openalex.org/W2121997342', 'https://openalex.org/W2174992124', 'https://openalex.org/W1942713348', 'https://openalex.org/W2404952642', 'https://openalex.org/W2150355110', 'https://openalex.org/W2171752983', 'https://openalex.org/W2073459066', 'https://openalex.org/W2057640376', 'https://openalex.org/W2110485445', 'https://openalex.org/W2017814685', 'https://openalex.org/W2084584260', 'https://openalex.org/W2111732304', 'https://openalex.org/W2148154194']",2016-08-01
https://openalex.org/W2407151108,https://doi.org/10.21437/interspeech.2011-304,Rapid evaluation of speech representations for spoken term discovery,"Acoustic front-ends are typically developed for supervised learning tasks and are thus optimized to minimize word error rate, phone error rate, etc. However, in recent efforts to develop zero-resource speech technologies, the goal is not to use transcribed speech to train systems but instead to discover the acoustic structure of the spoken language automatically. For this new setting, we require a framework for evaluating the quality of speech representations without coupling to a particular recognition architecture. Motivated by the spoken term discovery task, we present a dynamic time warping-based framework for quantifying how well a representation can associate words of the same type spoken by different speakers. We benchmark the quality of a wide range of speech representations using multiple frame-level distance metrics and demonstrate that our performance metrics can also accurately predict phone recognition accuracies.","['https://openalex.org/W30845872', 'https://openalex.org/W1980501707', 'https://openalex.org/W1539935047', 'https://openalex.org/W118636537', 'https://openalex.org/W2027499299', 'https://openalex.org/W197151588', 'https://openalex.org/W2113932204', 'https://openalex.org/W2114347655', 'https://openalex.org/W1606268232', 'https://openalex.org/W2079460648']",2011-08-27
https://openalex.org/W3097286738,https://doi.org/10.21437/interspeech.2020-1228,Vector-Quantized Autoregressive Predictive Coding,"Autoregressive Predictive Coding (APC), as a self-supervised objective, has enjoyed success in learning representations from large amounts of unlabeled data, and the learned representations are rich for many downstream tasks.However, the connection between low self-supervised loss and strong performance in downstream tasks remains unclear.In this work, we propose Vector-Quantized Autoregressive Predictive Coding (VQ-APC), a novel model that produces quantized representations, allowing us to explicitly control the amount of information encoded in the representations.By studying a sequence of increasingly limited models, we reveal the constituents of the learned representations.In particular, we confirm the presence of information with probing tasks, while showing the absence of information with mutual information, uncovering the model's preference in preserving speech information as its capacity becomes constrained.We find that there exists a point where phonetic and speaker information are amplified to maximize a selfsupervised objective.As a byproduct, the learned codes for a particular model capacity correspond well to English phones.","['https://openalex.org/W2972943112', 'https://openalex.org/W3002741552', 'https://openalex.org/W3125709657', 'https://openalex.org/W2885318751', 'https://openalex.org/W4214784181', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973157397', 'https://openalex.org/W2024490156', 'https://openalex.org/W2988736778', 'https://openalex.org/W1522301498', 'https://openalex.org/W4297808394', 'https://openalex.org/W3015419784', 'https://openalex.org/W2982223350', 'https://openalex.org/W2995680346', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963799213', 'https://openalex.org/W4385245566', 'https://openalex.org/W3016011332', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W3035202887', 'https://openalex.org/W2963045354', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015265920']",2020-10-25
https://openalex.org/W2400549570,https://doi.org/10.1109/icassp.2016.7472622,A deep scattering spectrum — Deep Siamese network pipeline for unsupervised acoustic modeling,"Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.","['https://openalex.org/W2171590421', 'https://openalex.org/W2128160875', 'https://openalex.org/W6602180557', 'https://openalex.org/W2252172689', 'https://openalex.org/W6973666849', 'https://openalex.org/W2044138293', 'https://openalex.org/W2057007397', 'https://openalex.org/W2404799143', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712202099', 'https://openalex.org/W2093231248', 'https://openalex.org/W6677154653', 'https://openalex.org/W6712444837', 'https://openalex.org/W6637061625', 'https://openalex.org/W2052697931', 'https://openalex.org/W6713745070', 'https://openalex.org/W6712553779', 'https://openalex.org/W2154833897', 'https://openalex.org/W52412328', 'https://openalex.org/W2786608204', 'https://openalex.org/W1666984270', 'https://openalex.org/W2025482506', 'https://openalex.org/W2395899413', 'https://openalex.org/W2962701684', 'https://openalex.org/W2406349064', 'https://openalex.org/W2399576818', 'https://openalex.org/W2396043527', 'https://openalex.org/W2112688413', 'https://openalex.org/W1796128977', 'https://openalex.org/W2963620343']",2016-03-01
https://openalex.org/W2117126688,https://doi.org/10.7551/mitpress/7503.003.0085,Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models,"This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with ""adaptors"" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.","['https://openalex.org/W2122228338', 'https://openalex.org/W2069429561', 'https://openalex.org/W2150507172', 'https://openalex.org/W2158266063', 'https://openalex.org/W2074546930', 'https://openalex.org/W2087309226', 'https://openalex.org/W2159399018', 'https://openalex.org/W1583697620', 'https://openalex.org/W2053218206', 'https://openalex.org/W2952343510']",2007-09-07
https://openalex.org/W2345913943,https://doi.org/10.1111/desc.12390,Co‐occurrence statistics as a language‐dependent cue for speech segmentation,"Abstract To what extent can language acquisition be explained in terms of different associative learning mechanisms? It has been hypothesized that distributional regularities in spoken languages are strong enough to elicit statistical learning about dependencies among speech units. Distributional regularities could be a useful cue for word learning even without rich language‐specific knowledge. However, it is not clear how strong and reliable the distributional cues are that humans might use to segment speech. We investigate cross‐linguistic viability of different statistical learning strategies by analyzing child‐directed speech corpora from nine languages and by modeling possible statistics‐based speech segmentations. We show that languages vary as to which statistical segmentation strategies are most successful. The variability of the results can be partially explained by systematic differences between languages, such as rhythmical differences. The results confirm previous findings that different statistical learning strategies are successful in different languages and suggest that infants may have to primarily rely on non‐statistical cues when they begin their process of speech segmentation.","['https://openalex.org/W1971245457', 'https://openalex.org/W2161922070', 'https://openalex.org/W2157262267', 'https://openalex.org/W2029948425', 'https://openalex.org/W2159637323', 'https://openalex.org/W1997218510', 'https://openalex.org/W1500689260', 'https://openalex.org/W2092975057', 'https://openalex.org/W2130518563', 'https://openalex.org/W2122739034', 'https://openalex.org/W1967642969', 'https://openalex.org/W2124235535', 'https://openalex.org/W2126377586', 'https://openalex.org/W1990486914', 'https://openalex.org/W4251556668', 'https://openalex.org/W2118599126', 'https://openalex.org/W2162367254', 'https://openalex.org/W1573928862', 'https://openalex.org/W2076111237', 'https://openalex.org/W2157149948', 'https://openalex.org/W2124973255', 'https://openalex.org/W2059824090', 'https://openalex.org/W2132573777', 'https://openalex.org/W1990397005', 'https://openalex.org/W4388346736', 'https://openalex.org/W2339004230', 'https://openalex.org/W1566018662', 'https://openalex.org/W2092567368', 'https://openalex.org/W2080876739', 'https://openalex.org/W3209469015', 'https://openalex.org/W2097335727', 'https://openalex.org/W2071815064', 'https://openalex.org/W4214853963', 'https://openalex.org/W2095908250', 'https://openalex.org/W2008122551', 'https://openalex.org/W1996171054', 'https://openalex.org/W1980862600', 'https://openalex.org/W2081357278', 'https://openalex.org/W2115867364', 'https://openalex.org/W2136549906', 'https://openalex.org/W1967421273', 'https://openalex.org/W1977881723', 'https://openalex.org/W2119863209', 'https://openalex.org/W2543796155', 'https://openalex.org/W2061094331', 'https://openalex.org/W1822165271', 'https://openalex.org/W2098717046', 'https://openalex.org/W2153592339', 'https://openalex.org/W1593045043', 'https://openalex.org/W2115197214', 'https://openalex.org/W1991865342', 'https://openalex.org/W2142111485', 'https://openalex.org/W2541409095', 'https://openalex.org/W2142375640', 'https://openalex.org/W2912372357', 'https://openalex.org/W2968573267', 'https://openalex.org/W1660390307', 'https://openalex.org/W2166061736', 'https://openalex.org/W2914753947', 'https://openalex.org/W2331834237', 'https://openalex.org/W2137279763', 'https://openalex.org/W2030931431', 'https://openalex.org/W2167834252', 'https://openalex.org/W2333916262', 'https://openalex.org/W2321033554', 'https://openalex.org/W2159772141', 'https://openalex.org/W205757930', 'https://openalex.org/W2082859819', 'https://openalex.org/W803770162']",2016-05-04
https://openalex.org/W2962799131,https://doi.org/10.21437/interspeech.2017-877,Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries,"In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.",[],2017-08-16
https://openalex.org/W3006094508,https://doi.org/10.1109/icassp40776.2020.9053053,Phoneme Boundary Detection Using Learnable Segmental Features,"Phoneme boundary detection plays an essential first step for a variety of speech processing applications such as speaker diarization, speech science, keyword spotting, etc. In this work, we propose a neural architecture coupled with a parameterized structured loss function to learn segmental representations for the task of phoneme boundary detection. First, we evaluated our model when the spoken phonemes were not given as input. Results on the TIMIT and Buckeye corpora suggest that the proposed model is superior to the baseline models and reaches state-of-the-art performance in terms of F1 and R-value. We further explore the use of phonetic transcription as additional supervision and show this yields minor improvements in performance but substantially better convergence rates. We additionally evaluate the model on a He-brew corpus and demonstrate such phonetic supervision can be beneficial in a multi-lingual setting.","['https://openalex.org/W2344888084', 'https://openalex.org/W2301095666', 'https://openalex.org/W2545132779', 'https://openalex.org/W6681658821', 'https://openalex.org/W6789826613', 'https://openalex.org/W2145410271', 'https://openalex.org/W2083904075', 'https://openalex.org/W2017814685', 'https://openalex.org/W2069732686', 'https://openalex.org/W1558402681', 'https://openalex.org/W2151244813', 'https://openalex.org/W130754613', 'https://openalex.org/W2077828209', 'https://openalex.org/W2096270777', 'https://openalex.org/W2116656681', 'https://openalex.org/W6684765037', 'https://openalex.org/W6738773844', 'https://openalex.org/W2542955097', 'https://openalex.org/W2098363562', 'https://openalex.org/W2074394031', 'https://openalex.org/W6637748604', 'https://openalex.org/W2124343046', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963795358', 'https://openalex.org/W6738767006', 'https://openalex.org/W6683825394', 'https://openalex.org/W6704862255', 'https://openalex.org/W2171752983', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963137467', 'https://openalex.org/W2620638943', 'https://openalex.org/W2291022022', 'https://openalex.org/W2145678789', 'https://openalex.org/W1761091820', 'https://openalex.org/W2343041606', 'https://openalex.org/W2161914416']",2020-04-09
https://openalex.org/W2964169922,,,"Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.","['https://openalex.org/W2048648518', 'https://openalex.org/W2758697525', 'https://openalex.org/W3102667484', 'https://openalex.org/W2951216052', 'https://openalex.org/W2117041980', 'https://openalex.org/W2114347655', 'https://openalex.org/W2962736743', 'https://openalex.org/W2463237750', 'https://openalex.org/W2686360660', 'https://openalex.org/W2719865699', 'https://openalex.org/W2641832364', 'https://openalex.org/W1796128977', 'https://openalex.org/W2291770225', 'https://openalex.org/W2295297373', 'https://openalex.org/W2020607164', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963311389', 'https://openalex.org/W2161562001', 'https://openalex.org/W2116330964', 'https://openalex.org/W2190506272', 'https://openalex.org/W113159538', 'https://openalex.org/W2962980711', 'https://openalex.org/W2032943813', 'https://openalex.org/W3098643042', 'https://openalex.org/W2963571336', 'https://openalex.org/W2483390977', 'https://openalex.org/W2251025892', 'https://openalex.org/W2154093685', 'https://openalex.org/W1577418252', 'https://openalex.org/W2566587499', 'https://openalex.org/W1778492285', 'https://openalex.org/W2072396742', 'https://openalex.org/W2143776582', 'https://openalex.org/W2022058071', 'https://openalex.org/W2468716020', 'https://openalex.org/W2059652594', 'https://openalex.org/W2100768664', 'https://openalex.org/W51277926', 'https://openalex.org/W2091746061', 'https://openalex.org/W2398490608', 'https://openalex.org/W2142775654', 'https://openalex.org/W1590183771', 'https://openalex.org/W2516890051', 'https://openalex.org/W2010188467', 'https://openalex.org/W2057007397', 'https://openalex.org/W2786608204']",
https://openalex.org/W2483390977,https://doi.org/10.1016/j.cognition.2017.11.008,Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner,,"['https://openalex.org/W2435103813', 'https://openalex.org/W2490982513', 'https://openalex.org/W6752889038', 'https://openalex.org/W6687566353', 'https://openalex.org/W6631811114', 'https://openalex.org/W1933349210', 'https://openalex.org/W2020607164', 'https://openalex.org/W2251803266', 'https://openalex.org/W3138934687', 'https://openalex.org/W2063303346', 'https://openalex.org/W2076493153', 'https://openalex.org/W6682841993', 'https://openalex.org/W588654549', 'https://openalex.org/W6678441659', 'https://openalex.org/W1984586950', 'https://openalex.org/W6680094886', 'https://openalex.org/W2407151108', 'https://openalex.org/W1911662473', 'https://openalex.org/W6675966832', 'https://openalex.org/W2164274485', 'https://openalex.org/W2134991647', 'https://openalex.org/W6679222651', 'https://openalex.org/W2161800831', 'https://openalex.org/W6745414609', 'https://openalex.org/W4234313254', 'https://openalex.org/W2084734691', 'https://openalex.org/W2035126305', 'https://openalex.org/W1982302451', 'https://openalex.org/W2027751800', 'https://openalex.org/W2046432510', 'https://openalex.org/W6697293080', 'https://openalex.org/W4245984049', 'https://openalex.org/W2011238950', 'https://openalex.org/W2110485445', 'https://openalex.org/W2046432185', 'https://openalex.org/W2107959623', 'https://openalex.org/W2000196122', 'https://openalex.org/W2057923756', 'https://openalex.org/W6648500443', 'https://openalex.org/W160318044', 'https://openalex.org/W6718561954', 'https://openalex.org/W2085297150', 'https://openalex.org/W2252172689', 'https://openalex.org/W6648374803', 'https://openalex.org/W2595479191', 'https://openalex.org/W2130518563', 'https://openalex.org/W2141038596', 'https://openalex.org/W1892018222', 'https://openalex.org/W2622671002', 'https://openalex.org/W2183182206', 'https://openalex.org/W2052262800', 'https://openalex.org/W2054704909', 'https://openalex.org/W1969005071', 'https://openalex.org/W2137967501', 'https://openalex.org/W2882319491', 'https://openalex.org/W2556930864', 'https://openalex.org/W2738724892', 'https://openalex.org/W2165545766', 'https://openalex.org/W2120321299', 'https://openalex.org/W1677182931', 'https://openalex.org/W2160815625', 'https://openalex.org/W2041394569', 'https://openalex.org/W2165345255', 'https://openalex.org/W2045495963', 'https://openalex.org/W2115912164', 'https://openalex.org/W2025482506', 'https://openalex.org/W1997125736', 'https://openalex.org/W2126953647', 'https://openalex.org/W2038056950', 'https://openalex.org/W2045343524', 'https://openalex.org/W2068116204', 'https://openalex.org/W2059824090', 'https://openalex.org/W6628771001', 'https://openalex.org/W2024899192', 'https://openalex.org/W1821584645', 'https://openalex.org/W2145056192', 'https://openalex.org/W2098500169', 'https://openalex.org/W2163605009', 'https://openalex.org/W1972159947', 'https://openalex.org/W2086880169', 'https://openalex.org/W2140661818', 'https://openalex.org/W1557379068', 'https://openalex.org/W1983578042', 'https://openalex.org/W6609936677', 'https://openalex.org/W2748742383', 'https://openalex.org/W2531882892', 'https://openalex.org/W2100768664', 'https://openalex.org/W6635469476', 'https://openalex.org/W2161002933', 'https://openalex.org/W2114156501', 'https://openalex.org/W2158653313', 'https://openalex.org/W2549835527', 'https://openalex.org/W2252142375', 'https://openalex.org/W2251025892', 'https://openalex.org/W6659825068', 'https://openalex.org/W2056323522', 'https://openalex.org/W6677594329', 'https://openalex.org/W2074488330', 'https://openalex.org/W2152496361', 'https://openalex.org/W2054948443', 'https://openalex.org/W2113153226', 'https://openalex.org/W2104752510', 'https://openalex.org/W2165627680', 'https://openalex.org/W2169991335', 'https://openalex.org/W2131070395', 'https://openalex.org/W1972102750', 'https://openalex.org/W2110221456', 'https://openalex.org/W1614298861', 'https://openalex.org/W2145339207', 'https://openalex.org/W1539935047', 'https://openalex.org/W2167834252', 'https://openalex.org/W2347098582', 'https://openalex.org/W2137918883', 'https://openalex.org/W2114347655', 'https://openalex.org/W2774845701', 'https://openalex.org/W4256214048', 'https://openalex.org/W2164747046', 'https://openalex.org/W6671047921', 'https://openalex.org/W2010188467', 'https://openalex.org/W2415378728', 'https://openalex.org/W2171849228', 'https://openalex.org/W2138410680', 'https://openalex.org/W6680143972', 'https://openalex.org/W4237938692', 'https://openalex.org/W2169578671', 'https://openalex.org/W1980862600', 'https://openalex.org/W2024534578', 'https://openalex.org/W2150820540', 'https://openalex.org/W2395899413', 'https://openalex.org/W2064135025', 'https://openalex.org/W2097515925', 'https://openalex.org/W2069048904', 'https://openalex.org/W6730578045', 'https://openalex.org/W2257979135', 'https://openalex.org/W6690317381', 'https://openalex.org/W2160783091', 'https://openalex.org/W1977811986', 'https://openalex.org/W2088110681', 'https://openalex.org/W2072364373', 'https://openalex.org/W2075208075', 'https://openalex.org/W2119165475', 'https://openalex.org/W2160464066', 'https://openalex.org/W1606268232', 'https://openalex.org/W2108443500', 'https://openalex.org/W1967307281', 'https://openalex.org/W2404799143', 'https://openalex.org/W4234079003', 'https://openalex.org/W2115099665', 'https://openalex.org/W2235661107', 'https://openalex.org/W6749972507', 'https://openalex.org/W2114831903', 'https://openalex.org/W2001771035', 'https://openalex.org/W1662133657', 'https://openalex.org/W2153767712', 'https://openalex.org/W2343593471', 'https://openalex.org/W2117041980', 'https://openalex.org/W2346964103', 'https://openalex.org/W6973666849', 'https://openalex.org/W1977531436', 'https://openalex.org/W2102040782', 'https://openalex.org/W4236521339', 'https://openalex.org/W2089883580', 'https://openalex.org/W2101509422', 'https://openalex.org/W2398830367', 'https://openalex.org/W2058616551', 'https://openalex.org/W2166571083', 'https://openalex.org/W2765364385', 'https://openalex.org/W4249427113', 'https://openalex.org/W2193413348', 'https://openalex.org/W2083148906', 'https://openalex.org/W2131273696', 'https://openalex.org/W2951493993', 'https://openalex.org/W2550821151', 'https://openalex.org/W2951279274', 'https://openalex.org/W4242257761', 'https://openalex.org/W1992638577', 'https://openalex.org/W2002103405', 'https://openalex.org/W1533806699', 'https://openalex.org/W2145482038', 'https://openalex.org/W1494198834', 'https://openalex.org/W2241947032', 'https://openalex.org/W2911978475', 'https://openalex.org/W4230637005', 'https://openalex.org/W1993750641', 'https://openalex.org/W4249920809', 'https://openalex.org/W2978172410', 'https://openalex.org/W4214717370', 'https://openalex.org/W2949382160', 'https://openalex.org/W2129705892', 'https://openalex.org/W2950761309', 'https://openalex.org/W2950005842', 'https://openalex.org/W635561569', 'https://openalex.org/W1993049588', 'https://openalex.org/W2963924008', 'https://openalex.org/W2727371918', 'https://openalex.org/W2442329935', 'https://openalex.org/W308248915', 'https://openalex.org/W2417262652', 'https://openalex.org/W2038185393', 'https://openalex.org/W2950416202', 'https://openalex.org/W2073022807', 'https://openalex.org/W1969885740', 'https://openalex.org/W579619532', 'https://openalex.org/W1508165687', 'https://openalex.org/W2063597751', 'https://openalex.org/W2103091632', 'https://openalex.org/W2740606810', 'https://openalex.org/W2004411856', 'https://openalex.org/W2121863487', 'https://openalex.org/W2159080219', 'https://openalex.org/W1534064252', 'https://openalex.org/W2038332531', 'https://openalex.org/W2328078142', 'https://openalex.org/W2101524054', 'https://openalex.org/W3098596645', 'https://openalex.org/W1568812407', 'https://openalex.org/W2153579005', 'https://openalex.org/W4210307751', 'https://openalex.org/W1524333225', 'https://openalex.org/W4238828888', 'https://openalex.org/W2070696251', 'https://openalex.org/W2011192906', 'https://openalex.org/W2914746235', 'https://openalex.org/W2233008733', 'https://openalex.org/W2026376994', 'https://openalex.org/W1581386830', 'https://openalex.org/W2162994165', 'https://openalex.org/W2963305465', 'https://openalex.org/W2460442863', 'https://openalex.org/W2778709969', 'https://openalex.org/W2101605059', 'https://openalex.org/W580971997', 'https://openalex.org/W2010315297', 'https://openalex.org/W2101196694', 'https://openalex.org/W1732736211', 'https://openalex.org/W2604132379', 'https://openalex.org/W2163028944', 'https://openalex.org/W50473013', 'https://openalex.org/W1532704504', 'https://openalex.org/W1706899115', 'https://openalex.org/W4298742451', 'https://openalex.org/W1480583224', 'https://openalex.org/W1740027839', 'https://openalex.org/W1602475114', 'https://openalex.org/W2519091744', 'https://openalex.org/W2111668269', 'https://openalex.org/W1778492285', 'https://openalex.org/W2026484633', 'https://openalex.org/W4285719527', 'https://openalex.org/W1493713542', 'https://openalex.org/W1974881914', 'https://openalex.org/W2055408826', 'https://openalex.org/W2786608204', 'https://openalex.org/W4254174115', 'https://openalex.org/W4252434862', 'https://openalex.org/W2949338531', 'https://openalex.org/W2168488947', 'https://openalex.org/W2125165590', 'https://openalex.org/W2105994703', 'https://openalex.org/W2111440402', 'https://openalex.org/W2025384410', 'https://openalex.org/W91514521', 'https://openalex.org/W2580178245', 'https://openalex.org/W2083367980', 'https://openalex.org/W1897139626', 'https://openalex.org/W2128359344', 'https://openalex.org/W4246559809', 'https://openalex.org/W1967924372', 'https://openalex.org/W2557283755', 'https://openalex.org/W1511986666', 'https://openalex.org/W4302803477', 'https://openalex.org/W2291963065', 'https://openalex.org/W1599016936', 'https://openalex.org/W2962753610', 'https://openalex.org/W2166206801', 'https://openalex.org/W2533523411', 'https://openalex.org/W4300988206', 'https://openalex.org/W2074546930', 'https://openalex.org/W4375819112', 'https://openalex.org/W2167543949', 'https://openalex.org/W2525778437', 'https://openalex.org/W2559169016', 'https://openalex.org/W4385773817', 'https://openalex.org/W2107917162', 'https://openalex.org/W4300721020', 'https://openalex.org/W1534728496', 'https://openalex.org/W1602422083', 'https://openalex.org/W4252994157', 'https://openalex.org/W2949640717', 'https://openalex.org/W4245122798', 'https://openalex.org/W2170716495', 'https://openalex.org/W1501555131', 'https://openalex.org/W2086426947', 'https://openalex.org/W1487155516', 'https://openalex.org/W2137735870', 'https://openalex.org/W2040300040', 'https://openalex.org/W1970208997', 'https://openalex.org/W2912805354', 'https://openalex.org/W2509570823', 'https://openalex.org/W3036063182', 'https://openalex.org/W182831726', 'https://openalex.org/W2798149936', 'https://openalex.org/W2328091329', 'https://openalex.org/W2728435982', 'https://openalex.org/W2108598243', 'https://openalex.org/W1532494781', 'https://openalex.org/W1832360048', 'https://openalex.org/W1917342115', 'https://openalex.org/W1932198206', 'https://openalex.org/W2312310973', 'https://openalex.org/W2541334028', 'https://openalex.org/W4388156388', 'https://openalex.org/W1993984071', 'https://openalex.org/W2950577311', 'https://openalex.org/W4300047444', 'https://openalex.org/W1922655562', 'https://openalex.org/W2963620343', 'https://openalex.org/W1506887983', 'https://openalex.org/W2134453531', 'https://openalex.org/W2398664438', 'https://openalex.org/W2963266252', 'https://openalex.org/W2398326651', 'https://openalex.org/W2164770604', 'https://openalex.org/W2078769636', 'https://openalex.org/W3083894204', 'https://openalex.org/W2161466446', 'https://openalex.org/W267509179', 'https://openalex.org/W2154095608', 'https://openalex.org/W2775670315', 'https://openalex.org/W2285479881', 'https://openalex.org/W4254816979', 'https://openalex.org/W2484448352', 'https://openalex.org/W2121114452', 'https://openalex.org/W4297159649', 'https://openalex.org/W2108622839', 'https://openalex.org/W1554540371', 'https://openalex.org/W72959365', 'https://openalex.org/W2094249282']",2018-01-08
https://openalex.org/W2404952642,,Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level,"Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level Okko Rasanen (okko.rasanen@aalto.fi) Department of Signal Processing and Acoustics, Aalto University PO Box 13000, 00076 Aalto, FINLAND Abstract Considerable effort has been put to understand how infants may utilize statistical regularities of speech in early word segmentation. Some studies suggest that infants are able to discover word boundaries at the points of high unpredictability across subsequent linguistic units such as phonemes or syllables. Meanwhile, the possible role of the statistical regularities in the temporal organization of the speech at a pre-linguistic acoustic level has not been widely addressed. The current work examines how the short-term temporal predictability of the acoustic speech signal correlates with linguistically motivated phone-, syllable-, and word-level units. The results indicate that the points of low predictability correlate mainly with the boundaries between phone-like segments. This suggests that the same statistical learning mechanisms hypothesized to operate at the word level can also aid in temporal organization of the speech stream into phone-like temporal segments before knowing the phonemic or syllabic units of the language. Keywords: distributional learning; language acquisition; phone segmentation; speech segmentation; statistical learning Introduction Segmentation of continuous speech into linguistically relevant units is essential for successful language acquisition (LA). Segmentation can take place at a number of levels, as the speech can be linguistically characterized in terms of units such as phones, syllables, and words, and with the latter always consisting of the former. In the early LA research, infants’ ability to segment words from speech has received a large amount of attention as the words are the main functional units of the language, standing for entities, events, actions, and states of the surrounding world. In the word segmentation studies, one of the major findings is that the infants can use statistical regularities in the speech input in order to discover boundaries between words (Saffran, Aslin & Newport, 1996). Also, these statistical learning mechanisms do not seem to be specific to words or even language faculty but operate across many levels of representation and perceptual domains (see, e.g., Romberg & Saffran, 2010, for a recent review). Importantly, a large body of the existing work on statistical word learning assumes that the infants are capable of representing speech input in terms of linguistically relevant units such as phones or syllables. Given the representational units, the infants are supposedly tracking transitional probabilities (TPs) between these units across time and use low-probability transitions as indications for word boundaries while the high-probability regions form representational units (Saffran et al., 1996). This strategy is valid as long as the TPs within words are higher than the TPs across word boundaries. However, the infant’s access to linguistic units such as phones or syllables and their statistics cannot be taken for granted. It is still unclear whether early adaptation to phonetic units drives lexical learning (c.f., NLM-e theory by Kuhl et al., 2008) or whether early lexical learning actually precedes, or at least parallels, the acquisition of sub-word representation of spoken language (e.g., Werker & Curtin, 2005). The “sub- word units –first” approach is challenged by the fact that the bottom-up organization of speech signal into temporally and categorically discrete units is far from trivial. Learning a phonetic or syllabic representation of the spoken language includes both the segmentation problem (division of the signal in time) and the categorization problem (assigning context-, talker-, and speaking style-dependent acoustic observations into a correct number of linguistic categories). Importantly, infants do not have access to any ground truth in either of the two tasks while learning the native language, suggesting that some speech-external factors such as feedback from lexical level or social interaction are required for successful learning. Still, it seems that even the basic problem of segmenting speech into sub-word units has been largely overlooked in the existing LA research. For example, it is unclear how well natural co-articulated speech can be segmented into sub-word units before learning the phonetic or lexical units of the language, and whether infants actually do such segmentation. Possibly the most concrete reference to early sub-word segmentation in the existing literature is the Kuhl’s concept of basic cuts: a perceptual mechanism that provides an initial low-level chunking of the speech stream into primitive phone-like units and which then gradually improves towards native language phone system through language exposure (Kuhl, 2004, and references therein). Segmentation into syllabic units is also central to many theories of LA (e.g., Jusczyk, 1993) although explicit and well-controlled studies on the segmentation process itself are few. In the speech engineering community, both phone- and syllable-level segmentation have been widely studied. The general finding is that the spectral changes (or “jumps”) in speech are good candidates for phone boundaries as they correlate with the changes in articulator positions (e.g., Almpanidis & Kotropulos, 2008; Esposito & Aversano, 2005; ten Bosch & Cranen, 2007; Scharenborg et al., 2007). On the other hand, it is known that syllabic segmentation","['https://openalex.org/W1911232923', 'https://openalex.org/W1589615572', 'https://openalex.org/W1993984071', 'https://openalex.org/W52412328', 'https://openalex.org/W2020944885', 'https://openalex.org/W3127686677', 'https://openalex.org/W2104752510', 'https://openalex.org/W2398858142', 'https://openalex.org/W2053952913', 'https://openalex.org/W2103091632', 'https://openalex.org/W2089883580', 'https://openalex.org/W2114777034', 'https://openalex.org/W135984148', 'https://openalex.org/W2010188467', 'https://openalex.org/W638114908', 'https://openalex.org/W1991274470', 'https://openalex.org/W1606268232', 'https://openalex.org/W2174992124']",2014-01-01
https://openalex.org/W2620638943,https://doi.org/10.5445/ir/1000166279,Phoneme Boundary Detection using Deep Bidirectional LSTMs,,"['https://openalex.org/W2146502635', 'https://openalex.org/W172543864', 'https://openalex.org/W2402366697', 'https://openalex.org/W2147568880', 'https://openalex.org/W2121997342', 'https://openalex.org/W2514012605', 'https://openalex.org/W1996580912', 'https://openalex.org/W2054665642', 'https://openalex.org/W2134202996', 'https://openalex.org/W2007055842', 'https://openalex.org/W2100768664', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963620343', 'https://openalex.org/W2250644439', 'https://openalex.org/W2110485445', 'https://openalex.org/W2102113734', 'https://openalex.org/W2026858810', 'https://openalex.org/W2150355110', 'https://openalex.org/W273093436', 'https://openalex.org/W2171631590', 'https://openalex.org/W2005708641', 'https://openalex.org/W2183815461']",2016-01-01
https://openalex.org/W2052697931,https://doi.org/10.1109/slt.2014.7078558,Phonetics embedding learning with side information,"We show that it is possible to learn an efficient acoustic model using only a<br>small amount of easily available word-level similarity annotations. In contrast<br>to the detailed phonetic labeling required by classical speech recognition<br>technologies, the only information our method requires are pairs of<br>speech excerpts which are known to be similar (same word) and pairs of<br>speech excerpts which are known to be different (different words). An acoustic model is obtained by training shallow and deep neural networks, using an<br>architecture and a cost function well-adapted to the nature of the provided information. The resulting model is evaluated on an ABX minimal-pair discrimination task and is shown to perform much better (11.8% ABX error<br>rate) than raw speech features (19.6%), not far from a fully supervised baseline (best neural network: 9.2%, HMM-GMM: 11%).","['https://openalex.org/W2162505970', 'https://openalex.org/W2035424729', 'https://openalex.org/W2143612262', 'https://openalex.org/W6670693114', 'https://openalex.org/W6908809', 'https://openalex.org/W2395899413', 'https://openalex.org/W2025482506', 'https://openalex.org/W6657703491', 'https://openalex.org/W6677328822', 'https://openalex.org/W30845872', 'https://openalex.org/W2252172689', 'https://openalex.org/W2054948443', 'https://openalex.org/W2136549906', 'https://openalex.org/W1993755070', 'https://openalex.org/W2114347655', 'https://openalex.org/W2153767712', 'https://openalex.org/W6602180557', 'https://openalex.org/W2078993594', 'https://openalex.org/W4285719527', 'https://openalex.org/W2171590421', 'https://openalex.org/W2117154949', 'https://openalex.org/W2407712691', 'https://openalex.org/W2029582325', 'https://openalex.org/W1576278180', 'https://openalex.org/W52412328', 'https://openalex.org/W2038056950', 'https://openalex.org/W2138621090', 'https://openalex.org/W2127589108', 'https://openalex.org/W2142152793', 'https://openalex.org/W1997460147']",2014-12-01
https://openalex.org/W2468716020,https://doi.org/10.1016/j.csl.2017.04.008,A segmental framework for fully-unsupervised large-vocabulary speech recognition,,"['https://openalex.org/W6712016426', 'https://openalex.org/W6655470396', 'https://openalex.org/W6712202099', 'https://openalex.org/W6676227212', 'https://openalex.org/W2157427027', 'https://openalex.org/W6712553779', 'https://openalex.org/W6677435873', 'https://openalex.org/W2044138293', 'https://openalex.org/W6602705600', 'https://openalex.org/W2067539019', 'https://openalex.org/W6602180557', 'https://openalex.org/W6645148572', 'https://openalex.org/W6602092770', 'https://openalex.org/W6675002571', 'https://openalex.org/W2126377586', 'https://openalex.org/W6658673440', 'https://openalex.org/W6712960331', 'https://openalex.org/W6656737381', 'https://openalex.org/W6641955892', 'https://openalex.org/W6664486393', 'https://openalex.org/W6632653590', 'https://openalex.org/W6640777149', 'https://openalex.org/W2295297373', 'https://openalex.org/W6649703416', 'https://openalex.org/W6687508285', 'https://openalex.org/W6675022971', 'https://openalex.org/W6638059883', 'https://openalex.org/W2030422732', 'https://openalex.org/W6665204316', 'https://openalex.org/W6634625101', 'https://openalex.org/W6691362072', 'https://openalex.org/W6713678193', 'https://openalex.org/W6696832885', 'https://openalex.org/W2058237037', 'https://openalex.org/W6680997062', 'https://openalex.org/W6684135220', 'https://openalex.org/W2114347655', 'https://openalex.org/W2145410271', 'https://openalex.org/W2010188467', 'https://openalex.org/W2398490608', 'https://openalex.org/W6638159135', 'https://openalex.org/W6677180724', 'https://openalex.org/W1539673959', 'https://openalex.org/W6719160716', 'https://openalex.org/W2078769636', 'https://openalex.org/W2032943813', 'https://openalex.org/W6663645435', 'https://openalex.org/W2220867547', 'https://openalex.org/W2404799143', 'https://openalex.org/W6677620606', 'https://openalex.org/W6704726871', 'https://openalex.org/W6973666849', 'https://openalex.org/W6655843320', 'https://openalex.org/W2507959295', 'https://openalex.org/W2516890051', 'https://openalex.org/W6712804684', 'https://openalex.org/W6659543308', 'https://openalex.org/W6670629611', 'https://openalex.org/W6682450205', 'https://openalex.org/W6607261905', 'https://openalex.org/W2099873701', 'https://openalex.org/W2401464865', 'https://openalex.org/W2915722758', 'https://openalex.org/W2251025892', 'https://openalex.org/W2612649659', 'https://openalex.org/W2400549570', 'https://openalex.org/W2407614114', 'https://openalex.org/W2107223151', 'https://openalex.org/W1967924372', 'https://openalex.org/W1503398984', 'https://openalex.org/W2116422968', 'https://openalex.org/W2116330964', 'https://openalex.org/W1942713348', 'https://openalex.org/W2166270474', 'https://openalex.org/W2140991203', 'https://openalex.org/W2057007397', 'https://openalex.org/W1796128977', 'https://openalex.org/W2022058071', 'https://openalex.org/W2396043527', 'https://openalex.org/W2154093685', 'https://openalex.org/W66167291', 'https://openalex.org/W3143835353', 'https://openalex.org/W1545920196', 'https://openalex.org/W1978741356', 'https://openalex.org/W2346964103', 'https://openalex.org/W2117041980', 'https://openalex.org/W2059652594', 'https://openalex.org/W52412328', 'https://openalex.org/W2025482506', 'https://openalex.org/W2805889152', 'https://openalex.org/W2020607164', 'https://openalex.org/W2033413759', 'https://openalex.org/W2190506272', 'https://openalex.org/W2035424729', 'https://openalex.org/W1997505733', 'https://openalex.org/W2291469555', 'https://openalex.org/W2395342389', 'https://openalex.org/W178496478', 'https://openalex.org/W2399576818', 'https://openalex.org/W2100768664', 'https://openalex.org/W2463237750', 'https://openalex.org/W1778492285', 'https://openalex.org/W2786608204', 'https://openalex.org/W1577418252', 'https://openalex.org/W2079460648', 'https://openalex.org/W2916018751', 'https://openalex.org/W51277926', 'https://openalex.org/W2052697931']",2017-05-18
https://openalex.org/W2025482506,https://doi.org/10.1109/icassp.2013.6639245,A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition,"We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.","['https://openalex.org/W2126377586', 'https://openalex.org/W2160306971', 'https://openalex.org/W2054948443', 'https://openalex.org/W1993660824', 'https://openalex.org/W2142152793', 'https://openalex.org/W1526995323', 'https://openalex.org/W2161952424', 'https://openalex.org/W2110073835', 'https://openalex.org/W6602180557', 'https://openalex.org/W6677207036', 'https://openalex.org/W2166391802', 'https://openalex.org/W2162638453', 'https://openalex.org/W2057007397', 'https://openalex.org/W2074546930', 'https://openalex.org/W2126449874', 'https://openalex.org/W6678998471', 'https://openalex.org/W6714100551', 'https://openalex.org/W1967924372', 'https://openalex.org/W1857273500', 'https://openalex.org/W2121947440', 'https://openalex.org/W2170580867', 'https://openalex.org/W6677461952', 'https://openalex.org/W30845872', 'https://openalex.org/W2114347655', 'https://openalex.org/W2406820985', 'https://openalex.org/W2401464865', 'https://openalex.org/W6607928866', 'https://openalex.org/W2117041980', 'https://openalex.org/W2062914951', 'https://openalex.org/W2142390309', 'https://openalex.org/W2399869768', 'https://openalex.org/W6676025551', 'https://openalex.org/W2126203737', 'https://openalex.org/W6675022971', 'https://openalex.org/W2114478143', 'https://openalex.org/W2407151108', 'https://openalex.org/W1779834323', 'https://openalex.org/W2126953647', 'https://openalex.org/W196692374', 'https://openalex.org/W2107959623', 'https://openalex.org/W3136512150', 'https://openalex.org/W2952343510', 'https://openalex.org/W2100768664', 'https://openalex.org/W2117786207', 'https://openalex.org/W52412328', 'https://openalex.org/W2117126688']",2013-05-01
https://openalex.org/W4297841320,https://doi.org/10.21437/interspeech.2022-52,A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS,"The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech.To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information.In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms.It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain.And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information.We conduct subjective tests to evaluate the proposed approach.Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity.When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder.Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multiscale discriminating.","['https://openalex.org/W2964243274', 'https://openalex.org/W3197273793', 'https://openalex.org/W2284050935', 'https://openalex.org/W3198213150', 'https://openalex.org/W2964153283', 'https://openalex.org/W3197294703', 'https://openalex.org/W3035687950', 'https://openalex.org/W3026874504', 'https://openalex.org/W2963971656', 'https://openalex.org/W2963929654', 'https://openalex.org/W4288265053', 'https://openalex.org/W2746654391', 'https://openalex.org/W4287083773', 'https://openalex.org/W2970006822', 'https://openalex.org/W1901129140', 'https://openalex.org/W3033411150', 'https://openalex.org/W2946200149', 'https://openalex.org/W3092028330', 'https://openalex.org/W2963609956', 'https://openalex.org/W4287111234', 'https://openalex.org/W2120847449', 'https://openalex.org/W2903739847', 'https://openalex.org/W2994143516', 'https://openalex.org/W3161296985']",2022-09-16
https://openalex.org/W3026874504,https://doi.org/10.48550/arxiv.2005.11129,Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search,"Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.","['https://openalex.org/W2463507112', 'https://openalex.org/W2788851830', 'https://openalex.org/W2911827218', 'https://openalex.org/W2963139417', 'https://openalex.org/W3015922793', 'https://openalex.org/W2127141656', 'https://openalex.org/W2962695743', 'https://openalex.org/W2963799213', 'https://openalex.org/W2125838338', 'https://openalex.org/W2970898247', 'https://openalex.org/W2769810959', 'https://openalex.org/W2964121744', 'https://openalex.org/W2767206889', 'https://openalex.org/W2794490148', 'https://openalex.org/W1583912456', 'https://openalex.org/W3025528898', 'https://openalex.org/W2903739847', 'https://openalex.org/W2970351109', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963712897', 'https://openalex.org/W2970730223', 'https://openalex.org/W2963927338', 'https://openalex.org/W2985856318', 'https://openalex.org/W2963403868', 'https://openalex.org/W3035083561', 'https://openalex.org/W2963300588', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963691546', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963925437', 'https://openalex.org/W2962882868', 'https://openalex.org/W2111284386']",2020-05-22
https://openalex.org/W2517788403,https://doi.org/10.21437/interspeech.2016-584,Objective Evaluation Using Association Between Dimensions Within Spectral Features for Statistical Parametric Speech Synthesis,,"['https://openalex.org/W2294797155', 'https://openalex.org/W2407028783', 'https://openalex.org/W2396043161', 'https://openalex.org/W2100140000', 'https://openalex.org/W2000513720', 'https://openalex.org/W2165700458', 'https://openalex.org/W2049686551', 'https://openalex.org/W2120605154']",2016-08-29
https://openalex.org/W2963971656,https://doi.org/10.1109/taslp.2017.2761547,Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks,"A method for statistical parametric speech synthesis incorporating generative adversarial networks (GANs) is proposed. Although powerful deep neural networks techniques can be applied to artificially synthesize speech waveform, the synthetic speech quality is low compared with that of natural speech. One of the issues causing the quality degradation is an oversmoothing effect often observed in the generated speech parameters. A GAN introduced in this paper consists of two neural networks: a discriminator to distinguish natural and generated samples, and a generator to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator is trained to distinguish natural and generated speech parameters, while the acoustic models are trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence (i.e., distribution difference) between the natural and generated speech parameters, the proposed method effectively alleviates the oversmoothing effect on the generated speech parameters. We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed method can generate more natural spectral parameters and F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> than conventional minimum generation error training algorithm regardless of its hyperparameter settings. Furthermore, we investigated the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms of improving the synthetic speech quality.","['https://openalex.org/W6712820016', 'https://openalex.org/W2023694213', 'https://openalex.org/W2282821441', 'https://openalex.org/W2100495367', 'https://openalex.org/W1576227399', 'https://openalex.org/W6713645886', 'https://openalex.org/W6682889407', 'https://openalex.org/W2165700458', 'https://openalex.org/W6732248266', 'https://openalex.org/W6629354409', 'https://openalex.org/W4233762729', 'https://openalex.org/W4293404332', 'https://openalex.org/W6712112783', 'https://openalex.org/W2115040572', 'https://openalex.org/W2129142580', 'https://openalex.org/W2029434926', 'https://openalex.org/W6717434760', 'https://openalex.org/W58497106', 'https://openalex.org/W6734564793', 'https://openalex.org/W6779669310', 'https://openalex.org/W2055070149', 'https://openalex.org/W6680012447', 'https://openalex.org/W6864847698', 'https://openalex.org/W2406654659', 'https://openalex.org/W2748379347', 'https://openalex.org/W6697322189', 'https://openalex.org/W2517788403', 'https://openalex.org/W2404100688', 'https://openalex.org/W6730746255', 'https://openalex.org/W2293049663', 'https://openalex.org/W1517202054', 'https://openalex.org/W2403471241', 'https://openalex.org/W6714093102', 'https://openalex.org/W6675380101', 'https://openalex.org/W2294013337', 'https://openalex.org/W6712941293', 'https://openalex.org/W2043003570', 'https://openalex.org/W2156142001', 'https://openalex.org/W2120605154', 'https://openalex.org/W2111284386', 'https://openalex.org/W2274854232', 'https://openalex.org/W6676358011', 'https://openalex.org/W2049686551', 'https://openalex.org/W2473388484', 'https://openalex.org/W6607663849', 'https://openalex.org/W6631309588', 'https://openalex.org/W6711777497', 'https://openalex.org/W2666408839', 'https://openalex.org/W2184310502', 'https://openalex.org/W2338186431', 'https://openalex.org/W2964301388', 'https://openalex.org/W2156387975', 'https://openalex.org/W2295634712', 'https://openalex.org/W2964024144', 'https://openalex.org/W2407039802', 'https://openalex.org/W2111194146', 'https://openalex.org/W2135029798', 'https://openalex.org/W4320013936', 'https://openalex.org/W3123963976', 'https://openalex.org/W385466589', 'https://openalex.org/W2402103843', 'https://openalex.org/W2405756170', 'https://openalex.org/W2099471712', 'https://openalex.org/W4395958265', 'https://openalex.org/W2102003408', 'https://openalex.org/W1487641199', 'https://openalex.org/W2519091744', 'https://openalex.org/W2055309977', 'https://openalex.org/W2963800509', 'https://openalex.org/W2395578248', 'https://openalex.org/W2396990910', 'https://openalex.org/W2401839215', 'https://openalex.org/W2593414223', 'https://openalex.org/W3037567775', 'https://openalex.org/W2419501139', 'https://openalex.org/W187033940', 'https://openalex.org/W2577946330', 'https://openalex.org/W1523372075']",2017-10-09
https://openalex.org/W2039225946,https://doi.org/10.1109/icmla.2009.48,Learning Deep Neural Networks for High Dimensional Output Problems,"State-of-the-art pattern recognition methods have difficulties dealing with problems where the dimension of the output space is large. In this article, we propose a framework based on deep architectures (e. g. Deep Neural Networks) in order to deal with this issue. Deep architectures have proven to be efficient for high dimensional input problems such as image classification, due to their ability to embed the input space. The main contribution of this article is the extension of the embedding procedure to both the input and output spaces to easily handle complex outputs. Using this extension, inter-output dependencies can be modelled efficiently. This provides an interesting alternative to probabilistic models such as HMM and CRF. Preliminary experiments on toy datasets and USPS character reconstruction show promising results.","['https://openalex.org/W2136922672', 'https://openalex.org/W6676481782', 'https://openalex.org/W2159291644', 'https://openalex.org/W6680887930', 'https://openalex.org/W2101037815', 'https://openalex.org/W2097581247', 'https://openalex.org/W2074392786', 'https://openalex.org/W2158619730', 'https://openalex.org/W6675357706', 'https://openalex.org/W6633362792', 'https://openalex.org/W2125838338', 'https://openalex.org/W2146156528', 'https://openalex.org/W6631399749', 'https://openalex.org/W2140833774', 'https://openalex.org/W1520377376', 'https://openalex.org/W2103194807', 'https://openalex.org/W2799061466', 'https://openalex.org/W1555711139', 'https://openalex.org/W2110798204', 'https://openalex.org/W1554544485', 'https://openalex.org/W4244494905', 'https://openalex.org/W1525954826']",2009-12-01
https://openalex.org/W1680622244,https://doi.org/10.1007/11494669_93,The Curse of Dimensionality in Data Mining and Time Series Prediction,,"['https://openalex.org/W4293747882', 'https://openalex.org/W1968263491', 'https://openalex.org/W4237171445', 'https://openalex.org/W4233014035', 'https://openalex.org/W1672197616', 'https://openalex.org/W2165533158', 'https://openalex.org/W2121122425', 'https://openalex.org/W2140095548', 'https://openalex.org/W2134312057', 'https://openalex.org/W2169507824', 'https://openalex.org/W2001141328', 'https://openalex.org/W4213332169', 'https://openalex.org/W2122538988', 'https://openalex.org/W2121594221', 'https://openalex.org/W2029401646', 'https://openalex.org/W2020421747', 'https://openalex.org/W2124314692', 'https://openalex.org/W3023537567', 'https://openalex.org/W2171029115', 'https://openalex.org/W1679913846', 'https://openalex.org/W2110847962', 'https://openalex.org/W2129905273', 'https://openalex.org/W2569586013', 'https://openalex.org/W1552694902', 'https://openalex.org/W1754759919', 'https://openalex.org/W3023807737', 'https://openalex.org/W1595303882', 'https://openalex.org/W2767905780', 'https://openalex.org/W2155130690']",2005-01-01
https://openalex.org/W3197273793,https://doi.org/10.21437/interspeech.2021-1016,UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation,"Most neural vocoders employ band-limited mel-spectrograms to generate waveforms.If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible.However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated.To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time.Inspired by works in the field of voice activity detection, we added a multiresolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets.Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input.In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers.These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.","['https://openalex.org/W2197404611', 'https://openalex.org/W4252713891', 'https://openalex.org/W2049686551', 'https://openalex.org/W2972359262', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963300588', 'https://openalex.org/W2120847449', 'https://openalex.org/W3015338123', 'https://openalex.org/W3150572638', 'https://openalex.org/W1552314771', 'https://openalex.org/W3092028330', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964243274', 'https://openalex.org/W3161236344', 'https://openalex.org/W3033411150', 'https://openalex.org/W3130774171', 'https://openalex.org/W2593414223', 'https://openalex.org/W2993118648', 'https://openalex.org/W2423557781', 'https://openalex.org/W4298580827', 'https://openalex.org/W2471520273', 'https://openalex.org/W3144035034', 'https://openalex.org/W2889329491', 'https://openalex.org/W3096442195', 'https://openalex.org/W2284050935', 'https://openalex.org/W2519091744', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963975282']",2021-08-27
https://openalex.org/W2946200149,https://doi.org/10.48550/arxiv.1905.09263,"FastSpeech: Fast, Robust and Controllable Text to Speech","Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.","['https://openalex.org/W2463507112', 'https://openalex.org/W2767206889', 'https://openalex.org/W2962936105', 'https://openalex.org/W2962969034', 'https://openalex.org/W2608207374', 'https://openalex.org/W2591927543', 'https://openalex.org/W2471520273', 'https://openalex.org/W2327501763', 'https://openalex.org/W3038172701', 'https://openalex.org/W2519091744', 'https://openalex.org/W2120847449', 'https://openalex.org/W2952711665', 'https://openalex.org/W2945613576', 'https://openalex.org/W2964265128', 'https://openalex.org/W2892140764', 'https://openalex.org/W2766812927', 'https://openalex.org/W2598638573', 'https://openalex.org/W2769810959', 'https://openalex.org/W2150658333', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964243274', 'https://openalex.org/W648786980', 'https://openalex.org/W2963536265', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963403868']",2019-05-22
https://openalex.org/W2894176037,https://doi.org/10.1007/978-3-030-01261-8_28,Triplet Loss in Siamese Network for Object Tracking,,"['https://openalex.org/W2964111344', 'https://openalex.org/W2470394683', 'https://openalex.org/W2467139031', 'https://openalex.org/W2557641257', 'https://openalex.org/W1997121481', 'https://openalex.org/W1955741794', 'https://openalex.org/W2518013266', 'https://openalex.org/W2798520605', 'https://openalex.org/W2556556019', 'https://openalex.org/W2964099559', 'https://openalex.org/W161114242', 'https://openalex.org/W2154889144', 'https://openalex.org/W6600710366', 'https://openalex.org/W2963775347', 'https://openalex.org/W6679027886', 'https://openalex.org/W2916780012', 'https://openalex.org/W2158827467', 'https://openalex.org/W818325216', 'https://openalex.org/W1857884451', 'https://openalex.org/W2474599091', 'https://openalex.org/W2117539524', 'https://openalex.org/W2096733369', 'https://openalex.org/W2788352579', 'https://openalex.org/W2768024897', 'https://openalex.org/W2963026686', 'https://openalex.org/W2408241409', 'https://openalex.org/W2962824803', 'https://openalex.org/W1963882359', 'https://openalex.org/W2471048925', 'https://openalex.org/W2089961441', 'https://openalex.org/W2158592639', 'https://openalex.org/W1995903777', 'https://openalex.org/W182940129', 'https://openalex.org/W2742165450', 'https://openalex.org/W2964076257', 'https://openalex.org/W2917435394', 'https://openalex.org/W3102624093', 'https://openalex.org/W2130026429', 'https://openalex.org/W3099206234', 'https://openalex.org/W2598634450', 'https://openalex.org/W2106053110']",2018-01-01
https://openalex.org/W3092028330,https://doi.org/10.48550/arxiv.2010.05646,HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,"Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.","['https://openalex.org/W2792995953', 'https://openalex.org/W2903739847', 'https://openalex.org/W2999160446', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963139417', 'https://openalex.org/W3015338123', 'https://openalex.org/W2527729766', 'https://openalex.org/W2963300588', 'https://openalex.org/W2963782041', 'https://openalex.org/W2248556341', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950541952', 'https://openalex.org/W2963685250', 'https://openalex.org/W2785678896', 'https://openalex.org/W2964243274', 'https://openalex.org/W2949382160', 'https://openalex.org/W3034971973', 'https://openalex.org/W2883853252', 'https://openalex.org/W2963073614', 'https://openalex.org/W2950299304', 'https://openalex.org/W2975414524', 'https://openalex.org/W2964167449', 'https://openalex.org/W2593414223']",2020-10-12
https://openalex.org/W2903739847,https://doi.org/10.1609/aaai.v33i01.33016706,Neural Speech Synthesis with Transformer Network,"Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).","['https://openalex.org/W1599623585', 'https://openalex.org/W2148228080', 'https://openalex.org/W2157331557', 'https://openalex.org/W6623517193', 'https://openalex.org/W2120847449', 'https://openalex.org/W6666761814', 'https://openalex.org/W2150658333', 'https://openalex.org/W2587284713', 'https://openalex.org/W6756197946', 'https://openalex.org/W6679436768', 'https://openalex.org/W2766557690', 'https://openalex.org/W2154920538', 'https://openalex.org/W6676641785', 'https://openalex.org/W2168510624', 'https://openalex.org/W1991133427', 'https://openalex.org/W2102003408', 'https://openalex.org/W6679146927', 'https://openalex.org/W2129142580', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962778134', 'https://openalex.org/W2604184139', 'https://openalex.org/W2613904329', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963850025', 'https://openalex.org/W2519091744', 'https://openalex.org/W4301368689', 'https://openalex.org/W4298857617', 'https://openalex.org/W2964243274', 'https://openalex.org/W4294619240', 'https://openalex.org/W2130942839', 'https://openalex.org/W2804078698', 'https://openalex.org/W4385245566', 'https://openalex.org/W2901997113', 'https://openalex.org/W2584032004', 'https://openalex.org/W854541894', 'https://openalex.org/W2111284386']",2019-07-17
https://openalex.org/W4200219715,https://doi.org/10.1145/3461615.3491114,TeNC: Low Bit-Rate Speech Coding with VQ-VAE and GAN,"Speech coding aims at compressing digital speech signals with fewer bits and reconstructing it back to raw signals, maintaining the speech quality as much as possible. But conventional codecs usually need a high bit-rate to achieve reconstructed speech with reasonable high quality. In this paper, we propose an end-to-end neural generative codec with a VQ-VAE based auto-encoder and the generative adversarial network (GAN), which achieves reconstructed speech with high-fidelity at a low bit-rate about 2 kb/s. The compression process of speech coding is carried out by a down-sampling module of the encoder and a learnable discrete codebook. GAN is used to further improve the reconstructed quality. Our experiments confirm the effectiveness of the proposed model in both objective and subjective tests, which significantly outperforms the conventional codecs at low bit-rate in terms of speech quality and speaker similarity.","['https://openalex.org/W3160576174', 'https://openalex.org/W3016003977', 'https://openalex.org/W2935711438', 'https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W2593414223', 'https://openalex.org/W2151409182', 'https://openalex.org/W2108532241', 'https://openalex.org/W2020883660', 'https://openalex.org/W1494198834', 'https://openalex.org/W1552314771', 'https://openalex.org/W1901129140', 'https://openalex.org/W3096468295', 'https://openalex.org/W3199364993', 'https://openalex.org/W2963091184', 'https://openalex.org/W2972519044', 'https://openalex.org/W3015338123', 'https://openalex.org/W3144035034']",2021-10-18
https://openalex.org/W2937909162,https://doi.org/10.1109/icassp.2019.8682861,Enhancing Hybrid Self-attention Structure with Relative-position-aware Bias for Speech Synthesis,"Compared with the conventional ""front-end""-""back-end""- ""vocoder"" structure, based on the attention mechanism, end-to-end speech synthesis systems directly train and synthesize from text sequence to the acoustic feature sequence as a whole. Recently, a more calculation efficient end-to-end architecture named transformer, which is solely based on self-attention, was proposed to model global dependencies between the input and output sequences. However, although with many advantages, transformer lacks position information in its structure. Moreover, the weighted sum form in self-attention may disperse the attention to the whole input sequence other than focusing on the more important neighbouring positions. In order to solve the above problems, this paper introduces a hybrid self-attention structure which combines self-attention with the recurrent neural networks (RNNs). We further enhance the proposed structure with relative-position-aware biases. Mean opinion score (MOS) test results indicate that by enhancing hybrid self-attention structure with relative-position-aware biases, the proposed system achieves the best performance with only 0.11 MOS score lower than natural recording.","['https://openalex.org/W6745697700', 'https://openalex.org/W6737778391', 'https://openalex.org/W6739901393', 'https://openalex.org/W6754925833', 'https://openalex.org/W2964302946', 'https://openalex.org/W2963925437', 'https://openalex.org/W2962911926', 'https://openalex.org/W6753640285', 'https://openalex.org/W2964045208', 'https://openalex.org/W6679436768', 'https://openalex.org/W2043003570', 'https://openalex.org/W2515943672', 'https://openalex.org/W6679434410', 'https://openalex.org/W2963609956', 'https://openalex.org/W6756197946', 'https://openalex.org/W2102003408', 'https://openalex.org/W3142087749', 'https://openalex.org/W2767052532', 'https://openalex.org/W1902237438', 'https://openalex.org/W6746700228', 'https://openalex.org/W2749651610', 'https://openalex.org/W2613904329', 'https://openalex.org/W2949382160', 'https://openalex.org/W2959575783', 'https://openalex.org/W2964243274', 'https://openalex.org/W2133564696', 'https://openalex.org/W2892140764', 'https://openalex.org/W2766812927', 'https://openalex.org/W3026414669', 'https://openalex.org/W2884852625', 'https://openalex.org/W2130942839', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963691546', 'https://openalex.org/W2963403868', 'https://openalex.org/W2866343820', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2903739847', 'https://openalex.org/W2933485668', 'https://openalex.org/W4385245566', 'https://openalex.org/W2901997113']",2019-04-17
https://openalex.org/W2191179271,https://doi.org/10.1016/j.patcog.2015.11.015,Overfitting in linear feature extraction for classification of high-dimensional image data,,"['https://openalex.org/W2132549764', 'https://openalex.org/W7299809', 'https://openalex.org/W1840338487', 'https://openalex.org/W2002645541', 'https://openalex.org/W2107542203', 'https://openalex.org/W2138153039', 'https://openalex.org/W2213612645', 'https://openalex.org/W6677603835', 'https://openalex.org/W6682981795', 'https://openalex.org/W2114367267', 'https://openalex.org/W1969204685', 'https://openalex.org/W2080591292', 'https://openalex.org/W1991605728', 'https://openalex.org/W2151821220', 'https://openalex.org/W1982434523', 'https://openalex.org/W7066667914', 'https://openalex.org/W2119387367', 'https://openalex.org/W2122599407', 'https://openalex.org/W2131987814', 'https://openalex.org/W2154053567', 'https://openalex.org/W6680765659', 'https://openalex.org/W6675969814', 'https://openalex.org/W2001141328', 'https://openalex.org/W2119290563', 'https://openalex.org/W6677267524', 'https://openalex.org/W1974097586', 'https://openalex.org/W2056024692', 'https://openalex.org/W2105055468', 'https://openalex.org/W2140095548', 'https://openalex.org/W2132822263', 'https://openalex.org/W2106115875', 'https://openalex.org/W2165786903', 'https://openalex.org/W2140833774', 'https://openalex.org/W2137570937', 'https://openalex.org/W4213090421', 'https://openalex.org/W1663973292', 'https://openalex.org/W4285719527', 'https://openalex.org/W2154776925', 'https://openalex.org/W1519405745', 'https://openalex.org/W1506806321', 'https://openalex.org/W2998216295', 'https://openalex.org/W2911433325', 'https://openalex.org/W1770825568', 'https://openalex.org/W2103333826', 'https://openalex.org/W2116882733', 'https://openalex.org/W2032407804', 'https://openalex.org/W2138570191', 'https://openalex.org/W2116801843', 'https://openalex.org/W1601795611', 'https://openalex.org/W2135346934', 'https://openalex.org/W1773771211']",2015-12-02
https://openalex.org/W2972702018,https://doi.org/10.21437/interspeech.2019-1972,Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS,"Neural TTS has demonstrated strong capabilities to generate human-like speech with high quality and naturalness, while its generalization to out-of-domain texts is still a challenging task, with regard to the design of attention-based sequence-tosequence acoustic modeling.Various errors occur in those inputs with unseen context, including attention collapse, skipping, repeating, etc., which limits the broader applications.In this paper, we propose a novel stepwise monotonic attention method in sequence-to-sequence acoustic modeling to improve the robustness on out-of-domain inputs.The method utilizes the strict monotonic property in TTS with constraints on monotonic hard attention that the alignments between inputs and outputs sequence must be not only monotonic but allowing no skipping on inputs.Soft attention could be used to evade mismatch between training and inference.The experimental results show that the proposed method could achieve significant improvements in robustness on out-of-domain scenarios for phoneme-based models, without any regression on the in-domain naturalness test.","['https://openalex.org/W2625720409', 'https://openalex.org/W1514535095', 'https://openalex.org/W1810943226', 'https://openalex.org/W2767601419', 'https://openalex.org/W2253795368', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964243274', 'https://openalex.org/W2133564696', 'https://openalex.org/W2745439869', 'https://openalex.org/W2619122421', 'https://openalex.org/W2125308790', 'https://openalex.org/W2758584285', 'https://openalex.org/W2767052532', 'https://openalex.org/W2963691546', 'https://openalex.org/W2519091744', 'https://openalex.org/W2886769154', 'https://openalex.org/W2963609956', 'https://openalex.org/W2605141709', 'https://openalex.org/W854541894', 'https://openalex.org/W2890909908', 'https://openalex.org/W2178654303']",2019-09-13
https://openalex.org/W3095277453,https://doi.org/10.21437/interspeech.2020-2189,Reformer-TTS: Neural Speech Synthesis with Reformer Network,,"['https://openalex.org/W2767052532', 'https://openalex.org/W2970730223', 'https://openalex.org/W2591927543', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963609956', 'https://openalex.org/W2129142580', 'https://openalex.org/W2964243274', 'https://openalex.org/W2079182758', 'https://openalex.org/W2886769154', 'https://openalex.org/W2154920538', 'https://openalex.org/W2194775991']",2020-10-25
https://openalex.org/W1919801718,https://doi.org/10.1109/icassp.1978.1170404,Automatic recognition of continuously spoken sentences from a finite state grammer,"We report performance results on the recognition of continuously spoken sentences from the finite state grammar for the ""New Raleigh Language"" (vocabulary-250 words; average sentence length-8 words; entropy-2.86 bits/word; perplexity-7.27 words). Sentence and word error rates of 5% and 0.6% , respectively, are achieved, using a new centisecond-level model for the acoustic processor. We also report results for the ""CMU-AIX05 Language"" (vocabulary-1011 words; average sentence length-about 7 words; entropy-2.18 bits/word; perplexity-4.53 words), using both our earlier phone-level model and the centisecond-level model. With the phone-level acoustic-processor model, sentence and word error rates of 2% and 0.8%, respectively, are achieved. With the centisecond-level model, sentence and word error rates are 1% and 0.1%, respectively.","['https://openalex.org/W2134587001', 'https://openalex.org/W2037543635', 'https://openalex.org/W2157477135', 'https://openalex.org/W2058373514', 'https://openalex.org/W2042083804']",2005-03-24
https://openalex.org/W2169248114,https://doi.org/10.1109/icassp.1977.1170183,Evaluation of a word recognition system using syntax analysis,"A speech recognition system has been implemented which accepts reasonably natural English sentences spoken as isolated words. The major components of the system are a speaker dependent word recognizer and a syntax analyzer. The set of sentences selected for investigation is intended for use as requests in an automated flight information and reservation system. Results are presented of evaluations for speakers using their own stored reference patterns, the reference patterns of other speakers and reference patterns averaged over several speakers. For speakers using their own reference pattern the median word recognition error rate fell from 11.7% to 0.4% with the use of syntax analysis.","['https://openalex.org/W1713311714', 'https://openalex.org/W2000905214', 'https://openalex.org/W2137089646']",2005-03-24
https://openalex.org/W1884432169,https://doi.org/10.1109/icassp.1986.1168963,Network-based connected digit recognition using explicit acoustic-phonetic modeling,"This paper describes a system for speaker-independent connected digit recognition in which explicit acoustic-phonetic features and constraints play a significant role. The digit vocabulary is modeled using a finite-state pronunciation network whose branches correspond to meaningful acoustic-phonetic units. Each branch is associated with an acoustic pattern matcher which employs a combination of whole-spectrum and feature-based metrics. The system has been evaluated using 8621 utterances from the Texas Instruments (TI) multi-dialect, connected digits database. The best configurations of the recognizer achieve string recognition accuracies of 96.5% and 97.8% when the length of the input string is unknown and known, respectively, and when different talkers are used for training and testing.","['https://openalex.org/W2112197391', 'https://openalex.org/W2152131029', 'https://openalex.org/W2104298309', 'https://openalex.org/W2012313369', 'https://openalex.org/W2585800450', 'https://openalex.org/W2170585560']",2005-03-24
https://openalex.org/W2152131029,https://doi.org/10.1109/icassp.1984.1172716,A database for speaker-independent digit recognition,"A large speech database has been collected for use in designing and evaluating algorithms for speaker independent recognition of connected digit sequences. This dialect balanced database consists of more than 25 thousand digit sequences spoken by over 300 men, women, and children. The data were collected in a quiet environment and digitized at 20 KHz. Formal human listening tests on this database provided certification of the labelling of the digit sequences, and also provided information about human recognition performance and the inherent recognizability of the data.",['https://openalex.org/W2111593200'],2005-03-24
https://openalex.org/W1989337816,https://doi.org/10.1109/tassp.1976.1162849,Distance measures for speech processing,"The properties and interrelationships among four measures of distance in speech processing are theoretically and experimentally discussed. The root mean square (rms) log spectral distance, cepstral distance, likelihood ratio (minimum residual principle or delta coding (DELCO) algorithm), and a cosh measure (based upon two nonsymmetrical likelihood ratios) are considered. It is shown that the cepstral measure bounds the rms log spectral measure from below, while the cosh measure bounds it from above. A simple nonlinear transformation of the likelihood ratio is shown to be highly correlated with the rms log spectral measure over expected ranges. Relationships between distance measure values and perception are also considered. The likelihood ratio, cepstral measure, and cosh measure are easily evaluated recursively from linear prediction filter coefficients, and each has a meaningful and interrelated frequency domain interpretation. Fortran programs are presented for computing the recursively evaluated distance measures.","['https://openalex.org/W2129015742', 'https://openalex.org/W6779487094', 'https://openalex.org/W2063739690', 'https://openalex.org/W1536990986', 'https://openalex.org/W1982624800', 'https://openalex.org/W1983049966', 'https://openalex.org/W2022554507', 'https://openalex.org/W2050693797', 'https://openalex.org/W2137089646', 'https://openalex.org/W2032857037', 'https://openalex.org/W3035139526', 'https://openalex.org/W2802653578']",1976-10-01
https://openalex.org/W2048648518,https://doi.org/10.1109/tassp.1985.1164581,A modified K-means clustering algorithm for use in isolated work recognition,"Abstract-Studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the per-formance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophis-ticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but was impossible to reproduce exactly because it was highly dependent on decisions made by the experimenter. Subsequent work led to an auto-matic clustering procedure which, given only a set of clustering param-eters, clustered patterns with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since a naive user of such a statistical clus-tering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recogni-tion system. It is the purpose of this paper to present a clustering al-gorithm based on a standard K-means approach which requires no user parameter specification. Experimental data show that this new algo-rithm performs as well or better than the previously used clustering techniques when tested as part of a speaker-independent isolated word recognition system. P I.","['https://openalex.org/W2150418026', 'https://openalex.org/W2150593711', 'https://openalex.org/W1826405859', 'https://openalex.org/W1813508333', 'https://openalex.org/W2112465852', 'https://openalex.org/W2046316417', 'https://openalex.org/W2022554507', 'https://openalex.org/W2137089646', 'https://openalex.org/W2072054026', 'https://openalex.org/W6602235367', 'https://openalex.org/W2126560900', 'https://openalex.org/W2077679389', 'https://openalex.org/W6678914141', 'https://openalex.org/W4252388808', 'https://openalex.org/W2163904446', 'https://openalex.org/W2134383396', 'https://openalex.org/W1968191505', 'https://openalex.org/W2123783347', 'https://openalex.org/W2138054245', 'https://openalex.org/W2166390283', 'https://openalex.org/W2132776552', 'https://openalex.org/W2050703968', 'https://openalex.org/W54230203', 'https://openalex.org/W2127218421']",1985-06-01
https://openalex.org/W2105594594,https://doi.org/10.1109/massp.1986.1165342,An introduction to hidden Markov models,"The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.","['https://openalex.org/W2077574412', 'https://openalex.org/W2086699924', 'https://openalex.org/W2020127877', 'https://openalex.org/W2112197391', 'https://openalex.org/W2011471577', 'https://openalex.org/W1998379597', 'https://openalex.org/W1796349162', 'https://openalex.org/W2139576349', 'https://openalex.org/W2022554507', 'https://openalex.org/W2021760654', 'https://openalex.org/W2171850596', 'https://openalex.org/W2165253089', 'https://openalex.org/W1990005915', 'https://openalex.org/W2163929346', 'https://openalex.org/W2142384583']",1986-01-01
https://openalex.org/W2065625684,https://doi.org/10.1109/mspec.1981.6369809,Computers: Speech recognition: Turning theory to practice: New ICs have brought the requisite computer power to speech technology; an evaluation of equipment shows where it stands today,Presents an evaluation of the equipment now available for turning the theory of electronic speech recognition into practice. The fulfilment of this goal seems much closer than it did because of the pace of advance in IC technology.,[],1981-09-01
https://openalex.org/W1990005915,https://doi.org/10.1109/proc.1976.10159,Continuous speech recognition by statistical methods,"Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.","['https://openalex.org/W2142384583', 'https://openalex.org/W1991133427', 'https://openalex.org/W2045407304', 'https://openalex.org/W2007321142', 'https://openalex.org/W2086699924', 'https://openalex.org/W2134587001', 'https://openalex.org/W2157477135', 'https://openalex.org/W2042083804', 'https://openalex.org/W1983520516', 'https://openalex.org/W1989226853', 'https://openalex.org/W2056536278', 'https://openalex.org/W2013238308', 'https://openalex.org/W1971292102', 'https://openalex.org/W2035227369', 'https://openalex.org/W1969483458', 'https://openalex.org/W4255791207', 'https://openalex.org/W1562979145', 'https://openalex.org/W2313819769', 'https://openalex.org/W2172256532', 'https://openalex.org/W1989008275', 'https://openalex.org/W158805393', 'https://openalex.org/W2142901448']",1976-01-01
https://openalex.org/W1975598412,https://doi.org/10.1109/proc.1985.13344,Structural methods in automatic speech recognition,"The past decade has witnessed substantial progress toward the goal of constructing a machine capable of understanding colloquial discourse. Central to this progress has been the development and application of mathematical methods that permit modeling the speech signal as a complex code with several coexisting levels of structure. The most successful of these are ""template matching,"" stochastic modeling, and probabilistic parsing. The manifestation of common themes such as dynamic programming and finite-state descriptions accentuates a superficial likeness amongst the methods which is often mistaken for the deeper similarity arising from their shared Bayesian foundation. In this paper, we outline the mathematical bases of these methods, invariant metrics, hidden Markov chains, and formal grammars, respectively. We then recount and briefly interpret the results of experiments in speech recognition to which the various methods were applied. Since these mathematical principles seem to bear little resemblance to traditional linguistic characterizations of speech, the success of the experiments is occasionally attributed, even by their authors, merely to excellent engineering. We conclude by speculating that, quite to the contrary, these methods actually constitute a powerful theory of speech that can be reconciled with and elucidate conventional linguistic theories while being used to build truly competent mechanical speech recognizers.","['https://openalex.org/W1687345816', 'https://openalex.org/W2070392176', 'https://openalex.org/W2096795584', 'https://openalex.org/W2148154194', 'https://openalex.org/W2063223471', 'https://openalex.org/W2122111042', 'https://openalex.org/W2169528473', 'https://openalex.org/W4244497272', 'https://openalex.org/W2049633694', 'https://openalex.org/W1979615679', 'https://openalex.org/W2040713190', 'https://openalex.org/W2111041233', 'https://openalex.org/W2158243901', 'https://openalex.org/W6871709107', 'https://openalex.org/W1796349162', 'https://openalex.org/W2034340995', 'https://openalex.org/W2118824742', 'https://openalex.org/W2015720298', 'https://openalex.org/W2771605469', 'https://openalex.org/W2011551012', 'https://openalex.org/W1832161235', 'https://openalex.org/W2134952451', 'https://openalex.org/W2089999944', 'https://openalex.org/W2051046595', 'https://openalex.org/W1989337816', 'https://openalex.org/W6617418634', 'https://openalex.org/W2074460300', 'https://openalex.org/W2018828510', 'https://openalex.org/W1941015899', 'https://openalex.org/W1919801718', 'https://openalex.org/W1579558060', 'https://openalex.org/W2026740477', 'https://openalex.org/W1802482518', 'https://openalex.org/W2157477135', 'https://openalex.org/W1536990986', 'https://openalex.org/W1975694365', 'https://openalex.org/W2056536278', 'https://openalex.org/W2052958516', 'https://openalex.org/W2099813345', 'https://openalex.org/W1970578741', 'https://openalex.org/W2101835908', 'https://openalex.org/W1978652777', 'https://openalex.org/W2141762019', 'https://openalex.org/W2000100543', 'https://openalex.org/W2171850596', 'https://openalex.org/W1965555277', 'https://openalex.org/W2126160338', 'https://openalex.org/W1986092967', 'https://openalex.org/W2089946344', 'https://openalex.org/W2060356208', 'https://openalex.org/W1975352209', 'https://openalex.org/W6606492353', 'https://openalex.org/W1991133427', 'https://openalex.org/W2112633196', 'https://openalex.org/W1774234760', 'https://openalex.org/W350151236', 'https://openalex.org/W2099616984', 'https://openalex.org/W2137089646', 'https://openalex.org/W2035227369', 'https://openalex.org/W2024172518', 'https://openalex.org/W2134587001', 'https://openalex.org/W2078744230', 'https://openalex.org/W1990005915', 'https://openalex.org/W2035032881', 'https://openalex.org/W2168171912', 'https://openalex.org/W1970961429', 'https://openalex.org/W1986958065', 'https://openalex.org/W4240043473', 'https://openalex.org/W2084106112', 'https://openalex.org/W2140165763', 'https://openalex.org/W2159782014', 'https://openalex.org/W2139576349', 'https://openalex.org/W2159775472', 'https://openalex.org/W2164463707', 'https://openalex.org/W2056133372', 'https://openalex.org/W1970673886', 'https://openalex.org/W1989932830', 'https://openalex.org/W1984047604', 'https://openalex.org/W2038107365', 'https://openalex.org/W2045123594', 'https://openalex.org/W2021760654', 'https://openalex.org/W1953255822', 'https://openalex.org/W2155368638', 'https://openalex.org/W1966812932', 'https://openalex.org/W2163929346', 'https://openalex.org/W2047706513', 'https://openalex.org/W2086699924', 'https://openalex.org/W1841036587', 'https://openalex.org/W2077574412', 'https://openalex.org/W2144405321', 'https://openalex.org/W2007321142', 'https://openalex.org/W1980800561', 'https://openalex.org/W2032005922', 'https://openalex.org/W2155126530', 'https://openalex.org/W2050913223', 'https://openalex.org/W2115739330', 'https://openalex.org/W2057833190', 'https://openalex.org/W2000905214', 'https://openalex.org/W2134383396', 'https://openalex.org/W2122544684', 'https://openalex.org/W6602235367', 'https://openalex.org/W2128160875', 'https://openalex.org/W2019955916', 'https://openalex.org/W6678914141', 'https://openalex.org/W2022554507', 'https://openalex.org/W4299807969', 'https://openalex.org/W54230203', 'https://openalex.org/W1535681052', 'https://openalex.org/W2011039300', 'https://openalex.org/W2076506437', 'https://openalex.org/W4206319965', 'https://openalex.org/W2317915144', 'https://openalex.org/W2091451843', 'https://openalex.org/W1582219060', 'https://openalex.org/W1981724541', 'https://openalex.org/W1974413746', 'https://openalex.org/W2144968237', 'https://openalex.org/W1575431606', 'https://openalex.org/W2150593711', 'https://openalex.org/W2097409619', 'https://openalex.org/W4401371936', 'https://openalex.org/W2799518931', 'https://openalex.org/W1505035441', 'https://openalex.org/W2752908210', 'https://openalex.org/W159314104', 'https://openalex.org/W2160645305', 'https://openalex.org/W586694261', 'https://openalex.org/W2010470211', 'https://openalex.org/W2752061190', 'https://openalex.org/W2127218421', 'https://openalex.org/W1571573569', 'https://openalex.org/W2592184324']",1985-01-01
https://openalex.org/W2144195083,https://doi.org/10.1109/proc.1976.10157,Practical applications of voice input to machines,"Voice input to machine is the most natural form of man-machine communications. In this type of system the machine responds to the mode of communications preferred by the user, rather than vice versa. Many practical applications exist today for limited capability voice input systems. The first operational voice input systems have taken place with limited vocabulary, isolated word voice input systems. Most of these initial systems were for industrial applications in which the users' hands or eyes were already busy with their normal work requirements. Future developments in both new applications and increased capability voice input systems can be expected to considerably expand the usage of this form of man-machine communications.","['https://openalex.org/W1999847858', 'https://openalex.org/W2140886571', 'https://openalex.org/W2048205846', 'https://openalex.org/W2137089646', 'https://openalex.org/W1986092967', 'https://openalex.org/W2017688745', 'https://openalex.org/W1996746519', 'https://openalex.org/W2020637097', 'https://openalex.org/W2063223471', 'https://openalex.org/W2070392176', 'https://openalex.org/W4244497272', 'https://openalex.org/W2005080020', 'https://openalex.org/W1777419623', 'https://openalex.org/W1976882641', 'https://openalex.org/W1536990986', 'https://openalex.org/W2079690395', 'https://openalex.org/W2001850558', 'https://openalex.org/W1995405517', 'https://openalex.org/W2052774093', 'https://openalex.org/W275989656', 'https://openalex.org/W2525374352', 'https://openalex.org/W2091451843', 'https://openalex.org/W2035224173', 'https://openalex.org/W4285719527', 'https://openalex.org/W2083417201', 'https://openalex.org/W1807136877', 'https://openalex.org/W44681557', 'https://openalex.org/W2292185534']",1976-01-01
https://openalex.org/W2057833190,https://doi.org/10.1109/tcom.1981.1095031,Isolated and Connected Word Recognition--Theory and Selected Applications,"The art and science of speech recognition have been advanced to the state where it is now possible to communicate reliably with a computer by speaking to it in a disciplined manner using a vocabulary of moderate size. It is the purpose of this paper to outline two aspects of speech-recognition research. First, we discuss word recognition as a classical pattern-recognition problem and show how some fundamental concepts of signal processing, information theory, and computer science can be combined to give us the capability of robust recognition of isolated words and simple connected word sequences. We then describe methods whereby these principles, augmented by modern theories of formal language and semantic analysis, can be used to study some of the more general problems in speech recognition. It is anticipated that these methods will ultimately lead to accurate mechanical recognition of fluent speech under certain controlled conditions.","['https://openalex.org/W1979649888', 'https://openalex.org/W1848194978', 'https://openalex.org/W2132552465', 'https://openalex.org/W1802482518', 'https://openalex.org/W2038786923', 'https://openalex.org/W2002103885', 'https://openalex.org/W2125519368', 'https://openalex.org/W2128160875', 'https://openalex.org/W1989337816', 'https://openalex.org/W2150249857', 'https://openalex.org/W1966264494', 'https://openalex.org/W2142384583', 'https://openalex.org/W1991133427', 'https://openalex.org/W2072054026', 'https://openalex.org/W2139106564', 'https://openalex.org/W2076799970', 'https://openalex.org/W2040713190', 'https://openalex.org/W2110693321', 'https://openalex.org/W2036318925', 'https://openalex.org/W2169528473', 'https://openalex.org/W2060356208', 'https://openalex.org/W2022554507', 'https://openalex.org/W2079145130', 'https://openalex.org/W2101835908', 'https://openalex.org/W1536990986', 'https://openalex.org/W2141762019', 'https://openalex.org/W2169248114', 'https://openalex.org/W1675537913', 'https://openalex.org/W4240043473', 'https://openalex.org/W2134587001', 'https://openalex.org/W1976882641', 'https://openalex.org/W2116982475', 'https://openalex.org/W1537971639', 'https://openalex.org/W2148081319', 'https://openalex.org/W2166469361', 'https://openalex.org/W1990005915', 'https://openalex.org/W6657787524', 'https://openalex.org/W2126560900', 'https://openalex.org/W2137089646', 'https://openalex.org/W2000905214', 'https://openalex.org/W2048205846', 'https://openalex.org/W2125894463', 'https://openalex.org/W2089999944', 'https://openalex.org/W2123783347', 'https://openalex.org/W2018828510', 'https://openalex.org/W6871709107', 'https://openalex.org/W1995875735', 'https://openalex.org/W6672791792', 'https://openalex.org/W2157477135', 'https://openalex.org/W2159775472', 'https://openalex.org/W2113496063', 'https://openalex.org/W2144195083', 'https://openalex.org/W2035227369', 'https://openalex.org/W2155126530', 'https://openalex.org/W2079839361', 'https://openalex.org/W1986958065', 'https://openalex.org/W2022160532', 'https://openalex.org/W2072934894', 'https://openalex.org/W2133992008', 'https://openalex.org/W2055815603', 'https://openalex.org/W2143912388', 'https://openalex.org/W2153913025', 'https://openalex.org/W2069465017', 'https://openalex.org/W1582219060', 'https://openalex.org/W2137095888', 'https://openalex.org/W1817451992', 'https://openalex.org/W1565967034', 'https://openalex.org/W1974784305', 'https://openalex.org/W2069501481', 'https://openalex.org/W4401371936', 'https://openalex.org/W1986781756', 'https://openalex.org/W2232907621', 'https://openalex.org/W3094114204', 'https://openalex.org/W1981864244', 'https://openalex.org/W2592184324', 'https://openalex.org/W4289259401', 'https://openalex.org/W2028528924', 'https://openalex.org/W2035224173', 'https://openalex.org/W2010470211', 'https://openalex.org/W2066873261', 'https://openalex.org/W2088136385']",1981-05-01
https://openalex.org/W111481704,https://doi.org/10.1016/s0095-4470(19)31059-9,Speech perception: a model of acoustic–phonetic analysis and lexical access,,"['https://openalex.org/W2274635302', 'https://openalex.org/W2079951026', 'https://openalex.org/W6723008242', 'https://openalex.org/W1998081072', 'https://openalex.org/W1994158173', 'https://openalex.org/W2040396431', 'https://openalex.org/W6677538579', 'https://openalex.org/W2054312063', 'https://openalex.org/W1985029311', 'https://openalex.org/W1993220683', 'https://openalex.org/W6750954923', 'https://openalex.org/W2005244830', 'https://openalex.org/W4231853049', 'https://openalex.org/W2009950940', 'https://openalex.org/W6680459006', 'https://openalex.org/W6798459481', 'https://openalex.org/W6640283853', 'https://openalex.org/W1993533823', 'https://openalex.org/W4240043473', 'https://openalex.org/W2067777920', 'https://openalex.org/W6635359909', 'https://openalex.org/W6644930742', 'https://openalex.org/W1984658462', 'https://openalex.org/W1976788995', 'https://openalex.org/W2064303346', 'https://openalex.org/W2068247585', 'https://openalex.org/W1985514814', 'https://openalex.org/W6808133334', 'https://openalex.org/W2005512008', 'https://openalex.org/W6643633218', 'https://openalex.org/W6682466427', 'https://openalex.org/W6681209334', 'https://openalex.org/W2043013057', 'https://openalex.org/W1979764868', 'https://openalex.org/W2030123246', 'https://openalex.org/W6684135536', 'https://openalex.org/W1970533835', 'https://openalex.org/W6600348874', 'https://openalex.org/W2083525895', 'https://openalex.org/W6631121731', 'https://openalex.org/W2078656782', 'https://openalex.org/W6654054879', 'https://openalex.org/W1975136431', 'https://openalex.org/W2028665913', 'https://openalex.org/W6744822531', 'https://openalex.org/W1999528738', 'https://openalex.org/W2989727754', 'https://openalex.org/W2026818305', 'https://openalex.org/W1972675515', 'https://openalex.org/W2056145427', 'https://openalex.org/W2101378202', 'https://openalex.org/W2078752270', 'https://openalex.org/W2015081763', 'https://openalex.org/W7014582953', 'https://openalex.org/W6745749302', 'https://openalex.org/W6694576185', 'https://openalex.org/W2046487969', 'https://openalex.org/W6776859104', 'https://openalex.org/W1970848230', 'https://openalex.org/W6676040099', 'https://openalex.org/W1977531436', 'https://openalex.org/W2007251230', 'https://openalex.org/W6637226882', 'https://openalex.org/W6630609784', 'https://openalex.org/W6631636263', 'https://openalex.org/W2010705669', 'https://openalex.org/W2066152050', 'https://openalex.org/W1978652777', 'https://openalex.org/W2335578026', 'https://openalex.org/W4210704033', 'https://openalex.org/W2489811056', 'https://openalex.org/W2015720298', 'https://openalex.org/W1553588790', 'https://openalex.org/W1528496016', 'https://openalex.org/W2010470211', 'https://openalex.org/W1517036110', 'https://openalex.org/W2118455293', 'https://openalex.org/W1934584000', 'https://openalex.org/W2064369002', 'https://openalex.org/W2137095888', 'https://openalex.org/W615517874', 'https://openalex.org/W3204146086', 'https://openalex.org/W1973581170', 'https://openalex.org/W3183411220', 'https://openalex.org/W2140505777', 'https://openalex.org/W579803826', 'https://openalex.org/W2106988383', 'https://openalex.org/W181056519', 'https://openalex.org/W2134952451', 'https://openalex.org/W2153327798', 'https://openalex.org/W594133280', 'https://openalex.org/W1511328613', 'https://openalex.org/W652994534', 'https://openalex.org/W1675537913', 'https://openalex.org/W2010800472', 'https://openalex.org/W2165490943', 'https://openalex.org/W2144968237', 'https://openalex.org/W2767963287', 'https://openalex.org/W2275806937', 'https://openalex.org/W8813361', 'https://openalex.org/W1592624587', 'https://openalex.org/W1565967034', 'https://openalex.org/W607079072']",1979-07-01
https://openalex.org/W4401371936,https://doi.org/10.25144/23416,CONNECTED WORD RECOGNITION USING WHOLE WORD TEMPLATES,,[],2024-07-29
https://openalex.org/W1981864244,https://doi.org/10.1002/j.1538-7305.1978.tb02114.x,Evaluation of a Word Recognition System Using Syntax Analysis,"A speech recognition system has been implemented which accepts reasonably natural English sentences spoken as isolated words. The major components of the system are a speaker-dependent word recognizer, a programmed grammar, and a syntax analyzer. The system permits formulation of complete sentences from a vocabulary of 127 words. The set of sentences selected for investigation is intended for use as requests in an automated travel information system. Results are presented of evaluations for speakers using their own stored reference patterns, the reference patterns of other speakers, and composite reference patterns averaged over several speakers. For speakers using their own reference patterns the median error rate for acoustic recognition of the individual words is 11.7 percent. When syntax analysis is applied to the complete sentence, word recognition errors can be corrected and the error rate reduced to 0.4 percent.","['https://openalex.org/W2137089646', 'https://openalex.org/W2000905214', 'https://openalex.org/W6665485688', 'https://openalex.org/W2060356208']",1978-05-06
https://openalex.org/W1994859265,https://doi.org/10.1121/1.2021691,A modified K-means clustering algorithm for use in speaker-independent isolated word recognition,"Recent studies of isolated word recognition systems have shown that a set of carefully chosen templates can be used to bring the performance of speaker-independent systems up to that of systems trained to the individual speaker. The earliest work in this area used a sophisticated set of pattern recognition algorithms in a human-interactive mode to create the set of templates (multiple patterns) for each word in the vocabulary. Not only was this procedure time consuming but it was impossible to reproduce exactly, because it was highly dependent on decisions made by the experimenter. Subsequent work led to an automatic clustering procedure which, given only a set of clustering parameters, clustered tokens with the same performance as the previously developed supervised algorithms. The one drawback of the automatic procedure was that the specification of the input parameter set was found to be somewhat dependent on the vocabulary type and size of population to be clustered. Since the user of such a statistical clustering algorithm could not be expected, in general, to know how to choose the word clustering parameters, even this automatic clustering algorithm was not appropriate for a completely general word recognition system. It is the purpose of this paper to present a new clustering algorithm based on a K-means approach which requires no user parameter specification. Experimental data show that this new algorithm performs as well or better than the previously used clustering techniques when tested as part of a speaker independent isolated word recognition system.",[],1984-05-01
https://openalex.org/W1542057217,,Automatic Speech and Speaker Recognition,,[],1979-08-01
https://openalex.org/W2974194285,https://doi.org/10.21437/ssw.2019-49,Sequence to Sequence Neural Speech Synthesis with Prosody Modification Capabilities,"Modern sequence to sequence neural TTS systems provide close to natural speech quality. Such systems usually comprise a network converting linguistic/phonetic features sequence to an acoustic features sequence, cascaded with a neural vocoder. The generated speech prosody (i.e. phoneme durations, pitch and loudness) is implicitly present in the acoustic features, being mixed with spectral information. Although the speech sounds natural, its prosody realization is randomly chosen and cannot be easily altered. The prosody control becomes an even more difficult task if no prosodic labeling is present in the training data. Recently, much progress has been achieved in unsupervised speaking style learning and generation, however human inspection is still required after the training for discovery and interpretation of the speaking styles learned by the system. In this work we introduce a fully automatic method that makes the system aware of the prosody and enables sentence-wise speaking pace and expressiveness control on a continuous scale. While being useful by itself in many applications, the proposed prosody control can also improve the overall quality and expressiveness of the synthesized speech, as demonstrated by subjective listening evaluations. We also propose a novel augmented attention mechanism, that facilitates better pace control sensitivity and faster attention convergence.",[],2019-09-14
https://openalex.org/W3097892637,https://doi.org/10.21437/interspeech.2020-2861,Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features,"Modern neural text-to-speech (TTS) synthesis can generate speech that is indistinguishable from natural speech.However, the prosody of generated utterances often represents the average prosodic style of the database instead of having wide prosodic variation.Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence.In this work, we train a sequence-to-sequence neural network conditioned on acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions.Experiments show that a model conditioned on sentencewise pitch, pitch range, phone duration, energy, and spectral tilt can effectively control each prosodic dimension and generate a wide variety of speaking styles, while maintaining similar mean opinion score (4.23) to our Tacotron baseline (4.26).","['https://openalex.org/W4295731579', 'https://openalex.org/W2069859485', 'https://openalex.org/W2794490148', 'https://openalex.org/W2952269766', 'https://openalex.org/W2964138190', 'https://openalex.org/W3016021263', 'https://openalex.org/W2970006822', 'https://openalex.org/W2154825638', 'https://openalex.org/W2972702018', 'https://openalex.org/W2130942839', 'https://openalex.org/W3101882441', 'https://openalex.org/W2605141709', 'https://openalex.org/W4298580827', 'https://openalex.org/W2885800352', 'https://openalex.org/W2964243274', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963609956']",2020-10-25
https://openalex.org/W2938102059,https://doi.org/10.1109/icassp.2019.8683827,Phonemic-level Duration Control Using Attention Alignment for Natural Speech Synthesis,"Recent attention-based end-to-end speech synthesis from text systems have achieved human-level performance. However, many approaches cause a sequence-to-sequence model to generate only averaged results of the input text, making it difficult to control the duration of utterance. In this study, we present a novel mechanism for phonemic-level duration control (PDC) in a nearly end-to-end manner in order to solve this problem. We used a teacher attention alignment generated by an annotation speech analyzer program. Our method is inspired by the idea that the duration of a phoneme is highly related to its phonemic features. These phonemic features are saved on the attention alignment by adding duration embedding to it. This enables the model to learn and control the phonemic and rhythmic features of speech. We also show that providing alignment information as a teacher loss term improves training speed and notably, makes the model better at controlling the speed of dramatic change in phonemic-level duration with subjective demonstration. As a result, we show that our PDC speech synthesis with alignment loss outperforms other baseline methods without losing the ability to control the duration of phonemes in extremely adjusted environments with faster convergence.","['https://openalex.org/W2963971656', 'https://openalex.org/W6734815144', 'https://openalex.org/W6738277540', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W6630875275', 'https://openalex.org/W2327501763', 'https://openalex.org/W6756197946', 'https://openalex.org/W2399979888', 'https://openalex.org/W2750077311', 'https://openalex.org/W6917585676', 'https://openalex.org/W2608207374', 'https://openalex.org/W2120847449', 'https://openalex.org/W2962902328', 'https://openalex.org/W1902237438', 'https://openalex.org/W6679434410', 'https://openalex.org/W2102003408', 'https://openalex.org/W6843615328', 'https://openalex.org/W6640090968', 'https://openalex.org/W2515943672', 'https://openalex.org/W6679436768', 'https://openalex.org/W6749489859', 'https://openalex.org/W6843673214', 'https://openalex.org/W6753855596', 'https://openalex.org/W6746700228', 'https://openalex.org/W2914049472', 'https://openalex.org/W2767052532', 'https://openalex.org/W2251701780', 'https://openalex.org/W2619368999', 'https://openalex.org/W2963782041', 'https://openalex.org/W2963975282', 'https://openalex.org/W2133564696', 'https://openalex.org/W2901997113', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963927338', 'https://openalex.org/W2949382160', 'https://openalex.org/W4300416988', 'https://openalex.org/W2963691546', 'https://openalex.org/W4294619240', 'https://openalex.org/W2591927543', 'https://openalex.org/W2519091744', 'https://openalex.org/W1922655562', 'https://openalex.org/W4295731579', 'https://openalex.org/W2963712897', 'https://openalex.org/W1514535095', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963609956', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964243274']",2019-04-17
https://openalex.org/W3095389792,https://doi.org/10.21437/interspeech.2020-2464,"High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency","This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests.","['https://openalex.org/W2964243274', 'https://openalex.org/W1522301498', 'https://openalex.org/W2767052532', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963300588', 'https://openalex.org/W4294619240', 'https://openalex.org/W2962970071', 'https://openalex.org/W3015338123', 'https://openalex.org/W4385245566', 'https://openalex.org/W2996286887', 'https://openalex.org/W3038172701', 'https://openalex.org/W854541894', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970971581', 'https://openalex.org/W4295312788', 'https://openalex.org/W2903739847', 'https://openalex.org/W3095459301', 'https://openalex.org/W4293714597', 'https://openalex.org/W2963091184', 'https://openalex.org/W3015922793', 'https://openalex.org/W2970006822', 'https://openalex.org/W2946200149', 'https://openalex.org/W2970730223', 'https://openalex.org/W4298580827', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963691546', 'https://openalex.org/W2948211236', 'https://openalex.org/W2095705004', 'https://openalex.org/W1810943226', 'https://openalex.org/W2964307104', 'https://openalex.org/W2963782041']",2020-10-25
https://openalex.org/W2795109282,https://doi.org/10.48550/arxiv.1803.09047,Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,"We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.","['https://openalex.org/W2778926431', 'https://openalex.org/W2107860279', 'https://openalex.org/W2794490148', 'https://openalex.org/W2777302760', 'https://openalex.org/W2963799213', 'https://openalex.org/W2130086727', 'https://openalex.org/W2766406951', 'https://openalex.org/W2950635152', 'https://openalex.org/W2651834199', 'https://openalex.org/W28194048', 'https://openalex.org/W174066062', 'https://openalex.org/W2963609956', 'https://openalex.org/W2949117887', 'https://openalex.org/W1810943226', 'https://openalex.org/W2964121744', 'https://openalex.org/W162654330', 'https://openalex.org/W2475998840', 'https://openalex.org/W2103869314', 'https://openalex.org/W89128468', 'https://openalex.org/W2156146072', 'https://openalex.org/W2619368999', 'https://openalex.org/W2578842094']",2018-03-24
https://openalex.org/W2964138190,https://doi.org/10.1109/icassp.2019.8683501,Robust and Fine-grained Prosody Control of End-to-end Speech Synthesis,"We propose prosody embeddings for emotional and expressive speech synthesis networks. The proposed methods introduce temporal structures in the embedding networks, thus enabling fine-grained control of the speaking style of the synthesized speech. The temporal structures can be designed either on the speech side or the text side, leading to different control resolutions in time. The prosody embedding networks are plugged into end-to-end speech synthesis networks and trained without any other supervision except for the target speech for synthesizing. It is demonstrated that the prosody embedding networks learned to extract prosodic features. By adjusting the learned prosody features, we could change the pitch and amplitude of the synthesized speech both at the frame level and the phoneme level. We also introduce the temporal normalization of prosody embeddings, which shows better robustness against speaker perturbations during prosody transfer tasks.","['https://openalex.org/W2964243274', 'https://openalex.org/W2584505851', 'https://openalex.org/W6753441378', 'https://openalex.org/W6739901393', 'https://openalex.org/W2795109282', 'https://openalex.org/W6748573829', 'https://openalex.org/W2885800352', 'https://openalex.org/W6750489868', 'https://openalex.org/W4390911804', 'https://openalex.org/W6606697926', 'https://openalex.org/W6765987481', 'https://openalex.org/W2963609956', 'https://openalex.org/W2157331557', 'https://openalex.org/W2619368999', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2962739369', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963712897', 'https://openalex.org/W4298174729', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963623257', 'https://openalex.org/W162654330', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963272440']",2019-04-17
https://openalex.org/W2904459034,https://doi.org/10.1109/icassp.2019.8683623,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,"In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.","['https://openalex.org/W2962691331', 'https://openalex.org/W2962850167', 'https://openalex.org/W6720208624', 'https://openalex.org/W2131774270', 'https://openalex.org/W2064675550', 'https://openalex.org/W6714142977', 'https://openalex.org/W6623517193', 'https://openalex.org/W6750489868', 'https://openalex.org/W2885800352', 'https://openalex.org/W2795109282', 'https://openalex.org/W6640963894', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963223306', 'https://openalex.org/W2892140764', 'https://openalex.org/W2807692250', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962970071', 'https://openalex.org/W2963272440', 'https://openalex.org/W4293714597', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2753738274', 'https://openalex.org/W2099471712', 'https://openalex.org/W2519091744', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963691546', 'https://openalex.org/W2467604901', 'https://openalex.org/W854541894']",2019-04-17
https://openalex.org/W3016021263,https://doi.org/10.1109/icassp40776.2020.9053520,Fully-Hierarchical Fine-Grained Prosody Modeling For Interpretable Speech Synthesis,"This paper proposes a hierarchical, fine-grained and interpretable latent variable model for prosody based on the Tacotron 2 text-to-speech model. It achieves multi-resolution modeling of prosody by conditioning finer level representations on coarser level ones. Additionally, it imposes hierarchical conditioning across all latent dimensions using a conditional variational auto-encoder (VAE) with an auto-regressive structure. Evaluation of reconstruction performance illustrates that the new structure does not degrade the model while allowing better interpretability. Interpretations of prosody attributes are provided together with the comparison between word-level and phone-level prosody representations. Moreover, both qualitative and quantitative evaluations are used to demonstrate the improvement in the disentanglement of the latent dimensions.","['https://openalex.org/W6675938391', 'https://openalex.org/W6680537413', 'https://openalex.org/W6739901393', 'https://openalex.org/W6751810238', 'https://openalex.org/W6730405185', 'https://openalex.org/W2091425152', 'https://openalex.org/W6677973343', 'https://openalex.org/W2972359262', 'https://openalex.org/W6738536549', 'https://openalex.org/W2907262790', 'https://openalex.org/W2795109282', 'https://openalex.org/W2964138190', 'https://openalex.org/W2964243274', 'https://openalex.org/W2904459034', 'https://openalex.org/W2962691331', 'https://openalex.org/W2889028433', 'https://openalex.org/W6745697700', 'https://openalex.org/W6755300632', 'https://openalex.org/W6718140377', 'https://openalex.org/W6679436768', 'https://openalex.org/W6750489868', 'https://openalex.org/W2963609956', 'https://openalex.org/W2069859485', 'https://openalex.org/W6756197946', 'https://openalex.org/W2962689740', 'https://openalex.org/W6744627333', 'https://openalex.org/W6745117592', 'https://openalex.org/W6745687250', 'https://openalex.org/W6748223763', 'https://openalex.org/W7075712485', 'https://openalex.org/W6756663807', 'https://openalex.org/W2758785877', 'https://openalex.org/W4293411471', 'https://openalex.org/W4289383906', 'https://openalex.org/W2963568578', 'https://openalex.org/W2963226019', 'https://openalex.org/W2130942839', 'https://openalex.org/W2962690557', 'https://openalex.org/W2963047245', 'https://openalex.org/W2963618559', 'https://openalex.org/W2794490148', 'https://openalex.org/W2948238043', 'https://openalex.org/W4295112158', 'https://openalex.org/W2971074500', 'https://openalex.org/W2805530975', 'https://openalex.org/W2950662112', 'https://openalex.org/W2903538854', 'https://openalex.org/W2901997113', 'https://openalex.org/W2981870653', 'https://openalex.org/W2621357189', 'https://openalex.org/W2977311057', 'https://openalex.org/W2884607399', 'https://openalex.org/W2963104724', 'https://openalex.org/W2963403868', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963272440', 'https://openalex.org/W4295731579', 'https://openalex.org/W2785519580', 'https://openalex.org/W2963927338', 'https://openalex.org/W2917688842', 'https://openalex.org/W2963366547', 'https://openalex.org/W2963264829', 'https://openalex.org/W2559823555', 'https://openalex.org/W4293849739', 'https://openalex.org/W2140574335', 'https://openalex.org/W2107740512', 'https://openalex.org/W4385245566', 'https://openalex.org/W2766812927', 'https://openalex.org/W2964204277', 'https://openalex.org/W2996573371']",2020-04-09
https://openalex.org/W88081813,,The HTK book,,['https://openalex.org/W47415966'],1995-09-16
https://openalex.org/W4295731579,https://doi.org/10.48550/arxiv.1803.09047,Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with\n Tacotron,"We present an extension to the Tacotron speech synthesis architecture that\nlearns a latent embedding space of prosody, derived from a reference acoustic\nrepresentation containing the desired prosody. We show that conditioning\nTacotron on this learned embedding space results in synthesized audio that\nmatches the prosody of the reference signal with fine time detail even when the\nreference and synthesis speakers are different. Additionally, we show that a\nreference prosody embedding can be used to synthesize text that is different\nfrom that of the reference utterance. We define several quantitative and\nsubjective metrics for evaluating prosody transfer, and report results with\naccompanying audio samples from single-speaker and 44-speaker Tacotron models\non a prosody transfer task.\n",[],2018-03-23
https://openalex.org/W2948211236,https://doi.org/10.48550/arxiv.1906.01083,MelNet: A Generative Model for Audio in the Frequency Domain,"Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.","['https://openalex.org/W2883853252', 'https://openalex.org/W2064675550', 'https://openalex.org/W1664573881', 'https://openalex.org/W2963139417', 'https://openalex.org/W2608207374', 'https://openalex.org/W2901997113', 'https://openalex.org/W2099471712', 'https://openalex.org/W2892620417', 'https://openalex.org/W1771459135', 'https://openalex.org/W2396144419', 'https://openalex.org/W2004453603', 'https://openalex.org/W2953331651', 'https://openalex.org/W2795485067', 'https://openalex.org/W2962942158', 'https://openalex.org/W2778792233', 'https://openalex.org/W1810943226', 'https://openalex.org/W1579853615', 'https://openalex.org/W3101648800', 'https://openalex.org/W2769810959', 'https://openalex.org/W2594961016', 'https://openalex.org/W2950547518', 'https://openalex.org/W2897548994', 'https://openalex.org/W2950946978', 'https://openalex.org/W2949382160', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963799213', 'https://openalex.org/W2788851830', 'https://openalex.org/W2963300588', 'https://openalex.org/W2940744433', 'https://openalex.org/W2766527293', 'https://openalex.org/W2950299304', 'https://openalex.org/W1685006559', 'https://openalex.org/W2910577860', 'https://openalex.org/W2194775991', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963636093', 'https://openalex.org/W2097039814', 'https://openalex.org/W2951986497', 'https://openalex.org/W2591927543', 'https://openalex.org/W2953318193', 'https://openalex.org/W2808631503', 'https://openalex.org/W2888169323', 'https://openalex.org/W2475988411', 'https://openalex.org/W2170942820', 'https://openalex.org/W2786254735', 'https://openalex.org/W2951004968', 'https://openalex.org/W2581236139', 'https://openalex.org/W2964243274', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963186101']",2019-06-04
https://openalex.org/W2107831318,,ACCURATE SHORT-TERM ANALYSIS OF THE FUNDAMENTAL FREQUENCY AND THE HARMONICS-TO-NOISE RATIO OF A SAMPLED SOUND,"We present a straightforward and robust algorithm for periodicity detection, working in the lag (autocorrelation) domain. When it is tested for periodic signals and for signals with additive noise or jitter, it proves to be several orders of magnitude more accurate than the methods commonly used for speech analysis. This makes our method capable of measuring harmonics-to-noise ratios in the lag domain with an accuracy and reliability much greater than that of any of the usual frequency-domain methods. By definition, the best candidate for the acoustic pitch period of a sound can be found from the position of the maximum of the autocorrelation function of the sound, while the degree of periodicity (the harmonics-to-noise ratio) of the sound can be found from the relative height of this maximum. However, sampling and windowing cause problems in accurately determining the position and height of the maximum. These problems have led to inaccurate timedomain and cepstral methods for pitch detection, and to the exclusive use of frequency-domain methods for the determination of the harmonics-to-noise ratio.","['https://openalex.org/W2138039466', 'https://openalex.org/W2003820141', 'https://openalex.org/W1964546269', 'https://openalex.org/W2043580765', 'https://openalex.org/W2110392754', 'https://openalex.org/W2084044763', 'https://openalex.org/W3005283740', 'https://openalex.org/W2341283081']",1993-01-01
https://openalex.org/W2095705004,,Dropout: a simple way to prevent neural networks from overfitting,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.","['https://openalex.org/W2085040216', 'https://openalex.org/W2335728318', 'https://openalex.org/W2611675901', 'https://openalex.org/W2096873754', 'https://openalex.org/W1993882792', 'https://openalex.org/W189596042', 'https://openalex.org/W35527955', 'https://openalex.org/W2963574257', 'https://openalex.org/W2053229256', 'https://openalex.org/W2147800946', 'https://openalex.org/W2136922672', 'https://openalex.org/W2103359087', 'https://openalex.org/W2100495367', 'https://openalex.org/W2971788173', 'https://openalex.org/W1567512734', 'https://openalex.org/W2546302380', 'https://openalex.org/W1492459858', 'https://openalex.org/W2135046866', 'https://openalex.org/W2145094598', 'https://openalex.org/W2150717117', 'https://openalex.org/W1524333225', 'https://openalex.org/W2163605009', 'https://openalex.org/W137106866', 'https://openalex.org/W2183112036', 'https://openalex.org/W2294059674', 'https://openalex.org/W2156163116', 'https://openalex.org/W3118608800', 'https://openalex.org/W2156297475', 'https://openalex.org/W2962820688', 'https://openalex.org/W2152722485', 'https://openalex.org/W2025768430', 'https://openalex.org/W2114296159', 'https://openalex.org/W2158542502', 'https://openalex.org/W2114733238', 'https://openalex.org/W2949821452', 'https://openalex.org/W2131241448']",2014-01-01
https://openalex.org/W4214968481,https://doi.org/10.48550/arxiv.2002.03788,Generating diverse and natural text-to-speech samples using a quantized\n fine-grained VAE and auto-regressive prosody prior,"Recent neural text-to-speech (TTS) models with fine-grained latent features\nenable precise control of the prosody of synthesized speech. Such models\ntypically incorporate a fine-grained variational autoencoder (VAE) structure,\nextracting latent features at each input token (e.g., phonemes). However,\ngenerating samples with the standard VAE prior often results in unnatural and\ndiscontinuous speech, with dramatic prosodic variation between tokens. This\npaper proposes a sequential prior in a discrete latent space which can generate\nmore naturally sounding samples. This is accomplished by discretizing the\nlatent features using vector quantization (VQ), and separately training an\nautoregressive (AR) prior model over the result. We evaluate the approach using\nlistening tests, objective metrics of automatic speech recognition (ASR)\nperformance, and measurements of prosody attributes. Experimental results show\nthat the proposed model significantly improves the naturalness in random sample\ngeneration. Furthermore, initial experiments demonstrate that randomly sampling\nfrom the proposed model can be used as data augmentation to improve the ASR\nperformance.\n",[],2020-02-06
https://openalex.org/W2794490148,https://doi.org/10.48550/arxiv.1803.09017,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","In this work, we propose ""global style tokens"" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable ""labels"" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.","['https://openalex.org/W2792995953', 'https://openalex.org/W2964301388', 'https://openalex.org/W2475998840', 'https://openalex.org/W2964281804', 'https://openalex.org/W2099057450', 'https://openalex.org/W2950527759', 'https://openalex.org/W2758785877', 'https://openalex.org/W2759925408', 'https://openalex.org/W2187089797', 'https://openalex.org/W2156146072', 'https://openalex.org/W2651834199', 'https://openalex.org/W2949382160', 'https://openalex.org/W28194048', 'https://openalex.org/W2409027918', 'https://openalex.org/W2963609956', 'https://openalex.org/W2617258110', 'https://openalex.org/W2963799213', 'https://openalex.org/W2777302760', 'https://openalex.org/W2736900972', 'https://openalex.org/W162654330', 'https://openalex.org/W2626778328']",2018-03-23
https://openalex.org/W2977311057,https://doi.org/10.48550/arxiv.1910.01709,Semi-Supervised Generative Modeling for Controllable Speech Synthesis,"We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. Audio samples are available on the web.","['https://openalex.org/W2963927338', 'https://openalex.org/W2963712897', 'https://openalex.org/W2149628368', 'https://openalex.org/W2964281804', 'https://openalex.org/W2949416428', 'https://openalex.org/W2794490148', 'https://openalex.org/W2012762214', 'https://openalex.org/W2785519580', 'https://openalex.org/W2963568578', 'https://openalex.org/W2108501770', 'https://openalex.org/W2952269766', 'https://openalex.org/W2948238043', 'https://openalex.org/W2645047556', 'https://openalex.org/W2948211236', 'https://openalex.org/W2963534259', 'https://openalex.org/W2107860279', 'https://openalex.org/W2792995953', 'https://openalex.org/W2559823555', 'https://openalex.org/W2946200149', 'https://openalex.org/W1810943226', 'https://openalex.org/W2608207374', 'https://openalex.org/W2964243274', 'https://openalex.org/W2954882393', 'https://openalex.org/W2753738274', 'https://openalex.org/W2210838531', 'https://openalex.org/W2951004968', 'https://openalex.org/W2795485067', 'https://openalex.org/W2788851830', 'https://openalex.org/W2946146305', 'https://openalex.org/W2736900972', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962897886', 'https://openalex.org/W2150791533', 'https://openalex.org/W2949382160', 'https://openalex.org/W2091425152', 'https://openalex.org/W2963264829']",2019-10-03
https://openalex.org/W2945544731,,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,,[],2019-05-24
https://openalex.org/W2991417167,https://doi.org/10.48550/arxiv.1911.09645,Prosody Transfer in Neural Text to Speech Using Global Pitch and Loudness Features,"This paper presents a simple yet effective method to achieve prosody transfer from a reference speech signal to synthesized speech. The main idea is to incorporate well-known acoustic correlates of prosody such as pitch and loudness contours of the reference speech into a modern neural text-to-speech (TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of acoustic features are extracted from reference audio and then used to condition a TC2 synthesizer. The trained model is evaluated using subjective listening tests and a novel objective evaluation of prosody transfer is proposed. Listening tests show that the synthesized speech is rated as highly natural and that prosody is successfully transferred from the reference speech signal to the synthesized signal.","['https://openalex.org/W2963272440', 'https://openalex.org/W2608207374', 'https://openalex.org/W2945544731', 'https://openalex.org/W2952269766', 'https://openalex.org/W2963712897', 'https://openalex.org/W1524333225', 'https://openalex.org/W2889092828', 'https://openalex.org/W2494980014', 'https://openalex.org/W2963568578', 'https://openalex.org/W83586041', 'https://openalex.org/W2973158936', 'https://openalex.org/W2964307104', 'https://openalex.org/W2156146072', 'https://openalex.org/W2171121512', 'https://openalex.org/W2981934523', 'https://openalex.org/W2962691331', 'https://openalex.org/W2964138190', 'https://openalex.org/W2085628288', 'https://openalex.org/W2400063444', 'https://openalex.org/W2964243274', 'https://openalex.org/W2795109282', 'https://openalex.org/W2963534259']",2019-11-21
https://openalex.org/W2952269766,https://doi.org/10.48550/arxiv.1905.07195,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,"The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational autoencoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.","['https://openalex.org/W2149017325', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963782041', 'https://openalex.org/W2884607399', 'https://openalex.org/W2788760202', 'https://openalex.org/W2049510512', 'https://openalex.org/W1959608418', 'https://openalex.org/W2188365844', 'https://openalex.org/W133559434', 'https://openalex.org/W2963609956', 'https://openalex.org/W2102003408', 'https://openalex.org/W2794490148', 'https://openalex.org/W1576227399', 'https://openalex.org/W2259472270', 'https://openalex.org/W2251189452', 'https://openalex.org/W2004590799', 'https://openalex.org/W3020031927', 'https://openalex.org/W2949382160', 'https://openalex.org/W2034277951', 'https://openalex.org/W2962691331', 'https://openalex.org/W2513112113', 'https://openalex.org/W2138660131', 'https://openalex.org/W2964301388', 'https://openalex.org/W2184310502', 'https://openalex.org/W2897548994', 'https://openalex.org/W38355094', 'https://openalex.org/W2524467597', 'https://openalex.org/W2963790827', 'https://openalex.org/W1600722501']",2019-05-17
https://openalex.org/W2963272440,,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","In this work, we propose global style tokens (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable labels they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",[],2018-07-03
https://openalex.org/W1810943226,https://doi.org/10.4230/lipics.fun.2016.3,"LOL: An Investigation into Cybernetic Humor, or: Can Machines Laugh?","The mechanisms of humour have been the subject of much study and investigation, starting with and up to our days. Much of this work is based on literary theories, put forward by some of the most eminent philosophers and thinkers of all times, or medical theories, investigating the impact of humor on brain activity or behaviour. Recent functional neuroimaging studies, for instance, have investigated the process of comprehending and appreciating humor by examining functional activity in distinctive regions of brains stimulated by joke corpora. Yet, there is precious little work on the computational side, possibly due to the less hilarious nature of computer scientists as compared to men of letters and sawbones. In this paper, we set to investigate whether literary theories of humour can stand the test of algorithmic laughter. Or, in other words, we ask ourselves the vexed question: Can machines laugh? We attempt to answer that question by testing whether an algorithm - namely, a neural network - can ""understand"" humour, and in particular whether it is possible to automatically identify abstractions that are predicted to be relevant by established literary theories about the mechanisms of humor. Notice that we do not focus here on distinguishing humorous from serious statements - a feat that is clearly way beyond the capabilities of the average human voter, not to mention the average machine - but rather on identifying the underlying mechanisms and triggers that are postulated to exist by literary theories, by verifying if similar mechanisms can be learned by machines.","['https://openalex.org/W1525783482', 'https://openalex.org/W2064675550', 'https://openalex.org/W2131462252', 'https://openalex.org/W1579853615', 'https://openalex.org/W1632114991', 'https://openalex.org/W2170942820', 'https://openalex.org/W2949305281', 'https://openalex.org/W1507680813', 'https://openalex.org/W2112656927', 'https://openalex.org/W2079735306', 'https://openalex.org/W2058641082', 'https://openalex.org/W1554663460', 'https://openalex.org/W2108677974', 'https://openalex.org/W2152550252', 'https://openalex.org/W44815768', 'https://openalex.org/W2135341757', 'https://openalex.org/W2120861206', 'https://openalex.org/W196214544', 'https://openalex.org/W134527144', 'https://openalex.org/W2107878631', 'https://openalex.org/W1828163288', 'https://openalex.org/W1964175594', 'https://openalex.org/W1674799117', 'https://openalex.org/W2962968839', 'https://openalex.org/W3023071679', 'https://openalex.org/W2161628678', 'https://openalex.org/W2143612262', 'https://openalex.org/W32508012', 'https://openalex.org/W2147568880', 'https://openalex.org/W2115096495']",2016-01-01
https://openalex.org/W2948238043,https://doi.org/10.48550/arxiv.1906.03402,Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis,"Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.","['https://openalex.org/W2796704765', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964243274', 'https://openalex.org/W1810943226', 'https://openalex.org/W2964121744', 'https://openalex.org/W2885800352', 'https://openalex.org/W2904459034', 'https://openalex.org/W2963568578', 'https://openalex.org/W2982174878', 'https://openalex.org/W2807692250', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963927338', 'https://openalex.org/W2107860279', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963135265', 'https://openalex.org/W2884607399', 'https://openalex.org/W2963090522', 'https://openalex.org/W2963534259', 'https://openalex.org/W1959608418', 'https://openalex.org/W2964138190', 'https://openalex.org/W2069859485', 'https://openalex.org/W2901997113', 'https://openalex.org/W2963145887', 'https://openalex.org/W2963609956']",2019-06-08
https://openalex.org/W4395958166,https://doi.org/10.21437/blizzard.2016-13,Expressive Speech Synthesis for Storytelling: The INNOETICS' Entry to the Blizzard Challenge 2016,,[],2016-09-16
https://openalex.org/W2137619888,https://doi.org/10.1109/nnsp.2002.1030094,Finding temporal structure in music: blues improvisation with LSTM recurrent networks,"We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.","['https://openalex.org/W6631636882', 'https://openalex.org/W2038111264', 'https://openalex.org/W2023425980', 'https://openalex.org/W2014978753', 'https://openalex.org/W6631321124', 'https://openalex.org/W2032197604', 'https://openalex.org/W2067621398', 'https://openalex.org/W1996579223', 'https://openalex.org/W1507589012', 'https://openalex.org/W2100649405', 'https://openalex.org/W2136848157', 'https://openalex.org/W2121029939', 'https://openalex.org/W1994629163', 'https://openalex.org/W6637157234', 'https://openalex.org/W1674799117', 'https://openalex.org/W2112349754', 'https://openalex.org/W2983025147', 'https://openalex.org/W1526055535', 'https://openalex.org/W1899504021']",2003-06-25
https://openalex.org/W2744457411,https://doi.org/10.1109/taffc.2017.2737984,MorpheuS: Generating Structured Music with Constrained Patterns and Tension,"Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS' novel framework has the ability to generate polyphonic pieces with a given tension profile and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantifies the tension profile and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efficient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best fit the prescribed tension profile to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with specific tension profile and long-term structure is particularly useful in a game or film music context. Music generated by the MorpheuS system has been performed live in concerts.","['https://openalex.org/W2099369211', 'https://openalex.org/W2163808566', 'https://openalex.org/W1983364832', 'https://openalex.org/W2043803464', 'https://openalex.org/W4230277160', 'https://openalex.org/W3021051756', 'https://openalex.org/W2137089646', 'https://openalex.org/W6605885477', 'https://openalex.org/W2133824856', 'https://openalex.org/W1512768581', 'https://openalex.org/W2048174296', 'https://openalex.org/W2004617458', 'https://openalex.org/W2032982318', 'https://openalex.org/W2181862404', 'https://openalex.org/W1598755738', 'https://openalex.org/W1931344168', 'https://openalex.org/W6634552080', 'https://openalex.org/W6633868512', 'https://openalex.org/W158274995', 'https://openalex.org/W6629496401', 'https://openalex.org/W6844019392', 'https://openalex.org/W6790449334', 'https://openalex.org/W6740880256', 'https://openalex.org/W2094860935', 'https://openalex.org/W2067621398', 'https://openalex.org/W2802501956', 'https://openalex.org/W6929360442', 'https://openalex.org/W2489051169', 'https://openalex.org/W2559726422', 'https://openalex.org/W2132549764', 'https://openalex.org/W2137699463', 'https://openalex.org/W2112213875', 'https://openalex.org/W6605505536', 'https://openalex.org/W2736555612', 'https://openalex.org/W1819710477', 'https://openalex.org/W2288593361', 'https://openalex.org/W2592737436', 'https://openalex.org/W2083817706', 'https://openalex.org/W6697053063', 'https://openalex.org/W2042465720', 'https://openalex.org/W2569781171', 'https://openalex.org/W2164368909', 'https://openalex.org/W3121748795', 'https://openalex.org/W2023412561', 'https://openalex.org/W2152287648', 'https://openalex.org/W1833443371', 'https://openalex.org/W2344527636', 'https://openalex.org/W2144840087', 'https://openalex.org/W1991886506', 'https://openalex.org/W6704376020', 'https://openalex.org/W1486260793', 'https://openalex.org/W2758804652', 'https://openalex.org/W2103498773', 'https://openalex.org/W6605045006', 'https://openalex.org/W6630534141', 'https://openalex.org/W2034161986', 'https://openalex.org/W2111066885', 'https://openalex.org/W2153628411', 'https://openalex.org/W2152304735', 'https://openalex.org/W2163568483', 'https://openalex.org/W1940312882', 'https://openalex.org/W6686867909', 'https://openalex.org/W1998140754', 'https://openalex.org/W1974300423', 'https://openalex.org/W6629513527', 'https://openalex.org/W1982673570', 'https://openalex.org/W4211231030', 'https://openalex.org/W2151245740', 'https://openalex.org/W2139735657', 'https://openalex.org/W6729047651', 'https://openalex.org/W2154841487', 'https://openalex.org/W1971493157', 'https://openalex.org/W2174276655', 'https://openalex.org/W6741723033', 'https://openalex.org/W2010334716', 'https://openalex.org/W1999664717', 'https://openalex.org/W2091312849', 'https://openalex.org/W4251711635', 'https://openalex.org/W1966004642', 'https://openalex.org/W2166558205', 'https://openalex.org/W2008912500', 'https://openalex.org/W2059927004', 'https://openalex.org/W1906285364', 'https://openalex.org/W3010506491', 'https://openalex.org/W1512123845', 'https://openalex.org/W751684614', 'https://openalex.org/W1901646636', 'https://openalex.org/W125452022', 'https://openalex.org/W2736443522', 'https://openalex.org/W2402426311', 'https://openalex.org/W1493346876', 'https://openalex.org/W1494287956', 'https://openalex.org/W2536271585', 'https://openalex.org/W3204855000', 'https://openalex.org/W648624281', 'https://openalex.org/W3123961192', 'https://openalex.org/W2340833527', 'https://openalex.org/W3128065392', 'https://openalex.org/W3122518304', 'https://openalex.org/W2798073141', 'https://openalex.org/W1489939723', 'https://openalex.org/W4298411395', 'https://openalex.org/W146927993', 'https://openalex.org/W2185930956', 'https://openalex.org/W2579406683', 'https://openalex.org/W1577013761', 'https://openalex.org/W2963535670', 'https://openalex.org/W2962968839', 'https://openalex.org/W2039568841', 'https://openalex.org/W2292826502', 'https://openalex.org/W2902620155', 'https://openalex.org/W1569111531', 'https://openalex.org/W4387864998', 'https://openalex.org/W228337361', 'https://openalex.org/W134527144']",2017-08-10
https://openalex.org/W2336031501,,An Emotion-Based Method to Perform Algorithmic Composition,"The generative music using algorithmic composition techniques has been developed in many years. However it usually lacks of emotion-based mechanism to generate music with specific affective features. In this article the automated music algorithm will be performed based on Prof. Phil Winosr’s “MusicSculptor” software with proper emotion parameter mapping to drive the music content with specific context using various music pa-rameters distribution with different probability control, in order to generate the necessary music emotion automatically. When the emotion scenario varies, the generative music will be logically made via the emotion and context control based on the emotion music classification method. This innovative technique not only generates the emotion music according to the scenario, but also plays the different content of the music every time to make listeners feel “fresh”. The emotion music classification method and the automated music development can be analyzed as the reference for the input of the automated music program. The result shows the proposed method generating music emotions successfully such as happy, angry, sad, and joy, with the correspondent parameter mapping between music and emotion. Although this paper only demonstrates the possibility of emotion-based algorithmic composition, hopefully the proposed idea can be extended to apply into the fields including multimedia and game, to make the background music automatically generated any time according to the context changed by the interaction between human and machine.","['https://openalex.org/W599334349', 'https://openalex.org/W2131705633', 'https://openalex.org/W1866281937', 'https://openalex.org/W2153771043', 'https://openalex.org/W1594185851', 'https://openalex.org/W1528099392']",2013-01-01
https://openalex.org/W1983627329,https://doi.org/10.1037/1196-1961.51.4.336,An exploratory study of musical emotions and psychophysiology.,"A basic issue about musical emotions concerns whether music elicits emotional responses in listeners (the 'emotivist' position) or simply expresses emotions that listeners recognize in the music (the 'cognitivist' position). To address this, psychophysiological measures were recorded while listners heard two excerpts chosen to represent each of three emotions: sad, fear, and happy. The measures covered a fairly wide spectrum of cardiac, vascular, electrodermal, and respiratory functions. Other subjects indicated dynamic changes in emotions they experienced while listening to the music on one of four scales: sad, fear, happy, and tension. Both physiological and emotion judgements were made on a second-by-second basis. The physiological measures all showed a significant effect of music compared to the pre-music interval. A number of analyses, including correlations between physiology and emotion judgments, found significant differences among the excerpts. The sad excerpts produced the largest changes in heart rate, blood pressure, skin conductance and temperature. The fear excerpts produced the largest changes in blood transit time and amplitude. The happy excerpts produced the largest changes in the measures of respiration. These emotion-specific physiological changes only partially replicated those found for nonmusical emotions. The physiological effects of music observed generally support the emotivist view of musical emotions.","['https://openalex.org/W4285719527', 'https://openalex.org/W2014937798', 'https://openalex.org/W2156948210', 'https://openalex.org/W2346038918', 'https://openalex.org/W2344527636', 'https://openalex.org/W2019581685', 'https://openalex.org/W2092914491', 'https://openalex.org/W2314687552', 'https://openalex.org/W2129800229', 'https://openalex.org/W2102610783', 'https://openalex.org/W2045889034', 'https://openalex.org/W2141067536', 'https://openalex.org/W2135735899', 'https://openalex.org/W1976314757', 'https://openalex.org/W2316487833', 'https://openalex.org/W2146355519', 'https://openalex.org/W2049586113', 'https://openalex.org/W2024765709', 'https://openalex.org/W1642204535', 'https://openalex.org/W2128323795', 'https://openalex.org/W2030986414', 'https://openalex.org/W2155060818', 'https://openalex.org/W1988060680', 'https://openalex.org/W2001172750', 'https://openalex.org/W2016737068', 'https://openalex.org/W2061796073', 'https://openalex.org/W2169945686', 'https://openalex.org/W2010677453', 'https://openalex.org/W1975911018', 'https://openalex.org/W1999908431', 'https://openalex.org/W2144840087', 'https://openalex.org/W2046513060', 'https://openalex.org/W2490910133', 'https://openalex.org/W2023543171', 'https://openalex.org/W2167022271', 'https://openalex.org/W2079975276', 'https://openalex.org/W1991697485', 'https://openalex.org/W2162719613', 'https://openalex.org/W2043947177', 'https://openalex.org/W2045565604', 'https://openalex.org/W1591116857', 'https://openalex.org/W2042491873', 'https://openalex.org/W2013439958', 'https://openalex.org/W1555461790', 'https://openalex.org/W2061131717', 'https://openalex.org/W2006519813', 'https://openalex.org/W2023723978', 'https://openalex.org/W2045034970', 'https://openalex.org/W2150851753', 'https://openalex.org/W2137250552', 'https://openalex.org/W2339542104', 'https://openalex.org/W2137672845', 'https://openalex.org/W1612190970', 'https://openalex.org/W2986108748', 'https://openalex.org/W2164517627', 'https://openalex.org/W2110085568', 'https://openalex.org/W1553424343']",1997-12-01
https://openalex.org/W2108032748,,Composing with computers: a survey of some compositional formalisms and music programming languages,,[],1989-08-01
https://openalex.org/W2068627759,https://doi.org/10.1162/comj_a_00105,Generative Musical Tension Modeling and Its Application to Dynamic Sonification,"March 01 2012 Generative Musical Tension Modeling and Its Application to Dynamic Sonification Ryan Nikolaidis, Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Bruce Walker, Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Gil Weinberg Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Search for other works by this author on: This Site Google Scholar Author and Article Information Ryan Nikolaidis Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Bruce Walker Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Gil Weinberg Georgia Tech Center for Music Technology, 840 McMillan St, Atlanta, Georgia 30320, USA. ryannikolaidis@gmail.combruce.walker@psych.gatech.edugil.weinberg@coa.gatech.edu Online ISSN: 1531-5169 Print ISSN: 0148-9267 © 2012 Massachusetts Institute of Technology.2012 Computer Music Journal (2012) 36 (1): 55–64. https://doi.org/10.1162/COMJ_a_00105 Cite Icon Cite Permissions Share Icon Share Facebook Twitter LinkedIn Email Views Icon Views Article contents Figures & tables Video Audio Supplementary Data Peer Review Search Site Citation Ryan Nikolaidis, Bruce Walker, Gil Weinberg; Generative Musical Tension Modeling and Its Application to Dynamic Sonification. Computer Music Journal 2012; 36 (1): 55–64. doi: https://doi.org/10.1162/COMJ_a_00105 Download citation file: Ris (Zotero) Reference Manager EasyBib Bookends Mendeley Papers EndNote RefWorks BibTex toolbar search Search Dropdown Menu toolbar search search input Search input auto suggest filter your search All ContentAll JournalsComputer Music Journal Search Advanced Search This content is only available as a PDF. © 2012 Massachusetts Institute of Technology.2012 Article PDF first page preview Close Modal You do not currently have access to this content.","['https://openalex.org/W2021275403', 'https://openalex.org/W4230995136', 'https://openalex.org/W2023412561', 'https://openalex.org/W2005177809', 'https://openalex.org/W2021311662', 'https://openalex.org/W4240740640', 'https://openalex.org/W2152287648', 'https://openalex.org/W2045034970', 'https://openalex.org/W1520825245', 'https://openalex.org/W3098011476', 'https://openalex.org/W2496469216', 'https://openalex.org/W2031344788', 'https://openalex.org/W2149619252', 'https://openalex.org/W1595115735', 'https://openalex.org/W1533710763', 'https://openalex.org/W2137509658', 'https://openalex.org/W2088418338', 'https://openalex.org/W1569111531', 'https://openalex.org/W1999528738', 'https://openalex.org/W2140762840', 'https://openalex.org/W2027867296', 'https://openalex.org/W2277368141']",2012-02-28
https://openalex.org/W52081385,,Real-Time Music Generation for a Virtual Environment,We describe how we are adapting musical techniques used in films to build a computer program to generate atmospheric music suited to an educational virtual environment. The generator produces music to convey fear using suspense and surprise. The paper motivates the search for a mapping between these emotions and musical structure and outlines how the music generator is implemented. It also explains how we intend to evaluate the effects the music has on the users&amp;apos; subjective sense of presence or &amp;quot;being there&amp;quot;.,"['https://openalex.org/W2006464497', 'https://openalex.org/W1815090327', 'https://openalex.org/W2058300759', 'https://openalex.org/W2163039781', 'https://openalex.org/W2039423452', 'https://openalex.org/W2069842197', 'https://openalex.org/W2082927583', 'https://openalex.org/W1878261120', 'https://openalex.org/W2112349754', 'https://openalex.org/W10838801', 'https://openalex.org/W52432549', 'https://openalex.org/W2086771214', 'https://openalex.org/W1522042771']",1998-01-01
https://openalex.org/W1976814248,https://doi.org/10.1525/mp.2004.22.1.15,The Feeling of Music Past: How Listeners Remember Musical Affect,"This study was conducted to determine how listeners derive global evaluations of past musical durations from moment-to-moment experience. Participants produced moment-to-moment affective intensity ratings by pressing a pressure-sensitive button while listening to various selections. They later reported the remembered affective intensity of each example. The data suggest that the assumption that remembered affect equals the sum of all momentary affects fundamentally misrepresents how listeners encode and label past affective experiences. The duration of particular rather than uniform episodes contributes minimally to remembered affect (duration neglect). Listeners rely on the peak of affective intensity during a selection, the last moment, and moments that are more emotionally intense than immediately previous moments to determine postperformance ratings. The peak proves to be the strongest predictor of remembered affect. We derive a formula that takes moment-to-moment experience as input and predicts how listeners will remember musical affect. The formula is a better predictor of postperformance affect than any other on-line characteristic considered. Last, the utility of the formula is demonstrated through a brief examination of compositional decisions in a string quartet movement by Borodin and one typical format of four-movement symphonies from the classical period.","['https://openalex.org/W1992616068', 'https://openalex.org/W4248374506', 'https://openalex.org/W2037993836', 'https://openalex.org/W2102610783', 'https://openalex.org/W2038865874', 'https://openalex.org/W1987183503', 'https://openalex.org/W4249867000', 'https://openalex.org/W1990614265', 'https://openalex.org/W4238095641', 'https://openalex.org/W2089867299', 'https://openalex.org/W1988166588', 'https://openalex.org/W2314687552', 'https://openalex.org/W1994865232', 'https://openalex.org/W4249165595', 'https://openalex.org/W2091449145', 'https://openalex.org/W2089278712', 'https://openalex.org/W1973893087', 'https://openalex.org/W2150851753', 'https://openalex.org/W2007082084', 'https://openalex.org/W2047432625', 'https://openalex.org/W2020974330', 'https://openalex.org/W2038007153', 'https://openalex.org/W2028555134', 'https://openalex.org/W1991754315', 'https://openalex.org/W2167022271', 'https://openalex.org/W2038481898', 'https://openalex.org/W2101240583', 'https://openalex.org/W1645964347', 'https://openalex.org/W2019305288', 'https://openalex.org/W44481585', 'https://openalex.org/W2014937798', 'https://openalex.org/W2083094773', 'https://openalex.org/W1923965623', 'https://openalex.org/W2023723978', 'https://openalex.org/W2505085393', 'https://openalex.org/W2034922209', 'https://openalex.org/W1977343123', 'https://openalex.org/W1666941850', 'https://openalex.org/W2127283001', 'https://openalex.org/W1642204535', 'https://openalex.org/W2500195762', 'https://openalex.org/W2004142478', 'https://openalex.org/W4213206451', 'https://openalex.org/W1988665788', 'https://openalex.org/W2322936702']",2004-01-01
https://openalex.org/W3128065392,,An Experiment in the automatic creation of music which has specific emotional content,,"['https://openalex.org/W2046513060', 'https://openalex.org/W2006915528', 'https://openalex.org/W52081385', 'https://openalex.org/W2082927583', 'https://openalex.org/W2006464497']",2002-01-01
https://openalex.org/W3158589047,,Computer-Generating emotional music: The design of an affective music algorithm,"This paper explores one way to use music in the context of affective design. We&amp;apos;ve made a real-time music generator that is designed around the concepts of valence and arousal, which are two components of certain models of emotion. When set to a desired valence and arousal, the algorithm plays music corresponding to the intersection of these two parameters. We designed our algorithm using psychological theory of emotion and parametrized features of music which have been tested for affect. The results are a modular algorithm design, in which our parameters can be implemented in other affective music algorithms. We describe our implementation of these parameters, and our strategy for manipulating the parameters to generate musical emotion. Finally we discuss possible applications for these techniques in the fields of the arts, medical systems, and research applications. We believe","['https://openalex.org/W1987183503', 'https://openalex.org/W2149628368', 'https://openalex.org/W2147957201', 'https://openalex.org/W2031971640', 'https://openalex.org/W2098905310', 'https://openalex.org/W2042362371', 'https://openalex.org/W2035934632', 'https://openalex.org/W1990114388', 'https://openalex.org/W2054560711', 'https://openalex.org/W2156848952', 'https://openalex.org/W1569314909', 'https://openalex.org/W2003653478', 'https://openalex.org/W1988060680', 'https://openalex.org/W1694080526']",2008-12-01
https://openalex.org/W2948210185,https://doi.org/10.48550/arxiv.1906.02629,When Does Label Smoothing Help?,"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.","['https://openalex.org/W2951093852', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964081807', 'https://openalex.org/W2593634001', 'https://openalex.org/W2133671888', 'https://openalex.org/W1541007220', 'https://openalex.org/W2151058089', 'https://openalex.org/W1529808766', 'https://openalex.org/W2785430118', 'https://openalex.org/W2194775991', 'https://openalex.org/W2950850913', 'https://openalex.org/W1498436455', 'https://openalex.org/W2950300355', 'https://openalex.org/W2918914336', 'https://openalex.org/W2626967530', 'https://openalex.org/W1821462560', 'https://openalex.org/W2790319220']",2019-06-06
https://openalex.org/W2530921900,https://doi.org/10.1121/1.4964509,Mechanics of human voice production and control,"As the primary means of communication, voice plays an important role in daily life. Voice also conveys personal information such as social status, personal traits, and the emotional state of the speaker. Mechanically, voice production involves complex fluid-structure interaction within the glottis and its control by laryngeal muscle activation. An important goal of voice research is to establish a causal theory linking voice physiology and biomechanics to how speakers use and control voice to communicate meaning and personal information. Establishing such a causal theory has important implications for clinical voice management, voice training, and many speech technology applications. This paper provides a review of voice physiology and biomechanics, the physics of vocal fold vibration and sound production, and laryngeal muscular control of the fundamental frequency of voice, vocal intensity, and voice quality. Current efforts to develop mechanical and computational models of voice production are also critically reviewed. Finally, issues and future challenges in developing a causal theory of voice production and perception are discussed.","['https://openalex.org/W2094982465', 'https://openalex.org/W2040575518', 'https://openalex.org/W2095062982', 'https://openalex.org/W1964029979', 'https://openalex.org/W1977957886', 'https://openalex.org/W2110567885', 'https://openalex.org/W1982473482', 'https://openalex.org/W2061302153', 'https://openalex.org/W1965758579', 'https://openalex.org/W2064746403', 'https://openalex.org/W1988067536', 'https://openalex.org/W1995421863', 'https://openalex.org/W2015525113', 'https://openalex.org/W2070115690', 'https://openalex.org/W1986112602', 'https://openalex.org/W2032189819', 'https://openalex.org/W2153115365', 'https://openalex.org/W1975114340', 'https://openalex.org/W2143535023', 'https://openalex.org/W2052016309', 'https://openalex.org/W2076626472', 'https://openalex.org/W2028203509', 'https://openalex.org/W2039461452', 'https://openalex.org/W2018837547', 'https://openalex.org/W2139135163', 'https://openalex.org/W2152753979', 'https://openalex.org/W2595862309', 'https://openalex.org/W1494707595', 'https://openalex.org/W2152002570', 'https://openalex.org/W2109138290', 'https://openalex.org/W4301035235', 'https://openalex.org/W2043350081', 'https://openalex.org/W2042112073', 'https://openalex.org/W2358981474', 'https://openalex.org/W1975643017', 'https://openalex.org/W2183682207', 'https://openalex.org/W2013301322', 'https://openalex.org/W2090814521', 'https://openalex.org/W405530312', 'https://openalex.org/W2027128447', 'https://openalex.org/W2006633963', 'https://openalex.org/W1964238143', 'https://openalex.org/W2007977421', 'https://openalex.org/W776759059', 'https://openalex.org/W6675619084', 'https://openalex.org/W2009179928', 'https://openalex.org/W1982864978', 'https://openalex.org/W2031552974', 'https://openalex.org/W2008161731', 'https://openalex.org/W2065534499', 'https://openalex.org/W2038299421', 'https://openalex.org/W2024677195', 'https://openalex.org/W2072346640', 'https://openalex.org/W1980326411', 'https://openalex.org/W1607893546', 'https://openalex.org/W2039052447', 'https://openalex.org/W2115013023', 'https://openalex.org/W2063660115', 'https://openalex.org/W2058699203', 'https://openalex.org/W2530512168', 'https://openalex.org/W2125799451', 'https://openalex.org/W2066452495', 'https://openalex.org/W2022194925', 'https://openalex.org/W2038799060', 'https://openalex.org/W2051522690', 'https://openalex.org/W2162219389', 'https://openalex.org/W2156033279', 'https://openalex.org/W1987479659', 'https://openalex.org/W2013731074', 'https://openalex.org/W1990285726', 'https://openalex.org/W2029778422', 'https://openalex.org/W2086203763', 'https://openalex.org/W2080731899', 'https://openalex.org/W2153181731', 'https://openalex.org/W2002397557', 'https://openalex.org/W1974909648', 'https://openalex.org/W2025714935', 'https://openalex.org/W2034628183', 'https://openalex.org/W2033232718', 'https://openalex.org/W1997937504', 'https://openalex.org/W2041400124', 'https://openalex.org/W2046521454', 'https://openalex.org/W2068946105', 'https://openalex.org/W2065925624', 'https://openalex.org/W2027568062', 'https://openalex.org/W2043540483', 'https://openalex.org/W1968873395', 'https://openalex.org/W2026321272', 'https://openalex.org/W1978606468', 'https://openalex.org/W2023694851', 'https://openalex.org/W1988755520', 'https://openalex.org/W2038408582', 'https://openalex.org/W1967085469', 'https://openalex.org/W2048235682', 'https://openalex.org/W2164874096', 'https://openalex.org/W2058255766', 'https://openalex.org/W4252216513', 'https://openalex.org/W2009119805', 'https://openalex.org/W2034079859', 'https://openalex.org/W2022880408', 'https://openalex.org/W2136336022', 'https://openalex.org/W1994171962', 'https://openalex.org/W2068854338', 'https://openalex.org/W4255696735', 'https://openalex.org/W2129444754', 'https://openalex.org/W1978472421', 'https://openalex.org/W2089935964', 'https://openalex.org/W2026952173', 'https://openalex.org/W1985492960', 'https://openalex.org/W2017857509', 'https://openalex.org/W2014064104', 'https://openalex.org/W2621688926', 'https://openalex.org/W2031905022', 'https://openalex.org/W2085538985', 'https://openalex.org/W2089178993', 'https://openalex.org/W2009589082', 'https://openalex.org/W2044675173', 'https://openalex.org/W2085864317', 'https://openalex.org/W2323816064', 'https://openalex.org/W2088381222', 'https://openalex.org/W916685487', 'https://openalex.org/W2469660560', 'https://openalex.org/W2050788534', 'https://openalex.org/W4239050591', 'https://openalex.org/W2092262863', 'https://openalex.org/W1996481850', 'https://openalex.org/W2030547377', 'https://openalex.org/W1967199098', 'https://openalex.org/W2082358005', 'https://openalex.org/W2015557618', 'https://openalex.org/W2058720914', 'https://openalex.org/W2002171216', 'https://openalex.org/W2042600257', 'https://openalex.org/W2051682525', 'https://openalex.org/W2029157916', 'https://openalex.org/W2315535101', 'https://openalex.org/W2173811272', 'https://openalex.org/W2092802946', 'https://openalex.org/W2079021492', 'https://openalex.org/W2090006833', 'https://openalex.org/W2065500337', 'https://openalex.org/W1997340369', 'https://openalex.org/W1983879173', 'https://openalex.org/W2063760525', 'https://openalex.org/W2092417163', 'https://openalex.org/W2130559863', 'https://openalex.org/W2321061684', 'https://openalex.org/W355475102', 'https://openalex.org/W4243259534', 'https://openalex.org/W1970555440', 'https://openalex.org/W2005060630', 'https://openalex.org/W98407407', 'https://openalex.org/W2102622981', 'https://openalex.org/W2079350716', 'https://openalex.org/W1605956270', 'https://openalex.org/W2154650517', 'https://openalex.org/W1524703083']",2016-10-01
https://openalex.org/W2606176153,https://doi.org/10.48550/arxiv.1704.01279,Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders,"Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.","['https://openalex.org/W1502260477', 'https://openalex.org/W1566660863', 'https://openalex.org/W2949117887', 'https://openalex.org/W2139788486', 'https://openalex.org/W2559688696', 'https://openalex.org/W2949899814', 'https://openalex.org/W1878791236', 'https://openalex.org/W2560512785', 'https://openalex.org/W2475687244', 'https://openalex.org/W2132085292', 'https://openalex.org/W2522389179', 'https://openalex.org/W2335728318', 'https://openalex.org/W2432004435', 'https://openalex.org/W2145094598', 'https://openalex.org/W2108598243', 'https://openalex.org/W2949382160', 'https://openalex.org/W2953318193', 'https://openalex.org/W2076608692', 'https://openalex.org/W1556219185', 'https://openalex.org/W1607142029', 'https://openalex.org/W2584032004', 'https://openalex.org/W2120847449', 'https://openalex.org/W2293272996', 'https://openalex.org/W2119929864', 'https://openalex.org/W3118608800', 'https://openalex.org/W1522301498']",2017-04-05
https://openalex.org/W2164767140,https://doi.org/10.1121/1.407371,Isolating the dynamic attributes of musical timbrea),"Three experiments examined the dynamic attributes of timbre by evaluating the role of onsets in similarity judgments. In separate experiments, subjects heard complete orchestral instrument tones, the onsets of those tones, and tones with the onsets removed (‘‘remainders’’). Ratings for complete tones corresponded to those for onsets, indicating that the salient acoustic attributes for complete tones are present at the onset. Ratings for complete tones also corresponded to those for remainders, indicating that the salient attributes for complete tones are present also in the absence of onsets. Subsequent acoustic analyses demonstrated that this pattern of similarity was due to the centroid frequencies and amplitude envelopes of the tones. The results indicate that the dynamic attributes of timbre are not only present at the onset, but also throughout, and that multiple acoustic attributes may contribute to the same perceptual dimensions.","['https://openalex.org/W1995268113', 'https://openalex.org/W1970423186', 'https://openalex.org/W1986595194', 'https://openalex.org/W2016645383', 'https://openalex.org/W2169371330', 'https://openalex.org/W2105672294', 'https://openalex.org/W1973192023', 'https://openalex.org/W1500344824', 'https://openalex.org/W2432517183', 'https://openalex.org/W2079080230', 'https://openalex.org/W2152825437', 'https://openalex.org/W1986023967', 'https://openalex.org/W2319608925', 'https://openalex.org/W2115067604', 'https://openalex.org/W2406477717']",1993-11-01
https://openalex.org/W2115067604,https://doi.org/10.1111/j.1467-9450.1972.tb00071.x,DIMENSION ANALYSIS OF THE PERCEPTION OF INSTRUMENTAL TIMBRE,"W edin L. &amp; G oude G. Dimension analysis of the perception of instrumental timbre. Scand. J. Psychol ., 1972, 1 3 , 228–240.—The dimensionality of the perception of instrumental timbre for single tones was investigated through multidimensional scaling according to Ekman's vector model of similarity. Three factors were extracted and they were identified with certain characteristics of the spectrum envelopes. It was found that initial transients were important for the identifiability of instrumental tones but their presence or absence did not influence the dimensional structure. The “perceptual structure” was found to be different from the “cognitive structure” (= knowledge about the classification of the instruments), and the results are general for both trained and naive listeners.","['https://openalex.org/W4255089681', 'https://openalex.org/W1970423186', 'https://openalex.org/W2033477891', 'https://openalex.org/W2008692283', 'https://openalex.org/W2020866611', 'https://openalex.org/W2063427269', 'https://openalex.org/W586186463', 'https://openalex.org/W4230165289', 'https://openalex.org/W2145944162', 'https://openalex.org/W2065143235', 'https://openalex.org/W2017048413', 'https://openalex.org/W2796384094', 'https://openalex.org/W615550134', 'https://openalex.org/W3142029008', 'https://openalex.org/W2066186107', 'https://openalex.org/W1970144714']",1972-09-01
https://openalex.org/W2143966220,https://doi.org/10.1007/bf02294578,"A Latent Class Approach to Fitting the Weighted Euclidean Model, Clascal","A weighted Euclidean distance model for analyzing three-way proximity data is proposed that incorporates a latent class approach. In this latent class weighted Euclidean model, the contribution to the distance function between two stimuli is per dimension weighted identically by all subjects in the same latent class. This model removes the rotational invariance of the classical multidimensional scaling model retaining psychologically meaningful dimensions, and drastically reduces the number of parameters in the traditional INDSCAL model. The probability density function for the data of a subject is posited to be a finite mixture of spherical multivariate normal densities. The maximum likelihood function is optimized by means of an EM algorithm; a modified Fisher scoring method is used to update the parameters in the M-step. A model selection strategy is proposed and illustrated on both real and artificial data.","['https://openalex.org/W1983748164', 'https://openalex.org/W1999597013', 'https://openalex.org/W2242071559', 'https://openalex.org/W2049633694', 'https://openalex.org/W1981903823', 'https://openalex.org/W2036683047', 'https://openalex.org/W1976647033', 'https://openalex.org/W2000215628', 'https://openalex.org/W2021800416', 'https://openalex.org/W1965323816', 'https://openalex.org/W196172961', 'https://openalex.org/W2905482938', 'https://openalex.org/W2080900581', 'https://openalex.org/W1990211480', 'https://openalex.org/W1988662480', 'https://openalex.org/W2056861650', 'https://openalex.org/W2168175751', 'https://openalex.org/W2030125991', 'https://openalex.org/W2094828137', 'https://openalex.org/W139757862', 'https://openalex.org/W84440327', 'https://openalex.org/W2033304942', 'https://openalex.org/W2796766534', 'https://openalex.org/W2554987453', 'https://openalex.org/W3129711340', 'https://openalex.org/W2038069242', 'https://openalex.org/W2975800720', 'https://openalex.org/W178857048', 'https://openalex.org/W2102491957', 'https://openalex.org/W2056930330', 'https://openalex.org/W2004748399', 'https://openalex.org/W2327624535', 'https://openalex.org/W4232492049']",1993-06-01
https://openalex.org/W2026736703,https://doi.org/10.1097/01.nnr.0000280659.88760.7c,Multidimensional Scaling,"Using MDS is ideal for the study of such complicated issues as a patient's perception of cancer pain, breathlessness in individuals with chronic obstructive pulmonary disease, and the assessment of vulnerable populations where social desirability bias is an issue.","['https://openalex.org/W2000215628', 'https://openalex.org/W4249361171', 'https://openalex.org/W2080969943', 'https://openalex.org/W2097643740', 'https://openalex.org/W2114771534', 'https://openalex.org/W2168735975', 'https://openalex.org/W2166305016', 'https://openalex.org/W3121431905', 'https://openalex.org/W42853988', 'https://openalex.org/W2068131344', 'https://openalex.org/W2068775959', 'https://openalex.org/W2172294513', 'https://openalex.org/W2104762987', 'https://openalex.org/W2139352318', 'https://openalex.org/W6645384488', 'https://openalex.org/W2050985708', 'https://openalex.org/W2169371330', 'https://openalex.org/W1993436046', 'https://openalex.org/W2163021033', 'https://openalex.org/W6635738559', 'https://openalex.org/W2083691223', 'https://openalex.org/W1597579683', 'https://openalex.org/W2055935157', 'https://openalex.org/W1979766417']",2008-01-01
https://openalex.org/W2169371330,https://doi.org/10.1007/bf02289630,The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.,"A computer program is described that is designed to reconstruct the metric configuration of a set of points in Euclidean space on the basis of essentially nonmetric information about that configuration. A minimum set of Cartesian coordinates for the points is determined when the only available information specifies for each pair of those points—not the distance between them—but some unknown, fixed monotonic function of that distance. The program is proposed as a tool for reductively analyzing several types of psychological data, particularly measures of interstimulus similarity or confusability, by making explicit the multidimensional structure underlying such data.","['https://openalex.org/W2328468864', 'https://openalex.org/W1967282267', 'https://openalex.org/W2013162218', 'https://openalex.org/W2018294412', 'https://openalex.org/W2416422682', 'https://openalex.org/W2042374674', 'https://openalex.org/W2024355049', 'https://openalex.org/W2033253727', 'https://openalex.org/W2087087428', 'https://openalex.org/W2050985708', 'https://openalex.org/W2045314411', 'https://openalex.org/W1966564306', 'https://openalex.org/W2082236330', 'https://openalex.org/W2016184697', 'https://openalex.org/W1978863299', 'https://openalex.org/W1972903097', 'https://openalex.org/W2064719427', 'https://openalex.org/W1966382716', 'https://openalex.org/W2064296000', 'https://openalex.org/W2007190765', 'https://openalex.org/W2023232184', 'https://openalex.org/W2042122853', 'https://openalex.org/W2152134063', 'https://openalex.org/W2056930330', 'https://openalex.org/W2783254024', 'https://openalex.org/W1969352414', 'https://openalex.org/W1989125394', 'https://openalex.org/W2031557011', 'https://openalex.org/W2486592656', 'https://openalex.org/W1993436046', 'https://openalex.org/W2071519987', 'https://openalex.org/W2075201227', 'https://openalex.org/W1988190207', 'https://openalex.org/W1966178388', 'https://openalex.org/W2152833934', 'https://openalex.org/W2085608881', 'https://openalex.org/W2014619294', 'https://openalex.org/W2975800720', 'https://openalex.org/W1975572869', 'https://openalex.org/W3026302707']",1962-06-01
https://openalex.org/W2910577860,https://doi.org/10.48550/arxiv.1902.08710,GANSynth: Adversarial Neural Audio Synthesis,"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.","['https://openalex.org/W2608907959', 'https://openalex.org/W2963981733', 'https://openalex.org/W2890043615', 'https://openalex.org/W2963609956', 'https://openalex.org/W2099471712', 'https://openalex.org/W2605195953', 'https://openalex.org/W2894295011', 'https://openalex.org/W2963857374', 'https://openalex.org/W2951535099', 'https://openalex.org/W2963571818', 'https://openalex.org/W2963684088', 'https://openalex.org/W2964121744', 'https://openalex.org/W2903035274', 'https://openalex.org/W2963836885', 'https://openalex.org/W2964122153', 'https://openalex.org/W2962879692', 'https://openalex.org/W2519091744', 'https://openalex.org/W2901997113', 'https://openalex.org/W2331927446', 'https://openalex.org/W2893749619', 'https://openalex.org/W2796010067', 'https://openalex.org/W2548275288', 'https://openalex.org/W2924212870', 'https://openalex.org/W2059652044', 'https://openalex.org/W2904367110', 'https://openalex.org/W2963073614', 'https://openalex.org/W1834627138', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962760235', 'https://openalex.org/W2093824445', 'https://openalex.org/W2739748921', 'https://openalex.org/W2903005299', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963373786', 'https://openalex.org/W2775288145', 'https://openalex.org/W2747543643', 'https://openalex.org/W2559246505']",2019-02-23
https://openalex.org/W2963411769,https://doi.org/10.48550/arxiv.1907.10380,NONOTO: A Model-agnostic Web Interface for Interactive Music Composition by Inpainting,"Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.","['https://openalex.org/W2916279611', 'https://openalex.org/W2891815651', 'https://openalex.org/W2099057450', 'https://openalex.org/W2552465644', 'https://openalex.org/W2514141612', 'https://openalex.org/W2807633959', 'https://openalex.org/W1819710477', 'https://openalex.org/W2963032576', 'https://openalex.org/W2963575853']",2019-07-23
https://openalex.org/W2099773255,https://doi.org/10.1080/09298215.2013.821496,"Breathy, Resonant, Pressed – Automatic Detection of Phonation Mode from Audio Recordings of Singing","Abstract In this paper we present an experiment on automatic detection of phonation modes from recordings of sustained sung vowels. We created an open dataset specifically for this experiment, containing recordings of nine vowels from multiple languages, sung by a female singer on all pitches in her vocal range in phonation modes breathy, neutral, flow (resonant) and pressed. The dataset is available under a Creative Commons license at http://www.proutskova.de/phonation-modes. First, glottal flow waveform is estimated via inverse filtering (IAIF) from audio recordings. Then six parameters of the glottal flow waveform are calculated. A 4-class Support Vector Machine classifier is constructed to separate these features into phonation mode classes. We automated the IAIF approach by computing the values of the input arguments – lip radiation and formant count – leading to the best-performing SVM classifiers (average classification accuracy over 60%), yielding a physical model for the articulation of the vowels. We examine the steps needed to generalize and extend the experimental work presented in this paper in order to apply this method in ethnomusicological investigations. Acknowledgments We would like to sincerely thank Victor Grauer, the co-inventor of Cantometrics, for suggesting to focus on the subordination of women hypothesis, which gave us the inspiration and the motivation for the current work. We are grateful to prof. Johan Sundberg for his recommendations, e.g. on the recordings set up and his general support. Our special thanks go to the peer reviewers for their balanced, insightful and fair reviews, which helped us to substantially revise the text. Notes http://www.youtube.com/watch?v=MLU0jndUGg4 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=k4SLSlSmW74 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=7iQQGBfbB0k (last accessed on 30/10/2012) http://www.youtube.com/watch?v=hRyDB4RWJdw (last accessed on 30/10/2012) http://www.youtube.com/watch?v=rgusCINe260 (last accessed on 30/10/2012) http://www.youtube.com/watch?v=XgDrJ5Z2rKw (last accessed on 30/10/2012)","['https://openalex.org/W2069702054', 'https://openalex.org/W2021320575', 'https://openalex.org/W2171015304', 'https://openalex.org/W1999319498', 'https://openalex.org/W4252021498', 'https://openalex.org/W1990037236', 'https://openalex.org/W2120349113', 'https://openalex.org/W2085475313', 'https://openalex.org/W1992761016', 'https://openalex.org/W1966611275', 'https://openalex.org/W2143429466', 'https://openalex.org/W2526416991', 'https://openalex.org/W181056519', 'https://openalex.org/W1581879603', 'https://openalex.org/W2040468544', 'https://openalex.org/W2318930124', 'https://openalex.org/W2051930451', 'https://openalex.org/W134063646', 'https://openalex.org/W2589125832', 'https://openalex.org/W2269330302', 'https://openalex.org/W2333960153', 'https://openalex.org/W1988996683', 'https://openalex.org/W2460037160', 'https://openalex.org/W4285719527', 'https://openalex.org/W2153635508', 'https://openalex.org/W1522757690', 'https://openalex.org/W2074875921', 'https://openalex.org/W1655514788', 'https://openalex.org/W2486339682', 'https://openalex.org/W2148257019', 'https://openalex.org/W2484545578', 'https://openalex.org/W2148486644', 'https://openalex.org/W2466270935', 'https://openalex.org/W1602667774', 'https://openalex.org/W1987280716', 'https://openalex.org/W2317390345']",2013-06-01
https://openalex.org/W1998871699,https://doi.org/10.1007/bf02289588,Hierarchical Clustering Schemes,"Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally “connected,” while the other forms clusters that are optimally “compact.”","['https://openalex.org/W2054541372', 'https://openalex.org/W2152825437', 'https://openalex.org/W2169371330', 'https://openalex.org/W6657053522', 'https://openalex.org/W2016381774', 'https://openalex.org/W1970533835', 'https://openalex.org/W4256550993', 'https://openalex.org/W2611775752', 'https://openalex.org/W2560320127', 'https://openalex.org/W2118942683', 'https://openalex.org/W2026513874']",1967-09-01
https://openalex.org/W1981455444,https://doi.org/10.1371/journal.pone.0089642,The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population,"Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of 'musical sophistication' which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.","['https://openalex.org/W2068085668', 'https://openalex.org/W2071715685', 'https://openalex.org/W2128251500', 'https://openalex.org/W2072061347', 'https://openalex.org/W2140149555', 'https://openalex.org/W2044645678', 'https://openalex.org/W2104521923', 'https://openalex.org/W1971080687', 'https://openalex.org/W2062639826', 'https://openalex.org/W1553384694', 'https://openalex.org/W2020333286', 'https://openalex.org/W2034751569', 'https://openalex.org/W1993075739', 'https://openalex.org/W1985220610', 'https://openalex.org/W2277513127', 'https://openalex.org/W2045300586', 'https://openalex.org/W2008493046', 'https://openalex.org/W2062432812', 'https://openalex.org/W2135347478', 'https://openalex.org/W2095322888', 'https://openalex.org/W4243229384', 'https://openalex.org/W1979304160', 'https://openalex.org/W2056735560', 'https://openalex.org/W2104665609', 'https://openalex.org/W2117195184', 'https://openalex.org/W2131216245', 'https://openalex.org/W1911060607', 'https://openalex.org/W1967943084', 'https://openalex.org/W2143884780', 'https://openalex.org/W2012587754', 'https://openalex.org/W2007944273', 'https://openalex.org/W1974760978', 'https://openalex.org/W2000405415', 'https://openalex.org/W2084579522', 'https://openalex.org/W2156909242', 'https://openalex.org/W2098288945', 'https://openalex.org/W1980862600', 'https://openalex.org/W2067154629', 'https://openalex.org/W2057528333', 'https://openalex.org/W2148108292', 'https://openalex.org/W2140430000', 'https://openalex.org/W2115591681', 'https://openalex.org/W2049603114', 'https://openalex.org/W2168729844', 'https://openalex.org/W2122081131', 'https://openalex.org/W2115826198', 'https://openalex.org/W2133097426', 'https://openalex.org/W1990517582', 'https://openalex.org/W2133907989', 'https://openalex.org/W2094102654', 'https://openalex.org/W2050853365', 'https://openalex.org/W2014480729', 'https://openalex.org/W2025843993', 'https://openalex.org/W2123478340', 'https://openalex.org/W2889272830', 'https://openalex.org/W2162929365', 'https://openalex.org/W4254972346', 'https://openalex.org/W1680939745', 'https://openalex.org/W2094229882', 'https://openalex.org/W2162090451', 'https://openalex.org/W2111621157', 'https://openalex.org/W1981419775', 'https://openalex.org/W2029880546', 'https://openalex.org/W1966169573', 'https://openalex.org/W2060905779', 'https://openalex.org/W2127906110', 'https://openalex.org/W2069682661', 'https://openalex.org/W136572389', 'https://openalex.org/W2103308120', 'https://openalex.org/W2023607107', 'https://openalex.org/W1968671768', 'https://openalex.org/W2094518516', 'https://openalex.org/W2003000826', 'https://openalex.org/W1998848034', 'https://openalex.org/W2131244036', 'https://openalex.org/W2131796923', 'https://openalex.org/W2021263427', 'https://openalex.org/W2015959845', 'https://openalex.org/W2050848840', 'https://openalex.org/W2128086545', 'https://openalex.org/W1987387731', 'https://openalex.org/W1989261615', 'https://openalex.org/W2148503694', 'https://openalex.org/W1999401028', 'https://openalex.org/W2156495989', 'https://openalex.org/W2034518382', 'https://openalex.org/W1992984099', 'https://openalex.org/W2054209121', 'https://openalex.org/W2143037334', 'https://openalex.org/W191886092', 'https://openalex.org/W2086429394', 'https://openalex.org/W2093562142', 'https://openalex.org/W1974416944', 'https://openalex.org/W2163555025', 'https://openalex.org/W2112744711', 'https://openalex.org/W2058821124', 'https://openalex.org/W2101660373', 'https://openalex.org/W2029369015', 'https://openalex.org/W1572815893', 'https://openalex.org/W6610017368', 'https://openalex.org/W2911964244', 'https://openalex.org/W2787894218', 'https://openalex.org/W2084341220', 'https://openalex.org/W2155920093', 'https://openalex.org/W2055992413', 'https://openalex.org/W2328846894', 'https://openalex.org/W2070230130', 'https://openalex.org/W2096732423', 'https://openalex.org/W6783860549', 'https://openalex.org/W2111507348', 'https://openalex.org/W2021370281', 'https://openalex.org/W1985324210', 'https://openalex.org/W2123604154', 'https://openalex.org/W2296116113', 'https://openalex.org/W1557888268', 'https://openalex.org/W2783473407', 'https://openalex.org/W3146697253', 'https://openalex.org/W2983626785', 'https://openalex.org/W1554944419', 'https://openalex.org/W50626113', 'https://openalex.org/W2134630055', 'https://openalex.org/W1003287974', 'https://openalex.org/W2134481098', 'https://openalex.org/W2481502702', 'https://openalex.org/W2081975242', 'https://openalex.org/W2072488690', 'https://openalex.org/W2091938887', 'https://openalex.org/W1825988435', 'https://openalex.org/W4285719527', 'https://openalex.org/W3090632431', 'https://openalex.org/W4237811819', 'https://openalex.org/W2110515480', 'https://openalex.org/W4245314207', 'https://openalex.org/W1500447176', 'https://openalex.org/W591131089', 'https://openalex.org/W2016227990', 'https://openalex.org/W2127359522', 'https://openalex.org/W2000619651', 'https://openalex.org/W2486002147', 'https://openalex.org/W2799099195', 'https://openalex.org/W2314309829', 'https://openalex.org/W2582743722', 'https://openalex.org/W575151914', 'https://openalex.org/W1991490794', 'https://openalex.org/W2047967231', 'https://openalex.org/W273955616', 'https://openalex.org/W2049343712', 'https://openalex.org/W2980946502', 'https://openalex.org/W2022030157', 'https://openalex.org/W2160455305']",2014-02-26
https://openalex.org/W2901638613,https://doi.org/10.1007/s00521-018-3868-4,"Anticipation-RNN: enforcing unary constraints in sequence generation, with application to interactive music generation","Recurrent neural networks (RNNs) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This article introduces a novel architecture called anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined unary constraints. We demonstrate its efficiency on the task of generating melodies satisfying unary constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.","['https://openalex.org/W4229633200', 'https://openalex.org/W2565964938', 'https://openalex.org/W1963871118', 'https://openalex.org/W2588217777', 'https://openalex.org/W4300958603', 'https://openalex.org/W2144499799', 'https://openalex.org/W1583837637', 'https://openalex.org/W2758804652', 'https://openalex.org/W2064675550', 'https://openalex.org/W2162995444', 'https://openalex.org/W2063703901', 'https://openalex.org/W2402555437', 'https://openalex.org/W2530599788', 'https://openalex.org/W2131774270', 'https://openalex.org/W2792210438', 'https://openalex.org/W950853366', 'https://openalex.org/W2962699318', 'https://openalex.org/W2584641794', 'https://openalex.org/W2579406683', 'https://openalex.org/W1819710477', 'https://openalex.org/W1912497050', 'https://openalex.org/W3122518304', 'https://openalex.org/W2523097914', 'https://openalex.org/W2129192849', 'https://openalex.org/W1573082642', 'https://openalex.org/W2740988298', 'https://openalex.org/W2963575853', 'https://openalex.org/W2747545581', 'https://openalex.org/W2557283755']",2018-11-20
https://openalex.org/W2519091744,https://doi.org/10.48550/arxiv.1609.03499,WaveNet: A Generative Model for Raw Audio,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",[],2016-09-12
https://openalex.org/W2955263139,https://doi.org/10.48550/arxiv.1907.00971,Universal audio synthesizer control with normalizing flows,"The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders (VAE) and Normalizing Flows (NF) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live","['https://openalex.org/W1663973292', 'https://openalex.org/W2951004968', 'https://openalex.org/W2560512785', 'https://openalex.org/W2791716806', 'https://openalex.org/W2963090522', 'https://openalex.org/W2055561493', 'https://openalex.org/W2399306074', 'https://openalex.org/W2810655876', 'https://openalex.org/W131146354', 'https://openalex.org/W2963047245', 'https://openalex.org/W2963746531', 'https://openalex.org/W2287082121', 'https://openalex.org/W2804722844', 'https://openalex.org/W2753738274', 'https://openalex.org/W2587284713', 'https://openalex.org/W2269892441']",2019-07-01
https://openalex.org/W3037798801,https://doi.org/10.48550/arxiv.2006.16236,Transformers are RNNs: Fast Autoregressive Transformers with Linear\n Attention,"Transformers achieve remarkable performance in several tasks but due to their\nquadratic complexity, with respect to the input's length, they are\nprohibitively slow for very long sequences. To address this limitation, we\nexpress the self-attention as a linear dot-product of kernel feature maps and\nmake use of the associativity property of matrix products to reduce the\ncomplexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$,\nwhere $N$ is the sequence length. We show that this formulation permits an\niterative implementation that dramatically accelerates autoregressive\ntransformers and reveals their relationship to recurrent neural networks. Our\nlinear transformers achieve similar performance to vanilla transformers and\nthey are up to 4000x faster on autoregressive prediction of very long\nsequences.\n","['https://openalex.org/W2946567085', 'https://openalex.org/W2176412452', 'https://openalex.org/W2964122153', 'https://openalex.org/W2964089206', 'https://openalex.org/W3118608800', 'https://openalex.org/W2131462252', 'https://openalex.org/W2944815030', 'https://openalex.org/W2024490156', 'https://openalex.org/W36903255', 'https://openalex.org/W3120633509', 'https://openalex.org/W2965373594', 'https://openalex.org/W2130942839', 'https://openalex.org/W3000514857', 'https://openalex.org/W2100714283', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963341956', 'https://openalex.org/W3177265267', 'https://openalex.org/W2994759459', 'https://openalex.org/W2968917279', 'https://openalex.org/W2127141656', 'https://openalex.org/W2964110616', 'https://openalex.org/W2970565456', 'https://openalex.org/W2940744433', 'https://openalex.org/W2996035354', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964324019', 'https://openalex.org/W2983902802', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970401203', 'https://openalex.org/W2970423380', 'https://openalex.org/W2964308564', 'https://openalex.org/W2996428491', 'https://openalex.org/W1522301498', 'https://openalex.org/W2970971581', 'https://openalex.org/W2949718784']",2020-06-29
https://openalex.org/W4294619240,https://doi.org/10.48550/arxiv.1711.10433,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,"The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.",[],2017-11-28
https://openalex.org/W1581879603,,The Science of the Singing Voice,"Althought there are numerous books dealing with the science and acoustics of speech, there are relatively few that deal with the singing voice as distinct from the speaking voice. Now, Johan Sundberg's The Science of the Singing Voice illustrated with over a hundred instructive and significant diagrams and drawings thoroughly describes the structure and functions of the vocal organs in singing, from the aerodynamics of respiration through the dynamics of articulation.",[],1987-01-01
https://openalex.org/W2152002570,https://doi.org/10.1006/jpho.2001.0149,Toward a taxonomy of nonmodal phonation,,"['https://openalex.org/W2076822935', 'https://openalex.org/W2110567885', 'https://openalex.org/W2010070470', 'https://openalex.org/W2075028167', 'https://openalex.org/W1987124964', 'https://openalex.org/W2161377991', 'https://openalex.org/W2040693418', 'https://openalex.org/W2075813705', 'https://openalex.org/W2153903782', 'https://openalex.org/W6672537702', 'https://openalex.org/W2013301322', 'https://openalex.org/W2025743655', 'https://openalex.org/W1987070859', 'https://openalex.org/W205146511', 'https://openalex.org/W2068148689', 'https://openalex.org/W1993836674', 'https://openalex.org/W1963732533', 'https://openalex.org/W1981942926', 'https://openalex.org/W2024677195', 'https://openalex.org/W1993320857', 'https://openalex.org/W1982418847', 'https://openalex.org/W2043224848', 'https://openalex.org/W2062051910', 'https://openalex.org/W2022194925', 'https://openalex.org/W2043007325', 'https://openalex.org/W2057564885', 'https://openalex.org/W2011585205', 'https://openalex.org/W2052673001', 'https://openalex.org/W2010672470', 'https://openalex.org/W2094741866', 'https://openalex.org/W1982849947', 'https://openalex.org/W2010280365', 'https://openalex.org/W2149395423', 'https://openalex.org/W2015501479', 'https://openalex.org/W1983350209', 'https://openalex.org/W2052802814', 'https://openalex.org/W4320573496', 'https://openalex.org/W2009119805', 'https://openalex.org/W2011381687', 'https://openalex.org/W2032939482', 'https://openalex.org/W4252127069', 'https://openalex.org/W1974789524', 'https://openalex.org/W2005893140', 'https://openalex.org/W2041352160', 'https://openalex.org/W568871800', 'https://openalex.org/W2093142574', 'https://openalex.org/W2321061684', 'https://openalex.org/W1483501585', 'https://openalex.org/W2087587326', 'https://openalex.org/W2088998700', 'https://openalex.org/W602985745', 'https://openalex.org/W652303900']",2001-10-01
https://openalex.org/W2100649345,https://doi.org/10.1109/icassp.2009.4960401,Voice Transformation: A survey,"Voice transformation refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice transformation is usually seen as an add-on or an external system in speech synthesis systems since it may create virtual voices in a simple and flexible way. In this paper we review the state-of-the-art Voice transformation methodology showing its limitations in producing good speech quality and its current challenges. Addressing quality issues of current voice transformation algorithms in conjunction with properties of the speech production and speech perception systems we try to pave the way for more natural Voice Transformation algorithms in the future. Facing the challenges, will allow Voice transformation systems to be applied in important and versatile areas of speech technology; applications that are far beyond speech synthesis.","['https://openalex.org/W2130362963', 'https://openalex.org/W2153057929', 'https://openalex.org/W2136166660', 'https://openalex.org/W23142961', 'https://openalex.org/W1991402032', 'https://openalex.org/W1926768285', 'https://openalex.org/W2118850452', 'https://openalex.org/W2114659828', 'https://openalex.org/W1518567289', 'https://openalex.org/W2009631033', 'https://openalex.org/W2161135987', 'https://openalex.org/W153527203', 'https://openalex.org/W6631584049', 'https://openalex.org/W5057334', 'https://openalex.org/W6600209388', 'https://openalex.org/W167578630', 'https://openalex.org/W2011916518', 'https://openalex.org/W2110420312', 'https://openalex.org/W2145130307', 'https://openalex.org/W2119125873', 'https://openalex.org/W7028309', 'https://openalex.org/W2164764235', 'https://openalex.org/W2168510624', 'https://openalex.org/W955355846', 'https://openalex.org/W2244925781', 'https://openalex.org/W87274615', 'https://openalex.org/W1528372724', 'https://openalex.org/W2111748458', 'https://openalex.org/W3113207934', 'https://openalex.org/W2112003680', 'https://openalex.org/W4927070', 'https://openalex.org/W8699629', 'https://openalex.org/W2398348908']",2009-04-01
https://openalex.org/W1984431516,https://doi.org/10.1016/s2173-5735(10)70082-x,The singing voice,,"['https://openalex.org/W2028761259', 'https://openalex.org/W2022688725', 'https://openalex.org/W2083383135', 'https://openalex.org/W2155638869', 'https://openalex.org/W2015743962', 'https://openalex.org/W2059506903', 'https://openalex.org/W2120735132', 'https://openalex.org/W2042828365', 'https://openalex.org/W2051011943', 'https://openalex.org/W2058392145', 'https://openalex.org/W6647697898', 'https://openalex.org/W2073960800', 'https://openalex.org/W2001300139', 'https://openalex.org/W2027550109', 'https://openalex.org/W2090640482', 'https://openalex.org/W1989114159', 'https://openalex.org/W2094668920', 'https://openalex.org/W2009543367', 'https://openalex.org/W2054093739', 'https://openalex.org/W2096605211', 'https://openalex.org/W2145610761', 'https://openalex.org/W1977745226', 'https://openalex.org/W2164964817', 'https://openalex.org/W6806539189', 'https://openalex.org/W1975496616', 'https://openalex.org/W2136936024', 'https://openalex.org/W1965033828', 'https://openalex.org/W2068673553', 'https://openalex.org/W2048414009', 'https://openalex.org/W2003619785', 'https://openalex.org/W2058328018', 'https://openalex.org/W2146377669', 'https://openalex.org/W2033724270', 'https://openalex.org/W2061678377', 'https://openalex.org/W2011310913', 'https://openalex.org/W2068017467', 'https://openalex.org/W1978943389', 'https://openalex.org/W1981149991', 'https://openalex.org/W1996229975', 'https://openalex.org/W2077015559', 'https://openalex.org/W2101904413', 'https://openalex.org/W2124495181', 'https://openalex.org/W2090778167', 'https://openalex.org/W2071995166', 'https://openalex.org/W2130182535', 'https://openalex.org/W4238442506', 'https://openalex.org/W4232857055', 'https://openalex.org/W1990313225', 'https://openalex.org/W2004759398', 'https://openalex.org/W2119870563', 'https://openalex.org/W2121432605', 'https://openalex.org/W2029633529', 'https://openalex.org/W2068780497', 'https://openalex.org/W4253532741', 'https://openalex.org/W2046440803', 'https://openalex.org/W1566860145', 'https://openalex.org/W46907135', 'https://openalex.org/W2155390888', 'https://openalex.org/W2160964011', 'https://openalex.org/W1983797204', 'https://openalex.org/W2023223782', 'https://openalex.org/W2440316187', 'https://openalex.org/W1989553796']",2010-01-01
https://openalex.org/W2000215628,https://doi.org/10.1007/bf02310791,Analysis of Individual Differences in Multidimensional Scaling Via an N-way Generalization of “Eckart-Young” Decomposition,"An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.","['https://openalex.org/W2024971339', 'https://openalex.org/W2056640768', 'https://openalex.org/W1963826206', 'https://openalex.org/W1963830757', 'https://openalex.org/W2004026774', 'https://openalex.org/W2089046632', 'https://openalex.org/W1997153468', 'https://openalex.org/W1992870649', 'https://openalex.org/W1977063224', 'https://openalex.org/W4251355114', 'https://openalex.org/W1498714636', 'https://openalex.org/W2185761757', 'https://openalex.org/W2124264256', 'https://openalex.org/W2001385242', 'https://openalex.org/W2056930330', 'https://openalex.org/W2083795909', 'https://openalex.org/W2082086964', 'https://openalex.org/W4252602300', 'https://openalex.org/W1572268863', 'https://openalex.org/W1979317087', 'https://openalex.org/W570916743']",1970-09-01
https://openalex.org/W2938704169,https://doi.org/10.48550/arxiv.1904.09751,The Curious Case of Neural Text Degeneration,"Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.","['https://openalex.org/W2964268978', 'https://openalex.org/W2963366196', 'https://openalex.org/W2805486818', 'https://openalex.org/W2807747378', 'https://openalex.org/W2402268235', 'https://openalex.org/W2099960657', 'https://openalex.org/W2557436004', 'https://openalex.org/W2995404354', 'https://openalex.org/W2892153332', 'https://openalex.org/W2788277448', 'https://openalex.org/W2963929190', 'https://openalex.org/W1902237438', 'https://openalex.org/W2963206148', 'https://openalex.org/W2995969307', 'https://openalex.org/W2410983263', 'https://openalex.org/W2172140247', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963970792', 'https://openalex.org/W2550147980', 'https://openalex.org/W2951520714', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963456134', 'https://openalex.org/W2803267010', 'https://openalex.org/W2799184518', 'https://openalex.org/W2963706817', 'https://openalex.org/W2962788902', 'https://openalex.org/W2898718449', 'https://openalex.org/W2996068536', 'https://openalex.org/W2048176942', 'https://openalex.org/W2739046565', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963403868', 'https://openalex.org/W2051840895', 'https://openalex.org/W2970692082', 'https://openalex.org/W2042492924', 'https://openalex.org/W2264742718', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963466651', 'https://openalex.org/W2964308564', 'https://openalex.org/W2557283755']",2019-04-22
https://openalex.org/W3029579848,https://doi.org/10.48550/arxiv.1907.02637,Neural Drum Machine : An Interactive System for Real-time Synthesis of\n Drum Sounds,"In this work, we introduce a system for real-time generation of drum sounds.\nThis system is composed of two parts: a generative model for drum sounds\ntogether with a Max4Live plugin providing intuitive controls on the generative\nprocess. The generative model consists of a Conditional Wasserstein autoencoder\n(CWAE), which learns to generate Mel-scaled magnitude spectrograms of short\npercussion samples, coupled with a Multi-Head Convolutional Neural Network\n(MCNN) which estimates the corresponding audio signal from the magnitude\nspectrogram. The design of this model makes it lightweight, so that it allows\none to perform real-time generation of novel drum sounds on an average CPU,\nremoving the need for the users to possess dedicated hardware in order to use\nthis system. We then present our Max4Live interface designed to interact with\nthis generative model. With this setup, the system can be easily integrated\ninto a studio-production environment and enhance the creative process. Finally,\nwe discuss the advantages of our system and how the interaction of music\nproducers with such tools could change the way drum tracks are composed.\n","['https://openalex.org/W2963889406', 'https://openalex.org/W2734957715', 'https://openalex.org/W2888169323', 'https://openalex.org/W2898847420', 'https://openalex.org/W2734498959', 'https://openalex.org/W2641889749', 'https://openalex.org/W1522301498', 'https://openalex.org/W2769811909', 'https://openalex.org/W2910409238', 'https://openalex.org/W2767757834', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963801305', 'https://openalex.org/W1663973292', 'https://openalex.org/W2535388113', 'https://openalex.org/W1565176583', 'https://openalex.org/W2953046278', 'https://openalex.org/W2760103357', 'https://openalex.org/W2099111195', 'https://openalex.org/W2949382160', 'https://openalex.org/W2212660284', 'https://openalex.org/W2804722844', 'https://openalex.org/W2951004968', 'https://openalex.org/W2950299304', 'https://openalex.org/W2810655876', 'https://openalex.org/W2951535099', 'https://openalex.org/W2188365844']",2019-07-04
https://openalex.org/W3007067948,https://doi.org/10.1109/asru46091.2019.9003859,Learning Hierarchical Representations for Expressive Speaking Style in End-to-End Speech Synthesis,"Although Global Style Tokens (GSTs) are a recently-proposed method to uncover expressive factors of variation in speaking style, they are a mixture of style attributes without explicitly considering the factorization of multiple-level speaking styles. In this work, we introduce a hierarchical GST architecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech. We make hierarchical evaluations conditioned on individual tokens from different GST layers. As the number of layers increases, we tend to observe a coarse to fine style decomposition. For example, the first GST layer learns a good representation of speaker IDs while finer speaking style or emotion variations can be found in higher-level layers. Meanwhile, the proposed model shows good performance of style transfer.","['https://openalex.org/W2102003408', 'https://openalex.org/W2603120115', 'https://openalex.org/W2793479148', 'https://openalex.org/W2516321201', 'https://openalex.org/W6745232783', 'https://openalex.org/W2885800352', 'https://openalex.org/W2904459034', 'https://openalex.org/W2962691331', 'https://openalex.org/W2120847449', 'https://openalex.org/W6753855596', 'https://openalex.org/W2769810959', 'https://openalex.org/W6750489868', 'https://openalex.org/W2795109282', 'https://openalex.org/W2117475030', 'https://openalex.org/W7043748487', 'https://openalex.org/W2777302760', 'https://openalex.org/W2129142580', 'https://openalex.org/W6736996214', 'https://openalex.org/W6755300632', 'https://openalex.org/W2475998840', 'https://openalex.org/W6643482484', 'https://openalex.org/W2107860279', 'https://openalex.org/W2651834199', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963272440', 'https://openalex.org/W1972420736', 'https://openalex.org/W2963927338', 'https://openalex.org/W2963568578', 'https://openalex.org/W2519091744', 'https://openalex.org/W4294619240', 'https://openalex.org/W2808706139', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963432880', 'https://openalex.org/W2766406951', 'https://openalex.org/W2949382160', 'https://openalex.org/W2794490148', 'https://openalex.org/W4289383906', 'https://openalex.org/W4295731579', 'https://openalex.org/W2527729766', 'https://openalex.org/W4327693937', 'https://openalex.org/W2187089797']",2019-12-01
https://openalex.org/W2966387353,https://doi.org/10.1109/lsp.2019.2931673,An Effective Style Token Weight Control Technique for End-to-End Emotional Speech Synthesis,"In this letter, we propose a high-quality emotional speech synthesis system, using emotional vector space, i.e., the weighted sum of global style tokens (GSTs). Our previous research verified the feasibility of GST-based emotional speech synthesis in an end-to-end text-to-speech synthesis framework. However, selecting appropriate reference audio (RA) signals to extract emotion embedding vectors to the specific types of target emotions remains problematic. To ameliorate the selection problem, we propose an effective way of generating emotion embedding vectors by utilizing the trained GSTs. By assuming that the trained GSTs represent an emotional vector space, we first investigate the distribution of all the training samples depending on the type of each emotion. We then regard the centroid of the distribution as an emotion-specific weighting value, which effectively controls the expressiveness of synthesized speech, even without using the RA for guidance, as it did before. Finally, we confirm that the proposed controlled weight-based method is superior to the conventional emotion label-based methods in terms of perceptual quality and emotion classification accuracy.","['https://openalex.org/W2591927543', 'https://openalex.org/W6765987481', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6745232783', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W6760861152', 'https://openalex.org/W2904459034', 'https://openalex.org/W6746238782', 'https://openalex.org/W2962699518', 'https://openalex.org/W2129142580', 'https://openalex.org/W6681095595', 'https://openalex.org/W1576227399', 'https://openalex.org/W6755300632', 'https://openalex.org/W6675380101', 'https://openalex.org/W2794838827', 'https://openalex.org/W6635953567', 'https://openalex.org/W6756197946', 'https://openalex.org/W2150658333', 'https://openalex.org/W6623517193', 'https://openalex.org/W6679434410', 'https://openalex.org/W6679436768', 'https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W2968201928', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964308564', 'https://openalex.org/W2932022923', 'https://openalex.org/W2963272440', 'https://openalex.org/W4289383906', 'https://openalex.org/W3026435290', 'https://openalex.org/W2141368411', 'https://openalex.org/W1599623585', 'https://openalex.org/W2949382160', 'https://openalex.org/W2133564696', 'https://openalex.org/W2619368999', 'https://openalex.org/W2102003408', 'https://openalex.org/W4385245566', 'https://openalex.org/W2794490148', 'https://openalex.org/W2901997113', 'https://openalex.org/W2963568578', 'https://openalex.org/W854541894', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963712897', 'https://openalex.org/W2964281804', 'https://openalex.org/W2770743791', 'https://openalex.org/W2766406951', 'https://openalex.org/W2187089797', 'https://openalex.org/W2130942839', 'https://openalex.org/W2519091744', 'https://openalex.org/W4295731579']",2019-07-29
https://openalex.org/W2962691331,https://doi.org/10.21437/interspeech.2018-1113,Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder,"Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS).However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue.In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE).This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner.Experiments using the VCTK and Bliz-zard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.","['https://openalex.org/W2963799213', 'https://openalex.org/W2515020857', 'https://openalex.org/W2519091744', 'https://openalex.org/W1522301498', 'https://openalex.org/W2532494225', 'https://openalex.org/W2471520273', 'https://openalex.org/W2160473997', 'https://openalex.org/W2953046278', 'https://openalex.org/W4322588875', 'https://openalex.org/W1978136968', 'https://openalex.org/W2770743791', 'https://openalex.org/W2962896155', 'https://openalex.org/W2745595539', 'https://openalex.org/W4298857617', 'https://openalex.org/W2963609956', 'https://openalex.org/W1517939602', 'https://openalex.org/W2059721297', 'https://openalex.org/W2766406951', 'https://openalex.org/W2962850167', 'https://openalex.org/W2407139314', 'https://openalex.org/W1492383498', 'https://openalex.org/W2759925408', 'https://openalex.org/W4293411878', 'https://openalex.org/W2156146072', 'https://openalex.org/W2745387029', 'https://openalex.org/W2901997113', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963223306']",2018-08-28
https://openalex.org/W2591927543,https://doi.org/10.48550/arxiv.1702.07825,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.","['https://openalex.org/W2193413348', 'https://openalex.org/W2471520273', 'https://openalex.org/W2901997113', 'https://openalex.org/W2949640717', 'https://openalex.org/W2952436057', 'https://openalex.org/W2559246505', 'https://openalex.org/W2160473997', 'https://openalex.org/W2474388053', 'https://openalex.org/W1522301498', 'https://openalex.org/W2099057450', 'https://openalex.org/W2953331651', 'https://openalex.org/W1924770834', 'https://openalex.org/W2949382160', 'https://openalex.org/W1576227399', 'https://openalex.org/W2953318193', 'https://openalex.org/W2277265840', 'https://openalex.org/W2513051195', 'https://openalex.org/W1570629387', 'https://openalex.org/W1916501714', 'https://openalex.org/W650122064']",2017-02-25
https://openalex.org/W2091425152,https://doi.org/10.1121/1.1458024,"YIN, a fundamental frequency estimator for speech and music","An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.","['https://openalex.org/W2060505532', 'https://openalex.org/W2107831318', 'https://openalex.org/W2059744545', 'https://openalex.org/W2024671804', 'https://openalex.org/W1970893633', 'https://openalex.org/W1728219059', 'https://openalex.org/W2130650491', 'https://openalex.org/W2005608041', 'https://openalex.org/W2055823333', 'https://openalex.org/W2159406493', 'https://openalex.org/W1974196562', 'https://openalex.org/W2033035341', 'https://openalex.org/W2059260101', 'https://openalex.org/W2100989275', 'https://openalex.org/W2247445314', 'https://openalex.org/W2049686551', 'https://openalex.org/W1966948181', 'https://openalex.org/W2107451257', 'https://openalex.org/W1971146038', 'https://openalex.org/W2051512859', 'https://openalex.org/W2060810557', 'https://openalex.org/W2084044763', 'https://openalex.org/W2034148627', 'https://openalex.org/W4241659927', 'https://openalex.org/W1976986618', 'https://openalex.org/W2171321863', 'https://openalex.org/W2028078738', 'https://openalex.org/W2002525568', 'https://openalex.org/W2055752619', 'https://openalex.org/W2117343934', 'https://openalex.org/W2627032709', 'https://openalex.org/W1492717434', 'https://openalex.org/W3005283740', 'https://openalex.org/W2912022664', 'https://openalex.org/W1916910924', 'https://openalex.org/W2141684970', 'https://openalex.org/W2156340797', 'https://openalex.org/W104734300', 'https://openalex.org/W1863426813']",2002-04-01
https://openalex.org/W2025722797,https://doi.org/10.1121/1.1917190,Standardizing Auditory Tests,"An understanding of the over-all process of hearing depends upon proper interpretation of the results of many individual experiments. In the field of subjective experimentation the problem has been complicated by the wide variety of test procedures that characterize available data. If a common technique could be applied to the many different types of auditory tests, such as thresholds of acuity, masking tests, difference limens, etc., the organization of these data would be facilitated. The purpose of the present paper is to describe a test procedure which has shown promise in this direction and to give descriptions of equipment which have been found helpful in minimizing the variability of the test results. The procedure, which we have called the “ABX” test, is a modification of the method of paired comparisons. An observer is presented with a time sequence of three signals for each judgment he is asked to make. During the first time interval he hears signal A, during the second, signal B, and finally signal X. His task is to indicate whether the sound heard during the X interval was more like that during the A interval or more like that during the B interval. For a threshold test, the A interval is quiet, the B interval is signal, and the X interval is either quiet or signal. For a masking test, A is the masking signal, B is the masking signal plus the signal being masked, and X is either A or B repeated. The apparatus for the ABX test is mechanized so all details of the method can be duplicated for each observer, and the variability of manual operation eliminated. The entire test is coded on teletype tape to reduce the time and effort of collecting large quantities of data.",[],1950-09-01
https://openalex.org/W2130086727,https://doi.org/10.1016/j.specom.2007.09.003,A method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments,,"['https://openalex.org/W6718334172', 'https://openalex.org/W6683576537', 'https://openalex.org/W2012060056', 'https://openalex.org/W104734300', 'https://openalex.org/W2107831318', 'https://openalex.org/W6769316794', 'https://openalex.org/W1926680708', 'https://openalex.org/W6636700880', 'https://openalex.org/W2091425152', 'https://openalex.org/W4235716345', 'https://openalex.org/W289873009', 'https://openalex.org/W1978244610', 'https://openalex.org/W2049686551', 'https://openalex.org/W2092655206', 'https://openalex.org/W6685452626', 'https://openalex.org/W1791774586', 'https://openalex.org/W2063619969', 'https://openalex.org/W2084044763', 'https://openalex.org/W2088632109', 'https://openalex.org/W2004163116', 'https://openalex.org/W2100384625', 'https://openalex.org/W2129120544', 'https://openalex.org/W2143039550', 'https://openalex.org/W2163067280', 'https://openalex.org/W3216401400', 'https://openalex.org/W2556247670', 'https://openalex.org/W2441833056', 'https://openalex.org/W2171840027', 'https://openalex.org/W86348706', 'https://openalex.org/W2294819182', 'https://openalex.org/W2160566140']",2007-09-26
https://openalex.org/W2107860279,https://doi.org/10.1109/pacrim.1993.407206,Mel-cepstral distance measure for objective speech quality assessment,"The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2013139519', 'https://openalex.org/W2146283888', 'https://openalex.org/W2148154194', 'https://openalex.org/W2163181067', 'https://openalex.org/W2479702386', 'https://openalex.org/W6726809291', 'https://openalex.org/W2078911760', 'https://openalex.org/W4285719527', 'https://openalex.org/W2519954739']",2002-12-30
https://openalex.org/W3033411150,https://doi.org/10.57702/v7a23pbp,FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech,"Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.","['https://openalex.org/W2102003408', 'https://openalex.org/W1498897359', 'https://openalex.org/W3128910262', 'https://openalex.org/W3017654531', 'https://openalex.org/W2398973091', 'https://openalex.org/W2963636093', 'https://openalex.org/W3033913438', 'https://openalex.org/W2926840633', 'https://openalex.org/W2949382160', 'https://openalex.org/W2970006822', 'https://openalex.org/W2127589467', 'https://openalex.org/W3016136182', 'https://openalex.org/W2963403868', 'https://openalex.org/W2591927543', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963712897', 'https://openalex.org/W2903739847', 'https://openalex.org/W2995233853', 'https://openalex.org/W3160919572', 'https://openalex.org/W1964668808', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963782041', 'https://openalex.org/W2945613576', 'https://openalex.org/W652196294', 'https://openalex.org/W3025793647', 'https://openalex.org/W3034949308', 'https://openalex.org/W2963300588', 'https://openalex.org/W3026874504', 'https://openalex.org/W278779762', 'https://openalex.org/W2294797155', 'https://openalex.org/W2964243274', 'https://openalex.org/W2629461003', 'https://openalex.org/W2962882868', 'https://openalex.org/W3163906773', 'https://openalex.org/W2096684483', 'https://openalex.org/W2964121744', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963330667', 'https://openalex.org/W2970730223']",2024-01-01
https://openalex.org/W854541894,https://doi.org/10.48550/arxiv.1506.07503,Attention-Based Models for Speech Recognition,"Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","['https://openalex.org/W2127141656', 'https://openalex.org/W1872489089', 'https://openalex.org/W2157331557', 'https://openalex.org/W2209647458', 'https://openalex.org/W1987841215', 'https://openalex.org/W2112796928', 'https://openalex.org/W6908809', 'https://openalex.org/W1586532344', 'https://openalex.org/W2113021982', 'https://openalex.org/W2130942839', 'https://openalex.org/W2064675550', 'https://openalex.org/W1915251500', 'https://openalex.org/W1524333225', 'https://openalex.org/W1810943226', 'https://openalex.org/W1514535095', 'https://openalex.org/W2619993508', 'https://openalex.org/W2147527908', 'https://openalex.org/W2142416747', 'https://openalex.org/W2950178297', 'https://openalex.org/W2160815625', 'https://openalex.org/W1922655562', 'https://openalex.org/W2108677974', 'https://openalex.org/W2964308564', 'https://openalex.org/W2143612262', 'https://openalex.org/W2152175008', 'https://openalex.org/W1828163288', 'https://openalex.org/W3037881859', 'https://openalex.org/W1606347560', 'https://openalex.org/W2167839676', 'https://openalex.org/W2102113734', 'https://openalex.org/W1904365287']",2015-06-24
https://openalex.org/W2107740512,https://doi.org/10.1109/icassp.2009.4960497,Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend,"In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.","['https://openalex.org/W1974387177', 'https://openalex.org/W2151484683', 'https://openalex.org/W2146871184', 'https://openalex.org/W23669922', 'https://openalex.org/W2130086727', 'https://openalex.org/W2088632109', 'https://openalex.org/W4402490932', 'https://openalex.org/W2108819501', 'https://openalex.org/W147159200', 'https://openalex.org/W2091425152', 'https://openalex.org/W6639350448', 'https://openalex.org/W2171748469', 'https://openalex.org/W2129120544', 'https://openalex.org/W1875231349', 'https://openalex.org/W2141684970', 'https://openalex.org/W2084562624']",2009-04-01
https://openalex.org/W4309129001,https://doi.org/10.48550/arxiv.2211.06474,Speech-to-Speech Translation For A Real-world Unwritten Language,"We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additional text supervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field. The demo can be found at https://huggingface.co/spaces/facebook/Hokkien_Translation .",[],2022-11-11
https://openalex.org/W4221164184,https://doi.org/10.48550/arxiv.2202.07359,textless-lib: a Library for Textless Spoken Language Processing,"Textless spoken language processing research aims to extend the applicability of standard NLP toolset onto spoken language and languages with few or no textual resources. In this paper, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in this research area. We describe the building blocks that the library provides and demonstrate its usability by discuss three different use-case examples: (i) speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation. We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large. The code, documentation, and pre-trained models are available at https://github.com/facebookresearch/textlesslib/ .",[],2022-02-15
https://openalex.org/W4378505287,https://doi.org/10.48550/arxiv.2305.16307,IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages,"India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/AI4Bharat/IndicTrans2.",[],2023-05-25
https://openalex.org/W2924093092,https://doi.org/10.1515/jisys-2019-2510,Neural Machine Translation System for English to Indian Language Translation Using MTIL Parallel Corpus,"Abstract Introduction of deep neural networks to the machine translation research ameliorated conventional machine translation systems in multiple ways, specifically in terms of translation quality. The ability of deep neural networks to learn a sensible representation of words is one of the major reasons for this improvement. Despite machine translation using deep neural architecture is showing state-of-the-art results in translating European languages, we cannot directly apply these algorithms in Indian languages mainly because of two reasons: unavailability of the good corpus and Indian languages are morphologically rich. In this paper, we propose a neural machine translation (NMT) system for four language pairs: English–Malayalam, English–Hindi, English–Tamil, and English–Punjabi. We also collected sentences from different sources and cleaned them to make four parallel corpora for each of the language pairs, and then used them to model the translation system. The encoder network in the NMT architecture was designed with long short-term memory (LSTM) networks and bi-directional recurrent neural networks (Bi-RNN). Evaluation of the obtained models was performed both automatically and manually. For automatic evaluation, the bilingual evaluation understudy (BLEU) score was used, and for manual evaluation, three metrics such as adequacy, fluency, and overall ranking were used. Analysis of the results showed the presence of lengthy sentences in English–Malayalam, and the English–Hindi corpus affected the translation. Attention mechanism was employed with a view to addressing the problem of translating lengthy sentences (sentences contain more than 50 words), and the system was able to perceive long-term contexts in the sentences.","['https://openalex.org/W2595715041', 'https://openalex.org/W2048228344', 'https://openalex.org/W2303560477', 'https://openalex.org/W2186356919', 'https://openalex.org/W606759049', 'https://openalex.org/W2561899685', 'https://openalex.org/W585740886', 'https://openalex.org/W2130942839', 'https://openalex.org/W1595847159', 'https://openalex.org/W2186120430', 'https://openalex.org/W2097801708', 'https://openalex.org/W2467281805', 'https://openalex.org/W2131774270', 'https://openalex.org/W2152367404', 'https://openalex.org/W2340675944', 'https://openalex.org/W2574872930', 'https://openalex.org/W2550821151', 'https://openalex.org/W19526395', 'https://openalex.org/W3212092983', 'https://openalex.org/W2020015729', 'https://openalex.org/W2156984567', 'https://openalex.org/W2100480499', 'https://openalex.org/W1902237438', 'https://openalex.org/W2668208913', 'https://openalex.org/W2218202018', 'https://openalex.org/W1753482797', 'https://openalex.org/W1602767452', 'https://openalex.org/W2172140247', 'https://openalex.org/W1945115148', 'https://openalex.org/W1924770834', 'https://openalex.org/W2133564696', 'https://openalex.org/W2064675550', 'https://openalex.org/W2525778437', 'https://openalex.org/W2566490639', 'https://openalex.org/W2605140907', 'https://openalex.org/W2912889685', 'https://openalex.org/W1916559533', 'https://openalex.org/W2131060370', 'https://openalex.org/W2963212250', 'https://openalex.org/W2964308564']",2019-03-20
https://openalex.org/W4378473793,https://doi.org/10.48550/arxiv.2305.14716,GlobalBench: A Benchmark for Global Progress in Natural Language Processing,"Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",[],2023-05-24
https://openalex.org/W4283121045,https://doi.org/10.3390/app12126223,Developing a Speech Recognition System for Recognizing Tonal Speech Signals Using a Convolutional Neural Network,"Deep learning-based machine learning models have shown significant results in speech recognition and numerous vision-related tasks. The performance of the present speech-to-text model relies upon the hyperparameters used in this research work. In this research work, it is shown that convolutional neural networks (CNNs) can model raw and tonal speech signals. Their performance is on par with existing recognition systems. This study extends the role of the CNN-based approach to robust and uncommon speech signals (tonal) using its own designed database for target research. The main objective of this research work was to develop a speech-to-text recognition system to recognize the tonal speech signals of Gurbani hymns using a CNN. Further, the CNN model, with six layers of 2DConv, 2DMax Pooling, and 256 dense layer units (Google’s TensorFlow service) was also used in this work, as well as Praat for speech segmentation. Feature extraction was enforced using the MFCC feature extraction technique, which extracts standard speech features and features of background music as well. Our study reveals that the CNN-based method for identifying tonal speech sentences and adding instrumental knowledge performs better than the existing and conventional approaches. The experimental results demonstrate the significant performance of the present CNN architecture by providing an 89.15% accuracy rate and a 10.56% WER for continuous and extensive vocabulary sentences of speech signals with different tones.","['https://openalex.org/W2951871139', 'https://openalex.org/W2981958259', 'https://openalex.org/W6687567705', 'https://openalex.org/W6754832338', 'https://openalex.org/W6697756902', 'https://openalex.org/W6729542162', 'https://openalex.org/W2037550275', 'https://openalex.org/W2161742217', 'https://openalex.org/W6703360075', 'https://openalex.org/W1995562189', 'https://openalex.org/W2566781703', 'https://openalex.org/W6703075364', 'https://openalex.org/W2912581782', 'https://openalex.org/W1972278020', 'https://openalex.org/W4221144554', 'https://openalex.org/W6795952400', 'https://openalex.org/W1555696814', 'https://openalex.org/W2962146825', 'https://openalex.org/W2997239896', 'https://openalex.org/W6729568197', 'https://openalex.org/W3187713544', 'https://openalex.org/W2787992438', 'https://openalex.org/W2103088716', 'https://openalex.org/W2515753980', 'https://openalex.org/W6677055868', 'https://openalex.org/W2091088554', 'https://openalex.org/W1520422233', 'https://openalex.org/W2024733869', 'https://openalex.org/W6697830260', 'https://openalex.org/W2885590625', 'https://openalex.org/W2551268863', 'https://openalex.org/W2112635125', 'https://openalex.org/W2335896301', 'https://openalex.org/W2962949934', 'https://openalex.org/W2338261539', 'https://openalex.org/W2894647744', 'https://openalex.org/W2304768685', 'https://openalex.org/W2308498479', 'https://openalex.org/W2551258216']",2022-06-19
https://openalex.org/W2936184970,https://doi.org/10.48550/arxiv.1904.06037,Direct speech-to-speech translation with a sequence-to-sequence model,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","['https://openalex.org/W2964243274', 'https://openalex.org/W2896538040', 'https://openalex.org/W2963782041', 'https://openalex.org/W2963192573', 'https://openalex.org/W2962911098', 'https://openalex.org/W2113106066', 'https://openalex.org/W2101105183', 'https://openalex.org/W2747920239', 'https://openalex.org/W2152834109', 'https://openalex.org/W3012492057', 'https://openalex.org/W2962739369', 'https://openalex.org/W2928664166', 'https://openalex.org/W2963403868', 'https://openalex.org/W1537859740', 'https://openalex.org/W2097203679', 'https://openalex.org/W2912492482', 'https://openalex.org/W2139647714', 'https://openalex.org/W2963568578', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972970915', 'https://openalex.org/W2949328740', 'https://openalex.org/W1962947832', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964104866', 'https://openalex.org/W2963011080', 'https://openalex.org/W2962970071', 'https://openalex.org/W2928941594', 'https://openalex.org/W2605131327', 'https://openalex.org/W2136545725', 'https://openalex.org/W2963779652', 'https://openalex.org/W2795581297', 'https://openalex.org/W2964308564', 'https://openalex.org/W2941115821', 'https://openalex.org/W2525778437', 'https://openalex.org/W2962824709', 'https://openalex.org/W2133300417', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963272440', 'https://openalex.org/W2011783148', 'https://openalex.org/W2120847449', 'https://openalex.org/W2582956876', 'https://openalex.org/W1538023239', 'https://openalex.org/W2963382687', 'https://openalex.org/W2777302760', 'https://openalex.org/W2963912924', 'https://openalex.org/W2964307104']",2019-04-12
https://openalex.org/W4387162606,https://doi.org/10.48550/arxiv.2309.15800,"Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study","Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.",[],2023-09-27
https://openalex.org/W3036601975,https://doi.org/10.48550/arxiv.2006.11477,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.","['https://openalex.org/W273093436', 'https://openalex.org/W3005680577', 'https://openalex.org/W3107298252', 'https://openalex.org/W3016181583', 'https://openalex.org/W2908336025', 'https://openalex.org/W2936295285', 'https://openalex.org/W2973049979', 'https://openalex.org/W2953190524', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962901777', 'https://openalex.org/W2971155163', 'https://openalex.org/W2936774411', 'https://openalex.org/W3025165719', 'https://openalex.org/W2127141656', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996159613', 'https://openalex.org/W3004728855', 'https://openalex.org/W2124509324', 'https://openalex.org/W2995680346', 'https://openalex.org/W10548402', 'https://openalex.org/W3026041220', 'https://openalex.org/W2896457183', 'https://openalex.org/W3103005696', 'https://openalex.org/W2121879602', 'https://openalex.org/W2794209590', 'https://openalex.org/W2991213871', 'https://openalex.org/W2944828972', 'https://openalex.org/W2949892913', 'https://openalex.org/W2994536315', 'https://openalex.org/W3021469861', 'https://openalex.org/W2152790380', 'https://openalex.org/W2964121744', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962942158', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963799213', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963807318', 'https://openalex.org/W3027083471', 'https://openalex.org/W2899663614', 'https://openalex.org/W2296701362', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W3002741552', 'https://openalex.org/W2981991061', 'https://openalex.org/W2962739339', 'https://openalex.org/W2952509486', 'https://openalex.org/W2988736778', 'https://openalex.org/W3035524453', 'https://openalex.org/W3037932933', 'https://openalex.org/W2963631907', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015419784', 'https://openalex.org/W2972943112', 'https://openalex.org/W2972374322']",2020-06-20
https://openalex.org/W3169320628,https://doi.org/10.1109/taslp.2021.3122291,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.","['https://openalex.org/W2937090315', 'https://openalex.org/W6790168579', 'https://openalex.org/W6745117592', 'https://openalex.org/W2962850167', 'https://openalex.org/W2155273149', 'https://openalex.org/W2032210463', 'https://openalex.org/W3011411500', 'https://openalex.org/W3033038061', 'https://openalex.org/W6947929050', 'https://openalex.org/W3026041220', 'https://openalex.org/W2933138175', 'https://openalex.org/W3097777922', 'https://openalex.org/W2347098582', 'https://openalex.org/W6784637704', 'https://openalex.org/W6675022971', 'https://openalex.org/W3008525923', 'https://openalex.org/W2750248772', 'https://openalex.org/W6844194202', 'https://openalex.org/W3160799772', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962739339', 'https://openalex.org/W6771917389', 'https://openalex.org/W2883725317', 'https://openalex.org/W3035524453', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6777232839', 'https://openalex.org/W6786669483', 'https://openalex.org/W6631190155', 'https://openalex.org/W2150593711', 'https://openalex.org/W6675354045', 'https://openalex.org/W6784436999', 'https://openalex.org/W2973157397', 'https://openalex.org/W6783797576', 'https://openalex.org/W6770514103', 'https://openalex.org/W3015522062', 'https://openalex.org/W6772883055', 'https://openalex.org/W4254197176', 'https://openalex.org/W3161101519', 'https://openalex.org/W3096338464', 'https://openalex.org/W6779997284', 'https://openalex.org/W6786614245', 'https://openalex.org/W6779326418', 'https://openalex.org/W6778883912', 'https://openalex.org/W6766673545', 'https://openalex.org/W6769311223', 'https://openalex.org/W6780483730', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W2982223350', 'https://openalex.org/W2110073835', 'https://openalex.org/W3035202887', 'https://openalex.org/W3016011332', 'https://openalex.org/W3003875258', 'https://openalex.org/W3015265920', 'https://openalex.org/W2127141656', 'https://openalex.org/W6784614252', 'https://openalex.org/W2888911345', 'https://openalex.org/W2752796333', 'https://openalex.org/W3093788532', 'https://openalex.org/W2164579587', 'https://openalex.org/W2996383576', 'https://openalex.org/W3025035610', 'https://openalex.org/W2965373594', 'https://openalex.org/W2101234009', 'https://openalex.org/W3135676170', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963618559', 'https://openalex.org/W3036601975', 'https://openalex.org/W3036224891', 'https://openalex.org/W3025165719', 'https://openalex.org/W3126816608', 'https://openalex.org/W2988736778', 'https://openalex.org/W3027083471', 'https://openalex.org/W3099782249', 'https://openalex.org/W2998532468', 'https://openalex.org/W3144810982', 'https://openalex.org/W3093579165', 'https://openalex.org/W2953190524', 'https://openalex.org/W2926827382', 'https://openalex.org/W2073459066', 'https://openalex.org/W1522301498', 'https://openalex.org/W2982399380', 'https://openalex.org/W3030163527', 'https://openalex.org/W3125709657', 'https://openalex.org/W3035060554', 'https://openalex.org/W3013571468', 'https://openalex.org/W3107668149', 'https://openalex.org/W3093533780', 'https://openalex.org/W3112034174', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963799213', 'https://openalex.org/W2100768664', 'https://openalex.org/W1553004968']",2021-01-01
https://openalex.org/W4387687030,https://doi.org/10.48550/arxiv.2310.08715,Toward Joint Language Modeling for Speech Units and Text,"Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",[],2023-10-12
https://openalex.org/W3025165719,https://doi.org/10.48550/arxiv.2005.08100,Conformer: Convolution-augmented Transformer for Speech Recognition,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.","['https://openalex.org/W1964175594', 'https://openalex.org/W3021469861', 'https://openalex.org/W2767286248', 'https://openalex.org/W1828163288', 'https://openalex.org/W2952180055', 'https://openalex.org/W3016010032', 'https://openalex.org/W3015194534', 'https://openalex.org/W2937843571', 'https://openalex.org/W2979636403', 'https://openalex.org/W2932319281', 'https://openalex.org/W2892009249', 'https://openalex.org/W2908336025', 'https://openalex.org/W2963970792', 'https://openalex.org/W2964110616', 'https://openalex.org/W2095705004', 'https://openalex.org/W2928941594', 'https://openalex.org/W2936774411', 'https://openalex.org/W2994771587', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963420686', 'https://openalex.org/W2626778328', 'https://openalex.org/W2981413347', 'https://openalex.org/W2962760690', 'https://openalex.org/W1522301498', 'https://openalex.org/W2112739286', 'https://openalex.org/W1995562189', 'https://openalex.org/W3019527251', 'https://openalex.org/W2798858969', 'https://openalex.org/W2981857663', 'https://openalex.org/W2972818416', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963414781', 'https://openalex.org/W2981581604', 'https://openalex.org/W2948981900']",2020-05-16
https://openalex.org/W4226543485,https://doi.org/10.48550/arxiv.2204.02967,Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation,"Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .",[],2022-04-06
https://openalex.org/W4384648564,https://doi.org/10.48550/arxiv.2307.08655,Multilingual Speech-to-Speech Translation into Multiple Target Languages,"Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.",[],2023-07-17
https://openalex.org/W4311731008,https://doi.org/10.48550/arxiv.2212.08055,UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units,"Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.",[],2022-12-15
https://openalex.org/W3142316150,https://doi.org/10.1109/slt48900.2021.9383496,Transformer-Based Direct Speech-To-Speech Translation with Transcoder,"Traditional speech translation systems use a cascade manner that concatenates speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis to translate speech from one language to another language in a step-by-step manner. Unfortunately, since those components are trained separately, MT often struggles to handle ASR errors, resulting in unnatural translation results. Recently, one work attempted to construct direct speech translation in a single model. The model used a multi-task scheme that learns to predict not only the target speech spectrograms directly but also the source and target phoneme transcription as auxiliary tasks. However, that work was only evaluated Spanish-English language pairs with similar syntax and word order. With syntactically distant language pairs, speech translation requires distant word order, and thus direct speech frame-to-frame alignments become difficult. Another direction was to construct a single deep-learning framework while keeping the step-by-step translation process. However, such studies focused only on speech-to-text translation. Furthermore, all of these works were based on a recurrent neural net-work (RNN) model. In this work, we propose a step-by-step scheme to a complete end-to-end speech-to-speech translation and propose a Transformer-based speech translation using Transcoder. We compare our proposed and multi-task model using syntactically similar and distant language pairs.","['https://openalex.org/W6679434410', 'https://openalex.org/W4241645538', 'https://openalex.org/W6736996214', 'https://openalex.org/W3017535695', 'https://openalex.org/W2605131327', 'https://openalex.org/W2605202026', 'https://openalex.org/W6608432165', 'https://openalex.org/W2161742089', 'https://openalex.org/W6898505805', 'https://openalex.org/W6677328238', 'https://openalex.org/W2972495969', 'https://openalex.org/W4300558631', 'https://openalex.org/W2936969148', 'https://openalex.org/W2962680099', 'https://openalex.org/W3037217258', 'https://openalex.org/W6739901393', 'https://openalex.org/W2582956876', 'https://openalex.org/W2136545725', 'https://openalex.org/W6623517193', 'https://openalex.org/W854541894', 'https://openalex.org/W206967138', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963403868', 'https://openalex.org/W2949328740', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964308564', 'https://openalex.org/W2116492146', 'https://openalex.org/W2963609956', 'https://openalex.org/W2153653739', 'https://openalex.org/W4385245566']",2021-01-19
https://openalex.org/W4381827575,https://doi.org/10.48550/arxiv.2306.12925,AudioPaLM: A Large Language Model That Can Speak and Listen,"We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",[],2023-06-22
https://openalex.org/W4386566860,https://doi.org/10.18653/v1/2023.eacl-demo.19,Towards Speech to Speech Machine Translation focusing on Indian Languages,"We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48.","['https://openalex.org/W4300963525', 'https://openalex.org/W4293873724', 'https://openalex.org/W2963532001', 'https://openalex.org/W3196103482', 'https://openalex.org/W2972495969', 'https://openalex.org/W187290754', 'https://openalex.org/W3212380133', 'https://openalex.org/W3214468144', 'https://openalex.org/W2128726682', 'https://openalex.org/W3201740621', 'https://openalex.org/W1525595230', 'https://openalex.org/W3202300147', 'https://openalex.org/W3034625919', 'https://openalex.org/W3047941943', 'https://openalex.org/W4210533994', 'https://openalex.org/W4281672148', 'https://openalex.org/W2404260248', 'https://openalex.org/W4287072252']",2023-01-01
https://openalex.org/W3215465553,https://doi.org/10.1109/ghtc53159.2021.9612485,ClassRoute: An English to Punjabi Educational Video Translation Pipeline for Supporting Punjabi Mother-Tongue Education,"Information Communication Technology (ICT) permeates almost every aspect of our daily lives and has become one of the most important priorities for formal and informal education. However, many people particularly those in least developed countries, are unable to reap the benefits due to lack of access to ICT but also due to lack of access to quality educational material. Additionally, in Punjab India, due to a shortage of resources and lack of infrastructure, the education system suffers from massive gaps including high student to teacher ratios, shortage of qualified teachers, and poor teacher training programs. This all has also been further exacerbated due to the COVID19 Pandemic as schools shut down globally and all teaching/learning activities moved online where possible or were canceled otherwise. In an effort to help relieve some of the burden on the Punjabi education system, and motivated by the proven efficiency of mother-tongue based education as well as the importance of visual-based learning, this paper introduces a pipeline for translating English educational videos into Punjabi equivalents which seeks to go beyond simple translation and in future iterations take into consideration the cultural needs of the learners in order to better connect them with the topics being taught. This pipeline is among a series of under construction pipelines aimed at translating English educational videos into other languages, dubbed as ClassRoute.","['https://openalex.org/W6781867862', 'https://openalex.org/W3036966439', 'https://openalex.org/W2055439081', 'https://openalex.org/W2114884426', 'https://openalex.org/W1972395797', 'https://openalex.org/W2003523075', 'https://openalex.org/W2084742732', 'https://openalex.org/W6963675136', 'https://openalex.org/W2765598583', 'https://openalex.org/W3158860828', 'https://openalex.org/W1964389215', 'https://openalex.org/W2016503558', 'https://openalex.org/W3126507491', 'https://openalex.org/W6786294857', 'https://openalex.org/W2991485441', 'https://openalex.org/W2598619133', 'https://openalex.org/W2994987178', 'https://openalex.org/W3089662918', 'https://openalex.org/W2803924230', 'https://openalex.org/W6746468368', 'https://openalex.org/W2798944620', 'https://openalex.org/W3158815815', 'https://openalex.org/W3046516384', 'https://openalex.org/W2102443632', 'https://openalex.org/W2792743482', 'https://openalex.org/W1844826643', 'https://openalex.org/W212028900', 'https://openalex.org/W2107753673', 'https://openalex.org/W3107173247', 'https://openalex.org/W3207728892', 'https://openalex.org/W2771735366']",2021-10-19
https://openalex.org/W2980109192,https://doi.org/10.1007/s10462-019-09775-8,ASRoIL: a comprehensive survey for automatic speech recognition of Indian languages,,"['https://openalex.org/W1990442385', 'https://openalex.org/W2055480715', 'https://openalex.org/W2128454066', 'https://openalex.org/W2109417079', 'https://openalex.org/W2008398864', 'https://openalex.org/W2137144002', 'https://openalex.org/W2901451702', 'https://openalex.org/W2032392962', 'https://openalex.org/W2078578680', 'https://openalex.org/W2207614471', 'https://openalex.org/W2091746061', 'https://openalex.org/W2190101731', 'https://openalex.org/W2789544814', 'https://openalex.org/W2145746713', 'https://openalex.org/W2782782657', 'https://openalex.org/W2059588204', 'https://openalex.org/W1963869606', 'https://openalex.org/W2027060129', 'https://openalex.org/W2095473347', 'https://openalex.org/W2889367800', 'https://openalex.org/W2046436020', 'https://openalex.org/W2093487040', 'https://openalex.org/W2766110517', 'https://openalex.org/W2789497025', 'https://openalex.org/W2536853808', 'https://openalex.org/W2030488530', 'https://openalex.org/W2889427787', 'https://openalex.org/W2032488553', 'https://openalex.org/W2090764203', 'https://openalex.org/W2143180712', 'https://openalex.org/W2041800793', 'https://openalex.org/W321571330', 'https://openalex.org/W2183922662', 'https://openalex.org/W2157569140', 'https://openalex.org/W2001219038', 'https://openalex.org/W2745025069', 'https://openalex.org/W2905083703', 'https://openalex.org/W2319302103', 'https://openalex.org/W2016618864', 'https://openalex.org/W2122026004', 'https://openalex.org/W2808683047', 'https://openalex.org/W2608063271', 'https://openalex.org/W2548268919', 'https://openalex.org/W2137744916', 'https://openalex.org/W2146071630', 'https://openalex.org/W2082487636', 'https://openalex.org/W1984192271', 'https://openalex.org/W2605220279', 'https://openalex.org/W2013153079', 'https://openalex.org/W2108827983', 'https://openalex.org/W2034159300', 'https://openalex.org/W2020743426', 'https://openalex.org/W1124560123', 'https://openalex.org/W1966589475', 'https://openalex.org/W2138079732', 'https://openalex.org/W2035259401', 'https://openalex.org/W1984176172', 'https://openalex.org/W1988099331', 'https://openalex.org/W2061203137', 'https://openalex.org/W2120110027', 'https://openalex.org/W2073253106', 'https://openalex.org/W2021102997', 'https://openalex.org/W2395399597', 'https://openalex.org/W2613787432', 'https://openalex.org/W2534790943', 'https://openalex.org/W2471977344', 'https://openalex.org/W1965993399', 'https://openalex.org/W2517200255', 'https://openalex.org/W2000169986', 'https://openalex.org/W2038598237', 'https://openalex.org/W2153163248', 'https://openalex.org/W2054145113', 'https://openalex.org/W1996639582', 'https://openalex.org/W1975592238', 'https://openalex.org/W2067508407', 'https://openalex.org/W2889044115', 'https://openalex.org/W2127643134', 'https://openalex.org/W2789616485', 'https://openalex.org/W1989186855', 'https://openalex.org/W2889502614', 'https://openalex.org/W2069581242', 'https://openalex.org/W2042181410', 'https://openalex.org/W2189417386', 'https://openalex.org/W2139231522', 'https://openalex.org/W2110911455', 'https://openalex.org/W2034766634', 'https://openalex.org/W2889275521', 'https://openalex.org/W2024733869', 'https://openalex.org/W2019287799', 'https://openalex.org/W151493266', 'https://openalex.org/W2115506669', 'https://openalex.org/W2076104081', 'https://openalex.org/W1974059926', 'https://openalex.org/W2159858406', 'https://openalex.org/W1979792881', 'https://openalex.org/W2767111014', 'https://openalex.org/W2087223769', 'https://openalex.org/W1982908312', 'https://openalex.org/W1524520041', 'https://openalex.org/W2057822435', 'https://openalex.org/W2078788212', 'https://openalex.org/W2609598179', 'https://openalex.org/W1997688490', 'https://openalex.org/W2401175036', 'https://openalex.org/W2116674923', 'https://openalex.org/W2140541760', 'https://openalex.org/W2128608950', 'https://openalex.org/W2032233631', 'https://openalex.org/W2062838813', 'https://openalex.org/W2535487273', 'https://openalex.org/W1986943394', 'https://openalex.org/W2119069889', 'https://openalex.org/W2784665486', 'https://openalex.org/W2138914649', 'https://openalex.org/W2525133911', 'https://openalex.org/W2096629947', 'https://openalex.org/W1998713209', 'https://openalex.org/W2091757188', 'https://openalex.org/W2802699874', 'https://openalex.org/W2036084470', 'https://openalex.org/W2338438311', 'https://openalex.org/W1967625181', 'https://openalex.org/W2101662502', 'https://openalex.org/W2071643438', 'https://openalex.org/W2548991519', 'https://openalex.org/W1913758332', 'https://openalex.org/W2065592785', 'https://openalex.org/W2316956449', 'https://openalex.org/W2738353274', 'https://openalex.org/W2169201993', 'https://openalex.org/W2407427462', 'https://openalex.org/W154036070', 'https://openalex.org/W2896610323', 'https://openalex.org/W2155705765', 'https://openalex.org/W1993519356', 'https://openalex.org/W2793508109', 'https://openalex.org/W179850243', 'https://openalex.org/W585740886', 'https://openalex.org/W1676190799', 'https://openalex.org/W4285719527', 'https://openalex.org/W2741763241', 'https://openalex.org/W44638342', 'https://openalex.org/W2187959814', 'https://openalex.org/W2181459931', 'https://openalex.org/W2020048803', 'https://openalex.org/W1787767473', 'https://openalex.org/W2099446268', 'https://openalex.org/W2938693516', 'https://openalex.org/W2099104918', 'https://openalex.org/W2338261539', 'https://openalex.org/W103272003', 'https://openalex.org/W2055487029', 'https://openalex.org/W2801093643', 'https://openalex.org/W2059429441', 'https://openalex.org/W2188732554', 'https://openalex.org/W51219981', 'https://openalex.org/W2112635963', 'https://openalex.org/W2172256532', 'https://openalex.org/W2049155070', 'https://openalex.org/W38137075', 'https://openalex.org/W1986062621', 'https://openalex.org/W1549535920', 'https://openalex.org/W2893842976']",2019-10-11
https://openalex.org/W4310079959,https://doi.org/10.1007/s11042-022-14273-1,"A review of machine transliteration, translation, evaluation metrics and datasets in Indian Languages",,"['https://openalex.org/W3200227504', 'https://openalex.org/W2106818711', 'https://openalex.org/W4206473080', 'https://openalex.org/W2062476972', 'https://openalex.org/W2144746247', 'https://openalex.org/W4231114479', 'https://openalex.org/W2897289087', 'https://openalex.org/W3119736906', 'https://openalex.org/W1602767452', 'https://openalex.org/W2000394794', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133836244', 'https://openalex.org/W3161226109', 'https://openalex.org/W2889326796', 'https://openalex.org/W2329187775', 'https://openalex.org/W2117583806', 'https://openalex.org/W2123034775', 'https://openalex.org/W2970397283', 'https://openalex.org/W2888541716', 'https://openalex.org/W2064675550', 'https://openalex.org/W3010484942', 'https://openalex.org/W2550821151', 'https://openalex.org/W1884583896', 'https://openalex.org/W2062036323', 'https://openalex.org/W2964343359', 'https://openalex.org/W1502957213', 'https://openalex.org/W2002535549', 'https://openalex.org/W2963212250', 'https://openalex.org/W1902237438', 'https://openalex.org/W4296580465', 'https://openalex.org/W1988099331', 'https://openalex.org/W2031046392', 'https://openalex.org/W2250929115', 'https://openalex.org/W2085854266', 'https://openalex.org/W2954252867', 'https://openalex.org/W1973923101', 'https://openalex.org/W2142768229', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963323070', 'https://openalex.org/W2805790316', 'https://openalex.org/W2331144166', 'https://openalex.org/W2333367237', 'https://openalex.org/W1972300018', 'https://openalex.org/W1480573250', 'https://openalex.org/W4205133930', 'https://openalex.org/W3196711103', 'https://openalex.org/W2131774270', 'https://openalex.org/W2766115939', 'https://openalex.org/W3033494475', 'https://openalex.org/W3035922389', 'https://openalex.org/W3088766183', 'https://openalex.org/W2138986769', 'https://openalex.org/W2130919220', 'https://openalex.org/W2114544510', 'https://openalex.org/W2038698865', 'https://openalex.org/W2895547158', 'https://openalex.org/W2964296923', 'https://openalex.org/W3126600571', 'https://openalex.org/W4230872509', 'https://openalex.org/W1490721004']",2022-11-25
https://openalex.org/W4311550865,https://doi.org/10.48550/arxiv.2212.05409,"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages","Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",[],2022-12-11
https://openalex.org/W4285077564,https://doi.org/10.48550/arxiv.2207.04672,No Language Left Behind: Scaling Human-Centered Machine Translation,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",[],2022-07-11
https://openalex.org/W2995929068,https://doi.org/10.48550/arxiv.1912.06670,Common Voice: A Massively-Multilingual Speech Corpus,"The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.","['https://openalex.org/W3084713055', 'https://openalex.org/W1922655562', 'https://openalex.org/W2292087804', 'https://openalex.org/W2757672955', 'https://openalex.org/W1533861849', 'https://openalex.org/W2127141656']",2019-12-13
https://openalex.org/W3054645415,https://doi.org/10.48550/arxiv.2007.10310,CoVoST 2 and Massively Multilingual Speech-to-Text Translation,"Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.","['https://openalex.org/W2327501763', 'https://openalex.org/W3008549139', 'https://openalex.org/W3092424727', 'https://openalex.org/W1524333225', 'https://openalex.org/W2933138175', 'https://openalex.org/W3103029570', 'https://openalex.org/W2962735107', 'https://openalex.org/W2101105183', 'https://openalex.org/W3015698636', 'https://openalex.org/W2963532001', 'https://openalex.org/W2945700568', 'https://openalex.org/W3008125272', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970295111', 'https://openalex.org/W3101860695', 'https://openalex.org/W2962784628', 'https://openalex.org/W2991213871', 'https://openalex.org/W2582956876', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963250244', 'https://openalex.org/W2958953787', 'https://openalex.org/W2936774411']",2020-07-20
https://openalex.org/W3106807794,https://doi.org/10.1109/slt48900.2021.9383459,VOXLINGUA107: A Dataset for Spoken Language Recognition,"This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.","['https://openalex.org/W6635144179', 'https://openalex.org/W6759215631', 'https://openalex.org/W2890964092', 'https://openalex.org/W2807627734', 'https://openalex.org/W6770979763', 'https://openalex.org/W2963371159', 'https://openalex.org/W2194775991', 'https://openalex.org/W2752782242', 'https://openalex.org/W2972552635', 'https://openalex.org/W2889519245', 'https://openalex.org/W3016021175', 'https://openalex.org/W2231567078', 'https://openalex.org/W2516764878', 'https://openalex.org/W3007095894', 'https://openalex.org/W2726515241', 'https://openalex.org/W2806839981', 'https://openalex.org/W6631362777', 'https://openalex.org/W2808631503', 'https://openalex.org/W2916297645', 'https://openalex.org/W6766978945', 'https://openalex.org/W1970481385', 'https://openalex.org/W2402735123', 'https://openalex.org/W2288817436', 'https://openalex.org/W2509913395', 'https://openalex.org/W2963420686', 'https://openalex.org/W3146842157', 'https://openalex.org/W2994088087']",2021-01-19
https://openalex.org/W2966095117,https://doi.org/10.48550/arxiv.1907.12895,MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken\n Utterances Extracted from the Bible,"The CMU Wilderness Multilingual Speech Dataset (Black, 2019) is a newly\npublished multilingual speech dataset based on recorded readings of the New\nTestament. It provides data to build Automatic Speech Recognition (ASR) and\nText-to-Speech (TTS) models for potentially 700 languages. However, the fact\nthat the source content (the Bible) is the same for all the languages is not\nexploited to date.Therefore, this article proposes to add multilingual links\nbetween speech segments in different languages, and shares a large and clean\ndataset of 8,130 parallel spoken utterances across 8 languages (56 language\npairs). We name this corpus MaSS (Multilingual corpus of Sentence-aligned\nSpoken utterances). The covered languages (Basque, English, Finnish, French,\nHungarian, Romanian, Russian and Spanish) allow researches on speech-to-speech\nalignment as well as on translation for typologically different language pairs.\nThe quality of the final corpus is attested by human evaluation performed on a\ncorpus subset (100 utterances, 8 language pairs). Lastly, we showcase the\nusefulness of the final product on a bilingual speech retrieval task.\n","['https://openalex.org/W2250981492', 'https://openalex.org/W2936969148', 'https://openalex.org/W2739967986', 'https://openalex.org/W2899274165', 'https://openalex.org/W2964104866', 'https://openalex.org/W2960374072', 'https://openalex.org/W2964308564', 'https://openalex.org/W2102153514', 'https://openalex.org/W3015698636', 'https://openalex.org/W2964172053', 'https://openalex.org/W2202833113', 'https://openalex.org/W2963330681', 'https://openalex.org/W2572044271', 'https://openalex.org/W2945700568', 'https://openalex.org/W2586602577', 'https://openalex.org/W2899134946', 'https://openalex.org/W2041532239', 'https://openalex.org/W2964161387', 'https://openalex.org/W2605131327', 'https://openalex.org/W2785350307', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963609956', 'https://openalex.org/W2919290281', 'https://openalex.org/W1984076147', 'https://openalex.org/W2937197076', 'https://openalex.org/W2796315435', 'https://openalex.org/W2593011301', 'https://openalex.org/W2963431393']",2019-07-30
https://openalex.org/W4308756394,https://doi.org/10.48550/arxiv.2211.04508,SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,"We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available.",[],2022-11-08
https://openalex.org/W4281621399,https://doi.org/10.48550/arxiv.2205.12446,FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech,"We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.",[],2022-05-25
https://openalex.org/W4293332626,https://doi.org/10.48550/arxiv.2208.11761,IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages,"A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76\% for the Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.",[],2022-08-24
https://openalex.org/W4311000453,https://doi.org/10.48550/arxiv.2212.04356,Robust Speech Recognition via Large-Scale Weak Supervision,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",[],2022-12-06
https://openalex.org/W2972495969,https://doi.org/10.21437/interspeech.2019-1951,Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","['https://openalex.org/W2963609956', 'https://openalex.org/W2133564696', 'https://openalex.org/W4298174729', 'https://openalex.org/W2747920239', 'https://openalex.org/W2972970915', 'https://openalex.org/W2892620417', 'https://openalex.org/W4293569541', 'https://openalex.org/W2133300417', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963779652', 'https://openalex.org/W2097203679', 'https://openalex.org/W4293714597', 'https://openalex.org/W4385245566', 'https://openalex.org/W2113106066', 'https://openalex.org/W2963011080', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962824709', 'https://openalex.org/W2139647714', 'https://openalex.org/W2525778437', 'https://openalex.org/W2120847449', 'https://openalex.org/W1962947832', 'https://openalex.org/W2896538040', 'https://openalex.org/W2928941594', 'https://openalex.org/W1494198834', 'https://openalex.org/W2011783148', 'https://openalex.org/W2808706139', 'https://openalex.org/W2795581297', 'https://openalex.org/W2912492482', 'https://openalex.org/W3012492057', 'https://openalex.org/W1538023239', 'https://openalex.org/W2605131327', 'https://openalex.org/W2152834109', 'https://openalex.org/W4289383906', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W4294619240', 'https://openalex.org/W2788357188', 'https://openalex.org/W2949328740', 'https://openalex.org/W4298580827', 'https://openalex.org/W2941115821', 'https://openalex.org/W2794490148', 'https://openalex.org/W4300558631']",2019-09-13
https://openalex.org/W4382202628,https://doi.org/10.1609/aaai.v37i11.26521,IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian Languages,"A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. To evaluate language models in other languages, several language-specific GLUE datasets were created. The area of speech language understanding (SLU) has followed a similar trajectory. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relatively easy to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1,684 hours of labelled speech data across 12 Indian languages from 1,218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside the a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76% for Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages.","['https://openalex.org/W3030655016', 'https://openalex.org/W3170889247', 'https://openalex.org/W2995929068', 'https://openalex.org/W4288726319', 'https://openalex.org/W4297841896', 'https://openalex.org/W2896457183', 'https://openalex.org/W6966564287', 'https://openalex.org/W4298277274', 'https://openalex.org/W3028954259', 'https://openalex.org/W3013840636', 'https://openalex.org/W6804060842', 'https://openalex.org/W3017311573', 'https://openalex.org/W3099919888', 'https://openalex.org/W2895676041', 'https://openalex.org/W4285077564', 'https://openalex.org/W1494198834', 'https://openalex.org/W6713502316', 'https://openalex.org/W3160186953', 'https://openalex.org/W2890964092', 'https://openalex.org/W2895103375', 'https://openalex.org/W2799054028', 'https://openalex.org/W3157923770', 'https://openalex.org/W3030437843', 'https://openalex.org/W3197580070', 'https://openalex.org/W2404169761', 'https://openalex.org/W3035032094', 'https://openalex.org/W3213029956', 'https://openalex.org/W4285250921', 'https://openalex.org/W3213618310', 'https://openalex.org/W3035164673', 'https://openalex.org/W3173649224', 'https://openalex.org/W2808631503', 'https://openalex.org/W2923014074', 'https://openalex.org/W2726515241']",2023-06-26
https://openalex.org/W3180374548,https://doi.org/10.18653/v1/2022.acl-long.235,Direct Speech-to-Speech Translation With Discrete Units,"Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3007068036', 'https://openalex.org/W3197580070', 'https://openalex.org/W3157923770', 'https://openalex.org/W3210177631', 'https://openalex.org/W2605131327', 'https://openalex.org/W2995181338', 'https://openalex.org/W3112092703', 'https://openalex.org/W3007142233', 'https://openalex.org/W3173767661', 'https://openalex.org/W3098403858', 'https://openalex.org/W3169320628', 'https://openalex.org/W2152834109', 'https://openalex.org/W2035108931', 'https://openalex.org/W3033411150', 'https://openalex.org/W4394671563', 'https://openalex.org/W2963609956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963532001', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963979492', 'https://openalex.org/W4287553982', 'https://openalex.org/W2949328740', 'https://openalex.org/W3160525311', 'https://openalex.org/W3112616666', 'https://openalex.org/W2936774411', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963403868', 'https://openalex.org/W2998353611', 'https://openalex.org/W3118578889', 'https://openalex.org/W1494198834', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W3142316150', 'https://openalex.org/W2127141656', 'https://openalex.org/W3175871055', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972495969', 'https://openalex.org/W2973157397', 'https://openalex.org/W3186843219', 'https://openalex.org/W3119308075', 'https://openalex.org/W2097203679', 'https://openalex.org/W2747920239', 'https://openalex.org/W3130016944', 'https://openalex.org/W3099782249', 'https://openalex.org/W3092424727', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330']",2022-01-01
https://openalex.org/W4306393960,https://doi.org/10.1016/j.smhl.2022.100340,Fall detection from audios with Audio Transformers,,"['https://openalex.org/W6786663500', 'https://openalex.org/W6780218876', 'https://openalex.org/W2724727449', 'https://openalex.org/W2735430014', 'https://openalex.org/W2746591362', 'https://openalex.org/W6738423677', 'https://openalex.org/W4205240931', 'https://openalex.org/W2055698923', 'https://openalex.org/W2987786464', 'https://openalex.org/W2913969639', 'https://openalex.org/W2419341522', 'https://openalex.org/W6747680906', 'https://openalex.org/W6808983327', 'https://openalex.org/W3083408317', 'https://openalex.org/W4220859602', 'https://openalex.org/W3094550259', 'https://openalex.org/W4214522912', 'https://openalex.org/W6792909391', 'https://openalex.org/W4223543660', 'https://openalex.org/W3045910778', 'https://openalex.org/W3048067284', 'https://openalex.org/W3040026329', 'https://openalex.org/W6807162611', 'https://openalex.org/W2950883524', 'https://openalex.org/W4213016839', 'https://openalex.org/W2543632018', 'https://openalex.org/W3191137513', 'https://openalex.org/W6739901393', 'https://openalex.org/W3021765248', 'https://openalex.org/W3212131113', 'https://openalex.org/W2056818943', 'https://openalex.org/W2052332753', 'https://openalex.org/W4212943157', 'https://openalex.org/W3198035615', 'https://openalex.org/W4246949806', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W4210384831', 'https://openalex.org/W4385245566', 'https://openalex.org/W3196974791', 'https://openalex.org/W4237959174']",2022-10-17
https://openalex.org/W3174758275,https://doi.org/10.48550/arxiv.2106.15561,A Survey on Neural Speech Synthesis,"Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.","['https://openalex.org/W2104012431', 'https://openalex.org/W2069859485', 'https://openalex.org/W3015796413', 'https://openalex.org/W2105594594', 'https://openalex.org/W3097587372', 'https://openalex.org/W2963330667', 'https://openalex.org/W2997540646', 'https://openalex.org/W3031409582', 'https://openalex.org/W2963192573', 'https://openalex.org/W2940405045', 'https://openalex.org/W2962793481', 'https://openalex.org/W3082910224', 'https://openalex.org/W3144667183', 'https://openalex.org/W854541894', 'https://openalex.org/W3091928890', 'https://openalex.org/W1821462560', 'https://openalex.org/W2888456631', 'https://openalex.org/W3095790275', 'https://openalex.org/W3146431306', 'https://openalex.org/W3194208059', 'https://openalex.org/W3113850747', 'https://openalex.org/W2963927338', 'https://openalex.org/W2000513720', 'https://openalex.org/W2964281804', 'https://openalex.org/W3128930043', 'https://openalex.org/W2963302875', 'https://openalex.org/W3098869438', 'https://openalex.org/W2428180336', 'https://openalex.org/W3096656663', 'https://openalex.org/W3096086473', 'https://openalex.org/W1861172732', 'https://openalex.org/W3015645837', 'https://openalex.org/W423487555', 'https://openalex.org/W3094402293', 'https://openalex.org/W3133185996', 'https://openalex.org/W3139170550', 'https://openalex.org/W2943543019', 'https://openalex.org/W2963712897', 'https://openalex.org/W3048023795', 'https://openalex.org/W3020570669', 'https://openalex.org/W2972359262', 'https://openalex.org/W648786980', 'https://openalex.org/W3094785744', 'https://openalex.org/W28194048', 'https://openalex.org/W2962824709', 'https://openalex.org/W2991417167', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972882294', 'https://openalex.org/W3099992561', 'https://openalex.org/W2401698713', 'https://openalex.org/W3095389792', 'https://openalex.org/W2976159681', 'https://openalex.org/W3095762785', 'https://openalex.org/W3037695135', 'https://openalex.org/W2919115771', 'https://openalex.org/W2546744831', 'https://openalex.org/W3023706973', 'https://openalex.org/W3097895838', 'https://openalex.org/W3028954259', 'https://openalex.org/W2009674825', 'https://openalex.org/W2904351664', 'https://openalex.org/W3174847158', 'https://openalex.org/W2962736171', 'https://openalex.org/W2973034126', 'https://openalex.org/W2963736842', 'https://openalex.org/W3146550708', 'https://openalex.org/W2765486990', 'https://openalex.org/W2924677654', 'https://openalex.org/W2972519044', 'https://openalex.org/W3098304089', 'https://openalex.org/W2972956685', 'https://openalex.org/W2945656493', 'https://openalex.org/W1608375737', 'https://openalex.org/W3156871171', 'https://openalex.org/W3094650042', 'https://openalex.org/W2964167449', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964307104', 'https://openalex.org/W2152205330', 'https://openalex.org/W3163906773', 'https://openalex.org/W2963912924', 'https://openalex.org/W3169905056', 'https://openalex.org/W3103104054', 'https://openalex.org/W2994373303', 'https://openalex.org/W9334768', 'https://openalex.org/W2147880316', 'https://openalex.org/W3133963484', 'https://openalex.org/W3095410713', 'https://openalex.org/W3097795905', 'https://openalex.org/W3093185524', 'https://openalex.org/W2120847449', 'https://openalex.org/W3033194228', 'https://openalex.org/W2250357346', 'https://openalex.org/W2917688842', 'https://openalex.org/W2605141709', 'https://openalex.org/W2947723875', 'https://openalex.org/W3097264669', 'https://openalex.org/W3096082720', 'https://openalex.org/W2749651610', 'https://openalex.org/W3165905282', 'https://openalex.org/W2970997853', 'https://openalex.org/W3036167779', 'https://openalex.org/W3092316169', 'https://openalex.org/W1492383498', 'https://openalex.org/W3107262928', 'https://openalex.org/W3104081910', 'https://openalex.org/W2963827314', 'https://openalex.org/W2964138190', 'https://openalex.org/W2801554275', 'https://openalex.org/W3171962689', 'https://openalex.org/W3098403858', 'https://openalex.org/W3015853838', 'https://openalex.org/W3030437843', 'https://openalex.org/W2907262790', 'https://openalex.org/W2093450784', 'https://openalex.org/W3125708547', 'https://openalex.org/W3104557543', 'https://openalex.org/W3021902032', 'https://openalex.org/W3097032879', 'https://openalex.org/W2963432880', 'https://openalex.org/W2516559027', 'https://openalex.org/W67332896', 'https://openalex.org/W2163377725', 'https://openalex.org/W3015651285', 'https://openalex.org/W3015347659', 'https://openalex.org/W2964301388', 'https://openalex.org/W2103636088', 'https://openalex.org/W2395578248', 'https://openalex.org/W2922185715', 'https://openalex.org/W2964343746', 'https://openalex.org/W2587284713', 'https://openalex.org/W3162673269', 'https://openalex.org/W3048217770', 'https://openalex.org/W3140294857', 'https://openalex.org/W3206275820', 'https://openalex.org/W3015841875', 'https://openalex.org/W3181257032', 'https://openalex.org/W3134361330', 'https://openalex.org/W2889048668', 'https://openalex.org/W2977311057', 'https://openalex.org/W2963300588', 'https://openalex.org/W2173520492', 'https://openalex.org/W3095851005', 'https://openalex.org/W2949382160', 'https://openalex.org/W2127141656', 'https://openalex.org/W2493353997', 'https://openalex.org/W2885800352', 'https://openalex.org/W2962882868', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963090522', 'https://openalex.org/W3016159759', 'https://openalex.org/W2972440097', 'https://openalex.org/W3155768880', 'https://openalex.org/W3015922793', 'https://openalex.org/W3025013833', 'https://openalex.org/W3098557217', 'https://openalex.org/W3092442149', 'https://openalex.org/W3090474612', 'https://openalex.org/W2526838772', 'https://openalex.org/W3169874582', 'https://openalex.org/W2972951102', 'https://openalex.org/W2066452495', 'https://openalex.org/W2969407538', 'https://openalex.org/W2964268978', 'https://openalex.org/W3016137096', 'https://openalex.org/W2972375191', 'https://openalex.org/W2903739847', 'https://openalex.org/W2111284386', 'https://openalex.org/W3088782775', 'https://openalex.org/W2991571120', 'https://openalex.org/W2145892079', 'https://openalex.org/W2296704011', 'https://openalex.org/W2937297115', 'https://openalex.org/W1861945760', 'https://openalex.org/W2096980176', 'https://openalex.org/W2912237252', 'https://openalex.org/W2970730223', 'https://openalex.org/W3048768961', 'https://openalex.org/W3144035034', 'https://openalex.org/W2072026461', 'https://openalex.org/W2962699523', 'https://openalex.org/W1964668808', 'https://openalex.org/W3162948689', 'https://openalex.org/W3095419383', 'https://openalex.org/W3168470634', 'https://openalex.org/W2806253224', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963620343', 'https://openalex.org/W3129651364', 'https://openalex.org/W2102737569', 'https://openalex.org/W3175871055', 'https://openalex.org/W3034794073', 'https://openalex.org/W1583912456', 'https://openalex.org/W3126404598', 'https://openalex.org/W2951004968', 'https://openalex.org/W2996286887', 'https://openalex.org/W2970079483', 'https://openalex.org/W2542835211', 'https://openalex.org/W2963078821', 'https://openalex.org/W1999885698', 'https://openalex.org/W3141224548', 'https://openalex.org/W2154920538', 'https://openalex.org/W2293049663', 'https://openalex.org/W2254292464', 'https://openalex.org/W2963929654', 'https://openalex.org/W1576227399', 'https://openalex.org/W3103034946', 'https://openalex.org/W2963047245', 'https://openalex.org/W3102451458', 'https://openalex.org/W2972457126', 'https://openalex.org/W2327501763', 'https://openalex.org/W2973203693', 'https://openalex.org/W174066062', 'https://openalex.org/W2071764087', 'https://openalex.org/W3149617007', 'https://openalex.org/W3134835907', 'https://openalex.org/W1600722501', 'https://openalex.org/W3114532049', 'https://openalex.org/W3015680182', 'https://openalex.org/W2999160446', 'https://openalex.org/W2954386831', 'https://openalex.org/W3015614880', 'https://openalex.org/W2403419258', 'https://openalex.org/W2964272710', 'https://openalex.org/W3125481789', 'https://openalex.org/W3158102353', 'https://openalex.org/W2026679866', 'https://openalex.org/W3141402801', 'https://openalex.org/W3016136182', 'https://openalex.org/W2894295011', 'https://openalex.org/W3131702728', 'https://openalex.org/W2998458489', 'https://openalex.org/W2963522141', 'https://openalex.org/W3172617364', 'https://openalex.org/W3015419784', 'https://openalex.org/W1916501714', 'https://openalex.org/W2164107060', 'https://openalex.org/W2972895078', 'https://openalex.org/W2049950192', 'https://openalex.org/W3034949308', 'https://openalex.org/W2884873108', 'https://openalex.org/W3141350488', 'https://openalex.org/W3016160783', 'https://openalex.org/W3097003111', 'https://openalex.org/W3097728852', 'https://openalex.org/W2963534259', 'https://openalex.org/W2972333964', 'https://openalex.org/W2972473628', 'https://openalex.org/W3161109662', 'https://openalex.org/W2112800201', 'https://openalex.org/W2612199078', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972374322', 'https://openalex.org/W3006108364', 'https://openalex.org/W2970006822', 'https://openalex.org/W2904459034', 'https://openalex.org/W3024747869', 'https://openalex.org/W3177989406', 'https://openalex.org/W162654330', 'https://openalex.org/W3096442195', 'https://openalex.org/W3035083561', 'https://openalex.org/W2182919134', 'https://openalex.org/W3035068567', 'https://openalex.org/W2964243274', 'https://openalex.org/W3097538987', 'https://openalex.org/W3147900189', 'https://openalex.org/W2962936105', 'https://openalex.org/W3097290232', 'https://openalex.org/W2603120115', 'https://openalex.org/W3161492781', 'https://openalex.org/W3015440759', 'https://openalex.org/W3111791812', 'https://openalex.org/W2166469361', 'https://openalex.org/W3081416955', 'https://openalex.org/W3153370059', 'https://openalex.org/W3041574813', 'https://openalex.org/W3148333649', 'https://openalex.org/W3163339651', 'https://openalex.org/W3135619128', 'https://openalex.org/W3015826515', 'https://openalex.org/W3047107405', 'https://openalex.org/W2122410182', 'https://openalex.org/W3130016944', 'https://openalex.org/W1526236009', 'https://openalex.org/W3007419862', 'https://openalex.org/W1810943226', 'https://openalex.org/W3150713669', 'https://openalex.org/W3025528898', 'https://openalex.org/W2963091184', 'https://openalex.org/W2963609956', 'https://openalex.org/W3178839419', 'https://openalex.org/W3167266074', 'https://openalex.org/W2808342999', 'https://openalex.org/W3169744561', 'https://openalex.org/W2294797155', 'https://openalex.org/W2990124956', 'https://openalex.org/W2252225757', 'https://openalex.org/W2747874407', 'https://openalex.org/W3097144763', 'https://openalex.org/W2162433174', 'https://openalex.org/W2972885185', 'https://openalex.org/W2972802841', 'https://openalex.org/W2888169323', 'https://openalex.org/W3095883095', 'https://openalex.org/W3150807214', 'https://openalex.org/W3130774171', 'https://openalex.org/W3029282897', 'https://openalex.org/W2963035245', 'https://openalex.org/W2929299742', 'https://openalex.org/W2550497374', 'https://openalex.org/W3123097577', 'https://openalex.org/W2973217961', 'https://openalex.org/W3015484365', 'https://openalex.org/W3015338123', 'https://openalex.org/W3094124570', 'https://openalex.org/W3160289600', 'https://openalex.org/W3022876224', 'https://openalex.org/W3096429957', 'https://openalex.org/W2972595148', 'https://openalex.org/W7606746', 'https://openalex.org/W2801291345', 'https://openalex.org/W2964308564', 'https://openalex.org/W2973084242', 'https://openalex.org/W2806733747', 'https://openalex.org/W1663973292', 'https://openalex.org/W3096702751', 'https://openalex.org/W2917245127', 'https://openalex.org/W3146104336', 'https://openalex.org/W2064076387', 'https://openalex.org/W3112470437', 'https://openalex.org/W3095491807', 'https://openalex.org/W2963782041', 'https://openalex.org/W3095545636', 'https://openalex.org/W2337335398', 'https://openalex.org/W2945078028', 'https://openalex.org/W2914049472', 'https://openalex.org/W3097376233', 'https://openalex.org/W2963242190', 'https://openalex.org/W3135197092', 'https://openalex.org/W2515943672', 'https://openalex.org/W2963568578', 'https://openalex.org/W2251811146', 'https://openalex.org/W3096456328', 'https://openalex.org/W2766812927', 'https://openalex.org/W2970925677', 'https://openalex.org/W2952127920', 'https://openalex.org/W3142087749', 'https://openalex.org/W2945544731', 'https://openalex.org/W3164133896', 'https://openalex.org/W2593414223', 'https://openalex.org/W3147652402', 'https://openalex.org/W2553303224', 'https://openalex.org/W3095787156', 'https://openalex.org/W2901997113', 'https://openalex.org/W2973190645', 'https://openalex.org/W2559246505', 'https://openalex.org/W1549877291', 'https://openalex.org/W2889028433', 'https://openalex.org/W1593247906', 'https://openalex.org/W2948211236', 'https://openalex.org/W3160919572', 'https://openalex.org/W2972574864', 'https://openalex.org/W2102003408', 'https://openalex.org/W3095199334', 'https://openalex.org/W2963796886', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963975282', 'https://openalex.org/W1761875812', 'https://openalex.org/W2971905065', 'https://openalex.org/W3174572320', 'https://openalex.org/W3141515915', 'https://openalex.org/W1514101016', 'https://openalex.org/W2616705520', 'https://openalex.org/W2962739369', 'https://openalex.org/W3172387178', 'https://openalex.org/W3153534135', 'https://openalex.org/W3161782335', 'https://openalex.org/W3095012670', 'https://openalex.org/W3154443551', 'https://openalex.org/W3097297926', 'https://openalex.org/W2972597685', 'https://openalex.org/W2963403868', 'https://openalex.org/W2906797124', 'https://openalex.org/W2963175743', 'https://openalex.org/W2134973740', 'https://openalex.org/W3095990227', 'https://openalex.org/W2090755665', 'https://openalex.org/W3144988954', 'https://openalex.org/W3212683077', 'https://openalex.org/W3155727213', 'https://openalex.org/W2064675550', 'https://openalex.org/W2527729766', 'https://openalex.org/W2996573371', 'https://openalex.org/W2150658333', 'https://openalex.org/W1973424788', 'https://openalex.org/W3134354180', 'https://openalex.org/W3151450932', 'https://openalex.org/W2962691331', 'https://openalex.org/W2964169091', 'https://openalex.org/W2409550820', 'https://openalex.org/W3007859642', 'https://openalex.org/W2889606145', 'https://openalex.org/W3097514409', 'https://openalex.org/W2049686551', 'https://openalex.org/W3149705978', 'https://openalex.org/W3016090709', 'https://openalex.org/W2968203043', 'https://openalex.org/W1901616594', 'https://openalex.org/W3095459301', 'https://openalex.org/W1570629387', 'https://openalex.org/W3128910262', 'https://openalex.org/W2905933322', 'https://openalex.org/W1574901103', 'https://openalex.org/W2969521066', 'https://openalex.org/W3139510151', 'https://openalex.org/W2926840633', 'https://openalex.org/W3093733783', 'https://openalex.org/W2605320104', 'https://openalex.org/W3095999419', 'https://openalex.org/W2995233853', 'https://openalex.org/W2962879692', 'https://openalex.org/W1578102511', 'https://openalex.org/W2963308316', 'https://openalex.org/W3096303254', 'https://openalex.org/W2099471712', 'https://openalex.org/W2972999331', 'https://openalex.org/W3094002217']",2021-06-29
https://openalex.org/W4385570009,https://doi.org/10.18653/v1/2023.acl-long.693,"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages","Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W3093517588', 'https://openalex.org/W3100880133', 'https://openalex.org/W2145755360', 'https://openalex.org/W2948902769', 'https://openalex.org/W3000965575', 'https://openalex.org/W3035497479', 'https://openalex.org/W3155618984', 'https://openalex.org/W3039838634', 'https://openalex.org/W4385572376', 'https://openalex.org/W3102483398', 'https://openalex.org/W2525778437', 'https://openalex.org/W3156404059', 'https://openalex.org/W3176765167', 'https://openalex.org/W2950733326', 'https://openalex.org/W3137010024', 'https://openalex.org/W4319915529', 'https://openalex.org/W2963341956', 'https://openalex.org/W2914120296', 'https://openalex.org/W2908510526', 'https://openalex.org/W2891555348', 'https://openalex.org/W3173172013', 'https://openalex.org/W3035579820', 'https://openalex.org/W2963748441', 'https://openalex.org/W3200218906', 'https://openalex.org/W3035390927', 'https://openalex.org/W3169369929', 'https://openalex.org/W2970062726', 'https://openalex.org/W3136221257', 'https://openalex.org/W3175018633', 'https://openalex.org/W2965373594', 'https://openalex.org/W4281660423', 'https://openalex.org/W3169483174', 'https://openalex.org/W3100198908', 'https://openalex.org/W2923014074', 'https://openalex.org/W3214173179', 'https://openalex.org/W4221151728', 'https://openalex.org/W4285077564', 'https://openalex.org/W2970752815', 'https://openalex.org/W2943552823', 'https://openalex.org/W4224213825', 'https://openalex.org/W4224310727', 'https://openalex.org/W4285255685', 'https://openalex.org/W2963846996', 'https://openalex.org/W2996428491', 'https://openalex.org/W2952087486', 'https://openalex.org/W3035032094', 'https://openalex.org/W3099919888']",2023-01-01
https://openalex.org/W4389524529,https://doi.org/10.18653/v1/2023.emnlp-main.875,GlobalBench: A Benchmark for Global Progress in Natural Language Processing,"Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages and includes 1,128 system submissions spanning 62 languages.","['https://openalex.org/W3035497479', 'https://openalex.org/W4283383808', 'https://openalex.org/W2923014074', 'https://openalex.org/W3035579820', 'https://openalex.org/W3213234281', 'https://openalex.org/W2962736243', 'https://openalex.org/W2579343286', 'https://openalex.org/W4386576720', 'https://openalex.org/W1840435438', 'https://openalex.org/W3171654528', 'https://openalex.org/W3034469191', 'https://openalex.org/W3214173179', 'https://openalex.org/W2790235966', 'https://openalex.org/W2804897457', 'https://openalex.org/W2077302143', 'https://openalex.org/W3090350559', 'https://openalex.org/W3162296828', 'https://openalex.org/W4285298878', 'https://openalex.org/W3102483398', 'https://openalex.org/W2513440303', 'https://openalex.org/W3207937903', 'https://openalex.org/W2251939518', 'https://openalex.org/W2943552823', 'https://openalex.org/W2163455955', 'https://openalex.org/W3086966320', 'https://openalex.org/W4281690148', 'https://openalex.org/W2898662126', 'https://openalex.org/W3206010442', 'https://openalex.org/W2060854787', 'https://openalex.org/W4297895859', 'https://openalex.org/W2127795553', 'https://openalex.org/W4385571031', 'https://openalex.org/W2952087486', 'https://openalex.org/W4389519066', 'https://openalex.org/W3168656614', 'https://openalex.org/W2891555348', 'https://openalex.org/W2973088264', 'https://openalex.org/W3045462440', 'https://openalex.org/W3169369929', 'https://openalex.org/W2963846996', 'https://openalex.org/W2070246124', 'https://openalex.org/W4385573424', 'https://openalex.org/W2250790822', 'https://openalex.org/W3035032094']",2023-01-01
https://openalex.org/W4296070387,https://doi.org/10.21437/interspeech.2022-11032,Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation,,[],2022-09-16
https://openalex.org/W4226444650,https://doi.org/10.48550/arxiv.2201.03713,CVSS Corpus and Massively Multilingual Speech-to-Speech Translation,"We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",[],2022-01-11
https://openalex.org/W4386142098,https://doi.org/10.48550/arxiv.2308.11596,SeamlessM4T: Massively Multilingual &amp; Multimodal Machine Translation,"What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced and accessible at https://github.com/facebookresearch/seamless_communication",[],2023-08-22
https://openalex.org/W4378105483,https://doi.org/10.48550/arxiv.2305.13516,"Scaling Speech Technology to 1,000+ Languages","Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",[],2023-05-22
https://openalex.org/W4389518827,https://doi.org/10.18653/v1/2023.findings-emnlp.438,Toward Joint Language Modeling for Speech Units and Text,"Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.","['https://openalex.org/W4285595742', 'https://openalex.org/W3030437843', 'https://openalex.org/W3169320628', 'https://openalex.org/W3095410713', 'https://openalex.org/W4292779060', 'https://openalex.org/W1522301498', 'https://openalex.org/W2747874407', 'https://openalex.org/W2896457183', 'https://openalex.org/W4394671563', 'https://openalex.org/W3197580070', 'https://openalex.org/W3119308075', 'https://openalex.org/W4320194748', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963250244', 'https://openalex.org/W2938704169', 'https://openalex.org/W4226120743', 'https://openalex.org/W3036601975', 'https://openalex.org/W4315705838', 'https://openalex.org/W4312052802', 'https://openalex.org/W2900260828', 'https://openalex.org/W4312121834', 'https://openalex.org/W4385571229', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963347649', 'https://openalex.org/W4308349017', 'https://openalex.org/W4226103796', 'https://openalex.org/W4229005866', 'https://openalex.org/W4375869259', 'https://openalex.org/W3207222250', 'https://openalex.org/W4223622550', 'https://openalex.org/W4287079508', 'https://openalex.org/W4310638188', 'https://openalex.org/W4385823130', 'https://openalex.org/W3209984917', 'https://openalex.org/W4297795751', 'https://openalex.org/W3148001440', 'https://openalex.org/W4221155340', 'https://openalex.org/W3097777922', 'https://openalex.org/W4286984129', 'https://openalex.org/W4300980246', 'https://openalex.org/W3035390927', 'https://openalex.org/W3140429000', 'https://openalex.org/W4226065498', 'https://openalex.org/W2933138175', 'https://openalex.org/W4313679638']",2023-01-01
https://openalex.org/W4385572318,https://doi.org/10.18653/v1/2023.acl-long.899,SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,International audience,[],2023-01-01
https://openalex.org/W4319862635,https://doi.org/10.1109/slt54892.2023.10023141,FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech,"We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4226033575', 'https://openalex.org/W6810259195', 'https://openalex.org/W3097777922', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198429080', 'https://openalex.org/W3204696009', 'https://openalex.org/W4210463634', 'https://openalex.org/W3096215352', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W3213029956', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W3196509775', 'https://openalex.org/W6771467084', 'https://openalex.org/W2743686029', 'https://openalex.org/W3015698636', 'https://openalex.org/W3092085609', 'https://openalex.org/W3197771105', 'https://openalex.org/W6810701745', 'https://openalex.org/W3139878283', 'https://openalex.org/W2937197076', 'https://openalex.org/W3169369929', 'https://openalex.org/W27049869', 'https://openalex.org/W6784577980', 'https://openalex.org/W2064675550', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015877095', 'https://openalex.org/W2914699162', 'https://openalex.org/W2105981609', 'https://openalex.org/W1984076147', 'https://openalex.org/W3213148312', 'https://openalex.org/W6803675045', 'https://openalex.org/W2965538726', 'https://openalex.org/W3039695075', 'https://openalex.org/W2220457451', 'https://openalex.org/W3169483174', 'https://openalex.org/W4226444650', 'https://openalex.org/W4221155340', 'https://openalex.org/W3213018012']",2023-01-09
https://openalex.org/W2937197076,https://doi.org/10.1109/icassp.2019.8683536,CMU Wilderness Multilingual Speech Dataset,"This paper describes the CMU Wilderness Multilingual Speech Dataset. A dataset of over 700 different languages providing audio, aligned text and word pronunciations. On average each language provides around 20 hours of sentence-lengthed transcriptions. We describe our multi-pass alignment techniques and evaluate the results by building speech synthesizers on the aligned data. Most of the resulting synthesizers are good enough for deployment and use. The tools to do this work are released as open source, and instructions on how to apply such alignment for novel languages are given.","['https://openalex.org/W6600088033', 'https://openalex.org/W2749074593', 'https://openalex.org/W2104653033', 'https://openalex.org/W6798679566', 'https://openalex.org/W6712032203', 'https://openalex.org/W6602491085', 'https://openalex.org/W6631852945', 'https://openalex.org/W6609724545', 'https://openalex.org/W2524541983', 'https://openalex.org/W6608197479', 'https://openalex.org/W6630961876', 'https://openalex.org/W256536324', 'https://openalex.org/W2396312253', 'https://openalex.org/W1516533146', 'https://openalex.org/W1529628403', 'https://openalex.org/W3177989406', 'https://openalex.org/W2533495', 'https://openalex.org/W202879582', 'https://openalex.org/W61749939']",2019-04-16
https://openalex.org/W3015315843,https://doi.org/10.1109/icassp40776.2020.9054567,Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?,"Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.","['https://openalex.org/W2606974598', 'https://openalex.org/W6637373629', 'https://openalex.org/W2962780374', 'https://openalex.org/W6629717138', 'https://openalex.org/W6627665034', 'https://openalex.org/W6732426729', 'https://openalex.org/W2972856300', 'https://openalex.org/W2917340025', 'https://openalex.org/W2340080546', 'https://openalex.org/W2973071600', 'https://openalex.org/W6741568435', 'https://openalex.org/W2903006902', 'https://openalex.org/W6746559740', 'https://openalex.org/W6675365184', 'https://openalex.org/W2327501763', 'https://openalex.org/W2766219058', 'https://openalex.org/W6683183305', 'https://openalex.org/W6713762819', 'https://openalex.org/W6604617924', 'https://openalex.org/W2888867175', 'https://openalex.org/W4235128394', 'https://openalex.org/W6712736788', 'https://openalex.org/W2889429804', 'https://openalex.org/W2935701729', 'https://openalex.org/W2098796164', 'https://openalex.org/W2104512693', 'https://openalex.org/W2320195920', 'https://openalex.org/W2134387846', 'https://openalex.org/W2133824856', 'https://openalex.org/W2144827818', 'https://openalex.org/W6745913901', 'https://openalex.org/W6751512325', 'https://openalex.org/W6631362777', 'https://openalex.org/W2158880150', 'https://openalex.org/W2407080277', 'https://openalex.org/W2736648940', 'https://openalex.org/W2102113734', 'https://openalex.org/W2774998006', 'https://openalex.org/W2057745663', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963452667', 'https://openalex.org/W2769075302', 'https://openalex.org/W2398871418', 'https://openalex.org/W1686810756', 'https://openalex.org/W1524333225', 'https://openalex.org/W2577008904', 'https://openalex.org/W112239495', 'https://openalex.org/W1164175658']",2020-04-09
https://openalex.org/W3081279708,https://doi.org/10.1145/3394486.3403249,DeepSinger: Singing Voice Synthesis with Data Mined From the Web,"In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffn-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness. Our audio samples are shown in https://speechresearch.github.io/deepsinger/.","['https://openalex.org/W2778460379', 'https://openalex.org/W2108862644', 'https://openalex.org/W2482558056', 'https://openalex.org/W2134387846', 'https://openalex.org/W2120847449', 'https://openalex.org/W2150658333', 'https://openalex.org/W6664164299', 'https://openalex.org/W2889244839', 'https://openalex.org/W2747874407', 'https://openalex.org/W2515336442', 'https://openalex.org/W2007815473', 'https://openalex.org/W2964243274', 'https://openalex.org/W2767052532', 'https://openalex.org/W1904711963', 'https://openalex.org/W2963975282', 'https://openalex.org/W2902351815', 'https://openalex.org/W2284628133', 'https://openalex.org/W2963403868', 'https://openalex.org/W2736648940']",2020-08-20
https://openalex.org/W3097514409,https://doi.org/10.21437/interspeech.2020-1410,XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis System,"This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling.We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added.2) To attenuate off-key issues, we add a residual connection in F0 prediction.3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement.Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively.In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.","['https://openalex.org/W2471520273', 'https://openalex.org/W2066598518', 'https://openalex.org/W3015437531', 'https://openalex.org/W2940405045', 'https://openalex.org/W59175527', 'https://openalex.org/W2515336442', 'https://openalex.org/W3015499232', 'https://openalex.org/W2778460379', 'https://openalex.org/W2972910332', 'https://openalex.org/W2516406502', 'https://openalex.org/W2594814449', 'https://openalex.org/W2946200149', 'https://openalex.org/W2124097505', 'https://openalex.org/W2973046048', 'https://openalex.org/W1525613233', 'https://openalex.org/W4385245566', 'https://openalex.org/W120415783', 'https://openalex.org/W29794711']",2020-10-25
https://openalex.org/W3133525064,https://doi.org/10.1109/iscslp49672.2021.9362104,ByteSing: A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders,"This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system based on duration allocated Tacotron-like acoustic models and WaveRNN neural vocoders. Different from the conventional SVS models, the proposed ByteSing employs Tacotron-like encoder-decoder structures as the acoustic models, in which the CBHG models and recurrent neural networks (RNNs) are explored as encoders and decoders respectively. Meanwhile an auxiliary phoneme duration prediction model is utilized to expand the input sequence, which can enhance the model controllable capacity, model stability and tempo prediction accuracy. WaveRNN vocoders are also adopted as neural vocoders to further improve the voice quality of synthesized songs. Both objective and subjective experimental results prove that the SVS method proposed in this paper can produce quite natural, expressive and high-fidelity songs by improving the pitch and spectrogram prediction accuracy and the models using attention mechanism can achieve best performance.","['https://openalex.org/W6745697700', 'https://openalex.org/W6771539914', 'https://openalex.org/W2973046048', 'https://openalex.org/W6763832098', 'https://openalex.org/W6767453231', 'https://openalex.org/W6769685950', 'https://openalex.org/W6772134836', 'https://openalex.org/W6748409065', 'https://openalex.org/W2963300588', 'https://openalex.org/W2921576841', 'https://openalex.org/W2043003570', 'https://openalex.org/W6761725420', 'https://openalex.org/W2515336442', 'https://openalex.org/W2963609956', 'https://openalex.org/W2889244839', 'https://openalex.org/W6601203246', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972910332', 'https://openalex.org/W6638273328', 'https://openalex.org/W6625166683', 'https://openalex.org/W6731370813', 'https://openalex.org/W6769754352', 'https://openalex.org/W2971753973', 'https://openalex.org/W2567070169', 'https://openalex.org/W2766812927', 'https://openalex.org/W960890183', 'https://openalex.org/W3015499232', 'https://openalex.org/W2946200149', 'https://openalex.org/W2519091744', 'https://openalex.org/W2937242376', 'https://openalex.org/W2970730223', 'https://openalex.org/W2963970792', 'https://openalex.org/W4298580827', 'https://openalex.org/W3015922793', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964307104', 'https://openalex.org/W29794711', 'https://openalex.org/W1810943226', 'https://openalex.org/W3096437652', 'https://openalex.org/W2995670387', 'https://openalex.org/W2408435475']",2021-01-24
https://openalex.org/W2516406502,https://doi.org/10.21437/interspeech.2016-872,Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016,"Comunicació presentada al Interspeech 2016, celebrat a San Francisco (Califòrnia, EUA) els dies 8 a 12 de septembre de 2016, i organitzat per la International Speech Communication Association (ISCA).","['https://openalex.org/W1542278309', 'https://openalex.org/W1990908299', 'https://openalex.org/W120043840', 'https://openalex.org/W2296910764', 'https://openalex.org/W1904711963', 'https://openalex.org/W2111284386', 'https://openalex.org/W14365481', 'https://openalex.org/W1491176105', 'https://openalex.org/W2045047031', 'https://openalex.org/W57034108', 'https://openalex.org/W2007463795']",2016-08-29
https://openalex.org/W29794711,https://doi.org/10.21437/interspeech.2006-584,An HMM-based singing voice synthesis system,"Abstract The present paper describes a corpus-based singing voice syn-thesis system based on hidden Markov models (HMMs). Thissystem employs the HMM-based speech synthesis to synthesizesingingvoice. Musical information such aslyrics, tones, durationsis modeled simultaneously in a uniﬁed framework of the context-dependent HMM. It can mimic the voice quality and singing styleof the original singer. Results of a singing voice synthesis exper-iment show that the proposed system can synthesize smooth andnatural-sounding singing voice. Index Terms : singing voice synthesis, HMM, time-lag model. 1. Introduction In recent years, various applications of speech synthesis systemshave been proposed and investigated. Singing voice synthesis isone of the hot topics in this area [1–5]. However, only a fewcorpus-based singing voice synthesis systems which can be con-structed automatically have been proposed.Currently, there are two main paradigms in the corpus-basedspeech synthesis area: sample-based approach and statistical ap-proach. The sample-based approach such as unit selection [6]can synthesize high-quality speech. However, it requires a hugeamountoftrainingdatatorealizevariousvoicecharacteristics. Onthe other hand, the quality of statistical approach such as HMM-basedspeechsynthesis[7]isbuzzybecauseitisbasedonavocod-ingtechnique. However,itissmoothandstable,anditsvoicechar-acteristics can easily be modiﬁed by transforming HMM parame-ters appropriately. For singing voice synthesis, applying the unitselection seems to be difﬁcult because a huge amount of singingspeech which covers vast combinations of contextual factors thataffect singing voice has to be recorded. On the other hand, theHMM-based system can be constructed using a relatively smallamount of training data. From this point of view, the HMM-basedapproach seems to be more suitable for the singing voice synthe-sizer. In the present paper, we apply the HMM-based synthesisapproach to singing voice synthesis.Although the singing voice synthesis system proposed in thepresent paper is quite similar to the HMM-based text-to-speechsynthesissystem[7],therearetwomaindifferencesbetweenthem.In the HMM-based text-to-speech synthesis system, contextualfactors which may affect reading speech (e.g. phonemes, sylla-bles, words, phrases, etc.) are taken into account. However, con-textual factors which may affect singing voice should be different","['https://openalex.org/W1600722501', 'https://openalex.org/W2096801154', 'https://openalex.org/W2154920538', 'https://openalex.org/W1525613233', 'https://openalex.org/W182265564', 'https://openalex.org/W2096980176', 'https://openalex.org/W2150658333', 'https://openalex.org/W2093450784']",2006-09-17
https://openalex.org/W2889244839,https://doi.org/10.21437/interspeech.2018-1575,Korean Singing Voice Synthesis Based on an LSTM Recurrent Neural Network,,[],2018-08-28
https://openalex.org/W2921576841,https://doi.org/10.23919/apsipa.2018.8659797,Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,"This paper describes a singing voice synthesis system based on deep neural networks (DNNs) named Sinsy. Singing voice synthesis systems based on hidden Markov models (HMMs) have grown in the last decade. Recently, singing voice synthesis systems based on DNNs have been proposed. It has improved the naturalness of the synthesized singing voices. In this paper, we introduce several techniques, i.e., trajectory training, a vibrato model, and a time-lag model, into the DNN-based singing voice synthesis system to synthesize the high quality singing voices. Experimental results show that the DNN-based systems with these techniques outperformed the HMM-based systems. In addition, the present paper describes the details of the on-line service for singing voice synthesis.","['https://openalex.org/W2136507169', 'https://openalex.org/W1963627370', 'https://openalex.org/W2166823384', 'https://openalex.org/W2160815625', 'https://openalex.org/W6675380101', 'https://openalex.org/W2134973740', 'https://openalex.org/W2515336442', 'https://openalex.org/W2402539796', 'https://openalex.org/W6674021436', 'https://openalex.org/W2106792148', 'https://openalex.org/W1984905644', 'https://openalex.org/W6601203246', 'https://openalex.org/W2049686551', 'https://openalex.org/W6603264027', 'https://openalex.org/W6713876852', 'https://openalex.org/W2129142580', 'https://openalex.org/W2111284386', 'https://openalex.org/W2778460379', 'https://openalex.org/W2403471241', 'https://openalex.org/W2154920538', 'https://openalex.org/W1979449467', 'https://openalex.org/W29794711', 'https://openalex.org/W2228674556', 'https://openalex.org/W80543058', 'https://openalex.org/W2102003408', 'https://openalex.org/W2591927543', 'https://openalex.org/W2408435475', 'https://openalex.org/W2519091744', 'https://openalex.org/W2949382160', 'https://openalex.org/W1579853615']",2018-11-01
https://openalex.org/W3204116061,https://doi.org/10.1109/aivr52153.2021.00067,A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems,"Singing voice synthesis (SVS) is a task that aims to generate audio signals according to musical scores and lyrics. With its multifaceted nature concerning music and language, producing singing voices indistinguishable from that of human singers has always remained an unfulfilled pursuit. Nonetheless, the advancements of deep learning techniques have brought about a substantial leap in the quality and naturalness of synthesized singing voice. This paper aims to review some of the state-of-the-art deep learning-driven SVS systems. We intend to summarize their deployed model architectures and identify the strengths and limitations for each of the introduced systems. Thereby, we picture the recent advancement trajectory of this field and conclude the challenges left to be resolved both in commercial applications and academic research.","['https://openalex.org/W3198020407', 'https://openalex.org/W2154920538', 'https://openalex.org/W3163031268', 'https://openalex.org/W2739748921', 'https://openalex.org/W6778823374', 'https://openalex.org/W3187128306', 'https://openalex.org/W6678318511', 'https://openalex.org/W29794711', 'https://openalex.org/W2515336442', 'https://openalex.org/W2889244839', 'https://openalex.org/W2940405045', 'https://openalex.org/W6750489868', 'https://openalex.org/W2471520273', 'https://openalex.org/W6757202746', 'https://openalex.org/W6763832098', 'https://openalex.org/W6783382068', 'https://openalex.org/W2102870814', 'https://openalex.org/W6769680054', 'https://openalex.org/W6796684895', 'https://openalex.org/W3015338123', 'https://openalex.org/W3097514409', 'https://openalex.org/W6779823529', 'https://openalex.org/W2778460379', 'https://openalex.org/W6761725420', 'https://openalex.org/W2107860279', 'https://openalex.org/W3133525064', 'https://openalex.org/W6739901393', 'https://openalex.org/W2973046048', 'https://openalex.org/W3015437531', 'https://openalex.org/W2794490148', 'https://openalex.org/W2937242376', 'https://openalex.org/W3130016944', 'https://openalex.org/W2949382160', 'https://openalex.org/W3148333649', 'https://openalex.org/W2519091744', 'https://openalex.org/W3033411150', 'https://openalex.org/W3158762648', 'https://openalex.org/W2970730223', 'https://openalex.org/W3169635929', 'https://openalex.org/W3035430139', 'https://openalex.org/W3082910224', 'https://openalex.org/W3100572490', 'https://openalex.org/W2903032817', 'https://openalex.org/W2963403868', 'https://openalex.org/W2124097505', 'https://openalex.org/W4385245566', 'https://openalex.org/W3112624375', 'https://openalex.org/W2982079758', 'https://openalex.org/W3036167779', 'https://openalex.org/W3190244907', 'https://openalex.org/W2946200149']",2021-11-01
https://openalex.org/W3015437531,https://doi.org/10.1109/icassp40776.2020.9053811,Fast and High-Quality Singing Voice Synthesis System Based on Convolutional Neural Networks,"The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method.","['https://openalex.org/W2790645443', 'https://openalex.org/W2049686551', 'https://openalex.org/W6755592152', 'https://openalex.org/W6743855016', 'https://openalex.org/W6677750764', 'https://openalex.org/W2064675550', 'https://openalex.org/W2154920538', 'https://openalex.org/W1576227399', 'https://openalex.org/W6739959895', 'https://openalex.org/W2921576841', 'https://openalex.org/W6736068285', 'https://openalex.org/W2972910332', 'https://openalex.org/W2964243274', 'https://openalex.org/W2515336442', 'https://openalex.org/W1903029394', 'https://openalex.org/W2134973740', 'https://openalex.org/W6754925833', 'https://openalex.org/W2402539796', 'https://openalex.org/W6748409065', 'https://openalex.org/W6732429163', 'https://openalex.org/W6675380101', 'https://openalex.org/W2890983311', 'https://openalex.org/W2160815625', 'https://openalex.org/W2973046048', 'https://openalex.org/W1502723613', 'https://openalex.org/W6696843773', 'https://openalex.org/W2129142580', 'https://openalex.org/W2403471241', 'https://openalex.org/W6712239235', 'https://openalex.org/W2892140764', 'https://openalex.org/W4298580827', 'https://openalex.org/W2394662942', 'https://openalex.org/W2584032004', 'https://openalex.org/W2294797155', 'https://openalex.org/W2749651610', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964307104', 'https://openalex.org/W2666279857', 'https://openalex.org/W2962883485', 'https://openalex.org/W2119196219', 'https://openalex.org/W2102003408', 'https://openalex.org/W2606052883']",2020-04-09
https://openalex.org/W2030149476,https://doi.org/10.2307/3680822,"Singing Voice Synthesis: History, Current Work, and Future Directions","This article will briefly review the history of singing voice synthesis, and will highlight some currently active projects in this area. It will survey and discuss the benefits and trade-offs of using different techniques and models. Performance control, some attractions of composing with vocal models, and exciting directions for future research will be highlighted.","['https://openalex.org/W2108072369', 'https://openalex.org/W1990030918', 'https://openalex.org/W2097645910', 'https://openalex.org/W1592066919', 'https://openalex.org/W49695251', 'https://openalex.org/W2093824445', 'https://openalex.org/W2088035623', 'https://openalex.org/W2058255766', 'https://openalex.org/W198728215', 'https://openalex.org/W361149621', 'https://openalex.org/W1990161388', 'https://openalex.org/W2402088838', 'https://openalex.org/W2332567314', 'https://openalex.org/W104660599', 'https://openalex.org/W410872943', 'https://openalex.org/W1990346118', 'https://openalex.org/W2401168654', 'https://openalex.org/W2129473705', 'https://openalex.org/W2068946105', 'https://openalex.org/W2037829145', 'https://openalex.org/W2294317068', 'https://openalex.org/W49640926', 'https://openalex.org/W1999885698', 'https://openalex.org/W72060051', 'https://openalex.org/W2088432713', 'https://openalex.org/W2164764235', 'https://openalex.org/W2005085398', 'https://openalex.org/W1581879603', 'https://openalex.org/W2971194844', 'https://openalex.org/W2211429916', 'https://openalex.org/W1559796135', 'https://openalex.org/W1512148200', 'https://openalex.org/W801492903', 'https://openalex.org/W2148371116', 'https://openalex.org/W2005332113', 'https://openalex.org/W2015394094', 'https://openalex.org/W2408685318', 'https://openalex.org/W1516409040', 'https://openalex.org/W1604651291', 'https://openalex.org/W2022554507']",1996-01-01
https://openalex.org/W3024973272,https://doi.org/10.21437/interspeech.2020-1137,Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization,"In a recent paper, we have presented a generative adversarial network\n(GAN)-based model for unconditional generation of the mel-spectrograms of\nsinging voices. As the generator of the model is designed to take a\nvariable-length sequence of noise vectors as input, it can generate\nmel-spectrograms of variable length. However, our previous listening test shows\nthat the quality of the generated audio leaves room for improvement. The\npresent paper extends and expands that previous work in the following aspects.\nFirst, we employ a hierarchical architecture in the generator to induce some\nstructure in the temporal dimension. Second, we introduce a cycle\nregularization mechanism to the generator to avoid mode collapse. Third, we\nevaluate the performance of the new model not only for generating singing\nvoices, but also for generating speech voices. Evaluation result shows that new\nmodel outperforms the prior one both objectively and subjectively. We also\nemploy the model to unconditionally generate sequences of piano and violin\nmusic and find the result promising. Audio examples, as well as the code for\nimplementing our model, will be publicly available online upon paper\npublication.\n","['https://openalex.org/W1522301498', 'https://openalex.org/W4295274059', 'https://openalex.org/W4288107125', 'https://openalex.org/W3125709657', 'https://openalex.org/W2950547518', 'https://openalex.org/W2932319787', 'https://openalex.org/W3020570669', 'https://openalex.org/W2964243274', 'https://openalex.org/W2099471712', 'https://openalex.org/W2973046048', 'https://openalex.org/W2423557781', 'https://openalex.org/W2995005087', 'https://openalex.org/W2962793481', 'https://openalex.org/W2777302760', 'https://openalex.org/W1924770834', 'https://openalex.org/W2963373786', 'https://openalex.org/W2605195953', 'https://openalex.org/W3034794073', 'https://openalex.org/W2970006822', 'https://openalex.org/W2120605154', 'https://openalex.org/W3114301328', 'https://openalex.org/W2811079561', 'https://openalex.org/W2964607787', 'https://openalex.org/W2898148140', 'https://openalex.org/W2962942158', 'https://openalex.org/W4320013936', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963804063', 'https://openalex.org/W3000389243', 'https://openalex.org/W2972706021', 'https://openalex.org/W3005991431', 'https://openalex.org/W2995233853', 'https://openalex.org/W3003673875', 'https://openalex.org/W2899724567', 'https://openalex.org/W2948211236', 'https://openalex.org/W2125389028', 'https://openalex.org/W2963306805', 'https://openalex.org/W3015843452', 'https://openalex.org/W3046715528', 'https://openalex.org/W2267126114', 'https://openalex.org/W2940544976', 'https://openalex.org/W2754229890', 'https://openalex.org/W2962706768', 'https://openalex.org/W2953318193', 'https://openalex.org/W2964199361', 'https://openalex.org/W3035574324', 'https://openalex.org/W2910577860', 'https://openalex.org/W2962981281', 'https://openalex.org/W2972867623']",2020-10-25
https://openalex.org/W2970006822,https://doi.org/10.48550/arxiv.1910.06711,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis,"Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.",[],2019-10-08
https://openalex.org/W2953022181,,Attention-Based Models for Speech Recognition,"Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","['https://openalex.org/W2950178297', 'https://openalex.org/W3127686677', 'https://openalex.org/W2102113734', 'https://openalex.org/W6908809', 'https://openalex.org/W2143612262', 'https://openalex.org/W2951527505', 'https://openalex.org/W1904365287', 'https://openalex.org/W1810943226', 'https://openalex.org/W2108677974', 'https://openalex.org/W3037881859', 'https://openalex.org/W2951008357', 'https://openalex.org/W1586532344', 'https://openalex.org/W1872489089', 'https://openalex.org/W2950527759', 'https://openalex.org/W2949888546', 'https://openalex.org/W1828163288', 'https://openalex.org/W2142416747', 'https://openalex.org/W2160815625', 'https://openalex.org/W2964308564', 'https://openalex.org/W2127141656', 'https://openalex.org/W2064675550', 'https://openalex.org/W2112796928', 'https://openalex.org/W1915251500', 'https://openalex.org/W2113021982']",2015-06-24
https://openalex.org/W3082910224,https://doi.org/10.48550/arxiv.2009.01776,HiFiSinger: Towards High-Fidelity Neural Singing Voice Synthesis,"High-fidelity singing voices usually require higher sampling rate (e.g., 48kHz) to convey expression and emotion. However, higher sampling rate causes the wider frequency band and longer waveform sequences and throws challenges for singing voice synthesis (SVS) in both frequency and time domains. Conventional SVS systems that adopt small sampling rate cannot well address the above challenges. In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voice. HiFiSinger consists of a FastSpeech based acoustic model and a Parallel WaveGAN based vocoder to ensure fast training and inference and also high voice quality. To tackle the difficulty of singing modeling caused by high sampling rate (wider frequency band and longer waveform), we introduce multi-scale adversarial training in both the acoustic model and vocoder to improve singing modeling. Specifically, 1) To handle the larger range of frequencies caused by higher sampling rate, we propose a novel sub-frequency GAN (SF-GAN) on mel-spectrogram generation, which splits the full 80-dimensional mel-frequency into multiple sub-bands and models each sub-band with a separate discriminator. 2) To model longer waveform sequences caused by higher sampling rate, we propose a multi-length GAN (ML-GAN) for waveform generation to model different lengths of waveform sequences with separate discriminators. 3) We also introduce several additional designs and findings in HiFiSinger that are crucial for high-fidelity voices, such as adding F0 (pitch) and V/UV (voiced/unvoiced flag) as acoustic features, choosing an appropriate window/hop size for mel-spectrogram, and increasing the receptive field in vocoder for long vowel modeling. Experiment results show that HiFiSinger synthesizes high-fidelity singing voices with much higher quality: 0.32/0.44 MOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems.","['https://openalex.org/W2591927543', 'https://openalex.org/W2964243274', 'https://openalex.org/W2937242376', 'https://openalex.org/W3015338123', 'https://openalex.org/W3082362645', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963403868', 'https://openalex.org/W2152205330', 'https://openalex.org/W2150658333', 'https://openalex.org/W1904711963', 'https://openalex.org/W2515336442', 'https://openalex.org/W2996286887', 'https://openalex.org/W2940405045', 'https://openalex.org/W2471520273', 'https://openalex.org/W3033411150', 'https://openalex.org/W2052384514', 'https://openalex.org/W2067111814', 'https://openalex.org/W2926840633', 'https://openalex.org/W2593414223', 'https://openalex.org/W2120847449', 'https://openalex.org/W2997493145', 'https://openalex.org/W2962936105', 'https://openalex.org/W3015437531', 'https://openalex.org/W3041199652', 'https://openalex.org/W59175527', 'https://openalex.org/W3019084079', 'https://openalex.org/W3015499232', 'https://openalex.org/W2405614646', 'https://openalex.org/W3035430139', 'https://openalex.org/W120415783', 'https://openalex.org/W2970730223', 'https://openalex.org/W2984106626', 'https://openalex.org/W2949382160', 'https://openalex.org/W2778460379', 'https://openalex.org/W2973046048', 'https://openalex.org/W2994689640']",2020-09-03
https://openalex.org/W3024605872,https://doi.org/10.48550/arxiv.2005.05525,DiscreTalk: Text-to-Speech as a Machine Translation Problem,"This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.","['https://openalex.org/W2963609956', 'https://openalex.org/W2111284386', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964243274', 'https://openalex.org/W3125709657', 'https://openalex.org/W2892140764', 'https://openalex.org/W2963975282', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963620343', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963300588', 'https://openalex.org/W2970206392', 'https://openalex.org/W3034729383', 'https://openalex.org/W2127141656', 'https://openalex.org/W2765486990', 'https://openalex.org/W2888169323', 'https://openalex.org/W2970730223', 'https://openalex.org/W2964060510', 'https://openalex.org/W2977997709', 'https://openalex.org/W2963799213', 'https://openalex.org/W2885185669', 'https://openalex.org/W2749651610', 'https://openalex.org/W2994689640', 'https://openalex.org/W2940544976', 'https://openalex.org/W3015338123', 'https://openalex.org/W72347498', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963691546', 'https://openalex.org/W2129142580', 'https://openalex.org/W2949382160', 'https://openalex.org/W3016160783', 'https://openalex.org/W2884607399']",2020-05-12
https://openalex.org/W3034573343,https://doi.org/10.48550/arxiv.2006.16236,Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,"Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\mathcal{O}\left(N^2\right)$ to $\mathcal{O}\left(N\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",[],2020-06-29
https://openalex.org/W2996287690,,The Curious Case of Neural Text Degeneration,"Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is the best decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text.","['https://openalex.org/W2739046565', 'https://openalex.org/W2099960657', 'https://openalex.org/W2963366196', 'https://openalex.org/W2898658996', 'https://openalex.org/W2963466651', 'https://openalex.org/W2963283805', 'https://openalex.org/W2785896739', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963167310', 'https://openalex.org/W1902237438', 'https://openalex.org/W2042492924', 'https://openalex.org/W2963403868', 'https://openalex.org/W2807747378', 'https://openalex.org/W2964268978', 'https://openalex.org/W2963706817', 'https://openalex.org/W2962788902', 'https://openalex.org/W2964308564', 'https://openalex.org/W2264742718', 'https://openalex.org/W2963506925', 'https://openalex.org/W2048176942', 'https://openalex.org/W2557436004', 'https://openalex.org/W2805486818', 'https://openalex.org/W2892153332', 'https://openalex.org/W2963970792', 'https://openalex.org/W2788277448', 'https://openalex.org/W2402268235', 'https://openalex.org/W2963929190', 'https://openalex.org/W2963595537']",2020-04-30
https://openalex.org/W2408435475,,Recent development of the HMM-based singing voice synthesis system - Sinsy.,,[],2010-01-01
https://openalex.org/W2124097505,,VOCALOID - Commercial singing synthesizer based on sample concatenation,"The song submitted here to the “Synthesis of Singing Challenge ” is synthesized by the latest version of the singing synthesizer “Vocaloid”, which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid. Index Terms: singing synthesis 1.","['https://openalex.org/W1512786422', 'https://openalex.org/W2135934764', 'https://openalex.org/W1661511720', 'https://openalex.org/W1577911745']",2007-01-01
https://openalex.org/W3021164770,https://doi.org/10.48550/arxiv.2005.00341,Jukebox: A Generative Model for Music,"We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox","['https://openalex.org/W2991040477', 'https://openalex.org/W2888169323', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963272440', 'https://openalex.org/W2962990490', 'https://openalex.org/W2963575853', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963975282', 'https://openalex.org/W1512123845', 'https://openalex.org/W2792210438', 'https://openalex.org/W2129142580', 'https://openalex.org/W2963192573', 'https://openalex.org/W2907121943', 'https://openalex.org/W1959608418', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963636093', 'https://openalex.org/W2963090522', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962981281', 'https://openalex.org/W2102870814', 'https://openalex.org/W2401343111', 'https://openalex.org/W2962695743', 'https://openalex.org/W2963047245', 'https://openalex.org/W2962897886', 'https://openalex.org/W2964307104', 'https://openalex.org/W29794711', 'https://openalex.org/W1510436408', 'https://openalex.org/W2150769028', 'https://openalex.org/W2549139847', 'https://openalex.org/W2963799213', 'https://openalex.org/W2320263333', 'https://openalex.org/W2962883485', 'https://openalex.org/W2963568578', 'https://openalex.org/W2951535099', 'https://openalex.org/W2893749619', 'https://openalex.org/W2150658333', 'https://openalex.org/W2963557407', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963403868', 'https://openalex.org/W189596042', 'https://openalex.org/W2116973068', 'https://openalex.org/W2962721334', 'https://openalex.org/W2991108091', 'https://openalex.org/W2940744433', 'https://openalex.org/W2963432880', 'https://openalex.org/W2996037775', 'https://openalex.org/W2922386270', 'https://openalex.org/W2408435475', 'https://openalex.org/W2998108143', 'https://openalex.org/W2949382160', 'https://openalex.org/W1997640156', 'https://openalex.org/W2606176153', 'https://openalex.org/W2964243274', 'https://openalex.org/W2949888546', 'https://openalex.org/W2919624000', 'https://openalex.org/W2973975824', 'https://openalex.org/W2804078698', 'https://openalex.org/W2898148140', 'https://openalex.org/W2964020555', 'https://openalex.org/W2892104732', 'https://openalex.org/W2948211236', 'https://openalex.org/W1999885698', 'https://openalex.org/W2753868141', 'https://openalex.org/W2962942158', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963534259', 'https://openalex.org/W2964281804', 'https://openalex.org/W2587284713', 'https://openalex.org/W2962691331', 'https://openalex.org/W2963300588', 'https://openalex.org/W2992790584', 'https://openalex.org/W2963681776']",2020-04-30
https://openalex.org/W2114347655,https://doi.org/10.1109/tasl.2007.909282,Unsupervised Pattern Discovery in Speech,"We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.","['https://openalex.org/W2111732304', 'https://openalex.org/W2095293504', 'https://openalex.org/W4251670979', 'https://openalex.org/W2086891622', 'https://openalex.org/W2089458547', 'https://openalex.org/W6635336695', 'https://openalex.org/W2099402246', 'https://openalex.org/W3021051756', 'https://openalex.org/W2143235280', 'https://openalex.org/W4237938692', 'https://openalex.org/W2074546930', 'https://openalex.org/W2161952424', 'https://openalex.org/W6678164431', 'https://openalex.org/W2112006116', 'https://openalex.org/W2028903194', 'https://openalex.org/W2114510609', 'https://openalex.org/W1980862600', 'https://openalex.org/W2103447044', 'https://openalex.org/W2121947440', 'https://openalex.org/W2009566340', 'https://openalex.org/W2074231493', 'https://openalex.org/W4245668478', 'https://openalex.org/W2054849588', 'https://openalex.org/W2016243284', 'https://openalex.org/W1610605641', 'https://openalex.org/W2138370049', 'https://openalex.org/W1964917299', 'https://openalex.org/W2140277151', 'https://openalex.org/W2118841860', 'https://openalex.org/W2128160875', 'https://openalex.org/W2164463707', 'https://openalex.org/W2009570821', 'https://openalex.org/W3036063182', 'https://openalex.org/W2122228338', 'https://openalex.org/W2999905431', 'https://openalex.org/W2165874743', 'https://openalex.org/W1589182518', 'https://openalex.org/W2107917162', 'https://openalex.org/W1499245496', 'https://openalex.org/W1483126227', 'https://openalex.org/W1978394996', 'https://openalex.org/W2171009857', 'https://openalex.org/W1530250655', 'https://openalex.org/W1207633162']",2007-12-20
https://openalex.org/W3035725276,https://doi.org/10.1109/tkde.2021.3090866,Self-supervised Learning: Generative or Contrastive,"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.","['https://openalex.org/W2102605133', 'https://openalex.org/W6747899497', 'https://openalex.org/W6715501732', 'https://openalex.org/W343636949', 'https://openalex.org/W6714644935', 'https://openalex.org/W6635084905', 'https://openalex.org/W6637618735', 'https://openalex.org/W6714590955', 'https://openalex.org/W2792234394', 'https://openalex.org/W6765052341', 'https://openalex.org/W2891649471', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962922117', 'https://openalex.org/W2596763562', 'https://openalex.org/W6744957266', 'https://openalex.org/W6771917389', 'https://openalex.org/W2964110616', 'https://openalex.org/W2962904108', 'https://openalex.org/W2108598243', 'https://openalex.org/W6762931180', 'https://openalex.org/W2113896236', 'https://openalex.org/W2963748441', 'https://openalex.org/W6758706709', 'https://openalex.org/W6772452955', 'https://openalex.org/W6729956949', 'https://openalex.org/W6779101013', 'https://openalex.org/W2808856341', 'https://openalex.org/W2998388430', 'https://openalex.org/W6745992979', 'https://openalex.org/W2788919350', 'https://openalex.org/W2326925005', 'https://openalex.org/W2558661413', 'https://openalex.org/W2953356739', 'https://openalex.org/W3099700870', 'https://openalex.org/W3011411500', 'https://openalex.org/W6758354414', 'https://openalex.org/W2963073614', 'https://openalex.org/W2738588019', 'https://openalex.org/W6725739302', 'https://openalex.org/W3012871709', 'https://openalex.org/W3080997787', 'https://openalex.org/W6760212410', 'https://openalex.org/W6741832134', 'https://openalex.org/W6763416564', 'https://openalex.org/W6770949304', 'https://openalex.org/W6604803494', 'https://openalex.org/W2906943923', 'https://openalex.org/W6754278344', 'https://openalex.org/W6690026940', 'https://openalex.org/W6779518175', 'https://openalex.org/W6774222543', 'https://openalex.org/W2194775991', 'https://openalex.org/W6770717842', 'https://openalex.org/W6779326418', 'https://openalex.org/W6791753252', 'https://openalex.org/W6682948231', 'https://openalex.org/W2962756421', 'https://openalex.org/W6739901393', 'https://openalex.org/W2518754566', 'https://openalex.org/W2963470893', 'https://openalex.org/W2949182780', 'https://openalex.org/W2919115771', 'https://openalex.org/W2963169753', 'https://openalex.org/W2599837529', 'https://openalex.org/W1903029394', 'https://openalex.org/W6762963088', 'https://openalex.org/W6780248173', 'https://openalex.org/W2612769033', 'https://openalex.org/W6766673545', 'https://openalex.org/W3035160371', 'https://openalex.org/W6770982027', 'https://openalex.org/W2964060161', 'https://openalex.org/W2798991696', 'https://openalex.org/W6636510571', 'https://openalex.org/W6770825270', 'https://openalex.org/W6783235295', 'https://openalex.org/W6763701032', 'https://openalex.org/W2962852342', 'https://openalex.org/W2962770929', 'https://openalex.org/W2889787757', 'https://openalex.org/W6752910514', 'https://openalex.org/W2963826423', 'https://openalex.org/W6640963894', 'https://openalex.org/W6726873649', 'https://openalex.org/W6751455638', 'https://openalex.org/W6730084236', 'https://openalex.org/W6752306858', 'https://openalex.org/W6768841368', 'https://openalex.org/W6784694379', 'https://openalex.org/W6684191040', 'https://openalex.org/W6779119530', 'https://openalex.org/W6768021236', 'https://openalex.org/W2963261224', 'https://openalex.org/W2952205826', 'https://openalex.org/W2308529009', 'https://openalex.org/W6634441602', 'https://openalex.org/W2966694634', 'https://openalex.org/W2809583854', 'https://openalex.org/W6638304892', 'https://openalex.org/W3036446966', 'https://openalex.org/W1932742904', 'https://openalex.org/W6763846873', 'https://openalex.org/W6771848067', 'https://openalex.org/W2154851992', 'https://openalex.org/W6738394178', 'https://openalex.org/W6774420841', 'https://openalex.org/W6740528845', 'https://openalex.org/W2985951359', 'https://openalex.org/W2607500032', 'https://openalex.org/W2970641574', 'https://openalex.org/W6763813028', 'https://openalex.org/W2998269939', 'https://openalex.org/W6766156693', 'https://openalex.org/W3011574394', 'https://openalex.org/W6685352114', 'https://openalex.org/W2493916176', 'https://openalex.org/W6755312952', 'https://openalex.org/W6746348307', 'https://openalex.org/W2883725317', 'https://openalex.org/W6779997284', 'https://openalex.org/W6774314701', 'https://openalex.org/W6763442200', 'https://openalex.org/W6779977557', 'https://openalex.org/W6784392697', 'https://openalex.org/W2270070752', 'https://openalex.org/W2640408555', 'https://openalex.org/W6771137614', 'https://openalex.org/W6774670964', 'https://openalex.org/W3035164673', 'https://openalex.org/W6777179611', 'https://openalex.org/W6786614245', 'https://openalex.org/W6748582592', 'https://openalex.org/W6761910064', 'https://openalex.org/W6759628261', 'https://openalex.org/W2022322548', 'https://openalex.org/W6682691769', 'https://openalex.org/W1888005072', 'https://openalex.org/W6766489549', 'https://openalex.org/W2963420272', 'https://openalex.org/W2423557781', 'https://openalex.org/W2752796333', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963465221', 'https://openalex.org/W6844194202']",2021-01-01
https://openalex.org/W2394873997,https://doi.org/10.1109/icassp.2016.7472808,Filterbank learning using Convolutional Restricted Boltzmann Machine for speech recognition,"Convolutional Restricted Boltzmann Machine (ConvRBM) as a model for speech signal is presented in this paper. We have developed ConvRBM with sampling from noisy rectified linear units (NReLUs). ConvRBM is trained in an unsupervised way to model speech signal of arbitrary lengths. Weights of the model can represent an auditory-like filterbank. Our proposed learned filterbank is also nonlinear with respect to center frequencies of subband filters similar to standard filterbanks (such as Mel, Bark, ERB, etc.). We have used our proposed model as a front-end to learn features and applied to speech recognition task. Performance of ConvRBM features is improved compared to MFCC with relative improvement of 5% on TIMIT test set and 7% on WSJ0 database for both Nov'92 test sets using GMM-HMM systems. With DNN-HMM systems, we achieved relative improvement of 3% on TIMIT test set over MFCC and Mel filterbank (FBANK). On WSJ0 Nov'92 test sets, we achieved relative improvement of 4-14% using ConvRBM features over MFCC features and 3.6-5.6% using ConvRBM filterbank over FBANK features.","['https://openalex.org/W2130325614', 'https://openalex.org/W6676071220', 'https://openalex.org/W2120077739', 'https://openalex.org/W6713548365', 'https://openalex.org/W2168249605', 'https://openalex.org/W6713197546', 'https://openalex.org/W6712560600', 'https://openalex.org/W1969851134', 'https://openalex.org/W2102512139', 'https://openalex.org/W6637242042', 'https://openalex.org/W2546302380', 'https://openalex.org/W2919115771', 'https://openalex.org/W6631362777', 'https://openalex.org/W2163922914', 'https://openalex.org/W6650649374', 'https://openalex.org/W2084336274', 'https://openalex.org/W2153381621', 'https://openalex.org/W2017399882', 'https://openalex.org/W1507423462', 'https://openalex.org/W2132037657', 'https://openalex.org/W1982109178', 'https://openalex.org/W2116064496', 'https://openalex.org/W1542280630', 'https://openalex.org/W6664918268', 'https://openalex.org/W2132606418', 'https://openalex.org/W2077804127', 'https://openalex.org/W2024490156', 'https://openalex.org/W2398826216', 'https://openalex.org/W2000232298', 'https://openalex.org/W2107789863', 'https://openalex.org/W2401869809', 'https://openalex.org/W1524333225', 'https://openalex.org/W2057498692', 'https://openalex.org/W1635512741', 'https://openalex.org/W1665214252', 'https://openalex.org/W2408093180']",2016-03-01
https://openalex.org/W2756577849,https://doi.org/10.1121/1.5001926,Unsupervised modulation filter learning for noise-robust speech recognition,"The modulation filtering approach to robust automatic speech recognition (ASR) is based on enhancing perceptually relevant regions of the modulation spectrum while suppressing the regions susceptible to noise. In this paper, a data-driven unsupervised modulation filter learning scheme is proposed using convolutional restricted Boltzmann machine. The initial filter is learned using the speech spectrogram while subsequent filters are learned using residual spectrograms. The modulation filtered spectrograms are used for ASR experiments on noisy and reverberant speech where these features provide significant improvements over other robust features. Furthermore, the application of the proposed method for semi-supervised learning is investigated.","['https://openalex.org/W2054139811', 'https://openalex.org/W2161224286', 'https://openalex.org/W1999686891', 'https://openalex.org/W2159373586', 'https://openalex.org/W1541452272', 'https://openalex.org/W2137075158', 'https://openalex.org/W2116064496', 'https://openalex.org/W1555696814', 'https://openalex.org/W2096051479', 'https://openalex.org/W2114719288', 'https://openalex.org/W2242685705', 'https://openalex.org/W1627087495', 'https://openalex.org/W2068359377', 'https://openalex.org/W2130325614', 'https://openalex.org/W2151693816', 'https://openalex.org/W2165720259', 'https://openalex.org/W2123811694', 'https://openalex.org/W617047264', 'https://openalex.org/W2394873997', 'https://openalex.org/W1969851134', 'https://openalex.org/W2099866409', 'https://openalex.org/W2023262923', 'https://openalex.org/W1990934990', 'https://openalex.org/W1974932989', 'https://openalex.org/W154677192', 'https://openalex.org/W2144359569']",2017-09-01
https://openalex.org/W2972984069,https://doi.org/10.21437/interspeech.2019-2652,Unsupervised Raw Waveform Representation Learning for ASR,,"['https://openalex.org/W2400622930', 'https://openalex.org/W1992475611', 'https://openalex.org/W2658929981', 'https://openalex.org/W2137075158', 'https://openalex.org/W2148154194', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963175699', 'https://openalex.org/W2132037657', 'https://openalex.org/W1542280630', 'https://openalex.org/W2114719288', 'https://openalex.org/W2394873997', 'https://openalex.org/W2398826216', 'https://openalex.org/W2289394825']",2019-09-13
https://openalex.org/W2889087444,https://doi.org/10.21437/interspeech.2018-1972,Comparison of Unsupervised Modulation Filter Learning Methods for ASR,,"['https://openalex.org/W1627087495', 'https://openalex.org/W2114719288', 'https://openalex.org/W182885046', 'https://openalex.org/W2116064496', 'https://openalex.org/W2136655611', 'https://openalex.org/W2963073614', 'https://openalex.org/W2130325614', 'https://openalex.org/W2130426352', 'https://openalex.org/W2165720259', 'https://openalex.org/W2113427327', 'https://openalex.org/W1524333225', 'https://openalex.org/W2100495367', 'https://openalex.org/W2167763959']",2018-08-28
https://openalex.org/W2787447541,https://doi.org/10.1109/asru.2017.8269011,Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017,"This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.","['https://openalex.org/W6678947187', 'https://openalex.org/W2001619934', 'https://openalex.org/W2124629003', 'https://openalex.org/W2106554350', 'https://openalex.org/W1599512239', 'https://openalex.org/W2002342963', 'https://openalex.org/W2019042707', 'https://openalex.org/W2078769636', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712553779', 'https://openalex.org/W6713256719', 'https://openalex.org/W2509930204', 'https://openalex.org/W6704305767', 'https://openalex.org/W2963620343', 'https://openalex.org/W6973666849', 'https://openalex.org/W2586754519', 'https://openalex.org/W6631362777', 'https://openalex.org/W2113641473', 'https://openalex.org/W1631260214', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W2404799143', 'https://openalex.org/W2128032727', 'https://openalex.org/W1796128977', 'https://openalex.org/W2786608204', 'https://openalex.org/W1524333225', 'https://openalex.org/W2399576818', 'https://openalex.org/W2345811097']",2017-12-01
https://openalex.org/W2785860501,https://doi.org/10.1109/asru.2017.8269014,Unsupervised HMM posteriograms for language independent acoustic modeling in zero resource conditions,"The task of language independent acoustic unit modeling in unlabeled raw speech (zero-resource setting) has gained significant interest over the recent years. The main challenge here is the extraction of acoustic representations that elicit good similarity between the same words or linguistic tokens spoken by different speakers and to derive these representations in a language independent manner. In this paper, we explore the use of Hidden Markov Model (HMM) based posteriograms for unsupervised acoustic unit modeling. The states of the HMM (which represent the language independent acoustic units) are initialized using a Gaussian mixture model (GMM) - Universal Background Model (UBM). The trained HMM is subsequently used to generate a temporally contiguous state alignment which are then modeled in a hybrid deep neural network (DNN) model. For the purpose of testing, we use the frame level HMM state posteriors obtained from the DNN as features for the ZeroSpeech challenge task. The minimal pair ABX error rate is measured for both the within and across speaker pairs. With several experiments on multiple languages in the ZeroSpeech corpus, we show that the proposed HMM based posterior features provides significant improvements over the baseline system using MFCC features (average relative improvements of 25% for within speaker pairs and 40% for across speaker pairs). Furthermore, the experiments where the target language is not seen training illustrate the proposed modeling approach is capable of learning global language independent representations.","['https://openalex.org/W2335112305', 'https://openalex.org/W6695606915', 'https://openalex.org/W2963620343', 'https://openalex.org/W2078769636', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W6712202099', 'https://openalex.org/W2041823554', 'https://openalex.org/W2127982613', 'https://openalex.org/W6973666849', 'https://openalex.org/W1970890968', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713745070', 'https://openalex.org/W2086115904', 'https://openalex.org/W2095458199', 'https://openalex.org/W6713256719', 'https://openalex.org/W6633431331', 'https://openalex.org/W2115008841', 'https://openalex.org/W6682825348', 'https://openalex.org/W2787223168', 'https://openalex.org/W6631362777', 'https://openalex.org/W1796128977', 'https://openalex.org/W330298975', 'https://openalex.org/W2404799143', 'https://openalex.org/W2786608204', 'https://openalex.org/W1560013842', 'https://openalex.org/W2406349064', 'https://openalex.org/W2286443923', 'https://openalex.org/W2396043527', 'https://openalex.org/W1553004968', 'https://openalex.org/W2152175008', 'https://openalex.org/W1524333225']",2017-12-01
https://openalex.org/W2787223168,https://doi.org/10.1109/asru.2017.8269013,Deep learning methods for unsupervised acoustic modeling — Leap submission to ZeroSpeech challenge 2017,"In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.","['https://openalex.org/W6713256719', 'https://openalex.org/W4231109964', 'https://openalex.org/W6685777803', 'https://openalex.org/W2120480077', 'https://openalex.org/W6682825348', 'https://openalex.org/W2041823554', 'https://openalex.org/W2785860501', 'https://openalex.org/W6973666849', 'https://openalex.org/W2009388533', 'https://openalex.org/W2126203737', 'https://openalex.org/W2963620343', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713745070', 'https://openalex.org/W6601311673', 'https://openalex.org/W2117041980', 'https://openalex.org/W2513125788', 'https://openalex.org/W2115008841', 'https://openalex.org/W6631362777', 'https://openalex.org/W2072128103', 'https://openalex.org/W30845872', 'https://openalex.org/W1553004968', 'https://openalex.org/W222076935', 'https://openalex.org/W2406349064', 'https://openalex.org/W1796128977', 'https://openalex.org/W1524333225', 'https://openalex.org/W1560013842', 'https://openalex.org/W2404799143', 'https://openalex.org/W2786608204', 'https://openalex.org/W2152175008', 'https://openalex.org/W2181347294']",2017-12-01
https://openalex.org/W4385822823,https://doi.org/10.21437/interspeech.2023-1079,How to Estimate Model Transferability of Pre-Trained Speech Models?,"In this work, we introduce a ""score-based assessment"" framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks.We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations.Our framework efficiently computes transferability scores without actual finetuning of candidate models or layers by making a temporal independent hypothesis.We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and selfsupervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data.Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth.Our proposed transferability framework requires less computational time and resources, making it a resource-saving and timeefficient approach for tuning speech foundation models.","['https://openalex.org/W1828163288', 'https://openalex.org/W3015213852', 'https://openalex.org/W4372346241', 'https://openalex.org/W3209059054', 'https://openalex.org/W1494198834', 'https://openalex.org/W3196974791', 'https://openalex.org/W3007522628', 'https://openalex.org/W4292779060', 'https://openalex.org/W4311000453', 'https://openalex.org/W4288089799', 'https://openalex.org/W4385484947', 'https://openalex.org/W4285144981', 'https://openalex.org/W2593116425', 'https://openalex.org/W4375869211', 'https://openalex.org/W3133604157', 'https://openalex.org/W3197580070', 'https://openalex.org/W2982343573', 'https://openalex.org/W4309427288', 'https://openalex.org/W2970112944', 'https://openalex.org/W2797583228', 'https://openalex.org/W4226380987', 'https://openalex.org/W4319862642', 'https://openalex.org/W3154806625', 'https://openalex.org/W4256161595', 'https://openalex.org/W3204696009', 'https://openalex.org/W3112034174', 'https://openalex.org/W3097777922', 'https://openalex.org/W3005343217', 'https://openalex.org/W3042317849', 'https://openalex.org/W4312903743', 'https://openalex.org/W3172443934', 'https://openalex.org/W3215322745', 'https://openalex.org/W2896457183', 'https://openalex.org/W4287198862', 'https://openalex.org/W3186596101', 'https://openalex.org/W2108598243', 'https://openalex.org/W3095410713', 'https://openalex.org/W3195577433', 'https://openalex.org/W2981848390', 'https://openalex.org/W3036601975', 'https://openalex.org/W2187089797', 'https://openalex.org/W4225274946', 'https://openalex.org/W2526050071']",2023-08-14
https://openalex.org/W4385822567,https://doi.org/10.21437/interspeech.2023-854,Label Aware Speech Representation Learning For Language Identification,,[],2023-08-14
https://openalex.org/W2883725317,https://doi.org/10.1007/978-3-030-01264-9_9,Deep Clustering for Unsupervised Learning of Visual Features,,"['https://openalex.org/W1520997877', 'https://openalex.org/W2110798204', 'https://openalex.org/W1955857676', 'https://openalex.org/W2963474899', 'https://openalex.org/W6600007113', 'https://openalex.org/W180242331', 'https://openalex.org/W2108598243', 'https://openalex.org/W343636949', 'https://openalex.org/W2962824366', 'https://openalex.org/W6776948474', 'https://openalex.org/W1893585201', 'https://openalex.org/W1480376833', 'https://openalex.org/W1677182931', 'https://openalex.org/W2139427956', 'https://openalex.org/W2963446712', 'https://openalex.org/W6601052344', 'https://openalex.org/W2086052791', 'https://openalex.org/W2100031962', 'https://openalex.org/W4205743601', 'https://openalex.org/W2308529009', 'https://openalex.org/W2112796928', 'https://openalex.org/W1989684337', 'https://openalex.org/W2136655611', 'https://openalex.org/W2962749380', 'https://openalex.org/W2321533354', 'https://openalex.org/W2750549109', 'https://openalex.org/W2511428026', 'https://openalex.org/W2575671312', 'https://openalex.org/W2963420272', 'https://openalex.org/W2212363941', 'https://openalex.org/W2141362318', 'https://openalex.org/W2148809531', 'https://openalex.org/W2117539524', 'https://openalex.org/W2062118960', 'https://openalex.org/W2121947440', 'https://openalex.org/W2162762921', 'https://openalex.org/W219040644', 'https://openalex.org/W2963749571', 'https://openalex.org/W2113221323', 'https://openalex.org/W2962852342', 'https://openalex.org/W1849277567', 'https://openalex.org/W2326925005', 'https://openalex.org/W2558661413', 'https://openalex.org/W1544092585', 'https://openalex.org/W2613718673', 'https://openalex.org/W2774008708', 'https://openalex.org/W2134670479', 'https://openalex.org/W2949578333', 'https://openalex.org/W2163605009', 'https://openalex.org/W2099471712', 'https://openalex.org/W2095705004', 'https://openalex.org/W2108282816', 'https://openalex.org/W2560977758', 'https://openalex.org/W2964074409', 'https://openalex.org/W2174726731', 'https://openalex.org/W2148349024', 'https://openalex.org/W2113896236', 'https://openalex.org/W1686810756', 'https://openalex.org/W2412782625', 'https://openalex.org/W2145094598', 'https://openalex.org/W2103716973', 'https://openalex.org/W2554692997', 'https://openalex.org/W1625255723', 'https://openalex.org/W2607510315', 'https://openalex.org/W2962877362', 'https://openalex.org/W2949117887']",2018-01-01
https://openalex.org/W2295598076,https://doi.org/10.1145/2939672.2939785,XGBoost,"Tree boosting is a highly effective and widely used machine learning method.\nIn this paper, we describe a scalable end-to-end tree boosting system called\nXGBoost, which is used widely by data scientists to achieve state-of-the-art\nresults on many machine learning challenges. We propose a novel sparsity-aware\nalgorithm for sparse data and weighted quantile sketch for approximate tree\nlearning. More importantly, we provide insights on cache access patterns, data\ncompression and sharding to build a scalable tree boosting system. By combining\nthese insights, XGBoost scales beyond billions of examples using far fewer\nresources than existing systems.\n","['https://openalex.org/W866281365', 'https://openalex.org/W6703949738', 'https://openalex.org/W2911964244', 'https://openalex.org/W6677385034', 'https://openalex.org/W6747597888', 'https://openalex.org/W3001645704', 'https://openalex.org/W1678356000', 'https://openalex.org/W2070493638', 'https://openalex.org/W2024046085', 'https://openalex.org/W6600465285', 'https://openalex.org/W2112452856', 'https://openalex.org/W2076618162', 'https://openalex.org/W2120391124', 'https://openalex.org/W6614148910', 'https://openalex.org/W2125816831', 'https://openalex.org/W2997591727', 'https://openalex.org/W1558918611', 'https://openalex.org/W1987356990', 'https://openalex.org/W2008183828', 'https://openalex.org/W2108214384', 'https://openalex.org/W2149591630']",2016-08-08
https://openalex.org/W2097749765,https://doi.org/10.1109/icdcsw.2011.20,"Finding a ""Kneedle"" in a Haystack: Detecting Knee Points in System Behavior","Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These ""knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.","['https://openalex.org/W6637131181', 'https://openalex.org/W2127258367', 'https://openalex.org/W78747592', 'https://openalex.org/W1570501033', 'https://openalex.org/W6670448442', 'https://openalex.org/W6673800192', 'https://openalex.org/W2169047226', 'https://openalex.org/W6606298606', 'https://openalex.org/W4247796932', 'https://openalex.org/W2146978235', 'https://openalex.org/W2098515641', 'https://openalex.org/W2155952380', 'https://openalex.org/W1999349314', 'https://openalex.org/W4248979535', 'https://openalex.org/W2079212753', 'https://openalex.org/W2092201353', 'https://openalex.org/W2044535354', 'https://openalex.org/W85497486', 'https://openalex.org/W1673310716', 'https://openalex.org/W2163093107', 'https://openalex.org/W1583802019', 'https://openalex.org/W156208468', 'https://openalex.org/W2173213060']",2011-06-01
https://openalex.org/W1973041621,https://doi.org/10.1198/016214503000000666,Finding the Number of Clusters in a Dataset,"One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.","['https://openalex.org/W2071306594', 'https://openalex.org/W4256276256', 'https://openalex.org/W4234300340', 'https://openalex.org/W2047555270', 'https://openalex.org/W2104930019', 'https://openalex.org/W2071949631', 'https://openalex.org/W1481566577', 'https://openalex.org/W1988997815', 'https://openalex.org/W1560089794', 'https://openalex.org/W2001619934', 'https://openalex.org/W1608472120', 'https://openalex.org/W2100736366', 'https://openalex.org/W2913066018', 'https://openalex.org/W2060442802', 'https://openalex.org/W4234966719', 'https://openalex.org/W2085487226', 'https://openalex.org/W1995945562', 'https://openalex.org/W1969557815', 'https://openalex.org/W1634005169', 'https://openalex.org/W2099111195', 'https://openalex.org/W1549664537', 'https://openalex.org/W2124670970', 'https://openalex.org/W2019359195', 'https://openalex.org/W2076580309', 'https://openalex.org/W4200406299', 'https://openalex.org/W2999729612', 'https://openalex.org/W2107903523', 'https://openalex.org/W1975152892', 'https://openalex.org/W2168175751', 'https://openalex.org/W1995875735', 'https://openalex.org/W2090215404', 'https://openalex.org/W2033235577', 'https://openalex.org/W2109820980', 'https://openalex.org/W2098113410', 'https://openalex.org/W2068632127', 'https://openalex.org/W2014078900', 'https://openalex.org/W2058815839', 'https://openalex.org/W2090658088', 'https://openalex.org/W2142901448', 'https://openalex.org/W1965555277']",2003-09-01
https://openalex.org/W3197349023,https://doi.org/10.21437/interspeech.2021-1465,Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw,"We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.","['https://openalex.org/W2142625445', 'https://openalex.org/W4385245566', 'https://openalex.org/W4362220304', 'https://openalex.org/W2053921957', 'https://openalex.org/W2963751529', 'https://openalex.org/W2996728628', 'https://openalex.org/W2252211741', 'https://openalex.org/W2965373594', 'https://openalex.org/W4287591426', 'https://openalex.org/W2995181338', 'https://openalex.org/W2786608204', 'https://openalex.org/W2251803266', 'https://openalex.org/W1494198834', 'https://openalex.org/W2741692265', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W1614298861', 'https://openalex.org/W2128160875']",2021-06-22
https://openalex.org/W1635512741,https://doi.org/10.6028/nist.ir.4930,DARPA TIMIT:,,[],1993-01-01
https://openalex.org/W2766219058,https://doi.org/10.1109/jstsp.2017.2763455,Hybrid CTC/Attention Architecture for End-to-End Speech Recognition,"Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.","['https://openalex.org/W2962826786', 'https://openalex.org/W6679436768', 'https://openalex.org/W6681794868', 'https://openalex.org/W2963447639', 'https://openalex.org/W1526236009', 'https://openalex.org/W6601563604', 'https://openalex.org/W2559260703', 'https://openalex.org/W6639156005', 'https://openalex.org/W2608712415', 'https://openalex.org/W6679434410', 'https://openalex.org/W6727690538', 'https://openalex.org/W2577366047', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963211739', 'https://openalex.org/W6687566353', 'https://openalex.org/W6728910023', 'https://openalex.org/W2526425061', 'https://openalex.org/W4251372957', 'https://openalex.org/W6606787761', 'https://openalex.org/W2515801922', 'https://openalex.org/W4299649720', 'https://openalex.org/W6678720029', 'https://openalex.org/W6631362777', 'https://openalex.org/W6675365184', 'https://openalex.org/W6635078382', 'https://openalex.org/W2160815625', 'https://openalex.org/W1990005915', 'https://openalex.org/W6623517193', 'https://openalex.org/W2739883972', 'https://openalex.org/W811578723', 'https://openalex.org/W6683955732', 'https://openalex.org/W2395416438', 'https://openalex.org/W2514741789', 'https://openalex.org/W2627092829', 'https://openalex.org/W179875071', 'https://openalex.org/W2005708641', 'https://openalex.org/W6728811460', 'https://openalex.org/W2064675550', 'https://openalex.org/W1855892484', 'https://openalex.org/W2163377725', 'https://openalex.org/W1586532344', 'https://openalex.org/W2130942839', 'https://openalex.org/W3145501851', 'https://openalex.org/W2964308564', 'https://openalex.org/W37526647', 'https://openalex.org/W1524333225', 'https://openalex.org/W2545177271', 'https://openalex.org/W2193413348', 'https://openalex.org/W2144499799', 'https://openalex.org/W165283731', 'https://openalex.org/W854541894', 'https://openalex.org/W2952288254', 'https://openalex.org/W2133564696', 'https://openalex.org/W2530486890', 'https://openalex.org/W1553004968', 'https://openalex.org/W2125529971', 'https://openalex.org/W2525778437', 'https://openalex.org/W2102113734', 'https://openalex.org/W2143017621']",2017-10-25
https://openalex.org/W3163793923,https://doi.org/10.1109/icassp39728.2021.9414858,Recent Developments on Espnet Toolkit Boosted By Conformer,"In this study, we present recent developments on ESPnet: End-to- End Speech Processing toolkit, which mainly involves a recently proposed architecture called Conformer, Convolution-augmented Transformer. This paper shows the results for a wide range of end- to-end speech processing applications, such as automatic speech recognition (ASR), speech translations (ST), speech separation (SS) and text-to-speech (TTS). Our experiments reveal various training tips and significant performance benefits obtained with the Conformer on different tasks. These results are competitive or even outperform the current state-of-art Transformer models. We are preparing to release all-in-one recipes using open source and publicly available corpora for all the above tasks with pre-trained models. Our aim for this work is to contribute to our research community by reducing the burden of preparing state-of-the-art research environments usually requiring high resources.","['https://openalex.org/W3015338123', 'https://openalex.org/W2903739847', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963542740', 'https://openalex.org/W6679434410', 'https://openalex.org/W6783267081', 'https://openalex.org/W6731370813', 'https://openalex.org/W6763608318', 'https://openalex.org/W2963250244', 'https://openalex.org/W2127141656', 'https://openalex.org/W2526425061', 'https://openalex.org/W2886180730', 'https://openalex.org/W2963587345', 'https://openalex.org/W6754299077', 'https://openalex.org/W2972389417', 'https://openalex.org/W2964110616', 'https://openalex.org/W2972818416', 'https://openalex.org/W2127851351', 'https://openalex.org/W3008191852', 'https://openalex.org/W2962780374', 'https://openalex.org/W3097777922', 'https://openalex.org/W6755207826', 'https://openalex.org/W3037217258', 'https://openalex.org/W6739901393', 'https://openalex.org/W2734774145', 'https://openalex.org/W6778823374', 'https://openalex.org/W6763832098', 'https://openalex.org/W6631362777', 'https://openalex.org/W3034949308', 'https://openalex.org/W2936774411', 'https://openalex.org/W6713762819', 'https://openalex.org/W2892009249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2133564696', 'https://openalex.org/W3085139254', 'https://openalex.org/W3115011379', 'https://openalex.org/W2896457183', 'https://openalex.org/W2407080277', 'https://openalex.org/W3130016944', 'https://openalex.org/W2963970792', 'https://openalex.org/W3033411150', 'https://openalex.org/W2970730223', 'https://openalex.org/W2964308564', 'https://openalex.org/W2567070169', 'https://openalex.org/W1524333225', 'https://openalex.org/W2948981900', 'https://openalex.org/W4385245566', 'https://openalex.org/W3007328579', 'https://openalex.org/W4287667694', 'https://openalex.org/W2963403868', 'https://openalex.org/W3150572638', 'https://openalex.org/W2946200149']",2021-05-13
https://openalex.org/W2107223151,https://doi.org/10.1109/icassp.2004.1326009,Bootstrap estimates for confidence intervals in ASR performance evaluation,"The field of speech recognition has clearly benefited from precisely defined testing conditions and objective performance measures such as word error rate. In the development and evaluation of new methods, the question arises whether the empirically observed difference in performance is due to a genuine advantage of one system over the other, or just an effect of chance. However, many publications still do not concern themselves with the statistical significance of the results reported. We present a bootstrap method for significance analysis which is, at the same time, intuitive, precise and and easy to use. Unlike some methods, we make no (possibly ill-founded) approximations and the results are immediately interpretable in terms of word error rate.","['https://openalex.org/W6713120608', 'https://openalex.org/W2115094636', 'https://openalex.org/W1932968309', 'https://openalex.org/W2009267849', 'https://openalex.org/W3106889297', 'https://openalex.org/W1995945562', 'https://openalex.org/W2401075988']",2004-09-28
https://openalex.org/W4239510810,https://doi.org/10.1007/bf00994018,Support-vector networks,,"['https://openalex.org/W1526146785', 'https://openalex.org/W1965751196', 'https://openalex.org/W2087347434', 'https://openalex.org/W2168228682', 'https://openalex.org/W1568787085', 'https://openalex.org/W6650842192', 'https://openalex.org/W2154579312', 'https://openalex.org/W1498436455', 'https://openalex.org/W2154642048', 'https://openalex.org/W1994530392', 'https://openalex.org/W2322002063', 'https://openalex.org/W3142915818', 'https://openalex.org/W4300402905', 'https://openalex.org/W4390545247']",1995-09-01
https://openalex.org/W3006926732,https://doi.org/10.21437/interspeech.2020-1242,Towards Learning a Universal Non-Semantic Representation of Speech,"The ultimate goal of transfer learning is to reduce labeled data requirements\nby exploiting a pre-existing embedding model trained for different datasets or\ntasks. The visual and language communities have established benchmarks to\ncompare embeddings, but the speech community has yet to do so. This paper\nproposes a benchmark for comparing speech representations on non-semantic\ntasks, and proposes a representation based on an unsupervised triplet-loss\nobjective. The proposed representation outperforms other representations on the\nbenchmark, and even exceeds state-of-the-art performance on a number of\ntransfer learning tasks. The embedding is trained on a publicly available\ndataset, and it is tested on a variety of low-resource downstream tasks,\nincluding personalization tasks and medical domain. The benchmark, models, and\nevaluation code are publicly released.\n","['https://openalex.org/W4297775537', 'https://openalex.org/W2963300719', 'https://openalex.org/W1608367484', 'https://openalex.org/W2964317695', 'https://openalex.org/W2996158613', 'https://openalex.org/W2944200841', 'https://openalex.org/W2620629206', 'https://openalex.org/W2726515241', 'https://openalex.org/W3099206234', 'https://openalex.org/W3094550259', 'https://openalex.org/W2030931454', 'https://openalex.org/W105569935', 'https://openalex.org/W2973157397', 'https://openalex.org/W2952558884', 'https://openalex.org/W2526050071', 'https://openalex.org/W2962839749', 'https://openalex.org/W2977259558', 'https://openalex.org/W2964013315', 'https://openalex.org/W2101234009', 'https://openalex.org/W2980287048', 'https://openalex.org/W2950007391', 'https://openalex.org/W2797583228', 'https://openalex.org/W2513507089', 'https://openalex.org/W2998249245', 'https://openalex.org/W2984843443', 'https://openalex.org/W2951828005', 'https://openalex.org/W4299518610', 'https://openalex.org/W2972943112', 'https://openalex.org/W2995254904', 'https://openalex.org/W2396589722', 'https://openalex.org/W2612445135', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394655213', 'https://openalex.org/W2953360861', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963194800', 'https://openalex.org/W2772935161', 'https://openalex.org/W3108131700', 'https://openalex.org/W2593116425', 'https://openalex.org/W2085662862', 'https://openalex.org/W2941715400', 'https://openalex.org/W2963087613', 'https://openalex.org/W2148154194', 'https://openalex.org/W2773070064', 'https://openalex.org/W3016011332', 'https://openalex.org/W2887280559', 'https://openalex.org/W2923014074', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963310665', 'https://openalex.org/W2149933564', 'https://openalex.org/W2767754137', 'https://openalex.org/W2994728585']",2020-10-25
https://openalex.org/W2726515241,https://doi.org/10.21437/interspeech.2017-950,VoxCeleb: A Large-Scale Speaker Identification Dataset,"Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.","['https://openalex.org/W2330149154', 'https://openalex.org/W2526050071', 'https://openalex.org/W2949117887', 'https://openalex.org/W2194775991']",2017-08-16
https://openalex.org/W2030931454,https://doi.org/10.1109/taffc.2014.2336244,CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset,"People convey their emotional state in their face and voice. We present an audio-visual data set uniquely suited for the study of multi-modal emotion expression and perception. The data set consists of facial and vocal emotional expressions in sentences spoken in a range of basic emotional states (happy, sad, anger, fear, disgust, and neutral). 7,442 clips of 91 actors with diverse ethnic backgrounds were rated by multiple raters in three modalities: audio, visual, and audio-visual. Categorical emotion labels and real-value intensity values for the perceived emotion were collected using crowd-sourcing from 2,443 raters. The human recognition of intended emotion for the audio-only, visual-only, and audio-visual data are 40.9%, 58.2% and 63.6% respectively. Recognition rates are highest for neutral, followed by happy, anger, disgust, fear, and sad. Average intensity levels of emotion are rated highest for visual-only perception. The accurate recognition of disgust and fear requires simultaneous audio-visual cues, while anger and happiness can be well recognized based on evidence from a single modality. The large dataset we introduce can be used to probe other questions concerning the audio-visual perception of emotion.","['https://openalex.org/W2104084893', 'https://openalex.org/W60557504', 'https://openalex.org/W2115839658', 'https://openalex.org/W2154739180', 'https://openalex.org/W414188911', 'https://openalex.org/W6678622670', 'https://openalex.org/W2146334809', 'https://openalex.org/W2097732741', 'https://openalex.org/W1496540195', 'https://openalex.org/W2110911841', 'https://openalex.org/W2088456207', 'https://openalex.org/W2077030430', 'https://openalex.org/W2136380346', 'https://openalex.org/W1966797434', 'https://openalex.org/W1986101067', 'https://openalex.org/W4245744384', 'https://openalex.org/W6608271823', 'https://openalex.org/W2151008812', 'https://openalex.org/W175750906', 'https://openalex.org/W2120227312', 'https://openalex.org/W2127531292', 'https://openalex.org/W2121222025', 'https://openalex.org/W6686245862', 'https://openalex.org/W2019029324', 'https://openalex.org/W2143350951', 'https://openalex.org/W2043152858', 'https://openalex.org/W1844030040', 'https://openalex.org/W2045528981', 'https://openalex.org/W6729439171', 'https://openalex.org/W2075274068', 'https://openalex.org/W1997537978', 'https://openalex.org/W2125127226', 'https://openalex.org/W2161634108', 'https://openalex.org/W2107497073', 'https://openalex.org/W1965696296', 'https://openalex.org/W2122348661', 'https://openalex.org/W2169294293', 'https://openalex.org/W2062207950', 'https://openalex.org/W2048765566', 'https://openalex.org/W2011604423', 'https://openalex.org/W1973378890', 'https://openalex.org/W6741840311', 'https://openalex.org/W1972280480', 'https://openalex.org/W2292984643', 'https://openalex.org/W2024166834', 'https://openalex.org/W6697452742', 'https://openalex.org/W2122098299', 'https://openalex.org/W1581332777', 'https://openalex.org/W2058647607', 'https://openalex.org/W2020944977', 'https://openalex.org/W125412883', 'https://openalex.org/W2294447324', 'https://openalex.org/W2739071228', 'https://openalex.org/W202968570', 'https://openalex.org/W4230579509', 'https://openalex.org/W2012378416', 'https://openalex.org/W2548474255', 'https://openalex.org/W2774090594', 'https://openalex.org/W2980097029', 'https://openalex.org/W2182392283', 'https://openalex.org/W2125744402', 'https://openalex.org/W2118789253', 'https://openalex.org/W4285719527']",2014-09-25
https://openalex.org/W4297808394,https://doi.org/10.48550/arxiv.1807.03748,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",[],2018-07-10
https://openalex.org/W2029685080,https://doi.org/10.1007/978-1-4612-4378-6,Model Assisted Survey Sampling,,[],1992-01-01
https://openalex.org/W2187089797,,Visualizing Data using t-SNE,"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.","['https://openalex.org/W2072128103', 'https://openalex.org/W101236918', 'https://openalex.org/W2120481923', 'https://openalex.org/W1988464212', 'https://openalex.org/W2156287497', 'https://openalex.org/W2134312057', 'https://openalex.org/W2158307450', 'https://openalex.org/W2167064216', 'https://openalex.org/W2100495367', 'https://openalex.org/W2001141328', 'https://openalex.org/W2065281378', 'https://openalex.org/W2117684310', 'https://openalex.org/W2160208155', 'https://openalex.org/W2137570937', 'https://openalex.org/W2126415191', 'https://openalex.org/W2139823104', 'https://openalex.org/W2156718197', 'https://openalex.org/W1515707356', 'https://openalex.org/W2157444450', 'https://openalex.org/W2053186076', 'https://openalex.org/W1993436046', 'https://openalex.org/W1587720067', 'https://openalex.org/W2107039700', 'https://openalex.org/W2169507824', 'https://openalex.org/W1697082725', 'https://openalex.org/W1742512077', 'https://openalex.org/W2119111481', 'https://openalex.org/W2076137473', 'https://openalex.org/W2017588182', 'https://openalex.org/W2104780023', 'https://openalex.org/W2155161883', 'https://openalex.org/W2122837498', 'https://openalex.org/W2125637308', 'https://openalex.org/W1539175566', 'https://openalex.org/W2071128523']",2008-01-01
https://openalex.org/W2219249508,https://doi.org/10.48550/arxiv.1510.08484,"MUSAN: A Music, Speech, and Noise Corpus","This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.","['https://openalex.org/W1556219185', 'https://openalex.org/W2290689761', 'https://openalex.org/W1980993072', 'https://openalex.org/W1524333225', 'https://openalex.org/W2942177450']",2015-10-28
https://openalex.org/W2520160253,https://doi.org/10.48550/arxiv.1609.03193,Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,"This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.","['https://openalex.org/W2147880316', 'https://openalex.org/W2028706510', 'https://openalex.org/W1902568950', 'https://openalex.org/W2160815625', 'https://openalex.org/W2402146185', 'https://openalex.org/W1736701665', 'https://openalex.org/W2898360541', 'https://openalex.org/W2193413348', 'https://openalex.org/W15592790', 'https://openalex.org/W1666984270', 'https://openalex.org/W2950051476', 'https://openalex.org/W2963175699', 'https://openalex.org/W2949640717', 'https://openalex.org/W38527073', 'https://openalex.org/W1993882792', 'https://openalex.org/W116332155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127141656', 'https://openalex.org/W2143612262', 'https://openalex.org/W2079623482', 'https://openalex.org/W1877570817', 'https://openalex.org/W2398826216', 'https://openalex.org/W2916986993', 'https://openalex.org/W1922655562']",2016-09-11
https://openalex.org/W2979476256,https://doi.org/10.48550/arxiv.1910.05453,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.,"['https://openalex.org/W2786459654', 'https://openalex.org/W3011411500', 'https://openalex.org/W2963807318', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963620343', 'https://openalex.org/W2787560479', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2346964103', 'https://openalex.org/W2547875792', 'https://openalex.org/W2963799213', 'https://openalex.org/W2520160253', 'https://openalex.org/W2920812691', 'https://openalex.org/W2970597249', 'https://openalex.org/W1494198834', 'https://openalex.org/W2965373594', 'https://openalex.org/W2124509324', 'https://openalex.org/W2941814890', 'https://openalex.org/W2896457183', 'https://openalex.org/W2141440284', 'https://openalex.org/W2963403868', 'https://openalex.org/W2842511635', 'https://openalex.org/W2936295285', 'https://openalex.org/W2940180244', 'https://openalex.org/W2153579005', 'https://openalex.org/W2936774411', 'https://openalex.org/W2794209590', 'https://openalex.org/W2518108298', 'https://openalex.org/W2963382687', 'https://openalex.org/W10548402', 'https://openalex.org/W2296701362', 'https://openalex.org/W3127686677', 'https://openalex.org/W2987741655', 'https://openalex.org/W2972943112', 'https://openalex.org/W2962901777', 'https://openalex.org/W2947591107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2951560313', 'https://openalex.org/W2889282842', 'https://openalex.org/W2963425185']",2019-10-12
https://openalex.org/W3015356564,https://doi.org/10.1109/icassp40776.2020.9054224,Effectiveness of Self-Supervised Pre-Training for ASR,"We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] self-supervision approach to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on test-clean, while achieving a 25% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.","['https://openalex.org/W6772641181', 'https://openalex.org/W6775452034', 'https://openalex.org/W2963807318', 'https://openalex.org/W6631362777', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963382687', 'https://openalex.org/W6769557084', 'https://openalex.org/W2953190524', 'https://openalex.org/W2964110616', 'https://openalex.org/W2963425185', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W2932675979', 'https://openalex.org/W6761176036', 'https://openalex.org/W2124558353', 'https://openalex.org/W2667408400', 'https://openalex.org/W6760911985', 'https://openalex.org/W2940322076', 'https://openalex.org/W2586148577', 'https://openalex.org/W2752796333', 'https://openalex.org/W2055408826', 'https://openalex.org/W6761563299', 'https://openalex.org/W30845872', 'https://openalex.org/W2059652594', 'https://openalex.org/W6763416564', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963571336', 'https://openalex.org/W6697456849', 'https://openalex.org/W2114347655', 'https://openalex.org/W2964115348', 'https://openalex.org/W6769196770', 'https://openalex.org/W2964001192', 'https://openalex.org/W2970119519', 'https://openalex.org/W6755207826', 'https://openalex.org/W6771812881', 'https://openalex.org/W1494198834', 'https://openalex.org/W6766673545', 'https://openalex.org/W2944255943', 'https://openalex.org/W2971155163', 'https://openalex.org/W2981857663', 'https://openalex.org/W2996383576', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963799213', 'https://openalex.org/W2948012107', 'https://openalex.org/W2973049979', 'https://openalex.org/W2296681920', 'https://openalex.org/W2963341956', 'https://openalex.org/W3103005696', 'https://openalex.org/W2998649947', 'https://openalex.org/W2995181338', 'https://openalex.org/W2965373594', 'https://openalex.org/W2896457183', 'https://openalex.org/W3015522062', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962907457', 'https://openalex.org/W1524333225', 'https://openalex.org/W4297808394']",2020-04-09
https://openalex.org/W10548402,https://doi.org/10.1097/00008469-199910000-00012,Statistical Theory of Extreme Values and Some Practical Applications : A Series of Lectures,,[],1954-01-01
https://openalex.org/W2888169323,https://doi.org/10.1109/lsp.2018.2880284,Fast Spectrogram Inversion Using Multi-Head Convolutional Neural Networks,"We propose the multi-head convolutional neural network (MCNN) architecture\nfor waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is\nemployed with transposed convolution layers in parallel heads. MCNN achieves\nmore than an order of magnitude higher compute intensity than commonly-used\niterative algorithms like Griffin-Lim, yielding efficient utilization for\nmodern multi-core processors, and very fast (more than 300x real-time) waveform\nsynthesis. For training of MCNN, we use a large-scale speech recognition\ndataset and losses defined on waveforms that are related to perceptual audio\nquality. We demonstrate that MCNN constitutes a very promising approach for\nhigh-quality speech synthesis, without any iterative algorithms or\nautoregression in computations.\n","['https://openalex.org/W6745697700', 'https://openalex.org/W6736356763', 'https://openalex.org/W2953212265', 'https://openalex.org/W1579666790', 'https://openalex.org/W2766465839', 'https://openalex.org/W2963321191', 'https://openalex.org/W6697974390', 'https://openalex.org/W6736723571', 'https://openalex.org/W1494198834', 'https://openalex.org/W2516594951', 'https://openalex.org/W6632769221', 'https://openalex.org/W2769810959', 'https://openalex.org/W6630600482', 'https://openalex.org/W6738277540', 'https://openalex.org/W2964243274', 'https://openalex.org/W2152859600', 'https://openalex.org/W2120847449', 'https://openalex.org/W6631190155', 'https://openalex.org/W2535388113', 'https://openalex.org/W6748588790', 'https://openalex.org/W6734815144', 'https://openalex.org/W2963712897', 'https://openalex.org/W2619368999', 'https://openalex.org/W2963192573', 'https://openalex.org/W2964281804', 'https://openalex.org/W2519091744', 'https://openalex.org/W2608207374', 'https://openalex.org/W2963285578', 'https://openalex.org/W2949382160', 'https://openalex.org/W2304648132', 'https://openalex.org/W2766812927', 'https://openalex.org/W1548018871', 'https://openalex.org/W2777302760', 'https://openalex.org/W2963782041', 'https://openalex.org/W1511414143', 'https://openalex.org/W2606176153', 'https://openalex.org/W1522301498', 'https://openalex.org/W2591927543', 'https://openalex.org/W2964121744', 'https://openalex.org/W2604184139', 'https://openalex.org/W2788357188', 'https://openalex.org/W4294619240', 'https://openalex.org/W2606722458', 'https://openalex.org/W2964084773', 'https://openalex.org/W3103301587']",2018-11-09
https://openalex.org/W3015338123,https://doi.org/10.1109/icassp40776.2020.9053795,Parallel Wavegan: A Fast Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution Spectrogram,"We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high-fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.","['https://openalex.org/W6755879856', 'https://openalex.org/W6753855596', 'https://openalex.org/W2972597685', 'https://openalex.org/W6733471323', 'https://openalex.org/W6755257315', 'https://openalex.org/W6739901393', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W2972333964', 'https://openalex.org/W2593414223', 'https://openalex.org/W2535388113', 'https://openalex.org/W6767164110', 'https://openalex.org/W2964243274', 'https://openalex.org/W2749651610', 'https://openalex.org/W6754850772', 'https://openalex.org/W2963522141', 'https://openalex.org/W2984862052', 'https://openalex.org/W2786868129', 'https://openalex.org/W2751205669', 'https://openalex.org/W6675380101', 'https://openalex.org/W2769810959', 'https://openalex.org/W2972824008', 'https://openalex.org/W2963341071', 'https://openalex.org/W2748379347', 'https://openalex.org/W1996021349', 'https://openalex.org/W2888169323', 'https://openalex.org/W6695676441', 'https://openalex.org/W2963175743', 'https://openalex.org/W2970730223', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963975282', 'https://openalex.org/W2284050935', 'https://openalex.org/W2102003408', 'https://openalex.org/W2099471712', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963782041', 'https://openalex.org/W2994689640', 'https://openalex.org/W2963685250', 'https://openalex.org/W2895976713', 'https://openalex.org/W2949382160', 'https://openalex.org/W3103913581', 'https://openalex.org/W2894295011', 'https://openalex.org/W2519091744', 'https://openalex.org/W2587284713', 'https://openalex.org/W4297817572', 'https://openalex.org/W4320013936', 'https://openalex.org/W2968917279', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963945466', 'https://openalex.org/W4294619240']",2020-04-09
https://openalex.org/W1512123845,https://doi.org/10.7551/mitpress/4360.003.0004,Musical Composition with a High-Speed Digital Computer,,[],1993-01-08
https://openalex.org/W2792210438,https://doi.org/10.48550/arxiv.1803.05428,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the ""posterior collapse"" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a ""flat"" baseline model. An implementation of our ""MusicVAE"" is available online at http://g.co/magenta/musicvae-code.","['https://openalex.org/W2964121744', 'https://openalex.org/W2130942839', 'https://openalex.org/W2115613106', 'https://openalex.org/W2769811909', 'https://openalex.org/W2572816092', 'https://openalex.org/W2963279312', 'https://openalex.org/W2962897886', 'https://openalex.org/W2794719876', 'https://openalex.org/W2963600562', 'https://openalex.org/W2963636093', 'https://openalex.org/W2606176153', 'https://openalex.org/W2099471712', 'https://openalex.org/W2587284713', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963790827', 'https://openalex.org/W2134800885', 'https://openalex.org/W2789776893', 'https://openalex.org/W2962714411', 'https://openalex.org/W2963223306', 'https://openalex.org/W2964167449', 'https://openalex.org/W2170973209', 'https://openalex.org/W2567627528', 'https://openalex.org/W648786980', 'https://openalex.org/W2964076986', 'https://openalex.org/W2064675550', 'https://openalex.org/W2522389179', 'https://openalex.org/W2131774270', 'https://openalex.org/W1959608418', 'https://openalex.org/W2574842964', 'https://openalex.org/W2153579005', 'https://openalex.org/W2951575317', 'https://openalex.org/W2475687244', 'https://openalex.org/W2606712314', 'https://openalex.org/W2766527293', 'https://openalex.org/W592244745', 'https://openalex.org/W2951535099', 'https://openalex.org/W2753738274', 'https://openalex.org/W2950752421', 'https://openalex.org/W2949382160', 'https://openalex.org/W2772689190', 'https://openalex.org/W2766227112', 'https://openalex.org/W2964308564']",2018-03-13
https://openalex.org/W2129142580,https://doi.org/10.1016/j.specom.2009.04.004,Statistical parametric speech synthesis,,"['https://openalex.org/W155946340', 'https://openalex.org/W1594223010', 'https://openalex.org/W2142635246', 'https://openalex.org/W234770729', 'https://openalex.org/W2109772895', 'https://openalex.org/W4302557958', 'https://openalex.org/W197345401', 'https://openalex.org/W2115979064', 'https://openalex.org/W58567859', 'https://openalex.org/W4395452434', 'https://openalex.org/W2033010331', 'https://openalex.org/W2051347452', 'https://openalex.org/W6729288257', 'https://openalex.org/W153299664', 'https://openalex.org/W202879582', 'https://openalex.org/W1574964715', 'https://openalex.org/W1599623585', 'https://openalex.org/W6600857238', 'https://openalex.org/W2404901654', 'https://openalex.org/W3152208832', 'https://openalex.org/W2147215240', 'https://openalex.org/W60443401', 'https://openalex.org/W2116046013', 'https://openalex.org/W4298786326', 'https://openalex.org/W2049633694', 'https://openalex.org/W1983628629', 'https://openalex.org/W2027915610', 'https://openalex.org/W1846837356', 'https://openalex.org/W75668230', 'https://openalex.org/W64730254', 'https://openalex.org/W2149572519', 'https://openalex.org/W202137502', 'https://openalex.org/W2161621855', 'https://openalex.org/W2401838240', 'https://openalex.org/W1650326136', 'https://openalex.org/W2151323943', 'https://openalex.org/W2127846433', 'https://openalex.org/W2166450756', 'https://openalex.org/W1493898446', 'https://openalex.org/W2093450784', 'https://openalex.org/W31925794', 'https://openalex.org/W2002342963', 'https://openalex.org/W2106554350', 'https://openalex.org/W2069631319', 'https://openalex.org/W199136187', 'https://openalex.org/W2100969003', 'https://openalex.org/W2167845555', 'https://openalex.org/W2397716941', 'https://openalex.org/W6605116510', 'https://openalex.org/W2403351921', 'https://openalex.org/W1981450667', 'https://openalex.org/W2034829178', 'https://openalex.org/W7037729874', 'https://openalex.org/W2140918324', 'https://openalex.org/W4302326871', 'https://openalex.org/W2150658333', 'https://openalex.org/W2145892079', 'https://openalex.org/W63250186', 'https://openalex.org/W2915907762', 'https://openalex.org/W2137089646', 'https://openalex.org/W2033516035', 'https://openalex.org/W2091738194', 'https://openalex.org/W2158289097', 'https://openalex.org/W1538228347', 'https://openalex.org/W4395681711', 'https://openalex.org/W1533051419', 'https://openalex.org/W164845301', 'https://openalex.org/W2049686551', 'https://openalex.org/W2531982967', 'https://openalex.org/W2394921947', 'https://openalex.org/W2155498861', 'https://openalex.org/W2150906086', 'https://openalex.org/W2117432245', 'https://openalex.org/W1593598571', 'https://openalex.org/W178225857', 'https://openalex.org/W6635111982', 'https://openalex.org/W6798679566', 'https://openalex.org/W2162070676', 'https://openalex.org/W1652730007', 'https://openalex.org/W2151416040', 'https://openalex.org/W2165108269', 'https://openalex.org/W113106864', 'https://openalex.org/W1995565802', 'https://openalex.org/W1982794085', 'https://openalex.org/W2146871184', 'https://openalex.org/W2064218608', 'https://openalex.org/W2123140629', 'https://openalex.org/W2613407020', 'https://openalex.org/W1973766695', 'https://openalex.org/W2131958409', 'https://openalex.org/W2108674328', 'https://openalex.org/W2394761397', 'https://openalex.org/W1543281296', 'https://openalex.org/W2168072248', 'https://openalex.org/W2064148454', 'https://openalex.org/W167371124', 'https://openalex.org/W1514941256', 'https://openalex.org/W2157273572', 'https://openalex.org/W1589634022', 'https://openalex.org/W2135797457', 'https://openalex.org/W2096555739', 'https://openalex.org/W75704375', 'https://openalex.org/W2428180336', 'https://openalex.org/W2155077107', 'https://openalex.org/W2113146449', 'https://openalex.org/W2130237282', 'https://openalex.org/W172929083', 'https://openalex.org/W2039800941', 'https://openalex.org/W2042600255', 'https://openalex.org/W300119222', 'https://openalex.org/W2082388554', 'https://openalex.org/W2126415164', 'https://openalex.org/W2171002058', 'https://openalex.org/W1669179077', 'https://openalex.org/W4239162268', 'https://openalex.org/W1513877108', 'https://openalex.org/W2107201284', 'https://openalex.org/W4362787', 'https://openalex.org/W2147868561', 'https://openalex.org/W330074099', 'https://openalex.org/W2163246542', 'https://openalex.org/W163616957', 'https://openalex.org/W2118825896', 'https://openalex.org/W6713420752', 'https://openalex.org/W59130454', 'https://openalex.org/W2159283619', 'https://openalex.org/W205224898', 'https://openalex.org/W2105080323', 'https://openalex.org/W1608375737', 'https://openalex.org/W3629425', 'https://openalex.org/W2168175751', 'https://openalex.org/W2402788381', 'https://openalex.org/W80543058', 'https://openalex.org/W197855092', 'https://openalex.org/W2149175990', 'https://openalex.org/W1963627370', 'https://openalex.org/W115463628', 'https://openalex.org/W2533081766', 'https://openalex.org/W2107541077', 'https://openalex.org/W2156142001', 'https://openalex.org/W2120514051', 'https://openalex.org/W2087110403', 'https://openalex.org/W2048973497', 'https://openalex.org/W2160379050', 'https://openalex.org/W1572419603', 'https://openalex.org/W1861150963', 'https://openalex.org/W2136282978', 'https://openalex.org/W96779174', 'https://openalex.org/W1543527380', 'https://openalex.org/W185519286', 'https://openalex.org/W2000513720', 'https://openalex.org/W2133035145', 'https://openalex.org/W2143490509', 'https://openalex.org/W2395052932', 'https://openalex.org/W2120605154', 'https://openalex.org/W70888257', 'https://openalex.org/W1935012542', 'https://openalex.org/W200094172', 'https://openalex.org/W6630838124', 'https://openalex.org/W344150399', 'https://openalex.org/W2142993035', 'https://openalex.org/W2168531172', 'https://openalex.org/W2151257151', 'https://openalex.org/W2167270514', 'https://openalex.org/W122077080', 'https://openalex.org/W233059834', 'https://openalex.org/W2164534744', 'https://openalex.org/W7056901721', 'https://openalex.org/W2111194146', 'https://openalex.org/W128185859', 'https://openalex.org/W2139751012', 'https://openalex.org/W2156453709', 'https://openalex.org/W6712582474', 'https://openalex.org/W1984905644', 'https://openalex.org/W1592103878', 'https://openalex.org/W2150000736', 'https://openalex.org/W1564152904', 'https://openalex.org/W2153914468', 'https://openalex.org/W2188951148', 'https://openalex.org/W2228674556', 'https://openalex.org/W133559434', 'https://openalex.org/W1600722501', 'https://openalex.org/W2010602269', 'https://openalex.org/W2116807854', 'https://openalex.org/W2103279870', 'https://openalex.org/W101758637', 'https://openalex.org/W167940514', 'https://openalex.org/W1563645159', 'https://openalex.org/W1979449467', 'https://openalex.org/W23255068', 'https://openalex.org/W2042691334', 'https://openalex.org/W2106792148', 'https://openalex.org/W79241043', 'https://openalex.org/W6680399173', 'https://openalex.org/W583042331', 'https://openalex.org/W4285719527', 'https://openalex.org/W58133226', 'https://openalex.org/W2077737920', 'https://openalex.org/W1854592333', 'https://openalex.org/W1512429158', 'https://openalex.org/W795690926', 'https://openalex.org/W1756939916', 'https://openalex.org/W2155433314', 'https://openalex.org/W2154920538', 'https://openalex.org/W2277221977', 'https://openalex.org/W1514737389', 'https://openalex.org/W2542677699', 'https://openalex.org/W1550297032', 'https://openalex.org/W2971194844', 'https://openalex.org/W2286166914', 'https://openalex.org/W2145575463', 'https://openalex.org/W2095437083', 'https://openalex.org/W2146927751', 'https://openalex.org/W2096980176', 'https://openalex.org/W3129711340', 'https://openalex.org/W2406017614', 'https://openalex.org/W2165856656', 'https://openalex.org/W1584341804', 'https://openalex.org/W2159528802', 'https://openalex.org/W2400063444', 'https://openalex.org/W1599512239', 'https://openalex.org/W1847897332', 'https://openalex.org/W2342418827', 'https://openalex.org/W39968598', 'https://openalex.org/W2376932199', 'https://openalex.org/W21123279', 'https://openalex.org/W2403168145', 'https://openalex.org/W3150439909', 'https://openalex.org/W2119929864', 'https://openalex.org/W2536114199', 'https://openalex.org/W175280642', 'https://openalex.org/W3177989406']",2009-04-20
https://openalex.org/W2963192573,,Neural Voice Cloning with a Few Samples,"Voice cloning is a highly desired feature for personalized speech interfaces. We introduce a neural voice cloning system that learns to synthesize a person's voice from only a few audio samples. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model. Speaker encoding is based on training a separate model to directly infer a new speaker embedding, which will be applied to a multi-speaker generative model. In terms of naturalness of the speech and similarity to the original speaker, both approaches can achieve good performance, even with a few cloning audios. While speaker adaptation can achieve slightly better naturalness and similarity, cloning time and required memory for the speaker encoding approach are significantly less, making it more favorable for low-resource deployment.",[],2018-02-14
https://openalex.org/W2907121943,https://doi.org/10.48550/arxiv.1901.09321,Fixup Initialization: Residual Learning Without Normalization,"Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.","['https://openalex.org/W2178237821', 'https://openalex.org/W2753358588', 'https://openalex.org/W2896060389', 'https://openalex.org/W2170319235', 'https://openalex.org/W2765407302', 'https://openalex.org/W2133319764', 'https://openalex.org/W1677182931', 'https://openalex.org/W2746314669', 'https://openalex.org/W2903105043', 'https://openalex.org/W2777843033', 'https://openalex.org/W2194775991', 'https://openalex.org/W2793416812', 'https://openalex.org/W2605287558', 'https://openalex.org/W2964065616', 'https://openalex.org/W2952574409', 'https://openalex.org/W2284050935', 'https://openalex.org/W2626778328', 'https://openalex.org/W2952626150', 'https://openalex.org/W2502312327', 'https://openalex.org/W2793904650', 'https://openalex.org/W2949117887', 'https://openalex.org/W2401231614', 'https://openalex.org/W2125930537', 'https://openalex.org/W2898211994', 'https://openalex.org/W2963919294', 'https://openalex.org/W2806311723', 'https://openalex.org/W2859444450', 'https://openalex.org/W2622263826', 'https://openalex.org/W2626017178']",2019-01-27
https://openalex.org/W2766812927,,Deep Voice 3: 2000-Speaker Neural Text-to-Speech,"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.","['https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2507771204', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963970792', 'https://openalex.org/W2049686551', 'https://openalex.org/W2591927543', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2605141709', 'https://openalex.org/W1563460361', 'https://openalex.org/W1570629387', 'https://openalex.org/W2165143604']",2017-10-20
https://openalex.org/W2963636093,https://doi.org/10.48550/arxiv.1606.05328,Conditional Image Generation with PixelCNN Decoders,"This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.","['https://openalex.org/W2148464528', 'https://openalex.org/W2271840356', 'https://openalex.org/W2962897886', 'https://openalex.org/W1771459135', 'https://openalex.org/W1026270304', 'https://openalex.org/W2153125595', 'https://openalex.org/W2949999304', 'https://openalex.org/W115742922', 'https://openalex.org/W2173051530', 'https://openalex.org/W2097039814', 'https://openalex.org/W1583912456', 'https://openalex.org/W2962841471', 'https://openalex.org/W3099206234', 'https://openalex.org/W1924619199', 'https://openalex.org/W2418784495', 'https://openalex.org/W2136655611', 'https://openalex.org/W2963276097', 'https://openalex.org/W2097268041', 'https://openalex.org/W2135181320', 'https://openalex.org/W2963147844', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963857374', 'https://openalex.org/W2962990490', 'https://openalex.org/W2898422183', 'https://openalex.org/W2962736171', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962741254', 'https://openalex.org/W648143168', 'https://openalex.org/W2949247522', 'https://openalex.org/W2963143316', 'https://openalex.org/W2170942820', 'https://openalex.org/W2953318193']",2016-06-16
https://openalex.org/W2963090522,https://doi.org/10.48550/arxiv.1505.05770,Variational Inference with Normalizing Flows,"The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.","['https://openalex.org/W2137945003', 'https://openalex.org/W2122262818', 'https://openalex.org/W2962897886', 'https://openalex.org/W1555695166', 'https://openalex.org/W2170111110', 'https://openalex.org/W2403726878', 'https://openalex.org/W2104067967', 'https://openalex.org/W2098694378', 'https://openalex.org/W1534417711', 'https://openalex.org/W1516111018', 'https://openalex.org/W1506806321', 'https://openalex.org/W2167433878', 'https://openalex.org/W2963319345', 'https://openalex.org/W1959608418', 'https://openalex.org/W2166851633', 'https://openalex.org/W2146502635', 'https://openalex.org/W2161340280', 'https://openalex.org/W2963173382', 'https://openalex.org/W2949416428', 'https://openalex.org/W2150807068', 'https://openalex.org/W1729198099', 'https://openalex.org/W2119717200', 'https://openalex.org/W2099450296', 'https://openalex.org/W2478027467', 'https://openalex.org/W2108501770', 'https://openalex.org/W1583912456', 'https://openalex.org/W2120633557', 'https://openalex.org/W2070792261', 'https://openalex.org/W2097268041', 'https://openalex.org/W2480024362', 'https://openalex.org/W1906598733', 'https://openalex.org/W2164613257', 'https://openalex.org/W1663973292', 'https://openalex.org/W1494871301']",2015-05-21
https://openalex.org/W2962981281,,GANSynth: Adversarial Neural Audio Synthesis,"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.",[],2019-02-23
https://openalex.org/W2102870814,https://doi.org/10.1109/msp.2007.323266,Synthesis of the Singing Voice by Performance Sampling and Spectral Models,"This paper introduces the concept of synthesis based on performance sampling. It explains that although sampling has been considered a way to capture and reproduce the sound of an instrument, it should be better considered a way to model the sonic space produced by a performer with an instrument. The paper presents a singing voice synthesizer, pointing out the main issues and complexities emerging along its design. Although the current system is able to generate convincing results in certain situations, there is still much room for improvements, especially in the areas of expression, spectral modeling and sonic space design. However, computer singing is definitely coming close to becoming indistinguishable from human performances.","['https://openalex.org/W2536114199', 'https://openalex.org/W6631917662', 'https://openalex.org/W1990030918', 'https://openalex.org/W4298331807', 'https://openalex.org/W2060732486', 'https://openalex.org/W2164498268', 'https://openalex.org/W2327857499', 'https://openalex.org/W2053708742', 'https://openalex.org/W2315896104', 'https://openalex.org/W1483221175', 'https://openalex.org/W2167258965', 'https://openalex.org/W1992463540', 'https://openalex.org/W2128045489', 'https://openalex.org/W2096801154', 'https://openalex.org/W2066452495', 'https://openalex.org/W7071538516', 'https://openalex.org/W6604037343', 'https://openalex.org/W2097143083', 'https://openalex.org/W6844210595', 'https://openalex.org/W6631239400', 'https://openalex.org/W2030149476', 'https://openalex.org/W2106246418', 'https://openalex.org/W2074310426', 'https://openalex.org/W2115143544', 'https://openalex.org/W2171015304', 'https://openalex.org/W6631489346', 'https://openalex.org/W6791802631', 'https://openalex.org/W6604793353', 'https://openalex.org/W2159524087', 'https://openalex.org/W6630833833', 'https://openalex.org/W3135766948', 'https://openalex.org/W4299030323', 'https://openalex.org/W2971194844', 'https://openalex.org/W1512786422', 'https://openalex.org/W1528256235', 'https://openalex.org/W1524247923', 'https://openalex.org/W2271680855', 'https://openalex.org/W1527758523', 'https://openalex.org/W100878505', 'https://openalex.org/W1581879603', 'https://openalex.org/W2944297474', 'https://openalex.org/W1521278492', 'https://openalex.org/W1536162321', 'https://openalex.org/W974068700', 'https://openalex.org/W2054159638', 'https://openalex.org/W116500682', 'https://openalex.org/W1577911745', 'https://openalex.org/W1569542462']",2007-03-01
https://openalex.org/W2401343111,,The Musical Universe of Cellular Automata,,[],1989-01-01
https://openalex.org/W2150769028,https://doi.org/10.1109/tasl.2010.2064307,Front-End Factor Analysis for Speaker Verification,"This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.","['https://openalex.org/W2170065313', 'https://openalex.org/W2121415728', 'https://openalex.org/W2156909104', 'https://openalex.org/W6712325649', 'https://openalex.org/W2129785530', 'https://openalex.org/W6712660717', 'https://openalex.org/W2041823554', 'https://openalex.org/W2136879537', 'https://openalex.org/W2144760012', 'https://openalex.org/W6640010188', 'https://openalex.org/W2130955819', 'https://openalex.org/W2112582577', 'https://openalex.org/W2107638917', 'https://openalex.org/W111477576', 'https://openalex.org/W1916834241', 'https://openalex.org/W740415', 'https://openalex.org/W2397634864', 'https://openalex.org/W2398362606']",2010-08-10
https://openalex.org/W2549139847,https://doi.org/10.1109/cvpr.2017.634,Aggregated Residual Transformations for Deep Neural Networks,"We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.","['https://openalex.org/W2183341477', 'https://openalex.org/W2097117768', 'https://openalex.org/W2117539524', 'https://openalex.org/W6620707391', 'https://openalex.org/W6622964371', 'https://openalex.org/W6694260854', 'https://openalex.org/W6637373629', 'https://openalex.org/W6629368666', 'https://openalex.org/W2102605133', 'https://openalex.org/W1677182931', 'https://openalex.org/W2194775991', 'https://openalex.org/W6698183232', 'https://openalex.org/W6638667902', 'https://openalex.org/W1996901117', 'https://openalex.org/W1903029394', 'https://openalex.org/W6639102338', 'https://openalex.org/W6632581263', 'https://openalex.org/W6684563725', 'https://openalex.org/W2151103935', 'https://openalex.org/W2161969291', 'https://openalex.org/W6682778277', 'https://openalex.org/W1536680647', 'https://openalex.org/W2288122362', 'https://openalex.org/W6685627644', 'https://openalex.org/W6684191040', 'https://openalex.org/W6787972765', 'https://openalex.org/W6688059459', 'https://openalex.org/W6638444622', 'https://openalex.org/W2964137095', 'https://openalex.org/W2147800946', 'https://openalex.org/W3118608800', 'https://openalex.org/W639708223', 'https://openalex.org/W2519091744', 'https://openalex.org/W2302255633', 'https://openalex.org/W1861492603', 'https://openalex.org/W2400429454', 'https://openalex.org/W2950094539', 'https://openalex.org/W1686810756', 'https://openalex.org/W2950248853', 'https://openalex.org/W2274287116', 'https://openalex.org/W4297791040', 'https://openalex.org/W1548328233', 'https://openalex.org/W2519224033', 'https://openalex.org/W2155893237', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962988160', 'https://openalex.org/W4294375521', 'https://openalex.org/W2613718673', 'https://openalex.org/W2167215970', 'https://openalex.org/W2155541015', 'https://openalex.org/W2963911037', 'https://openalex.org/W1549358575', 'https://openalex.org/W2963025229', 'https://openalex.org/W2963410064', 'https://openalex.org/W2964350391', 'https://openalex.org/W4301368689', 'https://openalex.org/W2525778437', 'https://openalex.org/W2163605009', 'https://openalex.org/W809122546', 'https://openalex.org/W2963542991', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963150697', 'https://openalex.org/W2206858481', 'https://openalex.org/W1836465849', 'https://openalex.org/W2413904250']",2017-07-01
https://openalex.org/W2320263333,https://doi.org/10.2307/3679940,Nonlinear Maps as Generators of Musical Design,,[],1988-01-01
https://openalex.org/W2951535099,,Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders,"Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.","['https://openalex.org/W2099057450', 'https://openalex.org/W2953318193', 'https://openalex.org/W2949899814', 'https://openalex.org/W2584032004', 'https://openalex.org/W2475687244', 'https://openalex.org/W2949382160', 'https://openalex.org/W2108598243', 'https://openalex.org/W1502260477', 'https://openalex.org/W2293272996', 'https://openalex.org/W2559688696', 'https://openalex.org/W2120847449', 'https://openalex.org/W2560512785', 'https://openalex.org/W2335728318', 'https://openalex.org/W2099471712', 'https://openalex.org/W2076608692', 'https://openalex.org/W1878791236', 'https://openalex.org/W2949117887', 'https://openalex.org/W1522301498', 'https://openalex.org/W3118608800', 'https://openalex.org/W2432004435', 'https://openalex.org/W2951004968', 'https://openalex.org/W1566660863']",2017-04-05
https://openalex.org/W2893749619,https://doi.org/10.48550/arxiv.1809.11096,Large Scale GAN Training for High Fidelity Natural Image Synthesis,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",[],2018-09-28
https://openalex.org/W2150658333,https://doi.org/10.1109/icassp.1996.541110,Unit selection in a concatenative speech synthesis system using a large speech database,"One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.","['https://openalex.org/W2428180336', 'https://openalex.org/W2014195737', 'https://openalex.org/W2125838338', 'https://openalex.org/W1572730534', 'https://openalex.org/W1544407768']",2002-12-24
https://openalex.org/W2963557407,https://doi.org/10.5281/zenodo.1415990,Midinet: A Convolutional Generative Adversarial Network For Symbolic-Domain Music Generation.,[TODO] Add abstract here.,"['https://openalex.org/W1994629163', 'https://openalex.org/W2074308058', 'https://openalex.org/W2137619888', 'https://openalex.org/W3131643527', 'https://openalex.org/W625810977', 'https://openalex.org/W2432004435', 'https://openalex.org/W2067621398']",2017-10-23
https://openalex.org/W189596042,,Deep Boltzmann machines,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training ” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks. 1","['https://openalex.org/W2613634265', 'https://openalex.org/W1813659000', 'https://openalex.org/W2096192494', 'https://openalex.org/W2116064496', 'https://openalex.org/W2110361616', 'https://openalex.org/W2083380015', 'https://openalex.org/W2157629899', 'https://openalex.org/W2159737176', 'https://openalex.org/W2615766368', 'https://openalex.org/W2100495367', 'https://openalex.org/W2122565151', 'https://openalex.org/W2149845449', 'https://openalex.org/W1994616650', 'https://openalex.org/W2102409316', 'https://openalex.org/W2116825644', 'https://openalex.org/W1513873506', 'https://openalex.org/W2136922672', 'https://openalex.org/W1990838964', 'https://openalex.org/W2124914669', 'https://openalex.org/W2134557905', 'https://openalex.org/W2567948266']",2009-04-15
https://openalex.org/W2116973068,https://doi.org/10.1145/1978802.1978809,Constraint programming systems for modeling music theories and composition,"Constraint programming is well suited for the computational modeling of music theories and composition: its declarative and modular approach shares similarities with the way music theory is traditionally expressed, namely by a set of rules which describe the intended result. Various music theory disciplines have been modeled, including counterpoint, harmony, rhythm, form, and instrumentation. Because modeling music theories “from scratch” is a complex task, generic music constraint programming systems have been proposed that predefine the required building blocks for modeling a range of music theories. After introducing the field and its problems in general, this survey compares these generic systems according to a number of criteria such as the range of music theories these systems support.","['https://openalex.org/W206348627', 'https://openalex.org/W600103004', 'https://openalex.org/W2124716399', 'https://openalex.org/W2033074620', 'https://openalex.org/W1733489975', 'https://openalex.org/W2032041077', 'https://openalex.org/W2022236864', 'https://openalex.org/W2121623862', 'https://openalex.org/W1487478819', 'https://openalex.org/W1966902936', 'https://openalex.org/W2058150069', 'https://openalex.org/W1980754243', 'https://openalex.org/W4293282487', 'https://openalex.org/W2144805983', 'https://openalex.org/W1515441675', 'https://openalex.org/W4212899374', 'https://openalex.org/W1512123845', 'https://openalex.org/W2086267322', 'https://openalex.org/W1588134392', 'https://openalex.org/W2053117068', 'https://openalex.org/W1579516283', 'https://openalex.org/W1964007532', 'https://openalex.org/W6366331', 'https://openalex.org/W1480435719', 'https://openalex.org/W2039105019', 'https://openalex.org/W2153628411', 'https://openalex.org/W4390009472', 'https://openalex.org/W2795193196', 'https://openalex.org/W6676823770', 'https://openalex.org/W2141619189', 'https://openalex.org/W2127908756', 'https://openalex.org/W2035039580', 'https://openalex.org/W1994697606', 'https://openalex.org/W2079158925', 'https://openalex.org/W1503382435', 'https://openalex.org/W2321233547', 'https://openalex.org/W2156037025', 'https://openalex.org/W2010384749', 'https://openalex.org/W1496620702', 'https://openalex.org/W2944297474', 'https://openalex.org/W2143082793', 'https://openalex.org/W1601617333', 'https://openalex.org/W2396749677', 'https://openalex.org/W2115590028', 'https://openalex.org/W4238340159', 'https://openalex.org/W1495736750', 'https://openalex.org/W2734263782', 'https://openalex.org/W2166279570', 'https://openalex.org/W2911520123', 'https://openalex.org/W254944493', 'https://openalex.org/W4232557450', 'https://openalex.org/W1967934146', 'https://openalex.org/W74362393', 'https://openalex.org/W2400204790', 'https://openalex.org/W4285719527', 'https://openalex.org/W2174206963', 'https://openalex.org/W1989916294', 'https://openalex.org/W2329208473', 'https://openalex.org/W1507054833', 'https://openalex.org/W2341594233', 'https://openalex.org/W564992226', 'https://openalex.org/W772216571', 'https://openalex.org/W1580399267', 'https://openalex.org/W2023723978', 'https://openalex.org/W1586765942', 'https://openalex.org/W2132210624', 'https://openalex.org/W2620883451', 'https://openalex.org/W1556121947', 'https://openalex.org/W3131643527', 'https://openalex.org/W26772505', 'https://openalex.org/W1508272550', 'https://openalex.org/W47957325', 'https://openalex.org/W1524486391', 'https://openalex.org/W2167379809', 'https://openalex.org/W1880321215', 'https://openalex.org/W1517590008', 'https://openalex.org/W1568389378', 'https://openalex.org/W3142621357', 'https://openalex.org/W1652823179', 'https://openalex.org/W2185401186', 'https://openalex.org/W2089674328', 'https://openalex.org/W140304179', 'https://openalex.org/W1709565248', 'https://openalex.org/W2396300712', 'https://openalex.org/W1544328127', 'https://openalex.org/W2058296776', 'https://openalex.org/W1502799200', 'https://openalex.org/W2147486635', 'https://openalex.org/W185703229', 'https://openalex.org/W1485680143', 'https://openalex.org/W1576420969', 'https://openalex.org/W96340285', 'https://openalex.org/W1501121384', 'https://openalex.org/W616650091', 'https://openalex.org/W1505395785', 'https://openalex.org/W2061981200', 'https://openalex.org/W1860826869', 'https://openalex.org/W2110382134', 'https://openalex.org/W2112349754', 'https://openalex.org/W2339500526', 'https://openalex.org/W2403651582', 'https://openalex.org/W1990473760', 'https://openalex.org/W2114737695', 'https://openalex.org/W1988046102', 'https://openalex.org/W2724313895', 'https://openalex.org/W1513488242', 'https://openalex.org/W2337098149']",2011-10-01
https://openalex.org/W2991108091,https://doi.org/10.48550/arxiv.1907.04868,LakhNES: Improving multi-instrumental music generation with cross-domain\n pre-training,"We are interested in the task of generating multi-instrumental music scores.\nThe Transformer architecture has recently shown great promise for the task of\npiano score generation; here we adapt it to the multi-instrumental setting.\nTransformers are complex, high-dimensional language models which are capable of\ncapturing long-term structure in sequence data, but require large amounts of\ndata to fit. Their success on piano score generation is partially explained by\nthe large volumes of symbolic data readily available for that domain. We\nleverage the recently-introduced NES-MDB dataset of four-instrument scores from\nan early video game sound synthesis chip (the NES), which we find to be\nwell-suited to training with the Transformer architecture. To further improve\nthe performance of our model, we propose a pre-training technique to leverage\nthe information in a large collection of heterogeneous music, namely the Lakh\nMIDI dataset. Despite differences between the two corpora, we find that this\ntransfer learning procedure improves both quantitative and qualitative\nperformance for our primary task.\n",[],2019-07-10
https://openalex.org/W2940744433,https://doi.org/10.48550/arxiv.1904.10509,Generating Long Sequences with Sparse Transformers,"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.","['https://openalex.org/W2949427019', 'https://openalex.org/W2962942158', 'https://openalex.org/W1793121960', 'https://openalex.org/W2613904329', 'https://openalex.org/W2787214294', 'https://openalex.org/W2338908902', 'https://openalex.org/W2763421725', 'https://openalex.org/W2953331651', 'https://openalex.org/W2519091744', 'https://openalex.org/W2891815651', 'https://openalex.org/W2953318193', 'https://openalex.org/W2962776038', 'https://openalex.org/W2952276042', 'https://openalex.org/W2724346673', 'https://openalex.org/W2950946978', 'https://openalex.org/W2773781902', 'https://openalex.org/W2594961016', 'https://openalex.org/W2259472270', 'https://openalex.org/W2906625520', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963139417', 'https://openalex.org/W2778792233', 'https://openalex.org/W2886490473', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964122153']",2019-04-23
https://openalex.org/W2963432880,,Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis,"We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.",[],2018-06-01
https://openalex.org/W2996037775,https://doi.org/10.48550/arxiv.1912.06680,Dota 2 with Large Scale Deep Reinforcement Learning,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.","['https://openalex.org/W2057653135', 'https://openalex.org/W2104733512', 'https://openalex.org/W2766447205', 'https://openalex.org/W2899205164', 'https://openalex.org/W1512315140', 'https://openalex.org/W2803740478', 'https://openalex.org/W2041367235', 'https://openalex.org/W2962957031', 'https://openalex.org/W2736601468', 'https://openalex.org/W2322316662', 'https://openalex.org/W2762117857', 'https://openalex.org/W2982316857', 'https://openalex.org/W2903697572', 'https://openalex.org/W2963296584', 'https://openalex.org/W2257979135', 'https://openalex.org/W2756154119', 'https://openalex.org/W2981030070', 'https://openalex.org/W2574978968', 'https://openalex.org/W2963836708', 'https://openalex.org/W2136848157', 'https://openalex.org/W2603088459', 'https://openalex.org/W2749988060', 'https://openalex.org/W2786036274', 'https://openalex.org/W1533453739', 'https://openalex.org/W2941355526', 'https://openalex.org/W2902907165', 'https://openalex.org/W2291986326', 'https://openalex.org/W2810602713', 'https://openalex.org/W2426267443', 'https://openalex.org/W2963184621', 'https://openalex.org/W2903181768', 'https://openalex.org/W1757796397', 'https://openalex.org/W2622263826', 'https://openalex.org/W2473930607', 'https://openalex.org/W1777239053', 'https://openalex.org/W2612675303', 'https://openalex.org/W2964121744']",2019-12-13
https://openalex.org/W2922386270,https://doi.org/10.48550/arxiv.1903.04933,Hierarchical Autoregressive Image Models with Auxiliary Decoders,"Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\times$128 and 256$\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models.","['https://openalex.org/W2593915460', 'https://openalex.org/W2962919088', 'https://openalex.org/W2962739339', 'https://openalex.org/W2559246505', 'https://openalex.org/W343636949', 'https://openalex.org/W2267126114', 'https://openalex.org/W1959608418', 'https://openalex.org/W2952583441', 'https://openalex.org/W2902630600', 'https://openalex.org/W2194775991', 'https://openalex.org/W2904367110', 'https://openalex.org/W2963373786', 'https://openalex.org/W2086161653', 'https://openalex.org/W2963799213', 'https://openalex.org/W2173520492', 'https://openalex.org/W2097039814', 'https://openalex.org/W2893749619', 'https://openalex.org/W2963341956', 'https://openalex.org/W2962942158', 'https://openalex.org/W2242818861', 'https://openalex.org/W2145094598', 'https://openalex.org/W2963223306', 'https://openalex.org/W2751118800', 'https://openalex.org/W2476548250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963522047', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963981733', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963139417', 'https://openalex.org/W2964020555', 'https://openalex.org/W2770298516', 'https://openalex.org/W2949899814', 'https://openalex.org/W2963420272', 'https://openalex.org/W2963428348', 'https://openalex.org/W2782980316', 'https://openalex.org/W2153579005', 'https://openalex.org/W1821462560', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962750131', 'https://openalex.org/W2016589492', 'https://openalex.org/W2963636093', 'https://openalex.org/W2934761288', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962760235', 'https://openalex.org/W2097117768', 'https://openalex.org/W2963857374', 'https://openalex.org/W2606176153', 'https://openalex.org/W2842511635', 'https://openalex.org/W2409550820', 'https://openalex.org/W2110798204', 'https://openalex.org/W2302255633', 'https://openalex.org/W2963527611', 'https://openalex.org/W2136922672']",2019-03-06
https://openalex.org/W2998108143,https://doi.org/10.48550/arxiv.1912.12180,Axial Attention in Multidimensional Transformers,"We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.","['https://openalex.org/W2963629403', 'https://openalex.org/W2413794162', 'https://openalex.org/W2423557781', 'https://openalex.org/W2963403868', 'https://openalex.org/W2948412951', 'https://openalex.org/W2950739196', 'https://openalex.org/W2962750131', 'https://openalex.org/W2940744433', 'https://openalex.org/W2964122153', 'https://openalex.org/W2962990490', 'https://openalex.org/W2963406904', 'https://openalex.org/W2267126114', 'https://openalex.org/W2981689412', 'https://openalex.org/W2918222882', 'https://openalex.org/W2963428348', 'https://openalex.org/W3005844337', 'https://openalex.org/W2950946978', 'https://openalex.org/W2097039814']",2019-12-20
https://openalex.org/W1997640156,https://doi.org/10.1145/361254.361265,Music and computer composition,"The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed. Several previous computer compositions are discussed, including the ILLIAC Suite. A program to generate short melody fragments was written to simulate some of the aspects of human composition. Five samples of its output are presented and discussed. It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound. It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used.","['https://openalex.org/W2142980624', 'https://openalex.org/W6634866879', 'https://openalex.org/W4240317410', 'https://openalex.org/W2085026357', 'https://openalex.org/W1507054833', 'https://openalex.org/W2802393548', 'https://openalex.org/W2034161986', 'https://openalex.org/W2109601663', 'https://openalex.org/W1512123845', 'https://openalex.org/W1540957701', 'https://openalex.org/W2139953297', 'https://openalex.org/W1583885735', 'https://openalex.org/W2074576127', 'https://openalex.org/W2057632']",1972-02-01
https://openalex.org/W2949888546,,Sequence to Sequence Learning with Neural Networks,"Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-TermMemory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier. 1","['https://openalex.org/W2161772046', 'https://openalex.org/W2101105183', 'https://openalex.org/W2964308564', 'https://openalex.org/W1753482797', 'https://openalex.org/W2064675550', 'https://openalex.org/W2150355110', 'https://openalex.org/W1578042930', 'https://openalex.org/W1908388472', 'https://openalex.org/W2147768505', 'https://openalex.org/W2145287260', 'https://openalex.org/W2402268235', 'https://openalex.org/W2949190276', 'https://openalex.org/W2253807446', 'https://openalex.org/W2316776689', 'https://openalex.org/W1498436455', 'https://openalex.org/W2964222437', 'https://openalex.org/W194249466', 'https://openalex.org/W2132339004', 'https://openalex.org/W2250489405', 'https://openalex.org/W2950635152', 'https://openalex.org/W2184045248', 'https://openalex.org/W2950577311', 'https://openalex.org/W179875071', 'https://openalex.org/W1810943226', 'https://openalex.org/W1828163288', 'https://openalex.org/W2251682575', 'https://openalex.org/W2112796928', 'https://openalex.org/W2127141656', 'https://openalex.org/W2136939460']",2014-09-10
https://openalex.org/W2919624000,,Music Transformer: Generating Music with Long-Term Structure,,[],2019-01-01
https://openalex.org/W2973975824,,Automatic Lyrics Transcription in Polyphonic Music: Does Background Music Help?,"Background music affects lyrics intelligibility of singing vocals in a music piece. Automatic lyrics transcription in polyphonic music is a challenging task because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. For this purpose, we firstly study and compare several automatic speech recognition pipelines for the application of lyrics transcription. Later, we present the lyrics transcription performance of these music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With this genre-based approach, we explicitly model the characteristics of music, instead of trying to remove the background music as noise. The proposed approach achieves a significant improvement in performance in the lyrics transcription and alignment tasks on several well-known polyphonic test datasets, outperforming all comparable existing systems.",[],2019-09-23
https://openalex.org/W2804078698,https://doi.org/10.48550/arxiv.1805.08318,Self-Attention Generative Adversarial Networks,"In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",[],2018-05-21
https://openalex.org/W2898148140,https://doi.org/10.48550/arxiv.1810.12247,Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset,"Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (~3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.",[],2018-10-29
https://openalex.org/W2964020555,,NICE: Non-linear Independent Components Estimation,"We propose a deep learning framework for modeling complex high-dimensional densities via Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. 1","['https://openalex.org/W99063960', 'https://openalex.org/W1494871301', 'https://openalex.org/W2949514005', 'https://openalex.org/W1710476689', 'https://openalex.org/W2254715784', 'https://openalex.org/W90610118', 'https://openalex.org/W2140574335', 'https://openalex.org/W2335728318', 'https://openalex.org/W2123649031', 'https://openalex.org/W1959608418', 'https://openalex.org/W1872489089', 'https://openalex.org/W1909320841', 'https://openalex.org/W189596042', 'https://openalex.org/W2097268041', 'https://openalex.org/W1533072162', 'https://openalex.org/W1606458877', 'https://openalex.org/W2135181320', 'https://openalex.org/W2122262818', 'https://openalex.org/W2103936488', 'https://openalex.org/W1606347560', 'https://openalex.org/W2096192494', 'https://openalex.org/W2072128103', 'https://openalex.org/W2131939418']",2014-10-30
https://openalex.org/W2892104732,https://doi.org/10.3929/ethz-b-000292318,MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer,"We introduce MIDI-VAE, a neural network model basedon Variational Autoencoders that is capable of handlingpolyphonic music with multiple instrument tracks, as wellas modeling the dynamics of music by incorporating notedurations and velocities. We show that MIDI-VAE can per-form style transfer on symbolic music by automaticallychanging pitches, dynamics and instruments of a musicpiece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separatestyle validation classifiers. Our model can also interpolatebetween short pieces of music, produce medleys and cre-ate mixtures of entire songs. The interpolations smoothlychange pitches, dynamics and instrumentation to create aharmonic bridge between two music pieces. To the best ofour knowledge, this work represents the first successful at-tempt at applying neural style transfer to complete musicalcompositions.","['https://openalex.org/W2559110679', 'https://openalex.org/W2951004968', 'https://openalex.org/W2962772087', 'https://openalex.org/W2605287558', 'https://openalex.org/W2163922914', 'https://openalex.org/W2772474126', 'https://openalex.org/W2408362375', 'https://openalex.org/W2795278834', 'https://openalex.org/W2951535099', 'https://openalex.org/W2953100410', 'https://openalex.org/W2769570009', 'https://openalex.org/W2624872001', 'https://openalex.org/W3123961192', 'https://openalex.org/W2747023714', 'https://openalex.org/W2963575853', 'https://openalex.org/W2746068898', 'https://openalex.org/W2067621398', 'https://openalex.org/W1522301498', 'https://openalex.org/W2601110281']",2018-01-01
https://openalex.org/W1999885698,https://doi.org/10.1121/1.383940,Software for a cascade/parallel formant synthesizer,"A software formant synthesizer is described that can generate synthetic speech using a laboratory digital computer. A flexible synthesizer configuration permits the synthesis of sonorants by either a cascade or parallel connection of digital resonators, but frication spectra must be synthesized by a set of resonators connected in parallel. A control program lets the user specify variable control parameter data, such as formant frequencies as a function of time, as a sequence of 〈time, value〉 points. The synthesizer design is described and motivated in Secs. I–III, and fortran listings for the synthesizer and control program are provided in an appendix. Computer requirements and necessary support software are described in Sec. IV. Strategies for the imitation of any speech utterance are described in Sec. V, and suggested values of control parameters for the synthesis of many English sounds are presented in tabular form.",[],1980-03-01
https://openalex.org/W2753868141,https://doi.org/10.5281/zenodo.1416370,Counterpoint By Convolution.,[TODO] Add abstract here.,"['https://openalex.org/W2950304420', 'https://openalex.org/W2542835211', 'https://openalex.org/W1931432374', 'https://openalex.org/W2170111110', 'https://openalex.org/W2135181320', 'https://openalex.org/W2033765726', 'https://openalex.org/W1512123845', 'https://openalex.org/W2053952057', 'https://openalex.org/W1819710477', 'https://openalex.org/W2898422183']",2017-10-23
https://openalex.org/W2962942158,,The challenge of realistic music generation: modelling raw audio at scale,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.",[],2018-06-01
https://openalex.org/W2584032004,https://doi.org/10.48550/arxiv.1612.07837,SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,"In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.",[],2016-12-22
https://openalex.org/W2964281804,,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice, a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries, a
grapheme-to-phoneme conversion model, a phoneme duration prediction model, a
fundamental frequency prediction model, and an audio synthesis model. For the
segmentation model, we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component, our system is simpler and more flexible than
traditional text-to-speech systems, where each component requires laborious
feature engineering and extensive domain expertise. Finally, we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.",[],2017-02-25
https://openalex.org/W2587284713,,Improved Variational Inference with Inverse Autoregressive Flow,"The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.","['https://openalex.org/W2951493172', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963147844', 'https://openalex.org/W1909320841', 'https://openalex.org/W2148464528', 'https://openalex.org/W2166851633', 'https://openalex.org/W581956982', 'https://openalex.org/W2194775991', 'https://openalex.org/W2293078015', 'https://openalex.org/W2101793402']",2016-01-01
https://openalex.org/W2992790584,https://doi.org/10.1109/tcyb.2019.2953194,A Hierarchical Recurrent Neural Network for Symbolic Melody Generation,"In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.","['https://openalex.org/W2174443198', 'https://openalex.org/W2112213875', 'https://openalex.org/W6605505536', 'https://openalex.org/W1819710477', 'https://openalex.org/W2601110281', 'https://openalex.org/W6719050207', 'https://openalex.org/W6746363961', 'https://openalex.org/W2809621972', 'https://openalex.org/W6743002019', 'https://openalex.org/W2598521311', 'https://openalex.org/W6732429163', 'https://openalex.org/W6732135373', 'https://openalex.org/W6749351710', 'https://openalex.org/W2064675550', 'https://openalex.org/W6730558285', 'https://openalex.org/W2028418738', 'https://openalex.org/W2115613106', 'https://openalex.org/W6680587008', 'https://openalex.org/W2962793481', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963073614', 'https://openalex.org/W1519304330', 'https://openalex.org/W3101639677', 'https://openalex.org/W4301959051', 'https://openalex.org/W2963681776', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963575853', 'https://openalex.org/W2953100410', 'https://openalex.org/W2963557407', 'https://openalex.org/W2792210438', 'https://openalex.org/W2963032576', 'https://openalex.org/W2138660131', 'https://openalex.org/W2962968839', 'https://openalex.org/W2746068898', 'https://openalex.org/W2772474126', 'https://openalex.org/W134527144', 'https://openalex.org/W1522301498', 'https://openalex.org/W2519091744']",2019-12-02
https://openalex.org/W2963681776,https://doi.org/10.5281/zenodo.5646573,Lakh Pianoroll Dataset,"The Lakh Pianoroll Dataset (LPD) is a collection of 174,154 multitrack pianorolls derived from the Lakh MIDI Dataset (LMD). For more information, please visit our project homepage.",[],2018-02-05
https://openalex.org/W2787560479,https://doi.org/10.18653/v1/n18-1202,Deep Contextualized Word Representations,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","['https://openalex.org/W2507974895', 'https://openalex.org/W3016169217', 'https://openalex.org/W2251599843', 'https://openalex.org/W2095705004', 'https://openalex.org/W2158847908', 'https://openalex.org/W2740711318', 'https://openalex.org/W581956982', 'https://openalex.org/W2147880316', 'https://openalex.org/W2064675550', 'https://openalex.org/W2756386045', 'https://openalex.org/W2250539671', 'https://openalex.org/W2259472270', 'https://openalex.org/W2172140247', 'https://openalex.org/W2771857412', 'https://openalex.org/W2610748790', 'https://openalex.org/W2964121744', 'https://openalex.org/W2739255396', 'https://openalex.org/W2963266340', 'https://openalex.org/W3037932933', 'https://openalex.org/W2556468274', 'https://openalex.org/W2608787653', 'https://openalex.org/W2296283641', 'https://openalex.org/W2950133940', 'https://openalex.org/W2605717780', 'https://openalex.org/W2158139315', 'https://openalex.org/W2493916176', 'https://openalex.org/W2765390718', 'https://openalex.org/W2740747242', 'https://openalex.org/W2103076621', 'https://openalex.org/W2953084091', 'https://openalex.org/W2162456950', 'https://openalex.org/W2743945814', 'https://openalex.org/W2336260055', 'https://openalex.org/W2526182867', 'https://openalex.org/W1899794420', 'https://openalex.org/W2251939518', 'https://openalex.org/W2611669587', 'https://openalex.org/W2757205734', 'https://openalex.org/W2738152205', 'https://openalex.org/W2427527485', 'https://openalex.org/W2740765036', 'https://openalex.org/W2949548130', 'https://openalex.org/W6908809', 'https://openalex.org/W2155069789', 'https://openalex.org/W2518202280', 'https://openalex.org/W2251035762', 'https://openalex.org/W2952230511', 'https://openalex.org/W2964091467', 'https://openalex.org/W2950621961', 'https://openalex.org/W2963625095', 'https://openalex.org/W2951559648', 'https://openalex.org/W1632114991', 'https://openalex.org/W2516255829']",2018-01-01
https://openalex.org/W3097787369,https://doi.org/10.21437/interspeech.2020-3094,Self-Supervised Representations Improve End-to-End Speech Translation,"End-to-end speech-to-text translation can provide a simpler and smaller system but is facing the challenge of data scarcity.Pre-training methods can leverage unlabeled data and have been shown to be effective on data-scarce settings.In this work, we explore whether self-supervised pre-trained speech representations can benefit the speech translation task in both highand low-resource settings, whether they can transfer well to other languages, and whether they can be effectively combined with other common methods that help improve low-resource end-to-end speech translation such as using a pre-trained highresource speech recognition system.We demonstrate that selfsupervised pre-trained features can consistently improve the translation performance, and cross-lingual transfer allows to extend to a variety of languages without or with little tuning.","['https://openalex.org/W3030437843', 'https://openalex.org/W2933138175', 'https://openalex.org/W2973049979', 'https://openalex.org/W2970119519', 'https://openalex.org/W3034999214', 'https://openalex.org/W2997436923', 'https://openalex.org/W2979476256', 'https://openalex.org/W3008549139', 'https://openalex.org/W3015440307', 'https://openalex.org/W3006988520', 'https://openalex.org/W2466918907', 'https://openalex.org/W2962739339', 'https://openalex.org/W3016181583', 'https://openalex.org/W3103029570', 'https://openalex.org/W2936774411', 'https://openalex.org/W4297808394', 'https://openalex.org/W4300558631', 'https://openalex.org/W2605131327', 'https://openalex.org/W3032433061', 'https://openalex.org/W2963250244', 'https://openalex.org/W1494198834', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963532001', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964172053', 'https://openalex.org/W3005680577', 'https://openalex.org/W2901607128', 'https://openalex.org/W1522301498', 'https://openalex.org/W3102342027']",2020-10-25
https://openalex.org/W3095698432,https://doi.org/10.48550/arxiv.2010.13826,Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining,"Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.","['https://openalex.org/W2917128112', 'https://openalex.org/W2097550833', 'https://openalex.org/W2973049979', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963341956', 'https://openalex.org/W3092630929', 'https://openalex.org/W2974831423', 'https://openalex.org/W2894164357', 'https://openalex.org/W2019116789', 'https://openalex.org/W2803609229', 'https://openalex.org/W3026408381', 'https://openalex.org/W3099944122', 'https://openalex.org/W2077302143', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963446094', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2928075308', 'https://openalex.org/W3025324327', 'https://openalex.org/W3015412890', 'https://openalex.org/W2981458636', 'https://openalex.org/W2979722627', 'https://openalex.org/W3015267417', 'https://openalex.org/W1936920915', 'https://openalex.org/W2926827382', 'https://openalex.org/W2982223350', 'https://openalex.org/W3080993257', 'https://openalex.org/W2885185669', 'https://openalex.org/W2842511635', 'https://openalex.org/W3049038774', 'https://openalex.org/W2327501763', 'https://openalex.org/W2251599843', 'https://openalex.org/W2889201969', 'https://openalex.org/W2963288440', 'https://openalex.org/W2766219058', 'https://openalex.org/W2806429264', 'https://openalex.org/W2964117975']",2020-10-26
https://openalex.org/W3035202887,https://doi.org/10.18653/v1/2020.acl-main.213,Improved Speech Representations with Multi-Target Autoregressive Predictive Coding,"Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.","['https://openalex.org/W2979476256', 'https://openalex.org/W2939710050', 'https://openalex.org/W2963618559', 'https://openalex.org/W3125709657', 'https://openalex.org/W3096485810', 'https://openalex.org/W2973049979', 'https://openalex.org/W2785350307', 'https://openalex.org/W2996383576', 'https://openalex.org/W2101105183', 'https://openalex.org/W2804648901', 'https://openalex.org/W2963317665', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973157397', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972943112', 'https://openalex.org/W4385245566', 'https://openalex.org/W2525778437', 'https://openalex.org/W2963045354', 'https://openalex.org/W2077804127', 'https://openalex.org/W2964199361', 'https://openalex.org/W3127686677', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963340922', 'https://openalex.org/W1494198834', 'https://openalex.org/W854541894', 'https://openalex.org/W2024490156', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963571336', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963425185', 'https://openalex.org/W2884305338', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963720603', 'https://openalex.org/W3016011332', 'https://openalex.org/W179875071']",2020-01-01
https://openalex.org/W3098403858,,HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,"Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",[],2020-10-12
https://openalex.org/W3148040514,https://doi.org/10.1109/slt48900.2021.9383575,Audio Albert: A Lite Bert for Self-Supervised Learning of Audio Representation,"Self-supervised speech models are powerful speech representation extractors for downstream applications. Recently, larger models have been utilized in acoustic model training to achieve better performance. We propose Audio ALBERT, a lite version of the self-supervised speech representation model. We apply the lightweight representation extractor to two downstream tasks, speaker classification and phoneme classification. We show that Audio ALBERT achieves performance comparable with massive pre-trained networks in the downstream tasks while having 91% fewer parameters. Moreover, we design probing models to measure how much the latent representations can encode the speaker's and phoneme's information. We find that the representations encoded in internal layers of Audio ALBERT contain more information for both phoneme and speaker than the last layer, which is generally used for downstream tasks. Our findings provide a new avenue for using self-supervised networks to achieve better performance and efficiency.","['https://openalex.org/W6768021236', 'https://openalex.org/W6774314701', 'https://openalex.org/W3035524453', 'https://openalex.org/W6763701032', 'https://openalex.org/W6766673545', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769686700', 'https://openalex.org/W6769196770', 'https://openalex.org/W2963347649', 'https://openalex.org/W2747874407', 'https://openalex.org/W1494198834', 'https://openalex.org/W6769238691', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972889948', 'https://openalex.org/W3015265920', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W6755207826', 'https://openalex.org/W2965046076', 'https://openalex.org/W6753640285', 'https://openalex.org/W2963928591', 'https://openalex.org/W3015412285', 'https://openalex.org/W2948947170', 'https://openalex.org/W6757817989', 'https://openalex.org/W2972808286', 'https://openalex.org/W2187089797', 'https://openalex.org/W2908510526', 'https://openalex.org/W2970597249', 'https://openalex.org/W2996428491', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2965373594', 'https://openalex.org/W2981991061', 'https://openalex.org/W3096485810', 'https://openalex.org/W2866343820', 'https://openalex.org/W2979476256', 'https://openalex.org/W3005680577', 'https://openalex.org/W2996383576', 'https://openalex.org/W4297808394']",2021-01-19
https://openalex.org/W2937090315,https://doi.org/10.1109/icassp.2019.8683131,A Factorial Deep Markov Model for Unsupervised Disentangled Representation Learning from Speech,"We present the Factorial Deep Markov Model (FDMM) for representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and dynamic latent variable to exploit the fact that information in a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline i-vector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition system in a domain mismatch scenario.","['https://openalex.org/W6680137814', 'https://openalex.org/W6731701471', 'https://openalex.org/W2963499843', 'https://openalex.org/W6631362777', 'https://openalex.org/W6604441197', 'https://openalex.org/W2005708641', 'https://openalex.org/W6696934422', 'https://openalex.org/W2748528945', 'https://openalex.org/W6640963894', 'https://openalex.org/W6732782712', 'https://openalex.org/W2963417023', 'https://openalex.org/W6745117592', 'https://openalex.org/W6749693530', 'https://openalex.org/W6728354068', 'https://openalex.org/W6675022971', 'https://openalex.org/W2055408826', 'https://openalex.org/W2225156818', 'https://openalex.org/W6749758385', 'https://openalex.org/W2963134917', 'https://openalex.org/W2791874451', 'https://openalex.org/W2293634267', 'https://openalex.org/W111477576', 'https://openalex.org/W4297730691', 'https://openalex.org/W2758785877', 'https://openalex.org/W3211294292', 'https://openalex.org/W2964232608', 'https://openalex.org/W3101380508', 'https://openalex.org/W2962922562', 'https://openalex.org/W2963618559', 'https://openalex.org/W1635512741', 'https://openalex.org/W2793111190', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134827050', 'https://openalex.org/W2572702041', 'https://openalex.org/W1524333225', 'https://openalex.org/W2584414011', 'https://openalex.org/W2100768664']",2019-04-17
https://openalex.org/W3095292526,https://doi.org/10.21437/interspeech.2021-349,Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies,"Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.","['https://openalex.org/W2842511635', 'https://openalex.org/W3097286738', 'https://openalex.org/W3035202887', 'https://openalex.org/W2982223350', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962739339', 'https://openalex.org/W2972943112', 'https://openalex.org/W2950577311', 'https://openalex.org/W3003875258', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963317665', 'https://openalex.org/W2996383576', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016181583', 'https://openalex.org/W2963403868', 'https://openalex.org/W2981458636', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973157397', 'https://openalex.org/W3096485810', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963799213', 'https://openalex.org/W3099782249', 'https://openalex.org/W2024490156']",2021-08-27
https://openalex.org/W2971274815,https://doi.org/10.48550/arxiv.1905.03197,Unified Language Model Pre-training for Natural Language Understanding\n and Generation,"This paper presents a new Unified pre-trained Language Model (UniLM) that can\nbe fine-tuned for both natural language understanding and generation tasks. The\nmodel is pre-trained using three types of language modeling tasks:\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\nmodeling is achieved by employing a shared Transformer network and utilizing\nspecific self-attention masks to control what context the prediction conditions\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\nand CoQA question answering tasks. Moreover, UniLM achieves new\nstate-of-the-art results on five natural language generation datasets,\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\nis 2.65). The code and pre-trained models are available at\nhttps://github.com/microsoft/unilm.\n",[],2019-05-08
https://openalex.org/W2950180292,https://doi.org/10.48550/arxiv.1807.05520,Deep Clustering for Unsupervised Learning of Visual Features,"Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.","['https://openalex.org/W1989684337', 'https://openalex.org/W1836465849', 'https://openalex.org/W2108282816', 'https://openalex.org/W1480376833', 'https://openalex.org/W2949971241', 'https://openalex.org/W2326925005', 'https://openalex.org/W2750912449', 'https://openalex.org/W1520997877', 'https://openalex.org/W180242331', 'https://openalex.org/W1677182931', 'https://openalex.org/W1849277567', 'https://openalex.org/W2113221323', 'https://openalex.org/W2963495051', 'https://openalex.org/W2613718673', 'https://openalex.org/W1625255723', 'https://openalex.org/W2112796928', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963474899', 'https://openalex.org/W2097820631', 'https://openalex.org/W2554692997', 'https://openalex.org/W2212123867', 'https://openalex.org/W2137145201', 'https://openalex.org/W343636949', 'https://openalex.org/W2737057113', 'https://openalex.org/W2575671312', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963420272', 'https://openalex.org/W2964074409', 'https://openalex.org/W1955857676', 'https://openalex.org/W2950043193', 'https://openalex.org/W2168912077', 'https://openalex.org/W2412782625', 'https://openalex.org/W2103716973', 'https://openalex.org/W2134670479', 'https://openalex.org/W2412320034', 'https://openalex.org/W2511730936', 'https://openalex.org/W1825675169', 'https://openalex.org/W2287334441', 'https://openalex.org/W2121947440', 'https://openalex.org/W2511428026', 'https://openalex.org/W219040644', 'https://openalex.org/W2108598243', 'https://openalex.org/W2163605009', 'https://openalex.org/W2949532563', 'https://openalex.org/W2123872146', 'https://openalex.org/W2132820034', 'https://openalex.org/W2593864460', 'https://openalex.org/W2774008708', 'https://openalex.org/W1544092585', 'https://openalex.org/W2560977758', 'https://openalex.org/W2113896236', 'https://openalex.org/W2212363941', 'https://openalex.org/W2949667497', 'https://openalex.org/W2139427956', 'https://openalex.org/W2321533354', 'https://openalex.org/W2110798204', 'https://openalex.org/W2100031962', 'https://openalex.org/W2743157634', 'https://openalex.org/W2162762921', 'https://openalex.org/W2145094598', 'https://openalex.org/W1686810756', 'https://openalex.org/W2148349024', 'https://openalex.org/W2411541852', 'https://openalex.org/W2095705004', 'https://openalex.org/W2962749380', 'https://openalex.org/W2962852342', 'https://openalex.org/W2308529009', 'https://openalex.org/W2148809531', 'https://openalex.org/W2174726731', 'https://openalex.org/W2099471712', 'https://openalex.org/W2098693229', 'https://openalex.org/W2141362318', 'https://openalex.org/W2750549109']",2018-07-15
https://openalex.org/W2160473997,https://doi.org/10.1109/icassp.2011.5946971,CROWDMOS: An approach for crowdsourcing mean opinion score studies,"MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this pa per, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.","['https://openalex.org/W2024714418', 'https://openalex.org/W6677973343', 'https://openalex.org/W3106889297', 'https://openalex.org/W2150285586', 'https://openalex.org/W2170931744', 'https://openalex.org/W2151401338', 'https://openalex.org/W1969235627', 'https://openalex.org/W2917438849', 'https://openalex.org/W2119929864', 'https://openalex.org/W1995945562']",2011-05-01
https://openalex.org/W2947445680,https://doi.org/10.21437/interspeech.2019-2048,Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion,"We present an unsupervised end-to-end training scheme where we discover\ndiscrete subword units from speech without using any labels. The discrete\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\ngiven a variety of speakers, and a TTS-Decoder trained to project the\ndiscovered units back to the designated speech. We propose a discrete encoding\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\ndifferentiable. We found that the proposed encoding method offers automatic\nextraction of speech content from speaker style, and is sufficient to cover\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\nsynthesize speech with the same content as the input of ASR-Encoder but with\ndifferent speaker characteristics, which achieves voice conversion (VC). We\nfurther improve the quality of VC using adversarial training, where we train a\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\nevaluations show that the proposed approach offers strong VC results as it\neliminates speaker identity while preserving content within speech. In the\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\nbitrate.\n","['https://openalex.org/W4295521014', 'https://openalex.org/W2547039119', 'https://openalex.org/W2963691546', 'https://openalex.org/W2899518769', 'https://openalex.org/W2963609956', 'https://openalex.org/W2550241133', 'https://openalex.org/W1522301498', 'https://openalex.org/W2792995953', 'https://openalex.org/W2020607164', 'https://openalex.org/W2767754137', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963073614', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964135678', 'https://openalex.org/W2962974898', 'https://openalex.org/W2532494225', 'https://openalex.org/W4320013936', 'https://openalex.org/W2964243274', 'https://openalex.org/W2347098582', 'https://openalex.org/W4394670483', 'https://openalex.org/W2598638573', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950776302', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963684067', 'https://openalex.org/W2395899413', 'https://openalex.org/W3125709657', 'https://openalex.org/W4288107125', 'https://openalex.org/W2962736743', 'https://openalex.org/W2608207374', 'https://openalex.org/W2476548250', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548275288', 'https://openalex.org/W2963799213', 'https://openalex.org/W2951216052', 'https://openalex.org/W2962879692', 'https://openalex.org/W2758785877', 'https://openalex.org/W2059652594', 'https://openalex.org/W2963571336']",2019-09-13
https://openalex.org/W3003750857,https://doi.org/10.1109/globalsip45357.2019.8969412,Virtual Phone Discovery for Speech Synthesis Without Text,The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.,"['https://openalex.org/W2786608204', 'https://openalex.org/W2346964103', 'https://openalex.org/W6697293080', 'https://openalex.org/W6675022971', 'https://openalex.org/W2347098582', 'https://openalex.org/W2745710152', 'https://openalex.org/W2345811097', 'https://openalex.org/W1967924372', 'https://openalex.org/W2052697931', 'https://openalex.org/W1796128977', 'https://openalex.org/W2020607164', 'https://openalex.org/W2826003142', 'https://openalex.org/W1510007267', 'https://openalex.org/W2134202996', 'https://openalex.org/W2962699523', 'https://openalex.org/W2964135678', 'https://openalex.org/W2532494225', 'https://openalex.org/W2963830550', 'https://openalex.org/W2902070858', 'https://openalex.org/W2747192917', 'https://openalex.org/W2803005441', 'https://openalex.org/W2890718354', 'https://openalex.org/W2598638573', 'https://openalex.org/W2059652594', 'https://openalex.org/W6680735885', 'https://openalex.org/W1975728937', 'https://openalex.org/W2964169922', 'https://openalex.org/W2471520273', 'https://openalex.org/W2940544976', 'https://openalex.org/W2057007397', 'https://openalex.org/W2398490608', 'https://openalex.org/W2407614114', 'https://openalex.org/W6631362777', 'https://openalex.org/W2774848319', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963620343', 'https://openalex.org/W1524333225', 'https://openalex.org/W2165874743', 'https://openalex.org/W2141465109', 'https://openalex.org/W2100768664']",2019-11-01
https://openalex.org/W3033038061,https://doi.org/10.21437/interspeech.2020-3084,A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning,"Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labeled training examples.","['https://openalex.org/W2951004968', 'https://openalex.org/W2127141656', 'https://openalex.org/W2953331651', 'https://openalex.org/W2787698019', 'https://openalex.org/W2949382160', 'https://openalex.org/W2995680346', 'https://openalex.org/W3101749733', 'https://openalex.org/W2210838531', 'https://openalex.org/W2807947437', 'https://openalex.org/W2949517790', 'https://openalex.org/W2926063217', 'https://openalex.org/W2808697642', 'https://openalex.org/W2750248772', 'https://openalex.org/W2937090315', 'https://openalex.org/W2944828972', 'https://openalex.org/W1494198834', 'https://openalex.org/W2935542736', 'https://openalex.org/W2941814890', 'https://openalex.org/W2963300588', 'https://openalex.org/W2347098582', 'https://openalex.org/W2793111190', 'https://openalex.org/W2995085126', 'https://openalex.org/W2973049979', 'https://openalex.org/W2556930864', 'https://openalex.org/W2134952451', 'https://openalex.org/W2926827382', 'https://openalex.org/W2982223350', 'https://openalex.org/W2153185114', 'https://openalex.org/W2126377586', 'https://openalex.org/W2068247585', 'https://openalex.org/W2100768664', 'https://openalex.org/W3125709657', 'https://openalex.org/W2483390977', 'https://openalex.org/W2962695963', 'https://openalex.org/W2948211236', 'https://openalex.org/W2964232608', 'https://openalex.org/W2963618559', 'https://openalex.org/W2970014727', 'https://openalex.org/W2962990490']",2020-10-25
https://openalex.org/W3112034174,https://doi.org/10.48550/arxiv.2012.06659,DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization,"Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.","['https://openalex.org/W2766219058', 'https://openalex.org/W2972943112', 'https://openalex.org/W1821462560', 'https://openalex.org/W3096485810', 'https://openalex.org/W2995680346', 'https://openalex.org/W2981857663', 'https://openalex.org/W3035202887', 'https://openalex.org/W2991213871', 'https://openalex.org/W2982223350', 'https://openalex.org/W3099782249', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963799213', 'https://openalex.org/W2110798204', 'https://openalex.org/W3097286738', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963341956', 'https://openalex.org/W2512655038', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W2981991061', 'https://openalex.org/W3036601975', 'https://openalex.org/W2842511635', 'https://openalex.org/W3015265920', 'https://openalex.org/W2124558353', 'https://openalex.org/W2941814890', 'https://openalex.org/W2184045248', 'https://openalex.org/W3007227084', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963414781', 'https://openalex.org/W2963403868', 'https://openalex.org/W2547875792', 'https://openalex.org/W3015419784', 'https://openalex.org/W3025035610', 'https://openalex.org/W3041561163']",2020-12-11
https://openalex.org/W3015265920,https://doi.org/10.1109/icassp40776.2020.9053176,Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition,"We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.","['https://openalex.org/W2486109868', 'https://openalex.org/W2962739339', 'https://openalex.org/W6755207826', 'https://openalex.org/W6763701032', 'https://openalex.org/W2972889948', 'https://openalex.org/W2972981541', 'https://openalex.org/W2136922672', 'https://openalex.org/W2160815625', 'https://openalex.org/W6676481782', 'https://openalex.org/W2964245029', 'https://openalex.org/W2944255943', 'https://openalex.org/W2889213362', 'https://openalex.org/W2936774411', 'https://openalex.org/W2802248956', 'https://openalex.org/W2124558353', 'https://openalex.org/W6765807869', 'https://openalex.org/W6760911985', 'https://openalex.org/W176510440', 'https://openalex.org/W6681588610', 'https://openalex.org/W2512655038', 'https://openalex.org/W2127499922', 'https://openalex.org/W1993660824', 'https://openalex.org/W2963425185', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963211739', 'https://openalex.org/W2127141656', 'https://openalex.org/W2184045248', 'https://openalex.org/W2787560479', 'https://openalex.org/W3103005696', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W2145494108', 'https://openalex.org/W2293363371', 'https://openalex.org/W2996383576', 'https://openalex.org/W2110798204', 'https://openalex.org/W2962753370', 'https://openalex.org/W2962907457', 'https://openalex.org/W3011411500', 'https://openalex.org/W2970597249', 'https://openalex.org/W2979476256', 'https://openalex.org/W3125709657', 'https://openalex.org/W2950813464']",2020-04-09
https://openalex.org/W2963456134,https://doi.org/10.1145/3209978.3210080,Texygen,"We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.","['https://openalex.org/W179875071', 'https://openalex.org/W2101105183', 'https://openalex.org/W2619206542', 'https://openalex.org/W2119717200', 'https://openalex.org/W2964268978', 'https://openalex.org/W2058373514', 'https://openalex.org/W2565378226', 'https://openalex.org/W2616969219', 'https://openalex.org/W3101023724', 'https://openalex.org/W2133564696']",2018-06-27
https://openalex.org/W2577366047,https://doi.org/10.21437/interspeech.2017-343,Towards Better Decoding and Language Model Integration in Sequence to Sequence Models,"The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion.In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters.We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used.We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.","['https://openalex.org/W1915251500', 'https://openalex.org/W2962792802', 'https://openalex.org/W2024539680', 'https://openalex.org/W2193413348', 'https://openalex.org/W1582482241', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963158258', 'https://openalex.org/W4255949318', 'https://openalex.org/W1855892484', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2345474290', 'https://openalex.org/W2530876040', 'https://openalex.org/W1736701665', 'https://openalex.org/W2964043796', 'https://openalex.org/W4297798436', 'https://openalex.org/W2963620441', 'https://openalex.org/W2109886035', 'https://openalex.org/W2361821140', 'https://openalex.org/W1993411524', 'https://openalex.org/W2952746495', 'https://openalex.org/W1922655562', 'https://openalex.org/W2163605009', 'https://openalex.org/W2271840356', 'https://openalex.org/W2102113734', 'https://openalex.org/W1902237438', 'https://openalex.org/W854541894', 'https://openalex.org/W2160815625', 'https://openalex.org/W1524333225', 'https://openalex.org/W2525778437', 'https://openalex.org/W2963572611', 'https://openalex.org/W2952288254', 'https://openalex.org/W2183341477']",2017-08-16
https://openalex.org/W3024040651,https://doi.org/10.21437/interspeech.2020-3127,Exploring TTS Without T Using Biologically/Psychologically Motivated Neural Network Modules (ZeroSpeech 2020),"In this study, we reported our exploration of Text-To-Speech without Text\n(TTS without T) in the Zero Resource Speech Challenge 2020, in which\nparticipants proposed an end-to-end, unsupervised system that learned speech\nrecognition and TTS together. We addressed the challenge using\nbiologically/psychologically motivated modules of Artificial Neural Networks\n(ANN), with a particular interest in unsupervised learning of human language as\na biological/psychological problem. The system first processes Mel Frequency\nCepstral Coefficient (MFCC) frames with an Echo-State Network (ESN), and\nsimulates computations in cortical microcircuits. The outcome is discretized by\nour original Variational Autoencoder (VAE) that implements the Dirichlet-based\nBayesian clustering widely accepted in computational linguistics and cognitive\nscience. The discretized signal is then reverted into sound waveform via a\nneural-network implementation of the source-filter model for speech production.\n","['https://openalex.org/W2964121744', 'https://openalex.org/W2598638573', 'https://openalex.org/W2972374322', 'https://openalex.org/W2547039119', 'https://openalex.org/W2547875792', 'https://openalex.org/W1959608418', 'https://openalex.org/W4288107125', 'https://openalex.org/W4391602018', 'https://openalex.org/W3125709657', 'https://openalex.org/W4385245566', 'https://openalex.org/W2092919341', 'https://openalex.org/W2347098582', 'https://openalex.org/W1586357756', 'https://openalex.org/W2963403868', 'https://openalex.org/W1993755070', 'https://openalex.org/W2016988675', 'https://openalex.org/W3185362781', 'https://openalex.org/W3023775752', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964308564', 'https://openalex.org/W2947591107', 'https://openalex.org/W2468716020', 'https://openalex.org/W2158266063', 'https://openalex.org/W2963341956', 'https://openalex.org/W2048269309', 'https://openalex.org/W2963799213', 'https://openalex.org/W1522301498', 'https://openalex.org/W2118706537', 'https://openalex.org/W2126377586', 'https://openalex.org/W3048188704', 'https://openalex.org/W2990440871', 'https://openalex.org/W2896457183', 'https://openalex.org/W2940544976', 'https://openalex.org/W2118072362', 'https://openalex.org/W2103179919']",2020-10-25
https://openalex.org/W2982399380,https://doi.org/10.18653/v1/2020.acl-main.703,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.","['https://openalex.org/W2130158090', 'https://openalex.org/W131533222', 'https://openalex.org/W2922709902', 'https://openalex.org/W1544827683', 'https://openalex.org/W2418388682', 'https://openalex.org/W2251939518', 'https://openalex.org/W2944815030', 'https://openalex.org/W2914120296', 'https://openalex.org/W2978670439', 'https://openalex.org/W2950300355', 'https://openalex.org/W2950813464', 'https://openalex.org/W1599016936', 'https://openalex.org/W2952942550', 'https://openalex.org/W2950577311', 'https://openalex.org/W2963846996', 'https://openalex.org/W2962801832', 'https://openalex.org/W2969740599', 'https://openalex.org/W2945260553', 'https://openalex.org/W2768716007', 'https://openalex.org/W2975059944', 'https://openalex.org/W2952913664', 'https://openalex.org/W2964040452', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963341956', 'https://openalex.org/W2427527485', 'https://openalex.org/W2913443447', 'https://openalex.org/W2965373594', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963403868', 'https://openalex.org/W2787560479', 'https://openalex.org/W2799054028', 'https://openalex.org/W2962753370']",2020-01-01
https://openalex.org/W3185206490,https://doi.org/10.1007/s10919-021-00376-0,Do People Agree on How Positive Emotions Are Expressed? A Survey of Four Emotions and Five Modalities Across 11 Cultures,"Abstract While much is known about how negative emotions are expressed in different modalities, our understanding of the nonverbal expressions of positive emotions remains limited. In the present research, we draw upon disparate lines of theoretical and empirical work on positive emotions, and systematically examine which channels are thought to be used for expressing four positive emotions: feeling moved, gratitude, interest, and triumph. Employing the intersubjective approach, an established method in cross-cultural psychology, we first explored how the four positive emotions were reported to be expressed in two North American community samples (Studies 1a and 1b: n = 1466). We next confirmed the cross-cultural generalizability of our findings by surveying respondents from ten countries that diverged on cultural values (Study 2: n = 1826). Feeling moved was thought to be signaled with facial expressions, gratitude with the use of words, interest with words, face and voice, and triumph with body posture, vocal cues, facial expressions, and words. These findings provide cross-culturally consistent findings of differential expressions across positive emotions. Notably, positive emotions were thought to be expressed via modalities that go beyond the face.","['https://openalex.org/W2103781831', 'https://openalex.org/W1977575297', 'https://openalex.org/W2127571795', 'https://openalex.org/W2163546575', 'https://openalex.org/W2076346682', 'https://openalex.org/W4245744384', 'https://openalex.org/W4253871341', 'https://openalex.org/W2113168798', 'https://openalex.org/W2081004291', 'https://openalex.org/W2123901740', 'https://openalex.org/W2114763674', 'https://openalex.org/W2979864122', 'https://openalex.org/W2756862522', 'https://openalex.org/W4252661582', 'https://openalex.org/W1971969367', 'https://openalex.org/W2626522798', 'https://openalex.org/W3190538731', 'https://openalex.org/W2950247511', 'https://openalex.org/W2905841571', 'https://openalex.org/W1604972595', 'https://openalex.org/W2085003717', 'https://openalex.org/W2034620660', 'https://openalex.org/W2074591627', 'https://openalex.org/W2738790848', 'https://openalex.org/W1966797434', 'https://openalex.org/W2022068631', 'https://openalex.org/W4236533540', 'https://openalex.org/W2168013334', 'https://openalex.org/W2071934951', 'https://openalex.org/W2072500831', 'https://openalex.org/W2770415942', 'https://openalex.org/W2804209642', 'https://openalex.org/W2007445014', 'https://openalex.org/W6638922503', 'https://openalex.org/W6852839900', 'https://openalex.org/W2789688997', 'https://openalex.org/W2804436848', 'https://openalex.org/W2089358714', 'https://openalex.org/W2152966241', 'https://openalex.org/W4399578533', 'https://openalex.org/W2316342334', 'https://openalex.org/W2401780381', 'https://openalex.org/W2114856148', 'https://openalex.org/W2163429658', 'https://openalex.org/W2917415925', 'https://openalex.org/W2164471543', 'https://openalex.org/W2112376540', 'https://openalex.org/W2150403218', 'https://openalex.org/W2996641430', 'https://openalex.org/W6640387931', 'https://openalex.org/W2045154139', 'https://openalex.org/W2588736771', 'https://openalex.org/W2067425942', 'https://openalex.org/W4248591389', 'https://openalex.org/W4230001657', 'https://openalex.org/W2045826950', 'https://openalex.org/W2114959145', 'https://openalex.org/W2140782513', 'https://openalex.org/W633327578', 'https://openalex.org/W2085099526', 'https://openalex.org/W2593392694', 'https://openalex.org/W2167621245', 'https://openalex.org/W2095829101', 'https://openalex.org/W2137412454', 'https://openalex.org/W2582743722', 'https://openalex.org/W2001815708', 'https://openalex.org/W2154480377', 'https://openalex.org/W2042863830', 'https://openalex.org/W1993363516', 'https://openalex.org/W1975238145', 'https://openalex.org/W2626982410', 'https://openalex.org/W2048765566', 'https://openalex.org/W2117237841', 'https://openalex.org/W2583542555', 'https://openalex.org/W2562161918', 'https://openalex.org/W3014201970', 'https://openalex.org/W2153900863', 'https://openalex.org/W2776575342', 'https://openalex.org/W2086814126', 'https://openalex.org/W4256441345', 'https://openalex.org/W1570597587', 'https://openalex.org/W2761122192', 'https://openalex.org/W2033428167', 'https://openalex.org/W2117651526', 'https://openalex.org/W2166118664', 'https://openalex.org/W2119318965', 'https://openalex.org/W1543806480', 'https://openalex.org/W2061213600', 'https://openalex.org/W2947915034', 'https://openalex.org/W2121247252', 'https://openalex.org/W1987937487', 'https://openalex.org/W2132673112', 'https://openalex.org/W2155144982', 'https://openalex.org/W2141960018', 'https://openalex.org/W2141093480', 'https://openalex.org/W2805967452', 'https://openalex.org/W601008645', 'https://openalex.org/W2113884929', 'https://openalex.org/W2601891535', 'https://openalex.org/W2079177735', 'https://openalex.org/W2586296314', 'https://openalex.org/W2607784970', 'https://openalex.org/W2315689228', 'https://openalex.org/W2072038603', 'https://openalex.org/W2260186468', 'https://openalex.org/W2169124432', 'https://openalex.org/W2596405528', 'https://openalex.org/W3048784195', 'https://openalex.org/W2802400506', 'https://openalex.org/W2911404604', 'https://openalex.org/W4367314869', 'https://openalex.org/W2157134953', 'https://openalex.org/W4200005433', 'https://openalex.org/W3012004216', 'https://openalex.org/W2737934660', 'https://openalex.org/W2118789253', 'https://openalex.org/W2044950274', 'https://openalex.org/W2019312772', 'https://openalex.org/W1937226553', 'https://openalex.org/W3128629234', 'https://openalex.org/W1588539311', 'https://openalex.org/W2145337957', 'https://openalex.org/W1850070871', 'https://openalex.org/W2006044072', 'https://openalex.org/W2156948210']",2021-07-22
https://openalex.org/W2745376520,,The ILSP/INNOETICS Text-to-Speech System for the Blizzard Challenge 2013,,"['https://openalex.org/W2283817422', 'https://openalex.org/W3143522270', 'https://openalex.org/W84258864', 'https://openalex.org/W2967173218', 'https://openalex.org/W2147760966', 'https://openalex.org/W2096766039', 'https://openalex.org/W84569894', 'https://openalex.org/W1538228347', 'https://openalex.org/W2147848987', 'https://openalex.org/W2108572107', 'https://openalex.org/W5726830', 'https://openalex.org/W2294232014', 'https://openalex.org/W2534037118', 'https://openalex.org/W1517939602', 'https://openalex.org/W2263694371']",2013-01-01
https://openalex.org/W2040587156,https://doi.org/10.1109/apsipa.2014.7041640,Exemplar-based emotional voice conversion using non-negative matrix factorization,"This paper presents an emotional voice conversion (VC) technology using non-negative matrix factorization, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The input source spectrum is decomposed into the source spectrum exemplars and their weights. By replacing source exemplars with target exemplars, the converted spectrum and FO are constructed from the target exemplars and the target FO, which is paired with exemplars. In order to reduce the computational time, we adopted non-negative matrix factorization using active Newton set algorithms to our VC method. We carried out emotional voice conversion tasks, which convert an emotional voice into a neutral voice. The effectiveness of this method was confirmed with objective and subjective evaluations.","['https://openalex.org/W6732045268', 'https://openalex.org/W1965255698', 'https://openalex.org/W6680012447', 'https://openalex.org/W2150415460', 'https://openalex.org/W185399533', 'https://openalex.org/W2062507334', 'https://openalex.org/W2078722660', 'https://openalex.org/W1963778986', 'https://openalex.org/W2120605154', 'https://openalex.org/W2105160541', 'https://openalex.org/W6602007935', 'https://openalex.org/W6635216677', 'https://openalex.org/W2077801020', 'https://openalex.org/W2129703931', 'https://openalex.org/W2407110532', 'https://openalex.org/W2117418893', 'https://openalex.org/W2123003832', 'https://openalex.org/W2152205330', 'https://openalex.org/W2156142001', 'https://openalex.org/W2017425464', 'https://openalex.org/W2074788634', 'https://openalex.org/W2154920538', 'https://openalex.org/W2141520175', 'https://openalex.org/W1572730534', 'https://openalex.org/W10800834', 'https://openalex.org/W1982641198', 'https://openalex.org/W1965912016', 'https://openalex.org/W2164241094', 'https://openalex.org/W2011378162', 'https://openalex.org/W6635180523', 'https://openalex.org/W3143596294', 'https://openalex.org/W2135029798', 'https://openalex.org/W1588037970', 'https://openalex.org/W2162295204', 'https://openalex.org/W49412823', 'https://openalex.org/W2577350056', 'https://openalex.org/W1588266896']",2014-12-01
https://openalex.org/W1966797434,https://doi.org/10.1080/02699939208411068,An argument for basic emotions,"Abstract Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.","['https://openalex.org/W2021463223', 'https://openalex.org/W35156556', 'https://openalex.org/W2160300495', 'https://openalex.org/W1991580276', 'https://openalex.org/W4254216043', 'https://openalex.org/W2009375902', 'https://openalex.org/W2167575933', 'https://openalex.org/W2137409775', 'https://openalex.org/W1969736828', 'https://openalex.org/W2073395356', 'https://openalex.org/W2152194780', 'https://openalex.org/W2023825291', 'https://openalex.org/W2004879394', 'https://openalex.org/W2070353225', 'https://openalex.org/W1975911018', 'https://openalex.org/W4230471314', 'https://openalex.org/W2141208341', 'https://openalex.org/W2010252136', 'https://openalex.org/W1982016687', 'https://openalex.org/W4231519654', 'https://openalex.org/W2084864466', 'https://openalex.org/W2018535666', 'https://openalex.org/W4388252364', 'https://openalex.org/W2146355519', 'https://openalex.org/W4248591389', 'https://openalex.org/W2111690475', 'https://openalex.org/W4313371768', 'https://openalex.org/W2080593835', 'https://openalex.org/W2035579615', 'https://openalex.org/W2154611638', 'https://openalex.org/W2004315656', 'https://openalex.org/W4241027858', 'https://openalex.org/W2024930196', 'https://openalex.org/W2014221158', 'https://openalex.org/W2025333761', 'https://openalex.org/W1982108488', 'https://openalex.org/W4256441345', 'https://openalex.org/W2068062858', 'https://openalex.org/W2061796073', 'https://openalex.org/W1986171833', 'https://openalex.org/W1483388411', 'https://openalex.org/W1993148431', 'https://openalex.org/W1977621542', 'https://openalex.org/W1506209491', 'https://openalex.org/W2047197602', 'https://openalex.org/W2059677238', 'https://openalex.org/W73267799', 'https://openalex.org/W1492041649', 'https://openalex.org/W2488533857', 'https://openalex.org/W2059425960', 'https://openalex.org/W2046677541', 'https://openalex.org/W3022671937', 'https://openalex.org/W2996004238', 'https://openalex.org/W1992760321', 'https://openalex.org/W1986101067', 'https://openalex.org/W4241339470', 'https://openalex.org/W125757691', 'https://openalex.org/W2317673213', 'https://openalex.org/W2788988160', 'https://openalex.org/W4285719527', 'https://openalex.org/W1571620383', 'https://openalex.org/W2325605806', 'https://openalex.org/W1847228729', 'https://openalex.org/W4246871892', 'https://openalex.org/W152725222', 'https://openalex.org/W2023585285', 'https://openalex.org/W4241000899', 'https://openalex.org/W4255000641', 'https://openalex.org/W2068418797', 'https://openalex.org/W1995828951', 'https://openalex.org/W2024209041', 'https://openalex.org/W1987726847', 'https://openalex.org/W2006044072', 'https://openalex.org/W2995034616', 'https://openalex.org/W2102548748', 'https://openalex.org/W1511161594', 'https://openalex.org/W2156948210', 'https://openalex.org/W139908175', 'https://openalex.org/W2501838219', 'https://openalex.org/W4298974124', 'https://openalex.org/W2618978628', 'https://openalex.org/W2030986414', 'https://openalex.org/W294882091', 'https://openalex.org/W2521562261', 'https://openalex.org/W2134436701', 'https://openalex.org/W1998430703', 'https://openalex.org/W2083450642', 'https://openalex.org/W1984186949', 'https://openalex.org/W2320601650', 'https://openalex.org/W2125744402']",1992-05-01
https://openalex.org/W3198217962,https://doi.org/10.18653/v1/2022.acl-long.593,Text-Free Prosody-Aware Generative Spoken Language Modeling,"Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2107740512', 'https://openalex.org/W2141885148', 'https://openalex.org/W1524333225', 'https://openalex.org/W3169320628', 'https://openalex.org/W2151083697', 'https://openalex.org/W2130086727', 'https://openalex.org/W3157923770', 'https://openalex.org/W2810914326', 'https://openalex.org/W2160473997', 'https://openalex.org/W3098403858', 'https://openalex.org/W3099782249', 'https://openalex.org/W3036601975', 'https://openalex.org/W2112078429', 'https://openalex.org/W2972943112', 'https://openalex.org/W2608207374', 'https://openalex.org/W2426479676', 'https://openalex.org/W2973049979', 'https://openalex.org/W3095948607', 'https://openalex.org/W3098903812', 'https://openalex.org/W2527729766', 'https://openalex.org/W1597422500', 'https://openalex.org/W3150572638', 'https://openalex.org/W2123409957', 'https://openalex.org/W2950349262', 'https://openalex.org/W2949382160', 'https://openalex.org/W2982223350', 'https://openalex.org/W1522301498', 'https://openalex.org/W2143827132', 'https://openalex.org/W3112034174', 'https://openalex.org/W359161088', 'https://openalex.org/W2963799213', 'https://openalex.org/W2118748593', 'https://openalex.org/W3039910566', 'https://openalex.org/W2842511635', 'https://openalex.org/W3094247854', 'https://openalex.org/W2913151434', 'https://openalex.org/W3210177631', 'https://openalex.org/W1494198834', 'https://openalex.org/W2093585241', 'https://openalex.org/W3033411150', 'https://openalex.org/W3148101939', 'https://openalex.org/W2995181338', 'https://openalex.org/W3140429000']",2022-01-01
https://openalex.org/W2018658329,https://doi.org/10.1037/h0024532,Decoding of inconsistent communications.,,"['https://openalex.org/W1540664512', 'https://openalex.org/W2010171127', 'https://openalex.org/W118211835', 'https://openalex.org/W1560978071', 'https://openalex.org/W2027636702', 'https://openalex.org/W2494008053']",1967-01-01
https://openalex.org/W3168292814,https://doi.org/10.48550/arxiv.2106.08519,Global Rhythm Style Transfer Without Text Transcriptions,"Prosody plays an important role in characterizing the style of a speaker or an emotion, but most non-parallel voice or emotion style transfer algorithms do not convert any prosody information. Two major components of prosody are pitch and rhythm. Disentangling the prosody information, particularly the rhythm component, from the speech is challenging because it involves breaking the synchrony between the input speech and the disentangled speech representation. As a result, most existing prosody style transfer algorithms would need to rely on some form of text transcriptions to identify the content information, which confines their application to high-resource languages only. Recently, SpeechSplit has made sizeable progress towards unsupervised prosody style transfer, but it is unable to extract high-level global prosody style in an unsupervised manner. In this paper, we propose AutoPST, which can disentangle global prosody style from speech without relying on any text transcriptions. AutoPST is an Autoencoder-based Prosody Style Transfer framework with a thorough rhythm removal module guided by the self-expressive representation learning. Experiments on different style transfer tasks show that AutoPST can effectively convert prosody that correctly reflects the styles of the target domains.","['https://openalex.org/W2945544731', 'https://openalex.org/W2162295204', 'https://openalex.org/W3044386551', 'https://openalex.org/W2962896155', 'https://openalex.org/W3034794073', 'https://openalex.org/W2972659941', 'https://openalex.org/W2148846882', 'https://openalex.org/W3015419784', 'https://openalex.org/W2939131199', 'https://openalex.org/W2963767194', 'https://openalex.org/W2532494225', 'https://openalex.org/W2963539064', 'https://openalex.org/W2891813127', 'https://openalex.org/W3128776229', 'https://openalex.org/W2963927338', 'https://openalex.org/W2946555236', 'https://openalex.org/W2795783309', 'https://openalex.org/W2963830550', 'https://openalex.org/W2949382160', 'https://openalex.org/W2972667718', 'https://openalex.org/W2964135678', 'https://openalex.org/W2161736993', 'https://openalex.org/W2626778328', 'https://openalex.org/W3015645837', 'https://openalex.org/W2766219058', 'https://openalex.org/W2949281321', 'https://openalex.org/W3097112431', 'https://openalex.org/W3100378519', 'https://openalex.org/W2077801020', 'https://openalex.org/W2997347790', 'https://openalex.org/W2774848319', 'https://openalex.org/W2810914326', 'https://openalex.org/W3097159218', 'https://openalex.org/W3004402693', 'https://openalex.org/W2511640485', 'https://openalex.org/W2972812066', 'https://openalex.org/W2972970915', 'https://openalex.org/W2952269766', 'https://openalex.org/W3015805741', 'https://openalex.org/W2517513811', 'https://openalex.org/W2526425061', 'https://openalex.org/W3020570669']",2021-06-16
https://openalex.org/W3045354608,https://doi.org/10.21437/interspeech.2020-1323,Multi-Speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network,"We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.","['https://openalex.org/W2972334330', 'https://openalex.org/W2471520273', 'https://openalex.org/W112588824', 'https://openalex.org/W2115167851', 'https://openalex.org/W2517513811', 'https://openalex.org/W2962793481', 'https://openalex.org/W95152782', 'https://openalex.org/W586270867', 'https://openalex.org/W2963684088', 'https://openalex.org/W2049686551', 'https://openalex.org/W1976725440', 'https://openalex.org/W2964121744', 'https://openalex.org/W2976159681', 'https://openalex.org/W2963091184', 'https://openalex.org/W2774848319', 'https://openalex.org/W2972366998', 'https://openalex.org/W2964243274', 'https://openalex.org/W2786270934', 'https://openalex.org/W2170167891', 'https://openalex.org/W3004402693', 'https://openalex.org/W2981934523', 'https://openalex.org/W2519091744', 'https://openalex.org/W2766406951', 'https://openalex.org/W2077801020', 'https://openalex.org/W2948238043', 'https://openalex.org/W2973138167', 'https://openalex.org/W2006302620']",2020-10-25
https://openalex.org/W3160799772,https://doi.org/10.1109/icassp39728.2021.9414460,Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?,"Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.","['https://openalex.org/W6675354045', 'https://openalex.org/W6631190155', 'https://openalex.org/W3003875258', 'https://openalex.org/W3015265920', 'https://openalex.org/W3035202887', 'https://openalex.org/W6769455919', 'https://openalex.org/W6783797576', 'https://openalex.org/W6780483730', 'https://openalex.org/W6777232839', 'https://openalex.org/W2982223350', 'https://openalex.org/W6770514103', 'https://openalex.org/W6766978945', 'https://openalex.org/W6780218876', 'https://openalex.org/W2110073835', 'https://openalex.org/W3015522062', 'https://openalex.org/W6772883055', 'https://openalex.org/W6753000030', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W2981283774', 'https://openalex.org/W3033038061', 'https://openalex.org/W3035524453', 'https://openalex.org/W2750248772', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W6760822226', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W2146444479', 'https://openalex.org/W6771917389', 'https://openalex.org/W6769196770', 'https://openalex.org/W6755207826', 'https://openalex.org/W6675022971', 'https://openalex.org/W4254197176', 'https://openalex.org/W3011411500', 'https://openalex.org/W6947929050', 'https://openalex.org/W2962850167', 'https://openalex.org/W2933138175', 'https://openalex.org/W2127141656', 'https://openalex.org/W2946417913', 'https://openalex.org/W6745117592', 'https://openalex.org/W3008525923', 'https://openalex.org/W3100270690', 'https://openalex.org/W2963618559', 'https://openalex.org/W2998532468', 'https://openalex.org/W2758785877', 'https://openalex.org/W2953190524', 'https://openalex.org/W3144810982', 'https://openalex.org/W2963341956', 'https://openalex.org/W2988736778', 'https://openalex.org/W4297786395', 'https://openalex.org/W3148040514', 'https://openalex.org/W2964121744', 'https://openalex.org/W3125709657', 'https://openalex.org/W3013571468', 'https://openalex.org/W1522301498', 'https://openalex.org/W2100768664', 'https://openalex.org/W2970971581', 'https://openalex.org/W2896457183', 'https://openalex.org/W3126816608', 'https://openalex.org/W4295312788', 'https://openalex.org/W2979476256', 'https://openalex.org/W3099782249', 'https://openalex.org/W4287824654', 'https://openalex.org/W2996383576', 'https://openalex.org/W2950180292', 'https://openalex.org/W3025035610', 'https://openalex.org/W2972943112', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963799213', 'https://openalex.org/W2101234009', 'https://openalex.org/W2883725317']",2021-05-13
https://openalex.org/W3144639365,https://doi.org/10.48550/arxiv.2103.16809,Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-stage Sequence-to-Sequence Training,"Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation.","['https://openalex.org/W3167167480', 'https://openalex.org/W2120847449', 'https://openalex.org/W2894821115', 'https://openalex.org/W3034420534', 'https://openalex.org/W3099078140', 'https://openalex.org/W2964308564', 'https://openalex.org/W2785608393', 'https://openalex.org/W2899877258', 'https://openalex.org/W3098557217', 'https://openalex.org/W2517513811', 'https://openalex.org/W2964307104', 'https://openalex.org/W2330979245', 'https://openalex.org/W3142644187', 'https://openalex.org/W2950542544', 'https://openalex.org/W3095169545', 'https://openalex.org/W2040587156', 'https://openalex.org/W2899361462', 'https://openalex.org/W2963609956', 'https://openalex.org/W2120605154', 'https://openalex.org/W2187089797', 'https://openalex.org/W2972999331', 'https://openalex.org/W3014201970', 'https://openalex.org/W2086796102', 'https://openalex.org/W3015719316', 'https://openalex.org/W3101689408', 'https://openalex.org/W3163573274', 'https://openalex.org/W3008297462', 'https://openalex.org/W2527729766', 'https://openalex.org/W2161736993', 'https://openalex.org/W2901254300', 'https://openalex.org/W3025680351', 'https://openalex.org/W2774848319', 'https://openalex.org/W3047107405', 'https://openalex.org/W2941094131', 'https://openalex.org/W2938833595', 'https://openalex.org/W3015241559', 'https://openalex.org/W3096939667', 'https://openalex.org/W2901997113']",2021-03-31
https://openalex.org/W3004402693,https://doi.org/10.48550/arxiv.2002.00198,Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data,"Emotional voice conversion aims to convert the spectrum and prosody to change the emotional patterns of speech, while preserving the speaker identity and linguistic content. Many studies require parallel speech data between different emotional patterns, which is not practical in real life. Moreover, they often model the conversion of fundamental frequency (F0) with a simple linear transform. As F0 is a key aspect of intonation that is hierarchical in nature, we believe that it is more adequate to model F0 in different temporal scales by using wavelet transform. We propose a CycleGAN network to find an optimal pseudo pair from non-parallel training data by learning forward and inverse mappings simultaneously using adversarial and cycle-consistency losses. We also study the use of continuous wavelet transform (CWT) to decompose F0 into ten temporal scales, that describes speech prosody at different time resolution, for effective F0 conversion. Experimental results show that our proposed framework outperforms the baselines both in objective and subjective evaluations.","['https://openalex.org/W2518172956', 'https://openalex.org/W2404839462', 'https://openalex.org/W2899361462', 'https://openalex.org/W2937579788', 'https://openalex.org/W2564591810', 'https://openalex.org/W1977362459', 'https://openalex.org/W2077801020', 'https://openalex.org/W2151262064', 'https://openalex.org/W2157412983', 'https://openalex.org/W3012970712', 'https://openalex.org/W2136922672', 'https://openalex.org/W2984809863', 'https://openalex.org/W2748654097', 'https://openalex.org/W2511640485', 'https://openalex.org/W2086796102', 'https://openalex.org/W2902070858', 'https://openalex.org/W2574092538', 'https://openalex.org/W2150791533', 'https://openalex.org/W2185573909', 'https://openalex.org/W2517513811', 'https://openalex.org/W113106864', 'https://openalex.org/W2600829178', 'https://openalex.org/W2148846882', 'https://openalex.org/W2013996527', 'https://openalex.org/W2785608393', 'https://openalex.org/W3012498027', 'https://openalex.org/W2127589467', 'https://openalex.org/W2099471712', 'https://openalex.org/W2608338293', 'https://openalex.org/W2774848319', 'https://openalex.org/W1988189165', 'https://openalex.org/W2883743124', 'https://openalex.org/W2120605154', 'https://openalex.org/W2964024144', 'https://openalex.org/W2941094131', 'https://openalex.org/W2396025094', 'https://openalex.org/W2807668517', 'https://openalex.org/W2532494225', 'https://openalex.org/W2018338387', 'https://openalex.org/W2137376927', 'https://openalex.org/W2950542544', 'https://openalex.org/W2330979245', 'https://openalex.org/W2618574778', 'https://openalex.org/W2889064624', 'https://openalex.org/W2889544410', 'https://openalex.org/W3012404734', 'https://openalex.org/W2149425161', 'https://openalex.org/W2787378487', 'https://openalex.org/W2395980997', 'https://openalex.org/W2740504963', 'https://openalex.org/W2785978752', 'https://openalex.org/W2161736993', 'https://openalex.org/W3008297462', 'https://openalex.org/W3003883423', 'https://openalex.org/W2105160541', 'https://openalex.org/W2793479148', 'https://openalex.org/W2493477844', 'https://openalex.org/W52175521', 'https://openalex.org/W2040587156', 'https://openalex.org/W2911340057', 'https://openalex.org/W2962793481', 'https://openalex.org/W2949785681', 'https://openalex.org/W2162295204']",2020-02-01
https://openalex.org/W3163573274,https://doi.org/10.1109/icassp39728.2021.9413391,Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset,"Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.","['https://openalex.org/W6781784828', 'https://openalex.org/W3015841875', 'https://openalex.org/W6773966927', 'https://openalex.org/W2471520273', 'https://openalex.org/W2885005742', 'https://openalex.org/W2146334809', 'https://openalex.org/W3008297462', 'https://openalex.org/W6683855838', 'https://openalex.org/W2040587156', 'https://openalex.org/W2511640485', 'https://openalex.org/W2517513811', 'https://openalex.org/W2938833595', 'https://openalex.org/W2883743124', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W2899361462', 'https://openalex.org/W2149628368', 'https://openalex.org/W3098557217', 'https://openalex.org/W1966797434', 'https://openalex.org/W3016151052', 'https://openalex.org/W2105160541', 'https://openalex.org/W2937154351', 'https://openalex.org/W2120605154', 'https://openalex.org/W2086796102', 'https://openalex.org/W2941094131', 'https://openalex.org/W6711854987', 'https://openalex.org/W6634507583', 'https://openalex.org/W3096939667', 'https://openalex.org/W2759925408', 'https://openalex.org/W2962896155', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W6772230580', 'https://openalex.org/W3014201970', 'https://openalex.org/W2161736993', 'https://openalex.org/W1581458799', 'https://openalex.org/W2187089797', 'https://openalex.org/W3044380931', 'https://openalex.org/W3168542456', 'https://openalex.org/W2080119116', 'https://openalex.org/W4295731579', 'https://openalex.org/W2319003035', 'https://openalex.org/W2714549561', 'https://openalex.org/W2963272440', 'https://openalex.org/W2998249245', 'https://openalex.org/W2963927338', 'https://openalex.org/W2396025094', 'https://openalex.org/W3047107405', 'https://openalex.org/W3006108364', 'https://openalex.org/W2608338293', 'https://openalex.org/W2794490148']",2021-05-13
https://openalex.org/W2979790850,https://doi.org/10.5772/intechopen.89849,The Theory behind Controllable Expressive Speech Synthesis: A Cross-Disciplinary Approach,"As part of the Human-Computer Interaction field, Expressive speech synthesis is a very rich domain as it requires knowledge in areas such as machine learning, signal processing, sociology, and psychology. In this chapter, we will focus mostly on the technical side. From the recording of expressive speech to its modeling, the reader will have an overview of the main paradigms used in this field, through some of the most prominent systems and methods. We explain how speech can be represented and encoded with audio features. We present a history of the main methods of Text-to-Speech synthesis: concatenative, parametric and statistical parametric speech synthesis. Finally, we focus on the last one, with the last techniques modeling Text-to-Speech synthesis as a sequence-to-sequence problem. This enables the use of Deep Learning blocks such as Convolutional and Recurrent Neural Networks as well as Attention Mechanism. The last part of the chapter intends to assemble the different aspects of the theory and summarize the concepts.","['https://openalex.org/W2503527153', 'https://openalex.org/W1966797434', 'https://openalex.org/W2149628368', 'https://openalex.org/W2787552263', 'https://openalex.org/W4233973055', 'https://openalex.org/W2103184652', 'https://openalex.org/W2399733683', 'https://openalex.org/W2150791533', 'https://openalex.org/W140611833', 'https://openalex.org/W2102003408', 'https://openalex.org/W2963609956', 'https://openalex.org/W2402539796', 'https://openalex.org/W1867691547', 'https://openalex.org/W2129142580', 'https://openalex.org/W2598638573', 'https://openalex.org/W2750499125', 'https://openalex.org/W2327501763', 'https://openalex.org/W7048060829', 'https://openalex.org/W2103636088', 'https://openalex.org/W2904351664', 'https://openalex.org/W2969658393', 'https://openalex.org/W2972699445', 'https://openalex.org/W2557283755', 'https://openalex.org/W2949382160', 'https://openalex.org/W3140968660', 'https://openalex.org/W2238694451', 'https://openalex.org/W2538626945']",2019-12-02
https://openalex.org/W2511640485,https://doi.org/10.1109/icis.2016.7550889,Emotional voice conversion using deep neural networks with MCC and F0 features,"An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.","['https://openalex.org/W2105160541', 'https://openalex.org/W2126143605', 'https://openalex.org/W1972420736', 'https://openalex.org/W6696767757', 'https://openalex.org/W6634569661', 'https://openalex.org/W2161736993', 'https://openalex.org/W2077801020', 'https://openalex.org/W6711832335', 'https://openalex.org/W2119655735', 'https://openalex.org/W6712665021', 'https://openalex.org/W2089624917', 'https://openalex.org/W6712742799', 'https://openalex.org/W6691118191', 'https://openalex.org/W6601280771', 'https://openalex.org/W6712645575', 'https://openalex.org/W2093450784', 'https://openalex.org/W6606266484', 'https://openalex.org/W2164241094', 'https://openalex.org/W2120605154', 'https://openalex.org/W2123003832', 'https://openalex.org/W2152205330', 'https://openalex.org/W2129142580', 'https://openalex.org/W6603446537', 'https://openalex.org/W6635180523', 'https://openalex.org/W2123771434', 'https://openalex.org/W6712560600', 'https://openalex.org/W6711854987', 'https://openalex.org/W1588037970', 'https://openalex.org/W2398826216', 'https://openalex.org/W2294351487', 'https://openalex.org/W4229870863', 'https://openalex.org/W2251981003', 'https://openalex.org/W2396025094', 'https://openalex.org/W2399915284', 'https://openalex.org/W1575712594', 'https://openalex.org/W2395700867', 'https://openalex.org/W2400683008', 'https://openalex.org/W155193863', 'https://openalex.org/W31448757', 'https://openalex.org/W84813673', 'https://openalex.org/W2400092632']",2016-06-01
https://openalex.org/W2517513811,https://doi.org/10.21437/interspeech.2016-1053,Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional Voice Conversion,,"['https://openalex.org/W2162295204', 'https://openalex.org/W2152627593', 'https://openalex.org/W251642908', 'https://openalex.org/W1988189165', 'https://openalex.org/W1976725440', 'https://openalex.org/W2600829178', 'https://openalex.org/W95152782', 'https://openalex.org/W2109938215', 'https://openalex.org/W2404839462', 'https://openalex.org/W1540083112', 'https://openalex.org/W1509691205', 'https://openalex.org/W2040587156', 'https://openalex.org/W2131062138', 'https://openalex.org/W2008199296']",2016-08-29
https://openalex.org/W3025035610,https://doi.org/10.48550/arxiv.2005.08575,Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation,"For self-supervised speech processing, it is crucial to use pretrained models as speech representation extractors. In recent works, increasing the size of the model has been utilized in acoustic model training in order to achieve better performance. In this paper, we propose Audio ALBERT, a lite version of the self-supervised speech representation model. We use the representations with two downstream tasks, speaker identification, and phoneme classification. We show that Audio ALBERT is capable of achieving competitive performance with those huge models in the downstream tasks while utilizing 91\% fewer parameters. Moreover, we use some simple probing models to measure how much the information of the speaker and phoneme is encoded in latent representations. In probing experiments, we find that the latent representations encode richer information of both phoneme and speaker than that of the last layer.","['https://openalex.org/W2996428491', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963347649', 'https://openalex.org/W2982223350', 'https://openalex.org/W1494198834', 'https://openalex.org/W3035524453', 'https://openalex.org/W2996383576', 'https://openalex.org/W2962739339', 'https://openalex.org/W2965046076', 'https://openalex.org/W2981363336', 'https://openalex.org/W3015265920', 'https://openalex.org/W2972808286', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963928591', 'https://openalex.org/W2970597249', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W3005680577', 'https://openalex.org/W2972889948', 'https://openalex.org/W2945785363', 'https://openalex.org/W2908510526', 'https://openalex.org/W2965373594', 'https://openalex.org/W3015412285', 'https://openalex.org/W2948947170', 'https://openalex.org/W2187089797']",2020-05-18
https://openalex.org/W2793479148,https://doi.org/10.1016/j.specom.2018.03.002,Investigating different representations for modeling and controlling multiple emotions in DNN-based speech synthesis,,"['https://openalex.org/W2490433728', 'https://openalex.org/W2082557737', 'https://openalex.org/W2129703931', 'https://openalex.org/W6728756453', 'https://openalex.org/W1975079546', 'https://openalex.org/W2510638983', 'https://openalex.org/W2167510900', 'https://openalex.org/W6696843773', 'https://openalex.org/W6712239235', 'https://openalex.org/W6675407435', 'https://openalex.org/W7037751743', 'https://openalex.org/W4395958177', 'https://openalex.org/W6723457072', 'https://openalex.org/W2020024436', 'https://openalex.org/W1993267429', 'https://openalex.org/W6744965914', 'https://openalex.org/W2012086895', 'https://openalex.org/W2471520273', 'https://openalex.org/W6660929288', 'https://openalex.org/W2102146461', 'https://openalex.org/W176245312', 'https://openalex.org/W6682918086', 'https://openalex.org/W6733202620', 'https://openalex.org/W6734596677', 'https://openalex.org/W6713438222', 'https://openalex.org/W6638081404', 'https://openalex.org/W6610843619', 'https://openalex.org/W2085013480', 'https://openalex.org/W4251158933', 'https://openalex.org/W2102003408', 'https://openalex.org/W2042691334', 'https://openalex.org/W2059450962', 'https://openalex.org/W2154920538', 'https://openalex.org/W2584521635', 'https://openalex.org/W2916164548', 'https://openalex.org/W3140706748', 'https://openalex.org/W2531662911', 'https://openalex.org/W2494654097', 'https://openalex.org/W304834817', 'https://openalex.org/W2402539796', 'https://openalex.org/W2093632031', 'https://openalex.org/W2915722758', 'https://openalex.org/W3150689196', 'https://openalex.org/W2609210086', 'https://openalex.org/W2592040118', 'https://openalex.org/W2042360461', 'https://openalex.org/W2759925408', 'https://openalex.org/W2394662942', 'https://openalex.org/W1783473872', 'https://openalex.org/W1490382748', 'https://openalex.org/W2564036667', 'https://openalex.org/W2100495367', 'https://openalex.org/W2294797155']",2018-03-15
https://openalex.org/W3015669407,https://doi.org/10.48550/arxiv.2004.03782,Multi-Target Emotional Voice Conversion With Neural Vocoders,"Emotional voice conversion (EVC) is one way to generate expressive synthetic speech. Previous approaches mainly focused on modeling one-to-one mapping, i.e., conversion from one emotional state to another emotional state, with Mel-cepstral vocoders. In this paper, we investigate building a multi-target EVC (MTEVC) architecture, which combines a deep bidirectional long-short term memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic posteriorgrams (PPGs) containing rich linguistic information are incorporated into the conversion model as auxiliary input features, which boost the conversion performance. To leverage the advantages of the newly emerged neural vocoders, we investigate the conditional WaveNet and flow-based WaveNet (FloWaveNet) as speech generators. The vocoders take in additional speaker information and emotion information as auxiliary features and are trained with a multi-speaker and multi-emotion speech corpus. Objective metrics and subjective evaluation of the experimental results verify the efficacy of the proposed MTEVC architecture for EVC.","['https://openalex.org/W2161736993', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964243274', 'https://openalex.org/W2973108288', 'https://openalex.org/W2120847449', 'https://openalex.org/W2109938215', 'https://openalex.org/W2517513811', 'https://openalex.org/W2963139417', 'https://openalex.org/W2889092828', 'https://openalex.org/W2802968248', 'https://openalex.org/W2793479148', 'https://openalex.org/W2471520273', 'https://openalex.org/W2526785520', 'https://openalex.org/W113106864', 'https://openalex.org/W2158504378', 'https://openalex.org/W2409550820', 'https://openalex.org/W2805038428', 'https://openalex.org/W2039800941', 'https://openalex.org/W3044514286', 'https://openalex.org/W2749651610', 'https://openalex.org/W2795109282', 'https://openalex.org/W2899882692', 'https://openalex.org/W2794490148', 'https://openalex.org/W2518172956', 'https://openalex.org/W2748654097', 'https://openalex.org/W2888922217', 'https://openalex.org/W2519091744', 'https://openalex.org/W2049686551', 'https://openalex.org/W3127686677', 'https://openalex.org/W3099078140', 'https://openalex.org/W2745595539', 'https://openalex.org/W2185573909', 'https://openalex.org/W226412370', 'https://openalex.org/W2162295204']",2020-04-08
https://openalex.org/W2077801020,https://doi.org/10.5923/j.ajsp.20120205.06,GMM-Based Emotional Voice Conversion Using Spectrum and Prosody Features,"We propose Gaussian Mixture Model (GMM)-based emotional voice conversion using spectrum and prosody features. In recent years, speech recognition and synthesis techniques have been developed, and an emotional voice conversion technique is required for synthesizing more expressive voices. The common emotional conversion was based on transformation of neutral prosody to emotional prosody by using huge speech corpus. In this paper, we convert a neutral voice to an emotional voice using GMMs. GMM-based spectrum conversion is widely used to modify non linguistic information such as voice characteristics while keeping linguistic information unchanged. Because the conventional method converts either prosody or voice quality (spectrum), some emotions are not converted well. In our method, both prosody and voice quality are used for converting a neutral voice to an emotional voice, and it is able to obtain more expressive voices in comparison with conventional methods, such as prosody or spectrum conversion.","['https://openalex.org/W2129343891', 'https://openalex.org/W1558063916', 'https://openalex.org/W1530874618', 'https://openalex.org/W2007023536', 'https://openalex.org/W2152205330', 'https://openalex.org/W23142961', 'https://openalex.org/W198589515', 'https://openalex.org/W2162295204', 'https://openalex.org/W43755971', 'https://openalex.org/W2164241094', 'https://openalex.org/W2407110532', 'https://openalex.org/W2395052932', 'https://openalex.org/W2120605154', 'https://openalex.org/W2026575276', 'https://openalex.org/W1495108311']",2012-12-01
https://openalex.org/W3161223924,https://doi.org/10.1109/icassp39728.2021.9414922,Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining,"Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.","['https://openalex.org/W2327501763', 'https://openalex.org/W6769457076', 'https://openalex.org/W2936774411', 'https://openalex.org/W2766219058', 'https://openalex.org/W2019116789', 'https://openalex.org/W2889201969', 'https://openalex.org/W6759393636', 'https://openalex.org/W2251599843', 'https://openalex.org/W2964117975', 'https://openalex.org/W3015267417', 'https://openalex.org/W3015412890', 'https://openalex.org/W3097414768', 'https://openalex.org/W3095552229', 'https://openalex.org/W3049038774', 'https://openalex.org/W2977838803', 'https://openalex.org/W6752437113', 'https://openalex.org/W1494198834', 'https://openalex.org/W2077302143', 'https://openalex.org/W2963446094', 'https://openalex.org/W2097550833', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973133192', 'https://openalex.org/W6640440542', 'https://openalex.org/W2963288440', 'https://openalex.org/W2894164357', 'https://openalex.org/W2972584841', 'https://openalex.org/W2971351151', 'https://openalex.org/W2963250244', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3096249532', 'https://openalex.org/W2982223350', 'https://openalex.org/W2981458636', 'https://openalex.org/W2803609229', 'https://openalex.org/W4289564002', 'https://openalex.org/W1936920915', 'https://openalex.org/W3092630929', 'https://openalex.org/W2806429264', 'https://openalex.org/W2917128112', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979722627', 'https://openalex.org/W3099944122']",2021-05-13
https://openalex.org/W3015719316,https://doi.org/10.1109/icassp40776.2020.9053255,Emotional Voice Conversion Using Multitask Learning with Text-To-Speech,"Voice conversion (VC) is a task that alters the voice of a person to suit different styles while conserving the linguistic content. Previous state-of-the-art technology used in VC was based on the sequence-to-sequence (seq2seq) model, which could lose linguistic information. There was an attempt to overcome this problem using textual supervision; however, this required explicit alignment, and therefore the benefit of using seq2seq model was lost. In this study, a voice converter that utilizes multitask learning with text-to-speech (TTS) is presented. By using multitask learning, VC is expected to capture linguistic information and preserve the training stability. This method does not require explicit alignment for capturing abundant text information. Experiments on VC were performed on a male-Korean-emotional-text-speech dataset to convert the neutral voice to emotional voice. It was shown that multitask learning helps to preserve the linguistic content.","['https://openalex.org/W6754925833', 'https://openalex.org/W2740504963', 'https://openalex.org/W2950542544', 'https://openalex.org/W2883743124', 'https://openalex.org/W6762492278', 'https://openalex.org/W2888922217', 'https://openalex.org/W2795109282', 'https://openalex.org/W6746238782', 'https://openalex.org/W2010910318', 'https://openalex.org/W186286710', 'https://openalex.org/W1509691205', 'https://openalex.org/W2157412983', 'https://openalex.org/W2897353073', 'https://openalex.org/W6679434410', 'https://openalex.org/W6736996214', 'https://openalex.org/W2901254300', 'https://openalex.org/W2120605154', 'https://openalex.org/W2013996527', 'https://openalex.org/W2964243274', 'https://openalex.org/W2911929781', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099078140', 'https://openalex.org/W4295731579', 'https://openalex.org/W2963927338', 'https://openalex.org/W2770743791', 'https://openalex.org/W2963609956', 'https://openalex.org/W2892140764', 'https://openalex.org/W3102905810', 'https://openalex.org/W2133564696', 'https://openalex.org/W2944711403']",2020-04-09
https://openalex.org/W2973138167,https://doi.org/10.21437/interspeech.2019-2386,Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks,,"['https://openalex.org/W2137581220', 'https://openalex.org/W2972334330', 'https://openalex.org/W2067470376', 'https://openalex.org/W2049686551', 'https://openalex.org/W226412370', 'https://openalex.org/W95152782', 'https://openalex.org/W586270867', 'https://openalex.org/W1597458585', 'https://openalex.org/W2120605154', 'https://openalex.org/W2949117887', 'https://openalex.org/W2293135480', 'https://openalex.org/W1965912016', 'https://openalex.org/W1963623641', 'https://openalex.org/W2118980630', 'https://openalex.org/W1976725440', 'https://openalex.org/W1540083112', 'https://openalex.org/W2171883843', 'https://openalex.org/W2095705004', 'https://openalex.org/W2964121744']",2019-09-13
https://openalex.org/W2810914326,https://doi.org/10.48550/arxiv.1806.09514,The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems,"In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.","['https://openalex.org/W2603532017', 'https://openalex.org/W2030931454', 'https://openalex.org/W2787552263', 'https://openalex.org/W2803193013', 'https://openalex.org/W2018613638', 'https://openalex.org/W2329093554', 'https://openalex.org/W2214475686', 'https://openalex.org/W95152782', 'https://openalex.org/W2225419387', 'https://openalex.org/W2511640485', 'https://openalex.org/W2342475039', 'https://openalex.org/W2471520273', 'https://openalex.org/W1588037970', 'https://openalex.org/W1916142944', 'https://openalex.org/W2759244429', 'https://openalex.org/W2949382160', 'https://openalex.org/W2777302760', 'https://openalex.org/W2963609956']",2018-06-25
https://openalex.org/W2972366998,https://doi.org/10.21437/interspeech.2019-2512,A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective,,"['https://openalex.org/W2137581220', 'https://openalex.org/W2131774270', 'https://openalex.org/W2972334330', 'https://openalex.org/W1976725440', 'https://openalex.org/W226412370', 'https://openalex.org/W95152782', 'https://openalex.org/W2040587156', 'https://openalex.org/W2054941444', 'https://openalex.org/W2949382160', 'https://openalex.org/W2049686551', 'https://openalex.org/W1540083112', 'https://openalex.org/W2095705004', 'https://openalex.org/W2745634895', 'https://openalex.org/W2152627593', 'https://openalex.org/W2949117887', 'https://openalex.org/W2964121744', 'https://openalex.org/W2120605154']",2019-09-13
https://openalex.org/W2148846882,https://doi.org/10.1016/j.specom.2008.09.006,Data-driven emotion conversion in spoken English,,"['https://openalex.org/W2102267302', 'https://openalex.org/W87461617', 'https://openalex.org/W2160464533', 'https://openalex.org/W2038662549', 'https://openalex.org/W1505264225', 'https://openalex.org/W2112390707', 'https://openalex.org/W2149425161', 'https://openalex.org/W1589602221', 'https://openalex.org/W2091738194', 'https://openalex.org/W2123003832', 'https://openalex.org/W6635180523', 'https://openalex.org/W6713420752', 'https://openalex.org/W4390911403', 'https://openalex.org/W6681821128', 'https://openalex.org/W162654330', 'https://openalex.org/W2156142001', 'https://openalex.org/W2161736993', 'https://openalex.org/W34295773', 'https://openalex.org/W1514737389', 'https://openalex.org/W2154920538', 'https://openalex.org/W344150399', 'https://openalex.org/W6630838124', 'https://openalex.org/W204428907', 'https://openalex.org/W1540078082', 'https://openalex.org/W2161135987', 'https://openalex.org/W39968598', 'https://openalex.org/W6676465519', 'https://openalex.org/W1598253959', 'https://openalex.org/W133559434', 'https://openalex.org/W6611528966', 'https://openalex.org/W943204654', 'https://openalex.org/W3216401400', 'https://openalex.org/W2110589840', 'https://openalex.org/W1588037970', 'https://openalex.org/W4285719527', 'https://openalex.org/W2403168145', 'https://openalex.org/W2150791533', 'https://openalex.org/W2135934764', 'https://openalex.org/W1512429158']",2008-09-27
https://openalex.org/W2114925438,https://doi.org/10.1109/icassp.2016.7472652,End-to-end text-dependent speaker verification,"In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal ""Ok Google"" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications Like ours that require highly accurate, easy-to-maintain systems with a small footprint.","['https://openalex.org/W6607467483', 'https://openalex.org/W2040105822', 'https://openalex.org/W2004497042', 'https://openalex.org/W2039057510', 'https://openalex.org/W6670093721', 'https://openalex.org/W2046056978', 'https://openalex.org/W6600129882', 'https://openalex.org/W2406392101', 'https://openalex.org/W6638670064', 'https://openalex.org/W2941689885', 'https://openalex.org/W6713401928', 'https://openalex.org/W2395750323', 'https://openalex.org/W1734538896', 'https://openalex.org/W2150769028', 'https://openalex.org/W6713727690', 'https://openalex.org/W2107638917', 'https://openalex.org/W2041823554', 'https://openalex.org/W1485222997', 'https://openalex.org/W6713097126', 'https://openalex.org/W2395985692', 'https://openalex.org/W6675122589', 'https://openalex.org/W6684859321', 'https://openalex.org/W6696934422', 'https://openalex.org/W2064675550', 'https://openalex.org/W6640036494', 'https://openalex.org/W2078953162', 'https://openalex.org/W2293634267', 'https://openalex.org/W2078169166', 'https://openalex.org/W182365161', 'https://openalex.org/W2402072010', 'https://openalex.org/W4297801963', 'https://openalex.org/W2100664567', 'https://openalex.org/W2168231600', 'https://openalex.org/W2402195372', 'https://openalex.org/W2406312423', 'https://openalex.org/W1831449718', 'https://openalex.org/W1904365287', 'https://openalex.org/W2950344723', 'https://openalex.org/W3128241']",2016-03-01
https://openalex.org/W3096457008,https://doi.org/10.21437/interspeech.2020-1854,Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS,,"['https://openalex.org/W3101882441', 'https://openalex.org/W2963091184', 'https://openalex.org/W2794490148', 'https://openalex.org/W2966387353', 'https://openalex.org/W3006913184']",2020-10-25
https://openalex.org/W2115098197,https://doi.org/10.1109/icassp.2002.5743729,Yet Another Algorithm for Pitch Tracking,"In this paper, we present a pitch detection algorithm that is extremely robust for both high quality and telephone speech. The kernel method for this algorithm is the ""NCCF or Normalized Cross Correlation"" reported by David Talkin [1]. Major innovations include: processing of the original acoustic signal and a nonlinearly processed version of the signal to partially restore very weak F0 components; intelligent peak picking to select multiple F0 candidates and assign merit factors; and, incorporation of highly rohust pitch contours obtained from smoothed versions of low frequency portions of spectrograms. Dynamic programming is used to find the ""best"" pitch track among all the candidates, using both local and transition costs. We evaluated our algorithm using the Keele pitch extraction reference database as ""ground truth"" for both ""high quality"" and ""telephone"" speech. For both types of speech, the error rates obtained are lower than the lowest reported in the literature.","['https://openalex.org/W6639375475', 'https://openalex.org/W6640922965', 'https://openalex.org/W2142608504', 'https://openalex.org/W2088632109', 'https://openalex.org/W1519346462', 'https://openalex.org/W6600956376', 'https://openalex.org/W2114992418', 'https://openalex.org/W2120549396', 'https://openalex.org/W23669922', 'https://openalex.org/W108550785', 'https://openalex.org/W2084562624', 'https://openalex.org/W2073409426', 'https://openalex.org/W1508193566', 'https://openalex.org/W2131774383', 'https://openalex.org/W1886421314', 'https://openalex.org/W185021866', 'https://openalex.org/W86348706', 'https://openalex.org/W2128413412', 'https://openalex.org/W1834472093', 'https://openalex.org/W1571176132', 'https://openalex.org/W1962687433', 'https://openalex.org/W2069501481']",2002-05-01
https://openalex.org/W3020570669,https://doi.org/10.48550/arxiv.2004.11284,Unsupervised Speech Decomposition via Triple Information Bottleneck,"Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain. In this paper, we propose SpeechSplit, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechSplit is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels. Our code is publicly available at https://github.com/auspicious3000/SpeechSplit.","['https://openalex.org/W2532494225', 'https://openalex.org/W2981934523', 'https://openalex.org/W2889061305', 'https://openalex.org/W2891813127', 'https://openalex.org/W2887264325', 'https://openalex.org/W2963539064', 'https://openalex.org/W1532854728', 'https://openalex.org/W1557133539', 'https://openalex.org/W2963927338', 'https://openalex.org/W2050675057', 'https://openalex.org/W2107740512', 'https://openalex.org/W3101882441', 'https://openalex.org/W2962896155', 'https://openalex.org/W2805669069', 'https://openalex.org/W2774848319', 'https://openalex.org/W2130821326', 'https://openalex.org/W2972970915', 'https://openalex.org/W3015805741', 'https://openalex.org/W2952269766', 'https://openalex.org/W2945544731', 'https://openalex.org/W2131062138', 'https://openalex.org/W2939131199', 'https://openalex.org/W2972659941', 'https://openalex.org/W2980286501', 'https://openalex.org/W2962752582', 'https://openalex.org/W2795783309', 'https://openalex.org/W2126289105', 'https://openalex.org/W2928664166', 'https://openalex.org/W2964135678', 'https://openalex.org/W3100378519', 'https://openalex.org/W1522301498', 'https://openalex.org/W2946809691', 'https://openalex.org/W2963767194', 'https://openalex.org/W2972812066', 'https://openalex.org/W2949281321', 'https://openalex.org/W2963830550', 'https://openalex.org/W2151626637', 'https://openalex.org/W2584393883', 'https://openalex.org/W2972667718']",2020-04-23
https://openalex.org/W2899361462,https://doi.org/10.21437/interspeech.2019-2878,Nonparallel Emotional Speech Conversion,"We propose a nonparallel data-driven emotional speech conversion method. It enables the transfer of emotion-related characteristics of a speech signal while preserving the speaker's identity and linguistic content. Most existing approaches require parallel data and time alignment, which is not available in most real applications. We achieve nonparallel training based on an unsupervised style transfer technique, which learns a translation model between two distributions instead of a deterministic one-to-one mapping between paired examples. The conversion model consists of an encoder and a decoder for each emotion domain. We assume that the speech signal can be decomposed into an emotion-invariant content code and an emotion-related style code in latent space. Emotion conversion is performed by extracting and recombining the content code of the source speech and the style code of the target emotion. We tested our method on a nonparallel corpora with four emotions. Both subjective and objective evaluations show the effectiveness of our approach.","['https://openalex.org/W2502312327', 'https://openalex.org/W2625297138', 'https://openalex.org/W2962793481', 'https://openalex.org/W2567070169', 'https://openalex.org/W2937579788', 'https://openalex.org/W2963539064', 'https://openalex.org/W2514440523', 'https://openalex.org/W2898853208', 'https://openalex.org/W2749651610', 'https://openalex.org/W2962896155', 'https://openalex.org/W2475287302', 'https://openalex.org/W2963890275', 'https://openalex.org/W2049686551', 'https://openalex.org/W175750906', 'https://openalex.org/W2603777577', 'https://openalex.org/W2099471712', 'https://openalex.org/W2936451900', 'https://openalex.org/W2892278723', 'https://openalex.org/W2576309025', 'https://openalex.org/W1970945271', 'https://openalex.org/W1588037970', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962993399', 'https://openalex.org/W2883743124', 'https://openalex.org/W2519091744', 'https://openalex.org/W2890402938', 'https://openalex.org/W2962947361', 'https://openalex.org/W2146334809', 'https://openalex.org/W2471520273', 'https://openalex.org/W2964069186', 'https://openalex.org/W1974843131', 'https://openalex.org/W2161736993', 'https://openalex.org/W2034277951', 'https://openalex.org/W2803193013', 'https://openalex.org/W2963970792', 'https://openalex.org/W2747744257']",2019-09-13
https://openalex.org/W3095930733,https://doi.org/10.21437/interspeech.2020-1647,Nonparallel Emotional Speech Conversion Using VAE-GAN,,"['https://openalex.org/W2502312327', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964121744', 'https://openalex.org/W2576309025', 'https://openalex.org/W2605762339', 'https://openalex.org/W2123771434', 'https://openalex.org/W2532494225', 'https://openalex.org/W2887264325', 'https://openalex.org/W2949382160', 'https://openalex.org/W2474531669', 'https://openalex.org/W2608338293', 'https://openalex.org/W3015719316', 'https://openalex.org/W2099471712', 'https://openalex.org/W1926768285', 'https://openalex.org/W2890402938', 'https://openalex.org/W2883743124', 'https://openalex.org/W2077801020', 'https://openalex.org/W2553897675', 'https://openalex.org/W2511640485', 'https://openalex.org/W2899361462']",2020-10-25
https://openalex.org/W2938833595,https://doi.org/10.1109/icassp.2019.8683865,Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion,"Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.","['https://openalex.org/W6602504493', 'https://openalex.org/W6713858911', 'https://openalex.org/W2184310502', 'https://openalex.org/W6751998403', 'https://openalex.org/W6696843773', 'https://openalex.org/W1576227399', 'https://openalex.org/W2517513811', 'https://openalex.org/W2613654124', 'https://openalex.org/W2747914378', 'https://openalex.org/W2513051195', 'https://openalex.org/W6600618385', 'https://openalex.org/W2808514527', 'https://openalex.org/W2746192915', 'https://openalex.org/W2106421426', 'https://openalex.org/W2740504963', 'https://openalex.org/W2886195082', 'https://openalex.org/W6604530167', 'https://openalex.org/W3142087749', 'https://openalex.org/W1570629387', 'https://openalex.org/W6683488333', 'https://openalex.org/W2018338387', 'https://openalex.org/W6679436768', 'https://openalex.org/W6736356763', 'https://openalex.org/W2157331557', 'https://openalex.org/W6679434410', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W15106243', 'https://openalex.org/W2407110532', 'https://openalex.org/W2604184139', 'https://openalex.org/W2133564696', 'https://openalex.org/W2805307277', 'https://openalex.org/W2605141709', 'https://openalex.org/W113106864', 'https://openalex.org/W2294797155', 'https://openalex.org/W2130942839', 'https://openalex.org/W63438600', 'https://openalex.org/W2158504378', 'https://openalex.org/W2519091744', 'https://openalex.org/W2674088828']",2019-04-17
https://openalex.org/W3167167480,https://doi.org/10.48550/arxiv.2105.14762,"Emotional Voice Conversion: Theory, Databases and ESD","In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database (ESD) that addresses the increasing research need. With this paper, the ESD database is now made available to the research community. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the ESD database. This paper provides a reference study on ESD in conjunction with its release.","['https://openalex.org/W2081837280', 'https://openalex.org/W2330979245', 'https://openalex.org/W2803098682', 'https://openalex.org/W2929315483', 'https://openalex.org/W2902070858', 'https://openalex.org/W2998249245', 'https://openalex.org/W2512449761', 'https://openalex.org/W2963539064', 'https://openalex.org/W33829787', 'https://openalex.org/W2969658393', 'https://openalex.org/W2105160541', 'https://openalex.org/W2759244429', 'https://openalex.org/W2889064624', 'https://openalex.org/W2025905516', 'https://openalex.org/W3099078140', 'https://openalex.org/W2511640485', 'https://openalex.org/W3150994787', 'https://openalex.org/W2972724712', 'https://openalex.org/W2127589467', 'https://openalex.org/W2998572311', 'https://openalex.org/W2532494225', 'https://openalex.org/W3089767655', 'https://openalex.org/W3045354608', 'https://openalex.org/W2120605154', 'https://openalex.org/W2090777335', 'https://openalex.org/W2924594913', 'https://openalex.org/W2162295204', 'https://openalex.org/W2810914326', 'https://openalex.org/W2941094131', 'https://openalex.org/W2937343983', 'https://openalex.org/W2510170536', 'https://openalex.org/W2951004968', 'https://openalex.org/W2972313557', 'https://openalex.org/W3008297462', 'https://openalex.org/W2514440523', 'https://openalex.org/W2123003832', 'https://openalex.org/W2185573909', 'https://openalex.org/W2085013480', 'https://openalex.org/W3146550708', 'https://openalex.org/W2785978752', 'https://openalex.org/W2803193013', 'https://openalex.org/W2165894799', 'https://openalex.org/W1978136968', 'https://openalex.org/W2129703931', 'https://openalex.org/W2107860279', 'https://openalex.org/W59075858', 'https://openalex.org/W3015434413', 'https://openalex.org/W3014201970', 'https://openalex.org/W2997399314', 'https://openalex.org/W3047855478', 'https://openalex.org/W2937870435', 'https://openalex.org/W3136699727', 'https://openalex.org/W198298781', 'https://openalex.org/W2165857685', 'https://openalex.org/W2035962301', 'https://openalex.org/W3144639365', 'https://openalex.org/W2149628368', 'https://openalex.org/W3015699566', 'https://openalex.org/W113106864', 'https://openalex.org/W2066090536', 'https://openalex.org/W2972359262', 'https://openalex.org/W2080119116', 'https://openalex.org/W2984809863', 'https://openalex.org/W2527729766', 'https://openalex.org/W1974843131', 'https://openalex.org/W2121387787', 'https://openalex.org/W3012404734', 'https://openalex.org/W2973138167', 'https://openalex.org/W3044380931', 'https://openalex.org/W2807668517', 'https://openalex.org/W2576309025', 'https://openalex.org/W2964069186', 'https://openalex.org/W2039800941', 'https://openalex.org/W2911340057', 'https://openalex.org/W2972334330', 'https://openalex.org/W2994715919', 'https://openalex.org/W2774848319', 'https://openalex.org/W388732865', 'https://openalex.org/W2748654097', 'https://openalex.org/W2168692779', 'https://openalex.org/W3155308523', 'https://openalex.org/W3015241559', 'https://openalex.org/W2106421426', 'https://openalex.org/W3047107405', 'https://openalex.org/W2517513811', 'https://openalex.org/W2085662862', 'https://openalex.org/W2972366998', 'https://openalex.org/W2962793481', 'https://openalex.org/W2787378487', 'https://openalex.org/W3160329778', 'https://openalex.org/W3008691130', 'https://openalex.org/W2119929864', 'https://openalex.org/W3025044797', 'https://openalex.org/W3046670326', 'https://openalex.org/W2899877258', 'https://openalex.org/W2995401804', 'https://openalex.org/W2889112744', 'https://openalex.org/W2965685620', 'https://openalex.org/W3045447113', 'https://openalex.org/W2051217765', 'https://openalex.org/W2787928077', 'https://openalex.org/W2404839462', 'https://openalex.org/W3047769339', 'https://openalex.org/W3163573274', 'https://openalex.org/W3130305523', 'https://openalex.org/W2130290868', 'https://openalex.org/W2608338293', 'https://openalex.org/W2167510900', 'https://openalex.org/W3101689408', 'https://openalex.org/W1588037970', 'https://openalex.org/W2568428461', 'https://openalex.org/W2158504378', 'https://openalex.org/W3082130377', 'https://openalex.org/W2077801020', 'https://openalex.org/W2966387353', 'https://openalex.org/W2889544410', 'https://openalex.org/W2796495654', 'https://openalex.org/W2040587156', 'https://openalex.org/W2899361462', 'https://openalex.org/W2936412271', 'https://openalex.org/W2161736993', 'https://openalex.org/W2342475039', 'https://openalex.org/W2086796102', 'https://openalex.org/W2950542544', 'https://openalex.org/W3015841875', 'https://openalex.org/W3080698515', 'https://openalex.org/W2190260761', 'https://openalex.org/W2753840835', 'https://openalex.org/W3118242141', 'https://openalex.org/W3015669407', 'https://openalex.org/W2478838513']",2021-05-31
https://openalex.org/W3015241559,https://doi.org/10.1109/icassp40776.2020.9054579,Stargan for Emotional Speech Conversion: Validated by Data Augmentation of End-To-End Emotion Recognition,"In this paper, we propose an adversarial network implementation for speech emotion conversion as a data augmentation method, validated by a multi-class speech affect recognition task. In our setting, we do not assume the availability of parallel data, and we additionally make it a priority to exploit as much as possible the available training data by adopting a cycle-consistent, class-conditional generative adversarial network with an auxiliary domain classifier. Our generated samples are valuable for data augmentation, achieving a corresponding 2% and 6% absolute increase in Micro- and MacroF1 compared to the baseline in a 3-class classification paradigm using a deep, end-to-end network. We finally perform a human perception evaluation of the samples, through which we conclude that our samples are indicative of their target emotion, albeit showing a tendency for confusion in cases where the emotional attribute of valence and arousal are inconsistent.","['https://openalex.org/W2921059071', 'https://openalex.org/W1993267429', 'https://openalex.org/W2740504963', 'https://openalex.org/W2517513811', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963767194', 'https://openalex.org/W2963539064', 'https://openalex.org/W1947595512', 'https://openalex.org/W6761745632', 'https://openalex.org/W2748699435', 'https://openalex.org/W2984972638', 'https://openalex.org/W2936764189', 'https://openalex.org/W6746638498', 'https://openalex.org/W2032254851', 'https://openalex.org/W2972640480', 'https://openalex.org/W4254718357', 'https://openalex.org/W2936774411', 'https://openalex.org/W6635996524', 'https://openalex.org/W6735913928', 'https://openalex.org/W2123771434', 'https://openalex.org/W2471520273', 'https://openalex.org/W2146334809', 'https://openalex.org/W2099471712', 'https://openalex.org/W1598253959', 'https://openalex.org/W4295521014', 'https://openalex.org/W2770173563', 'https://openalex.org/W2962879692', 'https://openalex.org/W2937977583', 'https://openalex.org/W567437002', 'https://openalex.org/W4320013936']",2020-04-09
https://openalex.org/W3098557217,https://doi.org/10.1109/taslp.2020.3038524,An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning,"Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.","['https://openalex.org/W2169579015', 'https://openalex.org/W2800788625', 'https://openalex.org/W2122159244', 'https://openalex.org/W2144351953', 'https://openalex.org/W6713985435', 'https://openalex.org/W2154920538', 'https://openalex.org/W6677570743', 'https://openalex.org/W6711854987', 'https://openalex.org/W2086796102', 'https://openalex.org/W2116261113', 'https://openalex.org/W2064675550', 'https://openalex.org/W2517513811', 'https://openalex.org/W1689711448', 'https://openalex.org/W6739901393', 'https://openalex.org/W6679434410', 'https://openalex.org/W2057609679', 'https://openalex.org/W6697270646', 'https://openalex.org/W2179428711', 'https://openalex.org/W2052274528', 'https://openalex.org/W2171019095', 'https://openalex.org/W2518172956', 'https://openalex.org/W6712160772', 'https://openalex.org/W2005438552', 'https://openalex.org/W1991402032', 'https://openalex.org/W2137983211', 'https://openalex.org/W2157412983', 'https://openalex.org/W2114543868', 'https://openalex.org/W6605232188', 'https://openalex.org/W2576309025', 'https://openalex.org/W2401207139', 'https://openalex.org/W6765779288', 'https://openalex.org/W2975414524', 'https://openalex.org/W3024752295', 'https://openalex.org/W2972394484', 'https://openalex.org/W6732637432', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963609956', 'https://openalex.org/W6729924827', 'https://openalex.org/W2511311723', 'https://openalex.org/W2157331557', 'https://openalex.org/W2899877258', 'https://openalex.org/W2897353073', 'https://openalex.org/W2767052532', 'https://openalex.org/W2963073614', 'https://openalex.org/W6756102363', 'https://openalex.org/W6737778391', 'https://openalex.org/W1902237438', 'https://openalex.org/W6602386084', 'https://openalex.org/W2161476805', 'https://openalex.org/W2327501763', 'https://openalex.org/W2135832479', 'https://openalex.org/W2402356521', 'https://openalex.org/W2785608393', 'https://openalex.org/W1965912016', 'https://openalex.org/W2152974894', 'https://openalex.org/W6693024853', 'https://openalex.org/W1552314771', 'https://openalex.org/W6676626839', 'https://openalex.org/W2963403924', 'https://openalex.org/W1562777581', 'https://openalex.org/W2802935216', 'https://openalex.org/W2964195110', 'https://openalex.org/W2515028311', 'https://openalex.org/W2963539064', 'https://openalex.org/W2889329491', 'https://openalex.org/W6840412704', 'https://openalex.org/W2996414377', 'https://openalex.org/W2588445447', 'https://openalex.org/W6685748340', 'https://openalex.org/W6637576829', 'https://openalex.org/W1572730534', 'https://openalex.org/W6606229746', 'https://openalex.org/W6631184489', 'https://openalex.org/W2123299109', 'https://openalex.org/W2145130307', 'https://openalex.org/W2963035245', 'https://openalex.org/W6601306257', 'https://openalex.org/W2512087624', 'https://openalex.org/W2404839462', 'https://openalex.org/W2785978752', 'https://openalex.org/W2532202862', 'https://openalex.org/W3007908230', 'https://openalex.org/W2911340057', 'https://openalex.org/W2972939746', 'https://openalex.org/W2973115941', 'https://openalex.org/W2749651610', 'https://openalex.org/W2786868129', 'https://openalex.org/W2963522141', 'https://openalex.org/W2803229097', 'https://openalex.org/W1509691205', 'https://openalex.org/W6696767757', 'https://openalex.org/W2021986246', 'https://openalex.org/W3096864844', 'https://openalex.org/W2963808252', 'https://openalex.org/W2651834199', 'https://openalex.org/W2938583109', 'https://openalex.org/W2156142001', 'https://openalex.org/W1969728648', 'https://openalex.org/W6691687657', 'https://openalex.org/W2972999331', 'https://openalex.org/W2331222812', 'https://openalex.org/W6769702557', 'https://openalex.org/W2013996527', 'https://openalex.org/W2150769028', 'https://openalex.org/W6777157395', 'https://openalex.org/W2964069186', 'https://openalex.org/W2153057929', 'https://openalex.org/W2151262064', 'https://openalex.org/W6635216677', 'https://openalex.org/W2947196194', 'https://openalex.org/W2507912506', 'https://openalex.org/W6746801104', 'https://openalex.org/W2962896155', 'https://openalex.org/W2518312472', 'https://openalex.org/W2574092538', 'https://openalex.org/W2806000759', 'https://openalex.org/W2532494225', 'https://openalex.org/W2120847449', 'https://openalex.org/W2802455234', 'https://openalex.org/W2797877891', 'https://openalex.org/W2796495654', 'https://openalex.org/W2067234399', 'https://openalex.org/W2129082420', 'https://openalex.org/W2428180336', 'https://openalex.org/W1963778986', 'https://openalex.org/W2096564043', 'https://openalex.org/W2121387787', 'https://openalex.org/W2903365642', 'https://openalex.org/W2941094131', 'https://openalex.org/W2889064624', 'https://openalex.org/W2972359262', 'https://openalex.org/W6748409065', 'https://openalex.org/W6936113694', 'https://openalex.org/W2963300588', 'https://openalex.org/W2800289214', 'https://openalex.org/W6756159577', 'https://openalex.org/W2981087920', 'https://openalex.org/W2936802426', 'https://openalex.org/W6720742747', 'https://openalex.org/W2745896134', 'https://openalex.org/W2601846999', 'https://openalex.org/W6603838645', 'https://openalex.org/W3026777299', 'https://openalex.org/W2962780374', 'https://openalex.org/W2802650881', 'https://openalex.org/W3034600949', 'https://openalex.org/W2798951647', 'https://openalex.org/W2962793481', 'https://openalex.org/W2984809863', 'https://openalex.org/W7008786231', 'https://openalex.org/W2887511658', 'https://openalex.org/W2803714261', 'https://openalex.org/W2888932932', 'https://openalex.org/W2963767194', 'https://openalex.org/W7002029315', 'https://openalex.org/W2989855043', 'https://openalex.org/W6734815144', 'https://openalex.org/W2890300789', 'https://openalex.org/W2973108288', 'https://openalex.org/W2972313557', 'https://openalex.org/W2938836228', 'https://openalex.org/W2049686551', 'https://openalex.org/W2889544410', 'https://openalex.org/W2126143605', 'https://openalex.org/W2145892079', 'https://openalex.org/W6770488218', 'https://openalex.org/W6745117592', 'https://openalex.org/W2962850167', 'https://openalex.org/W2964243274', 'https://openalex.org/W6750298367', 'https://openalex.org/W2954386831', 'https://openalex.org/W3100270690', 'https://openalex.org/W6757028937', 'https://openalex.org/W3012404734', 'https://openalex.org/W2963800363', 'https://openalex.org/W6745992979', 'https://openalex.org/W3012970712', 'https://openalex.org/W2997400081', 'https://openalex.org/W3076645148', 'https://openalex.org/W2888858245', 'https://openalex.org/W2787133659', 'https://openalex.org/W6761382815', 'https://openalex.org/W2902070858', 'https://openalex.org/W6730095352', 'https://openalex.org/W2936002583', 'https://openalex.org/W3008297462', 'https://openalex.org/W3025680351', 'https://openalex.org/W3096939667', 'https://openalex.org/W2885820941', 'https://openalex.org/W3025182306', 'https://openalex.org/W3048768961', 'https://openalex.org/W6781784828', 'https://openalex.org/W6771070734', 'https://openalex.org/W6783904828', 'https://openalex.org/W6687506355', 'https://openalex.org/W2892734764', 'https://openalex.org/W6775882176', 'https://openalex.org/W2972970915', 'https://openalex.org/W6777869919', 'https://openalex.org/W3006777338', 'https://openalex.org/W2901254300', 'https://openalex.org/W2168810263', 'https://openalex.org/W2019849101', 'https://openalex.org/W2077801020', 'https://openalex.org/W6635180523', 'https://openalex.org/W6712849908', 'https://openalex.org/W1968689546', 'https://openalex.org/W2395380967', 'https://openalex.org/W1599112982', 'https://openalex.org/W2168531172', 'https://openalex.org/W6712034360', 'https://openalex.org/W3096524539', 'https://openalex.org/W6640963894', 'https://openalex.org/W2972659941', 'https://openalex.org/W3015434413', 'https://openalex.org/W2804998325', 'https://openalex.org/W2922283382', 'https://openalex.org/W6762931180', 'https://openalex.org/W2752796333', 'https://openalex.org/W2889061305', 'https://openalex.org/W2972849140', 'https://openalex.org/W2020834817', 'https://openalex.org/W2143116775', 'https://openalex.org/W2156477760', 'https://openalex.org/W1524403222', 'https://openalex.org/W6680012447', 'https://openalex.org/W2239901959', 'https://openalex.org/W2013608223', 'https://openalex.org/W2125760250', 'https://openalex.org/W2158291955', 'https://openalex.org/W2123003832', 'https://openalex.org/W2167873188', 'https://openalex.org/W2083806749', 'https://openalex.org/W2747744257', 'https://openalex.org/W6752378368', 'https://openalex.org/W6720691552', 'https://openalex.org/W6783889628', 'https://openalex.org/W3002433751', 'https://openalex.org/W2102148524', 'https://openalex.org/W1574447377', 'https://openalex.org/W2107860279', 'https://openalex.org/W6777853906', 'https://openalex.org/W2039240409', 'https://openalex.org/W3015338123', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972309641', 'https://openalex.org/W2963175743', 'https://openalex.org/W2998498479', 'https://openalex.org/W2972789651', 'https://openalex.org/W2972910332', 'https://openalex.org/W2972882294', 'https://openalex.org/W6740753539', 'https://openalex.org/W2011916518', 'https://openalex.org/W6767111847', 'https://openalex.org/W278779762', 'https://openalex.org/W1975163393', 'https://openalex.org/W1526513105', 'https://openalex.org/W2269051851', 'https://openalex.org/W2093107730', 'https://openalex.org/W2603810172', 'https://openalex.org/W3034089333', 'https://openalex.org/W6600593648', 'https://openalex.org/W6605781699', 'https://openalex.org/W2128446656', 'https://openalex.org/W2120605154', 'https://openalex.org/W6603221913', 'https://openalex.org/W2514429423', 'https://openalex.org/W2105160541', 'https://openalex.org/W2076268407', 'https://openalex.org/W1985690171', 'https://openalex.org/W2017425464', 'https://openalex.org/W2114659828', 'https://openalex.org/W1977362459', 'https://openalex.org/W2484196375', 'https://openalex.org/W2154986869', 'https://openalex.org/W2093898552', 'https://openalex.org/W6694691911', 'https://openalex.org/W2136166660', 'https://openalex.org/W6636924704', 'https://openalex.org/W2171717581', 'https://openalex.org/W2060554399', 'https://openalex.org/W1523372075', 'https://openalex.org/W2786387703', 'https://openalex.org/W6675938391', 'https://openalex.org/W2130086727', 'https://openalex.org/W2795109282', 'https://openalex.org/W2791322592', 'https://openalex.org/W6603559778', 'https://openalex.org/W6714062273', 'https://openalex.org/W2996286887', 'https://openalex.org/W4385245566', 'https://openalex.org/W2613904329', 'https://openalex.org/W2758785877', 'https://openalex.org/W1608375737', 'https://openalex.org/W3146803896', 'https://openalex.org/W2577886607', 'https://openalex.org/W2116955094', 'https://openalex.org/W3125709657', 'https://openalex.org/W2962882868', 'https://openalex.org/W3047107405', 'https://openalex.org/W4293398859', 'https://openalex.org/W3142619648', 'https://openalex.org/W4288337064', 'https://openalex.org/W2963927338', 'https://openalex.org/W2970006822', 'https://openalex.org/W2111550316', 'https://openalex.org/W2964308564', 'https://openalex.org/W2395980997', 'https://openalex.org/W3027343259', 'https://openalex.org/W87473629', 'https://openalex.org/W155193863', 'https://openalex.org/W15066456', 'https://openalex.org/W2907162034', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964281804', 'https://openalex.org/W2608338293', 'https://openalex.org/W3099078140', 'https://openalex.org/W2768959015', 'https://openalex.org/W3095990227', 'https://openalex.org/W3049756574', 'https://openalex.org/W3143596294', 'https://openalex.org/W3102628737', 'https://openalex.org/W4301206121', 'https://openalex.org/W3143465836', 'https://openalex.org/W2099471712', 'https://openalex.org/W33829787', 'https://openalex.org/W2766812927', 'https://openalex.org/W2135029798', 'https://openalex.org/W2396025094', 'https://openalex.org/W3090528452', 'https://openalex.org/W3094635600', 'https://openalex.org/W2898654681', 'https://openalex.org/W2107740512', 'https://openalex.org/W2176804518', 'https://openalex.org/W2553897675', 'https://openalex.org/W3100378519', 'https://openalex.org/W2804078698', 'https://openalex.org/W3082130377', 'https://openalex.org/W2527729766', 'https://openalex.org/W2250668798', 'https://openalex.org/W3096567388', 'https://openalex.org/W1588266896', 'https://openalex.org/W2949382160', 'https://openalex.org/W4320013936', 'https://openalex.org/W3015669407', 'https://openalex.org/W3005862564', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963889406', 'https://openalex.org/W3016151052', 'https://openalex.org/W59075858', 'https://openalex.org/W2970997853', 'https://openalex.org/W4289305009', 'https://openalex.org/W1588037970', 'https://openalex.org/W2404109308', 'https://openalex.org/W2294246205', 'https://openalex.org/W3004402693', 'https://openalex.org/W2963981733', 'https://openalex.org/W2408526116', 'https://openalex.org/W3102326097', 'https://openalex.org/W3102905810', 'https://openalex.org/W2971074500', 'https://openalex.org/W2277581165', 'https://openalex.org/W142801778', 'https://openalex.org/W79241043', 'https://openalex.org/W2473388484', 'https://openalex.org/W2937579788', 'https://openalex.org/W3025044797', 'https://openalex.org/W2964307104', 'https://openalex.org/W2774848319', 'https://openalex.org/W2955616469', 'https://openalex.org/W2964265128', 'https://openalex.org/W3124274650', 'https://openalex.org/W2557915412', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963784072', 'https://openalex.org/W2605762339', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964167449', 'https://openalex.org/W2970351109', 'https://openalex.org/W3090052686', 'https://openalex.org/W2294351487', 'https://openalex.org/W2963890275', 'https://openalex.org/W156398200', 'https://openalex.org/W3154451338', 'https://openalex.org/W2402019113', 'https://openalex.org/W3034420534', 'https://openalex.org/W129217914', 'https://openalex.org/W2519091744', 'https://openalex.org/W4298580827', 'https://openalex.org/W1741199358', 'https://openalex.org/W1520370180', 'https://openalex.org/W1959608418', 'https://openalex.org/W2591927543', 'https://openalex.org/W2733416080', 'https://openalex.org/W2994715919', 'https://openalex.org/W4295731579', 'https://openalex.org/W2397499194', 'https://openalex.org/W3101689408', 'https://openalex.org/W2962919088', 'https://openalex.org/W2967606780', 'https://openalex.org/W2210497074', 'https://openalex.org/W3034058691', 'https://openalex.org/W2408732577', 'https://openalex.org/W2264797740', 'https://openalex.org/W3168542456', 'https://openalex.org/W95152782', 'https://openalex.org/W2963330667', 'https://openalex.org/W1658960529', 'https://openalex.org/W2273671322', 'https://openalex.org/W3020975377']",2020-11-17
https://openalex.org/W2527729766,https://doi.org/10.7488/ds/1495,SUPERSEDED - CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit,"# SUPERSEDED - This item has been replaced by the one which can be found at https://doi.org/10.7488/ds/1994 . # This CSTR VCTK Corpus (Centre for Speech Technology Voice Cloning Toolkit) includes speech data uttered by 109 native speakers of English with various accents. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald &amp; Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf . All speech data were recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, downsampled to 48 kHz based on STPK, and manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies. The file was previously available on the CSTR website, and was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf .",[],2016-01-01
https://openalex.org/W2083905766,https://doi.org/10.3758/bf03206502,The role of fundamental frequency in signaling linguistic stress and affect: Evidence for a dissociation,,"['https://openalex.org/W2015552356', 'https://openalex.org/W6703853445', 'https://openalex.org/W2089065822', 'https://openalex.org/W2000635945', 'https://openalex.org/W2035193374', 'https://openalex.org/W1571176132', 'https://openalex.org/W2191668891', 'https://openalex.org/W6724489487', 'https://openalex.org/W2005867010', 'https://openalex.org/W4301735423', 'https://openalex.org/W2077963559', 'https://openalex.org/W1983248018', 'https://openalex.org/W2166593103', 'https://openalex.org/W1991273024', 'https://openalex.org/W4236805417', 'https://openalex.org/W2033147189', 'https://openalex.org/W7005875397', 'https://openalex.org/W2021189082', 'https://openalex.org/W6982642911', 'https://openalex.org/W1986595082', 'https://openalex.org/W18218877', 'https://openalex.org/W2054171872', 'https://openalex.org/W1969349521', 'https://openalex.org/W2337917701', 'https://openalex.org/W2069220364', 'https://openalex.org/W2007977421', 'https://openalex.org/W6723115273', 'https://openalex.org/W1992705424', 'https://openalex.org/W6672945055', 'https://openalex.org/W6634495960', 'https://openalex.org/W2071941900', 'https://openalex.org/W6761236203', 'https://openalex.org/W2333513337', 'https://openalex.org/W2106439488', 'https://openalex.org/W2002103888', 'https://openalex.org/W2112890390', 'https://openalex.org/W1998981462', 'https://openalex.org/W7066828851', 'https://openalex.org/W2016645383', 'https://openalex.org/W2003547693', 'https://openalex.org/W2084288097', 'https://openalex.org/W2080878121', 'https://openalex.org/W1994976477', 'https://openalex.org/W2056242429', 'https://openalex.org/W1554626009', 'https://openalex.org/W2054305348', 'https://openalex.org/W2092145781', 'https://openalex.org/W2552302753', 'https://openalex.org/W2076186713', 'https://openalex.org/W2113396990', 'https://openalex.org/W1981362683', 'https://openalex.org/W35432380', 'https://openalex.org/W2504290514', 'https://openalex.org/W2046134482', 'https://openalex.org/W2091387799', 'https://openalex.org/W1540664512', 'https://openalex.org/W2128281523', 'https://openalex.org/W2564295779', 'https://openalex.org/W2065546066', 'https://openalex.org/W1532845219', 'https://openalex.org/W2090733448', 'https://openalex.org/W2492019858', 'https://openalex.org/W2338084601', 'https://openalex.org/W1575306796', 'https://openalex.org/W4302556758', 'https://openalex.org/W1970555440', 'https://openalex.org/W4230230169', 'https://openalex.org/W2081234301']",1995-01-01
https://openalex.org/W3087287714,https://doi.org/10.48550/arxiv.2009.08474,Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis,"This paper proposes a hierarchical generative model with a multi-grained latent variable to synthesize expressive speech. In recent years, fine-grained latent variables are introduced into the text-to-speech synthesis that enable the fine control of the prosody and speaking styles of synthesized speech. However, the naturalness of speech degrades when these latent variables are obtained by sampling from the standard Gaussian prior. To solve this problem, we propose a novel framework for modeling the fine-grained latent variables, considering the dependence on an input text, a hierarchical linguistic structure, and a temporal structure of latent variables. This framework consists of a multi-grained variational autoencoder, a conditional prior, and a multi-level auto-regressive latent converter to obtain the different time-resolution latent variables and sample the finer-level latent variables from the coarser-level ones by taking into account the input text. Experimental results indicate an appropriate method of sampling fine-grained latent variables without the reference signal at the synthesis stage. Our proposed framework also provides the controllability of speaking style in an entire utterance.","['https://openalex.org/W2794490148', 'https://openalex.org/W2294797155', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2964060510', 'https://openalex.org/W2962691331', 'https://openalex.org/W2964138190', 'https://openalex.org/W2795109282', 'https://openalex.org/W2064675550', 'https://openalex.org/W2903739847', 'https://openalex.org/W2792995953', 'https://openalex.org/W2106792148', 'https://openalex.org/W648786980', 'https://openalex.org/W3015440759', 'https://openalex.org/W1579853615', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963341956', 'https://openalex.org/W2785364623', 'https://openalex.org/W2904459034', 'https://openalex.org/W1959608418', 'https://openalex.org/W2403471241', 'https://openalex.org/W2884607399', 'https://openalex.org/W2102003408']",2020-09-17
https://openalex.org/W3025044797,https://doi.org/10.48550/arxiv.2005.07025,Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion,"Emotional voice conversion aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. The prior studies on emotional voice conversion are mostly carried out under the assumption that emotion is speaker-dependent. We consider that there is a common code between speakers for emotional expression in a spoken language, therefore, a speaker-independent mapping between emotional states is possible. In this paper, we propose a speaker-independent emotional voice conversion framework, that can convert anyone's emotion without the need for parallel data. We propose a VAW-GAN based encoder-decoder structure to learn the spectrum and prosody mapping. We perform prosody conversion by using continuous wavelet transform (CWT) to model the temporal dependencies. We also investigate the use of F0 as an additional input to the decoder to improve emotion conversion performance. Experiments show that the proposed speaker-independent framework achieves competitive results for both seen and unseen speakers.","['https://openalex.org/W2123864036', 'https://openalex.org/W2148846882', 'https://openalex.org/W2511640485', 'https://openalex.org/W2793479148', 'https://openalex.org/W2899361462', 'https://openalex.org/W1966797434', 'https://openalex.org/W2517513811', 'https://openalex.org/W2938833595', 'https://openalex.org/W2081837280', 'https://openalex.org/W2945478979', 'https://openalex.org/W1537481423', 'https://openalex.org/W2051217765', 'https://openalex.org/W3004402693', 'https://openalex.org/W2105160541', 'https://openalex.org/W2040587156', 'https://openalex.org/W2136922672', 'https://openalex.org/W3012970712', 'https://openalex.org/W2127589467', 'https://openalex.org/W2889112744', 'https://openalex.org/W2077801020', 'https://openalex.org/W2889544410', 'https://openalex.org/W2883743124', 'https://openalex.org/W2810914326', 'https://openalex.org/W2080119116', 'https://openalex.org/W113106864', 'https://openalex.org/W2330979245', 'https://openalex.org/W2972366998', 'https://openalex.org/W2161736993', 'https://openalex.org/W2608338293', 'https://openalex.org/W2941094131', 'https://openalex.org/W3015805741', 'https://openalex.org/W2889064624', 'https://openalex.org/W2946809691', 'https://openalex.org/W2785978752', 'https://openalex.org/W2997399314', 'https://openalex.org/W2973135352', 'https://openalex.org/W2785608393', 'https://openalex.org/W2120605154', 'https://openalex.org/W1988189165', 'https://openalex.org/W1994198923', 'https://openalex.org/W2984809863']",2020-05-13
https://openalex.org/W95152782,,The CMU Arctic speech databases.,"The CMU Arctic databases designed for the purpose of speech synthesis research. These single speaker speech databases have been carefully recorded under studio conditions and consist of approximately 1200 phonetically balanced English utterances. In addition to wavefiles, the databases provide complete support for the Festival Speech Synthesis System, including pre-built voices that may be used as is. The entire package is distributed as free software, without restriction on commercial or non-commercial use. 1.",['https://openalex.org/W183794703'],2004-01-01
https://openalex.org/W2471520273,https://doi.org/10.1587/transinf.2015edp7457,WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications,"A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.","['https://openalex.org/W2142747467', 'https://openalex.org/W2082321072', 'https://openalex.org/W4245284779', 'https://openalex.org/W1572730534', 'https://openalex.org/W2049686551', 'https://openalex.org/W4235716345', 'https://openalex.org/W2428180336', 'https://openalex.org/W2164764235', 'https://openalex.org/W125998882', 'https://openalex.org/W2077446647', 'https://openalex.org/W2130081501', 'https://openalex.org/W2131062138', 'https://openalex.org/W2030351702', 'https://openalex.org/W1563089615', 'https://openalex.org/W2012086895', 'https://openalex.org/W2094130157', 'https://openalex.org/W773905565', 'https://openalex.org/W86348706', 'https://openalex.org/W2064948657', 'https://openalex.org/W2091425152', 'https://openalex.org/W1975079546', 'https://openalex.org/W2097645910', 'https://openalex.org/W2579537926', 'https://openalex.org/W2018458134', 'https://openalex.org/W2109189270', 'https://openalex.org/W1498609987', 'https://openalex.org/W2052002522', 'https://openalex.org/W2404204166', 'https://openalex.org/W1926768285', 'https://openalex.org/W198589515', 'https://openalex.org/W2032030908', 'https://openalex.org/W2150619194', 'https://openalex.org/W1563460361', 'https://openalex.org/W2122627441', 'https://openalex.org/W2129142580', 'https://openalex.org/W2556247670']",2016-01-01
https://openalex.org/W3144988954,https://doi.org/10.48550/arxiv.2104.00436,Expressive Text-to-Speech using Style Tag,"As recent text-to-speech (TTS) systems have been rapidly improved in speech quality and generation speed, many researchers now focus on a more challenging issue: expressive TTS. To control speaking styles, existing expressive TTS models use categorical style index or reference speech as style input. In this work, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that utilizes a style tag written in natural language. Using a style-tagged TTS dataset and a pre-trained language model, we modeled the relationship between linguistic embedding and speaking style domain, which enables our model to work even with style tags unseen during training. As style tag is written in natural language, it can control speaking style in a more intuitive, interpretable, and scalable way compared with style index or reference speech. In addition, in terms of model architecture, we propose an efficient non-autoregressive (NAR) TTS architecture with single-stage training. The experimental result shows that ST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech quality and expressiveness.","['https://openalex.org/W1522301498', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963927338', 'https://openalex.org/W2187089797', 'https://openalex.org/W2946200149', 'https://openalex.org/W2969658393', 'https://openalex.org/W2963272440', 'https://openalex.org/W3048173247', 'https://openalex.org/W2409550820', 'https://openalex.org/W2963341956', 'https://openalex.org/W2903739847', 'https://openalex.org/W2808706139', 'https://openalex.org/W2971193649', 'https://openalex.org/W2770743791', 'https://openalex.org/W2963139417', 'https://openalex.org/W1836465849', 'https://openalex.org/W3092028330', 'https://openalex.org/W3033913438', 'https://openalex.org/W3026874504', 'https://openalex.org/W2767052532', 'https://openalex.org/W2963685250', 'https://openalex.org/W2626778328', 'https://openalex.org/W2972699445', 'https://openalex.org/W3015338123', 'https://openalex.org/W2046033161', 'https://openalex.org/W3095695385', 'https://openalex.org/W2803193013', 'https://openalex.org/W3130016944', 'https://openalex.org/W2810914326']",2021-04-01
https://openalex.org/W3029422373,https://doi.org/10.48550/arxiv.2003.13325,Investigating Language Impact in Bilingual Approaches for Computational\n Language Documentation,"For endangered languages, data collection campaigns have to accommodate the\nchallenge that many of them are from oral tradition, and producing\ntranscriptions is costly. Therefore, it is fundamental to translate them into a\nwidely spoken language to ensure interpretability of the recordings. In this\npaper we investigate how the choice of translation language affects the\nposterior documentation work and potential automatic approaches which will work\non top of the produced bilingual corpus. For answering this question, we use\nthe MaSS multilingual speech corpus (Boito et al., 2020) for creating 56\nbilingual pairs that we apply to the task of low-resource unsupervised word\nsegmentation and alignment. Our results highlight that the choice of language\nfor translation influences the word segmentation performance, and that\ndifferent lexicons are learned by using different aligned translations. Lastly,\nthis paper proposes a hybrid approach for bilingual word segmentation,\ncombining boundary clues extracted from a non-parametric Bayesian model\n(Goldwater et al., 2009a) with the attentional word segmentation neural model\nfrom Godard et al. (2018). Our results suggest that incorporating these clues\ninto the neural models' input representation increases their translation and\nalignment quality, specially for challenging language pairs.\n","['https://openalex.org/W240478693', 'https://openalex.org/W2883972335', 'https://openalex.org/W3032598645', 'https://openalex.org/W2955019563', 'https://openalex.org/W2586602577', 'https://openalex.org/W2762715843', 'https://openalex.org/W2964308564', 'https://openalex.org/W2038542953', 'https://openalex.org/W175497273', 'https://openalex.org/W2111668269', 'https://openalex.org/W2126449874', 'https://openalex.org/W2126377586', 'https://openalex.org/W2586232309', 'https://openalex.org/W2884066185', 'https://openalex.org/W2173413395', 'https://openalex.org/W2895097770', 'https://openalex.org/W2515167330', 'https://openalex.org/W2582956876', 'https://openalex.org/W1557247526', 'https://openalex.org/W2808682925', 'https://openalex.org/W2122228338', 'https://openalex.org/W2101281673', 'https://openalex.org/W2963378435', 'https://openalex.org/W2963819008', 'https://openalex.org/W2101105183', 'https://openalex.org/W2980433540', 'https://openalex.org/W2466918907']",2020-03-30
https://openalex.org/W3100202343,https://doi.org/10.1109/icassp39728.2021.9414899,A Hierarchical Subspace Model for Language-Attuned Acoustic Unit Discovery,"In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.","['https://openalex.org/W6679792166', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786902352', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W6865393803', 'https://openalex.org/W2962693497', 'https://openalex.org/W2641832364', 'https://openalex.org/W6640963894', 'https://openalex.org/W1981706894', 'https://openalex.org/W6744702808', 'https://openalex.org/W2940544976', 'https://openalex.org/W2127498532', 'https://openalex.org/W2963620343', 'https://openalex.org/W6769196770', 'https://openalex.org/W3100270690', 'https://openalex.org/W6770596778', 'https://openalex.org/W6675022971', 'https://openalex.org/W2752796333', 'https://openalex.org/W6973666849', 'https://openalex.org/W2347098582', 'https://openalex.org/W2483390977', 'https://openalex.org/W3092791109', 'https://openalex.org/W2084534958', 'https://openalex.org/W6712757354', 'https://openalex.org/W2195354', 'https://openalex.org/W6712648922', 'https://openalex.org/W6731521493', 'https://openalex.org/W2973026522', 'https://openalex.org/W1522301498', 'https://openalex.org/W2100768664', 'https://openalex.org/W2401396251', 'https://openalex.org/W2972574141', 'https://openalex.org/W2963799213', 'https://openalex.org/W2996383576', 'https://openalex.org/W2401271873', 'https://openalex.org/W2762715843', 'https://openalex.org/W2995680346', 'https://openalex.org/W2572097499', 'https://openalex.org/W2786608204', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134670479']",2021-05-13
https://openalex.org/W2466918907,https://doi.org/10.18653/v1/n16-1109,An Attentional Model for Speech Translation Without Transcription,"Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, Trevor Cohn. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.","['https://openalex.org/W2143612262', 'https://openalex.org/W1994167151', 'https://openalex.org/W1885617648', 'https://openalex.org/W2038698865', 'https://openalex.org/W1973923101', 'https://openalex.org/W1902237438', 'https://openalex.org/W2107468211', 'https://openalex.org/W2327501763', 'https://openalex.org/W1556470778', 'https://openalex.org/W2184135559', 'https://openalex.org/W2064675550', 'https://openalex.org/W2063655091', 'https://openalex.org/W156805311', 'https://openalex.org/W1514535095', 'https://openalex.org/W2146939522', 'https://openalex.org/W2127141656', 'https://openalex.org/W2490163484', 'https://openalex.org/W854541894', 'https://openalex.org/W2148708890', 'https://openalex.org/W2090861223', 'https://openalex.org/W2963333747', 'https://openalex.org/W2595715041', 'https://openalex.org/W2168310280', 'https://openalex.org/W2964308564', 'https://openalex.org/W2095705004', 'https://openalex.org/W2170353620', 'https://openalex.org/W2006969979', 'https://openalex.org/W3012492057', 'https://openalex.org/W6908809', 'https://openalex.org/W2251001376', 'https://openalex.org/W1586532344', 'https://openalex.org/W2252212004', 'https://openalex.org/W2133564696', 'https://openalex.org/W2124807415', 'https://openalex.org/W2950178297', 'https://openalex.org/W2133444727', 'https://openalex.org/W2130942839', 'https://openalex.org/W2146502635', 'https://openalex.org/W2293858598']",2016-01-01
https://openalex.org/W2895097770,https://doi.org/10.21437/sltu.2018-43,Building Speech Recognition Systems for Language Documentation: The CoEDL Endangered Language Pipeline and Inference System (ELPIS),"Machine learning has revolutionized speech technologies for major world languages, but these technologies have generally not been available for the roughly 4,000 languages with populations of fewer than 10,000 speakers. This paper describes the development of ELPIS, a pipeline which language documentation workers with minimal computational experience can use to build their own speech recognition models, resulting in models being built for 16 languages from the Asia-Pacific region. ELPIS puts machine learning speech technologies within reach of people working with languages with scarce data, in a scalable way. This is impactful since it enables language communities to cross the digital divide, and speeds up language documentation. Complete automation of the process is not feasible for languages with small quantities of data and potentially large vocabularies. Hence our goal is not full automation, but rather to make a practical and effective workflow that integrates machine learning technologies.","['https://openalex.org/W1875231349', 'https://openalex.org/W2288974476', 'https://openalex.org/W1573465296', 'https://openalex.org/W2895097770', 'https://openalex.org/W2897388834', 'https://openalex.org/W1524333225', 'https://openalex.org/W2735115738', 'https://openalex.org/W763498075', 'https://openalex.org/W2803214681', 'https://openalex.org/W2953320089']",2018-08-29
https://openalex.org/W2808682925,https://doi.org/10.21437/interspeech.2018-1308,Unsupervised Word Segmentation from Speech with Attention,International audience,[],2018-08-28
https://openalex.org/W2401271873,,DEVELOPMENTS OF SWAHILI RESOURCES FOR AN AUTOMATIC SPEECH RECOGNITION SYSTEM,"This article describes our efforts to provide ASR resources for Swahili, a Bantu language spoken in a wide area of East Africa. We start with an introduction on the language situation, both at linguistic and digital level. Then, we report the selected strategies to develop a text corpus, a pronunciation dictionary and a speech corpus for this under-resourced language. We explore methodologies as crowdsourcing or collaborative transcription process. Besides, we take advantage of some linguistic characteristics of the language such as rich morphology or shared vocabulary with English to improve performance of our baseline Swahili ASR system in a broadcast speech transcription task. Index Terms — Swahili, under-resourced languages, automatic speech recognition, speech resources","['https://openalex.org/W1520886244', 'https://openalex.org/W2034898550', 'https://openalex.org/W1733091409', 'https://openalex.org/W2400053790', 'https://openalex.org/W2160733034', 'https://openalex.org/W3183153947', 'https://openalex.org/W2227885422', 'https://openalex.org/W2142869276', 'https://openalex.org/W2115915304', 'https://openalex.org/W132775144', 'https://openalex.org/W2183833033', 'https://openalex.org/W2185757438', 'https://openalex.org/W2587822382', 'https://openalex.org/W2108776455', 'https://openalex.org/W1512430500', 'https://openalex.org/W2167646176', 'https://openalex.org/W2079642659', 'https://openalex.org/W2625455096', 'https://openalex.org/W10477584', 'https://openalex.org/W2142937603', 'https://openalex.org/W1586578069', 'https://openalex.org/W1510207615', 'https://openalex.org/W2000982178', 'https://openalex.org/W593802557', 'https://openalex.org/W2154607443', 'https://openalex.org/W2124000149', 'https://openalex.org/W2151046578', 'https://openalex.org/W2026689784', 'https://openalex.org/W2114747788', 'https://openalex.org/W64751979', 'https://openalex.org/W2043302885', 'https://openalex.org/W2258861176', 'https://openalex.org/W2803359084', 'https://openalex.org/W1967952448']",2014-01-01
https://openalex.org/W2407614114,https://doi.org/10.21437/interspeech.2015-646,An evaluation of graph clustering methods for unsupervised term discovery,"Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.","['https://openalex.org/W66167291', 'https://openalex.org/W2164998314', 'https://openalex.org/W2047940964', 'https://openalex.org/W2059652594', 'https://openalex.org/W2060108852', 'https://openalex.org/W2025482506', 'https://openalex.org/W2151936673', 'https://openalex.org/W2168080440', 'https://openalex.org/W2786608204', 'https://openalex.org/W2052697931', 'https://openalex.org/W1539935047', 'https://openalex.org/W1991274470', 'https://openalex.org/W2132202037', 'https://openalex.org/W2089458547', 'https://openalex.org/W1967924372', 'https://openalex.org/W1580203226', 'https://openalex.org/W1545920196', 'https://openalex.org/W2036964623', 'https://openalex.org/W2072396742', 'https://openalex.org/W1983345514', 'https://openalex.org/W2018562712', 'https://openalex.org/W2401464865', 'https://openalex.org/W2124209874', 'https://openalex.org/W2062914951', 'https://openalex.org/W2294799344', 'https://openalex.org/W2131681506', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W229097380', 'https://openalex.org/W2078769636', 'https://openalex.org/W2109726592', 'https://openalex.org/W2114347655', 'https://openalex.org/W2095293504']",2015-09-06
https://openalex.org/W2111668269,,Nonparametric bayesian models of lexical acquisition,"The child learning language is faced with a daunting task: to learn to extract meaning from an apparently meaningless stream of sound. This thesis rests on the assumption that the kinds of generalizations the learner may make are constrained by the interaction of many different types of stochastic information, including innate learning biases. I use computational modeling to investigate how the generalizations made by unsupervised learners are affected by the sources of information available to them. I adopt a Bayesian perspective, where both internal representations of language and any learning biases are made explicit. 
I begin by presenting a generic framework for language modeling based on nonparametric Bayesian statistics, where model complexity grows with the amount of input data. This framework divides the work of modeling between a generator, which generates lexical items, and an adaptor, which generates frequencies for those items. Separating the two tasks in this way makes the framework flexible, allowing individual components to be easily modified. Standard sampling methods, such as Gibbs or Metropolis-Hastings sampling, may be used for inference. 
Using this framework, I develop several specific models to investigate questions related to morphological acquisition (identifying stems and suffixes) and word segmentation (identifying word boundaries in phonemically transcribed speech). I apply these models to English corpora of newspaper text and phonemically transcribed child-directed speech. With regard to morphology, my experiments provide evidence that morphological information is learned better from word types than from word tokens. With regard to word segmentation, my results indicate that assuming independence between words (as many previous models have done) leads to undersegmentation of the data. Accounting for local context improves segmentation markedly and yields better results than previous models. 
I conclude by describing briefly how the models presented here can be extended in order to account for a wider range of linguistic phenomena, including phonetic variability and the relationship between morphology and syntactic class.","['https://openalex.org/W1971245457', 'https://openalex.org/W2952343510', 'https://openalex.org/W2070586582', 'https://openalex.org/W1586407478', 'https://openalex.org/W2121614717', 'https://openalex.org/W2025653016', 'https://openalex.org/W3088640591', 'https://openalex.org/W1608707468', 'https://openalex.org/W2071402670', 'https://openalex.org/W2093254167', 'https://openalex.org/W2132827946', 'https://openalex.org/W2006586657', 'https://openalex.org/W2121416338', 'https://openalex.org/W2047603832', 'https://openalex.org/W3036063182', 'https://openalex.org/W2064262426', 'https://openalex.org/W2062289558', 'https://openalex.org/W3043710305', 'https://openalex.org/W1773803948', 'https://openalex.org/W1970887833', 'https://openalex.org/W2150144720', 'https://openalex.org/W2172709584', 'https://openalex.org/W2097609022', 'https://openalex.org/W1995991622', 'https://openalex.org/W1980862600', 'https://openalex.org/W1993803315', 'https://openalex.org/W2089484716', 'https://openalex.org/W2108962772', 'https://openalex.org/W2115867364', 'https://openalex.org/W570851560', 'https://openalex.org/W1596615073', 'https://openalex.org/W2142111485', 'https://openalex.org/W2132179433', 'https://openalex.org/W2091797506', 'https://openalex.org/W1978394996', 'https://openalex.org/W2150507172', 'https://openalex.org/W1982647060', 'https://openalex.org/W1952546279', 'https://openalex.org/W2007976151', 'https://openalex.org/W2420792277', 'https://openalex.org/W2126110252', 'https://openalex.org/W2059824090', 'https://openalex.org/W2108443500', 'https://openalex.org/W1495446613', 'https://openalex.org/W203298151', 'https://openalex.org/W1969158886', 'https://openalex.org/W2118727822', 'https://openalex.org/W2070554026', 'https://openalex.org/W2061430448', 'https://openalex.org/W2112861996', 'https://openalex.org/W2073610029', 'https://openalex.org/W2164151151', 'https://openalex.org/W2161800831', 'https://openalex.org/W2097341304', 'https://openalex.org/W2126736494', 'https://openalex.org/W2120636621', 'https://openalex.org/W1898626314', 'https://openalex.org/W1993768374', 'https://openalex.org/W2016429292', 'https://openalex.org/W2138309709', 'https://openalex.org/W2136549906', 'https://openalex.org/W1880262756', 'https://openalex.org/W1983311927', 'https://openalex.org/W2000781199', 'https://openalex.org/W2071772291', 'https://openalex.org/W2074546930', 'https://openalex.org/W2021031353', 'https://openalex.org/W2029948425', 'https://openalex.org/W1535015163', 'https://openalex.org/W2135280482', 'https://openalex.org/W2166391802', 'https://openalex.org/W1485243506', 'https://openalex.org/W2098162425', 'https://openalex.org/W2003208764', 'https://openalex.org/W2040295565', 'https://openalex.org/W1586060904', 'https://openalex.org/W2117621558', 'https://openalex.org/W2157149948', 'https://openalex.org/W2063089147', 'https://openalex.org/W2036671379', 'https://openalex.org/W2042143122', 'https://openalex.org/W2080972498', 'https://openalex.org/W2123157758', 'https://openalex.org/W1574901103', 'https://openalex.org/W1994851566', 'https://openalex.org/W2110485445', 'https://openalex.org/W1969005071', 'https://openalex.org/W2101711363', 'https://openalex.org/W1867735041', 'https://openalex.org/W2132957691', 'https://openalex.org/W2049633694', 'https://openalex.org/W2159399018', 'https://openalex.org/W2123301651', 'https://openalex.org/W2158266063', 'https://openalex.org/W1616871572', 'https://openalex.org/W2882319491', 'https://openalex.org/W1487404635', 'https://openalex.org/W2142263282', 'https://openalex.org/W1491115232', 'https://openalex.org/W167685538', 'https://openalex.org/W2154756108', 'https://openalex.org/W1997125736', 'https://openalex.org/W2585613261', 'https://openalex.org/W1562911371', 'https://openalex.org/W2149368642', 'https://openalex.org/W2006969979', 'https://openalex.org/W1660390307', 'https://openalex.org/W2164770604', 'https://openalex.org/W1779834323', 'https://openalex.org/W2170716495', 'https://openalex.org/W2161952424', 'https://openalex.org/W1558333962', 'https://openalex.org/W2135631383', 'https://openalex.org/W2033937535', 'https://openalex.org/W263845233', 'https://openalex.org/W2092654472', 'https://openalex.org/W2020999234', 'https://openalex.org/W43530752', 'https://openalex.org/W1727944201', 'https://openalex.org/W2053218206', 'https://openalex.org/W1632114991', 'https://openalex.org/W2158190429', 'https://openalex.org/W2147231083', 'https://openalex.org/W2102870983', 'https://openalex.org/W2135704565', 'https://openalex.org/W2064699871', 'https://openalex.org/W2905594908', 'https://openalex.org/W2131287128', 'https://openalex.org/W114321176', 'https://openalex.org/W1508165687', 'https://openalex.org/W1967687173', 'https://openalex.org/W2061863127', 'https://openalex.org/W1855497951', 'https://openalex.org/W2158195707', 'https://openalex.org/W2996160789', 'https://openalex.org/W2144862731', 'https://openalex.org/W150776213', 'https://openalex.org/W2026992087', 'https://openalex.org/W2766324327', 'https://openalex.org/W2004411856', 'https://openalex.org/W201532657', 'https://openalex.org/W2086426947', 'https://openalex.org/W2087309226', 'https://openalex.org/W2056760934', 'https://openalex.org/W2027751800', 'https://openalex.org/W24718583', 'https://openalex.org/W2005902041', 'https://openalex.org/W2100881784', 'https://openalex.org/W201288405', 'https://openalex.org/W2130416410', 'https://openalex.org/W1967687583', 'https://openalex.org/W2065700902', 'https://openalex.org/W2571532437']",2007-01-01
https://openalex.org/W2401396251,,Speech Technologies for African Languages: Example of a Multilingual Calculator for Education,International audience,"['https://openalex.org/W2401271873', 'https://openalex.org/W2164579587']",2015-09-06
https://openalex.org/W2195354,https://doi.org/10.21437/interspeech.2005-467,An Amharic speech corpus for large vocabulary continuous speech recognition,"• has rich morphology -> many word forms. Phonetics Amharic has a set of 38 phones, seven vowels and thirty-one consonants. Consonants Manner Voicing Place of Articulation of Art/n Lab Dent Pal Vel Glo Stops Voiceless p[p] t[t] m[t∫ ] k[k] [?] Voiced b[b] d[d] ¥[d ] g[g] GlottalizedI[p‘] μ[t‘] 1⁄2[t∫ ‘]q[q] Rounded [kw], [gw], [qw] Fricatives Voiceless f[f] s[s] ][∫ ] h[h] Voiced z[z] [ ] Glottalized O[s‘] Rounded [hw] Nasals Voiced m[m]n[n] }[ ] Liquids Voiced l[l], r[r] Semi vowelsVoiced w[w] y[j]","['https://openalex.org/W1921940208', 'https://openalex.org/W71337130', 'https://openalex.org/W4285719527', 'https://openalex.org/W2119243469', 'https://openalex.org/W1976537418', 'https://openalex.org/W2013985509']",2005-09-04
https://openalex.org/W1563026167,,Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources,"In this paper we present a cognitively plausible approach to word segmentation that segments in an online fashion using only local information and a lexicon of previously segmented words. Unlike popular statistical optimization techniques, the learner uses structural information of the input syllables rather than distributional cues to segment words. We develop a memory model for the learner that like a child learner does not recall previously hypothesized words perfectly. The learner attains an F-score of 86.69 % in ideal conditions and 85.05 % when word recall is unreliable and stress in the input is reduced. These results demonstrate the power that a simple learner can have when paired with appropriate structural constraints on its hypotheses. 1","['https://openalex.org/W1969005071', 'https://openalex.org/W1994098317', 'https://openalex.org/W2136549906', 'https://openalex.org/W2161952424', 'https://openalex.org/W2126449874', 'https://openalex.org/W2033224265', 'https://openalex.org/W2952343510', 'https://openalex.org/W2086426947', 'https://openalex.org/W2009974983', 'https://openalex.org/W2063598038', 'https://openalex.org/W2029948425', 'https://openalex.org/W2159399018', 'https://openalex.org/W2059824090', 'https://openalex.org/W2105438133', 'https://openalex.org/W2011110202', 'https://openalex.org/W2074546930', 'https://openalex.org/W2102162397', 'https://openalex.org/W2035651139', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W2098717046', 'https://openalex.org/W2019363670', 'https://openalex.org/W2151834591', 'https://openalex.org/W1992119553', 'https://openalex.org/W2156909104', 'https://openalex.org/W2004334173', 'https://openalex.org/W2126377586']",2010-07-15
https://openalex.org/W2515167330,https://doi.org/10.21437/interspeech.2016-886,Preliminary Experiments on Unsupervised Word Discovery in Mboshi,International audience,"['https://openalex.org/W2162465526', 'https://openalex.org/W2156985047', 'https://openalex.org/W2592914315', 'https://openalex.org/W2117126688', 'https://openalex.org/W2095907708', 'https://openalex.org/W1969608442', 'https://openalex.org/W3040954139', 'https://openalex.org/W191292882', 'https://openalex.org/W2998215494', 'https://openalex.org/W2063655091', 'https://openalex.org/W2166270474']",2016-08-29
https://openalex.org/W2572097499,,Collecting Resources in Sub-Saharan African Languages for Automatic Speech Recognition: a Case Study of Wolof,International audience,"['https://openalex.org/W2251408482', 'https://openalex.org/W2345799635', 'https://openalex.org/W2114016253', 'https://openalex.org/W658802254', 'https://openalex.org/W1524333225', 'https://openalex.org/W2110006374', 'https://openalex.org/W2094544353', 'https://openalex.org/W1631260214', 'https://openalex.org/W19632593', 'https://openalex.org/W2117621558', 'https://openalex.org/W2002342963', 'https://openalex.org/W419876341', 'https://openalex.org/W596385150', 'https://openalex.org/W44815768', 'https://openalex.org/W2124629003', 'https://openalex.org/W1998449347', 'https://openalex.org/W2404901983']",2016-05-23
https://openalex.org/W2055408826,https://doi.org/10.1109/isspa.2012.6310546,Towards unsupervised speech processing,"The development of an automatic speech recognizer is typically a highly supervised process involving the specification of phonetic inventories, lexicons, acoustic and language models, and requiring annotated training corpora consisting of parallel speech and text data. Although some model parameters may be modified via adaptation, the overall structure of the speech recognizer usually remains relatively static. While this approach has been effective for problems where there is adequate human expertise, and labelled corpora are available, it is challenged by less-supervised or unsupervised scenarios. It also contrasts sharply with human speech processing where learning is an inherent ability. In this paper, three alternative scenarios for speech recognition ""training"" are described, each requiring decreasing amounts of human expertise and annotated resources, and increasing amounts of unsupervised learning. A speech deciphering challenge is then suggested whereby speech recognizers must learn sub-word inventories and word pronunciations from unannotated speech, supplemented with only non-parallel text resources. It is argued that such a capability will help alleviate the language barrier that currently limits the scope of speech recognition capabilities around the world, and empower speech recognizers to continually learn and evolve through use.","['https://openalex.org/W6630406810', 'https://openalex.org/W6602705600', 'https://openalex.org/W2084514013', 'https://openalex.org/W2403207842', 'https://openalex.org/W6677734967', 'https://openalex.org/W2083270362', 'https://openalex.org/W6675022971', 'https://openalex.org/W2401464865', 'https://openalex.org/W308497914', 'https://openalex.org/W2295531324', 'https://openalex.org/W2406451296', 'https://openalex.org/W4237938692', 'https://openalex.org/W6678282225', 'https://openalex.org/W2115367993', 'https://openalex.org/W2103933358', 'https://openalex.org/W1540371244', 'https://openalex.org/W6605306941', 'https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W1539935047', 'https://openalex.org/W2079460648', 'https://openalex.org/W2057200159', 'https://openalex.org/W2163929346', 'https://openalex.org/W6640506843', 'https://openalex.org/W30845872', 'https://openalex.org/W1990005915', 'https://openalex.org/W2006761268', 'https://openalex.org/W2153327798', 'https://openalex.org/W2399869768', 'https://openalex.org/W1993750641', 'https://openalex.org/W2121745180', 'https://openalex.org/W2115915304', 'https://openalex.org/W1510837697', 'https://openalex.org/W2107917162', 'https://openalex.org/W1685430874', 'https://openalex.org/W2100768664', 'https://openalex.org/W88864901', 'https://openalex.org/W1940042001', 'https://openalex.org/W130097250', 'https://openalex.org/W1589170661', 'https://openalex.org/W2119187236', 'https://openalex.org/W2727371918', 'https://openalex.org/W2164770604', 'https://openalex.org/W66167291', 'https://openalex.org/W60702959', 'https://openalex.org/W2050423830', 'https://openalex.org/W51277926', 'https://openalex.org/W2036102925', 'https://openalex.org/W1565967034']",2012-07-01
https://openalex.org/W2963819008,,Unwritten Languages Demand Attention Too! Word Discovery with Encoder-Decoder Models,"Word discovery is the task of extracting words from un-segmented text. In this paper we examine to what extent neu-ral networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.","['https://openalex.org/W2133564696', 'https://openalex.org/W2620638943', 'https://openalex.org/W2963506925', 'https://openalex.org/W2107038463', 'https://openalex.org/W2111668269', 'https://openalex.org/W2586232309', 'https://openalex.org/W2590585939', 'https://openalex.org/W1563026167', 'https://openalex.org/W2531207078', 'https://openalex.org/W2625092622', 'https://openalex.org/W2950613790', 'https://openalex.org/W2605131327', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126449874', 'https://openalex.org/W2126377586', 'https://openalex.org/W2563850823', 'https://openalex.org/W2347145335', 'https://openalex.org/W1557247526', 'https://openalex.org/W2514307064', 'https://openalex.org/W2521246420', 'https://openalex.org/W2101281673', 'https://openalex.org/W2122228338', 'https://openalex.org/W4317473142', 'https://openalex.org/W2345799635', 'https://openalex.org/W2346964103', 'https://openalex.org/W1778492285', 'https://openalex.org/W1968625547', 'https://openalex.org/W2515167330', 'https://openalex.org/W2025482506', 'https://openalex.org/W2949328740', 'https://openalex.org/W2964308564', 'https://openalex.org/W3040954139', 'https://openalex.org/W2963999449', 'https://openalex.org/W2963767893', 'https://openalex.org/W2466918907', 'https://openalex.org/W2949150836']",2017-12-16
https://openalex.org/W2985777044,,Unsupervised word discovery for computational language documentation,"Language diversity is under considerable pressure: half of the world’s languages could disappear by the end of this century. This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation. Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited. Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools. The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists. This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting. This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available. Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods. We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist. We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure. To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.",[],2019-04-16
https://openalex.org/W2173413395,,Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using WebMAUS,"Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler et al., 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.","['https://openalex.org/W267812377', 'https://openalex.org/W1503183868', 'https://openalex.org/W2040437955', 'https://openalex.org/W1952705488', 'https://openalex.org/W1871004397', 'https://openalex.org/W1845911164', 'https://openalex.org/W1593934207', 'https://openalex.org/W2278187120', 'https://openalex.org/W2557460871', 'https://openalex.org/W2014840417', 'https://openalex.org/W1718330812', 'https://openalex.org/W1963727751']",2014-05-01
https://openalex.org/W2962784628,,,"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem.Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units.This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.","['https://openalex.org/W2100664567', 'https://openalex.org/W2962732637', 'https://openalex.org/W2250342921', 'https://openalex.org/W2119202242', 'https://openalex.org/W2097532276', 'https://openalex.org/W1519942606', 'https://openalex.org/W2015350341', 'https://openalex.org/W1899794420', 'https://openalex.org/W2117621558', 'https://openalex.org/W2016856586', 'https://openalex.org/W2107468211', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964308564', 'https://openalex.org/W2118434577', 'https://openalex.org/W2916548775', 'https://openalex.org/W2595715041', 'https://openalex.org/W1526344253', 'https://openalex.org/W2130942839', 'https://openalex.org/W4285719527', 'https://openalex.org/W1753482797', 'https://openalex.org/W1501139663', 'https://openalex.org/W214028658', 'https://openalex.org/W2148708890', 'https://openalex.org/W2508316494', 'https://openalex.org/W2124807415', 'https://openalex.org/W6908809', 'https://openalex.org/W2133564696', 'https://openalex.org/W2116211107', 'https://openalex.org/W2116599427', 'https://openalex.org/W2951559648', 'https://openalex.org/W1938755728', 'https://openalex.org/W2251139225', 'https://openalex.org/W2251012068', 'https://openalex.org/W46679369', 'https://openalex.org/W1505680913', 'https://openalex.org/W2528305545', 'https://openalex.org/W2143134347', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250303366', 'https://openalex.org/W2220350356']",
https://openalex.org/W2586232309,https://doi.org/10.1109/slt.2016.7846246,Toward human-assisted lexical unit discovery without text resources,"This work addresses lexical unit discovery for languages without (usable) written resources. Previous work has addressed this problem using entirely unsupervised methodologies. Our approach in contrast investigates the use of linguistic and speaker knowledge which are often available even if text resources are not. We create a framework that benefits from such resources, not assuming orthographic representations and avoiding generation of word-level transcriptions. We adapt a universal phone recognizer to the target language and use it to convert audio into a searchable phone string for lexical unit discovery via fuzzy sub-string matching. Linguistic knowledge is used to constrain phone recognition output and to constrain lexical unit discovery on the phone recognizer output.","['https://openalex.org/W2407151108', 'https://openalex.org/W2002342963', 'https://openalex.org/W6889619760', 'https://openalex.org/W2251408482', 'https://openalex.org/W2517454157', 'https://openalex.org/W2140979961', 'https://openalex.org/W4251556668', 'https://openalex.org/W2117126688', 'https://openalex.org/W1778492285', 'https://openalex.org/W2184343439', 'https://openalex.org/W2021167964', 'https://openalex.org/W2066892873', 'https://openalex.org/W6602309969', 'https://openalex.org/W1997505733', 'https://openalex.org/W30845872', 'https://openalex.org/W2057007397', 'https://openalex.org/W2126377586', 'https://openalex.org/W2226889031', 'https://openalex.org/W56467943', 'https://openalex.org/W2014611589', 'https://openalex.org/W2070554026']",2016-12-01
https://openalex.org/W2883972335,,Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit,"Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using Persephone, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",[],2018-01-01
https://openalex.org/W3112613336,https://doi.org/10.21437/interspeech.2021-50,Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks,"We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.","['https://openalex.org/W2126377586', 'https://openalex.org/W130754613', 'https://openalex.org/W2145410271', 'https://openalex.org/W2478415332', 'https://openalex.org/W2407151108', 'https://openalex.org/W2971775690', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W2242818861', 'https://openalex.org/W2400549570', 'https://openalex.org/W3008499099', 'https://openalex.org/W2842511635', 'https://openalex.org/W2516890051', 'https://openalex.org/W3093096176', 'https://openalex.org/W2010188467', 'https://openalex.org/W3096656254', 'https://openalex.org/W3095361818', 'https://openalex.org/W3018535504', 'https://openalex.org/W2780786457', 'https://openalex.org/W2117126688', 'https://openalex.org/W3096196861', 'https://openalex.org/W3102519966', 'https://openalex.org/W2345913943', 'https://openalex.org/W2962799131', 'https://openalex.org/W3006094508', 'https://openalex.org/W2964169922', 'https://openalex.org/W2996383576', 'https://openalex.org/W1796128977', 'https://openalex.org/W3125709657', 'https://openalex.org/W2396043527', 'https://openalex.org/W3097485645', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W3097692357', 'https://openalex.org/W2483390977', 'https://openalex.org/W2972764223', 'https://openalex.org/W2404952642', 'https://openalex.org/W2395899413', 'https://openalex.org/W2620638943', 'https://openalex.org/W2963137467', 'https://openalex.org/W3098361150', 'https://openalex.org/W2052697931', 'https://openalex.org/W2973026522', 'https://openalex.org/W2468716020', 'https://openalex.org/W2025482506']",2021-08-27
https://openalex.org/W2101281673,https://doi.org/10.1109/slt.2006.326795,TOWARDS SPEECH TRANSLATION OF NON WRITTEN LANGUAGES,"A large amount of languages in the world do not have an acknowledged written form. However, for a task like speech to speech translation, the written form of a language may be considered as secondary and it might be possible, under certain conditions, to bypass it. This paper is our first attempt to show that such an approach is possible. We propose a phone-based speech translation approach where translation models are learned on a parallel corpus made of foreign phone sequences and their corresponding English translation. Our experiments show that using our so-called phone-based approach leads almost to the same performance as the baseline approach, while being theoretically applicable to any non written language.","['https://openalex.org/W4237938692', 'https://openalex.org/W2165095312', 'https://openalex.org/W2101711363', 'https://openalex.org/W6600595526', 'https://openalex.org/W2074546930', 'https://openalex.org/W3149335959', 'https://openalex.org/W2038395276', 'https://openalex.org/W6642785364', 'https://openalex.org/W4388277788', 'https://openalex.org/W4251556668', 'https://openalex.org/W2070554026', 'https://openalex.org/W14331692', 'https://openalex.org/W60702959', 'https://openalex.org/W2952343510', 'https://openalex.org/W4297941907', 'https://openalex.org/W2150907703', 'https://openalex.org/W3036063182', 'https://openalex.org/W1970887833', 'https://openalex.org/W2107917162', 'https://openalex.org/W1984672765']",2006-01-01
https://openalex.org/W2126449874,https://doi.org/10.3115/1620754.1620800,Improving nonparameteric Bayesian inference,"One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus.","['https://openalex.org/W48178473', 'https://openalex.org/W2132827946', 'https://openalex.org/W4292691288', 'https://openalex.org/W4254816979', 'https://openalex.org/W3140968660', 'https://openalex.org/W2074546930', 'https://openalex.org/W1607198972', 'https://openalex.org/W2087309226', 'https://openalex.org/W3037265734', 'https://openalex.org/W2952343510', 'https://openalex.org/W1479758177', 'https://openalex.org/W2159399018', 'https://openalex.org/W2110485445', 'https://openalex.org/W1575798196', 'https://openalex.org/W3136512150', 'https://openalex.org/W2158266063', 'https://openalex.org/W1985093013', 'https://openalex.org/W2117126688', 'https://openalex.org/W130710483', 'https://openalex.org/W2122228338', 'https://openalex.org/W2164151151', 'https://openalex.org/W1539587067', 'https://openalex.org/W2150507172', 'https://openalex.org/W2029948425', 'https://openalex.org/W4238744681', 'https://openalex.org/W2053218206', 'https://openalex.org/W2020099698', 'https://openalex.org/W2117786207']",2009-01-01
https://openalex.org/W175497273,,Transcription bottleneck of speech corpus exploitation,"While written corpora can be exploited without any linguistic annotations, speech corpora need at least a basic transcription to be of any use for linguistic research. The basic annotation of speech data usually consists of time-aligned orthographic transcriptions. To answer phonetic or phonological research questions, phonetic transcriptions are needed as well. However, manual annotation is very time-consuming and requires considerable skill and near-native competence. Therefore it can take years of speech corpus compilation and annotation before any analyses can be carried out. In this paper, approaches that address the transcription bottleneck of speech corpus exploitation are presented and discussed, including crowdsourcing the orthographic transcription, automatic phonetic alignment, and query-driven annotation. Currently, query-driven annotation and automatic phonetic alignment are being combined and applied in two speech research projects at the Institut fur Deutsche Sprache (IDS), whereas crowdsourcing the orthographic transcription still awaits implementation.","['https://openalex.org/W1718330812', 'https://openalex.org/W2572080576', 'https://openalex.org/W1492549949', 'https://openalex.org/W2160196178', 'https://openalex.org/W3216401400', 'https://openalex.org/W2617294414', 'https://openalex.org/W2053486405', 'https://openalex.org/W2022710553', 'https://openalex.org/W1746880588', 'https://openalex.org/W2155576284', 'https://openalex.org/W2141282920', 'https://openalex.org/W132534168']",2008-01-01
https://openalex.org/W2084534958,https://doi.org/10.1109/icassp.2013.6639248,GlobalPhone: A multilingual text &amp;amp; speech database in 20 languages,"This paper describes the advances in the multilingual text and speech database GlobalPhone, a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GlobalPhone was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GlobalPhone supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages.","['https://openalex.org/W2033436836', 'https://openalex.org/W6636811518', 'https://openalex.org/W2407441242', 'https://openalex.org/W198385923', 'https://openalex.org/W6605012280', 'https://openalex.org/W6602682705', 'https://openalex.org/W6675631943', 'https://openalex.org/W1631260214', 'https://openalex.org/W2104499423', 'https://openalex.org/W121945369', 'https://openalex.org/W273093436', 'https://openalex.org/W60702959', 'https://openalex.org/W66627554']",2013-05-01
https://openalex.org/W2582956876,https://doi.org/10.48550/arxiv.1612.01744,Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation,"This paper proposes a first attempt to build an end-to-end speech-to-text translation system, which does not use source language transcription during learning or decoding. We propose a model for direct speech-to-text translation, which gives promising results on a small French-English synthetic corpus. Relaxing the need for source language transcription would drastically change the data collection methodology in speech translation, especially in under-resourced scenarios. For instance, in the former project DARPA TRANSTAC (speech translation from spoken Arabic dialects), a large effort was devoted to the collection of speech transcripts (and a prerequisite to obtain transcripts was often a detailed transcription guide for languages with little standardized spelling). Now, if end-to-end approaches for speech-to-text translation are successful, one might consider collecting data by asking bilingual speakers to directly utter speech in the source language from target language text utterances. Such an approach has the advantage to be applicable to any unwritten (source) language.",[],2016-12-06
https://openalex.org/W2805697608,https://doi.org/10.48550/arxiv.1806.00195,Learning a Latent Space of Multitrack Measures,"Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical ""style"". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).","['https://openalex.org/W2591710685', 'https://openalex.org/W2054141820', 'https://openalex.org/W2475687244', 'https://openalex.org/W2746068898', 'https://openalex.org/W2547207889', 'https://openalex.org/W1499798934', 'https://openalex.org/W2792210438', 'https://openalex.org/W3104194627', 'https://openalex.org/W3131643527', 'https://openalex.org/W2112213875', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963681776', 'https://openalex.org/W2769570009', 'https://openalex.org/W2953100410', 'https://openalex.org/W2053952057', 'https://openalex.org/W1994629163', 'https://openalex.org/W2753868141', 'https://openalex.org/W1819710477', 'https://openalex.org/W1991133427', 'https://openalex.org/W2567627528', 'https://openalex.org/W1522301498', 'https://openalex.org/W2951535099', 'https://openalex.org/W2560316200', 'https://openalex.org/W2774077477', 'https://openalex.org/W2522389179', 'https://openalex.org/W2752134738', 'https://openalex.org/W2606712314']",2018-06-01
https://openalex.org/W2908510526,https://doi.org/10.48550/arxiv.1711.05101,Decoupled Weight Decay Regularization,"L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it ""weight decay"" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW",[],2017-11-14
https://openalex.org/W2963032576,,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the posterior collapse problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a flat baseline model. An implementation of our MusicVAE is available online at this http URL.",[],2018-03-13
https://openalex.org/W2946521317,https://doi.org/10.48550/arxiv.1905.06118,Learning to Groove with Inverse Sequence Transformations,"We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using Seq2Seq and recurrent Variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and models for learning to invert them have real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, including demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).",[],2019-05-14
https://openalex.org/W3119786062,,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","['https://openalex.org/W3035282577', 'https://openalex.org/W2147800946', 'https://openalex.org/W2626778328', 'https://openalex.org/W2163605009', 'https://openalex.org/W2963341956', 'https://openalex.org/W2948798935', 'https://openalex.org/W1977295328', 'https://openalex.org/W2981689412', 'https://openalex.org/W2971155163', 'https://openalex.org/W3012324658', 'https://openalex.org/W2968124245', 'https://openalex.org/W2971315489', 'https://openalex.org/W2963091558', 'https://openalex.org/W2194775991', 'https://openalex.org/W2970389371', 'https://openalex.org/W2964121744', 'https://openalex.org/W3035422918', 'https://openalex.org/W2086161653', 'https://openalex.org/W2983446232', 'https://openalex.org/W2981413347', 'https://openalex.org/W3034885317', 'https://openalex.org/W3034978746', 'https://openalex.org/W3008526508', 'https://openalex.org/W2994759459', 'https://openalex.org/W2962843773', 'https://openalex.org/W2994760783', 'https://openalex.org/W3034445277', 'https://openalex.org/W2931316642', 'https://openalex.org/W2949117887', 'https://openalex.org/W2533598788', 'https://openalex.org/W3040573126', 'https://openalex.org/W2963631907', 'https://openalex.org/W2799269579', 'https://openalex.org/W2940744433', 'https://openalex.org/W3096609285', 'https://openalex.org/W2998108143', 'https://openalex.org/W2970608575', 'https://openalex.org/W3035524453', 'https://openalex.org/W3102696055', 'https://openalex.org/W3090449556', 'https://openalex.org/W2108598243', 'https://openalex.org/W3097065222', 'https://openalex.org/W3118608800', 'https://openalex.org/W3035058308', 'https://openalex.org/W3033210410', 'https://openalex.org/W3030163527', 'https://openalex.org/W3097217077', 'https://openalex.org/W2964080601', 'https://openalex.org/W3035160371']",2021-05-03
https://openalex.org/W2475687244,https://doi.org/10.7916/d8n58mhv,"Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching","Sequences of feature vectors are a natural way of representing temporal data. Given a database of sequences, a fundamental task is to find the database entry which is the most similar to a query. In this thesis, we present learning-based methods for efficiently and accurately comparing sequences in order to facilitate large-scale sequence search. Throughout, we will focus on the problem of matching MIDI files (a digital score format) to a large collection of audio recordings of music. The combination of our proposed approaches enables us to create the largest corpus of paired MIDI files and audio recordings ever assembled. Dynamic time warping (DTW) has proven to be an extremely effective method for both aligning and matching sequences. However, its performance is heavily affected by factors such as the feature representation used and its adjustable parameters. We therefore investigate automatically optimizing DTW-based alignment and matching of MIDI and audio data. Our approach uses Bayesian optimization to tune system design and parameters over a synthetically-created dataset of audio and MIDI pairs. We then perform an exhaustive search over DTW score normalization techniques to find the optimal method for reporting a reliable alignment confidence score, as required in matching tasks. This results in a DTW-based system which is conceptually simple and highly accurate at both alignment and matching. We also verify that this system achieves high performance in a large-scale qualitative evaluation of real-world alignments. Unfortunately, DTW can be far too inefficient for large-scale search when sequences are very long and consist of high-dimensional feature vectors. We therefore propose a method for mapping sequences of continuously-valued feature vectors to downsampled sequences of binary vectors. Our approach involves training a pair of convolutional networks to map paired groups of subsequent feature vectors to a Hamming space where similarity is preserved. Evaluated on the task of matching MIDI files to a large database of audio recordings, we show that this technique enables 99.99\% of the database to be discarded with a modest false reject rate while only requiring 0.2\% of the time to compute. Even when sped-up with a more efficient representation, the quadratic complexity of DTW greatly hinders its feasibility for very large-scale search. This cost can be avoided by mapping entire sequences to fixed-length vectors in an embedded space where sequence similarity is approximated by Euclidean distance. To achieve this embedding, we propose a feed-forward attention-based neural network model which can integrate arbitrarily long sequences. We show that this approach can extremely efficiently prune 90\% of our audio recording database with high confidence. After developing these approaches, we applied them together to the practical task of matching 178,561 unique MIDI files to the Million Song Dataset. The resulting ``Lakh MIDI Dataset'' provides a potential bounty of ground truth information for audio content-based music information retrieval. This can include transcription, meter, lyrics, and high-level musicological features. The reliability of the resulting annotations depends both on the quality of the transcription and the accuracy of the score-to-audio alignment. We therefore establish a baseline of reliability for score-derived information for different content-based MIR tasks. Finally, we discuss potential future uses of our dataset and the learning-based sequence comparison methods we developed.","['https://openalex.org/W2403342846', 'https://openalex.org/W584173323', 'https://openalex.org/W1980497326', 'https://openalex.org/W2404620398', 'https://openalex.org/W2221409856', 'https://openalex.org/W2107878631', 'https://openalex.org/W2094728533', 'https://openalex.org/W2147800946', 'https://openalex.org/W2153579005', 'https://openalex.org/W2155273149', 'https://openalex.org/W2398243923']",2016-01-01
https://openalex.org/W2753738274,,beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.",[],2017-04-24
https://openalex.org/W1677182931,https://doi.org/10.1109/iccv.2015.123,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.","['https://openalex.org/W2035424729', 'https://openalex.org/W6674914833', 'https://openalex.org/W6681239517', 'https://openalex.org/W6674859346', 'https://openalex.org/W6674330103', 'https://openalex.org/W6688059459', 'https://openalex.org/W6600213771', 'https://openalex.org/W2964103341', 'https://openalex.org/W2145287260', 'https://openalex.org/W6696761078', 'https://openalex.org/W1932847118', 'https://openalex.org/W2179352600', 'https://openalex.org/W6640036494', 'https://openalex.org/W6637616945', 'https://openalex.org/W6638667902', 'https://openalex.org/W2155893237', 'https://openalex.org/W6635810480', 'https://openalex.org/W6684191040', 'https://openalex.org/W2147800946', 'https://openalex.org/W6676297131', 'https://openalex.org/W6678818196', 'https://openalex.org/W2141125852', 'https://openalex.org/W1536680647', 'https://openalex.org/W6637373629', 'https://openalex.org/W2031489346', 'https://openalex.org/W6631943919', 'https://openalex.org/W2102605133', 'https://openalex.org/W2963173190', 'https://openalex.org/W6637359451', 'https://openalex.org/W4238404964', 'https://openalex.org/W6638444622', 'https://openalex.org/W6684665197', 'https://openalex.org/W6637242042', 'https://openalex.org/W6677651945', 'https://openalex.org/W6682359208', 'https://openalex.org/W4919037', 'https://openalex.org/W2163605009', 'https://openalex.org/W2949117887', 'https://openalex.org/W1799366690', 'https://openalex.org/W2117539524', 'https://openalex.org/W2108598243', 'https://openalex.org/W2949194345', 'https://openalex.org/W1665214252', 'https://openalex.org/W2144172034', 'https://openalex.org/W2963606038', 'https://openalex.org/W1904365287', 'https://openalex.org/W1533861849', 'https://openalex.org/W2095705004', 'https://openalex.org/W2099049980', 'https://openalex.org/W2125930537', 'https://openalex.org/W2097117768', 'https://openalex.org/W1598866093', 'https://openalex.org/W2168894214', 'https://openalex.org/W2206858481', 'https://openalex.org/W2152424459', 'https://openalex.org/W1836465849', 'https://openalex.org/W3098722327', 'https://openalex.org/W2914484425', 'https://openalex.org/W1686810756', 'https://openalex.org/W2963542991', 'https://openalex.org/W1683511521', 'https://openalex.org/W1709548961']",2015-12-01
https://openalex.org/W1884029234,https://doi.org/10.48550/arxiv.1502.05988,Deep Learning for Multi-label Classification,"In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature","['https://openalex.org/W2084802027', 'https://openalex.org/W2053463056', 'https://openalex.org/W2119466907', 'https://openalex.org/W110325196', 'https://openalex.org/W2116064496', 'https://openalex.org/W2031585281', 'https://openalex.org/W2129026672', 'https://openalex.org/W2093238926', 'https://openalex.org/W3015464002', 'https://openalex.org/W2156935079', 'https://openalex.org/W2061453990', 'https://openalex.org/W55768394', 'https://openalex.org/W2023492421', 'https://openalex.org/W1953606363', 'https://openalex.org/W2042492924', 'https://openalex.org/W2166912588', 'https://openalex.org/W2123217057', 'https://openalex.org/W1811458238', 'https://openalex.org/W1999954155', 'https://openalex.org/W2404281525', 'https://openalex.org/W2146241755', 'https://openalex.org/W2136922672', 'https://openalex.org/W254202083', 'https://openalex.org/W66588809', 'https://openalex.org/W1563488833', 'https://openalex.org/W188554010', 'https://openalex.org/W2052684427', 'https://openalex.org/W2100495367']",2014-12-17
https://openalex.org/W648786980,https://doi.org/10.48550/arxiv.1506.03099,Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,"Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.","['https://openalex.org/W2964308564', 'https://openalex.org/W1869752048', 'https://openalex.org/W2130942839', 'https://openalex.org/W2088911157', 'https://openalex.org/W1895577753', 'https://openalex.org/W1527575280', 'https://openalex.org/W1861492603', 'https://openalex.org/W1947481528', 'https://openalex.org/W595069947', 'https://openalex.org/W1524333225', 'https://openalex.org/W2158349948', 'https://openalex.org/W2107878631', 'https://openalex.org/W2104917081', 'https://openalex.org/W2734523219', 'https://openalex.org/W1956340063', 'https://openalex.org/W2962957031', 'https://openalex.org/W1931639407', 'https://openalex.org/W2962706528', 'https://openalex.org/W2268617045', 'https://openalex.org/W1586532344', 'https://openalex.org/W1836465849', 'https://openalex.org/W1905882502', 'https://openalex.org/W2147880316', 'https://openalex.org/W2296073425', 'https://openalex.org/W2064675550']",2015-06-09
