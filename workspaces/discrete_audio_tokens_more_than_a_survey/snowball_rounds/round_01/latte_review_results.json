[
  {
    "title": "Unsupervised Discovery of Structured Acoustic Tokens With Applications to Spoken Term Detection",
    "abstract": "In this paper, we compare two paradigms for unsupervised discovery of structured acoustic tokens directly from speech corpora without any human annotation. The Multigranular Paradigm seeks to capture all available information in the corpora with multiple sets of tokens for different model granularities. The Hierarchical Paradigm attempts to jointly learn several levels of signal representations in a hierarchical structure. The two paradigms are unified within a theoretical framework in this paper. Query-by-Example Spoken Term Detection (QbE-STD) experiments on the QUESST dataset of MediaEval 2015 verifies the competitiveness of the acoustic tokens. The Enhanced Relevance Score (ERS) proposed in this work improves both paradigms for the task of QbE-STD. We also list results on the ABX evaluation task of the Zero Resource Challenge 2015 for comparison of the Paradigms.",
    "metadata": {
      "title": "Unsupervised Discovery of Structured Acoustic Tokens With Applications to Spoken Term Detection",
      "summary": "In this paper, we compare two paradigms for unsupervised discovery of structured acoustic tokens directly from speech corpora without any human annotation. The Multigranular Paradigm seeks to capture all available information in the corpora with multiple sets of tokens for different model granularities. The Hierarchical Paradigm attempts to jointly learn several levels of signal representations in a hierarchical structure. The two paradigms are unified within a theoretical framework in this paper. Query-by-Example Spoken Term Detection (QbE-STD) experiments on the QUESST dataset of MediaEval 2015 verifies the competitiveness of the acoustic tokens. The Enhanced Relevance Score (ERS) proposed in this work improves both paradigms for the task of QbE-STD. We also list results on the ABX evaluation task of the Zero Resource Challenge 2015 for comparison of the Paradigms.",
      "abstract": "In this paper, we compare two paradigms for unsupervised discovery of structured acoustic tokens directly from speech corpora without any human annotation. The Multigranular Paradigm seeks to capture all available information in the corpora with multiple sets of tokens for different model granularities. The Hierarchical Paradigm attempts to jointly learn several levels of signal representations in a hierarchical structure. The two paradigms are unified within a theoretical framework in this paper. Query-by-Example Spoken Term Detection (QbE-STD) experiments on the QUESST dataset of MediaEval 2015 verifies the competitiveness of the acoustic tokens. The Enhanced Relevance Score (ERS) proposed in this work improves both paradigms for the task of QbE-STD. We also list results on the ABX evaluation task of the Zero Resource Challenge 2015 for comparison of the Paradigms.",
      "doi": "https://doi.org/10.1109/taslp.2017.2778948",
      "openalex_id": "https://openalex.org/W2768580149",
      "arxiv_id": "",
      "publication_date": "2017-11-30",
      "published": "2017-11-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised discovery and evaluation of discrete acoustic tokens derived from speech, applying them to speech-related tasks (spoken term detection, ABX), which aligns with the inclusion criteria and does not trigger the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised discovery and evaluation of discrete acoustic tokens derived from speech, applying them to speech-related tasks (spoken term detection, ABX), which aligns with the inclusion criteria and does not trigger the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses specifically on unsupervised discovery of structured acoustic tokens from speech data and evaluates these discrete tokens in the context of spoken term detection, thereby meeting the inclusion criteria of proposing and analyzing discrete audio tokenization; it also provides methodological contributions and experimental validation rather than being an engineering report or unrelated to discrete audio tokens.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses specifically on unsupervised discovery of structured acoustic tokens from speech data and evaluates these discrete tokens in the context of spoken term detection, thereby meeting the inclusion criteria of proposing and analyzing discrete audio tokenization; it also provides methodological contributions and experimental validation rather than being an engineering report or unrelated to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Av-Data2Vec: Self-Supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations",
    "abstract": "Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.",
    "metadata": {
      "title": "Av-Data2Vec: Self-Supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations",
      "summary": "Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.",
      "abstract": "Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389642",
      "openalex_id": "https://openalex.org/W4391021811",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes continuous, end-to-end audio-visual speech representations with no discrete speech tokenization or quantization; therefore it does not meet the discrete-token inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes continuous, end-to-end audio-visual speech representations with no discrete speech tokenization or quantization; therefore it does not meet the discrete-token inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised method for audio-visual speech recognition using contextualized target representations but does not mention any discrete audio/speech tokenization or quantization of continuous audio into discrete tokens, which is central to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised method for audio-visual speech recognition using contextualized target representations but does not mention any discrete audio/speech tokenization or quantization of continuous audio into discrete tokens, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration",
    "abstract": "Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.",
    "metadata": {
      "title": "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration",
      "summary": "Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.",
      "abstract": "Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01802",
      "openalex_id": "https://openalex.org/W4386076005",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete units/tokens derived from a self-supervised speech model to bridge AVSR and TTS in an audio-visual speech resynthesis framework, making discrete tokenization a core component of the method, which aligns with the inclusion criterion focused on discrete speech tokenization.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete units/tokens derived from a self-supervised speech model to bridge AVSR and TTS in an audio-visual speech resynthesis framework, making discrete tokenization a core component of the method, which aligns with the inclusion criterion focused on discrete speech tokenization.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly discusses converting continuous audio signals into discrete units derived from a self-supervised speech model and uses these discrete tokens as the core component connecting pseudo audio-visual speech recognition and text-to-speech synthesis, aligning well with the inclusion criteria focused on discrete audio/speech tokenization. There is clear methodological contribution and experimental validation related to discrete audio token representations rather than mere engineering or continuous audio features. Therefore, it meets all inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly discusses converting continuous audio signals into discrete units derived from a self-supervised speech model and uses these discrete tokens as the core component connecting pseudo audio-visual speech recognition and text-to-speech synthesis, aligning well with the inclusion criteria focused on discrete audio/speech tokenization. There is clear methodological contribution and experimental validation related to discrete audio token representations rather than mere engineering or continuous audio features. Therefore, it meets all inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition",
    "abstract": "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts.",
    "metadata": {
      "title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts.",
      "abstract": "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.649",
      "openalex_id": "https://openalex.org/W4385571863",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on cross-modal representation learning for audio-visual speech recognition using continuous frame-level features; no discrete audio tokenization is proposed or analyzed.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on cross-modal representation learning for audio-visual speech recognition using continuous frame-level features; no discrete audio tokenization is proposed or analyzed.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on refining frame-level modality-invariant representations for audio-visual speech recognition, but it does not mention discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete tokens; thus, it does not meet the key inclusion criterion regarding discrete tokenization as the core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on refining frame-level modality-invariant representations for audio-visual speech recognition, but it does not mention discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete tokens; thus, it does not meet the key inclusion criterion regarding discrete tokenization as the core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
    "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.",
    "metadata": {
      "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
      "summary": "Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.",
      "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445891",
      "openalex_id": "https://openalex.org/W4392904484",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on AVSR architecture and multilingual performance rather than explicitly analyzing or proposing discrete audio tokenization as the core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study centers on AVSR architecture and multilingual performance rather than explicitly analyzing or proposing discrete audio tokenization as the core object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a multilingual audio-visual speech recognition model but does not explicitly discuss or analyze discrete audio/speech tokenization; it focuses on model architecture and performance improvements using continuous features and hybrid CTC/RNN-T, without evidence of a core discrete tokenization contribution as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a multilingual audio-visual speech recognition model but does not explicitly discuss or analyze discrete audio/speech tokenization; it focuses on model architecture and performance improvements using continuous features and hybrid CTC/RNN-T, without evidence of a core discrete tokenization contribution as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition",
    "abstract": "Audio-visual speech recognition (AVSR) research has gained a great success recently by improving the noise-robustness of audio-only automatic speech recognition (ASR) with noise-invariant visual information. However, most existing AVSR approaches simply fuse the audio and visual features by concatenation, without explicit interactions to capture the deep correlations between them, which results in sub-optimal multimodal representations for downstream speech recognition task. In this paper, we propose a cross-modal global interaction and local alignment (GILA) approach for AVSR, which captures the deep audio-visual (A-V) correlations from both global and local perspectives. Specifically, we design a global interaction model to capture the A-V complementary relationship on modality level, as well as a local alignment approach to model the A-V temporal consistency on frame level. Such a holistic view of cross-modal correlations enable better multimodal representations for AVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA outperforms the supervised learning state-of-the-art. Code is at https://github.com/YUCHEN005/GILA.",
    "metadata": {
      "title": "Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) research has gained a great success recently by improving the noise-robustness of audio-only automatic speech recognition (ASR) with noise-invariant visual information. However, most existing AVSR approaches simply fuse the audio and visual features by concatenation, without explicit interactions to capture the deep correlations between them, which results in sub-optimal multimodal representations for downstream speech recognition task. In this paper, we propose a cross-modal global interaction and local alignment (GILA) approach for AVSR, which captures the deep audio-visual (A-V) correlations from both global and local perspectives. Specifically, we design a global interaction model to capture the A-V complementary relationship on modality level, as well as a local alignment approach to model the A-V temporal consistency on frame level. Such a holistic view of cross-modal correlations enable better multimodal representations for AVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA outperforms the supervised learning state-of-the-art. Code is at https://github.com/YUCHEN005/GILA.",
      "abstract": "Audio-visual speech recognition (AVSR) research has gained a great success recently by improving the noise-robustness of audio-only automatic speech recognition (ASR) with noise-invariant visual information. However, most existing AVSR approaches simply fuse the audio and visual features by concatenation, without explicit interactions to capture the deep correlations between them, which results in sub-optimal multimodal representations for downstream speech recognition task. In this paper, we propose a cross-modal global interaction and local alignment (GILA) approach for AVSR, which captures the deep audio-visual (A-V) correlations from both global and local perspectives. Specifically, we design a global interaction model to capture the A-V complementary relationship on modality level, as well as a local alignment approach to model the A-V temporal consistency on frame level. Such a holistic view of cross-modal correlations enable better multimodal representations for AVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA outperforms the supervised learning state-of-the-art. Code is at https://github.com/YUCHEN005/GILA.",
      "doi": "https://doi.org/10.24963/ijcai.2023/564",
      "openalex_id": "https://openalex.org/W4385767994",
      "arxiv_id": "",
      "publication_date": "2023-08-01",
      "published": "2023-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on cross-modal interaction and alignment for audio-visual speech recognition using continuous features, with no discussion of discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on cross-modal interaction and alignment for audio-visual speech recognition using continuous features, with no discussion of discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual speech recognition using multimodal feature fusion and interaction without any mention or analysis of discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences, which is a core inclusion criterion; thus, it does not meet the inclusion criteria and falls into exclusion for lacking discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual speech recognition using multimodal feature fusion and interaction without any mention or analysis of discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences, which is a core inclusion criterion; thus, it does not meet the inclusion criteria and falls into exclusion for lacking discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Toward Joint Language Modeling for Speech Units and Text",
    "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
    "metadata": {
      "title": "Toward Joint Language Modeling for Speech Units and Text",
      "summary": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
      "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.438",
      "openalex_id": "https://openalex.org/W4389518827",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract explicitly analyzes and utilizes discrete speech tokenization by converting continuous speech into discrete units for joint language modeling with text, aligning with inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Abstract explicitly analyzes and utilizes discrete speech tokenization by converting continuous speech into discrete units for joint language modeling with text, aligning with inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly explores joint language modeling using discrete speech tokenizers which convert continuous speech signals into discrete units, analyzing their impact in downstream tasks, thus meeting the inclusion criteria focused on discrete audio tokenization with clear experimental validation and no exclusion criteria triggered.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly explores joint language modeling using discrete speech tokenizers which convert continuous speech signals into discrete units, analyzing their impact in downstream tasks, thus meeting the inclusion criteria focused on discrete audio tokenization with clear experimental validation and no exclusion criteria triggered.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition",
    "abstract": "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task.",
    "metadata": {
      "title": "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task.",
      "abstract": "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.848",
      "openalex_id": "https://openalex.org/W4385571549",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on audiovisual speech recognition using a viseme-phoneme mapping and visual-to-audio modality transfer for robustness, but it does not explicitly center on discrete audio tokenization (quantized tokens) as the core object of study, thus it does not meet the stated inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on audiovisual speech recognition using a viseme-phoneme mapping and visual-to-audio modality transfer for robustness, but it does not explicitly center on discrete audio tokenization (quantized tokens) as the core object of study, thus it does not meet the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual speech recognition and proposes a universal viseme-phoneme mapping approach to restore clean audio from visual signals, aiming at robustness against noise; however, there is no explicit mention or analysis of discrete audio/speech tokenization converting continuous audio into discrete symbols as the core study object.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual speech recognition and proposes a universal viseme-phoneme mapping approach to restore clean audio from visual signals, aiming at robustness against noise; however, there is no explicit mention or analysis of discrete audio/speech tokenization converting continuous audio into discrete symbols as the core study object.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multimodal Fish Feeding Intensity Assessment in Aquaculture",
    "abstract": "Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management.",
    "metadata": {
      "title": "Multimodal Fish Feeding Intensity Assessment in Aquaculture",
      "summary": "Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management.",
      "abstract": "Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management.",
      "doi": "https://doi.org/10.48550/arxiv.2309.05058",
      "openalex_id": "https://openalex.org/W4386648419",
      "arxiv_id": "",
      "publication_date": "2023-09-10",
      "published": "2023-09-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multimodal FFIA (audio/video) and dataset construction/assessment, but contains no discrete audio tokenization or token-level analysis; it relies on continuous features and modality fusion rather than token sequences, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multimodal FFIA (audio/video) and dataset construction/assessment, but contains no discrete audio tokenization or token-level analysis; it relies on continuous features and modality fusion rather than token sequences, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating fish feeding intensity using multi-modal audio and video data but does not propose, analyze, or evaluate discrete audio tokenization methods; it uses audio as a modality without discussing conversion into discrete token sequences or neural discrete tokenization relevant for speech or language-model-style tasks, thus not meeting the inclusion criteria and falling under exclusion criteria related to continuous audio features without discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating fish feeding intensity using multi-modal audio and video data but does not propose, analyze, or evaluate discrete audio tokenization methods; it uses audio as a modality without discussing conversion into discrete token sequences or neural discrete tokenization relevant for speech or language-model-style tasks, thus not meeting the inclusion criteria and falling under exclusion criteria related to continuous audio features without discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Audio Teacher-Student Transformer for Both Clip-Level and Frame-Level Tasks",
    "abstract": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
    "metadata": {
      "title": "Self-Supervised Audio Teacher-Student Transformer for Both Clip-Level and Frame-Level Tasks",
      "summary": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "abstract": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "doi": "https://doi.org/10.1109/taslp.2024.3352248",
      "openalex_id": "https://openalex.org/W4390738640",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe self-supervised audio learning with clip/frame tasks using continuous features, and there is no discrete audio tokenization or quantization analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe self-supervised audio learning with clip/frame tasks using continuous features, and there is no discrete audio tokenization or quantization analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised learning method using Transformer-based teacher-student models for audio tasks at clip-level and frame-level, focusing on learning continuous audio representations rather than discrete tokenization; there is no explicit mention or analysis of discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is a core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised learning method using Transformer-based teacher-student models for audio tasks at clip-level and frame-level, focusing on learning continuous audio representations rather than discrete tokenization; there is no explicit mention or analysis of discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is a core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
    "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
    "metadata": {
      "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
      "summary": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "doi": "https://doi.org/10.1609/aaai.v38i7.28486",
      "openalex_id": "https://openalex.org/W4393147998",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a continuous-audio feature pipeline with an adaptor to a text-to-video model and a novel alignment metric; it does not discuss discrete audio tokenization or symbolic audio representations, so it does not meet the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a continuous-audio feature pipeline with an adaptor to a text-to-video model and a novel alignment metric; it does not discuss discrete audio tokenization or symbolic audio representations, so it does not meet the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating videos conditioned on audio using a text-to-video generation model with an adaptor network mapping audio representations, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core object of study; it primarily uses continuous audio representations without specific discrete tokenization methods or analysis, and the audio modality is used for video generation rather than as a target for discrete tokenization relevant to speech tasks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating videos conditioned on audio using a text-to-video generation model with an adaptor network mapping audio representations, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core object of study; it primarily uses continuous audio representations without specific discrete tokenization methods or analysis, and the audio modality is used for video generation rather than as a target for discrete tokenization relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CED: Consistent Ensemble Distillation for Audio Tagging",
    "abstract": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
    "metadata": {
      "title": "CED: Consistent Ensemble Distillation for Audio Tagging",
      "summary": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "abstract": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446348",
      "openalex_id": "https://openalex.org/W4392904001",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on distillation for audio tagging using continuous features and logits, with no discrete audio tokenization studied, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on distillation for audio tagging using continuous features and logits, with no discrete audio tokenization studied, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a training framework called CED for audio tagging that involves augmentation, knowledge distillation, and model training on Audioset; however, the abstract does not mention any discrete audio or speech tokenization methods, discrete quantization, or token design as a core object of study, instead it seems to use continuous representations and logits for distillation without analyzing or proposing discrete tokenization approaches relevant to audio reconstruction or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a training framework called CED for audio tagging that involves augmentation, knowledge distillation, and model training on Audioset; however, the abstract does not mention any discrete audio or speech tokenization methods, discrete quantization, or token design as a core object of study, instead it seems to use continuous representations and logits for distillation without analyzing or proposing discrete tokenization approaches relevant to audio reconstruction or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
    "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
    "metadata": {
      "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
      "summary": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "doi": "https://doi.org/10.1109/tmm.2024.3405622",
      "openalex_id": "https://openalex.org/W4399146361",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on audio-visual segmentation and audio-visual correspondence without proposing or analyzing discrete audio tokenization into a sequence of tokens; it uses continuous audio features and aims at locating sounding objects, not token-based modeling for speech or discrete representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on audio-visual segmentation and audio-visual correspondence without proposing or analyzing discrete audio tokenization into a sequence of tokens; it uses continuous audio features and aims at locating sounding objects, not token-based modeling for speech or discrete representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual segmentation, aiming to locate sounding sources in images using pixel-wise maps and foundation audio classification models. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as converting continuous audio signals into sequences of discrete symbols, nor does it involve discrete audio token design or quantization. The core contribution pertains to segmentation and semantic alignment rather than discrete tokenization of audio. Therefore, it does not meet the inclusion criteria centered on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual segmentation, aiming to locate sounding sources in images using pixel-wise maps and foundation audio classification models. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as converting continuous audio signals into sequences of discrete symbols, nor does it involve discrete audio token design or quantization. The core contribution pertains to segmentation and semantic alignment rather than discrete tokenization of audio. Therefore, it does not meet the inclusion criteria centered on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SFC-Sup: Robust Two-Stage Underwater Acoustic Target Recognition Method Based on Supervised Contrastive Learning",
    "abstract": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
    "metadata": {
      "title": "SFC-Sup: Robust Two-Stage Underwater Acoustic Target Recognition Method Based on Supervised Contrastive Learning",
      "summary": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "abstract": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tgrs.2023.3329653",
      "openalex_id": "https://openalex.org/W4388240219",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study uses continuous spectral features and contrastive learning but does not propose or analyze any discrete audio tokenization or token-based representations; it aligns with continuous-feature approaches and should be excluded per the tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study uses continuous spectral features and contrastive learning but does not propose or analyze any discrete audio tokenization or token-based representations; it aligns with continuous-feature approaches and should be excluded per the tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on underwater acoustic target recognition using spectral features and supervised contrastive learning, but it does not propose or analyze discrete audio/speech tokenization; rather, it works with continuous audio features without mentioning any discrete token design or analysis relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on underwater acoustic target recognition using spectral features and supervised contrastive learning, but it does not propose or analyze discrete audio/speech tokenization; rather, it works with continuous audio features without mentioning any discrete token design or analysis relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Audio Captioning Models with Fine-Grained Audio Features, Text Embedding Supervision, and LLM Mix-Up Augmentation",
    "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
    "metadata": {
      "title": "Improving Audio Captioning Models with Fine-Grained Audio Features, Text Embedding Supervision, and LLM Mix-Up Augmentation",
      "summary": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447215",
      "openalex_id": "https://openalex.org/W4392903033",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes using continuous audio features (BEATS), text embeddings, and data augmentation for captions, with no discrete audio tokenization or tokenization analysis; thus it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes using continuous audio features (BEATS), text embeddings, and data augmentation for captions, with no discrete audio tokenization or tokenization analysis; thus it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving audio captioning models by leveraging pretrained models and large language models, using continuous fine-grained audio features (BEATS) and text embeddings, along with data augmentation and sampling strategies. However, the abstract does not mention any discrete audio/speech tokenization process, design, or analysis, nor does it describe converting continuous audio signals into discrete token sequences, which is a core inclusion criterion. Instead, the study appears to rely on continuous audio features and language-model-based augmentations without addressing discrete audio tokenization. Therefore, the study does not meet the core inclusion criteria and falls under the exclusion category of using only continuous audio features without discrete quantization/token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving audio captioning models by leveraging pretrained models and large language models, using continuous fine-grained audio features (BEATS) and text embeddings, along with data augmentation and sampling strategies. However, the abstract does not mention any discrete audio/speech tokenization process, design, or analysis, nor does it describe converting continuous audio signals into discrete token sequences, which is a core inclusion criterion. Instead, the study appears to rely on continuous audio features and language-model-based augmentations without addressing discrete audio tokenization. Therefore, the study does not meet the core inclusion criteria and falls under the exclusion category of using only continuous audio features without discrete quantization/token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast ship radiated noise recognition using three-dimensional mel-spectrograms with an additive attention based transformer",
    "abstract": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
    "metadata": {
      "title": "Fast ship radiated noise recognition using three-dimensional mel-spectrograms with an additive attention based transformer",
      "summary": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "abstract": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "doi": "https://doi.org/10.3389/fmars.2023.1280708",
      "openalex_id": "https://openalex.org/W4388973434",
      "arxiv_id": "",
      "publication_date": "2023-11-24",
      "published": "2023-11-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study relies on continuous 3D Mel-spectrogram features and an additive attention Transformer with no discrete tokenization/quantization of audio, so it does not meet the discrete-tokenization inclusion and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study relies on continuous 3D Mel-spectrogram features and an additive attention Transformer with no discrete tokenization/quantization of audio, so it does not meet the discrete-tokenization inclusion and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on ship-radiated noise recognition using continuous acoustic features such as 3D Mel-spectrograms and an additive attention-based Transformer for classification, without any mention or analysis of discrete audio/speech tokenization or conversion of audio signals into discrete token sequences, which is essential per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on ship-radiated noise recognition using continuous acoustic features such as 3D Mel-spectrograms and an additive attention-based Transformer for classification, without any mention or analysis of discrete audio/speech tokenization or conversion of audio signals into discrete token sequences, which is essential per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification",
    "abstract": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
    "metadata": {
      "title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification",
      "summary": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "abstract": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "doi": "https://doi.org/10.48550/arxiv.2211.13189",
      "openalex_id": "https://openalex.org/W4309957268",
      "arxiv_id": "",
      "publication_date": "2022-11-23",
      "published": "2022-11-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents ASiT, a self-supervised transformer for audio spectrograms, but there is no discussion of discrete audio tokenization or quantization into a finite vocabulary; it relies on continuous spectrogram representations, which violates the inclusion criteria and aligns with the exclusion criteria about lacking discrete token design.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper presents ASiT, a self-supervised transformer for audio spectrograms, but there is no discussion of discrete audio tokenization or quantization into a finite vocabulary; it relies on continuous spectrogram representations, which violates the inclusion criteria and aligns with the exclusion criteria about lacking discrete token design.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a self-supervised learning framework for audio spectrograms using a transformer architecture but does not explicitly describe discrete audio/speech tokenization or convert continuous audio signals into discrete token sequences; it mainly deals with continuous features and classification tasks without clear analysis or proposal of discrete tokenization methods, thus failing the inclusion criteria and falling under exclusion criteria regarding lack of discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a self-supervised learning framework for audio spectrograms using a transformer architecture but does not explicitly describe discrete audio/speech tokenization or convert continuous audio signals into discrete token sequences; it mainly deals with continuous features and classification tasks without clear analysis or proposal of discrete tokenization methods, thus failing the inclusion criteria and falling under exclusion criteria regarding lack of discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
    "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
    "metadata": {
      "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
      "summary": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "doi": "https://doi.org/10.48550/arxiv.2303.17395",
      "openalex_id": "https://openalex.org/W4361864998",
      "arxiv_id": "",
      "publication_date": "2023-03-30",
      "published": "2023-03-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a dataset of weakly-labelled audio captions and uses ChatGPT to filter/transform descriptions; it does not propose, analyze, or evaluate discrete audio tokenization into a finite vocabulary for reconstruction/generation/understanding, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work centers on a dataset of weakly-labelled audio captions and uses ChatGPT to filter/transform descriptions; it does not propose, analyze, or evaluate discrete audio tokenization into a finite vocabulary for reconstruction/generation/understanding, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a large-scale weakly-labeled audio captioning dataset and uses ChatGPT to filter and generate captions; however, it does not present or analyze a discrete audio tokenization method for converting continuous audio signals into sequences of discrete symbols, which is a core inclusion criterion. The focus is on dataset creation and caption generation rather than on discrete audio tokenization methods, and there is no evidence of discrete token design or analysis for audio reconstruction or generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces a large-scale weakly-labeled audio captioning dataset and uses ChatGPT to filter and generate captions; however, it does not present or analyze a discrete audio tokenization method for converting continuous audio signals into sequences of discrete symbols, which is a core inclusion criterion. The focus is on dataset creation and caption generation rather than on discrete audio tokenization methods, and there is no evidence of discrete token design or analysis for audio reconstruction or generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploring Self-supervised Contrastive Learning of Spatial Sound Event Representation",
    "abstract": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
    "metadata": {
      "title": "Exploring Self-supervised Contrastive Learning of Spatial Sound Event Representation",
      "summary": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "abstract": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447391",
      "openalex_id": "https://openalex.org/W4392903751",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates self-supervised learning for spatial audio representations using continuous features (waveforms, spectrograms, GCC) and does not involve any discrete audio tokenization or token-level representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates self-supervised learning for spatial audio representations using continuous features (waveforms, spectrograms, GCC) and does not involve any discrete audio tokenization or token-level representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive learning using continuous spatial audio features (waveforms, Mel spectrograms, GCC), enhancing classification and localization but does not mention proposing or analyzing discrete audio/speech tokenization or converting audio signals into discrete symbols; thus, it does not meet the inclusion criteria and falls under exclusion for using only continuous features without discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive learning using continuous spatial audio features (waveforms, Mel spectrograms, GCC), enhancing classification and localization but does not mention proposing or analyzing discrete audio/speech tokenization or converting audio signals into discrete symbols; thus, it does not meet the inclusion criteria and falls under exclusion for using only continuous features without discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SSL-Net: A Synergistic Spectral and Learning-Based Network for Efficient Bird Sound Classification",
    "abstract": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
    "metadata": {
      "title": "SSL-Net: A Synergistic Spectral and Learning-Based Network for Efficient Bird Sound Classification",
      "summary": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "abstract": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445889",
      "openalex_id": "https://openalex.org/W4392904623",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets bird sound classification using spectral and learned features and reports no discrete audio tokenization or quantization of the audio into a finite symbol sequence, thus it does not meet the criterion requiring explicit discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets bird sound classification using spectral and learned features and reports no discrete audio tokenization or quantization of the audio into a finite symbol sequence, thus it does not meet the criterion requiring explicit discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient bird sound classification using spectral and learned features but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous audio into discrete token sequences. It appears to use traditional continuous audio features and does not address discrete tokenization or neural encoding-based quantization relevant to language-model-style tokens, thus not satisfying the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient bird sound classification using spectral and learned features but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous audio into discrete token sequences. It appears to use traditional continuous audio features and does not address discrete tokenization or neural encoding-based quantization relevant to language-model-style tokens, thus not satisfying the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Foundation Models for Video Understanding: A Survey",
    "abstract": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
    "metadata": {
      "title": "Foundation Models for Video Understanding: A Survey",
      "summary": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "abstract": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "doi": "https://doi.org/10.36227/techrxiv.171769139.99464428/v1",
      "openalex_id": "https://openalex.org/W4399393261",
      "arxiv_id": "",
      "publication_date": "2024-06-06",
      "published": "2024-06-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on video foundation models and benchmarks, with no explicit discussion of discrete audio/speech tokenization or tokenization-based analysis, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on video foundation models and benchmarks, with no explicit discussion of discrete audio/speech tokenization or tokenization-based analysis, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study is a comprehensive survey on foundation models for video understanding, focusing on video representation and multiple modalities including image, video, audio, and text. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core subject, nor does it focus on discrete tokenization approaches for audio signals. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization as its core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study is a comprehensive survey on foundation models for video understanding, focusing on video representation and multiple modalities including image, video, audio, and text. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core subject, nor does it focus on discrete tokenization approaches for audio signals. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization as its core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DTF-AT: Decoupled Time-Frequency Audio Transformer for Event Classification",
    "abstract": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
    "metadata": {
      "title": "DTF-AT: Decoupled Time-Frequency Audio Transformer for Event Classification",
      "summary": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "abstract": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29716",
      "openalex_id": "https://openalex.org/W4393160941",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study relies on continuous spectrogram features and transformer models without proposing or analyzing discrete audio tokenization or quantization into a finite vocabulary.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study relies on continuous spectrogram features and transformer models without proposing or analyzing discrete audio tokenization or quantization into a finite vocabulary.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes and evaluates an audio transformer architecture for audio event classification using continuous audio features (e.g., spectrograms) without explicit mention or analysis of discrete audio/speech tokenization; thus, it does not address discrete token design or quantization as required by inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes and evaluates an audio transformer architecture for audio event classification using continuous audio features (e.g., spectrograms) without explicit mention or analysis of discrete audio/speech tokenization; thus, it does not address discrete token design or quantization as required by inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends",
    "abstract": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
    "metadata": {
      "title": "Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends",
      "summary": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "abstract": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "doi": "https://doi.org/10.1109/bigdata59044.2023.10386933",
      "openalex_id": "https://openalex.org/W4391092942",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses foundation models and dataspace governance; it does not propose, analyze, or evaluate discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses foundation models and dataspace governance; it does not propose, analyze, or evaluate discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Foundation Models in the context of dataspace, addressing technical, legal, and ethical considerations and future trends, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization, nor does it discuss token sequences or discrete quantization related to audio reconstruction, generation, or understanding tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Foundation Models in the context of dataspace, addressing technical, legal, and ethical considerations and future trends, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization, nor does it discuss token sequences or discrete quantization related to audio reconstruction, generation, or understanding tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Use of an Acoustic Sensor in the Tasks of Determining Defects in the Roadway",
    "abstract": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
    "metadata": {
      "title": "On the Use of an Acoustic Sensor in the Tasks of Determining Defects in the Roadway",
      "summary": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "abstract": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "doi": "https://doi.org/10.1109/ieeeconf60226.2024.10496721",
      "openalex_id": "https://openalex.org/W4394860713",
      "arxiv_id": "",
      "publication_date": "2024-03-12",
      "published": "2024-03-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes acoustic sensing for defect detection with continuous audio features and ML, but it does not address discrete audio tokenization/quantization into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes acoustic sensing for defect detection with continuous audio features and ML, but it does not address discrete audio tokenization/quantization into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using an acoustic sensor to detect road defects through acoustic analysis and machine learning classification; however, it does not mention any discrete audio/speech tokenization process or the conversion of continuous audio signals into sequences of discrete symbols related to speech or audio token reconstruction/generation tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using an acoustic sensor to detect road defects through acoustic analysis and machine learning classification; however, it does not mention any discrete audio/speech tokenization process or the conversion of continuous audio signals into sequences of discrete symbols related to speech or audio token reconstruction/generation tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Paᗧ-HuBERT: Self-Supervised Music Source Separation Via Primitive Auditory Clustering And Hidden-Unit Bert",
    "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
    "metadata": {
      "title": "Paᗧ-HuBERT: Self-Supervised Music Source Separation Via Primitive Auditory Clustering And Hidden-Unit Bert",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193575",
      "openalex_id": "https://openalex.org/W4385478423",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses discrete auditory clustering labels as targets for self-supervised pretraining (a discrete tokenization approach) within a music source separation framework, aligning with the criterion that a discrete tokenization objective is a core aspect, albeit applied to music rather than speech.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper uses discrete auditory clustering labels as targets for self-supervised pretraining (a discrete tokenization approach) within a music source separation framework, aligning with the criterion that a discrete tokenization objective is a core aspect, albeit applied to music rather than speech.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Paᗧ-HuBERT, a self-supervised model inspired by HuBERT that incorporates primitive auditory features as unsupervised clustering labels to initialize training, suggesting a discrete tokenization mechanism converting continuous audio into sequences of discrete symbols for source separation; it thus aligns with the inclusion criteria focusing on discrete audio tokenization and its analysis, while addressing music source separation and demonstrating improved performance, meeting criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes Paᗧ-HuBERT, a self-supervised model inspired by HuBERT that incorporates primitive auditory features as unsupervised clustering labels to initialize training, suggesting a discrete tokenization mechanism converting continuous audio into sequences of discrete symbols for source separation; it thus aligns with the inclusion criteria focusing on discrete audio tokenization and its analysis, while addressing music source separation and demonstrating improved performance, meeting criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Weak Supervised Sound Event Detection Based on Puzzle CAM",
    "abstract": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
    "metadata": {
      "title": "Weak Supervised Sound Event Detection Based on Puzzle CAM",
      "summary": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "abstract": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "doi": "https://doi.org/10.1109/access.2023.3305633",
      "openalex_id": "https://openalex.org/W4385834290",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper proposes a weakly supervised SED method using CAM and time-frame localization, but it does not introduce or analyze discrete audio tokenization or token-level representations, so it should be excluded per tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a weakly supervised SED method using CAM and time-frame localization, but it does not introduce or analyze discrete audio tokenization or token-level representations, so it should be excluded per tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a weakly supervised sound event detection method using Puzzle CAM but does not focus on discrete audio or speech tokenization; it works with continuous features and does not convert audio into discrete token sequences for reconstruction or generation tasks, thus failing to meet the inclusion criteria focused on discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a weakly supervised sound event detection method using Puzzle CAM but does not focus on discrete audio or speech tokenization; it works with continuous features and does not convert audio into discrete token sequences for reconstruction or generation tasks, thus failing to meet the inclusion criteria focused on discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers",
    "abstract": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
    "metadata": {
      "title": "On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers",
      "summary": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "abstract": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10289774",
      "openalex_id": "https://openalex.org/W4388183682",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on frequency-wise normalization of spectrogram inputs in Audio Spectrogram Transformers for ASC and does not propose or analyze discrete audio tokenization into a finite vocabulary, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on frequency-wise normalization of spectrogram inputs in Audio Spectrogram Transformers for ASC and does not propose or analyze discrete audio tokenization into a finite vocabulary, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on frequency-wise normalization techniques in Audio Spectrogram Transformers for Acoustic Scene Classification, primarily using continuous audio features like spectrograms without proposing or analyzing any discrete audio tokenization methods or discrete symbol sequences relevant to audio reconstruction or understanding; thus, it does not meet the inclusion criteria which require explicit discrete tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on frequency-wise normalization techniques in Audio Spectrogram Transformers for Acoustic Scene Classification, primarily using continuous audio features like spectrograms without proposing or analyzing any discrete audio tokenization methods or discrete symbol sequences relevant to audio reconstruction or understanding; thus, it does not meet the inclusion criteria which require explicit discrete tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
    "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
    "metadata": {
      "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
      "summary": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "doi": "https://doi.org/10.48550/arxiv.2306.00561",
      "openalex_id": "https://openalex.org/W4379256313",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a multi-window masked autoencoder for audio but does not explicitly state any discrete audio tokenization or a discrete vocabulary as the core object of study; without clear tokenization analysis, it does not satisfy the inclusion criterion focused on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a multi-window masked autoencoder for audio but does not explicitly state any discrete audio tokenization or a discrete vocabulary as the core object of study; without clear tokenization analysis, it does not satisfy the inclusion criterion focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Multi-Window Masked Autoencoder with novel attention modules for learning better audio representations, but it does not explicitly discuss discrete audio/speech tokenization or the conversion of continuous signals into discrete token sequences, focusing instead on continuous feature modeling and representation learning without discrete quantization analysis, thus failing to meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Multi-Window Masked Autoencoder with novel attention modules for learning better audio representations, but it does not explicitly discuss discrete audio/speech tokenization or the conversion of continuous signals into discrete token sequences, focusing instead on continuous feature modeling and representation learning without discrete quantization analysis, thus failing to meet the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures",
    "abstract": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
    "metadata": {
      "title": "uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures",
      "summary": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "abstract": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446342",
      "openalex_id": "https://openalex.org/W4392902846",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes MAEs, contrastive tuning, and unsupervised audio mixtures but does not present discrete audio tokenization or token-based representations as a core object of study; it focuses on continuous representations and mixing strategies, thus failing the discrete tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes MAEs, contrastive tuning, and unsupervised audio mixtures but does not present discrete audio tokenization or token-based representations as a core object of study; it focuses on continuous representations and mixing strategies, thus failing the discrete tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on tuning pretrained audio transformers using unsupervised audio mixtures and contrastive tuning, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous signals into discrete tokens; it rather deals with representation learning and adaptation of MAEs without clear discrete tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on tuning pretrained audio transformers using unsupervised audio mixtures and contrastive tuning, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous signals into discrete tokens; it rather deals with representation learning and adaptation of MAEs without clear discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enhancing Audio Generation Diversity with Visual Information",
    "abstract": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
    "metadata": {
      "title": "Enhancing Audio Generation Diversity with Visual Information",
      "summary": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "abstract": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447384",
      "openalex_id": "https://openalex.org/W4392909638",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses improving audio generation diversity using visual information with clustering, but does not propose or analyze discrete audio tokenization or token-based representations; it relies on continuous audio generation and visual cues, thus failing the inclusion criteria and aligning with exclusion criteria on lack of discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses improving audio generation diversity using visual information with clustering, but does not propose or analyze discrete audio tokenization or token-based representations; it relies on continuous audio generation and visual cues, thus failing the inclusion criteria and aligning with exclusion criteria on lack of discrete token analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on enhancing audio generation diversity using visual information through a clustering-based method, but it does not mention discrete audio/speech tokenization or related analysis; thus it does not meet the core inclusion criterion regarding discrete tokenization, nor does it discuss any discrete quantization or token design relevant to audio reconstruction or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on enhancing audio generation diversity using visual information through a clustering-based method, but it does not mention discrete audio/speech tokenization or related analysis; thus it does not meet the core inclusion criterion regarding discrete tokenization, nor does it discuss any discrete quantization or token design relevant to audio reconstruction or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Impact of Parroting Mode on Cross-Lingual Speaker Recognition",
    "abstract": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
    "metadata": {
      "title": "The Impact of Parroting Mode on Cross-Lingual Speaker Recognition",
      "summary": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "abstract": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00035",
      "openalex_id": "https://openalex.org/W4392980346",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on cross-lingual speaker recognition using embeddings and a parroting mode; it does not propose or evaluate discrete audio tokens or a tokenization scheme, hence it does not meet the inclusion criteria and falls under the exclusion criterion for studies relying on continuous audio features rather than discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on cross-lingual speaker recognition using embeddings and a parroting mode; it does not propose or evaluate discrete audio tokens or a tokenization scheme, hence it does not meet the inclusion criteria and falls under the exclusion criterion for studies relying on continuous audio features rather than discrete token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating audio embeddings for cross-lingual speaker recognition but does not mention proposing, analyzing, or evaluating discrete audio tokenization where a continuous signal is converted into discrete symbols; instead, it mainly discusses embeddings and their performance without detailing a discrete tokenization method, thus failing to meet the inclusion criteria and fitting exclusion criteria concerning lack of discrete quantization/token design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating audio embeddings for cross-lingual speaker recognition but does not mention proposing, analyzing, or evaluating discrete audio tokenization where a continuous signal is converted into discrete symbols; instead, it mainly discusses embeddings and their performance without detailing a discrete tokenization method, thus failing to meet the inclusion criteria and fitting exclusion criteria concerning lack of discrete quantization/token design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Weakly Labeled Sound Event Detection using Attention Mechanism with Teacher-Student Model",
    "abstract": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
    "metadata": {
      "title": "Weakly Labeled Sound Event Detection using Attention Mechanism with Teacher-Student Model",
      "summary": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "abstract": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00034",
      "openalex_id": "https://openalex.org/W4392980966",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on weakly supervised sound event detection and attention/teacher-student models using continuous audio features; there is no discrete audio tokenization or token-level representation discussed, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study focuses on weakly supervised sound event detection and attention/teacher-student models using continuous audio features; there is no discrete audio tokenization or token-level representation discussed, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing sound event detection using attention mechanisms and teacher-student learning models with low- and high-level continuous audio features such as MFCC and log-Mel spectrograms. However, there is no mention or analysis of discrete audio tokenization or quantization methods converting continuous signals into discrete token sequences, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria and falls under exclusion for lack of discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing sound event detection using attention mechanisms and teacher-student learning models with low- and high-level continuous audio features such as MFCC and log-Mel spectrograms. However, there is no mention or analysis of discrete audio tokenization or quantization methods converting continuous signals into discrete token sequences, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria and falls under exclusion for lack of discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speech2Face3D: A Two‐Stage Transfer‐Learning Framework for Speech‐Driven 3D Facial Animation",
    "abstract": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
    "metadata": {
      "title": "Speech2Face3D: A Two‐Stage Transfer‐Learning Framework for Speech‐Driven 3D Facial Animation",
      "summary": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "abstract": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "doi": "https://doi.org/10.1049/ipr2.70155",
      "openalex_id": "https://openalex.org/W4412613010",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets speech-driven 3D facial animation and mentions a multi-scale audio encoder, but it does not explicitly propose, analyze, or evaluate discrete audio/token quantization into a finite vocabulary; it appears to rely on continuous speech representations rather than discrete tokens, so it fails the inclusion criterion for discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets speech-driven 3D facial animation and mentions a multi-scale audio encoder, but it does not explicitly propose, analyze, or evaluate discrete audio/token quantization into a finite vocabulary; it appears to rely on continuous speech representations rather than discrete tokens, so it fails the inclusion criterion for discrete tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech-driven 3D facial animation framework using a transfer-learning approach with multi-scale audio encoding; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is the core inclusion criterion, nor does it discuss discrete token design or analysis relevant to language-model-style token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech-driven 3D facial animation framework using a transfer-learning approach with multi-scale audio encoding; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is the core inclusion criterion, nor does it discuss discrete token design or analysis relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Artificial Intelligence &amp; Creativity: A Manifesto for Collaboration",
    "abstract": "ABSTRACT With the advent of artificial intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human–machine collaboration on creative tasks and proposes “fundamental laws of generative AI” to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and “Shut down,” each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI‐generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
    "metadata": {
      "title": "Artificial Intelligence &amp; Creativity: A Manifesto for Collaboration",
      "summary": "ABSTRACT With the advent of artificial intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human–machine collaboration on creative tasks and proposes “fundamental laws of generative AI” to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and “Shut down,” each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI‐generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "abstract": "ABSTRACT With the advent of artificial intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human–machine collaboration on creative tasks and proposes “fundamental laws of generative AI” to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and “Shut down,” each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI‐generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "doi": "https://doi.org/10.1002/jocb.597",
      "openalex_id": "https://openalex.org/W4380988459",
      "arxiv_id": "",
      "publication_date": "2023-06-15",
      "published": "2023-06-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses AI creativity and collaboration, with no focus on discrete audio/speech tokenization, no tokenizer-based analysis, and thus fails to meet the inclusion criteria requiring discrete audio tokenization as the core object.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses AI creativity and collaboration, with no focus on discrete audio/speech tokenization, no tokenizer-based analysis, and thus fails to meet the inclusion criteria requiring discrete audio tokenization as the core object.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on AI and creativity collaboration, ethical use, and generative AI scenarios without any mention or analysis of discrete audio/speech tokenization or discrete token representations in audio processing, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on AI and creativity collaboration, ethical use, and generative AI scenarios without any mention or analysis of discrete audio/speech tokenization or discrete token representations in audio processing, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
    "metadata": {
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.1055",
      "openalex_id": "https://openalex.org/W4389524500",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete speech representations and cross-modal tokenization for a large language model, implying discrete audio tokenization as a core object of study.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete speech representations and cross-modal tokenization for a large language model, implying discrete audio tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes SpeechGPT, a multi-modal large language model featuring discrete speech representations and a large-scale cross-modal speech instruction dataset, indicating that discrete audio/speech tokenization is a core part of the work; experimental validation is provided, and the full text is available in English, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes SpeechGPT, a multi-modal large language model featuring discrete speech representations and a large-scale cross-modal speech instruction dataset, indicating that discrete audio/speech tokenization is a core part of the work; experimental validation is provided, and the full text is available in English, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Character-LLM: A Trainable Agent for Role-Playing",
    "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
    "metadata": {
      "title": "Character-LLM: A Trainable Agent for Role-Playing",
      "summary": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
      "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-main.814",
      "openalex_id": "https://openalex.org/W4389519488",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on character-based LLM agents and does not address discrete audio tokenization or token-level audio representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on character-based LLM agents and does not address discrete audio tokenization or token-level audio representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training language models as agents for role-playing specific characters, with no mention or analysis of discrete audio/speech tokenization, nor conversion of continuous audio signals into discrete tokens as required by the inclusion criteria; it primarily deals with text-based language modeling without addressing audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training language models as agents for role-playing specific characters, with no mention or analysis of discrete audio/speech tokenization, nor conversion of continuous audio signals into discrete tokens as required by the inclusion criteria; it primarily deals with text-based language modeling without addressing audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Combined scaling for zero-shot transfer learning",
    "abstract": "Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.",
    "metadata": {
      "title": "Combined scaling for zero-shot transfer learning",
      "summary": "Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.",
      "abstract": "Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.",
      "doi": "https://doi.org/10.1016/j.neucom.2023.126658",
      "openalex_id": "https://openalex.org/W3214803981",
      "arxiv_id": "",
      "publication_date": "2023-08-05",
      "published": "2023-08-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describes scaling CLIP/ALIGN image–text models and does not involve discrete audio/speech tokenization or analysis of discrete tokens for audio tasks, so it fails the audio/token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describes scaling CLIP/ALIGN image–text models and does not involve discrete audio/speech tokenization or analysis of discrete tokens for audio tasks, so it fails the audio/token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on combined scaling methods for zero-shot transfer learning using image-text models, discussing contrastive batch sizes and large-scale datasets, with no mention of audio/speech tokenization, discrete symbol conversion of audio signals, or relevant audio tasks. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to the core topic of tokenizing audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on combined scaling methods for zero-shot transfer learning using image-text models, discussing contrastive batch sizes and large-scale datasets, with no mention of audio/speech tokenization, discrete symbol conversion of audio signals, or relevant audio tasks. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to the core topic of tokenizing audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
    "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
    "metadata": {
      "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
      "summary": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389705",
      "openalex_id": "https://openalex.org/W4391021666",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses decoder-only architectures and CTC-based alignment but does not explicitly propose, analyze, or evaluate discrete audio tokenization (converting continuous audio signals into a discrete token sequence) as the core object of study; it appears to rely on continuous acoustic features mapped into an LLM’s semantic space rather than focusing on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses decoder-only architectures and CTC-based alignment but does not explicitly propose, analyze, or evaluate discrete audio tokenization (converting continuous audio signals into a discrete token sequence) as the core object of study; it appears to rely on continuous acoustic features mapped into an LLM’s semantic space rather than focusing on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Speech-LLaMA which integrates acoustic information into large language models using a simple audio encoder and Connectionist Temporal Classification, but it does not explicitly state that it performs discrete audio tokenization or quantization of audio into discrete symbols as a core component; instead, it seems to rely on continuous acoustic features mapped into semantic space without detailed analysis or design of discrete token sequences for audio reconstruction or generation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes Speech-LLaMA which integrates acoustic information into large language models using a simple audio encoder and Connectionist Temporal Classification, but it does not explicitly state that it performs discrete audio tokenization or quantization of audio into discrete symbols as a core component; instead, it seems to rely on continuous acoustic features mapped into semantic space without detailed analysis or design of discrete token sequences for audio reconstruction or generation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
    "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
    "metadata": {
      "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
      "summary": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447523",
      "openalex_id": "https://openalex.org/W4392903389",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a neural speech codec toolkit (FunCodec) focusing on reproducible training, inference, pretrained models, and integration with downstream tasks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study (no mention of discrete tokens/quantization for audio reconstruction or generation). It appears to be more of an engineering/toolkit paper rather than a study centered on tokenization methods.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item describes a neural speech codec toolkit (FunCodec) focusing on reproducible training, inference, pretrained models, and integration with downstream tasks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study (no mention of discrete tokens/quantization for audio reconstruction or generation). It appears to be more of an engineering/toolkit paper rather than a study centered on tokenization methods.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces FunCodec, a neural speech codec toolkit that includes models like SoundStream and Encodec, with a focus on discrete neural codec models which convert continuous speech into discrete representations for reconstruction and downstream tasks. It includes experimental validation comparing reconstruction quality and applicability to speech tasks such as ASR and TTS, indicating methodological contributions beyond a mere engineering report. Therefore, it aligns well with the inclusion criteria on discrete audio/speech tokenization and excludes traditional codecs or purely continuous feature usage, making it suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces FunCodec, a neural speech codec toolkit that includes models like SoundStream and Encodec, with a focus on discrete neural codec models which convert continuous speech into discrete representations for reconstruction and downstream tasks. It includes experimental validation comparing reconstruction quality and applicability to speech tasks such as ASR and TTS, indicating methodological contributions beyond a mere engineering report. Therefore, it aligns well with the inclusion criteria on discrete audio/speech tokenization and excludes traditional codecs or purely continuous feature usage, making it suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raised valid points: JuniorNano viewed this as an engineering/toolkit paper, while JuniorMini noted the toolkit centers on neural codec models (SoundStream, Encodec) that use quantization to produce discrete representations and that the paper proposes a new FreqCodec with experimental comparisons and downstream ASR/TTS evaluation. The abstract explicitly references modern neural codecs (which are quantization/discrete-code based), introduces a novel frequency-domain codec (methodological contribution), reports reconstruction and downstream-task experiments, and provides pretrained models and code (English, publicly available). Therefore it meets the inclusion criteria of proposing/evaluating discrete audio tokenization rather than being purely an engineering-only report, so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raised valid points: JuniorNano viewed this as an engineering/toolkit paper, while JuniorMini noted the toolkit centers on neural codec models (SoundStream, Encodec) that use quantization to produce discrete representations and that the paper proposes a new FreqCodec with experimental comparisons and downstream ASR/TTS evaluation. The abstract explicitly references modern neural codecs (which are quantization/discrete-code based), introduces a novel frequency-domain codec (methodological contribution), reports reconstruction and downstream-task experiments, and provides pretrained models and code (English, publicly available). Therefore it meets the inclusion criteria of proposing/evaluating discrete audio tokenization rather than being purely an engineering-only report, so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "title": "VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447112",
      "openalex_id": "https://openalex.org/W4392904805",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes using discrete speech tokens derived from self-supervised features as a core component, aligning with the inclusion criterion for discrete audio/speech tokenization, and there is no evidence of exclusion criteria such as relying solely on continuous features or focusing on non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes using discrete speech tokens derived from self-supervised features as a core component, aligning with the inclusion criterion for discrete audio/speech tokenization, and there is no evidence of exclusion criteria such as relying solely on continuous features or focusing on non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VoxtLM, a decoder-only language model integrating discrete speech tokens derived from self-supervised speech features, indicating the use and analysis of discrete audio tokenization as a core aspect, which meets the inclusion criteria. Additionally, the study is in English and appears to contribute methodologically by improving multiple speech tasks and open-sourcing the model, excluding it from engineering-only reports. There is no indication that only continuous features are used or that the focus is on traditional codecs; rather, it is modern quantization-based token representation relevant to language models. Therefore, the study aligns with inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VoxtLM, a decoder-only language model integrating discrete speech tokens derived from self-supervised speech features, indicating the use and analysis of discrete audio tokenization as a core aspect, which meets the inclusion criteria. Additionally, the study is in English and appears to contribute methodologically by improving multiple speech tasks and open-sourcing the model, excluding it from engineering-only reports. There is no indication that only continuous features are used or that the focus is on traditional codecs; rather, it is modern quantization-based token representation relevant to language models. Therefore, the study aligns with inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
    "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
    "metadata": {
      "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
      "summary": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "doi": "https://doi.org/10.1109/taslp.2024.3451951",
      "openalex_id": "https://openalex.org/W4402301063",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete speech representations via self-supervised quantization (quantized latent tokens) used for multilingual, multispeaker TTS, which constitutes a core discrete-tokenization object, not merely continuous features, so it meets inclusion and avoids exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete speech representations via self-supervised quantization (quantized latent tokens) used for multilingual, multispeaker TTS, which constitutes a core discrete-tokenization object, not merely continuous features, so it meets inclusion and avoids exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly employs quantized latent speech representations from a self-supervised model, indicating discrete audio tokenization as a core focus; it analyzes and evaluates these discrete tokens in a multilingual and multispeaker TTS system with experimental validation, thus meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly employs quantized latent speech representations from a self-supervised model, indicating discrete audio tokenization as a core focus; it analyzes and evaluates these discrete tokens in a multilingual and multispeaker TTS system with experimental validation, thus meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Ubiq-Genie: Leveraging External Frameworks for Enhanced Social VR Experiences",
    "abstract": "This paper describes the Ubiq-Genie framework for integrating external frameworks with the Ubiq social VR platform. The proposed architecture is modular, allowing for easy integration of services and providing mechanisms to offload computationally intensive processes to a server. To showcase the capabilities of the framework, we present two prototype applications: 1) a voice- and gesture-controlled texture generation method based on Stable Diffusion 2.0 and 2) an embodied conversational agent based on ChatGPT. This work aims to demonstrate the potential of integrating external frameworks into social VR for the creation of new types of collaborative experiences.",
    "metadata": {
      "title": "Ubiq-Genie: Leveraging External Frameworks for Enhanced Social VR Experiences",
      "summary": "This paper describes the Ubiq-Genie framework for integrating external frameworks with the Ubiq social VR platform. The proposed architecture is modular, allowing for easy integration of services and providing mechanisms to offload computationally intensive processes to a server. To showcase the capabilities of the framework, we present two prototype applications: 1) a voice- and gesture-controlled texture generation method based on Stable Diffusion 2.0 and 2) an embodied conversational agent based on ChatGPT. This work aims to demonstrate the potential of integrating external frameworks into social VR for the creation of new types of collaborative experiences.",
      "abstract": "This paper describes the Ubiq-Genie framework for integrating external frameworks with the Ubiq social VR platform. The proposed architecture is modular, allowing for easy integration of services and providing mechanisms to offload computationally intensive processes to a server. To showcase the capabilities of the framework, we present two prototype applications: 1) a voice- and gesture-controlled texture generation method based on Stable Diffusion 2.0 and 2) an embodied conversational agent based on ChatGPT. This work aims to demonstrate the potential of integrating external frameworks into social VR for the creation of new types of collaborative experiences.",
      "doi": "https://doi.org/10.1109/vrw58643.2023.00108",
      "openalex_id": "https://openalex.org/W4367662705",
      "arxiv_id": "",
      "publication_date": "2023-03-01",
      "published": "2023-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a modular VR framework and AI-enabled prototypes, but it does not address discrete audio tokenization or analysis of speech/audio tokens, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a modular VR framework and AI-enabled prototypes, but it does not address discrete audio tokenization or analysis of speech/audio tokens, which is the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a framework for social VR that integrates external services including voice- and gesture-controlled texture generation and a conversational agent; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization methods as a core object of study, nor does it address discrete audio token representations or their analysis, instead it appears more as an engineering/system integration report without a methodological contribution related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a framework for social VR that integrates external services including voice- and gesture-controlled texture generation and a conversational agent; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization methods as a core object of study, nor does it address discrete audio token representations or their analysis, instead it appears more as an engineering/system integration report without a methodological contribution related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems",
    "abstract": "This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions with, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multi-modal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.",
    "metadata": {
      "title": "Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems",
      "summary": "This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions with, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multi-modal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.",
      "abstract": "This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions with, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multi-modal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.",
      "doi": "https://doi.org/10.1109/itsc57777.2023.10422003",
      "openalex_id": "https://openalex.org/W4391770514",
      "arxiv_id": "",
      "publication_date": "2023-09-24",
      "published": "2023-09-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers high-level aspects of generative AI in intelligent vehicles but does not address discrete audio/speech tokenization as a core focus; there is no tokenization analysis of speech signals, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers high-level aspects of generative AI in intelligent vehicles but does not address discrete audio/speech tokenization as a core focus; there is no tokenization analysis of speech signals, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses broadly on generative artificial intelligence applications in intelligent vehicles, covering speech, audio, vision, and multimodal interactions, but does not explicitly discuss discrete audio/speech tokenization or analyze tokenization methods as required by the inclusion criteria; thus, it likely lacks the core methodological focus on discrete tokenization of audio signals necessary for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses broadly on generative artificial intelligence applications in intelligent vehicles, covering speech, audio, vision, and multimodal interactions, but does not explicitly discuss discrete audio/speech tokenization or analyze tokenization methods as required by the inclusion criteria; thus, it likely lacks the core methodological focus on discrete tokenization of audio signals necessary for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study",
    "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
    "metadata": {
      "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study",
      "summary": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447929",
      "openalex_id": "https://openalex.org/W4392909068",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on discrete speech units/tokenization derived from self-supervised representations and evaluates them across ASR, translation, and SLU tasks, which satisfies the core inclusion criteria and does not match any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article centers on discrete speech units/tokenization derived from self-supervised representations and evaluates them across ASR, translation, and SLU tasks, which satisfies the core inclusion criteria and does not match any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete speech units derived from self-supervised learning, analyzes their application in multiple speech processing tasks including recognition, translation, and understanding, and involves experimental validation, all meeting the inclusion criteria without any exclusion criteria being triggered.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete speech units derived from self-supervised learning, analyzes their application in multiple speech processing tasks including recognition, translation, and understanding, and involves experimental validation, all meeting the inclusion criteria without any exclusion criteria being triggered.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Artificial Intelligence &amp;amp; Creativity: A manifesto for collaboration",
    "abstract": "Creativity is a phenomenon that emerges in the human-sociocultural and machine-artificial layers. With the advent of Artificial Intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human-machine collaboration on creative tasks and proposes \"fundamental laws of generative AI\" to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: \"Co-Cre-AI-tion\", \"Organic\", \"Plagiarism 3.0\", and “Shut down”, each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI-generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
    "metadata": {
      "title": "Artificial Intelligence &amp;amp; Creativity: A manifesto for collaboration",
      "summary": "Creativity is a phenomenon that emerges in the human-sociocultural and machine-artificial layers. With the advent of Artificial Intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human-machine collaboration on creative tasks and proposes \"fundamental laws of generative AI\" to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: \"Co-Cre-AI-tion\", \"Organic\", \"Plagiarism 3.0\", and “Shut down”, each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI-generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "abstract": "Creativity is a phenomenon that emerges in the human-sociocultural and machine-artificial layers. With the advent of Artificial Intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human-machine collaboration on creative tasks and proposes \"fundamental laws of generative AI\" to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: \"Co-Cre-AI-tion\", \"Organic\", \"Plagiarism 3.0\", and “Shut down”, each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI-generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "doi": "https://doi.org/10.31234/osf.io/ukqc9",
      "openalex_id": "https://openalex.org/W4368370051",
      "arxiv_id": "",
      "publication_date": "2023-05-03",
      "published": "2023-05-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses AI-human collaboration in creativity and ethics and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations for audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses AI-human collaboration in creativity and ethics and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations for audio tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on human-machine collaboration in creativity and ethical considerations of AI but does not explicitly mention discrete audio/speech tokenization, neural encoding, or relevant methodological analysis related to audio tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on human-machine collaboration in creativity and ethical considerations of AI but does not explicitly mention discrete audio/speech tokenization, neural encoding, or relevant methodological analysis related to audio tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation",
    "abstract": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation",
      "summary": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447553",
      "openalex_id": "https://openalex.org/W4392903872",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The SALM abstract describes using a frozen text LLM, an audio encoder, and adapters to process speech via continuous representations, with no explicit discussion or evaluation of discrete audio tokenization, so it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The SALM abstract describes using a frozen text LLM, an audio encoder, and adapters to process speech via continuous representations, with no explicit discussion or evaluation of discrete audio tokenization, so it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Speech-Augmented Language Model (SALM) for speech recognition and translation but does not explicitly mention discrete audio/speech tokenization or analyze converting continuous audio signals into sequences of discrete tokens; it mainly focuses on model architecture and in-context learning capabilities using continuous speech features and does not provide evidence of methodological contributions or analysis related to discrete tokenization of audio.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Speech-Augmented Language Model (SALM) for speech recognition and translation but does not explicitly mention discrete audio/speech tokenization or analyze converting continuous audio signals into sequences of discrete tokens; it mainly focuses on model architecture and in-context learning capabilities using continuous speech features and does not provide evidence of methodological contributions or analysis related to discrete tokenization of audio.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection",
    "abstract": "Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the cross-dataset scenario.",
    "metadata": {
      "title": "An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection",
      "summary": "Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the cross-dataset scenario.",
      "abstract": "Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the cross-dataset scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448196",
      "openalex_id": "https://openalex.org/W4392903271",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on continuous feature embeddings and frame-level detection for partially spoofed audio and does not introduce or analyze discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criterion and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on continuous feature embeddings and frame-level detection for partially spoofed audio and does not introduce or analyze discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criterion and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on partially spoofed audio detection using embedding similarity and temporal convolution for frame-level authenticity detection, but it does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens; it only discusses continuous frame-level features without indication of discrete token vocabularies or quantization, thus not meeting the inclusion criteria and fitting the exclusion criteria for continuous features only.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on partially spoofed audio detection using embedding similarity and temporal convolution for frame-level authenticity detection, but it does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens; it only discusses continuous frame-level features without indication of discrete token vocabularies or quantization, thus not meeting the inclusion criteria and fitting the exclusion criteria for continuous features only.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
    "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
    "metadata": {
      "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
      "summary": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
      "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
      "doi": "https://doi.org/10.1109/taslp.2024.3393714",
      "openalex_id": "https://openalex.org/W4395447377",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses zero-shot and few-shot speaker-adaptive TTS and related modeling components, but there is no discussion of discrete audio tokenization or quantization as a core object; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses zero-shot and few-shot speaker-adaptive TTS and related modeling components, but there is no discussion of discrete audio tokenization or quantization as a core object; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker-adaptive text-to-speech synthesis, discussing zero-shot and few-shot adaptation methods and their architectural designs, but it does not explicitly mention discrete audio or speech tokenization concepts such as converting continuous audio signals into sequences of discrete tokens for reconstruction or generation tasks. Therefore, it does not meet the inclusion criterion about discrete tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker-adaptive text-to-speech synthesis, discussing zero-shot and few-shot adaptation methods and their architectural designs, but it does not explicitly mention discrete audio or speech tokenization concepts such as converting continuous audio signals into sequences of discrete tokens for reconstruction or generation tasks. Therefore, it does not meet the inclusion criterion about discrete tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speaker Adaptive Text-to-Speech With Timbre-Normalized Vector-Quantized Feature",
    "abstract": "Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.",
    "metadata": {
      "title": "Speaker Adaptive Text-to-Speech With Timbre-Normalized Vector-Quantized Feature",
      "summary": "Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.",
      "abstract": "Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.",
      "doi": "https://doi.org/10.1109/taslp.2023.3308374",
      "openalex_id": "https://openalex.org/W4386133927",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work centers on discrete vector-quantized acoustic features used as tokens for speech generation, aligning with the inclusion criterion for explicit discrete tokenization analysis.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work centers on discrete vector-quantized acoustic features used as tokens for speech generation, aligning with the inclusion criterion for explicit discrete tokenization analysis.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a text-to-speech system using timbre-normalized vector-quantized acoustic features, explicitly involving discrete tokenization of audio signals into quantized features for speaker adaptation and synthesis tasks. This aligns well with the inclusion criteria of analyzing discrete audio tokenization, and it distinctly avoids exclusion criteria as it employs modern neural quantization methods rather than traditional codecs or purely continuous features. Therefore, the study fits the criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a text-to-speech system using timbre-normalized vector-quantized acoustic features, explicitly involving discrete tokenization of audio signals into quantized features for speaker adaptation and synthesis tasks. This aligns well with the inclusion criteria of analyzing discrete audio tokenization, and it distinctly avoids exclusion criteria as it employs modern neural quantization methods rather than traditional codecs or purely continuous features. Therefore, the study fits the criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Fewer-Token Neural Speech Codec with Time-Invariant Codes",
    "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.",
    "metadata": {
      "title": "Fewer-Token Neural Speech Codec with Time-Invariant Codes",
      "summary": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.",
      "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448454",
      "openalex_id": "https://openalex.org/W4392903006",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a neural speech codec that encodes speech into discrete tokens/codes (time-invariant codes) to reduce token counts, directly addressing discrete tokenization for speech tasks, so it meets inclusion and does not fall under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural speech codec that encodes speech into discrete tokens/codes (time-invariant codes) to reduce token counts, directly addressing discrete tokenization for speech tasks, so it meets inclusion and does not fall under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a neural speech codec that converts continuous speech signals into discrete token representations, focusing on reducing token count while improving reconstruction quality and synthesis performance; it involves modern neural quantization methods relevant for language model-based TTS, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a neural speech codec that converts continuous speech signals into discrete token representations, focusing on reducing token count while improving reconstruction quality and synthesis performance; it involves modern neural quantization methods relevant for language model-based TTS, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TextrolSpeech: A Text Style Control Speech Corpus with Codec Language Text-to-Speech Models",
    "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.",
    "metadata": {
      "title": "TextrolSpeech: A Text Style Control Speech Corpus with Codec Language Text-to-Speech Models",
      "summary": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.",
      "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445879",
      "openalex_id": "https://openalex.org/W4392904245",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes using 'audio codec codes' as an intermediate discrete representation for speech in a TTS system, i.e., explicit discrete tokenization of speech, meeting inclusion criteria; the paper is in English.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes using 'audio codec codes' as an intermediate discrete representation for speech in a TTS system, i.e., explicit discrete tokenization of speech, meeting inclusion criteria; the paper is in English.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a new speech corpus with natural text prompts and introduces Salle, an architecture using audio codec codes as intermediate discrete representations to replace traditional mel-spectrograms in controllable TTS, indicating a focus on discrete audio tokenization with neural encoding relevant to language-model-style token sequences, thus meeting inclusion criteria and avoiding exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a new speech corpus with natural text prompts and introduces Salle, an architecture using audio codec codes as intermediate discrete representations to replace traditional mel-spectrograms in controllable TTS, indicating a focus on discrete audio tokenization with neural encoding relevant to language-model-style token sequences, thus meeting inclusion criteria and avoiding exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SELM: Speech Enhancement using Discrete Tokens and Language Models",
    "abstract": "Language models (LMs) have recently shown superior performances in various speech generation tasks, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information is advantageous for speech enhancement tasks. In light of this, we propose SELM, a novel speech enhancement paradigm that integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a de-tokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics and superior subjective perception results. Our demos are available <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "SELM: Speech Enhancement using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have recently shown superior performances in various speech generation tasks, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information is advantageous for speech enhancement tasks. In light of this, we propose SELM, a novel speech enhancement paradigm that integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a de-tokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics and superior subjective perception results. Our demos are available <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "Language models (LMs) have recently shown superior performances in various speech generation tasks, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information is advantageous for speech enhancement tasks. In light of this, we propose SELM, a novel speech enhancement paradigm that integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a de-tokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics and superior subjective perception results. Our demos are available <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447464",
      "openalex_id": "https://openalex.org/W4392909571",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract centers on converting continuous speech to discrete tokens via SSL and k-means tokenization for subsequent modeling and restoration, directly addressing discrete audio tokenization as the core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Abstract centers on converting continuous speech to discrete tokens via SSL and k-means tokenization for subsequent modeling and restoration, directly addressing discrete audio tokenization as the core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a speech enhancement method utilizing discrete tokens generated from continuous audio signals via self-supervised learning models and k-means tokenization, meeting the inclusion criteria of focusing on discrete audio tokenization for speech tasks, with experimental validation provided, thus warranting inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a speech enhancement method utilizing discrete tokens generated from continuous audio signals via self-supervised learning models and k-means tokenization, meeting the inclusion criteria of focusing on discrete audio tokenization for speech tasks, with experimental validation provided, thus warranting inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MM-TTS: Multi-Modal Prompt Based Style Transfer for Expressive Text-to-Speech Synthesis",
    "abstract": "The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.",
    "metadata": {
      "title": "MM-TTS: Multi-Modal Prompt Based Style Transfer for Expressive Text-to-Speech Synthesis",
      "summary": "The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.",
      "abstract": "The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29769",
      "openalex_id": "https://openalex.org/W4393152865",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on multi-modal style transfer in TTS and uses continuous spectral representations (Mel-spectrogram) and flow-based refiners; it does not propose or analyze discrete audio tokenization into a finite symbol sequence, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on multi-modal style transfer in TTS and uses continuous spectral representations (Mel-spectrogram) and flow-based refiners; it does not propose or analyze discrete audio tokenization into a finite symbol sequence, thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a multi-modal prompt-based style transfer framework for expressive Text-to-Speech synthesis focusing on style representation and transfer from various modalities (speech, facial images, text) but does not explicitly discuss discrete audio/speech tokenization or converting continuous audio into sequences of discrete tokens; it primarily works with continuous features like Mel-spectrograms and does not present methodological contributions or analysis related to discrete audio tokenization as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a multi-modal prompt-based style transfer framework for expressive Text-to-Speech synthesis focusing on style representation and transfer from various modalities (speech, facial images, text) but does not explicitly discuss discrete audio/speech tokenization or converting continuous audio into sequences of discrete tokens; it primarily works with continuous features like Mel-spectrograms and does not present methodological contributions or analysis related to discrete audio tokenization as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Using Joint Training Speaker Encoder With Consistency Loss to Achieve Cross-Lingual Voice Conversion and Expressive Voice Conversion",
    "abstract": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a joint training speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the Phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
    "metadata": {
      "title": "Using Joint Training Speaker Encoder With Consistency Loss to Achieve Cross-Lingual Voice Conversion and Expressive Voice Conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a joint training speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the Phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "abstract": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a joint training speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the Phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389651",
      "openalex_id": "https://openalex.org/W4391021367",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and phonetic/posterior features, but does not explicitly propose, analyze, or evaluate discrete audio tokenization (a quantized sequence of discrete tokens) as the core object; thus it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and phonetic/posterior features, but does not explicitly propose, analyze, or evaluate discrete audio tokenization (a quantized sequence of discrete tokens) as the core object; thus it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using continuous content features like phonetic posteriorgrams and speech recognition model outputs, but there is no explicit mention or analysis of discrete audio/speech tokenization or discrete token representations derived from continuous audio signals, which is a core inclusion criterion. Therefore, it does not meet the mandatory inclusion requirements related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using continuous content features like phonetic posteriorgrams and speech recognition model outputs, but there is no explicit mention or analysis of discrete audio/speech tokenization or discrete token representations derived from continuous audio signals, which is a core inclusion criterion. Therefore, it does not meet the mandatory inclusion requirements related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
    "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
    "metadata": {
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446998",
      "openalex_id": "https://openalex.org/W4392904154",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Explicitly studies discrete audio tokenization (audio tokens) as the core representation, comparing to mel-spectrogram features across speaker verification, diarization, and multilingual speech recognition.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Explicitly studies discrete audio tokenization (audio tokens) as the core representation, comparing to mel-spectrogram features across speaker verification, diarization, and multilingual speech recognition.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete audio tokenization methods as a core part of the research, comparing compression-based audio tokens with mel-spectrogram features across speech and speaker recognition tasks, satisfying the inclusion criteria while not falling into any exclusion criteria such as focusing only on continuous features or purely engineering reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete audio tokenization methods as a core part of the research, comparing compression-based audio tokens with mel-spectrogram features across speech and speaker recognition tasks, satisfying the inclusion criteria while not falling into any exclusion criteria such as focusing only on continuous features or purely engineering reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Creativity and Machine Learning: A Survey",
    "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
    "metadata": {
      "title": "Creativity and Machine Learning: A Survey",
      "summary": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
      "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
      "doi": "https://doi.org/10.48550/arxiv.2104.02726",
      "openalex_id": "https://openalex.org/W4287239377",
      "arxiv_id": "",
      "publication_date": "2021-04-06",
      "published": "2021-04-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a general survey of creativity in ML with no focus on discrete audio/speech tokenization or token-based representations for audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a general survey of creativity in ML with no focus on discrete audio/speech tokenization or token-based representations for audio tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a general survey on creativity and machine learning, focusing on computational creativity theories and generative deep learning, but do not mention any study or analysis of discrete audio/speech tokenization as a core subject; no indication of discrete audio token quantization or evaluation is present.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a general survey on creativity and machine learning, focusing on computational creativity theories and generative deep learning, but do not mention any study or analysis of discrete audio/speech tokenization as a core subject; no indication of discrete audio token quantization or evaluation is present.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding",
    "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding",
      "summary": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446203",
      "openalex_id": "https://openalex.org/W4392903524",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete speech representations and semantic coding for text-to-speech, analyzing discrete tokens/tokenization methods and diffusion-based token processing, which aligns with the inclusion criteria and avoids the stated exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete speech representations and semantic coding for text-to-speech, analyzing discrete tokens/tokenization methods and diffusion-based token processing, which aligns with the inclusion criteria and avoids the stated exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-to-speech methods that leverage discrete speech representations and semantic coding, addressing issues in discrete token modeling for speech synthesis. It proposes and evaluates novel models involving discrete semantic embeddings and diffusion models, directly aligning with the inclusion criterion of discrete audio/speech tokenization analysis. The abstract also indicates experimental validation and methodological contributions, and it concerns speech tasks specifically. Therefore, it meets all inclusion criteria and does not fall under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-to-speech methods that leverage discrete speech representations and semantic coding, addressing issues in discrete token modeling for speech synthesis. It proposes and evaluates novel models involving discrete semantic embeddings and diffusion models, directly aligning with the inclusion criterion of discrete audio/speech tokenization analysis. The abstract also indicates experimental validation and methodological contributions, and it concerns speech tasks specifically. Therefore, it meets all inclusion criteria and does not fall under the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Yet Another Generative Model for Room Impulse Response Estimation",
    "abstract": "Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.",
    "metadata": {
      "title": "Yet Another Generative Model for Room Impulse Response Estimation",
      "summary": "Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.",
      "abstract": "Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.",
      "doi": "https://doi.org/10.1109/waspaa58266.2023.10248189",
      "openalex_id": "https://openalex.org/W4386764631",
      "arxiv_id": "",
      "publication_date": "2023-09-15",
      "published": "2023-09-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on a discrete latent token space learned via residual quantization and casts room impulse response estimation as a reference-conditioned autoregressive token-generation task, i.e., a core discrete audio tokenization approach, with no evident exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on a discrete latent token space learned via residual quantization and casts room impulse response estimation as a reference-conditioned autoregressive token-generation task, i.e., a core discrete audio tokenization approach, with no evident exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an autoencoder with residual quantization that converts room impulse response audio into a discrete latent token space representing time-frequency patches, effectively addressing discrete audio tokenization for audio reconstruction and generation tasks, aligning well with the inclusion criteria and with methodological contributions and experimental validation; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes an autoencoder with residual quantization that converts room impulse response audio into a discrete latent token space representing time-frequency patches, effectively addressing discrete audio tokenization for audio reconstruction and generation tasks, aligning well with the inclusion criteria and with methodological contributions and experimental validation; thus it should be included.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "E3 TTS: Easy End-to-End Diffusion-Based Text To Speech",
    "abstract": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
    "metadata": {
      "title": "E3 TTS: Easy End-to-End Diffusion-Based Text To Speech",
      "summary": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "abstract": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389766",
      "openalex_id": "https://openalex.org/W4391021721",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents end-to-end diffusion-based TTS operating on continuous waveforms with no discrete audio tokenization, so it fails the discrete-token inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents end-to-end diffusion-based TTS operating on continuous waveforms with no discrete audio tokenization, so it fails the discrete-token inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on an end-to-end diffusion-based text-to-speech method generating audio waveforms directly from text without relying on intermediate discrete token representations or quantization; it focuses on continuous waveform modeling rather than proposing or analyzing discrete audio tokenization, thus failing to meet the inclusion criterion regarding discrete tokenization and fitting an exclusion category.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study centers on an end-to-end diffusion-based text-to-speech method generating audio waveforms directly from text without relying on intermediate discrete token representations or quantization; it focuses on continuous waveform modeling rather than proposing or analyzing discrete audio tokenization, thus failing to meet the inclusion criterion regarding discrete tokenization and fitting an exclusion category.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Noise-Robust Zero-Shot Text-to-Speech Synthesis Conditioned on Self-Supervised Speech-Representation Model with Adapters",
    "abstract": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
    "metadata": {
      "title": "Noise-Robust Zero-Shot Text-to-Speech Synthesis Conditioned on Self-Supervised Speech-Representation Model with Adapters",
      "summary": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
      "abstract": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447809",
      "openalex_id": "https://openalex.org/W4392910802",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on noise-robust zero-shot text-to-speech using SSL representations and adapters, with no discussion of discrete audio tokenization or quantization of audio into discrete symbols, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on noise-robust zero-shot text-to-speech using SSL representations and adapters, with no discussion of discrete audio tokenization or quantization of audio into discrete symbols, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving zero-shot text-to-speech synthesis robustness by incorporating adapters and speech enhancement but does not mention proposing or analyzing discrete audio/speech tokenization or discrete symbol sequences derived from continuous audio signals; it mainly deals with continuous features and model fine-tuning rather than discrete tokenization methods, failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving zero-shot text-to-speech synthesis robustness by incorporating adapters and speech enhancement but does not mention proposing or analyzing discrete audio/speech tokenization or discrete symbol sequences derived from continuous audio signals; it mainly deals with continuous features and model fine-tuning rather than discrete tokenization methods, failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Comparison of the Ability of Neural Network Model and Humans to Detect a Cloned Voice",
    "abstract": "The vulnerability of the speaker identity verification system to attacks using voice cloning was examined. The research project assumed creating a model for verifying the speaker’s identity based on voice biometrics and then testing its resistance to potential attacks using voice cloning. The Deep Speaker Neural Speaker Embedding System was trained, and the Real-Time Voice Cloning system was employed based on the SV2TTS, Tacotron, WaveRNN, and GE2E neural networks. The results of attacks using voice cloning were analyzed and discussed in the context of a subjective assessment of cloned voice fidelity. Subjective test results and attempts to authenticate speakers proved that the tested biometric identity verification system might resist voice cloning attacks even if humans cannot distinguish cloned samples from original ones.",
    "metadata": {
      "title": "Comparison of the Ability of Neural Network Model and Humans to Detect a Cloned Voice",
      "summary": "The vulnerability of the speaker identity verification system to attacks using voice cloning was examined. The research project assumed creating a model for verifying the speaker’s identity based on voice biometrics and then testing its resistance to potential attacks using voice cloning. The Deep Speaker Neural Speaker Embedding System was trained, and the Real-Time Voice Cloning system was employed based on the SV2TTS, Tacotron, WaveRNN, and GE2E neural networks. The results of attacks using voice cloning were analyzed and discussed in the context of a subjective assessment of cloned voice fidelity. Subjective test results and attempts to authenticate speakers proved that the tested biometric identity verification system might resist voice cloning attacks even if humans cannot distinguish cloned samples from original ones.",
      "abstract": "The vulnerability of the speaker identity verification system to attacks using voice cloning was examined. The research project assumed creating a model for verifying the speaker’s identity based on voice biometrics and then testing its resistance to potential attacks using voice cloning. The Deep Speaker Neural Speaker Embedding System was trained, and the Real-Time Voice Cloning system was employed based on the SV2TTS, Tacotron, WaveRNN, and GE2E neural networks. The results of attacks using voice cloning were analyzed and discussed in the context of a subjective assessment of cloned voice fidelity. Subjective test results and attempts to authenticate speakers proved that the tested biometric identity verification system might resist voice cloning attacks even if humans cannot distinguish cloned samples from original ones.",
      "doi": "https://doi.org/10.3390/electronics12214458",
      "openalex_id": "https://openalex.org/W4388017642",
      "arxiv_id": "",
      "publication_date": "2023-10-30",
      "published": "2023-10-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article analyzes voice-cloning attacks on biometric verification and compares NN/human detection but does not study discrete audio tokenization or token-based representations as its core object, so it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article analyzes voice-cloning attacks on biometric verification and compares NN/human detection but does not study discrete audio tokenization or token-based representations as its core object, so it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker verification and voice cloning attack resistance using neural network models, but there is no indication that it proposes, analyzes, or evaluates discrete audio/speech tokenization as a core subject; instead, it involves continuous audio feature-based neural systems without emphasis on discrete token representations or quantization-based methods relevant to discrete token sequences for audio reconstruction or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker verification and voice cloning attack resistance using neural network models, but there is no indication that it proposes, analyzes, or evaluates discrete audio/speech tokenization as a core subject; instead, it involves continuous audio feature-based neural systems without emphasis on discrete token representations or quantization-based methods relevant to discrete token sequences for audio reconstruction or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vision + Language Applications: A Survey",
    "abstract": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
    "metadata": {
      "title": "Vision + Language Applications: A Survey",
      "summary": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
      "abstract": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
      "doi": "https://doi.org/10.1109/cvprw59228.2023.00090",
      "openalex_id": "https://openalex.org/W4385801730",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses Vision + Language applications and a survey on text-to-image generation; it mentions audio only as part of multimodal scope and does not propose, analyze, or evaluate discrete audio/speech tokenization, so it fails the specific inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses Vision + Language applications and a survey on text-to-image generation; it mentions audio only as part of multimodal scope and does not propose, analyze, or evaluate discrete audio/speech tokenization, so it fails the specific inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a survey on vision and language applications, primarily focused on text-to-image generation and related multimodal applications, but does not explicitly address discrete audio/speech tokenization or any form of discrete audio token analysis as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper is a survey on vision and language applications, primarily focused on text-to-image generation and related multimodal applications, but does not explicitly address discrete audio/speech tokenization or any form of discrete audio token analysis as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023",
    "abstract": "In this paper, we present MuLanTTS, the Microsoft end-toend neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023.About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness.Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora.For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker finetuning.MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment.The excellent quality and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.",
    "metadata": {
      "title": "MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023",
      "summary": "In this paper, we present MuLanTTS, the Microsoft end-toend neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023.About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness.Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora.For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker finetuning.MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment.The excellent quality and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.",
      "abstract": "In this paper, we present MuLanTTS, the Microsoft end-toend neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023.About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness.Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora.For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker finetuning.MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment.The excellent quality and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.",
      "doi": "https://doi.org/10.21437/blizzard.2023-5",
      "openalex_id": "https://openalex.org/W4387941753",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on end-to-end neural TTS using continuous representations and does not analyze discrete audio tokenization or token-based discrete representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on end-to-end neural TTS using continuous representations and does not analyze discrete audio tokenization or token-based discrete representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a neural text-to-speech system focusing on TTS design, speaker adaptation, and prosody enhancement, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods for converting continuous audio into discrete symbols, nor does it discuss discrete token representations as a core object of study, thus failing the inclusion criteria and matching the exclusion criteria about lack of discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a neural text-to-speech system focusing on TTS design, speaker adaptation, and prosody enhancement, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods for converting continuous audio into discrete symbols, nor does it discuss discrete token representations as a core object of study, thus failing the inclusion criteria and matching the exclusion criteria about lack of discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "AutoPrep: An Automatic Preprocessing Framework for In-The-Wild Speech Data",
    "abstract": "Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "AutoPrep: An Automatic Preprocessing Framework for In-The-Wild Speech Data",
      "summary": "Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447759",
      "openalex_id": "https://openalex.org/W4392903247",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study describes an automatic preprocessing framework for in-the-wild speech data and ASR but does not propose or evaluate discrete audio tokenization or token representations, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study describes an automatic preprocessing framework for in-the-wild speech data and ASR but does not propose or evaluate discrete audio tokenization or token representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an automatic preprocessing framework for speech data, including tasks like speech enhancement, segmentation, speaker clustering, and transcription, but it does not explicitly propose or analyze discrete audio/speech tokenization where continuous audio is converted into discrete tokens for reconstruction or modeling; thus, it does not meet the inclusion criteria related to discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an automatic preprocessing framework for speech data, including tasks like speech enhancement, segmentation, speaker clustering, and transcription, but it does not explicitly propose or analyze discrete audio/speech tokenization where continuous audio is converted into discrete tokens for reconstruction or modeling; thus, it does not meet the inclusion criteria related to discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data",
    "abstract": "Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.",
    "metadata": {
      "title": "Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data",
      "summary": "Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.",
      "abstract": "Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448074",
      "openalex_id": "https://openalex.org/W4392903365",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes scaling multilingual TTS with unsupervised data and joint speech-text representations, but does not discuss discrete audio tokenization or converting audio into a discrete token sequence, which is a core requirement for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes scaling multilingual TTS with unsupervised data and joint speech-text representations, but does not discuss discrete audio tokenization or converting audio into a discrete token sequence, which is a core requirement for inclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on scaling multilingual speech synthesis without transcribed data, emphasizing joint speech and text representation learning, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization or a discrete quantization approach as a core contribution; rather, it revolves around TTS model training and intelligibility without detailing discrete token methods, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on scaling multilingual speech synthesis without transcribed data, emphasizing joint speech and text representation learning, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization or a discrete quantization approach as a core contribution; rather, it revolves around TTS model training and intelligibility without detailing discrete token methods, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
    "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
      "summary": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447926",
      "openalex_id": "https://openalex.org/W4392902778",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes a pre-processing stage that converts speech and text into two discrete token sequences of similar length, i.e., discrete audio tokenization, which is central to their cross-modal ST/MT framework; this aligns with the inclusion criterion and does not appear to be just continuous features or non-speech token analysis.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes a pre-processing stage that converts speech and text into two discrete token sequences of similar length, i.e., discrete audio tokenization, which is central to their cross-modal ST/MT framework; this aligns with the inclusion criterion and does not appear to be just continuous features or non-speech token analysis.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech-to-text translation framework involving hard parameter sharing and discrete token sequences of similar length for speech and text inputs, indicating some form of discrete tokenization for speech, but it does not clearly emphasize or analyze discrete audio/speech tokenization as a core object (in terms of neural discrete token design, quantization, or analysis) but rather on multi-tasking and shared parameter models for translation. Hence, it partially meets the inclusion criteria but lacks explicit core analysis or evaluation of discrete audio tokenization itself.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech-to-text translation framework involving hard parameter sharing and discrete token sequences of similar length for speech and text inputs, indicating some form of discrete tokenization for speech, but it does not clearly emphasize or analyze discrete audio/speech tokenization as a core object (in terms of neural discrete token design, quantization, or analysis) but rather on multi-tasking and shared parameter models for translation. Hence, it partially meets the inclusion criteria but lacks explicit core analysis or evaluation of discrete audio tokenization itself.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly states a pre-processing stage that converts speech and text into discrete token sequences of similar length, which suggests the use of discrete audio tokenization, and JuniorReviewer-A (JuniorNano) considered this sufficient for inclusion. However, the paper’s primary contribution described in the abstract is a hard-parameter-sharing multi-task ST/MT framework and BLEU improvements; the discrete-token conversion appears to be an enabling detail rather than the clearly proposed, analyzed, or systematically evaluated core object required by the inclusion criteria (JuniorMini raised this concern). Given the ambiguity in the abstract about whether the paper explicitly proposes, analyzes, or evaluates the discrete audio tokenization method itself (design, quantization, reconstruction, or ablation focus), I am uncertain if it meets the “core object of study” requirement.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly states a pre-processing stage that converts speech and text into discrete token sequences of similar length, which suggests the use of discrete audio tokenization, and JuniorReviewer-A (JuniorNano) considered this sufficient for inclusion. However, the paper’s primary contribution described in the abstract is a hard-parameter-sharing multi-task ST/MT framework and BLEU improvements; the discrete-token conversion appears to be an enabling detail rather than the clearly proposed, analyzed, or systematically evaluated core object required by the inclusion criteria (JuniorMini raised this concern). Given the ambiguity in the abstract about whether the paper explicitly proposes, analyzes, or evaluates the discrete audio tokenization method itself (design, quantization, reconstruction, or ablation focus), I am uncertain if it meets the “core object of study” requirement.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Personalized Aging-in-Place Support Through Fine-Tuning of Generative AI Models",
    "abstract": "Artificial intelligence (AI) has long been proposed as an enabling technology for addressing the challenges of the impending demographic shifts in western countries. While AI tools for identifying, monitoring, and assessing adherence to best health practices have been previously proposed, recent developments in generative AI models offer unique capabilities to address compliance-related challenges. This manuscript proposes a framework for enhancing AI-assistive technology through personalization enabled by generative AI. Namely, emerging generative models are fine-tuned using training data mined from text, audio, and visual records of target individuals to produce customized user experiences which can be integrated within the technology ecosystem available to the end-user. In addition to proposing this framework, this work-in-progress manuscript also reviews recent advancements in technologies which enable its implementation.",
    "metadata": {
      "title": "Personalized Aging-in-Place Support Through Fine-Tuning of Generative AI Models",
      "summary": "Artificial intelligence (AI) has long been proposed as an enabling technology for addressing the challenges of the impending demographic shifts in western countries. While AI tools for identifying, monitoring, and assessing adherence to best health practices have been previously proposed, recent developments in generative AI models offer unique capabilities to address compliance-related challenges. This manuscript proposes a framework for enhancing AI-assistive technology through personalization enabled by generative AI. Namely, emerging generative models are fine-tuned using training data mined from text, audio, and visual records of target individuals to produce customized user experiences which can be integrated within the technology ecosystem available to the end-user. In addition to proposing this framework, this work-in-progress manuscript also reviews recent advancements in technologies which enable its implementation.",
      "abstract": "Artificial intelligence (AI) has long been proposed as an enabling technology for addressing the challenges of the impending demographic shifts in western countries. While AI tools for identifying, monitoring, and assessing adherence to best health practices have been previously proposed, recent developments in generative AI models offer unique capabilities to address compliance-related challenges. This manuscript proposes a framework for enhancing AI-assistive technology through personalization enabled by generative AI. Namely, emerging generative models are fine-tuned using training data mined from text, audio, and visual records of target individuals to produce customized user experiences which can be integrated within the technology ecosystem available to the end-user. In addition to proposing this framework, this work-in-progress manuscript also reviews recent advancements in technologies which enable its implementation.",
      "doi": "https://doi.org/10.1109/mobisecserv58080.2023.10329224",
      "openalex_id": "https://openalex.org/W4389230249",
      "arxiv_id": "",
      "publication_date": "2023-11-04",
      "published": "2023-11-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes fine-tuning generative AI using multimodal data for personalized aging-in-place support but does not address discrete audio/speech tokenization as a core object of study; lacks analysis/assessment of discrete tokens. Therefore it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes fine-tuning generative AI using multimodal data for personalized aging-in-place support but does not address discrete audio/speech tokenization as a core object of study; lacks analysis/assessment of discrete tokens. Therefore it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on personalized AI support for aging-in-place through fine-tuning generative AI models, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; therefore, it does not meet the essential inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on personalized AI support for aging-in-place through fine-tuning generative AI models, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; therefore, it does not meet the essential inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
    "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
    "metadata": {
      "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
      "summary": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447938",
      "openalex_id": "https://openalex.org/W4392909867",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on leveraging ASR lattices/word confusion networks to improve SLU with LLMs and does not propose or analyze discrete audio tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on leveraging ASR lattices/word confusion networks to improve SLU with LLMs and does not propose or analyze discrete audio tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving spoken language understanding via ASR system lattice outputs and in-context learning with large language models, but there is no indication it proposes, analyzes, or evaluates discrete audio/speech tokenization methods converting continuous audio into discrete token sequences; rather, it addresses using lattice outputs and word confusion networks for robustness in SLU, which does not meet the core inclusion criterion regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving spoken language understanding via ASR system lattice outputs and in-context learning with large language models, but there is no indication it proposes, analyzes, or evaluates discrete audio/speech tokenization methods converting continuous audio into discrete token sequences; rather, it addresses using lattice outputs and word confusion networks for robustness in SLU, which does not meet the core inclusion criterion regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generative De-Quantization for Neural Speech Codec Via Latent Diffusion",
    "abstract": "End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "Generative De-Quantization for Neural Speech Codec Via Latent Diffusion",
      "summary": "End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446556",
      "openalex_id": "https://openalex.org/W4392931975",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete tokens learned by an end-to-end speech codec and then de-quantizes them via a latent diffusion model, making discrete audio tokenization the core focus of the study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete tokens learned by an end-to-end speech codec and then de-quantizes them via a latent diffusion model, making discrete audio tokenization the core focus of the study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes to use an end-to-end codec to learn low-dimensional discrete tokens representing speech and leverages a latent diffusion model on these discrete token features, explicitly focusing on discrete audio tokenization for speech coding and reconstruction. It includes systematic analysis through ablation studies and subjective evaluations, and the study focuses on neural speech codec with discrete quantization rather than traditional signal processing codecs. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes to use an end-to-end codec to learn low-dimensional discrete tokens representing speech and leverages a latent diffusion model on these discrete token features, explicitly focusing on discrete audio tokenization for speech coding and reconstruction. It includes systematic analysis through ablation studies and subjective evaluations, and the study focuses on neural speech codec with discrete quantization rather than traditional signal processing codecs. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An AI red team playbook",
    "abstract": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing <i>An AI Blue Team Playbook</i>, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.",
    "metadata": {
      "title": "An AI red team playbook",
      "summary": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing <i>An AI Blue Team Playbook</i>, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.",
      "abstract": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing <i>An AI Blue Team Playbook</i>, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.",
      "doi": "https://doi.org/10.1117/12.3021906",
      "openalex_id": "https://openalex.org/W4399422814",
      "arxiv_id": "",
      "publication_date": "2024-06-07",
      "published": "2024-06-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about an AI red team playbook and does not address discrete audio tokenization or any core tokenization for audio; therefore it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about an AI red team playbook and does not address discrete audio tokenization or any core tokenization for audio; therefore it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study titled 'An AI red team playbook' primarily focuses on the processes and framework for AI red teaming and security rather than on discrete audio/speech tokenization or related methods, thus it does not meet the inclusion criteria which require a focus on discrete audio tokenization, and it doesn't fit any exclusion nuances except that the core topic is unrelated to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study titled 'An AI red team playbook' primarily focuses on the processes and framework for AI red teaming and security rather than on discrete audio/speech tokenization or related methods, thus it does not meet the inclusion criteria which require a focus on discrete audio tokenization, and it doesn't fit any exclusion nuances except that the core topic is unrelated to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Latent Filling: Latent Space Data Augmentation for Zero-Shot Speech Synthesis",
    "abstract": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.",
    "metadata": {
      "title": "Latent Filling: Latent Space Data Augmentation for Zero-Shot Speech Synthesis",
      "summary": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.",
      "abstract": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446098",
      "openalex_id": "https://openalex.org/W4392902644",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on latent-space data augmentation for speaker embeddings and does not involve discrete audio tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on latent-space data augmentation for speaker embeddings and does not involve discrete audio tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on latent space data augmentation in the speaker embedding space for zero-shot speech synthesis and does not propose or analyze discrete audio/speech tokenization methods converting continuous audio signals into discrete token sequences, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on latent space data augmentation in the speaker embedding space for zero-shot speech synthesis and does not propose or analyze discrete audio/speech tokenization methods converting continuous audio signals into discrete token sequences, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation",
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
    "metadata": {
      "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation",
      "summary": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
      "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446214",
      "openalex_id": "https://openalex.org/W4392902773",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on streaming speech synthesis for LLM outputs and latency reduction, but it does not propose or analyze discrete audio tokenization or token-based representations, so it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on streaming speech synthesis for LLM outputs and latency reduction, but it does not propose or analyze discrete audio tokenization or token-based representations, so it fails the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synthesizing speech from text generated by large language models, emphasizing latency reduction in streaming speech synthesis, but it does not mention proposing, analyzing, or evaluating discrete audio/speech tokenization or any discrete audio token design as a core aspect; thus, it does not meet the inclusion criteria and aligns with exclusion due to lack of discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synthesizing speech from text generated by large language models, emphasizing latency reduction in streaming speech synthesis, but it does not mention proposing, analyzing, or evaluating discrete audio/speech tokenization or any discrete audio token design as a core aspect; thus, it does not meet the inclusion criteria and aligns with exclusion due to lack of discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GR0: Self-Supervised Global Representation Learning for Zero-Shot Voice Conversion",
    "abstract": "Research in generative self-supervised learning (SSL) has largely focused on local embeddings for tokenized sequences. We introduce a generative SSL framework that learns a global representation that is disentangled from local embeddings. We apply this technique to jointly learn a global speaker embedding and a zero-shot voice converter. The converter modifies recorded speech to sound as if it were spoken by a different person while preserving the content, using only a short reference clip unavailable to the model during training. Listening experiments conducted on an unseen dataset show that our models significantly outperform SOTA baselines in both quality and speaker similarity for various datasets and unseen languages.",
    "metadata": {
      "title": "GR0: Self-Supervised Global Representation Learning for Zero-Shot Voice Conversion",
      "summary": "Research in generative self-supervised learning (SSL) has largely focused on local embeddings for tokenized sequences. We introduce a generative SSL framework that learns a global representation that is disentangled from local embeddings. We apply this technique to jointly learn a global speaker embedding and a zero-shot voice converter. The converter modifies recorded speech to sound as if it were spoken by a different person while preserving the content, using only a short reference clip unavailable to the model during training. Listening experiments conducted on an unseen dataset show that our models significantly outperform SOTA baselines in both quality and speaker similarity for various datasets and unseen languages.",
      "abstract": "Research in generative self-supervised learning (SSL) has largely focused on local embeddings for tokenized sequences. We introduce a generative SSL framework that learns a global representation that is disentangled from local embeddings. We apply this technique to jointly learn a global speaker embedding and a zero-shot voice converter. The converter modifies recorded speech to sound as if it were spoken by a different person while preserving the content, using only a short reference clip unavailable to the model during training. Listening experiments conducted on an unseen dataset show that our models significantly outperform SOTA baselines in both quality and speaker similarity for various datasets and unseen languages.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448232",
      "openalex_id": "https://openalex.org/W4392903903",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on continuous embeddings for zero-shot voice conversion and does not propose or evaluate discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on continuous embeddings for zero-shot voice conversion and does not propose or evaluate discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion using a generative self-supervised learning approach that jointly learns a global speaker embedding and a converter, but the abstract does not mention any discrete audio/speech tokenization or discrete symbol sequences as a core object of study; it appears to work with representations without explicit discrete tokenization, thus failing to meet the main inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion using a generative self-supervised learning approach that jointly learns a global speaker embedding and a converter, but the abstract does not mention any discrete audio/speech tokenization or discrete symbol sequences as a core object of study; it appears to work with representations without explicit discrete tokenization, thus failing to meet the main inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision",
    "abstract": "Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.",
    "metadata": {
      "title": "Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision",
      "summary": "Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.",
      "abstract": "Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.",
      "doi": "https://doi.org/10.1109/taslp.2023.3306716",
      "openalex_id": "https://openalex.org/W4385975838",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on continuous speech representations (timbre/content features) for any-to-any voice conversion and does not analyze or employ discrete acoustic tokens/quantization; it does not meet the discrete tokenization inclusion criteria and aligns with exclusion of tokenization-focused work.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on continuous speech representations (timbre/content features) for any-to-any voice conversion and does not analyze or employ discrete acoustic tokens/quantization; it does not meet the discrete tokenization inclusion criteria and aligns with exclusion of tokenization-focused work.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on any-to-any voice conversion using speaker adaptation and content supervision, but it does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into discrete symbols; it primarily discusses speaker representations and content features without discrete quantization or token design relevant to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on any-to-any voice conversion using speaker adaptation and content supervision, but it does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into discrete symbols; it primarily discusses speaker representations and content features without discrete quantization or token design relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Mapache: Masked Parallel Transformer for Advanced Speech Editing and Synthesis",
    "abstract": "Recent advancements in Generative AI, such as scaled Transformer large language models (LLM) and diffusion decoders, have revolutionized speech synthesis. With speech encompassing the complexities of natural language and audio dimensionality, many recent models have relied on autoregressive modeling of quantized speech tokens. Such an approach limits speech synthesis to left-to-right generation, making these models unsuitable for speech edits free from audio discontinuities. We introduce Mapache, a novel architecture that combines a non-autoregressive masked speech language model with acoustic diffusion modeling, offering a unique, fully parallel pipeline. Mapache excels in precise speech editing that is indiscernible to human listeners, exhibiting inpainting and zero-shot synthesis capabilities that either surpass or rival those of other state-of-the-art models that specialize in just one of these tasks. This paper also sheds light on optimizing the decoding process for such non-autoregressive models.",
    "metadata": {
      "title": "Mapache: Masked Parallel Transformer for Advanced Speech Editing and Synthesis",
      "summary": "Recent advancements in Generative AI, such as scaled Transformer large language models (LLM) and diffusion decoders, have revolutionized speech synthesis. With speech encompassing the complexities of natural language and audio dimensionality, many recent models have relied on autoregressive modeling of quantized speech tokens. Such an approach limits speech synthesis to left-to-right generation, making these models unsuitable for speech edits free from audio discontinuities. We introduce Mapache, a novel architecture that combines a non-autoregressive masked speech language model with acoustic diffusion modeling, offering a unique, fully parallel pipeline. Mapache excels in precise speech editing that is indiscernible to human listeners, exhibiting inpainting and zero-shot synthesis capabilities that either surpass or rival those of other state-of-the-art models that specialize in just one of these tasks. This paper also sheds light on optimizing the decoding process for such non-autoregressive models.",
      "abstract": "Recent advancements in Generative AI, such as scaled Transformer large language models (LLM) and diffusion decoders, have revolutionized speech synthesis. With speech encompassing the complexities of natural language and audio dimensionality, many recent models have relied on autoregressive modeling of quantized speech tokens. Such an approach limits speech synthesis to left-to-right generation, making these models unsuitable for speech edits free from audio discontinuities. We introduce Mapache, a novel architecture that combines a non-autoregressive masked speech language model with acoustic diffusion modeling, offering a unique, fully parallel pipeline. Mapache excels in precise speech editing that is indiscernible to human listeners, exhibiting inpainting and zero-shot synthesis capabilities that either surpass or rival those of other state-of-the-art models that specialize in just one of these tasks. This paper also sheds light on optimizing the decoding process for such non-autoregressive models.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448121",
      "openalex_id": "https://openalex.org/W4392902860",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Mapache describes a non-autoregressive masked speech language model operating on discrete speech tokens in conjunction with diffusion-based methods, indicating discrete tokenization is central to the work.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Mapache describes a non-autoregressive masked speech language model operating on discrete speech tokens in conjunction with diffusion-based methods, indicating discrete tokenization is central to the work.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Mapache, a non-autoregressive masked speech language model combined with acoustic diffusion modeling, emphasizing speech editing and synthesis. However, the abstract does not explicitly state that it proposes, analyzes, or evaluates discrete audio/speech tokenization methods where continuous audio is converted into discrete symbols for reconstruction or understanding. It mentions autoregressive modeling of quantized speech tokens as background but does not focus on discrete token design or analysis as the core contribution. Hence, it is uncertain whether the core of the study aligns strictly with the inclusion criteria regarding discrete tokenization.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study introduces Mapache, a non-autoregressive masked speech language model combined with acoustic diffusion modeling, emphasizing speech editing and synthesis. However, the abstract does not explicitly state that it proposes, analyzes, or evaluates discrete audio/speech tokenization methods where continuous audio is converted into discrete symbols for reconstruction or understanding. It mentions autoregressive modeling of quantized speech tokens as background but does not focus on discrete token design or analysis as the core contribution. Hence, it is uncertain whether the core of the study aligns strictly with the inclusion criteria regarding discrete tokenization.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I examined the two junior reviews: one (JuniorNano) assumed discrete tokenization is central, while the other (JuniorMini) flagged uncertainty because the abstract mentions 'quantized speech tokens' only as background and does not describe proposing or evaluating a discrete tokenization method. The Mapache abstract focuses on a non-autoregressive masked transformer and acoustic diffusion decoding for editing/synthesis and does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme as a core contribution. Therefore it does not clearly meet the inclusion criterion that discrete tokenization must be explicitly proposed or analyzed.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I examined the two junior reviews: one (JuniorNano) assumed discrete tokenization is central, while the other (JuniorMini) flagged uncertainty because the abstract mentions 'quantized speech tokens' only as background and does not describe proposing or evaluating a discrete tokenization method. The Mapache abstract focuses on a non-autoregressive masked transformer and acoustic diffusion decoding for editing/synthesis and does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme as a core contribution. Therefore it does not clearly meet the inclusion criterion that discrete tokenization must be explicitly proposed or analyzed.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
    "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
      "summary": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448495",
      "openalex_id": "https://openalex.org/W4392931282",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes using discrete speech representations (Contrastive Token-Acoustic Pretraining CTAP) as a core component for semantic tokenization in a diffusion-based high-fidelity TTS system, meeting inclusion criteria for discrete tokenization analysis.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes using discrete speech representations (Contrastive Token-Acoustic Pretraining CTAP) as a core component for semantic tokenization in a diffusion-based high-fidelity TTS system, meeting inclusion criteria for discrete tokenization analysis.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a minimally-supervised speech synthesis method using diffusion models and discrete semantic representations (Contrastive Token-Acoustic Pretraining) to address issues like information redundancy and dimension explosion in semantic tokenization; thus, it explicitly analyzes discrete audio tokenization as a core object of study. It also evaluates these discrete semantic and acoustic representations for speech generation, fulfilling the inclusion criteria and avoiding all exclusion criteria related to continuous-only features or solely engineering reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a minimally-supervised speech synthesis method using diffusion models and discrete semantic representations (Contrastive Token-Acoustic Pretraining) to address issues like information redundancy and dimension explosion in semantic tokenization; thus, it explicitly analyzes discrete audio tokenization as a core object of study. It also evaluates these discrete semantic and acoustic representations for speech generation, fulfilling the inclusion criteria and avoiding all exclusion criteria related to continuous-only features or solely engineering reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Practical Study of Deep Learning Models for Speech Synthesis",
    "abstract": "Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.",
    "metadata": {
      "title": "Practical Study of Deep Learning Models for Speech Synthesis",
      "summary": "Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.",
      "abstract": "Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.",
      "doi": "https://doi.org/10.1145/3594806.3596536",
      "openalex_id": "https://openalex.org/W4385732548",
      "arxiv_id": "",
      "publication_date": "2023-07-05",
      "published": "2023-07-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses core TTS modeling and transfer learning but does not address discrete audio/tokenization of speech; it appears to rely on continuous representations (e.g., waveform or spectrogram) rather than discrete tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses core TTS modeling and transfer learning but does not address discrete audio/tokenization of speech; it appears to rely on continuous representations (e.g., waveform or spectrogram) rather than discrete tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deep learning models for speech synthesis and transfer learning with few-shot learning, but it does not explicitly mention the proposal, analysis, or evaluation of discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is the core inclusion criterion. The abstract suggests the use of continuous features or general deep learning approaches for TTS but lacks any reference to discrete tokenization methods or neural quantization-based audio token representations, thus not meeting the main inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deep learning models for speech synthesis and transfer learning with few-shot learning, but it does not explicitly mention the proposal, analysis, or evaluation of discrete audio/speech tokenization where continuous audio is converted into discrete symbols, which is the core inclusion criterion. The abstract suggests the use of continuous features or general deep learning approaches for TTS but lacks any reference to discrete tokenization methods or neural quantization-based audio token representations, thus not meeting the main inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DSVAE: Disentangled Representation Learning for Synthetic Speech Detection",
    "abstract": "Tools to generate high quality synthetic speech that is perceptually indistinguishable from speech recorded from hu-man speakers are easily available. Many incidents report misuse of synthetic speech for spreading misinformation and committing financial fraud. Several approaches have been proposed for detecting synthetic speech. Many of these approaches use deep learning methods without providing reasoning for the decisions they make. This limits the explainability of these approaches. In this paper, we use disentangled representation learning for developing a synthetic speech detector. We propose Disentangled Spectrogram Variational Auto Encoder (DSVAE) which is a two stage trained variational autoencoder that processes spectrograms of speech to generate features that disentangle synthetic and bona fide speech. We evaluated DSVAE using the ASVspoof2019 dataset. Our experimental results show high accuracy (&amp;gt; 98%) on detecting synthetic speech from 6 known and 10 unknown speech synthesizers. Further, the visualization of disentangled features obtained from DSVAE provides rea-soning behind the working principle of DSVAE, improving its explainability. DSVAE performs well compared to several existing methods. Additionally, DSVAE works in practical scenarios such as detecting synthetic speech uploaded on social platforms and against simple attacks such as removing silence regions.",
    "metadata": {
      "title": "DSVAE: Disentangled Representation Learning for Synthetic Speech Detection",
      "summary": "Tools to generate high quality synthetic speech that is perceptually indistinguishable from speech recorded from hu-man speakers are easily available. Many incidents report misuse of synthetic speech for spreading misinformation and committing financial fraud. Several approaches have been proposed for detecting synthetic speech. Many of these approaches use deep learning methods without providing reasoning for the decisions they make. This limits the explainability of these approaches. In this paper, we use disentangled representation learning for developing a synthetic speech detector. We propose Disentangled Spectrogram Variational Auto Encoder (DSVAE) which is a two stage trained variational autoencoder that processes spectrograms of speech to generate features that disentangle synthetic and bona fide speech. We evaluated DSVAE using the ASVspoof2019 dataset. Our experimental results show high accuracy (&amp;gt; 98%) on detecting synthetic speech from 6 known and 10 unknown speech synthesizers. Further, the visualization of disentangled features obtained from DSVAE provides rea-soning behind the working principle of DSVAE, improving its explainability. DSVAE performs well compared to several existing methods. Additionally, DSVAE works in practical scenarios such as detecting synthetic speech uploaded on social platforms and against simple attacks such as removing silence regions.",
      "abstract": "Tools to generate high quality synthetic speech that is perceptually indistinguishable from speech recorded from hu-man speakers are easily available. Many incidents report misuse of synthetic speech for spreading misinformation and committing financial fraud. Several approaches have been proposed for detecting synthetic speech. Many of these approaches use deep learning methods without providing reasoning for the decisions they make. This limits the explainability of these approaches. In this paper, we use disentangled representation learning for developing a synthetic speech detector. We propose Disentangled Spectrogram Variational Auto Encoder (DSVAE) which is a two stage trained variational autoencoder that processes spectrograms of speech to generate features that disentangle synthetic and bona fide speech. We evaluated DSVAE using the ASVspoof2019 dataset. Our experimental results show high accuracy (&amp;gt; 98%) on detecting synthetic speech from 6 known and 10 unknown speech synthesizers. Further, the visualization of disentangled features obtained from DSVAE provides rea-soning behind the working principle of DSVAE, improving its explainability. DSVAE performs well compared to several existing methods. Additionally, DSVAE works in practical scenarios such as detecting synthetic speech uploaded on social platforms and against simple attacks such as removing silence regions.",
      "doi": "https://doi.org/10.1109/icmla58977.2023.00072",
      "openalex_id": "https://openalex.org/W4392942164",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The work focuses on disentangled representation learning using spectrograms and VAEs for synthetic speech detection and does not propose or analyze discrete audio tokenization; it relies on continuous features, so it should be excluded per the inclusion/exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The work focuses on disentangled representation learning using spectrograms and VAEs for synthetic speech detection and does not propose or analyze discrete audio tokenization; it relies on continuous features, so it should be excluded per the inclusion/exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synthetic speech detection using a variational autoencoder that processes continuous spectrogram features without proposing or analyzing discrete audio/speech tokenization or discrete symbol sequences; thus, it does not meet the inclusion criteria regarding discrete tokenization and falls under exclusion criteria for using continuous features without discrete quantization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synthetic speech detection using a variational autoencoder that processes continuous spectrogram features without proposing or analyzing discrete audio/speech tokenization or discrete symbol sequences; thus, it does not meet the inclusion criteria regarding discrete tokenization and falls under exclusion criteria for using continuous features without discrete quantization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
    "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
    "metadata": {
      "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
      "summary": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "doi": "https://doi.org/10.21437/blizzard.2023-13",
      "openalex_id": "https://openalex.org/W4388272705",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a TTS system and phonemization but does not address discrete audio tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a TTS system and phonemization but does not address discrete audio tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a text-to-speech synthesis system using diffusion-based acoustic modeling and vocoding trained on provided data, but it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbol sequences; the system utilizes continuous acoustic features and phonemization pipelines without explicit methodological contributions or analyses of discrete tokenization, hence it does not meet the inclusion criteria and fits within the exclusion criteria for using continuous features and lacking discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a text-to-speech synthesis system using diffusion-based acoustic modeling and vocoding trained on provided data, but it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbol sequences; the system utilizes continuous acoustic features and phonemization pipelines without explicit methodological contributions or analyses of discrete tokenization, hence it does not meet the inclusion criteria and fits within the exclusion criteria for using continuous features and lacking discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The concept of deepfake in Russian law, classification of deepfake and issues of their legal regulation",
    "abstract": "The article deals with the issues of legal regulation of deepfake in the Russian Federation. Legal regulation of deepfake does not keep up with the pace of development of artificial intelligence technologies. The authors emphasize that there is no definition of deepfake in the current legislation, and the existing formulations in scientific works are extremely contradictory in nature. Taking into account the pace of development of artificial intelligence technologies, it is necessary to legislate the definition of deepfake. The authors note that the classification of deepfakes is fundamentally important for the legal regulation of these technologies. According to the results of the analysis of modern neural networks the species classification of deepfakes is offered. Taking into account the authors' proposed definition of the concept of \"deepfake\" and taking into account the lack of legal mechanisms to regulate social relations in the sphere of use and distribution of deepfake, which cause the development of digital transformation, it is important to form mechanisms to adapt the legal system to the challenges associated with the development of deepfake technologies.",
    "metadata": {
      "title": "The concept of deepfake in Russian law, classification of deepfake and issues of their legal regulation",
      "summary": "The article deals with the issues of legal regulation of deepfake in the Russian Federation. Legal regulation of deepfake does not keep up with the pace of development of artificial intelligence technologies. The authors emphasize that there is no definition of deepfake in the current legislation, and the existing formulations in scientific works are extremely contradictory in nature. Taking into account the pace of development of artificial intelligence technologies, it is necessary to legislate the definition of deepfake. The authors note that the classification of deepfakes is fundamentally important for the legal regulation of these technologies. According to the results of the analysis of modern neural networks the species classification of deepfakes is offered. Taking into account the authors' proposed definition of the concept of \"deepfake\" and taking into account the lack of legal mechanisms to regulate social relations in the sphere of use and distribution of deepfake, which cause the development of digital transformation, it is important to form mechanisms to adapt the legal system to the challenges associated with the development of deepfake technologies.",
      "abstract": "The article deals with the issues of legal regulation of deepfake in the Russian Federation. Legal regulation of deepfake does not keep up with the pace of development of artificial intelligence technologies. The authors emphasize that there is no definition of deepfake in the current legislation, and the existing formulations in scientific works are extremely contradictory in nature. Taking into account the pace of development of artificial intelligence technologies, it is necessary to legislate the definition of deepfake. The authors note that the classification of deepfakes is fundamentally important for the legal regulation of these technologies. According to the results of the analysis of modern neural networks the species classification of deepfakes is offered. Taking into account the authors' proposed definition of the concept of \"deepfake\" and taking into account the lack of legal mechanisms to regulate social relations in the sphere of use and distribution of deepfake, which cause the development of digital transformation, it is important to form mechanisms to adapt the legal system to the challenges associated with the development of deepfake technologies.",
      "doi": "https://doi.org/10.25136/2409-7136.2023.11.69014",
      "openalex_id": "https://openalex.org/W4388870810",
      "arxiv_id": "",
      "publication_date": "2023-11-01",
      "published": "2023-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses legal regulation of deepfakes and does not analyze or propose discrete audio tokenization (discrete speech tokens), so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses legal regulation of deepfakes and does not analyze or propose discrete audio tokenization (discrete speech tokens), so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article primarily focuses on the legal regulation and classification of deepfakes in Russian law and does not address discrete audio/speech tokenization, nor does it discuss audio signal processing or tokenization methods relevant to speech tasks. Furthermore, there is no mention of discrete audio symbol conversion, neural encoding, or any methodological contribution related to audio tokenization in the abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article primarily focuses on the legal regulation and classification of deepfakes in Russian law and does not address discrete audio/speech tokenization, nor does it discuss audio signal processing or tokenization methods relevant to speech tasks. Furthermore, there is no mention of discrete audio symbol conversion, neural encoding, or any methodological contribution related to audio tokenization in the abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Voice Cloning Quality through Data Selection and Alignment-based Metrics",
    "abstract": "Voice cloning, an emerging field in the speech processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigate the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also use two high-quality corpora for comparative analysis. We conduct exhaustive evaluations of the quality of the gathered corpora in order to select the most suitable audios for the training of a Voice Cloning system. Following these measurements, we conduct a series of ablations by removing audios with lower SNR and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduce a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 Text-to-Speech (TTS) system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increase the quality of synthesised audios for the challenging low-quality corpus. Notably, our findings indicate that models trained on a 3-hour corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.",
    "metadata": {
      "title": "Enhancing Voice Cloning Quality through Data Selection and Alignment-based Metrics",
      "summary": "Voice cloning, an emerging field in the speech processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigate the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also use two high-quality corpora for comparative analysis. We conduct exhaustive evaluations of the quality of the gathered corpora in order to select the most suitable audios for the training of a Voice Cloning system. Following these measurements, we conduct a series of ablations by removing audios with lower SNR and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduce a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 Text-to-Speech (TTS) system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increase the quality of synthesised audios for the challenging low-quality corpus. Notably, our findings indicate that models trained on a 3-hour corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.",
      "abstract": "Voice cloning, an emerging field in the speech processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigate the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also use two high-quality corpora for comparative analysis. We conduct exhaustive evaluations of the quality of the gathered corpora in order to select the most suitable audios for the training of a Voice Cloning system. Following these measurements, we conduct a series of ablations by removing audios with lower SNR and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduce a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 Text-to-Speech (TTS) system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increase the quality of synthesised audios for the challenging low-quality corpus. Notably, our findings indicate that models trained on a 3-hour corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.",
      "doi": "https://doi.org/10.20944/preprints202306.0223.v1",
      "openalex_id": "https://openalex.org/W4379387359",
      "arxiv_id": "",
      "publication_date": "2023-06-05",
      "published": "2023-06-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on data selection and alignment-based quality metrics for voice cloning and mentions alignment between input characters via Tacotron 2, but it does not propose or analyze discrete audio/speech tokenization (converting audio into a discrete token sequence) as a core object, relying instead on continuous audio features and text-audio alignment.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on data selection and alignment-based quality metrics for voice cloning and mentions alignment between input characters via Tacotron 2, but it does not propose or analyze discrete audio/speech tokenization (converting audio into a discrete token sequence) as a core object, relying instead on continuous audio features and text-audio alignment.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice cloning quality improvement through data selection, alignment-based metrics, and speech synthesis techniques; however, it does not mention any proposal, analysis, or evaluation of discrete audio/speech tokenization converting continuous audio into discrete symbols, nor does it discuss neural-based discrete token representations. Therefore, it does not meet the inclusion criteria related to discrete tokenization and is better to exclude.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice cloning quality improvement through data selection, alignment-based metrics, and speech synthesis techniques; however, it does not mention any proposal, analysis, or evaluation of discrete audio/speech tokenization converting continuous audio into discrete symbols, nor does it discuss neural-based discrete token representations. Therefore, it does not meet the inclusion criteria related to discrete tokenization and is better to exclude.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection",
    "abstract": "This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on a bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, if the proposed synthetic data is nor properly modelled using massive data by bot detectors, then that data will be very difficult to detect even for the most sophisticate bot detectors. Furthermore, these results show the great potential of the presented models for improving the training of bot detection technology.",
    "metadata": {
      "title": "BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection",
      "summary": "This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on a bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, if the proposed synthetic data is nor properly modelled using massive data by bot detectors, then that data will be very difficult to detect even for the most sophisticate bot detectors. Furthermore, these results show the great potential of the presented models for improving the training of bot detection technology.",
      "abstract": "This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on a bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, if the proposed synthetic data is nor properly modelled using massive data by bot detectors, then that data will be very difficult to detect even for the most sophisticate bot detectors. Furthermore, these results show the great potential of the presented models for improving the training of bot detection technology.",
      "doi": "https://doi.org/10.1109/cvprw59228.2023.00112",
      "openalex_id": "https://openalex.org/W4385800906",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses synthesizing keystroke biometric data for bot detection, with no discrete audio tokenization or tokenized audio representations; it is not about discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses synthesizing keystroke biometric data for bot detection, with no discrete audio tokenization or tokenized audio representations; it is not about discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on biometric keystroke data synthesis for bot detection and does not involve discrete audio/speech tokenization or analysis related to discrete audio token representations, which is the core requirement for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on biometric keystroke data synthesis for bot detection and does not involve discrete audio/speech tokenization or analysis related to discrete audio token representations, which is the core requirement for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data Redaction from Conditional Generative Models",
    "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
    "metadata": {
      "title": "Data Redaction from Conditional Generative Models",
      "summary": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "doi": "https://doi.org/10.1109/satml59370.2024.00035",
      "openalex_id": "https://openalex.org/W4396815655",
      "arxiv_id": "",
      "publication_date": "2024-04-09",
      "published": "2024-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses post-editing a trained conditional generative model to redact certain conditionals (prompts and voices) and distilling the conditioning network; it does not propose, analyze, or evaluate discrete audio/speech tokenization or a discrete tokenization scheme for audio signals.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses post-editing a trained conditional generative model to redact certain conditionals (prompts and voices) and distilling the conditioning network; it does not propose, analyze, or evaluate discrete audio/speech tokenization or a discrete tokenization scheme for audio signals.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper addresses redaction in conditional generative models specifically for text-to-image and text-to-speech models, focusing on post-editing conditioning networks to control undesirable content. However, it does not explicitly discuss or analyze discrete audio/speech tokenization as a core object of study, nor does it mention quantization or discrete token representations directly related to audio tokenization. Therefore, it does not meet the inclusion criteria and aligns more with content filtering or model editing than tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper addresses redaction in conditional generative models specifically for text-to-image and text-to-speech models, focusing on post-editing conditioning networks to control undesirable content. However, it does not explicitly discuss or analyze discrete audio/speech tokenization as a core object of study, nor does it mention quantization or discrete token representations directly related to audio tokenization. Therefore, it does not meet the inclusion criteria and aligns more with content filtering or model editing than tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Stack-and-Delay: A New Codebook Pattern for Music Generation",
    "abstract": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
    "metadata": {
      "title": "Stack-and-Delay: A New Codebook Pattern for Music Generation",
      "summary": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "abstract": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447392",
      "openalex_id": "https://openalex.org/W4392909680",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly studies discrete audio tokenization via a neural codec that maps audio frames to a finite set of codes (a codebook) for music generation, proposing and evaluating a stack-and-delay decoding method, and reporting objective and listening-test results; this directly aligns with the core inclusion criterion on discrete tokenization and does not fall into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly studies discrete audio tokenization via a neural codec that maps audio frames to a finite set of codes (a codebook) for music generation, proposing and evaluating a stack-and-delay decoding method, and reporting objective and listening-test results; this directly aligns with the core inclusion criterion on discrete tokenization and does not fall into any exclusion category.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly deals with discrete representations of audio frames for music generation, focusing on decoding strategies of such discrete codes derived from neural codecs, which constitutes an analysis and evaluation of discrete audio tokenization. The work is in English and presents systematic experimental validation, including spectral analysis and listening tests, indicating methodological contributions beyond engineering-only reporting. While the study focuses on music, it involves neural encoding-based discrete tokenization rather than traditional codecs and discusses implications on audio reconstruction quality, but it does not explicitly mention applicability or evaluation related to speech tasks or speech-relevant implications, which is a required inclusion criterion for tokenization methods of non-speech audio. Given this, the study largely meets the inclusion criteria for discrete tokenization analysis but potentially fails the requirement on applicability to speech or speech-relevant evaluation for non-speech audio tokenization, placing it close to the borderline between inclusion and exclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study explicitly deals with discrete representations of audio frames for music generation, focusing on decoding strategies of such discrete codes derived from neural codecs, which constitutes an analysis and evaluation of discrete audio tokenization. The work is in English and presents systematic experimental validation, including spectral analysis and listening tests, indicating methodological contributions beyond engineering-only reporting. While the study focuses on music, it involves neural encoding-based discrete tokenization rather than traditional codecs and discusses implications on audio reconstruction quality, but it does not explicitly mention applicability or evaluation related to speech tasks or speech-relevant implications, which is a required inclusion criterion for tokenization methods of non-speech audio. Given this, the study largely meets the inclusion criteria for discrete tokenization analysis but potentially fails the requirement on applicability to speech or speech-relevant evaluation for non-speech audio tokenization, placing it close to the borderline between inclusion and exclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviews and the abstract: the paper clearly studies discrete neural-codebook tokenization and proposes/validates a novel stack-and-delay decoding strategy for music generation (so it meets the core discrete-tokenization inclusion), but the work is focused solely on music and the abstract contains no indication of applicability to or evaluation on speech—triggering the explicit exclusion for non-speech-only tokenization that does not address speech implications. Therefore it should be excluded. One-sentence explanation for score: Better to exclude (2) because although it analyzes discrete audio tokens, it is music-only and does not demonstrate applicability or evaluation relevant to speech as required by the criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviews and the abstract: the paper clearly studies discrete neural-codebook tokenization and proposes/validates a novel stack-and-delay decoding strategy for music generation (so it meets the core discrete-tokenization inclusion), but the work is focused solely on music and the abstract contains no indication of applicability to or evaluation on speech—triggering the explicit exclusion for non-speech-only tokenization that does not address speech implications. Therefore it should be excluded. One-sentence explanation for score: Better to exclude (2) because although it analyzes discrete audio tokens, it is music-only and does not demonstrate applicability or evaluation relevant to speech as required by the criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
    "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
    "metadata": {
      "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
      "summary": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446991",
      "openalex_id": "https://openalex.org/W4392931320",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses adverse impacts and detection of synthetic speech for ASR but does not describe discrete audio tokenization or token-level representations; hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses adverse impacts and detection of synthetic speech for ASR but does not describe discrete audio tokenization or token-level representations; hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distinguishing synthetic versus real speech using detection modules and prompt labels within speech recognition models, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods or discrete token representations as required by the inclusion criteria; the abstract indicates reliance on acoustic modeling and language semantics rather than discrete token quantization approaches, and there is no mention of neural quantization or discrete symbol sequences for audio reconstruction or understanding.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distinguishing synthetic versus real speech using detection modules and prompt labels within speech recognition models, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods or discrete token representations as required by the inclusion criteria; the abstract indicates reliance on acoustic modeling and language semantics rather than discrete token quantization approaches, and there is no mention of neural quantization or discrete symbol sequences for audio reconstruction or understanding.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification",
    "abstract": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.",
    "metadata": {
      "title": "Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification",
      "summary": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.",
      "abstract": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.",
      "doi": "https://doi.org/10.1609/aaai.v38i5.28249",
      "openalex_id": "https://openalex.org/W4393153727",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses data adaptation for vision-language models in image classification and does not propose or analyze discrete audio/speech tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses data adaptation for vision-language models in image classification and does not propose or analyze discrete audio/speech tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vision-language foundation models for image classification and adaptation methods, with no mention or analysis of discrete audio/speech tokenization, discrete audio signal conversion, or related audio token generation or understanding tasks, thus failing to meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vision-language foundation models for image classification and adaptation methods, with no mention or analysis of discrete audio/speech tokenization, discrete audio signal conversion, or related audio token generation or understanding tasks, thus failing to meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
    "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
    "metadata": {
      "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
      "summary": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "doi": "https://doi.org/10.48550/arxiv.2306.07691",
      "openalex_id": "https://openalex.org/W4380714711",
      "arxiv_id": "",
      "publication_date": "2023-06-13",
      "published": "2023-06-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on TTS using style diffusion and adversarial training with large speech language models and continuity, not on discrete audio tokenization or quantized token representations; it relies on continuous latent-style diffusion rather than analyzing discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on TTS using style diffusion and adversarial training with large speech language models and continuity, not on discrete audio tokenization or quantized token representations; it relies on continuous latent-style diffusion rather than analyzing discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on a text-to-speech (TTS) model utilizing style diffusion and adversarial training with large speech language models, but it does not mention discrete audio/speech tokenization as a core element; continuous speech features or latent variables through diffusion models are used instead without explicit discrete token sequences or quantization-based representations, which are essential for inclusion per the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on a text-to-speech (TTS) model utilizing style diffusion and adversarial training with large speech language models, but it does not mention discrete audio/speech tokenization as a core element; continuous speech features or latent variables through diffusion models are used instead without explicit discrete token sequences or quantization-based representations, which are essential for inclusion per the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Proceedings of the 4th International Workshop on Distributed Infrastructure for the Common Good",
    "abstract": "In contrast to many online services based on client-server infrastructure, peer-to-peer systems are usually designed as open commons. This is partly because, by design, peer-to-peer systems replicate data on end-user devices and typically use open ...",
    "metadata": {
      "title": "Proceedings of the 4th International Workshop on Distributed Infrastructure for the Common Good",
      "summary": "In contrast to many online services based on client-server infrastructure, peer-to-peer systems are usually designed as open commons. This is partly because, by design, peer-to-peer systems replicate data on end-user devices and typically use open ...",
      "abstract": "In contrast to many online services based on client-server infrastructure, peer-to-peer systems are usually designed as open commons. This is partly because, by design, peer-to-peer systems replicate data on end-user devices and typically use open ...",
      "doi": "https://doi.org/10.1145/3631310",
      "openalex_id": "https://openalex.org/W4391054691",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on distributed peer-to-peer infrastructure and does not involve discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on distributed peer-to-peer infrastructure and does not involve discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate the study concerns distributed infrastructure for the common good and peer-to-peer systems, with no mention of discrete audio/speech tokenization or related analysis; thus, it does not meet the inclusion criteria and likely falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate the study concerns distributed infrastructure for the common good and peer-to-peer systems, with no mention of discrete audio/speech tokenization or related analysis; thus, it does not meet the inclusion criteria and likely falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
    "abstract": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily altering phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation.",
    "metadata": {
      "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
      "summary": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily altering phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation.",
      "abstract": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily altering phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation.",
      "doi": "https://doi.org/10.48550/arxiv.2306.03389",
      "openalex_id": "https://openalex.org/W4379919722",
      "arxiv_id": "",
      "publication_date": "2023-06-06",
      "published": "2023-06-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on phase perturbation in time-domain speech countermeasures and does not involve discrete audio tokenization or tokenization-based representations; therefore it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on phase perturbation in time-domain speech countermeasures and does not involve discrete audio tokenization or tokenization-based representations; therefore it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phase perturbation to improve channel robustness in speech spoofing countermeasures but does not mention discrete audio/speech tokenization or discrete symbol sequences derived from continuous audio signals; it relies on continuous features and phase information for spoof detection rather than proposing or analyzing discrete token representations.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phase perturbation to improve channel robustness in speech spoofing countermeasures but does not mention discrete audio/speech tokenization or discrete symbol sequences derived from continuous audio signals; it relies on continuous features and phase information for spoof detection rather than proposing or analyzing discrete token representations.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Context, Perception, Production: A Model of Vocal Persona",
    "abstract": "We present a contextualized production-perception model of vocal persona based on a deductive thematic analysis of interviews with voice and performance experts. The model formalizes how the vocal persona frames and bounds expressive vocal interactions (both biological and synthesized), and centers a person's agency over their communicative role. This article provides insights into opportunities for improvement in Voice User Interfaces (VUI) and Augmentative \\&amp;amp; Assistive Communications (AAC) technologies based on the study's results. The proposed contextualized production-perception model fills an important gap in the literature on expressive and interactive speech technologies by identifying a key missing mechanism in contextualized vocal communication models and providing a resource for more nuanced approaches to expressive speech synthesis methods. Incorporating vocal persona into expressive vocal synthesis has the potential to significantly enhance the level of agency and embodiment experienced by VUI and AAC users during communication, resulting in a heightened sense of authenticity and an improved relationship with their environment.",
    "metadata": {
      "title": "Context, Perception, Production: A Model of Vocal Persona",
      "summary": "We present a contextualized production-perception model of vocal persona based on a deductive thematic analysis of interviews with voice and performance experts. The model formalizes how the vocal persona frames and bounds expressive vocal interactions (both biological and synthesized), and centers a person's agency over their communicative role. This article provides insights into opportunities for improvement in Voice User Interfaces (VUI) and Augmentative \\&amp;amp; Assistive Communications (AAC) technologies based on the study's results. The proposed contextualized production-perception model fills an important gap in the literature on expressive and interactive speech technologies by identifying a key missing mechanism in contextualized vocal communication models and providing a resource for more nuanced approaches to expressive speech synthesis methods. Incorporating vocal persona into expressive vocal synthesis has the potential to significantly enhance the level of agency and embodiment experienced by VUI and AAC users during communication, resulting in a heightened sense of authenticity and an improved relationship with their environment.",
      "abstract": "We present a contextualized production-perception model of vocal persona based on a deductive thematic analysis of interviews with voice and performance experts. The model formalizes how the vocal persona frames and bounds expressive vocal interactions (both biological and synthesized), and centers a person's agency over their communicative role. This article provides insights into opportunities for improvement in Voice User Interfaces (VUI) and Augmentative \\&amp;amp; Assistive Communications (AAC) technologies based on the study's results. The proposed contextualized production-perception model fills an important gap in the literature on expressive and interactive speech technologies by identifying a key missing mechanism in contextualized vocal communication models and providing a resource for more nuanced approaches to expressive speech synthesis methods. Incorporating vocal persona into expressive vocal synthesis has the potential to significantly enhance the level of agency and embodiment experienced by VUI and AAC users during communication, resulting in a heightened sense of authenticity and an improved relationship with their environment.",
      "doi": "https://doi.org/10.31234/osf.io/ebh2s",
      "openalex_id": "https://openalex.org/W4385381365",
      "arxiv_id": "",
      "publication_date": "2023-07-28",
      "published": "2023-07-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a model of vocal persona and its implications for VUI/AAC, but it does not address discrete audio/speech tokenization or analysis of converting continuous speech into discrete tokens, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a model of vocal persona and its implications for VUI/AAC, but it does not address discrete audio/speech tokenization or analysis of converting continuous speech into discrete tokens, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a model of vocal persona and its implications for voice interfaces, emphasizing contextualized vocal communication and expressive speech synthesis, but it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbol sequences, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a model of vocal persona and its implications for voice interfaces, emphasizing contextualized vocal communication and expressive speech synthesis, but it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbol sequences, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neutral TTS Female Voice Corpus in Brazilian Portuguese",
    "abstract": "This paper introduces a new dataset designed to address the limitations in high-quality, diverse and representative datasets for training text-to-speech (TTS) models, specifically for female voices in Brazilian Portuguese.The dataset features a female voice recorded in a professional and controlled environment with neutral emotion and comprises more than 20 hours of recordings.The goal is to facilitate transfer learning and enable the development of more natural-sounding, high-quality, and gender-balanced TTS systems.Alongside the dataset, genderaware voice transfer experiments are performed to understand the impact of utilizing gender-specific pretrained models for speech synthesis.The results obtained show that same-gender voice transfer yields better speech similarity and intelligibility when compared to cross-gender transfer, emphasizing the importance of gender-aware training procedures and highlighting the need for balanced gender data.",
    "metadata": {
      "title": "Neutral TTS Female Voice Corpus in Brazilian Portuguese",
      "summary": "This paper introduces a new dataset designed to address the limitations in high-quality, diverse and representative datasets for training text-to-speech (TTS) models, specifically for female voices in Brazilian Portuguese.The dataset features a female voice recorded in a professional and controlled environment with neutral emotion and comprises more than 20 hours of recordings.The goal is to facilitate transfer learning and enable the development of more natural-sounding, high-quality, and gender-balanced TTS systems.Alongside the dataset, genderaware voice transfer experiments are performed to understand the impact of utilizing gender-specific pretrained models for speech synthesis.The results obtained show that same-gender voice transfer yields better speech similarity and intelligibility when compared to cross-gender transfer, emphasizing the importance of gender-aware training procedures and highlighting the need for balanced gender data.",
      "abstract": "This paper introduces a new dataset designed to address the limitations in high-quality, diverse and representative datasets for training text-to-speech (TTS) models, specifically for female voices in Brazilian Portuguese.The dataset features a female voice recorded in a professional and controlled environment with neutral emotion and comprises more than 20 hours of recordings.The goal is to facilitate transfer learning and enable the development of more natural-sounding, high-quality, and gender-balanced TTS systems.Alongside the dataset, genderaware voice transfer experiments are performed to understand the impact of utilizing gender-specific pretrained models for speech synthesis.The results obtained show that same-gender voice transfer yields better speech similarity and intelligibility when compared to cross-gender transfer, emphasizing the importance of gender-aware training procedures and highlighting the need for balanced gender data.",
      "doi": "https://doi.org/10.14209/sbrt.2023.1570917697",
      "openalex_id": "https://openalex.org/W4388272221",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a Brazilian Portuguese TTS voice corpus and gender-aware transfer experiments, but it does not involve any discrete audio tokenization or analysis of discrete audio tokens, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a Brazilian Portuguese TTS voice corpus and gender-aware transfer experiments, but it does not involve any discrete audio tokenization or analysis of discrete audio tokens, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a TTS dataset creation and gender-aware voice transfer experiments but does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens, which is a required core object for inclusion; hence it does not meet the inclusion criteria and falls under exclusion since it uses continuous audio data without discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a TTS dataset creation and gender-aware voice transfer experiments but does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens, which is a required core object for inclusion; hence it does not meet the inclusion criteria and falls under exclusion since it uses continuous audio data without discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Current and Evolving Applications to Speech Processing",
    "abstract": "Automatic speech recognition (ASR) and text-to-speech (TTS) synthesis have been around for several decades. Although ASR and TTS deal with basic voice processing, including traditional vocoding mechanisms, artificial intelligence, especially machine learning/deep learning techniques, is increasingly being used to enhance the accuracy of recognition in ASR, and to enhance sentence fluency and intonation in TTS. The last decade has also seen major breakthroughs in speech synthesis by applying neural network methods and end-to-end modeling. ASR entails methods for converting the acoustic speech waveform into a sequence of textual words. This is traditionally done using statistical modeling of the speech signal. Noise cancellation mechanisms are typically used to deal with noise. In training of a dense neural network architecture used in ASR, a regression mechanism often includes a minimization of a pre-defined cost function.",
    "metadata": {
      "title": "Current and Evolving Applications to Speech Processing",
      "summary": "Automatic speech recognition (ASR) and text-to-speech (TTS) synthesis have been around for several decades. Although ASR and TTS deal with basic voice processing, including traditional vocoding mechanisms, artificial intelligence, especially machine learning/deep learning techniques, is increasingly being used to enhance the accuracy of recognition in ASR, and to enhance sentence fluency and intonation in TTS. The last decade has also seen major breakthroughs in speech synthesis by applying neural network methods and end-to-end modeling. ASR entails methods for converting the acoustic speech waveform into a sequence of textual words. This is traditionally done using statistical modeling of the speech signal. Noise cancellation mechanisms are typically used to deal with noise. In training of a dense neural network architecture used in ASR, a regression mechanism often includes a minimization of a pre-defined cost function.",
      "abstract": "Automatic speech recognition (ASR) and text-to-speech (TTS) synthesis have been around for several decades. Although ASR and TTS deal with basic voice processing, including traditional vocoding mechanisms, artificial intelligence, especially machine learning/deep learning techniques, is increasingly being used to enhance the accuracy of recognition in ASR, and to enhance sentence fluency and intonation in TTS. The last decade has also seen major breakthroughs in speech synthesis by applying neural network methods and end-to-end modeling. ASR entails methods for converting the acoustic speech waveform into a sequence of textual words. This is traditionally done using statistical modeling of the speech signal. Noise cancellation mechanisms are typically used to deal with noise. In training of a dense neural network architecture used in ASR, a regression mechanism often includes a minimization of a pre-defined cost function.",
      "doi": "https://doi.org/10.1002/9781394190034.ch3",
      "openalex_id": "https://openalex.org/W4388558441",
      "arxiv_id": "",
      "publication_date": "2023-11-10",
      "published": "2023-11-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses speech processing technologies and neural methods but does not propose, analyze, or evaluate discrete audio tokenization or token-based representations, focusing instead on continuous features and end-to-end models.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses speech processing technologies and neural methods but does not propose, analyze, or evaluate discrete audio tokenization or token-based representations, focusing instead on continuous features and end-to-end models.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses various speech processing technologies, including ASR and TTS, mentioning neural network methods and end-to-end modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods; it primarily describes continuous modeling and does not mention discrete token quantization or analysis, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses various speech processing technologies, including ASR and TTS, mentioning neural network methods and end-to-end modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods; it primarily describes continuous modeling and does not mention discrete token quantization or analysis, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ELEGANT: End-to-end Language Grounded Speech Denoiser for Efficient Generation of Talking Face",
    "abstract": "Existing speech driven talking face generation methods (a.k.a Speech2Face models) provide realistic-looking talking avatars. Yet, they are not appropriate if (1) the input speech signals are from the wild which could contain background noise; and (2) the input signal contains hate speech. In the presence of in-the-wild audio signals, the Speech2Face models do a bad lip-sync, unwanted facial movements, and sudden jitters on the head movements. While on the other hand, the Speech2Face models do not perform any reasoning on language understanding of input speech signal which could enable malicious users to translate hateful speech to a synthetic talking face promoting internet, social, and political threats. In this paper, we serve dual objectives on a single go. To the best of our knowledge, our method ELEGANT is the first Speech2Face generative model that performs a language grounding on the input speech that eliminates the transfer of spurious features originating from audio noise. Subsequently, the text embedding is associated with the speech style and passed on to a generative model with a view to learn the phoneme-viseme correspondence. In this way, our proposed ELEGANT model suppresses negative and hateful words using text embedding and also suppresses audio-specific noises using text embedding since noise-to-phoneme mapping would be random. Our experiments show that adopting the speech-denoising technique through text grounding eliminates the transfer of spurious features originating from audio noise to the vision domain. Consequently, a good phoneme-viseme correspondence leads to a comparable performances of SSIM and PSNR scores w.r.t state of the art methods.",
    "metadata": {
      "title": "ELEGANT: End-to-end Language Grounded Speech Denoiser for Efficient Generation of Talking Face",
      "summary": "Existing speech driven talking face generation methods (a.k.a Speech2Face models) provide realistic-looking talking avatars. Yet, they are not appropriate if (1) the input speech signals are from the wild which could contain background noise; and (2) the input signal contains hate speech. In the presence of in-the-wild audio signals, the Speech2Face models do a bad lip-sync, unwanted facial movements, and sudden jitters on the head movements. While on the other hand, the Speech2Face models do not perform any reasoning on language understanding of input speech signal which could enable malicious users to translate hateful speech to a synthetic talking face promoting internet, social, and political threats. In this paper, we serve dual objectives on a single go. To the best of our knowledge, our method ELEGANT is the first Speech2Face generative model that performs a language grounding on the input speech that eliminates the transfer of spurious features originating from audio noise. Subsequently, the text embedding is associated with the speech style and passed on to a generative model with a view to learn the phoneme-viseme correspondence. In this way, our proposed ELEGANT model suppresses negative and hateful words using text embedding and also suppresses audio-specific noises using text embedding since noise-to-phoneme mapping would be random. Our experiments show that adopting the speech-denoising technique through text grounding eliminates the transfer of spurious features originating from audio noise to the vision domain. Consequently, a good phoneme-viseme correspondence leads to a comparable performances of SSIM and PSNR scores w.r.t state of the art methods.",
      "abstract": "Existing speech driven talking face generation methods (a.k.a Speech2Face models) provide realistic-looking talking avatars. Yet, they are not appropriate if (1) the input speech signals are from the wild which could contain background noise; and (2) the input signal contains hate speech. In the presence of in-the-wild audio signals, the Speech2Face models do a bad lip-sync, unwanted facial movements, and sudden jitters on the head movements. While on the other hand, the Speech2Face models do not perform any reasoning on language understanding of input speech signal which could enable malicious users to translate hateful speech to a synthetic talking face promoting internet, social, and political threats. In this paper, we serve dual objectives on a single go. To the best of our knowledge, our method ELEGANT is the first Speech2Face generative model that performs a language grounding on the input speech that eliminates the transfer of spurious features originating from audio noise. Subsequently, the text embedding is associated with the speech style and passed on to a generative model with a view to learn the phoneme-viseme correspondence. In this way, our proposed ELEGANT model suppresses negative and hateful words using text embedding and also suppresses audio-specific noises using text embedding since noise-to-phoneme mapping would be random. Our experiments show that adopting the speech-denoising technique through text grounding eliminates the transfer of spurious features originating from audio noise to the vision domain. Consequently, a good phoneme-viseme correspondence leads to a comparable performances of SSIM and PSNR scores w.r.t state of the art methods.",
      "doi": "https://doi.org/10.1109/apsipaasc58517.2023.10317456",
      "openalex_id": "https://openalex.org/W4388821451",
      "arxiv_id": "",
      "publication_date": "2023-10-31",
      "published": "2023-10-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on end-to-end speech-driven talking-face generation with language grounding and text embeddings for denoising, but does not explicitly propose or analyze discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on end-to-end speech-driven talking-face generation with language grounding and text embeddings for denoising, but does not explicitly propose or analyze discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Speech2Face generative model that performs language grounding on input speech to eliminate noise and hate speech transfer, but it does not mention any discrete audio/speech tokenization process or analysis where continuous audio is tokenized into discrete symbols; it mainly discusses speech-denoising and language grounding via text embeddings without elaborating on discrete token design or neural discrete quantization, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Speech2Face generative model that performs language grounding on input speech to eliminate noise and hate speech transfer, but it does not mention any discrete audio/speech tokenization process or analysis where continuous audio is tokenized into discrete symbols; it mainly discusses speech-denoising and language grounding via text embeddings without elaborating on discrete token design or neural discrete quantization, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Invert-Classify: Recovering Discrete Prosody Inputs for Text-To-Speech",
    "abstract": "Modeling prosody in Text-to-Speech (TTS) is challenging due to ambiguous orthography and the high cost of annotating prosodic events. This study focuses on the modeling of contrastive focus, the emphasis of a word to contrast it to presuppositions held by an interlocutor. Modeling of contrastive focus can be done in TTS by using binary, symbolic inputs at the word level in a supervised setting. To address the absence of annotated data, we propose the Invert-Classify method, which leverages a frozen TTS model and unlabeled parallel text-speech data to recover missing contrastive focus inputs. Our approach achieves a binary F-score of up to 0.71 for contrastive focus annotation recovery, utilizing only 5-10 % of annotated training data. Furthermore, subjective listening tests show that training on additional data labeled via Invert-Classify enhances overall synthesis quality, as well as providing good control and plausible-sounding contrastive focus.",
    "metadata": {
      "title": "Invert-Classify: Recovering Discrete Prosody Inputs for Text-To-Speech",
      "summary": "Modeling prosody in Text-to-Speech (TTS) is challenging due to ambiguous orthography and the high cost of annotating prosodic events. This study focuses on the modeling of contrastive focus, the emphasis of a word to contrast it to presuppositions held by an interlocutor. Modeling of contrastive focus can be done in TTS by using binary, symbolic inputs at the word level in a supervised setting. To address the absence of annotated data, we propose the Invert-Classify method, which leverages a frozen TTS model and unlabeled parallel text-speech data to recover missing contrastive focus inputs. Our approach achieves a binary F-score of up to 0.71 for contrastive focus annotation recovery, utilizing only 5-10 % of annotated training data. Furthermore, subjective listening tests show that training on additional data labeled via Invert-Classify enhances overall synthesis quality, as well as providing good control and plausible-sounding contrastive focus.",
      "abstract": "Modeling prosody in Text-to-Speech (TTS) is challenging due to ambiguous orthography and the high cost of annotating prosodic events. This study focuses on the modeling of contrastive focus, the emphasis of a word to contrast it to presuppositions held by an interlocutor. Modeling of contrastive focus can be done in TTS by using binary, symbolic inputs at the word level in a supervised setting. To address the absence of annotated data, we propose the Invert-Classify method, which leverages a frozen TTS model and unlabeled parallel text-speech data to recover missing contrastive focus inputs. Our approach achieves a binary F-score of up to 0.71 for contrastive focus annotation recovery, utilizing only 5-10 % of annotated training data. Furthermore, subjective listening tests show that training on additional data labeled via Invert-Classify enhances overall synthesis quality, as well as providing good control and plausible-sounding contrastive focus.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389773",
      "openalex_id": "https://openalex.org/W4391021539",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on recovering and evaluating discrete, binary symbolic prosody inputs (tokenization) for TTS, treating discrete prosody tokens as the core object rather than continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on recovering and evaluating discrete, binary symbolic prosody inputs (tokenization) for TTS, treating discrete prosody tokens as the core object rather than continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modeling contrastive focus in TTS using binary symbolic inputs at the word level and proposes a method to recover these discrete prosody inputs from text-speech data, but it does not explicitly mention discrete audio/speech tokenization converting continuous audio into sequences of discrete tokens for audio reconstruction or generation, which is the core inclusion criterion; the discrete inputs are symbolic linguistic features rather than audio tokenization, and the study does not analyze or evaluate discrete audio tokens themselves.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modeling contrastive focus in TTS using binary symbolic inputs at the word level and proposes a method to recover these discrete prosody inputs from text-speech data, but it does not explicitly mention discrete audio/speech tokenization converting continuous audio into sequences of discrete tokens for audio reconstruction or generation, which is the core inclusion criterion; the discrete inputs are symbolic linguistic features rather than audio tokenization, and the study does not analyze or evaluate discrete audio tokens themselves.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano interpreted the work as centering on discrete prosody inputs, while JuniorMini correctly noted these are symbolic linguistic labels rather than discrete audio tokens. The paper recovers binary, word-level contrastive-focus labels for TTS from unlabeled data but does not propose, analyze, or evaluate any discrete audio/speech tokenization (no quantization of continuous audio into token sequences for reconstruction/generation). Therefore it fails the inclusion criterion and should be excluded. Explanation (1 sentence): Does not propose or evaluate discrete audio tokenization—uses symbolic binary prosody labels rather than quantized audio tokens, so exclude.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano interpreted the work as centering on discrete prosody inputs, while JuniorMini correctly noted these are symbolic linguistic labels rather than discrete audio tokens. The paper recovers binary, word-level contrastive-focus labels for TTS from unlabeled data but does not propose, analyze, or evaluate any discrete audio/speech tokenization (no quantization of continuous audio into token sequences for reconstruction/generation). Therefore it fails the inclusion criterion and should be excluded. Explanation (1 sentence): Does not propose or evaluate discrete audio tokenization—uses symbolic binary prosody labels rather than quantized audio tokens, so exclude.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Generation of Synthetic Echocardiograms Using Video Diffusion Models",
    "abstract": "An echocardiogram is a video sequence of a human heart captured using ultrasound imaging, which helps in diagnosis of cardiovascular diseases. Deep learning methods, which require large amounts of training data, have shown success in using echocardiograms to detect cardiovascular disorders. Large datasets of echocardiograms that can be used for machine learning training are scarce. This problem can be addressed by generating synthetic echocardiograms that can be used for machine learning training. In this paper, we propose a video diffusion method for echocardiograms generation. We show that our method generates better echocardiograms with higher resolution as compared to existing methods.",
    "metadata": {
      "title": "Generation of Synthetic Echocardiograms Using Video Diffusion Models",
      "summary": "An echocardiogram is a video sequence of a human heart captured using ultrasound imaging, which helps in diagnosis of cardiovascular diseases. Deep learning methods, which require large amounts of training data, have shown success in using echocardiograms to detect cardiovascular disorders. Large datasets of echocardiograms that can be used for machine learning training are scarce. This problem can be addressed by generating synthetic echocardiograms that can be used for machine learning training. In this paper, we propose a video diffusion method for echocardiograms generation. We show that our method generates better echocardiograms with higher resolution as compared to existing methods.",
      "abstract": "An echocardiogram is a video sequence of a human heart captured using ultrasound imaging, which helps in diagnosis of cardiovascular diseases. Deep learning methods, which require large amounts of training data, have shown success in using echocardiograms to detect cardiovascular disorders. Large datasets of echocardiograms that can be used for machine learning training are scarce. This problem can be addressed by generating synthetic echocardiograms that can be used for machine learning training. In this paper, we propose a video diffusion method for echocardiograms generation. We show that our method generates better echocardiograms with higher resolution as compared to existing methods.",
      "doi": "https://doi.org/10.1109/ssiai59505.2024.10508643",
      "openalex_id": "https://openalex.org/W4396215619",
      "arxiv_id": "",
      "publication_date": "2024-03-17",
      "published": "2024-03-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about generating synthetic echocardiograms with video diffusion models and discusses no discrete audio tokenization or token-level representations, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about generating synthetic echocardiograms with video diffusion models and discusses no discrete audio tokenization or token-level representations, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating synthetic echocardiograms using video diffusion models, which relates to ultrasound video data rather than audio or speech signals. It does not discuss any discrete audio/speech tokenization, token design, or quantization relevant to audio signal processing or speech tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio/speech tokenization and deals with a different modality (video ultrasound imaging) unrelated to the required scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating synthetic echocardiograms using video diffusion models, which relates to ultrasound video data rather than audio or speech signals. It does not discuss any discrete audio/speech tokenization, token design, or quantization relevant to audio signal processing or speech tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio/speech tokenization and deals with a different modality (video ultrasound imaging) unrelated to the required scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Notes on artificial intelligence: concepts, applications and techniques",
    "abstract": "Artificial Intelligence (AI) has been increasingly present in the contemporary world, with applicability in various fields of knowledge. AI-based solutions, implemented with different techniques, are present in different systems. The purpose of this article is to (1) present a brief history and the main concepts related to Artificial Intelligence - with a focus on machine learning (ML) techniques -, (2) discuss its nuances, techniques and (3) present some application examples in several areas. The intention is to approach the subject in an accessible way to the non-specialized public, in order to promote the understanding of its main concepts, but without the need to resort to very technical details. For this, a narrative review of the literature on AI is carried out, briefly addressing its history, concepts, intersections with other areas and applications. The machine learning segment receives special attention, with emphasis on supervised learning and its application in classification problems. The article also highlights the capabilities presented by the most modern machine learning techniques which, in some cases, present better results than those obtained by humans. Given these promising results, the future of AI points to the creation of systems with capabilities that will increasingly resemble those of the human intellect. However, the creation of truly thinking machines, and not of thinking simulators, should still remain for a long time as an objective to be achieved.",
    "metadata": {
      "title": "Notes on artificial intelligence: concepts, applications and techniques",
      "summary": "Artificial Intelligence (AI) has been increasingly present in the contemporary world, with applicability in various fields of knowledge. AI-based solutions, implemented with different techniques, are present in different systems. The purpose of this article is to (1) present a brief history and the main concepts related to Artificial Intelligence - with a focus on machine learning (ML) techniques -, (2) discuss its nuances, techniques and (3) present some application examples in several areas. The intention is to approach the subject in an accessible way to the non-specialized public, in order to promote the understanding of its main concepts, but without the need to resort to very technical details. For this, a narrative review of the literature on AI is carried out, briefly addressing its history, concepts, intersections with other areas and applications. The machine learning segment receives special attention, with emphasis on supervised learning and its application in classification problems. The article also highlights the capabilities presented by the most modern machine learning techniques which, in some cases, present better results than those obtained by humans. Given these promising results, the future of AI points to the creation of systems with capabilities that will increasingly resemble those of the human intellect. However, the creation of truly thinking machines, and not of thinking simulators, should still remain for a long time as an objective to be achieved.",
      "abstract": "Artificial Intelligence (AI) has been increasingly present in the contemporary world, with applicability in various fields of knowledge. AI-based solutions, implemented with different techniques, are present in different systems. The purpose of this article is to (1) present a brief history and the main concepts related to Artificial Intelligence - with a focus on machine learning (ML) techniques -, (2) discuss its nuances, techniques and (3) present some application examples in several areas. The intention is to approach the subject in an accessible way to the non-specialized public, in order to promote the understanding of its main concepts, but without the need to resort to very technical details. For this, a narrative review of the literature on AI is carried out, briefly addressing its history, concepts, intersections with other areas and applications. The machine learning segment receives special attention, with emphasis on supervised learning and its application in classification problems. The article also highlights the capabilities presented by the most modern machine learning techniques which, in some cases, present better results than those obtained by humans. Given these promising results, the future of AI points to the creation of systems with capabilities that will increasingly resemble those of the human intellect. However, the creation of truly thinking machines, and not of thinking simulators, should still remain for a long time as an objective to be achieved.",
      "doi": "https://doi.org/10.61411/rsc202457217",
      "openalex_id": "https://openalex.org/W4400268946",
      "arxiv_id": "",
      "publication_date": "2024-07-03",
      "published": "2024-07-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is a general overview of AI/ML with no mention or analysis of discrete audio/speech tokenization (token-based representations); it does not meet the inclusion criteria and does not address the relevant exclusion points. Therefore, it should be absolutely excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is a general overview of AI/ML with no mention or analysis of discrete audio/speech tokenization (token-based representations); it does not meet the inclusion criteria and does not address the relevant exclusion points. Therefore, it should be absolutely excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article is a broad, narrative review of AI, focusing on machine learning concepts and applications, mainly targeted at non-specialists, without any mention of discrete audio or speech tokenization or related methodological contributions; thus, it does not meet inclusion criteria nor avoid exclusion criteria concerning discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article is a broad, narrative review of AI, focusing on machine learning concepts and applications, mainly targeted at non-specialists, without any mention of discrete audio or speech tokenization or related methodological contributions; thus, it does not meet inclusion criteria nor avoid exclusion criteria concerning discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ConvAtt Network: A Low Parameter Approach For Sign Language Recognition",
    "abstract": "Despite recent advances in Large Language Models in text processing, Sign Language Recognition (SLR) remains an unresolved task. This is, in part, due to limitations in the available data. In this paper, we investigate combining 1D convolutions with transformer layers to capture local features and global interactions in a low-parameter SLR model. We experimented using multiple data augmentation and regularization techniques to categorize signs of the French Belgian Sign Language. We achieved a top-1 accuracy of 42.7% and a top-10 accuracy of 81.9% in 600 different signs. This model is competitive with the current state of the art while using a significantly lower number of parameters.",
    "metadata": {
      "title": "ConvAtt Network: A Low Parameter Approach For Sign Language Recognition",
      "summary": "Despite recent advances in Large Language Models in text processing, Sign Language Recognition (SLR) remains an unresolved task. This is, in part, due to limitations in the available data. In this paper, we investigate combining 1D convolutions with transformer layers to capture local features and global interactions in a low-parameter SLR model. We experimented using multiple data augmentation and regularization techniques to categorize signs of the French Belgian Sign Language. We achieved a top-1 accuracy of 42.7% and a top-10 accuracy of 81.9% in 600 different signs. This model is competitive with the current state of the art while using a significantly lower number of parameters.",
      "abstract": "Despite recent advances in Large Language Models in text processing, Sign Language Recognition (SLR) remains an unresolved task. This is, in part, due to limitations in the available data. In this paper, we investigate combining 1D convolutions with transformer layers to capture local features and global interactions in a low-parameter SLR model. We experimented using multiple data augmentation and regularization techniques to categorize signs of the French Belgian Sign Language. We achieved a top-1 accuracy of 42.7% and a top-10 accuracy of 81.9% in 600 different signs. This model is competitive with the current state of the art while using a significantly lower number of parameters.",
      "doi": "https://doi.org/10.24215/16666038.24.e10",
      "openalex_id": "https://openalex.org/W4403524969",
      "arxiv_id": "",
      "publication_date": "2024-10-18",
      "published": "2024-10-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on sign-language recognition using neural architectures (1D conv + transformers) with no discussion of discrete audio tokenization or quantized speech tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on sign-language recognition using neural architectures (1D conv + transformers) with no discussion of discrete audio tokenization or quantized speech tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition using a model combining convolution and transformer layers, but there is no mention of discrete audio/speech tokenization or any discrete tokenization of audio signals; the task concerns visual sign language recognition and not discrete audio token representation, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition using a model combining convolution and transformer layers, but there is no mention of discrete audio/speech tokenization or any discrete tokenization of audio signals; the task concerns visual sign language recognition and not discrete audio token representation, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Comparing human-labeled and AI-labeled speech datasets for TTS",
    "abstract": "As the output quality of neural networks in the fields of automatic speech recognition (ASR) and text-to-speech (TTS) continues to improve, new opportunities are becoming available to train models in a weakly supervised fashion, thus minimizing the manual effort required to annotate new audio data for supervised training. While weak supervision has recently shown very promising results in the domain of ASR, speech synthesis has not yet been thoroughly investigated regarding this technique despite requiring the equivalent training dataset structure of aligned audio-transcript pairs. In this work, we compare the performance of TTS models trained using a well-curated and manually labeled training dataset to others trained on the same audio data with text labels generated using both grapheme- and phoneme-based ASR models. Phoneme-based approaches seem especially promising, since even for wrongly predicted phonemes, the resulting word is more likely to sound similar to the originally spoken word than for grapheme-based predictions. For evaluation and ranking, we generate synthesized audio outputs from all previously trained models using input texts sourced from a selection of speech recognition datasets covering a wide range of application domains. These synthesized outputs are subsequently fed into multiple state-of-the-art ASR models with their output text predictions being compared to the initial TTS model input texts. This comparison enables an objective assessment of the intelligibility of the audio outputs from all TTS models, by utilizing metrics like word error rate and character error rate. Our results not only show that models trained on data generated with weak supervision achieve comparable quality to models trained on manually labeled datasets, but can outperform the latter, even for small, well-curated speech datasets. These findings suggest that the future creation of labeled datasets for supervised training of TTS models may not require any manual annotation but can be fully automated.",
    "metadata": {
      "title": "Comparing human-labeled and AI-labeled speech datasets for TTS",
      "summary": "As the output quality of neural networks in the fields of automatic speech recognition (ASR) and text-to-speech (TTS) continues to improve, new opportunities are becoming available to train models in a weakly supervised fashion, thus minimizing the manual effort required to annotate new audio data for supervised training. While weak supervision has recently shown very promising results in the domain of ASR, speech synthesis has not yet been thoroughly investigated regarding this technique despite requiring the equivalent training dataset structure of aligned audio-transcript pairs. In this work, we compare the performance of TTS models trained using a well-curated and manually labeled training dataset to others trained on the same audio data with text labels generated using both grapheme- and phoneme-based ASR models. Phoneme-based approaches seem especially promising, since even for wrongly predicted phonemes, the resulting word is more likely to sound similar to the originally spoken word than for grapheme-based predictions. For evaluation and ranking, we generate synthesized audio outputs from all previously trained models using input texts sourced from a selection of speech recognition datasets covering a wide range of application domains. These synthesized outputs are subsequently fed into multiple state-of-the-art ASR models with their output text predictions being compared to the initial TTS model input texts. This comparison enables an objective assessment of the intelligibility of the audio outputs from all TTS models, by utilizing metrics like word error rate and character error rate. Our results not only show that models trained on data generated with weak supervision achieve comparable quality to models trained on manually labeled datasets, but can outperform the latter, even for small, well-curated speech datasets. These findings suggest that the future creation of labeled datasets for supervised training of TTS models may not require any manual annotation but can be fully automated.",
      "abstract": "As the output quality of neural networks in the fields of automatic speech recognition (ASR) and text-to-speech (TTS) continues to improve, new opportunities are becoming available to train models in a weakly supervised fashion, thus minimizing the manual effort required to annotate new audio data for supervised training. While weak supervision has recently shown very promising results in the domain of ASR, speech synthesis has not yet been thoroughly investigated regarding this technique despite requiring the equivalent training dataset structure of aligned audio-transcript pairs. In this work, we compare the performance of TTS models trained using a well-curated and manually labeled training dataset to others trained on the same audio data with text labels generated using both grapheme- and phoneme-based ASR models. Phoneme-based approaches seem especially promising, since even for wrongly predicted phonemes, the resulting word is more likely to sound similar to the originally spoken word than for grapheme-based predictions. For evaluation and ranking, we generate synthesized audio outputs from all previously trained models using input texts sourced from a selection of speech recognition datasets covering a wide range of application domains. These synthesized outputs are subsequently fed into multiple state-of-the-art ASR models with their output text predictions being compared to the initial TTS model input texts. This comparison enables an objective assessment of the intelligibility of the audio outputs from all TTS models, by utilizing metrics like word error rate and character error rate. Our results not only show that models trained on data generated with weak supervision achieve comparable quality to models trained on manually labeled datasets, but can outperform the latter, even for small, well-curated speech datasets. These findings suggest that the future creation of labeled datasets for supervised training of TTS models may not require any manual annotation but can be fully automated.",
      "doi": "https://doi.org/10.34190/icair.5.1.3030",
      "openalex_id": "https://openalex.org/W4405087680",
      "arxiv_id": "",
      "publication_date": "2024-12-04",
      "published": "2024-12-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly analyzes discrete speech tokenization (phoneme- vs grapheme-based labels) used to train and evaluate TTS, and assesses how discrete token representations affect intelligibility, matching the inclusion criterion and not falling into exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly analyzes discrete speech tokenization (phoneme- vs grapheme-based labels) used to train and evaluate TTS, and assesses how discrete token representations affect intelligibility, matching the inclusion criterion and not falling into exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study compares TTS models using manually labeled vs. AI-labeled datasets but does not focus on discrete audio/speech tokenization or the transformation of continuous audio signals into discrete tokens from a finite vocabulary; rather, it evaluates overall model performance and intelligibility using existing representations without proposing or analyzing discrete tokenization methods relevant to language-model-style sequences, thus not meeting the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study compares TTS models using manually labeled vs. AI-labeled datasets but does not focus on discrete audio/speech tokenization or the transformation of continuous audio signals into discrete tokens from a finite vocabulary; rather, it evaluates overall model performance and intelligibility using existing representations without proposing or analyzing discrete tokenization methods relevant to language-model-style sequences, thus not meeting the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one thought it matched, one thought it did not); after review I agree with the latter—this work compares manually vs AI-generated text transcripts (grapheme/phoneme labels) for training TTS and evaluates intelligibility, but it does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., quantizing continuous audio into discrete token sequences for generation/reconstruction), so it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one thought it matched, one thought it did not); after review I agree with the latter—this work compares manually vs AI-generated text transcripts (grapheme/phoneme labels) for training TTS and evaluates intelligibility, but it does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., quantizing continuous audio into discrete token sequences for generation/reconstruction), so it fails the core inclusion criterion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Development of Arabic Reading Materials Based on Moderation with Artificial Intelligence/Tathwir Mawad al-Qira’ah ‘ala Asas al-Wasathiyah bi Istikhdam al-Dzaka’ al-Ishthina’iy",
    "abstract": "Qira’ah learning materials often fail to effectively integrate the values of moderation. Additionally, conventional learning methods sometimes struggle to engage students and promote independent learning. Therefore, there is a need for innovative qira’ah materials that not only focus on Arabic reading skills but also instill the values of moderation. Artificial Intelligence (AI) technology offers a solution through interactive features, automated feedback, and adaptive learning, which can enrich the learning experience and deepen students’ understanding of moderation values in Arabic texts. This article explored the development of qira’ah learning materials based on moderation, supported by AI technology, at STAI Darul Ulum Banyuanyar Pamekasan. This research employed the ADDIE development model (Analysis, Design, Development, Implementation, Evaluation). Data collection techniques included observation, interviews, questionnaires, documentation, and tests. The qualitative data analysis technique used was classification analysis, while the quantitative analysis applies the gain score formula: d=T2−T1d = T2 - T1d=T2−T1. The findings indicate that qira’ah learning materials based on moderation, supported by AI, can enhance students' understanding of moderation values and boost their learning motivation. With interactive features, learning becomes more adaptive and responsive to students’ needs, allowing them to study more independently and effectively. This research offered a substantial contribution to the advancement of Arabic language education that integrates both technological innovation and the principles of moderation. It highlighted how AI integration in qira’ah materials can significantly improve learning effectiveness, promote learner autonomy, and embed moderation values in a more profound and contextualized way.",
    "metadata": {
      "title": "Development of Arabic Reading Materials Based on Moderation with Artificial Intelligence/Tathwir Mawad al-Qira’ah ‘ala Asas al-Wasathiyah bi Istikhdam al-Dzaka’ al-Ishthina’iy",
      "summary": "Qira’ah learning materials often fail to effectively integrate the values of moderation. Additionally, conventional learning methods sometimes struggle to engage students and promote independent learning. Therefore, there is a need for innovative qira’ah materials that not only focus on Arabic reading skills but also instill the values of moderation. Artificial Intelligence (AI) technology offers a solution through interactive features, automated feedback, and adaptive learning, which can enrich the learning experience and deepen students’ understanding of moderation values in Arabic texts. This article explored the development of qira’ah learning materials based on moderation, supported by AI technology, at STAI Darul Ulum Banyuanyar Pamekasan. This research employed the ADDIE development model (Analysis, Design, Development, Implementation, Evaluation). Data collection techniques included observation, interviews, questionnaires, documentation, and tests. The qualitative data analysis technique used was classification analysis, while the quantitative analysis applies the gain score formula: d=T2−T1d = T2 - T1d=T2−T1. The findings indicate that qira’ah learning materials based on moderation, supported by AI, can enhance students' understanding of moderation values and boost their learning motivation. With interactive features, learning becomes more adaptive and responsive to students’ needs, allowing them to study more independently and effectively. This research offered a substantial contribution to the advancement of Arabic language education that integrates both technological innovation and the principles of moderation. It highlighted how AI integration in qira’ah materials can significantly improve learning effectiveness, promote learner autonomy, and embed moderation values in a more profound and contextualized way.",
      "abstract": "Qira’ah learning materials often fail to effectively integrate the values of moderation. Additionally, conventional learning methods sometimes struggle to engage students and promote independent learning. Therefore, there is a need for innovative qira’ah materials that not only focus on Arabic reading skills but also instill the values of moderation. Artificial Intelligence (AI) technology offers a solution through interactive features, automated feedback, and adaptive learning, which can enrich the learning experience and deepen students’ understanding of moderation values in Arabic texts. This article explored the development of qira’ah learning materials based on moderation, supported by AI technology, at STAI Darul Ulum Banyuanyar Pamekasan. This research employed the ADDIE development model (Analysis, Design, Development, Implementation, Evaluation). Data collection techniques included observation, interviews, questionnaires, documentation, and tests. The qualitative data analysis technique used was classification analysis, while the quantitative analysis applies the gain score formula: d=T2−T1d = T2 - T1d=T2−T1. The findings indicate that qira’ah learning materials based on moderation, supported by AI, can enhance students' understanding of moderation values and boost their learning motivation. With interactive features, learning becomes more adaptive and responsive to students’ needs, allowing them to study more independently and effectively. This research offered a substantial contribution to the advancement of Arabic language education that integrates both technological innovation and the principles of moderation. It highlighted how AI integration in qira’ah materials can significantly improve learning effectiveness, promote learner autonomy, and embed moderation values in a more profound and contextualized way.",
      "doi": "https://doi.org/10.24042/c1qfz450",
      "openalex_id": "https://openalex.org/W4411350491",
      "arxiv_id": "",
      "publication_date": "2025-06-06",
      "published": "2025-06-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on AI-enabled Arabic reading materials and moderation values, but does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion of continuous audio signals into discrete token sequences) as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on AI-enabled Arabic reading materials and moderation values, but does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion of continuous audio signals into discrete token sequences) as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing Arabic reading materials integrating AI to promote moderation values and enhance learning motivation, but it does not discuss or analyze discrete audio/speech tokenization of continuous audio signals into discrete tokens, nor does it address neural encoding or modern quantization-based audio token representations relevant to language models; the research is also not solely an engineering report but an educational development study and the full text language availability is not confirmed.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing Arabic reading materials integrating AI to promote moderation values and enhance learning motivation, but it does not discuss or analyze discrete audio/speech tokenization of continuous audio signals into discrete tokens, nor does it address neural encoding or modern quantization-based audio token representations relevant to language models; the research is also not solely an engineering report but an educational development study and the full text language availability is not confirmed.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploring fairness in service robotics",
    "abstract": "Abstract Fairness in service robotics is a complex and multidimensional concept shaped by legal, social and technical considerations. As service robots increasingly operate in personal and professional domains, questions of fairness – ranging from legal certainty and anti-discrimination to user protection and algorithmic transparency – require systematic and interdisciplinary engagement. This paper develops a working definition of fairness tailored to the domain of service robotics based on a doctrinal analysis of how fairness is understood across different fields. It identifies four key dimensions essential to fair service robotics: (i) furthering legal certainty, (ii) preventing bias and discrimination, (iii) protecting users from exploitation and (iv) ensuring transparency and accountability. The paper explores how developers, policymakers and researchers can contribute to these goals. While fairness may resist universal definition, articulating its core components offers a foundation for guiding more equitable and trustworthy human–robot interactions.",
    "metadata": {
      "title": "Exploring fairness in service robotics",
      "summary": "Abstract Fairness in service robotics is a complex and multidimensional concept shaped by legal, social and technical considerations. As service robots increasingly operate in personal and professional domains, questions of fairness – ranging from legal certainty and anti-discrimination to user protection and algorithmic transparency – require systematic and interdisciplinary engagement. This paper develops a working definition of fairness tailored to the domain of service robotics based on a doctrinal analysis of how fairness is understood across different fields. It identifies four key dimensions essential to fair service robotics: (i) furthering legal certainty, (ii) preventing bias and discrimination, (iii) protecting users from exploitation and (iv) ensuring transparency and accountability. The paper explores how developers, policymakers and researchers can contribute to these goals. While fairness may resist universal definition, articulating its core components offers a foundation for guiding more equitable and trustworthy human–robot interactions.",
      "abstract": "Abstract Fairness in service robotics is a complex and multidimensional concept shaped by legal, social and technical considerations. As service robots increasingly operate in personal and professional domains, questions of fairness – ranging from legal certainty and anti-discrimination to user protection and algorithmic transparency – require systematic and interdisciplinary engagement. This paper develops a working definition of fairness tailored to the domain of service robotics based on a doctrinal analysis of how fairness is understood across different fields. It identifies four key dimensions essential to fair service robotics: (i) furthering legal certainty, (ii) preventing bias and discrimination, (iii) protecting users from exploitation and (iv) ensuring transparency and accountability. The paper explores how developers, policymakers and researchers can contribute to these goals. While fairness may resist universal definition, articulating its core components offers a foundation for guiding more equitable and trustworthy human–robot interactions.",
      "doi": "https://doi.org/10.1017/cfl.2025.10012",
      "openalex_id": "https://openalex.org/W4411650237",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not address discrete audio/speech tokenization or discrete token representations; it focuses on fairness in service robotics rather than tokenization methods, so it fails the inclusion criteria and should be excluded (score 1).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not address discrete audio/speech tokenization or discrete token representations; it focuses on fairness in service robotics rather than tokenization methods, so it fails the inclusion criteria and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fairness in service robotics and does not discuss or analyze discrete audio/speech tokenization, nor does it involve any audio signal quantization or token design relevant to the inclusion criteria. Therefore, it does not meet the criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fairness in service robotics and does not discuss or analyze discrete audio/speech tokenization, nor does it involve any audio signal quantization or token design relevant to the inclusion criteria. Therefore, it does not meet the criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Expressiveness in Vocal Conversational Agents through Large Language Model-Generated Speech Synthesis Markup Language",
    "abstract": "Advancements in speech synthesis have enabled more natural and engaging conversational agents, including neural text-to-speech models that can adjust speech inflections to produce distinct vocal styles. For example, Azure Neural Voices can adjust speech using Speech Synthesis Markup Language (SSML) style tags, such as “affectionate,” “cheerful,” and “hopeful.” However, determining when to apply these tags in real-time interactions can be challenging and time-consuming. In this paper, we present a prompt-based approach that enables large language models (LLMs) to dynamically stylize their responses with appropriate SSML tags, enhancing synthesized speech expressiveness across 34 different styles. Using targeted probes designed to elicit specific speech styles, we demonstrate that LLM-generated responses are syntactically well-formed and correctly apply style tags to enhance expressiveness. This simple, customizable approach facilitates the rapid development of expressive vocal conversational agents.",
    "metadata": {
      "title": "Enhancing Expressiveness in Vocal Conversational Agents through Large Language Model-Generated Speech Synthesis Markup Language",
      "summary": "Advancements in speech synthesis have enabled more natural and engaging conversational agents, including neural text-to-speech models that can adjust speech inflections to produce distinct vocal styles. For example, Azure Neural Voices can adjust speech using Speech Synthesis Markup Language (SSML) style tags, such as “affectionate,” “cheerful,” and “hopeful.” However, determining when to apply these tags in real-time interactions can be challenging and time-consuming. In this paper, we present a prompt-based approach that enables large language models (LLMs) to dynamically stylize their responses with appropriate SSML tags, enhancing synthesized speech expressiveness across 34 different styles. Using targeted probes designed to elicit specific speech styles, we demonstrate that LLM-generated responses are syntactically well-formed and correctly apply style tags to enhance expressiveness. This simple, customizable approach facilitates the rapid development of expressive vocal conversational agents.",
      "abstract": "Advancements in speech synthesis have enabled more natural and engaging conversational agents, including neural text-to-speech models that can adjust speech inflections to produce distinct vocal styles. For example, Azure Neural Voices can adjust speech using Speech Synthesis Markup Language (SSML) style tags, such as “affectionate,” “cheerful,” and “hopeful.” However, determining when to apply these tags in real-time interactions can be challenging and time-consuming. In this paper, we present a prompt-based approach that enables large language models (LLMs) to dynamically stylize their responses with appropriate SSML tags, enhancing synthesized speech expressiveness across 34 different styles. Using targeted probes designed to elicit specific speech styles, we demonstrate that LLM-generated responses are syntactically well-formed and correctly apply style tags to enhance expressiveness. This simple, customizable approach facilitates the rapid development of expressive vocal conversational agents.",
      "doi": "https://doi.org/10.32473/flairs.38.1.138814",
      "openalex_id": "https://openalex.org/W4410398715",
      "arxiv_id": "",
      "publication_date": "2025-05-14",
      "published": "2025-05-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study concentrates on SSML-based expressiveness and LLM-driven speech styling rather than discrete audio/tokenization analysis, so it does not meet the criterion of evaluating discrete audio tokens as a core object.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study concentrates on SSML-based expressiveness and LLM-driven speech styling rather than discrete audio/tokenization analysis, so it does not meet the criterion of evaluating discrete audio tokens as a core object.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using large language models to generate Speech Synthesis Markup Language (SSML) tags to enhance speech expressiveness, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study; rather, it deals with speech style tagging and synthesis control without addressing discrete tokenization of audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using large language models to generate Speech Synthesis Markup Language (SSML) tags to enhance speech expressiveness, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study; rather, it deals with speech style tagging and synthesis control without addressing discrete tokenization of audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
    "abstract": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
    "metadata": {
      "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
      "summary": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "abstract": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.327",
      "openalex_id": "https://openalex.org/W4389519824",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses latent continuous representations to augment training with text data and does not analyze or employ discrete audio tokenization as a core component, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work uses latent continuous representations to augment training with text data and does not analyze or employ discrete audio tokenization as a core component, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving end-to-end speech processing by utilizing latent synthesis to convert textual data into intermediate latent representations for speech model training. However, the method does not explicitly propose or analyze discrete audio/speech tokenization converting continuous audio signals into discrete symbols; instead, it deals with latent representations and pseudo acoustic data without mentioning discrete quantization or token sequences as the core object of study, thus not meeting the key inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving end-to-end speech processing by utilizing latent synthesis to convert textual data into intermediate latent representations for speech model training. However, the method does not explicitly propose or analyze discrete audio/speech tokenization converting continuous audio signals into discrete symbols; instead, it deals with latent representations and pseudo acoustic data without mentioning discrete quantization or token sequences as the core object of study, thus not meeting the key inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Joint speech and text machine translation for up to 100 languages",
    "abstract": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
    "metadata": {
      "title": "Joint speech and text machine translation for up to 100 languages",
      "summary": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
      "abstract": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
      "doi": "https://doi.org/10.1038/s41586-024-08359-z",
      "openalex_id": "https://openalex.org/W4406385636",
      "arxiv_id": "",
      "publication_date": "2025-01-15",
      "published": "2025-01-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work presents a unified multilingual/multimodal translation model but does not explicitly propose, analyze, or evaluate discrete audio tokenization as a core object of study; it focuses on end-to-end translation performance rather than discrete audio token representations.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work presents a unified multilingual/multimodal translation model but does not explicitly propose, analyze, or evaluate discrete audio tokenization as a core object of study; it focuses on end-to-end translation performance rather than discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces SEAMLESSM4T, a multilingual and multimodal machine translation system supporting speech and text translation across many languages. However, the abstract does not mention any novel discrete audio/speech tokenization methodology or analysis converting continuous audio signals into discrete tokens; instead, it focuses on the overall system performance and robustness. It appears to employ typical speech translation pipelines without explicit discrete token design or examination, thus not meeting the inclusion criteria which require discrete audio tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces SEAMLESSM4T, a multilingual and multimodal machine translation system supporting speech and text translation across many languages. However, the abstract does not mention any novel discrete audio/speech tokenization methodology or analysis converting continuous audio signals into discrete tokens; instead, it focuses on the overall system performance and robustness. It appears to employ typical speech translation pipelines without explicit discrete token design or examination, thus not meeting the inclusion criteria which require discrete audio tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
    "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
    "metadata": {
      "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
      "summary": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "doi": "https://doi.org/10.1051/itmconf/20246901003",
      "openalex_id": "https://openalex.org/W4405379347",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a chatbot and dataset for Darija STT/TTS but does not analyze or propose discrete audio tokenization as a core object, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a chatbot and dataset for Darija STT/TTS but does not analyze or propose discrete audio tokenization as a core object, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing a chatbot system and curating a database related to agricultural terminology and Darija language processing, with emphasis on text-to-text interactions and future fine-tuning of speech models; however, there is no explicit mention of discrete audio/speech tokenization or conversion of continuous audio to discrete tokens as a core object of study, nor a methodological contribution related to discrete tokenization, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing a chatbot system and curating a database related to agricultural terminology and Darija language processing, with emphasis on text-to-text interactions and future fine-tuning of speech models; however, there is no explicit mention of discrete audio/speech tokenization or conversion of continuous audio to discrete tokens as a core object of study, nor a methodological contribution related to discrete tokenization, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
    "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
    "metadata": {
      "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
      "summary": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "doi": "https://doi.org/10.18653/v1/2023.iwslt-1.33",
      "openalex_id": "https://openalex.org/W4385571610",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The abstract describes a speech-to-speech translation system using continuous audio features and a cascade TTS model, with no explicit discrete audio tokenization or token-based representations analyzed, hence it does not meet the inclusion criteria and aligns with exclusion categories focused on non-tokenized or engineering-only audio processing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The abstract describes a speech-to-speech translation system using continuous audio features and a cascade TTS model, with no explicit discrete audio tokenization or token-based representations analyzed, hence it does not meet the inclusion criteria and aligns with exclusion categories focused on non-tokenized or engineering-only audio processing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a speech-to-speech translation system using dual-decoder Transformer and cascade TTS models but does not mention proposing or analyzing discrete audio tokenization nor any methodological contribution on discrete token design or neural quantization relevant to audio token sequences, thus it does not meet the inclusion criteria nor avoids the exclusions related to pure continuous features and engineering-only report.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper describes a speech-to-speech translation system using dual-decoder Transformer and cascade TTS models but does not mention proposing or analyzing discrete audio tokenization nor any methodological contribution on discrete token design or neural quantization relevant to audio token sequences, thus it does not meet the inclusion criteria nor avoids the exclusions related to pure continuous features and engineering-only report.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "STEMGEN: A Music Generation Model That Listens",
    "abstract": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
    "metadata": {
      "title": "STEMGEN: A Music Generation Model That Listens",
      "summary": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "abstract": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446088",
      "openalex_id": "https://openalex.org/W4392904237",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes end-to-end audio generation without explicit discrete tokenization/quantization into a finite vocabulary, so it does not meet the core inclusion criterion for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes end-to-end audio generation without explicit discrete tokenization/quantization into a finite vocabulary, so it does not meet the core inclusion criterion for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on music generation using a transformer-based model that listens and responds to musical context, but it does not explicitly mention the use, analysis, or evaluation of discrete audio or speech tokenization as a core aspect; instead, it appears to work with continuous audio features and generation, without elaborating on discrete quantization or token design applicable to speech, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on music generation using a transformer-based model that listens and responds to musical context, but it does not explicitly mention the use, analysis, or evaluation of discrete audio or speech tokenization as a core aspect; instead, it appears to work with continuous audio features and generation, without elaborating on discrete quantization or token design applicable to speech, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Automatic Speech Recognition: A survey of deep learning techniques and approaches",
    "abstract": "Significant research has been conducted during the last decade on the application of machine learning for speech processing, particularly speech recognition. However, in recent years, deep learning models have shown promising results for different speech related applications. With the emergence of end-to-end models, deep learning has revolutionized the field of Automatic Speech Recognition (ASR). A recent surge in transfer learning-based models and attention-based approaches on large datasets has further given an impetus to ASR. This paper provides a thorough review of the numerous studies conducted since 2010, as well as an extensive comparison of the state-of-the-art methods that are now being used in this research area, with a special focus on the numerous deep learning models, along with an analysis of contemporary approaches for both monolingual and multilingual models. Deep learning approaches are data dependent and their accuracy varies on different datasets. In this paper, we have also analyzed the various models on publicly accessible speech datasets to understand model performance across diverse datasets for practical deployment. This study also highlights the research findings and challenges with way forward that may be used as a beginning point for academicians interested in open-source Automatic Speech Recognition (ASR) research, particularly focusing on mitigating data dependency and generalizability across low resource languages, speaker variability, and noise conditions.",
    "metadata": {
      "title": "Automatic Speech Recognition: A survey of deep learning techniques and approaches",
      "summary": "Significant research has been conducted during the last decade on the application of machine learning for speech processing, particularly speech recognition. However, in recent years, deep learning models have shown promising results for different speech related applications. With the emergence of end-to-end models, deep learning has revolutionized the field of Automatic Speech Recognition (ASR). A recent surge in transfer learning-based models and attention-based approaches on large datasets has further given an impetus to ASR. This paper provides a thorough review of the numerous studies conducted since 2010, as well as an extensive comparison of the state-of-the-art methods that are now being used in this research area, with a special focus on the numerous deep learning models, along with an analysis of contemporary approaches for both monolingual and multilingual models. Deep learning approaches are data dependent and their accuracy varies on different datasets. In this paper, we have also analyzed the various models on publicly accessible speech datasets to understand model performance across diverse datasets for practical deployment. This study also highlights the research findings and challenges with way forward that may be used as a beginning point for academicians interested in open-source Automatic Speech Recognition (ASR) research, particularly focusing on mitigating data dependency and generalizability across low resource languages, speaker variability, and noise conditions.",
      "abstract": "Significant research has been conducted during the last decade on the application of machine learning for speech processing, particularly speech recognition. However, in recent years, deep learning models have shown promising results for different speech related applications. With the emergence of end-to-end models, deep learning has revolutionized the field of Automatic Speech Recognition (ASR). A recent surge in transfer learning-based models and attention-based approaches on large datasets has further given an impetus to ASR. This paper provides a thorough review of the numerous studies conducted since 2010, as well as an extensive comparison of the state-of-the-art methods that are now being used in this research area, with a special focus on the numerous deep learning models, along with an analysis of contemporary approaches for both monolingual and multilingual models. Deep learning approaches are data dependent and their accuracy varies on different datasets. In this paper, we have also analyzed the various models on publicly accessible speech datasets to understand model performance across diverse datasets for practical deployment. This study also highlights the research findings and challenges with way forward that may be used as a beginning point for academicians interested in open-source Automatic Speech Recognition (ASR) research, particularly focusing on mitigating data dependency and generalizability across low resource languages, speaker variability, and noise conditions.",
      "doi": "https://doi.org/10.1016/j.ijcce.2024.12.007",
      "openalex_id": "https://openalex.org/W4406107190",
      "arxiv_id": "",
      "publication_date": "2025-01-06",
      "published": "2025-01-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract reviews deep learning methods for ASR and end-to-end models but does not address discrete audio tokenization or token-level representations as the core focus; it concentrates on model architectures, datasets, and general performance rather than tokenization design.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract reviews deep learning methods for ASR and end-to-end models but does not address discrete audio tokenization or token-level representations as the core focus; it concentrates on model architectures, datasets, and general performance rather than tokenization design.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a comprehensive survey of deep learning techniques in Automatic Speech Recognition but does not specifically focus on discrete audio/speech tokenization methods or propose, analyze, or evaluate discrete tokenization approaches; the emphasis is more on overall ASR techniques and deep learning models without explicit mention of discrete token design or analysis as a core study object.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a comprehensive survey of deep learning techniques in Automatic Speech Recognition but does not specifically focus on discrete audio/speech tokenization methods or propose, analyze, or evaluate discrete tokenization approaches; the emphasis is more on overall ASR techniques and deep learning models without explicit mention of discrete token design or analysis as a core study object.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HiddenSinger: High-quality singing voice synthesis via neural audio codec and latent diffusion models",
    "abstract": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, there are limitations in complexity and controllability to apply diffusion models for time-varying audio synthesis. Particularly, a singing voice synthesis (SVS) task, which has begun to emerge as a practical application in the game and entertainment industries, requires high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in the SVS task, we propose HiddenSinger, a high-quality SVS system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models regarding audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
    "metadata": {
      "title": "HiddenSinger: High-quality singing voice synthesis via neural audio codec and latent diffusion models",
      "summary": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, there are limitations in complexity and controllability to apply diffusion models for time-varying audio synthesis. Particularly, a singing voice synthesis (SVS) task, which has begun to emerge as a practical application in the game and entertainment industries, requires high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in the SVS task, we propose HiddenSinger, a high-quality SVS system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models regarding audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "abstract": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, there are limitations in complexity and controllability to apply diffusion models for time-varying audio synthesis. Particularly, a singing voice synthesis (SVS) task, which has begun to emerge as a practical application in the game and entertainment industries, requires high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in the SVS task, we propose HiddenSinger, a high-quality SVS system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models regarding audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106762",
      "openalex_id": "https://openalex.org/W4402905495",
      "arxiv_id": "",
      "publication_date": "2024-09-27",
      "published": "2024-09-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study describes singing voice synthesis using a neural audio codec and latent diffusion in a compressed continuous latent space; it does not explicitly propose or evaluate discrete audio tokenization (quantized tokens or a finite-symbol vocabulary), which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study describes singing voice synthesis using a neural audio codec and latent diffusion in a compressed continuous latent space; it does not explicitly propose or evaluate discrete audio tokenization (quantized tokens or a finite-symbol vocabulary), which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural audio codec-based singing voice synthesis system employing latent diffusion models, indicating the use of neural encoding and presumably discrete token-like latent representations derived from compressed audio representations; however, the abstract does not explicitly clarify the discrete tokenization mechanism nor its analysis as a core focus, leading to some ambiguity about meeting the inclusion criteria precisely.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural audio codec-based singing voice synthesis system employing latent diffusion models, indicating the use of neural encoding and presumably discrete token-like latent representations derived from compressed audio representations; however, the abstract does not explicitly clarify the discrete tokenization mechanism nor its analysis as a core focus, leading to some ambiguity about meeting the inclusion criteria precisely.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged ambiguity: JuniorNano concluded the work does not explicitly address discrete tokenization and recommended exclusion; JuniorMini noted possible latent representations but the abstract lacks clarity on discrete token design. The paper describes a neural audio codec and encoding to a “low-dimensional compressed latent vector” and uses latent diffusion for generation, but it does not explicitly propose, analyze, or evaluate conversion of audio into sequences of discrete symbols from a finite vocabulary (no mention of quantization, token vocabularies, or token-based modeling). Because the inclusion criteria require explicit treatment of discrete audio/speech tokenization as a core subject, and the abstract only describes continuous compressed latents, this study should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged ambiguity: JuniorNano concluded the work does not explicitly address discrete tokenization and recommended exclusion; JuniorMini noted possible latent representations but the abstract lacks clarity on discrete token design. The paper describes a neural audio codec and encoding to a “low-dimensional compressed latent vector” and uses latent diffusion for generation, but it does not explicitly propose, analyze, or evaluate conversion of audio into sequences of discrete symbols from a finite vocabulary (no mention of quantization, token vocabularies, or token-based modeling). Because the inclusion criteria require explicit treatment of discrete audio/speech tokenization as a core subject, and the abstract only describes continuous compressed latents, this study should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "EEG-Based Music Emotion Prediction Using Supervised Feature Extraction for MIDI Generation",
    "abstract": "Advancements in music emotion prediction are driving AI-driven algorithmic composition, enabling the generation of complex melodies. However, bridging neural and auditory domains remains challenging due to the semantic gap between brain-derived low-level features and high-level musical concepts, making alignment computationally demanding. This study proposes a deep learning framework for generating MIDI sequences aligned with labeled emotion predictions through supervised feature extraction from neural and auditory domains. EEGNet is employed to process neural data, while an autoencoder-based piano algorithm handles auditory data. To address modality heterogeneity, Centered Kernel Alignment is incorporated to enhance the separation of emotional states. Furthermore, regression between feature domains is applied to reduce intra-subject variability in extracted Electroencephalography (EEG) patterns, followed by the clustering of latent auditory representations into denser partitions to improve MIDI reconstruction quality. Using musical metrics, evaluation on real-world data shows that the proposed approach improves emotion classification (namely, between arousal and valence) and the system’s ability to produce MIDI sequences that better preserve temporal alignment, tonal consistency, and structural integrity. Subject-specific analysis reveals that subjects with stronger imagery paradigms produced higher-quality MIDI outputs, as their neural patterns aligned more closely with the training data. In contrast, subjects with weaker performance exhibited auditory data that were less consistent.",
    "metadata": {
      "title": "EEG-Based Music Emotion Prediction Using Supervised Feature Extraction for MIDI Generation",
      "summary": "Advancements in music emotion prediction are driving AI-driven algorithmic composition, enabling the generation of complex melodies. However, bridging neural and auditory domains remains challenging due to the semantic gap between brain-derived low-level features and high-level musical concepts, making alignment computationally demanding. This study proposes a deep learning framework for generating MIDI sequences aligned with labeled emotion predictions through supervised feature extraction from neural and auditory domains. EEGNet is employed to process neural data, while an autoencoder-based piano algorithm handles auditory data. To address modality heterogeneity, Centered Kernel Alignment is incorporated to enhance the separation of emotional states. Furthermore, regression between feature domains is applied to reduce intra-subject variability in extracted Electroencephalography (EEG) patterns, followed by the clustering of latent auditory representations into denser partitions to improve MIDI reconstruction quality. Using musical metrics, evaluation on real-world data shows that the proposed approach improves emotion classification (namely, between arousal and valence) and the system’s ability to produce MIDI sequences that better preserve temporal alignment, tonal consistency, and structural integrity. Subject-specific analysis reveals that subjects with stronger imagery paradigms produced higher-quality MIDI outputs, as their neural patterns aligned more closely with the training data. In contrast, subjects with weaker performance exhibited auditory data that were less consistent.",
      "abstract": "Advancements in music emotion prediction are driving AI-driven algorithmic composition, enabling the generation of complex melodies. However, bridging neural and auditory domains remains challenging due to the semantic gap between brain-derived low-level features and high-level musical concepts, making alignment computationally demanding. This study proposes a deep learning framework for generating MIDI sequences aligned with labeled emotion predictions through supervised feature extraction from neural and auditory domains. EEGNet is employed to process neural data, while an autoencoder-based piano algorithm handles auditory data. To address modality heterogeneity, Centered Kernel Alignment is incorporated to enhance the separation of emotional states. Furthermore, regression between feature domains is applied to reduce intra-subject variability in extracted Electroencephalography (EEG) patterns, followed by the clustering of latent auditory representations into denser partitions to improve MIDI reconstruction quality. Using musical metrics, evaluation on real-world data shows that the proposed approach improves emotion classification (namely, between arousal and valence) and the system’s ability to produce MIDI sequences that better preserve temporal alignment, tonal consistency, and structural integrity. Subject-specific analysis reveals that subjects with stronger imagery paradigms produced higher-quality MIDI outputs, as their neural patterns aligned more closely with the training data. In contrast, subjects with weaker performance exhibited auditory data that were less consistent.",
      "doi": "https://doi.org/10.3390/s25051471",
      "openalex_id": "https://openalex.org/W4408039472",
      "arxiv_id": "",
      "publication_date": "2025-02-27",
      "published": "2025-02-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on predicting emotions from EEG and generating MIDI sequences; it does not explicitly propose or analyze discrete audio tokenization as its core object, even though MIDI is a discrete sequence, tokenization of audio into a finite vocabulary is not discussed.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work centers on predicting emotions from EEG and generating MIDI sequences; it does not explicitly propose or analyze discrete audio tokenization as its core object, even though MIDI is a discrete sequence, tokenization of audio into a finite vocabulary is not discussed.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on emotion prediction from EEG data and generating MIDI sequences of music based on aligned neural and auditory features; however, it does not explicitly discuss or propose discrete audio/speech tokenization where continuous audio signals are converted into sequences of discrete tokens for audio reconstruction or understanding, which is the core inclusion criterion. The approach involves deep learning, feature extraction, and clustering in latent auditory representations but lacks mention of discrete token design or quantization methods related to audio or speech tokens. Therefore, it does not meet the inclusion criteria focused on discrete tokenization nor does it clearly fall under exclusion criteria related to continuous features or non-discrete token-based methods, leading to uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on emotion prediction from EEG data and generating MIDI sequences of music based on aligned neural and auditory features; however, it does not explicitly discuss or propose discrete audio/speech tokenization where continuous audio signals are converted into sequences of discrete tokens for audio reconstruction or understanding, which is the core inclusion criterion. The approach involves deep learning, feature extraction, and clustering in latent auditory representations but lacks mention of discrete token design or quantization methods related to audio or speech tokens. Therefore, it does not meet the inclusion criteria focused on discrete tokenization nor does it clearly fall under exclusion criteria related to continuous features or non-discrete token-based methods, leading to uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the manuscript focuses on EEG-driven emotion prediction and MIDI generation and does not explicitly propose or analyze discrete audio/speech tokenization or quantization schemes; although it clusters latent auditory representations (a hint of discretization) the paper treats MIDI/music generation and neural–auditory alignment rather than designing or evaluating a finite-vocabulary tokenization of continuous audio nor addressing speech relevance. Therefore it fails the inclusion requirement that discrete audio/speech tokenization be a core object and also falls under the exclusion of music-only work without speech-applicable discrete token methods. Brief justification: the study lacks an explicit discrete-token/quantization methodology for audio/speech and is music-focused, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the manuscript focuses on EEG-driven emotion prediction and MIDI generation and does not explicitly propose or analyze discrete audio/speech tokenization or quantization schemes; although it clusters latent auditory representations (a hint of discretization) the paper treats MIDI/music generation and neural–auditory alignment rather than designing or evaluating a finite-vocabulary tokenization of continuous audio nor addressing speech relevance. Therefore it fails the inclusion requirement that discrete audio/speech tokenization be a core object and also falls under the exclusion of music-only work without speech-applicable discrete token methods. Brief justification: the study lacks an explicit discrete-token/quantization methodology for audio/speech and is music-focused, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation",
    "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
    "metadata": {
      "title": "Multi-View Midivae: Fusing Track- and Bar-View Representations for Long Multi-Track Symbolic Music Generation",
      "summary": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "abstract": "Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448249",
      "openalex_id": "https://openalex.org/W4392902987",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses symbolic MIDI sequence generation with VAEs and discrete MIDI events, but it does not explicitly study discrete audio/speech tokenization of real audio signals into a finite vocabulary, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper addresses symbolic MIDI sequence generation with VAEs and discrete MIDI events, but it does not explicitly study discrete audio/speech tokenization of real audio signals into a finite vocabulary, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating long multi-track symbolic music using variational autoencoders and does not address discrete audio/speech tokenization, nor does it analyze or propose tokenization related to speech or audio reconstruction; also, it deals solely with music as a data modality without applying to speech tasks, thus it does not meet the inclusion criteria and falls within the exclusion criteria regarding tokenization methods for non-speech audio only without speech relevance.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating long multi-track symbolic music using variational autoencoders and does not address discrete audio/speech tokenization, nor does it analyze or propose tokenization related to speech or audio reconstruction; also, it deals solely with music as a data modality without applying to speech tasks, thus it does not meet the inclusion criteria and falls within the exclusion criteria regarding tokenization methods for non-speech audio only without speech relevance.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generating Stereophonic Music with Single-Stage Language Models",
    "abstract": "The recent success of audio language models (LMs) has revolutionized the field of neural music generation. Among all audio LM approaches, MusicGen has demonstrated the success of a single-stage LMs based music generation framework, without needing to train multiple LMs. Despite its promising performance in generating monophonic (mono) music, directly generating stereophonic (stereo) music following the previous framework has resulted in perceptible quality degradation. In this paper, we first discuss the difficulty of directly encoding stereo music with neural codec, and then provide a stable and practical solution based on a dual encoding approach. To utilize the dually encoded tokens in single-stage LMs, we also propose two forms of token sequence patterns. An extensive evaluation has been conducted using various aspects of stereo music audios to examine the performance of stereo neural codec approaches and the generation quality of single-stage LMs. Finally, our experimental results suggest that (i) our proposed dual encoding approach for neural codec is significantly better than the typical joint encoding approach in terms of reconstruction quality, and (ii) the stereo single-stage LMs trained with our proposed token sequence patterns substantially improved the perceptual quality of the state-of-the-art music generation model (i.e. MusicGen) in subjective tests.",
    "metadata": {
      "title": "Generating Stereophonic Music with Single-Stage Language Models",
      "summary": "The recent success of audio language models (LMs) has revolutionized the field of neural music generation. Among all audio LM approaches, MusicGen has demonstrated the success of a single-stage LMs based music generation framework, without needing to train multiple LMs. Despite its promising performance in generating monophonic (mono) music, directly generating stereophonic (stereo) music following the previous framework has resulted in perceptible quality degradation. In this paper, we first discuss the difficulty of directly encoding stereo music with neural codec, and then provide a stable and practical solution based on a dual encoding approach. To utilize the dually encoded tokens in single-stage LMs, we also propose two forms of token sequence patterns. An extensive evaluation has been conducted using various aspects of stereo music audios to examine the performance of stereo neural codec approaches and the generation quality of single-stage LMs. Finally, our experimental results suggest that (i) our proposed dual encoding approach for neural codec is significantly better than the typical joint encoding approach in terms of reconstruction quality, and (ii) the stereo single-stage LMs trained with our proposed token sequence patterns substantially improved the perceptual quality of the state-of-the-art music generation model (i.e. MusicGen) in subjective tests.",
      "abstract": "The recent success of audio language models (LMs) has revolutionized the field of neural music generation. Among all audio LM approaches, MusicGen has demonstrated the success of a single-stage LMs based music generation framework, without needing to train multiple LMs. Despite its promising performance in generating monophonic (mono) music, directly generating stereophonic (stereo) music following the previous framework has resulted in perceptible quality degradation. In this paper, we first discuss the difficulty of directly encoding stereo music with neural codec, and then provide a stable and practical solution based on a dual encoding approach. To utilize the dually encoded tokens in single-stage LMs, we also propose two forms of token sequence patterns. An extensive evaluation has been conducted using various aspects of stereo music audios to examine the performance of stereo neural codec approaches and the generation quality of single-stage LMs. Finally, our experimental results suggest that (i) our proposed dual encoding approach for neural codec is significantly better than the typical joint encoding approach in terms of reconstruction quality, and (ii) the stereo single-stage LMs trained with our proposed token sequence patterns substantially improved the perceptual quality of the state-of-the-art music generation model (i.e. MusicGen) in subjective tests.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446643",
      "openalex_id": "https://openalex.org/W4392902968",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on discrete token representations (dual encoding of tokens and token sequence patterns) used by single-stage language models to generate stereo music, directly addressing discrete audio tokenization as a core object of study, with English abstract and empirical evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study centers on discrete token representations (dual encoding of tokens and token sequence patterns) used by single-stage language models to generate stereo music, directly addressing discrete audio tokenization as a core object of study, with English abstract and empirical evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly addresses discrete audio tokenization through a dual encoding approach for neural codecs to generate stereophonic music, focusing on converting continuous audio into sequences of discrete tokens for music generation and reconstruction tasks; it avoids exclusion issues such as exclusive reliance on continuous features or traditional codecs and includes experimental validation of discrete tokenization methods, though the scope is music-specific with implications for audio tokenization research generally.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly addresses discrete audio tokenization through a dual encoding approach for neural codecs to generate stereophonic music, focusing on converting continuous audio into sequences of discrete tokens for music generation and reconstruction tasks; it avoids exclusion issues such as exclusive reliance on continuous features or traditional codecs and includes experimental validation of discrete tokenization methods, though the scope is music-specific with implications for audio tokenization research generally.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms",
    "abstract": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
    "metadata": {
      "title": "Exploring Music Style Transfer and Innovative Composition using Deep Learning Algorithms",
      "summary": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "abstract": "Automatic music generation represents a challenging task within the field of artificial intelligence, aiming to harness machine learning techniques to compose music that is appreciable by humans. In this context, we introduce a text-based music data representation method that bridges the gap for the application of large text-generation models in music creation. Addressing the characteristics of music such as smaller note dimensionality and longer length, we employed a deep generative adversarial network model based on music measures (MT-CHSE-GAN). This model integrates paragraph text generation methods, improves the quality and efficiency of music melody generation through measure-wise processing and channel attention mechanisms. The MT-CHSE-GAN model provides a novel framework for music data processing and generation, offering an effective solution to the problem of long-sequence music generation. To comprehensively evaluate the quality of the generated music, we used accuracy, loss rate, and music theory knowledge as evaluation metrics and compared our model with other music generation models. Experimental results demonstrate our method's significant advantages in music generation quality. Despite progress in the field of automatic music generation, its application still faces challenges, particularly in terms of quantitative evaluation metrics and the breadth of model applications. Future research will continue to explore expanding the model's application scope, enriching evaluation methods, and further improving the quality and expressiveness of the generated music. This study not only advances the development of music generation technology but also provides valuable experience and insights for research in related fields.",
      "doi": "https://doi.org/10.14569/ijacsa.2024.01505101",
      "openalex_id": "https://openalex.org/W4399260034",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on music generation using a text-based representation and a MT-CHSE-GAN framework; there is no explicit analysis or proposal of discrete audio tokenization into a finite vocabulary for audio reconstruction or generation, so it does not satisfy the inclusion criteria for discrete audio tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article focuses on music generation using a text-based representation and a MT-CHSE-GAN framework; there is no explicit analysis or proposal of discrete audio tokenization into a finite vocabulary for audio reconstruction or generation, so it does not satisfy the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on music generation using a deep learning model and discusses a text-based music data representation and measure-wise processing, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods for audio reconstruction or generation; it primarily deals with music data without addressing speech tasks or discrete audio tokenization relevant to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on music generation using a deep learning model and discusses a text-based music data representation and measure-wise processing, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods for audio reconstruction or generation; it primarily deals with music data without addressing speech tasks or discrete audio tokenization relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "VoiceFlow: Efficient Text-To-Speech with Rectified Flow Matching",
    "abstract": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
    "metadata": {
      "title": "VoiceFlow: Efficient Text-To-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "abstract": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445948",
      "openalex_id": "https://openalex.org/W4392904491",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on continuous mel-spectrogram-based TTS modeling and diffusion/flow methods, with no analysis or use of discrete audio tokens, thus failing the discrete-tokenization inclusion criterion and satisfying the exclusion criterion for continuous features only.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on continuous mel-spectrogram-based TTS modeling and diffusion/flow methods, with no analysis or use of discrete audio tokens, thus failing the discrete-tokenization inclusion criterion and satisfying the exclusion criterion for continuous features only.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel acoustic model (VoiceFlow) for generating mel-spectrograms using rectified flow matching, which involves continuous audio features rather than discrete audio tokenization or quantization into finite symbol sequences; hence, it does not explicitly propose or analyze discrete audio tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel acoustic model (VoiceFlow) for generating mel-spectrograms using rectified flow matching, which involves continuous audio features rather than discrete audio tokenization or quantization into finite symbol sequences; hence, it does not explicitly propose or analyze discrete audio tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generation-Based Target Speech Extraction with Speech Discretization and Vocoder",
    "abstract": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
    "metadata": {
      "title": "Generation-Based Target Speech Extraction with Speech Discretization and Vocoder",
      "summary": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "abstract": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446418",
      "openalex_id": "https://openalex.org/W4392903977",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete tokens via speech discretization and a vocoder as core components for generation-based target speech extraction, and evaluates this discrete-token approach on standard speech datasets, meeting the inclusion criteria and not fitting the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete tokens via speech discretization and a vocoder as core components for generation-based target speech extraction, and evaluates this discrete-token approach on standard speech datasets, meeting the inclusion criteria and not fitting the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a discrete token based target speech extraction method, combining speech discretization and vocoder techniques to convert continuous speech signals into discrete tokens for re-synthesis, directly aligning with the inclusion criterion of analyzing discrete audio tokenization for speech applications; it also includes experimental validation on benchmark datasets and is presented in English, with no exclusion criteria triggered such as traditional codecs or unrelated tokenization domains.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a discrete token based target speech extraction method, combining speech discretization and vocoder techniques to convert continuous speech signals into discrete tokens for re-synthesis, directly aligning with the inclusion criterion of analyzing discrete audio tokenization for speech applications; it also includes experimental validation on benchmark datasets and is presented in English, with no exclusion criteria triggered such as traditional codecs or unrelated tokenization domains.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Survey on Speech Deepfake Detection",
    "abstract": "The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defenses against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.",
    "metadata": {
      "title": "A Survey on Speech Deepfake Detection",
      "summary": "The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defenses against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.",
      "abstract": "The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defenses against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.",
      "doi": "https://doi.org/10.1145/3714458",
      "openalex_id": "https://openalex.org/W4406803348",
      "arxiv_id": "",
      "publication_date": "2025-01-24",
      "published": "2025-01-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers a survey of speech deepfake detection techniques and datasets but does not discuss discrete audio tokenization or quantized token representations; it relies on conventional speech features, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers a survey of speech deepfake detection techniques and datasets but does not discuss discrete audio tokenization or quantized token representations; it relies on conventional speech features, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The survey covers speech Deepfake detection techniques comprehensively but does not mention discrete audio/speech tokenization as a core object of study or analyze any discrete tokenization methods; rather, it appears focused on detection pipelines, architectures, and general evaluation, without explicit treatment of discrete token representations, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The survey covers speech Deepfake detection techniques comprehensively but does not mention discrete audio/speech tokenization as a core object of study or analyze any discrete tokenization methods; rather, it appears focused on detection pipelines, architectures, and general evaluation, without explicit treatment of discrete token representations, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
    "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
    "metadata": {
      "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
      "summary": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.704",
      "openalex_id": "https://openalex.org/W4389519009",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets non-autoregressive SLU and RL-based reference modification for intent/slot tasks and does not address discrete audio tokenization into a finite vocabulary, so it does not meet the discrete tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets non-autoregressive SLU and RL-based reference modification for intent/slot tasks and does not address discrete audio tokenization into a finite vocabulary, so it does not meet the discrete tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a reinforcement learning method to improve joint multiple intent detection and slot filling in spoken language understanding, but it does not address or analyze discrete audio/speech tokenization where continuous audio signals are converted into discrete tokens for reconstruction or generation; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a reinforcement learning method to improve joint multiple intent detection and slot filling in spoken language understanding, but it does not address or analyze discrete audio/speech tokenization where continuous audio signals are converted into discrete tokens for reconstruction or generation; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
    "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
    "metadata": {
      "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
      "summary": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "doi": "https://doi.org/10.1109/ic2pct60090.2024.10486323",
      "openalex_id": "https://openalex.org/W4394583388",
      "arxiv_id": "",
      "publication_date": "2024-02-09",
      "published": "2024-02-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on OCR for handwritten Telugu using tools like pytesseract, with no discrete audio tokenization or speech-related analysis, thus not meeting the audio/tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on OCR for handwritten Telugu using tools like pytesseract, with no discrete audio tokenization or speech-related analysis, thus not meeting the audio/tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses entirely on optical character recognition for handwritten Telugu scripts and does not address audio/speech tokenization, discrete audio tokens, or any related analysis required by the inclusion criteria, making it irrelevant to the task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses entirely on optical character recognition for handwritten Telugu scripts and does not address audio/speech tokenization, discrete audio tokens, or any related analysis required by the inclusion criteria, making it irrelevant to the task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting",
    "abstract": "We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.",
    "metadata": {
      "title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting",
      "summary": "We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.",
      "abstract": "We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.",
      "doi": "https://doi.org/10.1109/taslp.2024.3463395",
      "openalex_id": "https://openalex.org/W4402592728",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on Bayesian parameter-efficient fine-tuning to prevent catastrophic forgetting in language/speech models and does not analyze or propose discrete audio tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on Bayesian parameter-efficient fine-tuning to prevent catastrophic forgetting in language/speech models and does not analyze or propose discrete audio tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Bayesian parameter-efficient fine-tuning to prevent catastrophic forgetting in text-to-speech synthesis models, but it does not address or analyze discrete audio/speech tokenization; rather, its emphasis is on fine-tuning methods and preserving pre-training knowledge without involving discrete tokenization approaches for audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Bayesian parameter-efficient fine-tuning to prevent catastrophic forgetting in text-to-speech synthesis models, but it does not address or analyze discrete audio/speech tokenization; rather, its emphasis is on fine-tuning methods and preserving pre-training knowledge without involving discrete tokenization approaches for audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
    "abstract": "Self-supervised learning (SSL) has achieved great success in speech-related tasks, driven by advancements in speech encoder architectures and the expansion of datasets. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, with a focus on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech and Libri-Light demonstrate that Zipformer-based SSL systems significantly outperform comparable HuBERT and WavLM systems, achieving a relative WER reduction on dev-other/test-other of up to 34.8%/32.4% compared to HuBERT Base after supervised fine-tuning, along with a 3.5x pre-training speedup in total GPU hours.",
    "metadata": {
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks, driven by advancements in speech encoder architectures and the expansion of datasets. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, with a focus on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech and Libri-Light demonstrate that Zipformer-based SSL systems significantly outperform comparable HuBERT and WavLM systems, achieving a relative WER reduction on dev-other/test-other of up to 34.8%/32.4% compared to HuBERT Base after supervised fine-tuning, along with a 3.5x pre-training speedup in total GPU hours.",
      "abstract": "Self-supervised learning (SSL) has achieved great success in speech-related tasks, driven by advancements in speech encoder architectures and the expansion of datasets. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, with a focus on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech and Libri-Light demonstrate that Zipformer-based SSL systems significantly outperform comparable HuBERT and WavLM systems, achieving a relative WER reduction on dev-other/test-other of up to 34.8%/32.4% compared to HuBERT Base after supervised fine-tuning, along with a 3.5x pre-training speedup in total GPU hours.",
      "doi": "https://doi.org/10.32388/2c9tpu",
      "openalex_id": "https://openalex.org/W4404938578",
      "arxiv_id": "",
      "publication_date": "2024-12-01",
      "published": "2024-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a faster framework for self-supervised speech representation learning and encoder architectures but does not explicitly address discrete audio tokenization or discrete audio token representations as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a faster framework for self-supervised speech representation learning and encoder architectures but does not explicitly address discrete audio tokenization or discrete audio token representations as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised speech representation learning framework focusing on architectures and training efficiency for ASR, but it does not explicitly discuss discrete audio/speech tokenization where continuous audio signals are converted into discrete symbol sequences as a core contribution; thus, it does not meet the inclusion criteria related to discrete tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised speech representation learning framework focusing on architectures and training efficiency for ASR, but it does not explicitly discuss discrete audio/speech tokenization where continuous audio signals are converted into discrete symbol sequences as a core contribution; thus, it does not meet the inclusion criteria related to discrete tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Foundation Models for Low-Resource Language Education (Vision Paper)",
    "abstract": "Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.",
    "metadata": {
      "title": "Foundation Models for Low-Resource Language Education (Vision Paper)",
      "summary": "Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.",
      "abstract": "Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.",
      "doi": "https://doi.org/10.32388/iqu339",
      "openalex_id": "https://openalex.org/W4406247986",
      "arxiv_id": "",
      "publication_date": "2025-01-10",
      "published": "2025-01-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on foundation models for low-resource language education and multilingual NLP, with no discussion of discrete audio/speech tokenization as a core object of study, so it fails to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on foundation models for low-resource language education and multilingual NLP, with no discussion of discrete audio/speech tokenization as a core object of study, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the application of large language models for low-resource language education and does not discuss discrete audio/speech tokenization, neural encoding, or quantization-based audio token representations as a core subject; thus, it fails to meet the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the application of large language models for low-resource language education and does not discuss discrete audio/speech tokenization, neural encoding, or quantization-based audio token representations as a core subject; thus, it fails to meet the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Survey on Data Selection for Efficient Speech Processing",
    "abstract": "While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.",
    "metadata": {
      "title": "A Survey on Data Selection for Efficient Speech Processing",
      "summary": "While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.",
      "abstract": "While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.",
      "doi": "https://doi.org/10.1109/access.2025.3582395",
      "openalex_id": "https://openalex.org/W4411550778",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses data selection methods for efficient speech processing and a taxonomy of selection strategies, but it does not propose, analyze, or evaluate discrete audio/speech tokenization (quantized tokens) as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses data selection methods for efficient speech processing and a taxonomy of selection strategies, but it does not propose, analyze, or evaluate discrete audio/speech tokenization (quantized tokens) as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study is a survey on data selection methods for efficient speech processing and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as defined by the criteria (i.e., conversion of continuous audio into discrete tokens for reconstruction or generation). It focuses on data selection strategies rather than discrete tokenization, thus does not meet the inclusion criteria and aligns with exclusion criteria related to lack of discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study is a survey on data selection methods for efficient speech processing and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as defined by the criteria (i.e., conversion of continuous audio into discrete tokens for reconstruction or generation). It focuses on data selection strategies rather than discrete tokenization, thus does not meet the inclusion criteria and aligns with exclusion criteria related to lack of discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison",
    "abstract": "Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/WLASL/.",
    "metadata": {
      "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison",
      "summary": "Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/WLASL/.",
      "abstract": "Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/WLASL/.",
      "doi": "https://doi.org/10.1109/wacv45572.2020.9093512",
      "openalex_id": "https://openalex.org/W3009828227",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on video-based sign language recognition and a large-scale word-level ASL dataset, with no discrete audio tokenization or analysis of discrete audio tokens, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on video-based sign language recognition and a large-scale word-level ASL dataset, with no discrete audio tokenization or analysis of discrete audio tokens, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition from video data and proposes a new large-scale dataset and deep learning methods for visual recognition of sign language at the word level. There is no mention or analysis of discrete audio/speech tokenization or discrete audio representations, nor are audio signals or speech tokenization a core object of study. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition from video data and proposes a new large-scale dataset and deep learning methods for visual recognition of sign language at the word level. There is no mention or analysis of discrete audio/speech tokenization or discrete audio representations, nor are audio signals or speech tokenization a core object of study. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish",
    "abstract": "Considerable evidence now shows that all languages, signed and spoken, exhibit a significant amount of iconicity. We examined how the visual-gestural modality of signed languages facilitates iconicity for different kinds of lexical meanings compared to the auditory-vocal modality of spoken languages. We used iconicity ratings of hundreds of signs and words to compare iconicity across the vocabularies of two signed languages - American Sign Language and British Sign Language, and two spoken languages - English and Spanish. We examined (1) the correlation in iconicity ratings between the languages; (2) the relationship between iconicity and an array of semantic variables (ratings of concreteness, sensory experience, imageability, perceptual strength of vision, audition, touch, smell and taste); (3) how iconicity varies between broad lexical classes (nouns, verbs, adjectives, grammatical words and adverbs); and (4) between more specific semantic categories (e.g., manual actions, clothes, colors). The results show several notable patterns that characterize how iconicity is spread across the four vocabularies. There were significant correlations in the iconicity ratings between the four languages, including English with ASL, BSL, and Spanish. The highest correlation was between ASL and BSL, suggesting iconicity may be more transparent in signs than words. In each language, iconicity was distributed according to the semantic variables in ways that reflect the semiotic affordances of the modality (e.g., more concrete meanings more iconic in signs, not words; more auditory meanings more iconic in words, not signs; more tactile meanings more iconic in both signs and words). Analysis of the 220 meanings with ratings in all four languages further showed characteristic patterns of iconicity across broad and specific semantic domains, including those that distinguished between signed and spoken languages (e.g., verbs more iconic in ASL, BSL, and English, but not Spanish; manual actions especially iconic in ASL and BSL; adjectives more iconic in English and Spanish; color words especially low in iconicity in ASL and BSL). These findings provide the first quantitative account of how iconicity is spread across the lexicons of signed languages in comparison to spoken languages.",
    "metadata": {
      "title": "Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish",
      "summary": "Considerable evidence now shows that all languages, signed and spoken, exhibit a significant amount of iconicity. We examined how the visual-gestural modality of signed languages facilitates iconicity for different kinds of lexical meanings compared to the auditory-vocal modality of spoken languages. We used iconicity ratings of hundreds of signs and words to compare iconicity across the vocabularies of two signed languages - American Sign Language and British Sign Language, and two spoken languages - English and Spanish. We examined (1) the correlation in iconicity ratings between the languages; (2) the relationship between iconicity and an array of semantic variables (ratings of concreteness, sensory experience, imageability, perceptual strength of vision, audition, touch, smell and taste); (3) how iconicity varies between broad lexical classes (nouns, verbs, adjectives, grammatical words and adverbs); and (4) between more specific semantic categories (e.g., manual actions, clothes, colors). The results show several notable patterns that characterize how iconicity is spread across the four vocabularies. There were significant correlations in the iconicity ratings between the four languages, including English with ASL, BSL, and Spanish. The highest correlation was between ASL and BSL, suggesting iconicity may be more transparent in signs than words. In each language, iconicity was distributed according to the semantic variables in ways that reflect the semiotic affordances of the modality (e.g., more concrete meanings more iconic in signs, not words; more auditory meanings more iconic in words, not signs; more tactile meanings more iconic in both signs and words). Analysis of the 220 meanings with ratings in all four languages further showed characteristic patterns of iconicity across broad and specific semantic domains, including those that distinguished between signed and spoken languages (e.g., verbs more iconic in ASL, BSL, and English, but not Spanish; manual actions especially iconic in ASL and BSL; adjectives more iconic in English and Spanish; color words especially low in iconicity in ASL and BSL). These findings provide the first quantitative account of how iconicity is spread across the lexicons of signed languages in comparison to spoken languages.",
      "abstract": "Considerable evidence now shows that all languages, signed and spoken, exhibit a significant amount of iconicity. We examined how the visual-gestural modality of signed languages facilitates iconicity for different kinds of lexical meanings compared to the auditory-vocal modality of spoken languages. We used iconicity ratings of hundreds of signs and words to compare iconicity across the vocabularies of two signed languages - American Sign Language and British Sign Language, and two spoken languages - English and Spanish. We examined (1) the correlation in iconicity ratings between the languages; (2) the relationship between iconicity and an array of semantic variables (ratings of concreteness, sensory experience, imageability, perceptual strength of vision, audition, touch, smell and taste); (3) how iconicity varies between broad lexical classes (nouns, verbs, adjectives, grammatical words and adverbs); and (4) between more specific semantic categories (e.g., manual actions, clothes, colors). The results show several notable patterns that characterize how iconicity is spread across the four vocabularies. There were significant correlations in the iconicity ratings between the four languages, including English with ASL, BSL, and Spanish. The highest correlation was between ASL and BSL, suggesting iconicity may be more transparent in signs than words. In each language, iconicity was distributed according to the semantic variables in ways that reflect the semiotic affordances of the modality (e.g., more concrete meanings more iconic in signs, not words; more auditory meanings more iconic in words, not signs; more tactile meanings more iconic in both signs and words). Analysis of the 220 meanings with ratings in all four languages further showed characteristic patterns of iconicity across broad and specific semantic domains, including those that distinguished between signed and spoken languages (e.g., verbs more iconic in ASL, BSL, and English, but not Spanish; manual actions especially iconic in ASL and BSL; adjectives more iconic in English and Spanish; color words especially low in iconicity in ASL and BSL). These findings provide the first quantitative account of how iconicity is spread across the lexicons of signed languages in comparison to spoken languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2018.01433",
      "openalex_id": "https://openalex.org/W2885539092",
      "arxiv_id": "",
      "publication_date": "2018-08-14",
      "published": "2018-08-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article studies iconicity across languages using ratings and comparisons between sign and spoken vocabularies, but it does not propose, analyze, or evaluate discrete audio tokenization into a finite set of tokens for reconstruction/generation/understanding tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article studies iconicity across languages using ratings and comparisons between sign and spoken vocabularies, but it does not propose, analyze, or evaluate discrete audio tokenization into a finite set of tokens for reconstruction/generation/understanding tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on iconicity comparison across signed and spoken vocabularies and analyzes semantic correlations rather than proposing or evaluating discrete audio/speech tokenization methods; it lacks discussion of converting continuous audio signals into discrete token sequences relevant to audio reconstruction, generation, or understanding tasks, and therefore does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on iconicity comparison across signed and spoken vocabularies and analyzes semantic correlations rather than proposing or evaluating discrete audio/speech tokenization methods; it lacks discussion of converting continuous audio signals into discrete token sequences relevant to audio reconstruction, generation, or understanding tasks, and therefore does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language",
    "abstract": "Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.",
    "metadata": {
      "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language",
      "summary": "Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.",
      "abstract": "Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.",
      "doi": "https://doi.org/10.48550/arxiv.1812.01053",
      "openalex_id": "https://openalex.org/W2903314716",
      "arxiv_id": "",
      "publication_date": "2018-12-03",
      "published": "2018-12-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a large-scale sign language video dataset and recognition benchmarks, not on discrete audio tokenization or token-based representations for speech, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a large-scale sign language video dataset and recognition benchmarks, not on discrete audio tokenization or token-based representations for speech, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a large-scale data set and benchmark for American Sign Language recognition using video data, without any mention or analysis of discrete audio or speech tokenization; it involves visual modalities rather than audio and thus does not meet the inclusion criteria related to audio discrete tokenization nor addresses any exclusion criteria since audio discrete tokenization is absent altogether.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a large-scale data set and benchmark for American Sign Language recognition using video data, without any mention or analysis of discrete audio or speech tokenization; it involves visual modalities rather than audio and thus does not meet the inclusion criteria related to audio discrete tokenization nor addresses any exclusion criteria since audio discrete tokenization is absent altogether.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Construals of iconicity: experimental approaches to form–meaning resemblances in language",
    "abstract": "abstract While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.",
    "metadata": {
      "title": "Construals of iconicity: experimental approaches to form–meaning resemblances in language",
      "summary": "abstract While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.",
      "abstract": "abstract While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.",
      "doi": "https://doi.org/10.1017/langcog.2019.48",
      "openalex_id": "https://openalex.org/W2988385843",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article surveys experimental work on iconicity in language and discusses typologies and approaches to form–meaning resemblances, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, thus not meeting inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article surveys experimental work on iconicity in language and discusses typologies and approaches to form–meaning resemblances, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, thus not meeting inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on experimental approaches to iconicity and form-meaning resemblances in language rather than on discrete audio or speech tokenization methods, quantization, or analysis of audio token sequences; therefore, they do not address the core inclusion criteria centered on discrete audio tokenization or representation for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on experimental approaches to iconicity and form-meaning resemblances in language rather than on discrete audio or speech tokenization methods, quantization, or analysis of audio token sequences; therefore, they do not address the core inclusion criteria centered on discrete audio tokenization or representation for speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Road to Language Learning Is Not Entirely Iconic: Iconicity, Neighborhood Density, and Frequency Facilitate Acquisition of Sign Language",
    "abstract": "Iconic mappings between words and their meanings are far more prevalent than once estimated and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows two other factors known to support the acquisition of spoken vocabulary: neighborhood density (the number of lexical items phonologically similar to the target) and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s productive acquisition of 332 signs in American Sign Language (ASL; Anderson &amp; Reilly, 2002) and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure between signed and spoken language, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage this information to expand their vocabulary.",
    "metadata": {
      "title": "The Road to Language Learning Is Not Entirely Iconic: Iconicity, Neighborhood Density, and Frequency Facilitate Acquisition of Sign Language",
      "summary": "Iconic mappings between words and their meanings are far more prevalent than once estimated and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows two other factors known to support the acquisition of spoken vocabulary: neighborhood density (the number of lexical items phonologically similar to the target) and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s productive acquisition of 332 signs in American Sign Language (ASL; Anderson &amp; Reilly, 2002) and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure between signed and spoken language, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage this information to expand their vocabulary.",
      "abstract": "Iconic mappings between words and their meanings are far more prevalent than once estimated and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows two other factors known to support the acquisition of spoken vocabulary: neighborhood density (the number of lexical items phonologically similar to the target) and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s productive acquisition of 332 signs in American Sign Language (ASL; Anderson &amp; Reilly, 2002) and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure between signed and spoken language, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage this information to expand their vocabulary.",
      "doi": "https://doi.org/10.1177/0956797617700498",
      "openalex_id": "https://openalex.org/W2619923810",
      "arxiv_id": "",
      "publication_date": "2017-05-30",
      "published": "2017-05-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes iconicity, neighborhood density, and frequency in ASL acquisition but does not address discrete audio tokenization or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes iconicity, neighborhood density, and frequency in ASL acquisition but does not address discrete audio tokenization or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language acquisition factors such as iconicity, neighborhood density, and lexical frequency, with no mention or analysis of discrete audio/speech tokenization or quantization of continuous audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization and is not relevant to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language acquisition factors such as iconicity, neighborhood density, and lexical frequency, with no mention or analysis of discrete audio/speech tokenization or quantization of continuous audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization and is not relevant to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The ASL-LEX 2.0 Project: A Database of Lexical and Phonological Properties for 2,723 Signs in American Sign Language",
    "abstract": "Abstract ASL-LEX is a publicly available, large-scale lexical database for American Sign Language (ASL). We report on the expanded database (ASL-LEX 2.0) that contains 2,723 ASL signs. For each sign, ASL-LEX now includes a more detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (“guessability”) ratings (from non-signers), sign and videoclip durations, lexical class, and more. We document the steps used to create ASL-LEX 2.0 and describe the distributional characteristics for sign properties across the lexicon and examine the relationships among lexical and phonological properties of signs. Correlation analyses revealed that frequent signs were less iconic and phonologically simpler than infrequent signs and iconic signs tended to be phonologically simpler than less iconic signs. The complete ASL-LEX dataset and supplementary materials are available at https://osf.io/zpha4/ and an interactive visualization of the entire lexicon can be accessed on the ASL-LEX page: http://asl-lex.org/.",
    "metadata": {
      "title": "The ASL-LEX 2.0 Project: A Database of Lexical and Phonological Properties for 2,723 Signs in American Sign Language",
      "summary": "Abstract ASL-LEX is a publicly available, large-scale lexical database for American Sign Language (ASL). We report on the expanded database (ASL-LEX 2.0) that contains 2,723 ASL signs. For each sign, ASL-LEX now includes a more detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (“guessability”) ratings (from non-signers), sign and videoclip durations, lexical class, and more. We document the steps used to create ASL-LEX 2.0 and describe the distributional characteristics for sign properties across the lexicon and examine the relationships among lexical and phonological properties of signs. Correlation analyses revealed that frequent signs were less iconic and phonologically simpler than infrequent signs and iconic signs tended to be phonologically simpler than less iconic signs. The complete ASL-LEX dataset and supplementary materials are available at https://osf.io/zpha4/ and an interactive visualization of the entire lexicon can be accessed on the ASL-LEX page: http://asl-lex.org/.",
      "abstract": "Abstract ASL-LEX is a publicly available, large-scale lexical database for American Sign Language (ASL). We report on the expanded database (ASL-LEX 2.0) that contains 2,723 ASL signs. For each sign, ASL-LEX now includes a more detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (“guessability”) ratings (from non-signers), sign and videoclip durations, lexical class, and more. We document the steps used to create ASL-LEX 2.0 and describe the distributional characteristics for sign properties across the lexicon and examine the relationships among lexical and phonological properties of signs. Correlation analyses revealed that frequent signs were less iconic and phonologically simpler than infrequent signs and iconic signs tended to be phonologically simpler than less iconic signs. The complete ASL-LEX dataset and supplementary materials are available at https://osf.io/zpha4/ and an interactive visualization of the entire lexicon can be accessed on the ASL-LEX page: http://asl-lex.org/.",
      "doi": "https://doi.org/10.1093/deafed/enaa038",
      "openalex_id": "https://openalex.org/W3132179798",
      "arxiv_id": "",
      "publication_date": "2021-02-10",
      "published": "2021-02-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a sign-language lexical database with video signs and phonological properties, not discrete audio tokenization or a study of tokenizing speech-like audio into discrete symbols, so it fails the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a sign-language lexical database with video signs and phonological properties, not discrete audio tokenization or a study of tokenizing speech-like audio into discrete symbols, so it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a lexical and phonological database of American Sign Language signs, without any mention of discrete audio or speech tokenization, continuous audio signals, or audio reconstruction/generation tasks; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to audio tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a lexical and phonological database of American Sign Language signs, without any mention of discrete audio or speech tokenization, continuous audio signals, or audio reconstruction/generation tasks; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to audio tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Assessment for Reading Instruction",
    "abstract": "Abstract Chapter 2 provides an extensive overview of language and literacy assessments and how to use assessment results to guide literacy instruction for deaf/hard-of-hearing (DHH) students. A myriad of assessment purposes is covered, including documentation of eligibility, vocabulary, language(s), emergent reading skills, print awareness, phonological skills, fluency, comprehension, writing, spelling, and motivation. Administration and application of assessment results is addressed through seven case vignettes with DHH learners. Assessment accommodations and modifications, considerations when creating assessments, and the roles of educators in the assessment process are discussed. A summary of recommended assessment practices concludes this chapter.",
    "metadata": {
      "title": "Assessment for Reading Instruction",
      "summary": "Abstract Chapter 2 provides an extensive overview of language and literacy assessments and how to use assessment results to guide literacy instruction for deaf/hard-of-hearing (DHH) students. A myriad of assessment purposes is covered, including documentation of eligibility, vocabulary, language(s), emergent reading skills, print awareness, phonological skills, fluency, comprehension, writing, spelling, and motivation. Administration and application of assessment results is addressed through seven case vignettes with DHH learners. Assessment accommodations and modifications, considerations when creating assessments, and the roles of educators in the assessment process are discussed. A summary of recommended assessment practices concludes this chapter.",
      "abstract": "Abstract Chapter 2 provides an extensive overview of language and literacy assessments and how to use assessment results to guide literacy instruction for deaf/hard-of-hearing (DHH) students. A myriad of assessment purposes is covered, including documentation of eligibility, vocabulary, language(s), emergent reading skills, print awareness, phonological skills, fluency, comprehension, writing, spelling, and motivation. Administration and application of assessment results is addressed through seven case vignettes with DHH learners. Assessment accommodations and modifications, considerations when creating assessments, and the roles of educators in the assessment process are discussed. A summary of recommended assessment practices concludes this chapter.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0002",
      "openalex_id": "https://openalex.org/W4401771242",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes literacy and language assessments for deaf/hard-of-hearing students and does not address discrete audio/speech tokenization or token-based representation, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes literacy and language assessments for deaf/hard-of-hearing students and does not address discrete audio/speech tokenization or token-based representation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on educational assessments for reading instruction among deaf/hard-of-hearing students and do not discuss any aspect of audio/speech tokenization or discrete audio signal processing, which is the core inclusion criterion; thus, the study is outside the scope of the defined criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on educational assessments for reading instruction among deaf/hard-of-hearing students and do not discuss any aspect of audio/speech tokenization or discrete audio signal processing, which is the core inclusion criterion; thus, the study is outside the scope of the defined criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Oxford Handbook of the Mental Lexicon",
    "abstract": "Abstract The present handbook is a state-of-the-art compilation of papers from leading scholars on the mental lexicon—the representation of language in the mind/brain at the level of individual words and meaningful sub-word units. In recent years, the study of words as mental objects has grown rapidly across several fields including linguistics, psychology, philosophy, neuroscience, education, and computational cognitive science. This comprehensive collection spans multiple disciplines, topics, theories, and methods, to highlight important advances in the study of the mental lexicon, identify areas of debate, and inspire innovation in the field from present and future generations of scholars. The book is divided into three parts. Part I presents modern linguistic and cognitive theories of how the mind/brain represents words at the phonological, morphological, syntactic, semantic, and pragmatic levels. This part also discusses broad architectural issues pertaining to the organization of the lexicon, the relation between words and concepts, and the role of compositionality. Part II discusses how children learn the form and meaning of words in their native language drawing from the key domains of phonology, morphology, syntax, semantics, and pragmatics. Multiple approaches to lexical learning are introduced to explain how learner- and environment-driven factors contribute to both the stability and the variability of lexical learning across both individual learners and communities. Part III examines how the mental lexicon contributes to language use during listening, speaking, and conversation, and includes perspectives from bilingualism, sign languages, and disorders of lexical access and production.",
    "metadata": {
      "title": "The Oxford Handbook of the Mental Lexicon",
      "summary": "Abstract The present handbook is a state-of-the-art compilation of papers from leading scholars on the mental lexicon—the representation of language in the mind/brain at the level of individual words and meaningful sub-word units. In recent years, the study of words as mental objects has grown rapidly across several fields including linguistics, psychology, philosophy, neuroscience, education, and computational cognitive science. This comprehensive collection spans multiple disciplines, topics, theories, and methods, to highlight important advances in the study of the mental lexicon, identify areas of debate, and inspire innovation in the field from present and future generations of scholars. The book is divided into three parts. Part I presents modern linguistic and cognitive theories of how the mind/brain represents words at the phonological, morphological, syntactic, semantic, and pragmatic levels. This part also discusses broad architectural issues pertaining to the organization of the lexicon, the relation between words and concepts, and the role of compositionality. Part II discusses how children learn the form and meaning of words in their native language drawing from the key domains of phonology, morphology, syntax, semantics, and pragmatics. Multiple approaches to lexical learning are introduced to explain how learner- and environment-driven factors contribute to both the stability and the variability of lexical learning across both individual learners and communities. Part III examines how the mental lexicon contributes to language use during listening, speaking, and conversation, and includes perspectives from bilingualism, sign languages, and disorders of lexical access and production.",
      "abstract": "Abstract The present handbook is a state-of-the-art compilation of papers from leading scholars on the mental lexicon—the representation of language in the mind/brain at the level of individual words and meaningful sub-word units. In recent years, the study of words as mental objects has grown rapidly across several fields including linguistics, psychology, philosophy, neuroscience, education, and computational cognitive science. This comprehensive collection spans multiple disciplines, topics, theories, and methods, to highlight important advances in the study of the mental lexicon, identify areas of debate, and inspire innovation in the field from present and future generations of scholars. The book is divided into three parts. Part I presents modern linguistic and cognitive theories of how the mind/brain represents words at the phonological, morphological, syntactic, semantic, and pragmatic levels. This part also discusses broad architectural issues pertaining to the organization of the lexicon, the relation between words and concepts, and the role of compositionality. Part II discusses how children learn the form and meaning of words in their native language drawing from the key domains of phonology, morphology, syntax, semantics, and pragmatics. Multiple approaches to lexical learning are introduced to explain how learner- and environment-driven factors contribute to both the stability and the variability of lexical learning across both individual learners and communities. Part III examines how the mental lexicon contributes to language use during listening, speaking, and conversation, and includes perspectives from bilingualism, sign languages, and disorders of lexical access and production.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.001.0001",
      "openalex_id": "https://openalex.org/W3215506352",
      "arxiv_id": "",
      "publication_date": "2022-01-07",
      "published": "2022-01-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Oxford Handbook of the Mental Lexicon is a general compilation on word representations across disciplines and does not propose or evaluate discrete audio tokenization into discrete speech tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The Oxford Handbook of the Mental Lexicon is a general compilation on word representations across disciplines and does not propose or evaluate discrete audio tokenization into discrete speech tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a handbook focused on linguistic and cognitive aspects of the mental lexicon, covering theories and empirical studies from multiple disciplines, without any mention of discrete audio/speech tokenization or related audio signal processing techniques; thus, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it address any tokenization methods or neural quantization relevant to audio reconstruction or generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a handbook focused on linguistic and cognitive aspects of the mental lexicon, covering theories and empirical studies from multiple disciplines, without any mention of discrete audio/speech tokenization or related audio signal processing techniques; thus, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it address any tokenization methods or neural quantization relevant to audio reconstruction or generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Real‐time lexical comprehension in young children learning American Sign Language",
    "abstract": "Abstract When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world. This language–vision interaction can provide insights into children's developing efficiency in language comprehension. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? Here, we measured eye movements during real‐time comprehension of a visual‐manual language, American Sign Language ( ASL ), by 29 native ASL ‐learning children (16–53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset. Deaf and hearing ASL ‐learners showed similar gaze patterns, suggesting that the in‐the‐moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day‐to‐day life. Finally, variation in children's ASL processing was positively correlated with age and vocabulary size. Thus, despite competition for attention within a single modality, the timing and accuracy of visual fixations during ASL comprehension reflect information processing skills that are important for language acquisition regardless of language modality.",
    "metadata": {
      "title": "Real‐time lexical comprehension in young children learning American Sign Language",
      "summary": "Abstract When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world. This language–vision interaction can provide insights into children's developing efficiency in language comprehension. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? Here, we measured eye movements during real‐time comprehension of a visual‐manual language, American Sign Language ( ASL ), by 29 native ASL ‐learning children (16–53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset. Deaf and hearing ASL ‐learners showed similar gaze patterns, suggesting that the in‐the‐moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day‐to‐day life. Finally, variation in children's ASL processing was positively correlated with age and vocabulary size. Thus, despite competition for attention within a single modality, the timing and accuracy of visual fixations during ASL comprehension reflect information processing skills that are important for language acquisition regardless of language modality.",
      "abstract": "Abstract When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world. This language–vision interaction can provide insights into children's developing efficiency in language comprehension. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? Here, we measured eye movements during real‐time comprehension of a visual‐manual language, American Sign Language ( ASL ), by 29 native ASL ‐learning children (16–53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset. Deaf and hearing ASL ‐learners showed similar gaze patterns, suggesting that the in‐the‐moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day‐to‐day life. Finally, variation in children's ASL processing was positively correlated with age and vocabulary size. Thus, despite competition for attention within a single modality, the timing and accuracy of visual fixations during ASL comprehension reflect information processing skills that are important for language acquisition regardless of language modality.",
      "doi": "https://doi.org/10.1111/desc.12672",
      "openalex_id": "https://openalex.org/W2797842776",
      "arxiv_id": "",
      "publication_date": "2018-04-16",
      "published": "2018-04-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes ASL real-time comprehension with eye-tracking and has no discrete audio tokenization or analysis of discrete audio tokens, thus it does not meet the inclusion criterion requiring discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes ASL real-time comprehension with eye-tracking and has no discrete audio tokenization or analysis of discrete audio tokens, thus it does not meet the inclusion criterion requiring discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time comprehension of American Sign Language by children, analyzing eye movements and visual attention dynamics, with no mention of audio signal processing, discrete audio tokenization, or any form of audio signal conversion into discrete symbols. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and instead aligns with none of the discrete audio token-focused research areas described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time comprehension of American Sign Language by children, analyzing eye movements and visual attention dynamics, with no mention of audio signal processing, discrete audio tokenization, or any form of audio signal conversion into discrete symbols. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and instead aligns with none of the discrete audio token-focused research areas described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gesture recognition algorithm based on multi‐scale feature fusion in RGB‐D images",
    "abstract": "Abstract With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human‐computer interaction more natural and flexible, bringing richer interactive experience to teaching, on‐board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi‐level feature fusion of two‐stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB‐D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi‐level feature fusion of two‐stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single‐channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1‐miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.",
    "metadata": {
      "title": "Gesture recognition algorithm based on multi‐scale feature fusion in RGB‐D images",
      "summary": "Abstract With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human‐computer interaction more natural and flexible, bringing richer interactive experience to teaching, on‐board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi‐level feature fusion of two‐stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB‐D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi‐level feature fusion of two‐stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single‐channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1‐miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.",
      "abstract": "Abstract With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human‐computer interaction more natural and flexible, bringing richer interactive experience to teaching, on‐board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi‐level feature fusion of two‐stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB‐D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi‐level feature fusion of two‐stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single‐channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1‐miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.",
      "doi": "https://doi.org/10.1049/ipr2.12712",
      "openalex_id": "https://openalex.org/W4312105654",
      "arxiv_id": "",
      "publication_date": "2022-12-23",
      "published": "2022-12-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on visual gesture recognition using RGB-D data and neural networks, with no discrete audio tokenization analysis; it does not meet the audio/speech tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on visual gesture recognition using RGB-D data and neural networks, with no discrete audio tokenization analysis; it does not meet the audio/speech tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gesture recognition using multi-scale feature fusion and convolutional neural networks with RGB-D images, not on discrete audio/speech tokenization or analysis of discrete audio tokens as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gesture recognition using multi-scale feature fusion and convolutional neural networks with RGB-D images, not on discrete audio/speech tokenization or analysis of discrete audio tokens as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effects of Video Reversal on Gaze Patterns during Signed Narrative Comprehension",
    "abstract": "Abstract Language knowledge, age of acquisition (AoA), and stimulus intelligibility all affect gaze behavior for reading print, but it is unknown how these factors affect “sign-watching” among signers. This study investigated how these factors affect gaze behavior during sign language comprehension in 52 adult signers who acquired American Sign Language (ASL) at different ages. We examined gaze patterns and story comprehension in four subject groups who differ in hearing status and when they learned ASL (i.e. Deaf Early, Deaf Late, Hearing Late, and Hearing Novice). Participants watched signed stories in normal (high intelligibility) and video-reversed (low intelligibility) conditions. This video manipulation was used because it distorts word order and thus disrupts the syntax and semantic content of narratives, while preserving most surface phonological features of individual signs. Video reversal decreased story comprehension accuracy, and this effect was greater for those who learned ASL later in life. Reversal also was associated with more dispersed gaze behavior. Although each subject group had unique gaze patterns, the effect of video reversal on gaze measures was similar across all groups. Among fluent signers, gaze behavior was not correlated with AoA, suggesting that “efficient” sign watching can be quickly learnt even among signers exposed to signed language later in life.",
    "metadata": {
      "title": "Effects of Video Reversal on Gaze Patterns during Signed Narrative Comprehension",
      "summary": "Abstract Language knowledge, age of acquisition (AoA), and stimulus intelligibility all affect gaze behavior for reading print, but it is unknown how these factors affect “sign-watching” among signers. This study investigated how these factors affect gaze behavior during sign language comprehension in 52 adult signers who acquired American Sign Language (ASL) at different ages. We examined gaze patterns and story comprehension in four subject groups who differ in hearing status and when they learned ASL (i.e. Deaf Early, Deaf Late, Hearing Late, and Hearing Novice). Participants watched signed stories in normal (high intelligibility) and video-reversed (low intelligibility) conditions. This video manipulation was used because it distorts word order and thus disrupts the syntax and semantic content of narratives, while preserving most surface phonological features of individual signs. Video reversal decreased story comprehension accuracy, and this effect was greater for those who learned ASL later in life. Reversal also was associated with more dispersed gaze behavior. Although each subject group had unique gaze patterns, the effect of video reversal on gaze measures was similar across all groups. Among fluent signers, gaze behavior was not correlated with AoA, suggesting that “efficient” sign watching can be quickly learnt even among signers exposed to signed language later in life.",
      "abstract": "Abstract Language knowledge, age of acquisition (AoA), and stimulus intelligibility all affect gaze behavior for reading print, but it is unknown how these factors affect “sign-watching” among signers. This study investigated how these factors affect gaze behavior during sign language comprehension in 52 adult signers who acquired American Sign Language (ASL) at different ages. We examined gaze patterns and story comprehension in four subject groups who differ in hearing status and when they learned ASL (i.e. Deaf Early, Deaf Late, Hearing Late, and Hearing Novice). Participants watched signed stories in normal (high intelligibility) and video-reversed (low intelligibility) conditions. This video manipulation was used because it distorts word order and thus disrupts the syntax and semantic content of narratives, while preserving most surface phonological features of individual signs. Video reversal decreased story comprehension accuracy, and this effect was greater for those who learned ASL later in life. Reversal also was associated with more dispersed gaze behavior. Although each subject group had unique gaze patterns, the effect of video reversal on gaze measures was similar across all groups. Among fluent signers, gaze behavior was not correlated with AoA, suggesting that “efficient” sign watching can be quickly learnt even among signers exposed to signed language later in life.",
      "doi": "https://doi.org/10.1093/deafed/enaa007",
      "openalex_id": "https://openalex.org/W3028570272",
      "arxiv_id": "",
      "publication_date": "2020-02-24",
      "published": "2020-02-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on gaze patterns and comprehension in sign language with video-reversed stimuli and does not involve discrete audio tokenization or analysis of converting continuous audio into discrete tokens; thus it does not meet the inclusion criteria centered on discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on gaze patterns and comprehension in sign language with video-reversed stimuli and does not involve discrete audio tokenization or analysis of converting continuous audio into discrete tokens; thus it does not meet the inclusion criteria centered on discrete audio tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gaze behavior during sign language comprehension and does not discuss discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete tokens, which is the core inclusion criterion, nor does it evaluate any discrete audio token representations. Therefore, it does not meet the inclusion criteria relevant to discrete audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gaze behavior during sign language comprehension and does not discuss discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete tokens, which is the core inclusion criterion, nor does it evaluate any discrete audio token representations. Therefore, it does not meet the inclusion criteria relevant to discrete audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-modal translation priming and iconicity effects in deaf signers and hearing learners of American Sign Language",
    "abstract": "Abstract This study used ERPs to a) assess the neural correlates of cross-linguistic, cross-modal translation priming in hearing beginning learners of American Sign Language (ASL) and deaf highly proficient signers and b) examine whether sign iconicity modulates these priming effects. Hearing learners exhibited translation priming for ASL signs preceded by English words (greater negativity for unrelated than translation primes) later in the ERP waveform than deaf signers and exhibited earlier and greater priming for iconic than non-iconic signs. Iconicity did not modulate translation priming effects either behaviorally or in the ERPs for deaf signers (except in a 800–1000 ms time window). Because deaf signers showed early translation priming effects (beginning at 400ms-600ms), we suggest that iconicity did not facilitate lexical access, but deaf signers may have recognized sign iconicity later in processing. Overall, the results indicate that iconicity speeds lexical access for L2 sign language learners, but not for proficient signers.",
    "metadata": {
      "title": "Cross-modal translation priming and iconicity effects in deaf signers and hearing learners of American Sign Language",
      "summary": "Abstract This study used ERPs to a) assess the neural correlates of cross-linguistic, cross-modal translation priming in hearing beginning learners of American Sign Language (ASL) and deaf highly proficient signers and b) examine whether sign iconicity modulates these priming effects. Hearing learners exhibited translation priming for ASL signs preceded by English words (greater negativity for unrelated than translation primes) later in the ERP waveform than deaf signers and exhibited earlier and greater priming for iconic than non-iconic signs. Iconicity did not modulate translation priming effects either behaviorally or in the ERPs for deaf signers (except in a 800–1000 ms time window). Because deaf signers showed early translation priming effects (beginning at 400ms-600ms), we suggest that iconicity did not facilitate lexical access, but deaf signers may have recognized sign iconicity later in processing. Overall, the results indicate that iconicity speeds lexical access for L2 sign language learners, but not for proficient signers.",
      "abstract": "Abstract This study used ERPs to a) assess the neural correlates of cross-linguistic, cross-modal translation priming in hearing beginning learners of American Sign Language (ASL) and deaf highly proficient signers and b) examine whether sign iconicity modulates these priming effects. Hearing learners exhibited translation priming for ASL signs preceded by English words (greater negativity for unrelated than translation primes) later in the ERP waveform than deaf signers and exhibited earlier and greater priming for iconic than non-iconic signs. Iconicity did not modulate translation priming effects either behaviorally or in the ERPs for deaf signers (except in a 800–1000 ms time window). Because deaf signers showed early translation priming effects (beginning at 400ms-600ms), we suggest that iconicity did not facilitate lexical access, but deaf signers may have recognized sign iconicity later in processing. Overall, the results indicate that iconicity speeds lexical access for L2 sign language learners, but not for proficient signers.",
      "doi": "https://doi.org/10.1017/s1366728919000889",
      "openalex_id": "https://openalex.org/W3004227195",
      "arxiv_id": "",
      "publication_date": "2020-01-31",
      "published": "2020-01-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates sign language processing and iconicity using ERPs; it does not involve discrete audio tokenization or any analysis of discrete acoustic tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates sign language processing and iconicity using ERPs; it does not involve discrete audio tokenization or any analysis of discrete acoustic tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural correlates of translation priming and iconicity effects in sign language users, involving cognitive neuroscience and language processing. It does not mention any discrete audio/speech tokenization, quantization, or audio signal processing as part of its core methods or analysis. The core study is about ERPs and language processing, not discrete audio tokenization or related neural encoding methods relevant to speech token sequences. Thus, it fails to meet the inclusion criteria and aligns with exclusion criteria related to absence of discrete audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural correlates of translation priming and iconicity effects in sign language users, involving cognitive neuroscience and language processing. It does not mention any discrete audio/speech tokenization, quantization, or audio signal processing as part of its core methods or analysis. The core study is about ERPs and language processing, not discrete audio tokenization or related neural encoding methods relevant to speech token sequences. Thus, it fails to meet the inclusion criteria and aligns with exclusion criteria related to absence of discrete audio tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A survey on sign language literature",
    "abstract": "Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.",
    "metadata": {
      "title": "A survey on sign language literature",
      "summary": "Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.",
      "abstract": "Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.",
      "doi": "https://doi.org/10.1016/j.mlwa.2023.100504",
      "openalex_id": "https://openalex.org/W4387924637",
      "arxiv_id": "",
      "publication_date": "2023-10-24",
      "published": "2023-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on sign language recognition/translation (a visual modality) and mentions datasets; it does not propose, analyze, or evaluate discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on sign language recognition/translation (a visual modality) and mentions datasets; it does not propose, analyze, or evaluate discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on sign language literature, highlighting sign language recognition and translation research, but do not mention discrete audio/speech tokenization or any associated analysis or evaluation; thus, the study does not meet the inclusion criteria centered on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on sign language literature, highlighting sign language recognition and translation research, but do not mention discrete audio/speech tokenization or any associated analysis or evaluation; thus, the study does not meet the inclusion criteria centered on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign Language Recognition: A Comprehensive Review of Traditional and Deep Learning Approaches, Datasets, and Challenges",
    "abstract": "The Deaf are a large social group in society. Their unique way of communicating through sign language is often confined within their community due to limited understanding by individuals outside of this demographic. This is where sign language recognition (SLR) comes in to help people without hearing impairments understand the meaning of sign language. In recent years, new methods of sign language recognition have been developed and achieved good results, so it is necessary to make a summary. This review mainly focuses on the introduction of sign language recognition techniques based on algorithms especially in recent years, including the recognition models based on traditional methods and deep learning approaches, sign language datasets, challenges and future directions in SLR. To make the method structure clearer, this article explains and compares the basic principles of different methods from the perspectives of feature extraction and temporal modelling. We hope that this review will provide some reference and help for future research in sign language recognition.",
    "metadata": {
      "title": "Sign Language Recognition: A Comprehensive Review of Traditional and Deep Learning Approaches, Datasets, and Challenges",
      "summary": "The Deaf are a large social group in society. Their unique way of communicating through sign language is often confined within their community due to limited understanding by individuals outside of this demographic. This is where sign language recognition (SLR) comes in to help people without hearing impairments understand the meaning of sign language. In recent years, new methods of sign language recognition have been developed and achieved good results, so it is necessary to make a summary. This review mainly focuses on the introduction of sign language recognition techniques based on algorithms especially in recent years, including the recognition models based on traditional methods and deep learning approaches, sign language datasets, challenges and future directions in SLR. To make the method structure clearer, this article explains and compares the basic principles of different methods from the perspectives of feature extraction and temporal modelling. We hope that this review will provide some reference and help for future research in sign language recognition.",
      "abstract": "The Deaf are a large social group in society. Their unique way of communicating through sign language is often confined within their community due to limited understanding by individuals outside of this demographic. This is where sign language recognition (SLR) comes in to help people without hearing impairments understand the meaning of sign language. In recent years, new methods of sign language recognition have been developed and achieved good results, so it is necessary to make a summary. This review mainly focuses on the introduction of sign language recognition techniques based on algorithms especially in recent years, including the recognition models based on traditional methods and deep learning approaches, sign language datasets, challenges and future directions in SLR. To make the method structure clearer, this article explains and compares the basic principles of different methods from the perspectives of feature extraction and temporal modelling. We hope that this review will provide some reference and help for future research in sign language recognition.",
      "doi": "https://doi.org/10.1109/access.2024.3398806",
      "openalex_id": "https://openalex.org/W4396753522",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article reviews sign language recognition from a visual/modal perspective with no focus on discrete audio tokenization or quantized speech tokens, so it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article reviews sign language recognition from a visual/modal perspective with no focus on discrete audio tokenization or quantized speech tokens, so it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition using traditional and deep learning methods, dealing primarily with visual gesture recognition rather than audio/speech tokenization. There is no indication that the study discusses discrete audio tokenization or conversion of continuous audio signals into discrete symbols, which is required by the inclusion criteria; thus, it does not meet the core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition using traditional and deep learning methods, dealing primarily with visual gesture recognition rather than audio/speech tokenization. There is no indication that the study discusses discrete audio tokenization or conversion of continuous audio signals into discrete symbols, which is required by the inclusion criteria; thus, it does not meet the core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Activation of ASL signs during sentence reading for deaf readers: evidence from eye-tracking",
    "abstract": "Abstract Bilinguals activate both of their languages as they process written words, regardless of modality (spoken or signed); these effects have primarily been documented in single word reading paradigms. We used eye-tracking to determine whether deaf bilingual readers ( n = 23) activate American Sign Language (ASL) translations as they read English sentences. Sentences contained a target word and one of the two possible prime words: a related prime which shared phonological parameters (location, handshape or movement) with the target when translated into ASL or an unrelated prime. The results revealed that first fixation durations and gaze durations (early processing measures) were shorter when target words were preceded by ASL-related primes, but prime condition did not impact later processing measures (e.g., regressions). Further, less-skilled readers showed a larger ASL co-activation effect. Together, the results indicate that ASL co-activation impacts early lexical access and can facilitate reading, particularly for less-skilled deaf readers.",
    "metadata": {
      "title": "Activation of ASL signs during sentence reading for deaf readers: evidence from eye-tracking",
      "summary": "Abstract Bilinguals activate both of their languages as they process written words, regardless of modality (spoken or signed); these effects have primarily been documented in single word reading paradigms. We used eye-tracking to determine whether deaf bilingual readers ( n = 23) activate American Sign Language (ASL) translations as they read English sentences. Sentences contained a target word and one of the two possible prime words: a related prime which shared phonological parameters (location, handshape or movement) with the target when translated into ASL or an unrelated prime. The results revealed that first fixation durations and gaze durations (early processing measures) were shorter when target words were preceded by ASL-related primes, but prime condition did not impact later processing measures (e.g., regressions). Further, less-skilled readers showed a larger ASL co-activation effect. Together, the results indicate that ASL co-activation impacts early lexical access and can facilitate reading, particularly for less-skilled deaf readers.",
      "abstract": "Abstract Bilinguals activate both of their languages as they process written words, regardless of modality (spoken or signed); these effects have primarily been documented in single word reading paradigms. We used eye-tracking to determine whether deaf bilingual readers ( n = 23) activate American Sign Language (ASL) translations as they read English sentences. Sentences contained a target word and one of the two possible prime words: a related prime which shared phonological parameters (location, handshape or movement) with the target when translated into ASL or an unrelated prime. The results revealed that first fixation durations and gaze durations (early processing measures) were shorter when target words were preceded by ASL-related primes, but prime condition did not impact later processing measures (e.g., regressions). Further, less-skilled readers showed a larger ASL co-activation effect. Together, the results indicate that ASL co-activation impacts early lexical access and can facilitate reading, particularly for less-skilled deaf readers.",
      "doi": "https://doi.org/10.1017/s1366728924000336",
      "openalex_id": "https://openalex.org/W4395673765",
      "arxiv_id": "",
      "publication_date": "2024-04-26",
      "published": "2024-04-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Inclusion requires explicit analysis of discrete audio tokenization; the study focuses on ASL sign activation during reading with eye-tracking and does not involve discrete audio tokenization or token representations for speech. It centers on sign language and early lexical processing, not on converting continuous audio into discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Inclusion requires explicit analysis of discrete audio tokenization; the study focuses on ASL sign activation during reading with eye-tracking and does not involve discrete audio tokenization or token representations for speech. It centers on sign language and early lexical processing, not on converting continuous audio into discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates ASL sign activation during sentence reading for deaf readers using eye-tracking, focusing on bilingual language processing and lexical access, but does not propose, analyze, or evaluate any discrete audio/speech tokenization methods, nor does it address audio signal tokenization from continuous to discrete symbols; therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates ASL sign activation during sentence reading for deaf readers using eye-tracking, focusing on bilingual language processing and lexical access, but does not propose, analyze, or evaluate any discrete audio/speech tokenization methods, nor does it address audio signal tokenization from continuous to discrete symbols; therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Recent Issues in the Use of Signed Language Assessments for Diagnosis of Language Disorders in Signing Deaf and Hard of Hearing Children",
    "abstract": "In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are limitations in their application, particularly for the diagnosis of language disorders, and learning disabilities. Here, we discuss some of the available normed, signed language assessments and some of their limitations. We have also provided information related to practices that should lead to improvement in the quality of signed language assessments.",
    "metadata": {
      "title": "Recent Issues in the Use of Signed Language Assessments for Diagnosis of Language Disorders in Signing Deaf and Hard of Hearing Children",
      "summary": "In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are limitations in their application, particularly for the diagnosis of language disorders, and learning disabilities. Here, we discuss some of the available normed, signed language assessments and some of their limitations. We have also provided information related to practices that should lead to improvement in the quality of signed language assessments.",
      "abstract": "In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are limitations in their application, particularly for the diagnosis of language disorders, and learning disabilities. Here, we discuss some of the available normed, signed language assessments and some of their limitations. We have also provided information related to practices that should lead to improvement in the quality of signed language assessments.",
      "doi": "https://doi.org/10.1093/deafed/eny014",
      "openalex_id": "https://openalex.org/W2803946838",
      "arxiv_id": "",
      "publication_date": "2018-04-11",
      "published": "2018-04-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The abstract discusses normed signed language assessments for diagnosing language disorders and improving quality, but it does not address discrete audio/tokenization of speech or any token-based representations necessary to meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The abstract discusses normed signed language assessments for diagnosing language disorders and improving quality, but it does not address discrete audio/tokenization of speech or any token-based representations necessary to meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on signed language assessments for diagnosing language disorders in deaf and hard of hearing children, discussing assessment tools and their limitations, without mentioning discrete audio/speech tokenization or any related analysis of audio signals. Therefore, the study does not meet the inclusion criteria requiring discrete tokenization of audio, nor does it address any audio token-based methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on signed language assessments for diagnosing language disorders in deaf and hard of hearing children, discussing assessment tools and their limitations, without mentioning discrete audio/speech tokenization or any related analysis of audio signals. Therefore, the study does not meet the inclusion criteria requiring discrete tokenization of audio, nor does it address any audio token-based methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Systematic mappings between semantic categories and types of iconic representations in the manual modality: A normed database of silent gesture",
    "abstract": "An unprecedented number of empirical studies have shown that iconic gestures-those that mimic the sensorimotor attributes of a referent-contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture-meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture's mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.",
    "metadata": {
      "title": "Systematic mappings between semantic categories and types of iconic representations in the manual modality: A normed database of silent gesture",
      "summary": "An unprecedented number of empirical studies have shown that iconic gestures-those that mimic the sensorimotor attributes of a referent-contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture-meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture's mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.",
      "abstract": "An unprecedented number of empirical studies have shown that iconic gestures-those that mimic the sensorimotor attributes of a referent-contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture-meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture's mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.",
      "doi": "https://doi.org/10.3758/s13428-019-01204-6",
      "openalex_id": "https://openalex.org/W2918118147",
      "arxiv_id": "",
      "publication_date": "2019-02-20",
      "published": "2019-02-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses silent gestures and a normed database of gesture mappings; there is no focus on discrete audio/speech tokenization, token quantization, or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded under the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses silent gestures and a normed database of gesture mappings; there is no focus on discrete audio/speech tokenization, token quantization, or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded under the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on silent gestures and their mapping to semantic categories without any mention of audio/speech tokenization, discrete audio token representations, or neural encoding related to speech. Therefore, it does not meet the inclusion criteria which require a core focus on discrete audio/speech tokenization, nor does it overlap with the exclusions related to continuous audio features or codec-based approaches. The content is unrelated to the discrete tokenization of speech audio signals or their analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on silent gestures and their mapping to semantic categories without any mention of audio/speech tokenization, discrete audio token representations, or neural encoding related to speech. Therefore, it does not meet the inclusion criteria which require a core focus on discrete audio/speech tokenization, nor does it overlap with the exclusions related to continuous audio features or codec-based approaches. The content is unrelated to the discrete tokenization of speech audio signals or their analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The perceived mapping between form and meaning in American Sign Language depends on linguistic knowledge and task: evidence from iconicity and transparency judgments",
    "abstract": "abstract Iconicity is often defined as the resemblance between a form and a given meaning, while transparency is defined as the ability to infer a given meaning based on the form. This study examined the influence of knowledge of American Sign Language (ASL) on the perceived iconicity of signs and the relationship between iconicity, transparency (correctly guessed signs), ‘perceived transparency’ (transparency ratings of the guesses), and ‘semantic potential’ (the diversity (H index) of guesses). Experiment 1 compared iconicity ratings by deaf ASL signers and hearing non-signers for 991 signs from the ASL-LEX database. Signers and non-signers’ ratings were highly correlated; however, the groups provided different iconicity ratings for subclasses of signs: nouns vs. verbs, handling vs. entity, and one- vs. two-handed signs. In Experiment 2, non-signers guessed the meaning of 430 signs and rated them for how transparent their guessed meaning would be for others. Only 10% of guesses were correct. Iconicity ratings correlated with transparency (correct guesses), perceived transparency ratings, and semantic potential (H index). Further, some iconic signs were perceived as non-transparent and vice versa. The study demonstrates that linguistic knowledge mediates perceived iconicity distinctly from gesture and highlights critical distinctions between iconicity, transparency (perceived and objective), and semantic potential.",
    "metadata": {
      "title": "The perceived mapping between form and meaning in American Sign Language depends on linguistic knowledge and task: evidence from iconicity and transparency judgments",
      "summary": "abstract Iconicity is often defined as the resemblance between a form and a given meaning, while transparency is defined as the ability to infer a given meaning based on the form. This study examined the influence of knowledge of American Sign Language (ASL) on the perceived iconicity of signs and the relationship between iconicity, transparency (correctly guessed signs), ‘perceived transparency’ (transparency ratings of the guesses), and ‘semantic potential’ (the diversity (H index) of guesses). Experiment 1 compared iconicity ratings by deaf ASL signers and hearing non-signers for 991 signs from the ASL-LEX database. Signers and non-signers’ ratings were highly correlated; however, the groups provided different iconicity ratings for subclasses of signs: nouns vs. verbs, handling vs. entity, and one- vs. two-handed signs. In Experiment 2, non-signers guessed the meaning of 430 signs and rated them for how transparent their guessed meaning would be for others. Only 10% of guesses were correct. Iconicity ratings correlated with transparency (correct guesses), perceived transparency ratings, and semantic potential (H index). Further, some iconic signs were perceived as non-transparent and vice versa. The study demonstrates that linguistic knowledge mediates perceived iconicity distinctly from gesture and highlights critical distinctions between iconicity, transparency (perceived and objective), and semantic potential.",
      "abstract": "abstract Iconicity is often defined as the resemblance between a form and a given meaning, while transparency is defined as the ability to infer a given meaning based on the form. This study examined the influence of knowledge of American Sign Language (ASL) on the perceived iconicity of signs and the relationship between iconicity, transparency (correctly guessed signs), ‘perceived transparency’ (transparency ratings of the guesses), and ‘semantic potential’ (the diversity (H index) of guesses). Experiment 1 compared iconicity ratings by deaf ASL signers and hearing non-signers for 991 signs from the ASL-LEX database. Signers and non-signers’ ratings were highly correlated; however, the groups provided different iconicity ratings for subclasses of signs: nouns vs. verbs, handling vs. entity, and one- vs. two-handed signs. In Experiment 2, non-signers guessed the meaning of 430 signs and rated them for how transparent their guessed meaning would be for others. Only 10% of guesses were correct. Iconicity ratings correlated with transparency (correct guesses), perceived transparency ratings, and semantic potential (H index). Further, some iconic signs were perceived as non-transparent and vice versa. The study demonstrates that linguistic knowledge mediates perceived iconicity distinctly from gesture and highlights critical distinctions between iconicity, transparency (perceived and objective), and semantic potential.",
      "doi": "https://doi.org/10.1017/langcog.2019.18",
      "openalex_id": "https://openalex.org/W2958944944",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract investigates perceived iconicity and transparency of ASL signs and how linguistic knowledge shapes judgments, but it does not propose or analyze discrete audio/speech tokenization or a token-based representation for speech; thus it does not meet the criterion requiring discrete audio tokenization as core focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract investigates perceived iconicity and transparency of ASL signs and how linguistic knowledge shapes judgments, but it does not propose or analyze discrete audio/speech tokenization or a token-based representation for speech; thus it does not meet the criterion requiring discrete audio tokenization as core focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the perceived mapping of form and meaning in American Sign Language, specifically addressing iconicity, transparency, and linguistic knowledge, but it does not involve discrete audio or speech tokenization methods, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization, and does not fit the topic of tokenizing continuous audio signals into discrete symbols.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the perceived mapping of form and meaning in American Sign Language, specifically addressing iconicity, transparency, and linguistic knowledge, but it does not involve discrete audio or speech tokenization methods, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization, and does not fit the topic of tokenizing continuous audio signals into discrete symbols.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The iconicity toolbox: empirical approaches to measuring iconicity",
    "abstract": "abstract Growing evidence from across the cognitive sciences indicates that iconicity plays an important role in a number of fundamental language processes, spanning learning, comprehension, and online use. One benefit of this recent upsurge in empirical work is the diversification of methods available for measuring iconicity. In this paper, we provide an overview of methods in the form of a ‘toolbox’. We lay out empirical methods for measuring iconicity at a behavioural level, in the perception, production, and comprehension of iconic forms. We also discuss large-scale studies that look at iconicity on a system-wide level, based on objective measures of similarity between signals and meanings. We give a detailed overview of how different measures of iconicity can better address specific hypotheses, providing greater clarity when choosing testing methods.",
    "metadata": {
      "title": "The iconicity toolbox: empirical approaches to measuring iconicity",
      "summary": "abstract Growing evidence from across the cognitive sciences indicates that iconicity plays an important role in a number of fundamental language processes, spanning learning, comprehension, and online use. One benefit of this recent upsurge in empirical work is the diversification of methods available for measuring iconicity. In this paper, we provide an overview of methods in the form of a ‘toolbox’. We lay out empirical methods for measuring iconicity at a behavioural level, in the perception, production, and comprehension of iconic forms. We also discuss large-scale studies that look at iconicity on a system-wide level, based on objective measures of similarity between signals and meanings. We give a detailed overview of how different measures of iconicity can better address specific hypotheses, providing greater clarity when choosing testing methods.",
      "abstract": "abstract Growing evidence from across the cognitive sciences indicates that iconicity plays an important role in a number of fundamental language processes, spanning learning, comprehension, and online use. One benefit of this recent upsurge in empirical work is the diversification of methods available for measuring iconicity. In this paper, we provide an overview of methods in the form of a ‘toolbox’. We lay out empirical methods for measuring iconicity at a behavioural level, in the perception, production, and comprehension of iconic forms. We also discuss large-scale studies that look at iconicity on a system-wide level, based on objective measures of similarity between signals and meanings. We give a detailed overview of how different measures of iconicity can better address specific hypotheses, providing greater clarity when choosing testing methods.",
      "doi": "https://doi.org/10.1017/langcog.2019.14",
      "openalex_id": "https://openalex.org/W2946721775",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is a methodological overview of measuring iconicity across behavior and system-level similarity, but it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite vocabulary for reconstruction/generation/understanding tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article is a methodological overview of measuring iconicity across behavior and system-level similarity, but it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite vocabulary for reconstruction/generation/understanding tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on empirical methods for measuring iconicity in language processes and discusses behavioral and system-wide approaches, without any mention of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization and focuses more on cognitive and linguistic aspects rather than audio signal processing or tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on empirical methods for measuring iconicity in language processes and discusses behavioral and system-wide approaches, without any mention of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization and focuses more on cognitive and linguistic aspects rather than audio signal processing or tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Degree and not type of iconicity affects sign language vocabulary acquisition.",
    "abstract": "Lexical iconicity-signs or words that resemble their meaning-is overrepresented in children's early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child's firsthand experiences. As such, pantomimic iconic signs, which use the signer's body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the structure mapping theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in American Sign Language (ASL), we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We reanalyzed a previously published set of ASL Communicative Development Inventory (CDI) reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a structure mapping theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
    "metadata": {
      "title": "Degree and not type of iconicity affects sign language vocabulary acquisition.",
      "summary": "Lexical iconicity-signs or words that resemble their meaning-is overrepresented in children's early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child's firsthand experiences. As such, pantomimic iconic signs, which use the signer's body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the structure mapping theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in American Sign Language (ASL), we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We reanalyzed a previously published set of ASL Communicative Development Inventory (CDI) reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a structure mapping theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "abstract": "Lexical iconicity-signs or words that resemble their meaning-is overrepresented in children's early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child's firsthand experiences. As such, pantomimic iconic signs, which use the signer's body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the structure mapping theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in American Sign Language (ASL), we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We reanalyzed a previously published set of ASL Communicative Development Inventory (CDI) reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a structure mapping theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "doi": "https://doi.org/10.1037/xlm0000713",
      "openalex_id": "https://openalex.org/W2945201830",
      "arxiv_id": "",
      "publication_date": "2019-05-16",
      "published": "2019-05-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article studies iconicity types in ASL vocabulary and sign acquisition, not discrete audio tokenization or analysis of discrete speech tokens; since inclusion requires explicit focus on converting continuous audio into discrete tokens for speech tasks, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article studies iconicity types in ASL vocabulary and sign acquisition, not discrete audio tokenization or analysis of discrete speech tokens; since inclusion requires explicit focus on converting continuous audio into discrete tokens for speech tasks, it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language vocabulary acquisition and iconicity effects rather than on discrete audio/speech tokenization or quantization of continuous audio into discrete tokens. It does not address analysis or methods related to converting audio signals into token sequences, nor does it involve neural encoding or modern token representations for speech-related tasks. Therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language vocabulary acquisition and iconicity effects rather than on discrete audio/speech tokenization or quantization of continuous audio into discrete tokens. It does not address analysis or methods related to converting audio signals into token sequences, nor does it involve neural encoding or modern token representations for speech-related tasks. Therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ASL Trigger Recognition in Mixed Activity/Signing Sequences for RF Sensor-Based User Interfaces",
    "abstract": "The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.",
    "metadata": {
      "title": "ASL Trigger Recognition in Mixed Activity/Signing Sequences for RF Sensor-Based User Interfaces",
      "summary": "The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.",
      "abstract": "The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.",
      "doi": "https://doi.org/10.1109/thms.2021.3131675",
      "openalex_id": "https://openalex.org/W3213624573",
      "arxiv_id": "",
      "publication_date": "2021-12-22",
      "published": "2021-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This item targets RF-sensor-based ASL recognition and does not involve discrete audio tokenization or speech token sequences; thus it fails the required inclusion criteria for discrete audio/tokenization, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This item targets RF-sensor-based ASL recognition and does not involve discrete audio tokenization or speech token sequences; thus it fails the required inclusion criteria for discrete audio/tokenization, and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on recognizing American Sign Language (ASL) trigger signs using RF sensor data rather than audio signals, and it does not discuss discrete audio/speech tokenization or related methodologies; thus, it does not meet the inclusion criteria related to audio tokenization and involves no discrete audio token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on recognizing American Sign Language (ASL) trigger signs using RF sensor data rather than audio signals, and it does not discuss discrete audio/speech tokenization or related methodologies; thus, it does not meet the inclusion criteria related to audio tokenization and involves no discrete audio token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ERP Evidence for Co-Activation of English Words during Recognition of American Sign Language Signs",
    "abstract": "Event-related potentials (ERPs) were used to investigate co-activation of English words during recognition of American Sign Language (ASL) signs. Deaf and hearing signers viewed pairs of ASL signs and judged their semantic relatedness. Half of the semantically unrelated signs had English translations that shared an orthographic and phonological rime (e.g., BAR–STAR) and half did not (e.g., NURSE–STAR). Classic N400 and behavioral semantic priming effects were observed in both groups. For hearing signers, targets in sign pairs with English rime translations elicited a smaller N400 compared to targets in pairs with unrelated English translations. In contrast, a reversed N400 effect was observed for deaf signers: target signs in English rime translation pairs elicited a larger N400 compared to targets in pairs with unrelated English translations. This reversed effect was overtaken by a later, more typical ERP priming effect for deaf signers who were aware of the manipulation. These findings provide evidence that implicit language co-activation in bimodal bilinguals is bidirectional. However, the distinct pattern of effects in deaf and hearing signers suggests that it may be modulated by differences in language proficiency and dominance as well as by asymmetric reliance on orthographic versus phonological representations.",
    "metadata": {
      "title": "ERP Evidence for Co-Activation of English Words during Recognition of American Sign Language Signs",
      "summary": "Event-related potentials (ERPs) were used to investigate co-activation of English words during recognition of American Sign Language (ASL) signs. Deaf and hearing signers viewed pairs of ASL signs and judged their semantic relatedness. Half of the semantically unrelated signs had English translations that shared an orthographic and phonological rime (e.g., BAR–STAR) and half did not (e.g., NURSE–STAR). Classic N400 and behavioral semantic priming effects were observed in both groups. For hearing signers, targets in sign pairs with English rime translations elicited a smaller N400 compared to targets in pairs with unrelated English translations. In contrast, a reversed N400 effect was observed for deaf signers: target signs in English rime translation pairs elicited a larger N400 compared to targets in pairs with unrelated English translations. This reversed effect was overtaken by a later, more typical ERP priming effect for deaf signers who were aware of the manipulation. These findings provide evidence that implicit language co-activation in bimodal bilinguals is bidirectional. However, the distinct pattern of effects in deaf and hearing signers suggests that it may be modulated by differences in language proficiency and dominance as well as by asymmetric reliance on orthographic versus phonological representations.",
      "abstract": "Event-related potentials (ERPs) were used to investigate co-activation of English words during recognition of American Sign Language (ASL) signs. Deaf and hearing signers viewed pairs of ASL signs and judged their semantic relatedness. Half of the semantically unrelated signs had English translations that shared an orthographic and phonological rime (e.g., BAR–STAR) and half did not (e.g., NURSE–STAR). Classic N400 and behavioral semantic priming effects were observed in both groups. For hearing signers, targets in sign pairs with English rime translations elicited a smaller N400 compared to targets in pairs with unrelated English translations. In contrast, a reversed N400 effect was observed for deaf signers: target signs in English rime translation pairs elicited a larger N400 compared to targets in pairs with unrelated English translations. This reversed effect was overtaken by a later, more typical ERP priming effect for deaf signers who were aware of the manipulation. These findings provide evidence that implicit language co-activation in bimodal bilinguals is bidirectional. However, the distinct pattern of effects in deaf and hearing signers suggests that it may be modulated by differences in language proficiency and dominance as well as by asymmetric reliance on orthographic versus phonological representations.",
      "doi": "https://doi.org/10.3390/brainsci9060148",
      "openalex_id": "https://openalex.org/W2950142550",
      "arxiv_id": "",
      "publication_date": "2019-06-21",
      "published": "2019-06-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This ERP study on co-activation of English words during ASL recognition does not involve discrete audio/speech tokenization or token-based representations; it uses visual sign stimuli and ERP effects, so it fails the inclusion criterion and should be excluded under the tokenization-focused criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This ERP study on co-activation of English words during ASL recognition does not involve discrete audio/speech tokenization or token-based representations; it uses visual sign stimuli and ERP effects, so it fails the inclusion criterion and should be excluded under the tokenization-focused criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study investigates cognitive and neurological processes related to American Sign Language and English word co-activation using ERP measures, but it does not address discrete audio/speech tokenization, quantization, or audio signal processing as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study investigates cognitive and neurological processes related to American Sign Language and English word co-activation using ERP measures, but it does not address discrete audio/speech tokenization, quantization, or audio signal processing as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The signed mental lexicon: Effects of phonological neighborhood density, iconicity, and childhood language experience",
    "abstract": "Most of what is known about the mental lexicon comes from studies of spoken language and their written forms. Signs differ from spoken/written words in two important ways that may affect lexical recognition: their phonological composition is unique (e.g., more simultaneous than serial structure; few minimal pairs) and many signs are iconic. Using an unprimed lexical decision task in American Sign Language (ASL) and the first available estimates of phonological neighborhood density for any sign language, we found that phonological neighborhood density had an inhibitory effect on latency among low frequency signs. This is the first clear evidence that phonological neighbors spontaneously compete during sign recognition. Iconicity negatively affected accuracy but not reaction times, suggesting that iconicity plays a role in task-related decision processes but not lexical retrieval. Many deaf signers have delayed first language acquisition, and we found that language deprivation had lasting, negative effects on phonological processing and sign recognition speed and accuracy. This work indicates that the lexicons of both spoken and signed languages are organized by form, that lexical recognition occurs through form-based competition (most evident for low frequency items), and that form-meaning mappings do not drive lexical access even when iconicity is pervasive in the lexicon.",
    "metadata": {
      "title": "The signed mental lexicon: Effects of phonological neighborhood density, iconicity, and childhood language experience",
      "summary": "Most of what is known about the mental lexicon comes from studies of spoken language and their written forms. Signs differ from spoken/written words in two important ways that may affect lexical recognition: their phonological composition is unique (e.g., more simultaneous than serial structure; few minimal pairs) and many signs are iconic. Using an unprimed lexical decision task in American Sign Language (ASL) and the first available estimates of phonological neighborhood density for any sign language, we found that phonological neighborhood density had an inhibitory effect on latency among low frequency signs. This is the first clear evidence that phonological neighbors spontaneously compete during sign recognition. Iconicity negatively affected accuracy but not reaction times, suggesting that iconicity plays a role in task-related decision processes but not lexical retrieval. Many deaf signers have delayed first language acquisition, and we found that language deprivation had lasting, negative effects on phonological processing and sign recognition speed and accuracy. This work indicates that the lexicons of both spoken and signed languages are organized by form, that lexical recognition occurs through form-based competition (most evident for low frequency items), and that form-meaning mappings do not drive lexical access even when iconicity is pervasive in the lexicon.",
      "abstract": "Most of what is known about the mental lexicon comes from studies of spoken language and their written forms. Signs differ from spoken/written words in two important ways that may affect lexical recognition: their phonological composition is unique (e.g., more simultaneous than serial structure; few minimal pairs) and many signs are iconic. Using an unprimed lexical decision task in American Sign Language (ASL) and the first available estimates of phonological neighborhood density for any sign language, we found that phonological neighborhood density had an inhibitory effect on latency among low frequency signs. This is the first clear evidence that phonological neighbors spontaneously compete during sign recognition. Iconicity negatively affected accuracy but not reaction times, suggesting that iconicity plays a role in task-related decision processes but not lexical retrieval. Many deaf signers have delayed first language acquisition, and we found that language deprivation had lasting, negative effects on phonological processing and sign recognition speed and accuracy. This work indicates that the lexicons of both spoken and signed languages are organized by form, that lexical recognition occurs through form-based competition (most evident for low frequency items), and that form-meaning mappings do not drive lexical access even when iconicity is pervasive in the lexicon.",
      "doi": "https://doi.org/10.1016/j.jml.2021.104282",
      "openalex_id": "https://openalex.org/W3188819639",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study concerns sign-language phonology and lexical processing and does not involve discrete audio tokenization or token-based representations, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study concerns sign-language phonology and lexical processing and does not involve discrete audio tokenization or token-based representations, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological neighborhood density, iconicity, and language experience in sign language recognition, without addressing discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it relate to audio token-based language model tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological neighborhood density, iconicity, and language experience in sign language recognition, without addressing discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it relate to audio token-based language model tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neurophysiological Correlates of Frequency, Concreteness, and Iconicity in American Sign Language",
    "abstract": "To investigate possible universal and modality-specific factors that influence the neurophysiological response during lexical processing, we recorded event-related potentials while a large group of deaf adults ( n = 40) viewed 404 signs in American Sign Language (ASL) that varied in ASL frequency, concreteness, and iconicity. Participants performed a go/no-go semantic categorization task (does the sign refer to people?) to videoclips of ASL signs (clips began with the signer’s hands at rest). Linear mixed-effects regression models were fit with per-participant, per-trial, and per-electrode data, allowing us to identify unique effects of each lexical variable. We observed an early effect of frequency (greater negativity for less frequent signs) beginning at 400 ms postvideo onset at anterior sites, which we interpreted as reflecting form-based lexical processing. This effect was followed by a more widely distributed posterior response that we interpreted as reflecting lexical-semantic processing. Paralleling spoken language, more concrete signs elicited greater negativities, beginning 600 ms postvideo onset with a wide scalp distribution. Finally, there were no effects of iconicity (except for a weak effect in the latest epochs; 1,000–1,200 ms), suggesting that iconicity does not modulate the neural response during sign recognition. Despite the perceptual and sensorimotoric differences between signed and spoken languages, the overall results indicate very similar neurophysiological processes underlie lexical access for both signs and words.",
    "metadata": {
      "title": "Neurophysiological Correlates of Frequency, Concreteness, and Iconicity in American Sign Language",
      "summary": "To investigate possible universal and modality-specific factors that influence the neurophysiological response during lexical processing, we recorded event-related potentials while a large group of deaf adults ( n = 40) viewed 404 signs in American Sign Language (ASL) that varied in ASL frequency, concreteness, and iconicity. Participants performed a go/no-go semantic categorization task (does the sign refer to people?) to videoclips of ASL signs (clips began with the signer’s hands at rest). Linear mixed-effects regression models were fit with per-participant, per-trial, and per-electrode data, allowing us to identify unique effects of each lexical variable. We observed an early effect of frequency (greater negativity for less frequent signs) beginning at 400 ms postvideo onset at anterior sites, which we interpreted as reflecting form-based lexical processing. This effect was followed by a more widely distributed posterior response that we interpreted as reflecting lexical-semantic processing. Paralleling spoken language, more concrete signs elicited greater negativities, beginning 600 ms postvideo onset with a wide scalp distribution. Finally, there were no effects of iconicity (except for a weak effect in the latest epochs; 1,000–1,200 ms), suggesting that iconicity does not modulate the neural response during sign recognition. Despite the perceptual and sensorimotoric differences between signed and spoken languages, the overall results indicate very similar neurophysiological processes underlie lexical access for both signs and words.",
      "abstract": "To investigate possible universal and modality-specific factors that influence the neurophysiological response during lexical processing, we recorded event-related potentials while a large group of deaf adults ( n = 40) viewed 404 signs in American Sign Language (ASL) that varied in ASL frequency, concreteness, and iconicity. Participants performed a go/no-go semantic categorization task (does the sign refer to people?) to videoclips of ASL signs (clips began with the signer’s hands at rest). Linear mixed-effects regression models were fit with per-participant, per-trial, and per-electrode data, allowing us to identify unique effects of each lexical variable. We observed an early effect of frequency (greater negativity for less frequent signs) beginning at 400 ms postvideo onset at anterior sites, which we interpreted as reflecting form-based lexical processing. This effect was followed by a more widely distributed posterior response that we interpreted as reflecting lexical-semantic processing. Paralleling spoken language, more concrete signs elicited greater negativities, beginning 600 ms postvideo onset with a wide scalp distribution. Finally, there were no effects of iconicity (except for a weak effect in the latest epochs; 1,000–1,200 ms), suggesting that iconicity does not modulate the neural response during sign recognition. Despite the perceptual and sensorimotoric differences between signed and spoken languages, the overall results indicate very similar neurophysiological processes underlie lexical access for both signs and words.",
      "doi": "https://doi.org/10.1162/nol_a_00012",
      "openalex_id": "https://openalex.org/W3017459497",
      "arxiv_id": "",
      "publication_date": "2020-04-24",
      "published": "2020-04-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates ERP responses to ASL signs and lexical properties but does not involve discrete audio/speech tokenization or token-design for language tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates ERP responses to ASL signs and lexical properties but does not involve discrete audio/speech tokenization or token-design for language tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neurophysiological responses to lexical variables in American Sign Language, without discussing any form of discrete audio/speech tokenization or quantization. It does not address converting continuous audio signals into discrete token sequences, and it is centered on sign language rather than audio signal processing or tokenization methods relevant to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neurophysiological responses to lexical variables in American Sign Language, without discussing any form of discrete audio/speech tokenization or quantization. It does not address converting continuous audio signals into discrete token sequences, and it is centered on sign language rather than audio signal processing or tokenization methods relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data Representativeness in Accessibility Datasets: A Meta-Analysis",
    "abstract": "As data-driven systems are increasingly deployed at scale, ethical concerns have arisen around unfair and discriminatory outcomes for historically marginalized groups that are underrepresented in training data. In response, work around AI fairness and inclusion has called for datasets that are representative of various demographic groups. In this paper, we contribute an analysis of the representativeness of age, gender, and race & ethnicity in accessibility datasets-datasets sourced from people with disabilities and older adults-that can potentially play an important role in mitigating bias for inclusive AI-infused applications. We examine the current state of representation within datasets sourced by people with disabilities by reviewing publicly-available information of 190 datasets, we call these accessibility datasets. We find that accessibility datasets represent diverse ages, but have gender and race representation gaps. Additionally, we investigate how the sensitive and complex nature of demographic variables makes classification difficult and inconsistent (<i>e.g.</i>, gender, race & ethnicity), with the source of labeling often unknown. By reflecting on the current challenges and opportunities for representation of disabled data contributors, we hope our effort expands the space of possibility for greater inclusion of marginalized communities in AI-infused systems.",
    "metadata": {
      "title": "Data Representativeness in Accessibility Datasets: A Meta-Analysis",
      "summary": "As data-driven systems are increasingly deployed at scale, ethical concerns have arisen around unfair and discriminatory outcomes for historically marginalized groups that are underrepresented in training data. In response, work around AI fairness and inclusion has called for datasets that are representative of various demographic groups. In this paper, we contribute an analysis of the representativeness of age, gender, and race & ethnicity in accessibility datasets-datasets sourced from people with disabilities and older adults-that can potentially play an important role in mitigating bias for inclusive AI-infused applications. We examine the current state of representation within datasets sourced by people with disabilities by reviewing publicly-available information of 190 datasets, we call these accessibility datasets. We find that accessibility datasets represent diverse ages, but have gender and race representation gaps. Additionally, we investigate how the sensitive and complex nature of demographic variables makes classification difficult and inconsistent (<i>e.g.</i>, gender, race & ethnicity), with the source of labeling often unknown. By reflecting on the current challenges and opportunities for representation of disabled data contributors, we hope our effort expands the space of possibility for greater inclusion of marginalized communities in AI-infused systems.",
      "abstract": "As data-driven systems are increasingly deployed at scale, ethical concerns have arisen around unfair and discriminatory outcomes for historically marginalized groups that are underrepresented in training data. In response, work around AI fairness and inclusion has called for datasets that are representative of various demographic groups. In this paper, we contribute an analysis of the representativeness of age, gender, and race & ethnicity in accessibility datasets-datasets sourced from people with disabilities and older adults-that can potentially play an important role in mitigating bias for inclusive AI-infused applications. We examine the current state of representation within datasets sourced by people with disabilities by reviewing publicly-available information of 190 datasets, we call these accessibility datasets. We find that accessibility datasets represent diverse ages, but have gender and race representation gaps. Additionally, we investigate how the sensitive and complex nature of demographic variables makes classification difficult and inconsistent (<i>e.g.</i>, gender, race & ethnicity), with the source of labeling often unknown. By reflecting on the current challenges and opportunities for representation of disabled data contributors, we hope our effort expands the space of possibility for greater inclusion of marginalized communities in AI-infused systems.",
      "doi": "https://doi.org/10.1145/3517428.3544826",
      "openalex_id": "https://openalex.org/W4297888912",
      "arxiv_id": "",
      "publication_date": "2022-10-22",
      "published": "2022-10-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes demographic representativeness in accessibility datasets and fairness, but it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes demographic representativeness in accessibility datasets and fairness, but it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing demographic representativeness in accessibility datasets involving disabled and older adults but does not discuss or analyze discrete audio/speech tokenization or any form of audio signal discrete quantization/token design, which are core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing demographic representativeness in accessibility datasets involving disabled and older adults but does not discuss or analyze discrete audio/speech tokenization or any form of audio signal discrete quantization/token design, which are core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign2Pose: A Pose-Based Approach for Gloss Prediction Using a Transformer Model",
    "abstract": "Word-level sign language recognition (WSLR) is the backbone for continuous sign language recognition (CSLR) that infers glosses from sign videos. Finding the relevant gloss from the sign sequence and detecting explicit boundaries of the glosses from sign videos is a persistent challenge. In this paper, we propose a systematic approach for gloss prediction in WLSR using the Sign2Pose Gloss prediction transformer model. The primary goal of this work is to enhance WLSR’s gloss prediction accuracy with reduced time and computational overhead. The proposed approach uses hand-crafted features rather than automated feature extraction, which is computationally expensive and less accurate. A modified key frame extraction technique is proposed that uses histogram difference and Euclidean distance metrics to select and drop redundant frames. To enhance the model’s generalization ability, pose vector augmentation using perspective transformation along with joint angle rotation is performed. Further, for normalization, we employed YOLOv3 (You Only Look Once) to detect the signing space and track the hand gestures of the signers in the frames. The proposed model experiments on WLASL datasets achieved the top 1% recognition accuracy of 80.9% in WLASL100 and 64.21% in WLASL300. The performance of the proposed model surpasses state-of-the-art approaches. The integration of key frame extraction, augmentation, and pose estimation improved the performance of the proposed gloss prediction model by increasing the model’s precision in locating minor variations in their body posture. We observed that introducing YOLOv3 improved gloss prediction accuracy and helped prevent model overfitting. Overall, the proposed model showed 17% improved performance in the WLASL 100 dataset.",
    "metadata": {
      "title": "Sign2Pose: A Pose-Based Approach for Gloss Prediction Using a Transformer Model",
      "summary": "Word-level sign language recognition (WSLR) is the backbone for continuous sign language recognition (CSLR) that infers glosses from sign videos. Finding the relevant gloss from the sign sequence and detecting explicit boundaries of the glosses from sign videos is a persistent challenge. In this paper, we propose a systematic approach for gloss prediction in WLSR using the Sign2Pose Gloss prediction transformer model. The primary goal of this work is to enhance WLSR’s gloss prediction accuracy with reduced time and computational overhead. The proposed approach uses hand-crafted features rather than automated feature extraction, which is computationally expensive and less accurate. A modified key frame extraction technique is proposed that uses histogram difference and Euclidean distance metrics to select and drop redundant frames. To enhance the model’s generalization ability, pose vector augmentation using perspective transformation along with joint angle rotation is performed. Further, for normalization, we employed YOLOv3 (You Only Look Once) to detect the signing space and track the hand gestures of the signers in the frames. The proposed model experiments on WLASL datasets achieved the top 1% recognition accuracy of 80.9% in WLASL100 and 64.21% in WLASL300. The performance of the proposed model surpasses state-of-the-art approaches. The integration of key frame extraction, augmentation, and pose estimation improved the performance of the proposed gloss prediction model by increasing the model’s precision in locating minor variations in their body posture. We observed that introducing YOLOv3 improved gloss prediction accuracy and helped prevent model overfitting. Overall, the proposed model showed 17% improved performance in the WLASL 100 dataset.",
      "abstract": "Word-level sign language recognition (WSLR) is the backbone for continuous sign language recognition (CSLR) that infers glosses from sign videos. Finding the relevant gloss from the sign sequence and detecting explicit boundaries of the glosses from sign videos is a persistent challenge. In this paper, we propose a systematic approach for gloss prediction in WLSR using the Sign2Pose Gloss prediction transformer model. The primary goal of this work is to enhance WLSR’s gloss prediction accuracy with reduced time and computational overhead. The proposed approach uses hand-crafted features rather than automated feature extraction, which is computationally expensive and less accurate. A modified key frame extraction technique is proposed that uses histogram difference and Euclidean distance metrics to select and drop redundant frames. To enhance the model’s generalization ability, pose vector augmentation using perspective transformation along with joint angle rotation is performed. Further, for normalization, we employed YOLOv3 (You Only Look Once) to detect the signing space and track the hand gestures of the signers in the frames. The proposed model experiments on WLASL datasets achieved the top 1% recognition accuracy of 80.9% in WLASL100 and 64.21% in WLASL300. The performance of the proposed model surpasses state-of-the-art approaches. The integration of key frame extraction, augmentation, and pose estimation improved the performance of the proposed gloss prediction model by increasing the model’s precision in locating minor variations in their body posture. We observed that introducing YOLOv3 improved gloss prediction accuracy and helped prevent model overfitting. Overall, the proposed model showed 17% improved performance in the WLASL 100 dataset.",
      "doi": "https://doi.org/10.3390/s23052853",
      "openalex_id": "https://openalex.org/W4323314027",
      "arxiv_id": "",
      "publication_date": "2023-03-06",
      "published": "2023-03-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study is about sign-language gloss prediction from video/pose data and does not involve discrete audio/speech tokenization or token representations for audio; thus it does not meet the inclusion criterion focused on discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study is about sign-language gloss prediction from video/pose data and does not involve discrete audio/speech tokenization or token representations for audio; thus it does not meet the inclusion criterion focused on discrete audio tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on sign language recognition using pose-based gloss prediction and Transformer models, without any reference to discrete audio or speech tokenization; it does not address the conversion of continuous audio signals into discrete audio tokens, which is a required inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on sign language recognition using pose-based gloss prediction and Transformer models, without any reference to discrete audio or speech tokenization; it does not address the conversion of continuous audio signals into discrete audio tokens, which is a required inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attitudes Toward Signing Avatars Vary Depending on Hearing Status, Age of Signed Language Acquisition, and Avatar Type",
    "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer's preferred or native language. As the development of signing avatars has gained traction in recent years, researchers have come up with many different methods of creating signing avatars. The resulting avatars vary widely in their appearance, the naturalness of their movements, and facial expressions—all of which may potentially impact users' acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars while also examining the extent to which people's own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by (1) a live human signer (Human), (2) an avatar made using computer-synthesized animation (CS Avatar), and (3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf ( N = 83), Hard-of-Hearing ( N = 34), and Hearing ( N = 67) groups. Participants rated the three signers on multiple dimensions, which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but we did not see this association for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users' ratings of signing avatars and show that signed language users with earlier age of ASL acquisition are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars consider retaining the fluid movement qualities integral to signed languages.",
    "metadata": {
      "title": "Attitudes Toward Signing Avatars Vary Depending on Hearing Status, Age of Signed Language Acquisition, and Avatar Type",
      "summary": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer's preferred or native language. As the development of signing avatars has gained traction in recent years, researchers have come up with many different methods of creating signing avatars. The resulting avatars vary widely in their appearance, the naturalness of their movements, and facial expressions—all of which may potentially impact users' acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars while also examining the extent to which people's own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by (1) a live human signer (Human), (2) an avatar made using computer-synthesized animation (CS Avatar), and (3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf ( N = 83), Hard-of-Hearing ( N = 34), and Hearing ( N = 67) groups. Participants rated the three signers on multiple dimensions, which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but we did not see this association for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users' ratings of signing avatars and show that signed language users with earlier age of ASL acquisition are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars consider retaining the fluid movement qualities integral to signed languages.",
      "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer's preferred or native language. As the development of signing avatars has gained traction in recent years, researchers have come up with many different methods of creating signing avatars. The resulting avatars vary widely in their appearance, the naturalness of their movements, and facial expressions—all of which may potentially impact users' acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars while also examining the extent to which people's own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by (1) a live human signer (Human), (2) an avatar made using computer-synthesized animation (CS Avatar), and (3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf ( N = 83), Hard-of-Hearing ( N = 34), and Hearing ( N = 67) groups. Participants rated the three signers on multiple dimensions, which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but we did not see this association for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users' ratings of signing avatars and show that signed language users with earlier age of ASL acquisition are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars consider retaining the fluid movement qualities integral to signed languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.730917",
      "openalex_id": "https://openalex.org/W4211140127",
      "arxiv_id": "",
      "publication_date": "2022-02-10",
      "published": "2022-02-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates user attitudes toward signing avatars and their movement/visual quality, with no analysis or proposal of discrete audio tokenization for speech tasks, so it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates user attitudes toward signing avatars and their movement/visual quality, with no analysis or proposal of discrete audio tokenization for speech tasks, so it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating attitudes toward signing avatars in signed language users and does not propose, analyze, or evaluate discrete audio/speech tokenization methods or discrete audio representations; hence, it does not meet the inclusion criteria related to discrete audio tokenization and is outside the scope of audio signal tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating attitudes toward signing avatars in signed language users and does not propose, analyze, or evaluate discrete audio/speech tokenization methods or discrete audio representations; hence, it does not meet the inclusion criteria related to discrete audio tokenization and is outside the scope of audio signal tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effect of Automatic Sign Recognition Performance on the Usability of Video-Based Search Interfaces for Sign Language Dictionaries",
    "abstract": "Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.",
    "metadata": {
      "title": "Effect of Automatic Sign Recognition Performance on the Usability of Video-Based Search Interfaces for Sign Language Dictionaries",
      "summary": "Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.",
      "abstract": "Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.",
      "doi": "https://doi.org/10.1145/3308561.3353791",
      "openalex_id": "https://openalex.org/W2982201827",
      "arxiv_id": "",
      "publication_date": "2019-10-24",
      "published": "2019-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates automatic sign recognition performance in a video-based ASL dictionary search interface and user usability, which centers on visual gesture recognition rather than discrete audio/speech tokenization of continuous audio into a finite vocabulary. It does not analyze or evaluate discrete audio tokens, and thus does not meet the specified inclusion criteria. Therefore it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item investigates automatic sign recognition performance in a video-based ASL dictionary search interface and user usability, which centers on visual gesture recognition rather than discrete audio/speech tokenization of continuous audio into a finite vocabulary. It does not analyze or evaluate discrete audio tokens, and thus does not meet the specified inclusion criteria. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition and usability of search interfaces but does not involve discrete audio/speech tokenization, nor does it analyze discrete token representations of audio signals as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition and usability of search interfaces but does not involve discrete audio/speech tokenization, nor does it analyze discrete token representations of audio signals as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CV-SincNet: Learning Complex Sinc Filters From Raw Radar Data for Computationally Efficient Human Motion Recognition",
    "abstract": "The utilization of radio-frequency (RF) sensing in cyber-physical human systems, such as human-computer interfaces or smart environments, is an emerging application that requires real-time human motion recognition. However, current state-of-the-art radar-based recognition techniques rely on computing various RF data representations, such as range-Doppler or range-Angle maps, micro-Doppler signatures, or higher dimensional representations, which have great computational complexity. Consequently, classification of raw radar data has garnered increasing interest, while remaining limited in the accuracy that can be attained for recognition of even simple gross motor activities. To help address this challenge, this paper proposes a more interpretable complex-valued neural network design. Complex sinc filters are designed to learn frequency-based relationships directly from the complex raw radar data in the initial layer of the proposed model. The complex-valued sinc layer consists of windowed band-pass filters that learn the center frequency and bandwidth of each filter. A challenging RF dataset consisting of 100 words from American Sign Language (ASL) is selected to verify the model. About 40% improvement in classification accuracy was achieved over the application of a 1D CNN on raw RF data, while 8% improvement was achieved compared to real-valued SincNet. Our proposed approach achieved a 4% improvement in accuracy over that attained with a 2D CNN applied to micro-Doppler spectrograms, while also reducing the overall computational latency by 71%.",
    "metadata": {
      "title": "CV-SincNet: Learning Complex Sinc Filters From Raw Radar Data for Computationally Efficient Human Motion Recognition",
      "summary": "The utilization of radio-frequency (RF) sensing in cyber-physical human systems, such as human-computer interfaces or smart environments, is an emerging application that requires real-time human motion recognition. However, current state-of-the-art radar-based recognition techniques rely on computing various RF data representations, such as range-Doppler or range-Angle maps, micro-Doppler signatures, or higher dimensional representations, which have great computational complexity. Consequently, classification of raw radar data has garnered increasing interest, while remaining limited in the accuracy that can be attained for recognition of even simple gross motor activities. To help address this challenge, this paper proposes a more interpretable complex-valued neural network design. Complex sinc filters are designed to learn frequency-based relationships directly from the complex raw radar data in the initial layer of the proposed model. The complex-valued sinc layer consists of windowed band-pass filters that learn the center frequency and bandwidth of each filter. A challenging RF dataset consisting of 100 words from American Sign Language (ASL) is selected to verify the model. About 40% improvement in classification accuracy was achieved over the application of a 1D CNN on raw RF data, while 8% improvement was achieved compared to real-valued SincNet. Our proposed approach achieved a 4% improvement in accuracy over that attained with a 2D CNN applied to micro-Doppler spectrograms, while also reducing the overall computational latency by 71%.",
      "abstract": "The utilization of radio-frequency (RF) sensing in cyber-physical human systems, such as human-computer interfaces or smart environments, is an emerging application that requires real-time human motion recognition. However, current state-of-the-art radar-based recognition techniques rely on computing various RF data representations, such as range-Doppler or range-Angle maps, micro-Doppler signatures, or higher dimensional representations, which have great computational complexity. Consequently, classification of raw radar data has garnered increasing interest, while remaining limited in the accuracy that can be attained for recognition of even simple gross motor activities. To help address this challenge, this paper proposes a more interpretable complex-valued neural network design. Complex sinc filters are designed to learn frequency-based relationships directly from the complex raw radar data in the initial layer of the proposed model. The complex-valued sinc layer consists of windowed band-pass filters that learn the center frequency and bandwidth of each filter. A challenging RF dataset consisting of 100 words from American Sign Language (ASL) is selected to verify the model. About 40% improvement in classification accuracy was achieved over the application of a 1D CNN on raw RF data, while 8% improvement was achieved compared to real-valued SincNet. Our proposed approach achieved a 4% improvement in accuracy over that attained with a 2D CNN applied to micro-Doppler spectrograms, while also reducing the overall computational latency by 71%.",
      "doi": "https://doi.org/10.1109/trs.2023.3310894",
      "openalex_id": "https://openalex.org/W4386320441",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on learning complex Sinc filters from raw radar data for human motion recognition and does not propose or analyze discrete audio/speech tokenization or token sequences; it is not about speech tasks or discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on learning complex Sinc filters from raw radar data for human motion recognition and does not propose or analyze discrete audio/speech tokenization or token sequences; it is not about speech tasks or discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on radar-based human motion recognition using complex-valued neural networks on raw radar data; it does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete quantization/token design related to converting continuous audio signals into discrete tokens, nor does it address audio reconstruction, generation, or understanding tasks based on such tokenization, making it inapplicable to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on radar-based human motion recognition using complex-valued neural networks on raw radar data; it does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete quantization/token design related to converting continuous audio signals into discrete tokens, nor does it address audio reconstruction, generation, or understanding tasks based on such tokenization, making it inapplicable to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition With RF Sensors",
    "abstract": "RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.",
    "metadata": {
      "title": "Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition With RF Sensors",
      "summary": "RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.",
      "abstract": "RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.",
      "doi": "https://doi.org/10.1109/taes.2021.3139848",
      "openalex_id": "https://openalex.org/W4206328226",
      "arxiv_id": "",
      "publication_date": "2022-01-04",
      "published": "2022-01-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates RF-based sensing of signing kinematics and adversarial data generation; it does not address discrete audio/speech tokenization or token-level representations, so it does not meet the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates RF-based sensing of signing kinematics and adversarial data generation; it does not address discrete audio/speech tokenization or token-level representations, so it does not meet the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on RF sensor data for American Sign Language recognition, analyzing kinematic properties and synthetic data generation via adversarial learning, without any mention of audio/speech tokenization, discrete audio symbols, or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization and relates to a different modality (sign language motion data) rather than audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on RF sensor data for American Sign Language recognition, analyzing kinematic properties and synthetic data generation via adversarial learning, without any mention of audio/speech tokenization, discrete audio symbols, or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization and relates to a different modality (sign language motion data) rather than audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos",
    "abstract": "The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"translate\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.",
    "metadata": {
      "title": "Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos",
      "summary": "The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"translate\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.",
      "abstract": "The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"translate\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.",
      "doi": "https://doi.org/10.1145/3596261",
      "openalex_id": "https://openalex.org/W4380361332",
      "arxiv_id": "",
      "publication_date": "2023-06-12",
      "published": "2023-06-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item deals with translating ASL video to wrist IMU data for gesture recognition and does not involve discrete audio tokenization, so it fails the inclusion criteria demanding discrete audio/speech tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item deals with translating ASL video to wrist IMU data for gesture recognition and does not involve discrete audio tokenization, so it fails the inclusion criteria demanding discrete audio/speech tokenization analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating synthetic IMU data from ASL videos, which relates to motion data modeling rather than audio/speech tokenization; it does not address discrete audio tokenization, quantization, or analysis of audio tokens for speech-related tasks, and therefore does not meet inclusion criteria centered on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating synthetic IMU data from ASL videos, which relates to motion data modeling rather than audio/speech tokenization; it does not address discrete audio tokenization, quantization, or analysis of audio tokens for speech-related tasks, and therefore does not meet inclusion criteria centered on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Iconicity ratings really do measure iconicity, and they open a new window onto the nature of language",
    "abstract": "Abstract This paper reviews recent research using participant ratings to measure the iconicity (form-meaning resemblance) of words and signs. This method, by enabling wide coverage of lexical items and cross-linguistic comparison, has revealed systematic patterns in how iconicity is distributed across the vocabularies of different languages. These findings are consistent with established linguistic and psychological theory on iconicity, and they connect iconicity to factors like learning and acquisition, semantics, pragmatic aspects of language like playfulness, and to the semantic neighborhood density of words and signs. After taking stock of this research, we look critically at the construct validity of iconicity ratings, considering an alternative account of iconicity ratings recently put forward by Thompson, Arthur Lewis, Kimi Akita &amp; Youngah Do. 2020a. Iconicity ratings across the Japanese lexicon: A comparative study with English. Linguistics Vanguard 6. 20190088. They propose that, for most vocabulary, participants might rate the iconicity of different words based on their meaning alone – specifically the degree to which it relates to the senses – independently of actual form-meaning resemblance. We argue that their hypothesis cannot account for many of the various, theory-driven results from this line of research, which strongly support the conclusion that the ratings really do measure iconicity.",
    "metadata": {
      "title": "Iconicity ratings really do measure iconicity, and they open a new window onto the nature of language",
      "summary": "Abstract This paper reviews recent research using participant ratings to measure the iconicity (form-meaning resemblance) of words and signs. This method, by enabling wide coverage of lexical items and cross-linguistic comparison, has revealed systematic patterns in how iconicity is distributed across the vocabularies of different languages. These findings are consistent with established linguistic and psychological theory on iconicity, and they connect iconicity to factors like learning and acquisition, semantics, pragmatic aspects of language like playfulness, and to the semantic neighborhood density of words and signs. After taking stock of this research, we look critically at the construct validity of iconicity ratings, considering an alternative account of iconicity ratings recently put forward by Thompson, Arthur Lewis, Kimi Akita &amp; Youngah Do. 2020a. Iconicity ratings across the Japanese lexicon: A comparative study with English. Linguistics Vanguard 6. 20190088. They propose that, for most vocabulary, participants might rate the iconicity of different words based on their meaning alone – specifically the degree to which it relates to the senses – independently of actual form-meaning resemblance. We argue that their hypothesis cannot account for many of the various, theory-driven results from this line of research, which strongly support the conclusion that the ratings really do measure iconicity.",
      "abstract": "Abstract This paper reviews recent research using participant ratings to measure the iconicity (form-meaning resemblance) of words and signs. This method, by enabling wide coverage of lexical items and cross-linguistic comparison, has revealed systematic patterns in how iconicity is distributed across the vocabularies of different languages. These findings are consistent with established linguistic and psychological theory on iconicity, and they connect iconicity to factors like learning and acquisition, semantics, pragmatic aspects of language like playfulness, and to the semantic neighborhood density of words and signs. After taking stock of this research, we look critically at the construct validity of iconicity ratings, considering an alternative account of iconicity ratings recently put forward by Thompson, Arthur Lewis, Kimi Akita &amp; Youngah Do. 2020a. Iconicity ratings across the Japanese lexicon: A comparative study with English. Linguistics Vanguard 6. 20190088. They propose that, for most vocabulary, participants might rate the iconicity of different words based on their meaning alone – specifically the degree to which it relates to the senses – independently of actual form-meaning resemblance. We argue that their hypothesis cannot account for many of the various, theory-driven results from this line of research, which strongly support the conclusion that the ratings really do measure iconicity.",
      "doi": "https://doi.org/10.1515/lingvan-2020-0135",
      "openalex_id": "https://openalex.org/W3173157255",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses iconicity ratings for words/signs and evaluates construct validity; it does not address discrete audio/speech tokenization as a core object of study, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses iconicity ratings for words/signs and evaluates construct validity; it does not address discrete audio/speech tokenization as a core object of study, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on iconicity ratings of words and signs related to language and cognition, without any mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for speech-related tasks. Therefore, it does not meet the inclusion criteria, nor does it touch on any discrete tokenization methods relevant to audio or speech processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on iconicity ratings of words and signs related to language and cognition, without any mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for speech-related tasks. Therefore, it does not meet the inclusion criteria, nor does it touch on any discrete tokenization methods relevant to audio or speech processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper",
    "abstract": "Language processing is influenced by sensorimotor experiences. Here, we review behavioral evidence for embodied and grounded influences in language processing across six linguistic levels of granularity. We examine (a) sub-word features, discussing grounded influences on iconicity (systematic associations between word form and meaning); (b) words, discussing boundary conditions and generalizations for the simulation of color, sensory modality, and spatial position; (c) sentences, discussing boundary conditions and applications of action direction simulation; (d) texts, discussing how the teaching of simulation can improve comprehension in beginning readers; (e) conversations, discussing how multi-modal cues improve turn taking and alignment; and (f) text corpora, discussing how distributional semantic models can reveal how grounded and embodied knowledge is encoded in texts. These approaches are converging on a convincing account of the psychology of language, but at the same time, there are important criticisms of the embodied approach and of specific experimental paradigms. The surest way forward requires the adoption of a wide array of scientific methods. By providing complimentary evidence, a combination of multiple methods on various levels of granularity can help us gain a more complete understanding of the role of embodiment and grounding in language processing.",
    "metadata": {
      "title": "Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper",
      "summary": "Language processing is influenced by sensorimotor experiences. Here, we review behavioral evidence for embodied and grounded influences in language processing across six linguistic levels of granularity. We examine (a) sub-word features, discussing grounded influences on iconicity (systematic associations between word form and meaning); (b) words, discussing boundary conditions and generalizations for the simulation of color, sensory modality, and spatial position; (c) sentences, discussing boundary conditions and applications of action direction simulation; (d) texts, discussing how the teaching of simulation can improve comprehension in beginning readers; (e) conversations, discussing how multi-modal cues improve turn taking and alignment; and (f) text corpora, discussing how distributional semantic models can reveal how grounded and embodied knowledge is encoded in texts. These approaches are converging on a convincing account of the psychology of language, but at the same time, there are important criticisms of the embodied approach and of specific experimental paradigms. The surest way forward requires the adoption of a wide array of scientific methods. By providing complimentary evidence, a combination of multiple methods on various levels of granularity can help us gain a more complete understanding of the role of embodiment and grounding in language processing.",
      "abstract": "Language processing is influenced by sensorimotor experiences. Here, we review behavioral evidence for embodied and grounded influences in language processing across six linguistic levels of granularity. We examine (a) sub-word features, discussing grounded influences on iconicity (systematic associations between word form and meaning); (b) words, discussing boundary conditions and generalizations for the simulation of color, sensory modality, and spatial position; (c) sentences, discussing boundary conditions and applications of action direction simulation; (d) texts, discussing how the teaching of simulation can improve comprehension in beginning readers; (e) conversations, discussing how multi-modal cues improve turn taking and alignment; and (f) text corpora, discussing how distributional semantic models can reveal how grounded and embodied knowledge is encoded in texts. These approaches are converging on a convincing account of the psychology of language, but at the same time, there are important criticisms of the embodied approach and of specific experimental paradigms. The surest way forward requires the adoption of a wide array of scientific methods. By providing complimentary evidence, a combination of multiple methods on various levels of granularity can help us gain a more complete understanding of the role of embodiment and grounding in language processing.",
      "doi": "https://doi.org/10.5334/joc.231",
      "openalex_id": "https://openalex.org/W4387507685",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is a theoretical/empirical review of embodied language processing across six granularity levels and discusses sensorimotor grounding; it does not propose, analyze, or evaluate discrete audio tokenization or discrete token representations for speech.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is a theoretical/empirical review of embodied language processing across six granularity levels and discusses sensorimotor grounding; it does not propose, analyze, or evaluate discrete audio tokenization or discrete token representations for speech.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on embodied language processing across multiple linguistic levels and does not mention any discrete audio/speech tokenization, quantization, or related neural encoding methods; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to discrete token representations or analyses in speech.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on embodied language processing across multiple linguistic levels and does not mention any discrete audio/speech tokenization, quantization, or related neural encoding methods; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to discrete token representations or analyses in speech.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Impoverished language in early childhood affects the development of complex sentence structure",
    "abstract": "Abstract The hypothesis that impoverished language experience affects complex sentence structure development around the end of early childhood was tested using a fully randomized, sentence‐to‐picture matching study in American Sign Language (ASL). The participants were ASL signers who had impoverished or typical access to language in early childhood. Deaf signers whose access to language was highly impoverished in early childhood ( N = 11) primarily comprehended structures consisting of a single verb and argument (Subject or Object), agreeing verbs, and the spatial relation or path of semantic classifiers. They showed difficulty comprehending more complex sentence structures involving dual lexical arguments or multiple verbs. As predicted, participants with typical language access in early childhood, deaf native signers ( N = 17) or hearing second‐language learners ( N = 10), comprehended the range of 12 ASL sentence structures, independent of the subjective iconicity or frequency of the stimulus lexical items, or length of ASL experience and performance on non‐verbal cognitive tasks. The results show that language experience in early childhood is necessary for the development of complex syntax. Research Highlights Previous research with deaf signers suggests an inflection point around the end of early childhood for sentence structure development. Deaf signers who experienced impoverished language until the age of 9 or older comprehend several basic sentence structures but few complex structures. Language experience in early childhood is necessary for the development of complex sentence structure.",
    "metadata": {
      "title": "Impoverished language in early childhood affects the development of complex sentence structure",
      "summary": "Abstract The hypothesis that impoverished language experience affects complex sentence structure development around the end of early childhood was tested using a fully randomized, sentence‐to‐picture matching study in American Sign Language (ASL). The participants were ASL signers who had impoverished or typical access to language in early childhood. Deaf signers whose access to language was highly impoverished in early childhood ( N = 11) primarily comprehended structures consisting of a single verb and argument (Subject or Object), agreeing verbs, and the spatial relation or path of semantic classifiers. They showed difficulty comprehending more complex sentence structures involving dual lexical arguments or multiple verbs. As predicted, participants with typical language access in early childhood, deaf native signers ( N = 17) or hearing second‐language learners ( N = 10), comprehended the range of 12 ASL sentence structures, independent of the subjective iconicity or frequency of the stimulus lexical items, or length of ASL experience and performance on non‐verbal cognitive tasks. The results show that language experience in early childhood is necessary for the development of complex syntax. Research Highlights Previous research with deaf signers suggests an inflection point around the end of early childhood for sentence structure development. Deaf signers who experienced impoverished language until the age of 9 or older comprehend several basic sentence structures but few complex structures. Language experience in early childhood is necessary for the development of complex sentence structure.",
      "abstract": "Abstract The hypothesis that impoverished language experience affects complex sentence structure development around the end of early childhood was tested using a fully randomized, sentence‐to‐picture matching study in American Sign Language (ASL). The participants were ASL signers who had impoverished or typical access to language in early childhood. Deaf signers whose access to language was highly impoverished in early childhood ( N = 11) primarily comprehended structures consisting of a single verb and argument (Subject or Object), agreeing verbs, and the spatial relation or path of semantic classifiers. They showed difficulty comprehending more complex sentence structures involving dual lexical arguments or multiple verbs. As predicted, participants with typical language access in early childhood, deaf native signers ( N = 17) or hearing second‐language learners ( N = 10), comprehended the range of 12 ASL sentence structures, independent of the subjective iconicity or frequency of the stimulus lexical items, or length of ASL experience and performance on non‐verbal cognitive tasks. The results show that language experience in early childhood is necessary for the development of complex syntax. Research Highlights Previous research with deaf signers suggests an inflection point around the end of early childhood for sentence structure development. Deaf signers who experienced impoverished language until the age of 9 or older comprehend several basic sentence structures but few complex structures. Language experience in early childhood is necessary for the development of complex sentence structure.",
      "doi": "https://doi.org/10.1111/desc.13416",
      "openalex_id": "https://openalex.org/W4378783225",
      "arxiv_id": "",
      "publication_date": "2023-05-31",
      "published": "2023-05-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study concerns ASL language exposure and the development of complex syntax and does not address discrete audio/speech tokenization or token-quantization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study concerns ASL language exposure and the development of complex syntax and does not address discrete audio/speech tokenization or token-quantization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on language development and comprehension in deaf signers and does not discuss or analyze discrete audio/speech tokenization or any conversion of continuous audio signals into discrete tokens, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on language development and comprehension in deaf signers and does not discuss or analyze discrete audio/speech tokenization or any conversion of continuous audio signals into discrete tokens, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Complex SincNet for More Interpretable Radar Based Activity Recognition",
    "abstract": "Radio frequency (RF) sensing has been increasingly used in many applications such as fall-motion recognition, human-machine interfacing, gesture controlled home appliances, and American sign language (ASL) recognition. However, most current activity classification techniques employ a two-stage process. First, the micro-Doppler spectrograms or a temporal sequence of range-Doppler (RD) maps are created. Then, in the second stage, the created representation is used with deep learning (DL) or machine learning (ML) techniques. In this paper, we propose a more interpretable complex-valued neural network architecture to directly classify human activities from the complex-valued raw radar data. The one-dimensional (1D) slow-time radar data is used as the raw radar input, and the complex sinc function is used as the first layer of the proposed model. The sinc filter is a windowed band-pass filter in which the model learns only the lower and higher frequencies of the bandpass filter. To verify our model, an American Sign Language (ASL) dataset consisting of 15 activity classes is selected. The proposed complex-valued SincNet (CV-SincNet) provides higher classification accuracy compared to direct application of a convolutional neural network (CNN) or real-valued SincNet. Enhanced results are observed compared to standard CNN models applied on the Micro-Doppler spectrogram images.",
    "metadata": {
      "title": "Complex SincNet for More Interpretable Radar Based Activity Recognition",
      "summary": "Radio frequency (RF) sensing has been increasingly used in many applications such as fall-motion recognition, human-machine interfacing, gesture controlled home appliances, and American sign language (ASL) recognition. However, most current activity classification techniques employ a two-stage process. First, the micro-Doppler spectrograms or a temporal sequence of range-Doppler (RD) maps are created. Then, in the second stage, the created representation is used with deep learning (DL) or machine learning (ML) techniques. In this paper, we propose a more interpretable complex-valued neural network architecture to directly classify human activities from the complex-valued raw radar data. The one-dimensional (1D) slow-time radar data is used as the raw radar input, and the complex sinc function is used as the first layer of the proposed model. The sinc filter is a windowed band-pass filter in which the model learns only the lower and higher frequencies of the bandpass filter. To verify our model, an American Sign Language (ASL) dataset consisting of 15 activity classes is selected. The proposed complex-valued SincNet (CV-SincNet) provides higher classification accuracy compared to direct application of a convolutional neural network (CNN) or real-valued SincNet. Enhanced results are observed compared to standard CNN models applied on the Micro-Doppler spectrogram images.",
      "abstract": "Radio frequency (RF) sensing has been increasingly used in many applications such as fall-motion recognition, human-machine interfacing, gesture controlled home appliances, and American sign language (ASL) recognition. However, most current activity classification techniques employ a two-stage process. First, the micro-Doppler spectrograms or a temporal sequence of range-Doppler (RD) maps are created. Then, in the second stage, the created representation is used with deep learning (DL) or machine learning (ML) techniques. In this paper, we propose a more interpretable complex-valued neural network architecture to directly classify human activities from the complex-valued raw radar data. The one-dimensional (1D) slow-time radar data is used as the raw radar input, and the complex sinc function is used as the first layer of the proposed model. The sinc filter is a windowed band-pass filter in which the model learns only the lower and higher frequencies of the bandpass filter. To verify our model, an American Sign Language (ASL) dataset consisting of 15 activity classes is selected. The proposed complex-valued SincNet (CV-SincNet) provides higher classification accuracy compared to direct application of a convolutional neural network (CNN) or real-valued SincNet. Enhanced results are observed compared to standard CNN models applied on the Micro-Doppler spectrogram images.",
      "doi": "https://doi.org/10.1109/radarconf2351548.2023.10149682",
      "openalex_id": "https://openalex.org/W4381745561",
      "arxiv_id": "",
      "publication_date": "2023-05-01",
      "published": "2023-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes radar-based activity recognition with a complex-valued neural network and does not involve discrete audio tokenization or discrete audio representations, so it does not meet the inclusion criteria and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes radar-based activity recognition with a complex-valued neural network and does not involve discrete audio tokenization or discrete audio representations, so it does not meet the inclusion criteria and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on radar-based human activity recognition using complex-valued neural networks directly on radar data, not on discrete audio/speech tokenization or with an emphasis on converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization or speech-related token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on radar-based human activity recognition using complex-valued neural networks directly on radar data, not on discrete audio/speech tokenization or with an emphasis on converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization or speech-related token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Introduction and Analysis of an Event-Based Sign Language Dataset",
    "abstract": "Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.",
    "metadata": {
      "title": "Introduction and Analysis of an Event-Based Sign Language Dataset",
      "summary": "Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.",
      "abstract": "Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.",
      "doi": "https://doi.org/10.1109/fg47880.2020.00069",
      "openalex_id": "https://openalex.org/W3128431487",
      "arxiv_id": "",
      "publication_date": "2020-11-01",
      "published": "2020-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses an event-based visual dataset of sign language gestures using a Dynamic Vision Sensor (DVS) and SNNs; it does not address discrete audio/speech tokenization, thus it fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses an event-based visual dataset of sign language gestures using a Dynamic Vision Sensor (DVS) and SNNs; it does not address discrete audio/speech tokenization, thus it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces an event-based sign language dataset using visual Dynamic Vision Sensor data and benchmarks recognition using Spiking Neural Networks; it does not focus on discrete audio/speech tokenization or any audio signal processing tasks, so it fails to meet the inclusion criteria related to audio tokenization and is unrelated to audio tokenization methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces an event-based sign language dataset using visual Dynamic Vision Sensor data and benchmarks recognition using Spiking Neural Networks; it does not focus on discrete audio/speech tokenization or any audio signal processing tasks, so it fails to meet the inclusion criteria related to audio tokenization and is unrelated to audio tokenization methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonological and semantic priming in American Sign Language: N300 and N400 effects",
    "abstract": "This study investigated the electrophysiological signatures of phonological and semantic priming in American Sign Language (ASL). Deaf signers made semantic relatedness judgments to pairs of ASL signs separated by a 1300 ms prime-target SOA. Phonologically related sign pairs shared two of three phonological parameters (handshape, location, and movement). Target signs preceded by phonologically related and semantically related prime signs elicited smaller negativities within the N300 and N400 windows than those preceded by unrelated primes. N300 effects, typically reported in studies of picture processing, are interpreted to reflect the mapping from the visual features of the signs to more abstract linguistic representations. N400 effects, consistent with rhyme priming effects in the spoken language literature, are taken to index lexico-semantic processes that appear to be largely modality independent. Together, these results highlight both the unique visual-manual nature of sign languages and the linguistic processing characteristics they share with spoken languages.",
    "metadata": {
      "title": "Phonological and semantic priming in American Sign Language: N300 and N400 effects",
      "summary": "This study investigated the electrophysiological signatures of phonological and semantic priming in American Sign Language (ASL). Deaf signers made semantic relatedness judgments to pairs of ASL signs separated by a 1300 ms prime-target SOA. Phonologically related sign pairs shared two of three phonological parameters (handshape, location, and movement). Target signs preceded by phonologically related and semantically related prime signs elicited smaller negativities within the N300 and N400 windows than those preceded by unrelated primes. N300 effects, typically reported in studies of picture processing, are interpreted to reflect the mapping from the visual features of the signs to more abstract linguistic representations. N400 effects, consistent with rhyme priming effects in the spoken language literature, are taken to index lexico-semantic processes that appear to be largely modality independent. Together, these results highlight both the unique visual-manual nature of sign languages and the linguistic processing characteristics they share with spoken languages.",
      "abstract": "This study investigated the electrophysiological signatures of phonological and semantic priming in American Sign Language (ASL). Deaf signers made semantic relatedness judgments to pairs of ASL signs separated by a 1300 ms prime-target SOA. Phonologically related sign pairs shared two of three phonological parameters (handshape, location, and movement). Target signs preceded by phonologically related and semantically related prime signs elicited smaller negativities within the N300 and N400 windows than those preceded by unrelated primes. N300 effects, typically reported in studies of picture processing, are interpreted to reflect the mapping from the visual features of the signs to more abstract linguistic representations. N400 effects, consistent with rhyme priming effects in the spoken language literature, are taken to index lexico-semantic processes that appear to be largely modality independent. Together, these results highlight both the unique visual-manual nature of sign languages and the linguistic processing characteristics they share with spoken languages.",
      "doi": "https://doi.org/10.1080/23273798.2018.1446543",
      "openalex_id": "https://openalex.org/W2793554983",
      "arxiv_id": "",
      "publication_date": "2018-03-07",
      "published": "2018-03-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes phonological/semantic processing in ASL via electrophysiological priming but does not address discrete audio tokenization or any discrete tokenization design/analysis for speech, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes phonological/semantic processing in ASL via electrophysiological priming but does not address discrete audio tokenization or any discrete tokenization design/analysis for speech, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates phonological and semantic priming in American Sign Language using electrophysiological measures, focusing on linguistic processing rather than discrete audio/speech tokenization. It does not propose or analyze discrete audio tokenization methods or representations relevant to audio reconstruction or generation. Therefore, it does not meet the inclusion criteria related to discrete audio signal tokenization, nor does it meet any exclusion criteria beyond that.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates phonological and semantic priming in American Sign Language using electrophysiological measures, focusing on linguistic processing rather than discrete audio/speech tokenization. It does not propose or analyze discrete audio tokenization methods or representations relevant to audio reconstruction or generation. Therefore, it does not meet the inclusion criteria related to discrete audio signal tokenization, nor does it meet any exclusion criteria beyond that.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The role of iconicity, construal, and proficiency in the online processing of handshape",
    "abstract": "abstract Iconicity has traditionally been considered an objective, fixed, unidimensional property of language forms, often operationalized as transparency for experimental purposes. Within a Cognitive Linguistics framework, iconicity is a mapping between an individual’s construal of form and construal of meaning, such that iconicity is subjective, dynamic, and multidimensional. We test the latter alternative by asking signers who differed in ASL proficiency to complete a handshape monitoring task in which we manipulated the number of form–meaning construals that target handshapes participated in. We estimated the interaction of iconicity, proficiency, and construal density using mixed-effects models for response time and accuracy with crossed random effects for participants and items. Results show a significant three-way interaction between iconicity, proficiency, and construal density such that less-proficient signers detected handshapes in more iconic signs faster than less iconic signs regardless of the handshape they were monitoring, but highly proficient signers’ performance was only improved by iconicity for handshapes that participate in many construals. Taken in conjunction with growing evidence of the subjectivity of iconicity, we interpret these results as support for the claim that construal is a core mechanism underlying iconicity, both for transparent and systematic language-internal form–meaning mappings.",
    "metadata": {
      "title": "The role of iconicity, construal, and proficiency in the online processing of handshape",
      "summary": "abstract Iconicity has traditionally been considered an objective, fixed, unidimensional property of language forms, often operationalized as transparency for experimental purposes. Within a Cognitive Linguistics framework, iconicity is a mapping between an individual’s construal of form and construal of meaning, such that iconicity is subjective, dynamic, and multidimensional. We test the latter alternative by asking signers who differed in ASL proficiency to complete a handshape monitoring task in which we manipulated the number of form–meaning construals that target handshapes participated in. We estimated the interaction of iconicity, proficiency, and construal density using mixed-effects models for response time and accuracy with crossed random effects for participants and items. Results show a significant three-way interaction between iconicity, proficiency, and construal density such that less-proficient signers detected handshapes in more iconic signs faster than less iconic signs regardless of the handshape they were monitoring, but highly proficient signers’ performance was only improved by iconicity for handshapes that participate in many construals. Taken in conjunction with growing evidence of the subjectivity of iconicity, we interpret these results as support for the claim that construal is a core mechanism underlying iconicity, both for transparent and systematic language-internal form–meaning mappings.",
      "abstract": "abstract Iconicity has traditionally been considered an objective, fixed, unidimensional property of language forms, often operationalized as transparency for experimental purposes. Within a Cognitive Linguistics framework, iconicity is a mapping between an individual’s construal of form and construal of meaning, such that iconicity is subjective, dynamic, and multidimensional. We test the latter alternative by asking signers who differed in ASL proficiency to complete a handshape monitoring task in which we manipulated the number of form–meaning construals that target handshapes participated in. We estimated the interaction of iconicity, proficiency, and construal density using mixed-effects models for response time and accuracy with crossed random effects for participants and items. Results show a significant three-way interaction between iconicity, proficiency, and construal density such that less-proficient signers detected handshapes in more iconic signs faster than less iconic signs regardless of the handshape they were monitoring, but highly proficient signers’ performance was only improved by iconicity for handshapes that participate in many construals. Taken in conjunction with growing evidence of the subjectivity of iconicity, we interpret these results as support for the claim that construal is a core mechanism underlying iconicity, both for transparent and systematic language-internal form–meaning mappings.",
      "doi": "https://doi.org/10.1017/langcog.2020.1",
      "openalex_id": "https://openalex.org/W3009084745",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on sign language processing and iconicity without analyzing discrete audio/tokenization of speech; it does not address discrete token representations for speech tasks, failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on sign language processing and iconicity without analyzing discrete audio/tokenization of speech; it does not address discrete token representations for speech tasks, failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates cognitive linguistics aspects of handshape processing in ASL signers rather than focusing on discrete audio or speech tokenization. It does not address conversion of continuous audio signals into discrete tokens, nor does it propose or evaluate tokenization methods for audio data. Hence, it does not meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates cognitive linguistics aspects of handshape processing in ASL signers rather than focusing on discrete audio or speech tokenization. It does not address conversion of continuous audio signals into discrete tokens, nor does it propose or evaluate tokenization methods for audio data. Hence, it does not meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sensorimotor characteristics of sign translations modulate EEG when deaf signers read English",
    "abstract": "Bilingual individuals automatically translate written words from one language to another. While this process is established in spoken-language bilinguals, there is less known about its occurrence in deaf bilinguals who know signed and spoken languages. Since sign language uses motion and space to convey linguistic content, it is possible that action simulation in the brain's sensorimotor system plays a role in this process. We recorded EEG from deaf participants fluent in ASL as they read individual English words and found significant differences in alpha and beta EEG at central electrode sites during the reading of English words whose ASL translations use two hands, compared to English words whose ASL translations use one hand. Hearing non-signers did not show any differences between conditions. These results demonstrate the involvement of the sensorimotor system in cross-linguistic, cross-modal translation, and suggest that covert action simulation processes are involved when deaf signers read.",
    "metadata": {
      "title": "Sensorimotor characteristics of sign translations modulate EEG when deaf signers read English",
      "summary": "Bilingual individuals automatically translate written words from one language to another. While this process is established in spoken-language bilinguals, there is less known about its occurrence in deaf bilinguals who know signed and spoken languages. Since sign language uses motion and space to convey linguistic content, it is possible that action simulation in the brain's sensorimotor system plays a role in this process. We recorded EEG from deaf participants fluent in ASL as they read individual English words and found significant differences in alpha and beta EEG at central electrode sites during the reading of English words whose ASL translations use two hands, compared to English words whose ASL translations use one hand. Hearing non-signers did not show any differences between conditions. These results demonstrate the involvement of the sensorimotor system in cross-linguistic, cross-modal translation, and suggest that covert action simulation processes are involved when deaf signers read.",
      "abstract": "Bilingual individuals automatically translate written words from one language to another. While this process is established in spoken-language bilinguals, there is less known about its occurrence in deaf bilinguals who know signed and spoken languages. Since sign language uses motion and space to convey linguistic content, it is possible that action simulation in the brain's sensorimotor system plays a role in this process. We recorded EEG from deaf participants fluent in ASL as they read individual English words and found significant differences in alpha and beta EEG at central electrode sites during the reading of English words whose ASL translations use two hands, compared to English words whose ASL translations use one hand. Hearing non-signers did not show any differences between conditions. These results demonstrate the involvement of the sensorimotor system in cross-linguistic, cross-modal translation, and suggest that covert action simulation processes are involved when deaf signers read.",
      "doi": "https://doi.org/10.1016/j.bandl.2018.10.001",
      "openalex_id": "https://openalex.org/W2884597521",
      "arxiv_id": "",
      "publication_date": "2018-11-03",
      "published": "2018-11-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes sensorimotor processing in cross-modal translation between ASL and English but does not address discrete audio tokenization or token-based representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes sensorimotor processing in cross-modal translation between ASL and English but does not address discrete audio tokenization or token-based representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on EEG-based sensorimotor characteristics in deaf bilinguals reading English words and their sign language translations; it does not involve any audio or speech tokenization or conversion of continuous audio signals into discrete tokens, which is a core inclusion criterion for this review task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on EEG-based sensorimotor characteristics in deaf bilinguals reading English words and their sign language translations; it does not involve any audio or speech tokenization or conversion of continuous audio signals into discrete tokens, which is a core inclusion criterion for this review task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Higher Order Feature Extraction and Selection for Robust Human Gesture Recognition using CSI of COTS Wi-Fi Devices",
    "abstract": "Device-free human gesture recognition (HGR) using commercial off the shelf (COTS) Wi-Fi devices has gained attention with recent advances in wireless technology. HGR recognizes the human activity performed, by capturing the reflections of Wi-Fi signals from moving humans and storing them as raw channel state information (CSI) traces. Existing work on HGR applies noise reduction and transformation to pre-process the raw CSI traces. However, these methods fail to capture the non-Gaussian information in the raw CSI data due to its limitation to deal with linear signal representation alone. The proposed higher order statistics-based recognition (HOS-Re) model extracts higher order statistical (HOS) features from raw CSI traces and selects a robust feature subset for the recognition task. HOS-Re addresses the limitations in the existing methods, by extracting third order cumulant features that maximizes the recognition accuracy. Subsequently, feature selection methods derived from information theory construct a robust and highly informative feature subset, fed as input to the multilevel support vector machine (SVM) classifier in order to measure the performance. The proposed methodology is validated using a public database SignFi, consisting of 276 gestures with 8280 gesture instances, out of which 5520 are from the laboratory and 2760 from the home environment using a 10 × 5 cross-validation. HOS-Re achieved an average recognition accuracy of 97.84%, 98.26% and 96.34% for the lab, home and lab + home environment respectively. The average recognition accuracy for 150 sign gestures with 7500 instances, collected from five different users was 96.23% in the laboratory environment.",
    "metadata": {
      "title": "Higher Order Feature Extraction and Selection for Robust Human Gesture Recognition using CSI of COTS Wi-Fi Devices",
      "summary": "Device-free human gesture recognition (HGR) using commercial off the shelf (COTS) Wi-Fi devices has gained attention with recent advances in wireless technology. HGR recognizes the human activity performed, by capturing the reflections of Wi-Fi signals from moving humans and storing them as raw channel state information (CSI) traces. Existing work on HGR applies noise reduction and transformation to pre-process the raw CSI traces. However, these methods fail to capture the non-Gaussian information in the raw CSI data due to its limitation to deal with linear signal representation alone. The proposed higher order statistics-based recognition (HOS-Re) model extracts higher order statistical (HOS) features from raw CSI traces and selects a robust feature subset for the recognition task. HOS-Re addresses the limitations in the existing methods, by extracting third order cumulant features that maximizes the recognition accuracy. Subsequently, feature selection methods derived from information theory construct a robust and highly informative feature subset, fed as input to the multilevel support vector machine (SVM) classifier in order to measure the performance. The proposed methodology is validated using a public database SignFi, consisting of 276 gestures with 8280 gesture instances, out of which 5520 are from the laboratory and 2760 from the home environment using a 10 × 5 cross-validation. HOS-Re achieved an average recognition accuracy of 97.84%, 98.26% and 96.34% for the lab, home and lab + home environment respectively. The average recognition accuracy for 150 sign gestures with 7500 instances, collected from five different users was 96.23% in the laboratory environment.",
      "abstract": "Device-free human gesture recognition (HGR) using commercial off the shelf (COTS) Wi-Fi devices has gained attention with recent advances in wireless technology. HGR recognizes the human activity performed, by capturing the reflections of Wi-Fi signals from moving humans and storing them as raw channel state information (CSI) traces. Existing work on HGR applies noise reduction and transformation to pre-process the raw CSI traces. However, these methods fail to capture the non-Gaussian information in the raw CSI data due to its limitation to deal with linear signal representation alone. The proposed higher order statistics-based recognition (HOS-Re) model extracts higher order statistical (HOS) features from raw CSI traces and selects a robust feature subset for the recognition task. HOS-Re addresses the limitations in the existing methods, by extracting third order cumulant features that maximizes the recognition accuracy. Subsequently, feature selection methods derived from information theory construct a robust and highly informative feature subset, fed as input to the multilevel support vector machine (SVM) classifier in order to measure the performance. The proposed methodology is validated using a public database SignFi, consisting of 276 gestures with 8280 gesture instances, out of which 5520 are from the laboratory and 2760 from the home environment using a 10 × 5 cross-validation. HOS-Re achieved an average recognition accuracy of 97.84%, 98.26% and 96.34% for the lab, home and lab + home environment respectively. The average recognition accuracy for 150 sign gestures with 7500 instances, collected from five different users was 96.23% in the laboratory environment.",
      "doi": "https://doi.org/10.3390/s19132959",
      "openalex_id": "https://openalex.org/W2953413701",
      "arxiv_id": "",
      "publication_date": "2019-07-04",
      "published": "2019-07-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses gesture recognition from CSI data using higher-order statistics; it does not propose or analyze discrete audio tokenization or any discrete audio token representations, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses gesture recognition from CSI data using higher-order statistics; it does not propose or analyze discrete audio tokenization or any discrete audio token representations, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on human gesture recognition using Wi-Fi CSI data, which is a signal processing task unrelated to discrete audio/speech tokenization; it neither proposes nor analyzes converting continuous audio signals into discrete token sequences, and thus does not meet the inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on human gesture recognition using Wi-Fi CSI data, which is a signal processing task unrelated to discrete audio/speech tokenization; it neither proposes nor analyzes converting continuous audio signals into discrete token sequences, and thus does not meet the inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A data-driven approach to the semantics of iconicity in American Sign Language and English",
    "abstract": "abstract A growing body of research shows that both signed and spoken languages display regular patterns of iconicity in their vocabularies. We compared iconicity in the lexicons of American Sign Language (ASL) and English by combining previously collected ratings of ASL signs (Caselli, Sevcikova Sehyr, Cohen-Goldberg, &amp; Emmorey, 2017) and English words (Winter, Perlman, Perry, &amp; Lupyan, 2017) with the use of data-driven semantic vectors derived from English. Our analyses show that models of spoken language lexical semantics drawn from large text corpora can be useful for predicting the iconicity of signs as well as words. Compared to English, ASL has a greater number of regions of semantic space with concentrations of highly iconic vocabulary. There was an overall negative relationship between semantic density and the iconicity of both English words and ASL signs. This negative relationship disappeared for highly iconic signs, suggesting that iconic forms may be more easily discriminable in ASL than in English. Our findings contribute to an increasingly detailed picture of how iconicity is distributed across different languages.",
    "metadata": {
      "title": "A data-driven approach to the semantics of iconicity in American Sign Language and English",
      "summary": "abstract A growing body of research shows that both signed and spoken languages display regular patterns of iconicity in their vocabularies. We compared iconicity in the lexicons of American Sign Language (ASL) and English by combining previously collected ratings of ASL signs (Caselli, Sevcikova Sehyr, Cohen-Goldberg, &amp; Emmorey, 2017) and English words (Winter, Perlman, Perry, &amp; Lupyan, 2017) with the use of data-driven semantic vectors derived from English. Our analyses show that models of spoken language lexical semantics drawn from large text corpora can be useful for predicting the iconicity of signs as well as words. Compared to English, ASL has a greater number of regions of semantic space with concentrations of highly iconic vocabulary. There was an overall negative relationship between semantic density and the iconicity of both English words and ASL signs. This negative relationship disappeared for highly iconic signs, suggesting that iconic forms may be more easily discriminable in ASL than in English. Our findings contribute to an increasingly detailed picture of how iconicity is distributed across different languages.",
      "abstract": "abstract A growing body of research shows that both signed and spoken languages display regular patterns of iconicity in their vocabularies. We compared iconicity in the lexicons of American Sign Language (ASL) and English by combining previously collected ratings of ASL signs (Caselli, Sevcikova Sehyr, Cohen-Goldberg, &amp; Emmorey, 2017) and English words (Winter, Perlman, Perry, &amp; Lupyan, 2017) with the use of data-driven semantic vectors derived from English. Our analyses show that models of spoken language lexical semantics drawn from large text corpora can be useful for predicting the iconicity of signs as well as words. Compared to English, ASL has a greater number of regions of semantic space with concentrations of highly iconic vocabulary. There was an overall negative relationship between semantic density and the iconicity of both English words and ASL signs. This negative relationship disappeared for highly iconic signs, suggesting that iconic forms may be more easily discriminable in ASL than in English. Our findings contribute to an increasingly detailed picture of how iconicity is distributed across different languages.",
      "doi": "https://doi.org/10.1017/langcog.2019.52",
      "openalex_id": "https://openalex.org/W3010194027",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes iconicity and semantic representations in ASL and English using ratings and data-driven semantic vectors, but it does not involve discrete audio/speech tokenization or token-level representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes iconicity and semantic representations in ASL and English using ratings and data-driven semantic vectors, but it does not involve discrete audio/speech tokenization or token-level representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the semantics of iconicity in American Sign Language and English using data-driven semantic analyses but does not address discrete audio or speech tokenization, nor does it involve the conversion of continuous audio signals into discrete tokens, which is required by the inclusion criteria. The focus is on lexical semantics and iconicity rather than audio signal processing or tokenization, so it does not meet the required criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the semantics of iconicity in American Sign Language and English using data-driven semantic analyses but does not address discrete audio or speech tokenization, nor does it involve the conversion of continuous audio signals into discrete tokens, which is required by the inclusion criteria. The focus is on lexical semantics and iconicity rather than audio signal processing or tokenization, so it does not meet the required criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Turning languages on and off: Switching into and out of code-blends reveals the nature of bilingual language control.",
    "abstract": "When spoken language (unimodal) bilinguals switch between languages, they must simultaneously inhibit 1 language and activate the other language. Because American Sign Language (ASL)-English (bimodal) bilinguals can switch into and out of code-blends (simultaneous production of a sign and a word), we can tease apart the cost of inhibition (turning a language off) and activation (turning a language on). Results from a cued picture-naming task with 43 bimodal bilinguals revealed a significant cost to turn off a language (switching out of a code-blend), but no cost to turn on a language (switching into a code-blend). Switching from single to dual lexical retrieval (adding a language) was also not costly. These patterns held for both languages regardless of default language, that is, whether switching between speaking and code-blending (English default) or between signing and code-blending (ASL default). Overall, the results support models of bilingual language control that assume a primary role for inhibitory control and indicate that disengaging from producing a language is more difficult than engaging a new language. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",
    "metadata": {
      "title": "Turning languages on and off: Switching into and out of code-blends reveals the nature of bilingual language control.",
      "summary": "When spoken language (unimodal) bilinguals switch between languages, they must simultaneously inhibit 1 language and activate the other language. Because American Sign Language (ASL)-English (bimodal) bilinguals can switch into and out of code-blends (simultaneous production of a sign and a word), we can tease apart the cost of inhibition (turning a language off) and activation (turning a language on). Results from a cued picture-naming task with 43 bimodal bilinguals revealed a significant cost to turn off a language (switching out of a code-blend), but no cost to turn on a language (switching into a code-blend). Switching from single to dual lexical retrieval (adding a language) was also not costly. These patterns held for both languages regardless of default language, that is, whether switching between speaking and code-blending (English default) or between signing and code-blending (ASL default). Overall, the results support models of bilingual language control that assume a primary role for inhibitory control and indicate that disengaging from producing a language is more difficult than engaging a new language. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",
      "abstract": "When spoken language (unimodal) bilinguals switch between languages, they must simultaneously inhibit 1 language and activate the other language. Because American Sign Language (ASL)-English (bimodal) bilinguals can switch into and out of code-blends (simultaneous production of a sign and a word), we can tease apart the cost of inhibition (turning a language off) and activation (turning a language on). Results from a cued picture-naming task with 43 bimodal bilinguals revealed a significant cost to turn off a language (switching out of a code-blend), but no cost to turn on a language (switching into a code-blend). Switching from single to dual lexical retrieval (adding a language) was also not costly. These patterns held for both languages regardless of default language, that is, whether switching between speaking and code-blending (English default) or between signing and code-blending (ASL default). Overall, the results support models of bilingual language control that assume a primary role for inhibitory control and indicate that disengaging from producing a language is more difficult than engaging a new language. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",
      "doi": "https://doi.org/10.1037/xlm0000734",
      "openalex_id": "https://openalex.org/W2955094614",
      "arxiv_id": "",
      "publication_date": "2019-06-27",
      "published": "2019-06-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article reports on bilingual language control and code-blends and does not engage with discrete audio tokenization or token-level representations for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article reports on bilingual language control and code-blends and does not engage with discrete audio tokenization or token-level representations for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on bilingual language control and the cognitive cost of switching between languages and code-blends, without any mention or analysis of discrete audio/speech tokenization or neural quantization-based discrete token representations. It is a psycholinguistic experimental study rather than an audio processing or tokenization study, thus not meeting any of the inclusion criteria and falls under exclusion criteria related to lack of discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on bilingual language control and the cognitive cost of switching between languages and code-blends, without any mention or analysis of discrete audio/speech tokenization or neural quantization-based discrete token representations. It is a psycholinguistic experimental study rather than an audio processing or tokenization study, thus not meeting any of the inclusion criteria and falls under exclusion criteria related to lack of discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Age, frequency, and iconicity in early sign language acquisition: Evidence from the Israeli Sign Language MacArthur–Bates Communicative Developmental Inventory",
    "abstract": "Abstract The current study described the development of the MacArthur–Bates Communicative Developmental Inventory (CDI) for Israeli Sign Language (ISL) and investigated the effects of age, sign iconicity, and sign frequency on lexical acquisition of bimodal-bilingual toddlers acquiring ISL. Previous findings bring inconclusive evidence on the role of sign iconicity (the relationship between form and meaning) and sign frequency (how often a word/sign is used in the language) on the acquisition of signs. The ISL-CDI consisted of 563 video clips. Iconicity ratings from 41 sign-naïve Hebrew-speaking adults (Study 1A) and sign frequency ratings from 19 native ISL adult signers (Study 1B) were collected. ISL vocabulary was evaluated in 34 toddlers, native signers (Study 2). Results indicated significant effects of age, strong correlations between parental ISL ratings and ISL size even when age was controlled for, and strong correlations between naturalistic data and ISL-CDI scores, supporting the validity of the ISL-CDI. Moreover, the results revealed effects of iconicity, frequency, and interactions between age and the iconicity and frequency factors, suggesting that both iconicity and frequency are modulated by age. The findings contribute to the field of sign language acquisition and to our understanding of potential factors affecting human language acquisition beyond language modality.",
    "metadata": {
      "title": "Age, frequency, and iconicity in early sign language acquisition: Evidence from the Israeli Sign Language MacArthur–Bates Communicative Developmental Inventory",
      "summary": "Abstract The current study described the development of the MacArthur–Bates Communicative Developmental Inventory (CDI) for Israeli Sign Language (ISL) and investigated the effects of age, sign iconicity, and sign frequency on lexical acquisition of bimodal-bilingual toddlers acquiring ISL. Previous findings bring inconclusive evidence on the role of sign iconicity (the relationship between form and meaning) and sign frequency (how often a word/sign is used in the language) on the acquisition of signs. The ISL-CDI consisted of 563 video clips. Iconicity ratings from 41 sign-naïve Hebrew-speaking adults (Study 1A) and sign frequency ratings from 19 native ISL adult signers (Study 1B) were collected. ISL vocabulary was evaluated in 34 toddlers, native signers (Study 2). Results indicated significant effects of age, strong correlations between parental ISL ratings and ISL size even when age was controlled for, and strong correlations between naturalistic data and ISL-CDI scores, supporting the validity of the ISL-CDI. Moreover, the results revealed effects of iconicity, frequency, and interactions between age and the iconicity and frequency factors, suggesting that both iconicity and frequency are modulated by age. The findings contribute to the field of sign language acquisition and to our understanding of potential factors affecting human language acquisition beyond language modality.",
      "abstract": "Abstract The current study described the development of the MacArthur–Bates Communicative Developmental Inventory (CDI) for Israeli Sign Language (ISL) and investigated the effects of age, sign iconicity, and sign frequency on lexical acquisition of bimodal-bilingual toddlers acquiring ISL. Previous findings bring inconclusive evidence on the role of sign iconicity (the relationship between form and meaning) and sign frequency (how often a word/sign is used in the language) on the acquisition of signs. The ISL-CDI consisted of 563 video clips. Iconicity ratings from 41 sign-naïve Hebrew-speaking adults (Study 1A) and sign frequency ratings from 19 native ISL adult signers (Study 1B) were collected. ISL vocabulary was evaluated in 34 toddlers, native signers (Study 2). Results indicated significant effects of age, strong correlations between parental ISL ratings and ISL size even when age was controlled for, and strong correlations between naturalistic data and ISL-CDI scores, supporting the validity of the ISL-CDI. Moreover, the results revealed effects of iconicity, frequency, and interactions between age and the iconicity and frequency factors, suggesting that both iconicity and frequency are modulated by age. The findings contribute to the field of sign language acquisition and to our understanding of potential factors affecting human language acquisition beyond language modality.",
      "doi": "https://doi.org/10.1017/s0142716420000247",
      "openalex_id": "https://openalex.org/W3044174589",
      "arxiv_id": "",
      "publication_date": "2020-07-01",
      "published": "2020-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates development of a sign-language vocabulary inventory and factors like iconicity and frequency; there is no analysis or proposal of discrete audio/speech tokenization or token-based representations, so it does not meet the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates development of a sign-language vocabulary inventory and factors like iconicity and frequency; there is no analysis or proposal of discrete audio/speech tokenization or token-based representations, so it does not meet the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on early sign language acquisition using video clips and ratings related to Israeli Sign Language, with no mention of discrete audio or speech tokenization, audio signal quantization, or neural encoding of audio tokens, which are core to the inclusion criteria. It studies sign language acquisition mechanisms without addressing discrete audio token conversion or related audio reconstruction/generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on early sign language acquisition using video clips and ratings related to Israeli Sign Language, with no mention of discrete audio or speech tokenization, audio signal quantization, or neural encoding of audio tokens, which are core to the inclusion criteria. It studies sign language acquisition mechanisms without addressing discrete audio token conversion or related audio reconstruction/generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-Frequency RF Sensor Fusion for Word-Level Fluent ASL Recognition",
    "abstract": "Deaf spaces are unique indoor environments designed to optimize visual communication and Deaf cultural expression. However, much of the technological research geared towards the deaf involve use of video or wearables for American sign language (ASL) translation, with little consideration for Deaf perspective on privacy and usability of the technology. In contrast to video, RF sensors offer the avenue for ambient ASL recognition while also preserving privacy for Deaf signers. Methods: This paper investigates the RF transmit waveform parameters required for effective measurement of ASL signs and their effect on word-level classification accuracy attained with transfer learning and convolutional autoencoders (CAE). A multi-frequency fusion network is proposed to exploit data from all sensors in an RF sensor network and improve the recognition accuracy of fluent ASL signing. Results: For fluent signers, CAEs yield a 20-sign classification accuracy of %76 at 77 GHz and %73 at 24 GHz, while at X-band (10 Ghz) accuracy drops to 67%. For hearing imitation signers, signs are more separable, resulting in a 96% accuracy with CAEs. Further, fluent ASL recognition accuracy is significantly increased with use of the multi-frequency fusion network, which boosts the 20-sign fluent ASL recognition accuracy to 95%, surpassing conventional feature level fusion by 12%. Implications: Signing involves finer spatiotemporal dynamics than typical hand gestures, and thus requires interrogation with a transmit waveform that has a rapid succession of pulses and high bandwidth. Millimeter wave RF frequencies also yield greater accuracy due to the increased Doppler spread of the radar backscatter. Comparative analysis of articulation dynamics also shows that imitation signing is not representative of fluent signing, and not effective in pre-training networks for fluent ASL classification. Deep neural networks employing multi-frequency fusion capture both shared, as well as sensor-specific features and thus offer significant performance gains in comparison to using a single sensor or feature-level fusion.",
    "metadata": {
      "title": "Multi-Frequency RF Sensor Fusion for Word-Level Fluent ASL Recognition",
      "summary": "Deaf spaces are unique indoor environments designed to optimize visual communication and Deaf cultural expression. However, much of the technological research geared towards the deaf involve use of video or wearables for American sign language (ASL) translation, with little consideration for Deaf perspective on privacy and usability of the technology. In contrast to video, RF sensors offer the avenue for ambient ASL recognition while also preserving privacy for Deaf signers. Methods: This paper investigates the RF transmit waveform parameters required for effective measurement of ASL signs and their effect on word-level classification accuracy attained with transfer learning and convolutional autoencoders (CAE). A multi-frequency fusion network is proposed to exploit data from all sensors in an RF sensor network and improve the recognition accuracy of fluent ASL signing. Results: For fluent signers, CAEs yield a 20-sign classification accuracy of %76 at 77 GHz and %73 at 24 GHz, while at X-band (10 Ghz) accuracy drops to 67%. For hearing imitation signers, signs are more separable, resulting in a 96% accuracy with CAEs. Further, fluent ASL recognition accuracy is significantly increased with use of the multi-frequency fusion network, which boosts the 20-sign fluent ASL recognition accuracy to 95%, surpassing conventional feature level fusion by 12%. Implications: Signing involves finer spatiotemporal dynamics than typical hand gestures, and thus requires interrogation with a transmit waveform that has a rapid succession of pulses and high bandwidth. Millimeter wave RF frequencies also yield greater accuracy due to the increased Doppler spread of the radar backscatter. Comparative analysis of articulation dynamics also shows that imitation signing is not representative of fluent signing, and not effective in pre-training networks for fluent ASL classification. Deep neural networks employing multi-frequency fusion capture both shared, as well as sensor-specific features and thus offer significant performance gains in comparison to using a single sensor or feature-level fusion.",
      "abstract": "Deaf spaces are unique indoor environments designed to optimize visual communication and Deaf cultural expression. However, much of the technological research geared towards the deaf involve use of video or wearables for American sign language (ASL) translation, with little consideration for Deaf perspective on privacy and usability of the technology. In contrast to video, RF sensors offer the avenue for ambient ASL recognition while also preserving privacy for Deaf signers. Methods: This paper investigates the RF transmit waveform parameters required for effective measurement of ASL signs and their effect on word-level classification accuracy attained with transfer learning and convolutional autoencoders (CAE). A multi-frequency fusion network is proposed to exploit data from all sensors in an RF sensor network and improve the recognition accuracy of fluent ASL signing. Results: For fluent signers, CAEs yield a 20-sign classification accuracy of %76 at 77 GHz and %73 at 24 GHz, while at X-band (10 Ghz) accuracy drops to 67%. For hearing imitation signers, signs are more separable, resulting in a 96% accuracy with CAEs. Further, fluent ASL recognition accuracy is significantly increased with use of the multi-frequency fusion network, which boosts the 20-sign fluent ASL recognition accuracy to 95%, surpassing conventional feature level fusion by 12%. Implications: Signing involves finer spatiotemporal dynamics than typical hand gestures, and thus requires interrogation with a transmit waveform that has a rapid succession of pulses and high bandwidth. Millimeter wave RF frequencies also yield greater accuracy due to the increased Doppler spread of the radar backscatter. Comparative analysis of articulation dynamics also shows that imitation signing is not representative of fluent signing, and not effective in pre-training networks for fluent ASL classification. Deep neural networks employing multi-frequency fusion capture both shared, as well as sensor-specific features and thus offer significant performance gains in comparison to using a single sensor or feature-level fusion.",
      "doi": "https://doi.org/10.1109/jsen.2021.3078339",
      "openalex_id": "https://openalex.org/W3163591631",
      "arxiv_id": "",
      "publication_date": "2021-05-07",
      "published": "2021-05-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on RF-based ASL recognition and multi-frequency sensor fusion, and it does not analyze or propose discrete audio/speech tokenization or token representations for language tasks, so it fails the required inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on RF-based ASL recognition and multi-frequency sensor fusion, and it does not analyze or propose discrete audio/speech tokenization or token representations for language tasks, so it fails the required inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on ASL recognition using RF sensor data rather than audio signals; it does not discuss discrete audio or speech tokenization, neural encoding, or discrete token representations related to audio, and thus does not meet the inclusion criteria tied specifically to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on ASL recognition using RF sensor data rather than audio signals; it does not discuss discrete audio or speech tokenization, neural encoding, or discrete token representations related to audio, and thus does not meet the inclusion criteria tied specifically to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perceptual optimization of language: Evidence from American Sign Language",
    "abstract": "If language has evolved for communication, languages should be structured such that they maximize the efficiency of processing. What is efficient for communication in the visual-gestural modality is different from the auditory-oral modality, and we ask here whether sign languages have adapted to the affordances and constraints of the signed modality. During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (high frequency signs, signs with common handshapes) can be produced further from the face because precise visual resolution is not necessary for recognition. Using pose estimation algorithms, we examined the structure of over 2000 American Sign Language lexical signs to identify whether lexical frequency and handshape probability affect the position of the wrist in 2D space. We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs. Together these results provide empirical support for anecdotal assertions that the phonological structure of sign language is shaped by the properties of the human visual and motor systems.",
    "metadata": {
      "title": "Perceptual optimization of language: Evidence from American Sign Language",
      "summary": "If language has evolved for communication, languages should be structured such that they maximize the efficiency of processing. What is efficient for communication in the visual-gestural modality is different from the auditory-oral modality, and we ask here whether sign languages have adapted to the affordances and constraints of the signed modality. During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (high frequency signs, signs with common handshapes) can be produced further from the face because precise visual resolution is not necessary for recognition. Using pose estimation algorithms, we examined the structure of over 2000 American Sign Language lexical signs to identify whether lexical frequency and handshape probability affect the position of the wrist in 2D space. We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs. Together these results provide empirical support for anecdotal assertions that the phonological structure of sign language is shaped by the properties of the human visual and motor systems.",
      "abstract": "If language has evolved for communication, languages should be structured such that they maximize the efficiency of processing. What is efficient for communication in the visual-gestural modality is different from the auditory-oral modality, and we ask here whether sign languages have adapted to the affordances and constraints of the signed modality. During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (high frequency signs, signs with common handshapes) can be produced further from the face because precise visual resolution is not necessary for recognition. Using pose estimation algorithms, we examined the structure of over 2000 American Sign Language lexical signs to identify whether lexical frequency and handshape probability affect the position of the wrist in 2D space. We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs. Together these results provide empirical support for anecdotal assertions that the phonological structure of sign language is shaped by the properties of the human visual and motor systems.",
      "doi": "https://doi.org/10.1016/j.cognition.2022.105040",
      "openalex_id": "https://openalex.org/W4213262042",
      "arxiv_id": "",
      "publication_date": "2022-02-19",
      "published": "2022-02-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates perceptual optimization in sign language and its phonological structure using visual data, with no discrete audio tokenization or speech token sequences; it does not analyze or implement discrete audio tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates perceptual optimization in sign language and its phonological structure using visual data, with no discrete audio tokenization or speech token sequences; it does not analyze or implement discrete audio tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the perceptual optimization in American Sign Language by analyzing physical sign parameters and their correlation with frequency and handshape probability; it does not propose, analyze, or evaluate discrete audio or speech tokenization, nor does it address any discrete audio signal representation or neural quantization techniques. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and is outside the scope of audio/speech signal processing token studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the perceptual optimization in American Sign Language by analyzing physical sign parameters and their correlation with frequency and handshape probability; it does not propose, analyze, or evaluate discrete audio or speech tokenization, nor does it address any discrete audio signal representation or neural quantization techniques. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and is outside the scope of audio/speech signal processing token studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Iconicity in American Sign Language–English translation recognition",
    "abstract": "abstract Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.",
    "metadata": {
      "title": "Iconicity in American Sign Language–English translation recognition",
      "summary": "abstract Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.",
      "abstract": "abstract Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.",
      "doi": "https://doi.org/10.1017/langcog.2019.51",
      "openalex_id": "https://openalex.org/W3010109748",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study examines ASL-English translation recognition and iconicity in sign language, and does not involve discrete audio tokenization or a tokenization-based representation, so it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study examines ASL-English translation recognition and iconicity in sign language, and does not involve discrete audio tokenization or a tokenization-based representation, so it fails the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on translation recognition and iconicity in sign language and English, without proposing, analyzing, or evaluating discrete audio or speech tokenization methods; hence, it does not meet the inclusion criteria related to discrete audio tokenization or quantization of continuous audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on translation recognition and iconicity in sign language and English, without proposing, analyzing, or evaluating discrete audio or speech tokenization methods; hence, it does not meet the inclusion criteria related to discrete audio tokenization or quantization of continuous audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Picture-naming in American Sign Language: an electrophysiological study of the effects of iconicity and structured alignment",
    "abstract": "A picture-naming task and ERPs were used to investigate effects of iconicity and visual alignment between signs and pictures in American Sign Language (ASL). For iconic signs, half the pictures visually overlapped with phonological features of the sign (e.g., the fingers of CAT align with a picture of a cat with prominent whiskers), while half did not (whiskers are not shown). Iconic signs were produced numerically faster than non-iconic signs and were associated with larger N400 amplitudes, akin to concreteness effects. Pictures aligned with iconic signs were named faster than non-aligned pictures, and there was a reduction in N400 amplitude. No behavioral effects were observed for the control group (English speakers). We conclude that sensory-motoric semantic features are represented more robustly for iconic than non-iconic signs (eliciting a concreteness-like N400 effect) and visual overlap between pictures and the phonological form of iconic signs facilitates lexical retrieval (eliciting a reduced N400).",
    "metadata": {
      "title": "Picture-naming in American Sign Language: an electrophysiological study of the effects of iconicity and structured alignment",
      "summary": "A picture-naming task and ERPs were used to investigate effects of iconicity and visual alignment between signs and pictures in American Sign Language (ASL). For iconic signs, half the pictures visually overlapped with phonological features of the sign (e.g., the fingers of CAT align with a picture of a cat with prominent whiskers), while half did not (whiskers are not shown). Iconic signs were produced numerically faster than non-iconic signs and were associated with larger N400 amplitudes, akin to concreteness effects. Pictures aligned with iconic signs were named faster than non-aligned pictures, and there was a reduction in N400 amplitude. No behavioral effects were observed for the control group (English speakers). We conclude that sensory-motoric semantic features are represented more robustly for iconic than non-iconic signs (eliciting a concreteness-like N400 effect) and visual overlap between pictures and the phonological form of iconic signs facilitates lexical retrieval (eliciting a reduced N400).",
      "abstract": "A picture-naming task and ERPs were used to investigate effects of iconicity and visual alignment between signs and pictures in American Sign Language (ASL). For iconic signs, half the pictures visually overlapped with phonological features of the sign (e.g., the fingers of CAT align with a picture of a cat with prominent whiskers), while half did not (whiskers are not shown). Iconic signs were produced numerically faster than non-iconic signs and were associated with larger N400 amplitudes, akin to concreteness effects. Pictures aligned with iconic signs were named faster than non-aligned pictures, and there was a reduction in N400 amplitude. No behavioral effects were observed for the control group (English speakers). We conclude that sensory-motoric semantic features are represented more robustly for iconic than non-iconic signs (eliciting a concreteness-like N400 effect) and visual overlap between pictures and the phonological form of iconic signs facilitates lexical retrieval (eliciting a reduced N400).",
      "doi": "https://doi.org/10.1080/23273798.2020.1804601",
      "openalex_id": "https://openalex.org/W3063240284",
      "arxiv_id": "",
      "publication_date": "2020-08-19",
      "published": "2020-08-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates ASL perception using ERP and discusses iconicity and visual alignment, but it does not address discrete audio/speech tokenization or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates ASL perception using ERP and discusses iconicity and visual alignment, but it does not address discrete audio/speech tokenization or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on electrophysiological effects of iconicity in American Sign Language using picture-naming tasks and ERP measures, without any mention or analysis of discrete audio/speech tokenization or discrete token representations from continuous audio signals, which is central to the inclusion criteria. It does not address audio signal processing, discrete audio tokenization, or relevant neural encoding techniques, and therefore does not meet the core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on electrophysiological effects of iconicity in American Sign Language using picture-naming tasks and ERP measures, without any mention or analysis of discrete audio/speech tokenization or discrete token representations from continuous audio signals, which is central to the inclusion criteria. It does not address audio signal processing, discrete audio tokenization, or relevant neural encoding techniques, and therefore does not meet the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Two measures are better than one: combining iconicity ratings and guessing experiments for a more nuanced picture of iconicity in the lexicon",
    "abstract": "Abstract Iconicity in language is receiving increased attention from many fields, but our understanding of iconicity is only as good as the measures we use to quantify it. We collected iconicity measures for 304 Japanese words from English-speaking participants, using rating and guessing tasks. The words included ideophones (structurally marked depictive words) along with regular lexical items from similar semantic domains (e.g., fuwafuwa ‘fluffy’, jawarakai ‘soft’). The two measures correlated, speaking to their validity. However, ideophones received consistently higher iconicity ratings than other items, even when guessed at the same accuracies, suggesting the rating task is more sensitive to cues like structural markedness that frame words as iconic. These cues did not always guide participants to the meanings of ideophones in the guessing task, but they did make them more confident in their guesses, even when they were wrong. Consistently poor guessing results reflect the role different experiences play in shaping construals of iconicity. Using multiple measures in tandem allows us to explore the interplay between iconicity and these external factors. To facilitate this, we introduce a reproducible workflow for creating rating and guessing tasks from standardised wordlists, while also making improvements to the robustness, sensitivity and discriminability of previous approaches.",
    "metadata": {
      "title": "Two measures are better than one: combining iconicity ratings and guessing experiments for a more nuanced picture of iconicity in the lexicon",
      "summary": "Abstract Iconicity in language is receiving increased attention from many fields, but our understanding of iconicity is only as good as the measures we use to quantify it. We collected iconicity measures for 304 Japanese words from English-speaking participants, using rating and guessing tasks. The words included ideophones (structurally marked depictive words) along with regular lexical items from similar semantic domains (e.g., fuwafuwa ‘fluffy’, jawarakai ‘soft’). The two measures correlated, speaking to their validity. However, ideophones received consistently higher iconicity ratings than other items, even when guessed at the same accuracies, suggesting the rating task is more sensitive to cues like structural markedness that frame words as iconic. These cues did not always guide participants to the meanings of ideophones in the guessing task, but they did make them more confident in their guesses, even when they were wrong. Consistently poor guessing results reflect the role different experiences play in shaping construals of iconicity. Using multiple measures in tandem allows us to explore the interplay between iconicity and these external factors. To facilitate this, we introduce a reproducible workflow for creating rating and guessing tasks from standardised wordlists, while also making improvements to the robustness, sensitivity and discriminability of previous approaches.",
      "abstract": "Abstract Iconicity in language is receiving increased attention from many fields, but our understanding of iconicity is only as good as the measures we use to quantify it. We collected iconicity measures for 304 Japanese words from English-speaking participants, using rating and guessing tasks. The words included ideophones (structurally marked depictive words) along with regular lexical items from similar semantic domains (e.g., fuwafuwa ‘fluffy’, jawarakai ‘soft’). The two measures correlated, speaking to their validity. However, ideophones received consistently higher iconicity ratings than other items, even when guessed at the same accuracies, suggesting the rating task is more sensitive to cues like structural markedness that frame words as iconic. These cues did not always guide participants to the meanings of ideophones in the guessing task, but they did make them more confident in their guesses, even when they were wrong. Consistently poor guessing results reflect the role different experiences play in shaping construals of iconicity. Using multiple measures in tandem allows us to explore the interplay between iconicity and these external factors. To facilitate this, we introduce a reproducible workflow for creating rating and guessing tasks from standardised wordlists, while also making improvements to the robustness, sensitivity and discriminability of previous approaches.",
      "doi": "https://doi.org/10.1017/langcog.2023.9",
      "openalex_id": "https://openalex.org/W4364379271",
      "arxiv_id": "",
      "publication_date": "2023-04-11",
      "published": "2023-04-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on measuring iconicity in language via rating and guessing tasks and does not involve discrete audio/speech tokenization or token-level representations, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on measuring iconicity in language via rating and guessing tasks and does not involve discrete audio/speech tokenization or token-level representations, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on iconicity measures for Japanese words using rating and guessing tasks, without any mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens; thus, it does not meet the core inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on iconicity measures for Japanese words using rating and guessing tasks, without any mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens; thus, it does not meet the core inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Tracking the time course of sign recognition using ERP repetition priming",
    "abstract": "Abstract Repetition priming and event‐related potentials (ERPs) were used to investigate the time course of sign recognition in deaf users of American Sign Language. Signers performed a go/no‐go semantic categorization task to rare probe signs referring to people; critical target items were repeated and unrelated signs. In Experiment 1, ERPs were time‐locked either to the onset of the video or to sign onset within the video; in Experiment 2, the same full videos were clipped so that video and sign onset were aligned (removing transitional movements), and ERPs were time‐locked to video/sign onset. All analyses revealed an N400 repetition priming effect (less negativity for repeated than unrelated signs) but differed in the timing and/or duration of the N400 effect. Results from Experiment 1 revealed that repetition priming effects began before sign onset within a video, suggesting that signers are sensitive to linguistic information within the transitional movement to sign onset. The timing and duration of the N400 for clipped videos were more parallel to that observed previously for auditorily presented words and was 200 ms shorter than either time‐locking analysis from Experiment 1. We conclude that time‐locking to full video onset is optimal when early ERP components or sensitivity to transitional movements are of interest and that time‐locking to the onset of clipped videos is optimal for priming studies with fluent signers.",
    "metadata": {
      "title": "Tracking the time course of sign recognition using ERP repetition priming",
      "summary": "Abstract Repetition priming and event‐related potentials (ERPs) were used to investigate the time course of sign recognition in deaf users of American Sign Language. Signers performed a go/no‐go semantic categorization task to rare probe signs referring to people; critical target items were repeated and unrelated signs. In Experiment 1, ERPs were time‐locked either to the onset of the video or to sign onset within the video; in Experiment 2, the same full videos were clipped so that video and sign onset were aligned (removing transitional movements), and ERPs were time‐locked to video/sign onset. All analyses revealed an N400 repetition priming effect (less negativity for repeated than unrelated signs) but differed in the timing and/or duration of the N400 effect. Results from Experiment 1 revealed that repetition priming effects began before sign onset within a video, suggesting that signers are sensitive to linguistic information within the transitional movement to sign onset. The timing and duration of the N400 for clipped videos were more parallel to that observed previously for auditorily presented words and was 200 ms shorter than either time‐locking analysis from Experiment 1. We conclude that time‐locking to full video onset is optimal when early ERP components or sensitivity to transitional movements are of interest and that time‐locking to the onset of clipped videos is optimal for priming studies with fluent signers.",
      "abstract": "Abstract Repetition priming and event‐related potentials (ERPs) were used to investigate the time course of sign recognition in deaf users of American Sign Language. Signers performed a go/no‐go semantic categorization task to rare probe signs referring to people; critical target items were repeated and unrelated signs. In Experiment 1, ERPs were time‐locked either to the onset of the video or to sign onset within the video; in Experiment 2, the same full videos were clipped so that video and sign onset were aligned (removing transitional movements), and ERPs were time‐locked to video/sign onset. All analyses revealed an N400 repetition priming effect (less negativity for repeated than unrelated signs) but differed in the timing and/or duration of the N400 effect. Results from Experiment 1 revealed that repetition priming effects began before sign onset within a video, suggesting that signers are sensitive to linguistic information within the transitional movement to sign onset. The timing and duration of the N400 for clipped videos were more parallel to that observed previously for auditorily presented words and was 200 ms shorter than either time‐locking analysis from Experiment 1. We conclude that time‐locking to full video onset is optimal when early ERP components or sensitivity to transitional movements are of interest and that time‐locking to the onset of clipped videos is optimal for priming studies with fluent signers.",
      "doi": "https://doi.org/10.1111/psyp.13975",
      "openalex_id": "https://openalex.org/W3212089245",
      "arxiv_id": "",
      "publication_date": "2021-11-17",
      "published": "2021-11-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract reports ERP repetition priming in deaf signers using American Sign Language; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria focused on discrete audio tokenization for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract reports ERP repetition priming in deaf signers using American Sign Language; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria focused on discrete audio tokenization for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the time course of sign recognition in American Sign Language using ERP repetition priming but does not propose, analyze, or evaluate discrete audio or speech tokenization methods; instead, it focuses on sign language recognition and ERP analysis without addressing discrete audio token conversion or quantization. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the time course of sign recognition in American Sign Language using ERP repetition priming but does not propose, analyze, or evaluate discrete audio or speech tokenization methods; instead, it focuses on sign language recognition and ERP analysis without addressing discrete audio token conversion or quantization. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effect of Sign-recognition Performance on the Usability of Sign-language Dictionary Search",
    "abstract": "Advances in sign-language recognition technology have enabled researchers to investigate various methods that can assist users in searching for an unfamiliar sign in ASL using sign-recognition technology. Users can generate a query by submitting a video of themselves performing the sign they believe they encountered somewhere and obtain a list of possible matches. However, there is disagreement among developers of such technology on how to report the performance of their systems, and prior research has not examined the relationship between the performance of search technology and users’ subjective judgements for this task. We conducted three studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that, in addition to the position of the desired word in a list of results, the placement of the desired word above or below the fold and the similarity of the other words in the results list affected users’ judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users’ judgements than did metrics currently reported in prior ASL dictionary research.",
    "metadata": {
      "title": "Effect of Sign-recognition Performance on the Usability of Sign-language Dictionary Search",
      "summary": "Advances in sign-language recognition technology have enabled researchers to investigate various methods that can assist users in searching for an unfamiliar sign in ASL using sign-recognition technology. Users can generate a query by submitting a video of themselves performing the sign they believe they encountered somewhere and obtain a list of possible matches. However, there is disagreement among developers of such technology on how to report the performance of their systems, and prior research has not examined the relationship between the performance of search technology and users’ subjective judgements for this task. We conducted three studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that, in addition to the position of the desired word in a list of results, the placement of the desired word above or below the fold and the similarity of the other words in the results list affected users’ judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users’ judgements than did metrics currently reported in prior ASL dictionary research.",
      "abstract": "Advances in sign-language recognition technology have enabled researchers to investigate various methods that can assist users in searching for an unfamiliar sign in ASL using sign-recognition technology. Users can generate a query by submitting a video of themselves performing the sign they believe they encountered somewhere and obtain a list of possible matches. However, there is disagreement among developers of such technology on how to report the performance of their systems, and prior research has not examined the relationship between the performance of search technology and users’ subjective judgements for this task. We conducted three studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that, in addition to the position of the desired word in a list of results, the placement of the desired word above or below the fold and the similarity of the other words in the results list affected users’ judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users’ judgements than did metrics currently reported in prior ASL dictionary research.",
      "doi": "https://doi.org/10.1145/3470650",
      "openalex_id": "https://openalex.org/W3206077713",
      "arxiv_id": "",
      "publication_date": "2021-10-15",
      "published": "2021-10-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on sign-language recognition from video and user usability, not on discrete audio tokenization or generation of discrete audio tokens; it does not meet the core inclusion criterion of analyzing discrete audio tokens, so it should be excluded under the stated criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on sign-language recognition from video and user usability, not on discrete audio tokenization or generation of discrete audio tokens; it does not meet the core inclusion criterion of analyzing discrete audio tokens, so it should be excluded under the stated criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign-language dictionary search using sign-recognition technology, which deals with visual sign inputs rather than audio or speech tokenization. There is no indication that the work proposes, analyzes, or evaluates discrete audio/speech tokenization or any form of audio quantization; it centers on sign recognition and user usability aspects. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and violates the requirement that the core object of study be audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign-language dictionary search using sign-recognition technology, which deals with visual sign inputs rather than audio or speech tokenization. There is no indication that the work proposes, analyzes, or evaluates discrete audio/speech tokenization or any form of audio quantization; it centers on sign recognition and user usability aspects. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and violates the requirement that the core object of study be audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Support in the Moment: Benefits and use of video-span selection and search for sign-language video comprehension among ASL learners",
    "abstract": "As they develop comprehension skills, American Sign Language (ASL) learners often view challenging ASL videos, which may contain unfamiliar signs. Current dictionary tools require students to isolate a single sign they do not understand and input a search query, by selecting linguistic properties or by performing the sign into a webcam. Students may struggle with extracting and re-creating an unfamiliar sign, and they must leave the video-watching task to use an external dictionary tool. We investigate a technology that enables users, in the moment, i.e., while they are viewing a video, to select a span of one or more signs that they do not understand, to view dictionary results. We interviewed 14 American Sign Language (ASL) learners about their challenges in understanding ASL video and workarounds for unfamiliar vocabulary. We then conducted a comparative study and an in-depth analysis with 15 ASL learners to investigate the benefits of using video sub-spans for searching, and their interactions with a Wizard-of-Oz prototype during a video-comprehension task. Our findings revealed benefits of our tool in terms of quality of video translation produced and perceived workload to produce translations. Our in-depth analysis also revealed benefits of an integrated search tool and use of span-selection to constrain video play. These findings inform future designers of such systems, computer vision researchers working on the underlying sign matching technologies, and sign language educators.",
    "metadata": {
      "title": "Support in the Moment: Benefits and use of video-span selection and search for sign-language video comprehension among ASL learners",
      "summary": "As they develop comprehension skills, American Sign Language (ASL) learners often view challenging ASL videos, which may contain unfamiliar signs. Current dictionary tools require students to isolate a single sign they do not understand and input a search query, by selecting linguistic properties or by performing the sign into a webcam. Students may struggle with extracting and re-creating an unfamiliar sign, and they must leave the video-watching task to use an external dictionary tool. We investigate a technology that enables users, in the moment, i.e., while they are viewing a video, to select a span of one or more signs that they do not understand, to view dictionary results. We interviewed 14 American Sign Language (ASL) learners about their challenges in understanding ASL video and workarounds for unfamiliar vocabulary. We then conducted a comparative study and an in-depth analysis with 15 ASL learners to investigate the benefits of using video sub-spans for searching, and their interactions with a Wizard-of-Oz prototype during a video-comprehension task. Our findings revealed benefits of our tool in terms of quality of video translation produced and perceived workload to produce translations. Our in-depth analysis also revealed benefits of an integrated search tool and use of span-selection to constrain video play. These findings inform future designers of such systems, computer vision researchers working on the underlying sign matching technologies, and sign language educators.",
      "abstract": "As they develop comprehension skills, American Sign Language (ASL) learners often view challenging ASL videos, which may contain unfamiliar signs. Current dictionary tools require students to isolate a single sign they do not understand and input a search query, by selecting linguistic properties or by performing the sign into a webcam. Students may struggle with extracting and re-creating an unfamiliar sign, and they must leave the video-watching task to use an external dictionary tool. We investigate a technology that enables users, in the moment, i.e., while they are viewing a video, to select a span of one or more signs that they do not understand, to view dictionary results. We interviewed 14 American Sign Language (ASL) learners about their challenges in understanding ASL video and workarounds for unfamiliar vocabulary. We then conducted a comparative study and an in-depth analysis with 15 ASL learners to investigate the benefits of using video sub-spans for searching, and their interactions with a Wizard-of-Oz prototype during a video-comprehension task. Our findings revealed benefits of our tool in terms of quality of video translation produced and perceived workload to produce translations. Our in-depth analysis also revealed benefits of an integrated search tool and use of span-selection to constrain video play. These findings inform future designers of such systems, computer vision researchers working on the underlying sign matching technologies, and sign language educators.",
      "doi": "https://doi.org/10.1145/3517428.3544883",
      "openalex_id": "https://openalex.org/W4307095343",
      "arxiv_id": "",
      "publication_date": "2022-10-22",
      "published": "2022-10-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a sign-language video comprehension tool using video spans; it does not discuss discrete audio tokenization or token-based audio representations, so it does not meet the inclusion criterion focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a sign-language video comprehension tool using video spans; it does not discuss discrete audio tokenization or token-based audio representations, so it does not meet the inclusion criterion focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on American Sign Language video comprehension and a video-span selection tool to aid dictionary searches for unfamiliar signs; it does not address discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on American Sign Language video comprehension and a video-span selection tool to aid dictionary searches for unfamiliar signs; it does not address discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mapping Word to World in ASL: Evidence from a Human Simulation Paradigm",
    "abstract": "Abstract Across languages, children map words to meaning with great efficiency, despite a seemingly unconstrained space of potential mappings. The literature on how children do this is primarily limited to spoken language. This leaves a gap in our understanding of sign language acquisition, because several of the hypothesized mechanisms that children use are visual (e.g., visual attention to the referent), and sign languages are perceived in the visual modality. Here, we used the Human Simulation Paradigm in American Sign Language (ASL) to determine potential cues to word learning. Sign‐naïve adult participants viewed video clips of parent–child interactions in ASL, and at a designated point, had to guess what ASL sign the parent produced. Across two studies, we demonstrate that referential clarity in ASL interactions is characterized by access to information about word class and referent presence (for verbs), similarly to spoken language. Unlike spoken language, iconicity is a cue to word meaning in ASL, although this is not always a fruitful cue. We also present evidence that verbs are highlighted well in the input, relative to spoken English. The results shed light on both similarities and differences in the information that learners may have access to in acquiring signed versus spoken languages.",
    "metadata": {
      "title": "Mapping Word to World in ASL: Evidence from a Human Simulation Paradigm",
      "summary": "Abstract Across languages, children map words to meaning with great efficiency, despite a seemingly unconstrained space of potential mappings. The literature on how children do this is primarily limited to spoken language. This leaves a gap in our understanding of sign language acquisition, because several of the hypothesized mechanisms that children use are visual (e.g., visual attention to the referent), and sign languages are perceived in the visual modality. Here, we used the Human Simulation Paradigm in American Sign Language (ASL) to determine potential cues to word learning. Sign‐naïve adult participants viewed video clips of parent–child interactions in ASL, and at a designated point, had to guess what ASL sign the parent produced. Across two studies, we demonstrate that referential clarity in ASL interactions is characterized by access to information about word class and referent presence (for verbs), similarly to spoken language. Unlike spoken language, iconicity is a cue to word meaning in ASL, although this is not always a fruitful cue. We also present evidence that verbs are highlighted well in the input, relative to spoken English. The results shed light on both similarities and differences in the information that learners may have access to in acquiring signed versus spoken languages.",
      "abstract": "Abstract Across languages, children map words to meaning with great efficiency, despite a seemingly unconstrained space of potential mappings. The literature on how children do this is primarily limited to spoken language. This leaves a gap in our understanding of sign language acquisition, because several of the hypothesized mechanisms that children use are visual (e.g., visual attention to the referent), and sign languages are perceived in the visual modality. Here, we used the Human Simulation Paradigm in American Sign Language (ASL) to determine potential cues to word learning. Sign‐naïve adult participants viewed video clips of parent–child interactions in ASL, and at a designated point, had to guess what ASL sign the parent produced. Across two studies, we demonstrate that referential clarity in ASL interactions is characterized by access to information about word class and referent presence (for verbs), similarly to spoken language. Unlike spoken language, iconicity is a cue to word meaning in ASL, although this is not always a fruitful cue. We also present evidence that verbs are highlighted well in the input, relative to spoken English. The results shed light on both similarities and differences in the information that learners may have access to in acquiring signed versus spoken languages.",
      "doi": "https://doi.org/10.1111/cogs.13061",
      "openalex_id": "https://openalex.org/W3215451835",
      "arxiv_id": "",
      "publication_date": "2021-12-01",
      "published": "2021-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper studies word-learning cues in ASL via a human simulation paradigm and does not involve discrete audio tokenization or conversion of audio into a finite token sequence, so it does not meet the inclusion criteria requiring discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper studies word-learning cues in ASL via a human simulation paradigm and does not involve discrete audio tokenization or conversion of audio into a finite token sequence, so it does not meet the inclusion criteria requiring discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on how children map words to meanings in American Sign Language, using visual cues rather than audio, and does not discuss any audio or speech tokenization methods, discrete audio token design, or analysis relevant to continuous audio signals being tokenized into discrete units. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on how children map words to meanings in American Sign Language, using visual cues rather than audio, and does not discuss any audio or speech tokenization methods, discrete audio token design, or analysis relevant to continuous audio signals being tokenized into discrete units. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to discrete audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Simultaneous structures in sign languages: Acquisition and emergence",
    "abstract": "The visual-gestural modality affords its users simultaneous movement of several independent articulators and thus lends itself to simultaneous encoding of information. Much research has focused on the fact that sign languages coordinate two manual articulators in addition to a range of non-manual articulators to present different types of linguistic information simultaneously, from phonological contrasts to inflection, spatial relations, and information structure. Children and adults acquiring a signed language arguably thus need to comprehend and produce simultaneous structures to a greater extent than individuals acquiring a spoken language. In this paper, we discuss the simultaneous encoding that is found in emerging and established sign languages; we also discuss places where sign languages are unexpectedly sequential. We explore potential constraints on simultaneity in cognition and motor coordination that might impact the acquisition and use of simultaneous structures.",
    "metadata": {
      "title": "Simultaneous structures in sign languages: Acquisition and emergence",
      "summary": "The visual-gestural modality affords its users simultaneous movement of several independent articulators and thus lends itself to simultaneous encoding of information. Much research has focused on the fact that sign languages coordinate two manual articulators in addition to a range of non-manual articulators to present different types of linguistic information simultaneously, from phonological contrasts to inflection, spatial relations, and information structure. Children and adults acquiring a signed language arguably thus need to comprehend and produce simultaneous structures to a greater extent than individuals acquiring a spoken language. In this paper, we discuss the simultaneous encoding that is found in emerging and established sign languages; we also discuss places where sign languages are unexpectedly sequential. We explore potential constraints on simultaneity in cognition and motor coordination that might impact the acquisition and use of simultaneous structures.",
      "abstract": "The visual-gestural modality affords its users simultaneous movement of several independent articulators and thus lends itself to simultaneous encoding of information. Much research has focused on the fact that sign languages coordinate two manual articulators in addition to a range of non-manual articulators to present different types of linguistic information simultaneously, from phonological contrasts to inflection, spatial relations, and information structure. Children and adults acquiring a signed language arguably thus need to comprehend and produce simultaneous structures to a greater extent than individuals acquiring a spoken language. In this paper, we discuss the simultaneous encoding that is found in emerging and established sign languages; we also discuss places where sign languages are unexpectedly sequential. We explore potential constraints on simultaneity in cognition and motor coordination that might impact the acquisition and use of simultaneous structures.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.992589",
      "openalex_id": "https://openalex.org/W4312188350",
      "arxiv_id": "",
      "publication_date": "2022-12-22",
      "published": "2022-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on sign languages and simultaneity without discussing discrete audio tokenization or any discrete speech token representations, so it does not meet the inclusion criteria requiring analysis of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on sign languages and simultaneity without discussing discrete audio tokenization or any discrete speech token representations, so it does not meet the inclusion criteria requiring analysis of discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simultaneous structures in sign languages and discusses linguistic and cognitive aspects of visual-gestural communication, without any mention of audio or speech tokenization, discrete audio tokenization methods, or analysis related to audio signals, which are core to the inclusion criteria. Therefore, it does not meet the requirement to analyze or propose discrete audio/speech tokenization, nor is it related to audio reconstructions or discrete token representations relevant to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simultaneous structures in sign languages and discusses linguistic and cognitive aspects of visual-gestural communication, without any mention of audio or speech tokenization, discrete audio tokenization methods, or analysis related to audio signals, which are core to the inclusion criteria. Therefore, it does not meet the requirement to analyze or propose discrete audio/speech tokenization, nor is it related to audio reconstructions or discrete token representations relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HRSpecNET: A Deep Learning-Based High-Resolution Radar Micro-Doppler Signature Reconstruction for Improved HAR Classification",
    "abstract": "Micro-Doppler signatures (μ-DS) are widely used for human activity recognition (HAR) using radar. However, traditional methods for generating μ-DS, such as the Short-Time Fourier Transform (STFT), suffer from limitations, such as the trade-off between time and frequency resolution, noise sensitivity, and parameter calibration. To address these limitations, we propose a novel deep learning-based approach to reconstruct high-resolution μ-DS directly from 1D complex time-domain signal. Our deep learning architecture consists of an autoencoder block to improve signal-to-noise ratio (SNR), an STFT block to learn frequency transformations to generate pseudo spectrograms, and finally, a UNET block to reconstruct high-resolution spectrogram images. We evaluated our proposed architecture on both synthetic and real-world data. For synthetic data, we generated 1D complex time domain signals with multiple time-varying frequencies and evaluated and compared the ability of our network to generate high-resolution μ-DS and perform in different SNR levels. For real-world data, a challenging radar-based American Sign Language (ASL) dataset consisting of 100 words was used to evaluate the classification performance achieved using the μ-DS generated by the proposed approach. The results showed that the proposed approach outperforms the classification accuracy of traditional STFT-based μ-DS by 3.48%. Both synthetic and experimental μ-DS show that the proposed approach learns to reconstruct higher-resolution and sparser spectrograms.",
    "metadata": {
      "title": "HRSpecNET: A Deep Learning-Based High-Resolution Radar Micro-Doppler Signature Reconstruction for Improved HAR Classification",
      "summary": "Micro-Doppler signatures (μ-DS) are widely used for human activity recognition (HAR) using radar. However, traditional methods for generating μ-DS, such as the Short-Time Fourier Transform (STFT), suffer from limitations, such as the trade-off between time and frequency resolution, noise sensitivity, and parameter calibration. To address these limitations, we propose a novel deep learning-based approach to reconstruct high-resolution μ-DS directly from 1D complex time-domain signal. Our deep learning architecture consists of an autoencoder block to improve signal-to-noise ratio (SNR), an STFT block to learn frequency transformations to generate pseudo spectrograms, and finally, a UNET block to reconstruct high-resolution spectrogram images. We evaluated our proposed architecture on both synthetic and real-world data. For synthetic data, we generated 1D complex time domain signals with multiple time-varying frequencies and evaluated and compared the ability of our network to generate high-resolution μ-DS and perform in different SNR levels. For real-world data, a challenging radar-based American Sign Language (ASL) dataset consisting of 100 words was used to evaluate the classification performance achieved using the μ-DS generated by the proposed approach. The results showed that the proposed approach outperforms the classification accuracy of traditional STFT-based μ-DS by 3.48%. Both synthetic and experimental μ-DS show that the proposed approach learns to reconstruct higher-resolution and sparser spectrograms.",
      "abstract": "Micro-Doppler signatures (μ-DS) are widely used for human activity recognition (HAR) using radar. However, traditional methods for generating μ-DS, such as the Short-Time Fourier Transform (STFT), suffer from limitations, such as the trade-off between time and frequency resolution, noise sensitivity, and parameter calibration. To address these limitations, we propose a novel deep learning-based approach to reconstruct high-resolution μ-DS directly from 1D complex time-domain signal. Our deep learning architecture consists of an autoencoder block to improve signal-to-noise ratio (SNR), an STFT block to learn frequency transformations to generate pseudo spectrograms, and finally, a UNET block to reconstruct high-resolution spectrogram images. We evaluated our proposed architecture on both synthetic and real-world data. For synthetic data, we generated 1D complex time domain signals with multiple time-varying frequencies and evaluated and compared the ability of our network to generate high-resolution μ-DS and perform in different SNR levels. For real-world data, a challenging radar-based American Sign Language (ASL) dataset consisting of 100 words was used to evaluate the classification performance achieved using the μ-DS generated by the proposed approach. The results showed that the proposed approach outperforms the classification accuracy of traditional STFT-based μ-DS by 3.48%. Both synthetic and experimental μ-DS show that the proposed approach learns to reconstruct higher-resolution and sparser spectrograms.",
      "doi": "https://doi.org/10.1109/trs.2024.3396172",
      "openalex_id": "https://openalex.org/W4396594900",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on reconstructing high-resolution micro-Doppler spectrograms for HAR using deep learning; it deals with continuous radar signals and spectrogram representations rather than discrete audio tokenization or a vocabulary-based token approach, so it does not meet the discrete tokenization inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on reconstructing high-resolution micro-Doppler spectrograms for HAR using deep learning; it deals with continuous radar signals and spectrogram representations rather than discrete audio tokenization or a vocabulary-based token approach, so it does not meet the discrete tokenization inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reconstructing high-resolution radar Micro-Doppler signatures for human activity recognition using deep learning, without any mention or analysis of discrete audio or speech tokenization; it deals with radar signal processing rather than audio tokenization into discrete symbols for language or speech-related tasks, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reconstructing high-resolution radar Micro-Doppler signatures for human activity recognition using deep learning, without any mention or analysis of discrete audio or speech tokenization; it deals with radar signal processing rather than audio tokenization into discrete symbols for language or speech-related tasks, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonological Proximity in Costa Rican Sign Language",
    "abstract": "The study of phonological proximity makes it possible to establish a basis for future decision-making in the treatment of sign languages. Knowing how close a set of signs are allows the interested party to decide more easily its study by clustering, as well as the teaching of the language to third parties based on similarities. In addition, it lays the foundation for strengthening disambiguation modules in automatic recognition systems. To the best of our knowledge, this is the first study of its kind for Costa Rican Sign Language (LESCO, for its Spanish acronym), and forms the basis for one of the modules of the already operational system of sign and speech editing called the International Platform for Sign Language Edition (PIELS). A database of 2665 signs, grouped into eight contexts, is used, and a comparison of similarity measures is made, using standard statistical formulas to measure their degree of correlation. This corpus will be especially useful in machine learning approaches. In this work, we have proposed an analysis of different similarity measures between signs in order to find out the phonological proximity between them. After analyzing the results obtained, we can conclude that LESCO is a sign language with high levels of phonological proximity, particularly in the orientation and location components, but they are noticeably lower in the form component. We have also concluded as an outstanding contribution of our research that automatic recognition systems can take as a basis for their first prototypes the contexts or sign domains that map to clusters with lower levels of similarity. As mentioned, the results obtained have multiple applications such as in the teaching area or the Natural Language Processing area for automatic recognition tasks.",
    "metadata": {
      "title": "Phonological Proximity in Costa Rican Sign Language",
      "summary": "The study of phonological proximity makes it possible to establish a basis for future decision-making in the treatment of sign languages. Knowing how close a set of signs are allows the interested party to decide more easily its study by clustering, as well as the teaching of the language to third parties based on similarities. In addition, it lays the foundation for strengthening disambiguation modules in automatic recognition systems. To the best of our knowledge, this is the first study of its kind for Costa Rican Sign Language (LESCO, for its Spanish acronym), and forms the basis for one of the modules of the already operational system of sign and speech editing called the International Platform for Sign Language Edition (PIELS). A database of 2665 signs, grouped into eight contexts, is used, and a comparison of similarity measures is made, using standard statistical formulas to measure their degree of correlation. This corpus will be especially useful in machine learning approaches. In this work, we have proposed an analysis of different similarity measures between signs in order to find out the phonological proximity between them. After analyzing the results obtained, we can conclude that LESCO is a sign language with high levels of phonological proximity, particularly in the orientation and location components, but they are noticeably lower in the form component. We have also concluded as an outstanding contribution of our research that automatic recognition systems can take as a basis for their first prototypes the contexts or sign domains that map to clusters with lower levels of similarity. As mentioned, the results obtained have multiple applications such as in the teaching area or the Natural Language Processing area for automatic recognition tasks.",
      "abstract": "The study of phonological proximity makes it possible to establish a basis for future decision-making in the treatment of sign languages. Knowing how close a set of signs are allows the interested party to decide more easily its study by clustering, as well as the teaching of the language to third parties based on similarities. In addition, it lays the foundation for strengthening disambiguation modules in automatic recognition systems. To the best of our knowledge, this is the first study of its kind for Costa Rican Sign Language (LESCO, for its Spanish acronym), and forms the basis for one of the modules of the already operational system of sign and speech editing called the International Platform for Sign Language Edition (PIELS). A database of 2665 signs, grouped into eight contexts, is used, and a comparison of similarity measures is made, using standard statistical formulas to measure their degree of correlation. This corpus will be especially useful in machine learning approaches. In this work, we have proposed an analysis of different similarity measures between signs in order to find out the phonological proximity between them. After analyzing the results obtained, we can conclude that LESCO is a sign language with high levels of phonological proximity, particularly in the orientation and location components, but they are noticeably lower in the form component. We have also concluded as an outstanding contribution of our research that automatic recognition systems can take as a basis for their first prototypes the contexts or sign domains that map to clusters with lower levels of similarity. As mentioned, the results obtained have multiple applications such as in the teaching area or the Natural Language Processing area for automatic recognition tasks.",
      "doi": "https://doi.org/10.3390/electronics9081302",
      "openalex_id": "https://openalex.org/W3048407964",
      "arxiv_id": "",
      "publication_date": "2020-08-13",
      "published": "2020-08-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit discrete audio/speech tokenization as a core object of study; this article analyzes phonological proximity in a sign language and discusses similarity measures for signs, with no discrete audio tokenization or speech token representations, so it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit discrete audio/speech tokenization as a core object of study; this article analyzes phonological proximity in a sign language and discusses similarity measures for signs, with no discrete audio tokenization or speech token representations, so it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological proximity in Costa Rican Sign Language, analyzing similarity measures between signs based on their phonological components, without discussing discrete audio or speech tokenization, discrete quantization, or audio signal processing; thus, it does not meet the inclusion criteria centered on discrete audio/speech tokenization and instead deals with sign language visual features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological proximity in Costa Rican Sign Language, analyzing similarity measures between signs based on their phonological components, without discussing discrete audio or speech tokenization, discrete quantization, or audio signal processing; thus, it does not meet the inclusion criteria centered on discrete audio/speech tokenization and instead deals with sign language visual features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Give Me a Sign: Using Data Gloves for Static Hand-Shape Recognition",
    "abstract": "Human-to-human communication via the computer is mainly carried out using a keyboard or microphone. In the field of virtual reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g., silent commands during task-force training) or simply not possible (e.g., if the user has hearing loss). Data gloves help to increase immersion within VR, as they correspond to our natural interaction. At the same time, they offer the possibility of accurately capturing hand shapes, such as those used in non-verbal communication (e.g., thumbs up, okay gesture, …) and in sign language. In this paper, we present a hand-shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an outlier detection and a feature selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial data augmentation, i.e., we created new artificial data from the recorded and filtered data to augment the training data set. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. The voting meta-classifier (VL2) proved to be the most accurate, albeit slowest, classifier. A good alternative is random forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. outlier detection was proven to be an effective approach, especially in improving the classification time. Overall, we have shown that our hand-shape recognition system using data gloves is suitable for communication within VR.",
    "metadata": {
      "title": "Give Me a Sign: Using Data Gloves for Static Hand-Shape Recognition",
      "summary": "Human-to-human communication via the computer is mainly carried out using a keyboard or microphone. In the field of virtual reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g., silent commands during task-force training) or simply not possible (e.g., if the user has hearing loss). Data gloves help to increase immersion within VR, as they correspond to our natural interaction. At the same time, they offer the possibility of accurately capturing hand shapes, such as those used in non-verbal communication (e.g., thumbs up, okay gesture, …) and in sign language. In this paper, we present a hand-shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an outlier detection and a feature selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial data augmentation, i.e., we created new artificial data from the recorded and filtered data to augment the training data set. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. The voting meta-classifier (VL2) proved to be the most accurate, albeit slowest, classifier. A good alternative is random forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. outlier detection was proven to be an effective approach, especially in improving the classification time. Overall, we have shown that our hand-shape recognition system using data gloves is suitable for communication within VR.",
      "abstract": "Human-to-human communication via the computer is mainly carried out using a keyboard or microphone. In the field of virtual reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g., silent commands during task-force training) or simply not possible (e.g., if the user has hearing loss). Data gloves help to increase immersion within VR, as they correspond to our natural interaction. At the same time, they offer the possibility of accurately capturing hand shapes, such as those used in non-verbal communication (e.g., thumbs up, okay gesture, …) and in sign language. In this paper, we present a hand-shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an outlier detection and a feature selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial data augmentation, i.e., we created new artificial data from the recorded and filtered data to augment the training data set. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. The voting meta-classifier (VL2) proved to be the most accurate, albeit slowest, classifier. A good alternative is random forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. outlier detection was proven to be an effective approach, especially in improving the classification time. Overall, we have shown that our hand-shape recognition system using data gloves is suitable for communication within VR.",
      "doi": "https://doi.org/10.3390/s23249847",
      "openalex_id": "https://openalex.org/W4389922996",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "No discrete audio tokenization is analyzed; paper studies hand-gesture recognition with data gloves rather than speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "No discrete audio tokenization is analyzed; paper studies hand-gesture recognition with data gloves rather than speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hand-shape recognition using data gloves within virtual reality, with no mention of audio or speech tokenization, discrete audio symbol sequences, or analysis of audio tokenization methods; thus, it does not meet the inclusion criteria related to audio/speech tokenization and instead studies a non-audio modality.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hand-shape recognition using data gloves within virtual reality, with no mention of audio or speech tokenization, discrete audio symbol sequences, or analysis of audio tokenization methods; thus, it does not meet the inclusion criteria related to audio/speech tokenization and instead studies a non-audio modality.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Impact of face swapping and data augmentation on sign language recognition",
    "abstract": "Abstract This study addresses the challenge of improving communication between the deaf and hearing community by exploring different sign language recognition (SLR) techniques. Due to privacy issues and the need for validation by interpreters, creating large-scale sign language (SL) datasets can be difficult. The authors address this by presenting a new Spanish isolated sign language recognition dataset, CALSE-1000, consisting of 5000 videos representing 1000 glosses, with various signers and scenarios. The study also proposes using different computer vision techniques, such as face swapping and affine transformations, to augment the SL dataset and improve the accuracy of the model I3D trained using them. The results show that the inclusion of these augmentations during training leads to an improvement in accuracy in top-1 metrics by up to 11.7 points, top-5 by up to 8.8 points and top-10 by up to 9 points. This has great potential to improve the state of the art in other datasets and other models. Furthermore, the analysis confirms the importance of facial expressions in the model by testing with a facial omission dataset and shows how face swapping can be used to include new anonymous signers without the costly and time-consuming process of recording.",
    "metadata": {
      "title": "Impact of face swapping and data augmentation on sign language recognition",
      "summary": "Abstract This study addresses the challenge of improving communication between the deaf and hearing community by exploring different sign language recognition (SLR) techniques. Due to privacy issues and the need for validation by interpreters, creating large-scale sign language (SL) datasets can be difficult. The authors address this by presenting a new Spanish isolated sign language recognition dataset, CALSE-1000, consisting of 5000 videos representing 1000 glosses, with various signers and scenarios. The study also proposes using different computer vision techniques, such as face swapping and affine transformations, to augment the SL dataset and improve the accuracy of the model I3D trained using them. The results show that the inclusion of these augmentations during training leads to an improvement in accuracy in top-1 metrics by up to 11.7 points, top-5 by up to 8.8 points and top-10 by up to 9 points. This has great potential to improve the state of the art in other datasets and other models. Furthermore, the analysis confirms the importance of facial expressions in the model by testing with a facial omission dataset and shows how face swapping can be used to include new anonymous signers without the costly and time-consuming process of recording.",
      "abstract": "Abstract This study addresses the challenge of improving communication between the deaf and hearing community by exploring different sign language recognition (SLR) techniques. Due to privacy issues and the need for validation by interpreters, creating large-scale sign language (SL) datasets can be difficult. The authors address this by presenting a new Spanish isolated sign language recognition dataset, CALSE-1000, consisting of 5000 videos representing 1000 glosses, with various signers and scenarios. The study also proposes using different computer vision techniques, such as face swapping and affine transformations, to augment the SL dataset and improve the accuracy of the model I3D trained using them. The results show that the inclusion of these augmentations during training leads to an improvement in accuracy in top-1 metrics by up to 11.7 points, top-5 by up to 8.8 points and top-10 by up to 9 points. This has great potential to improve the state of the art in other datasets and other models. Furthermore, the analysis confirms the importance of facial expressions in the model by testing with a facial omission dataset and shows how face swapping can be used to include new anonymous signers without the costly and time-consuming process of recording.",
      "doi": "https://doi.org/10.1007/s10209-024-01133-y",
      "openalex_id": "https://openalex.org/W4400965347",
      "arxiv_id": "",
      "publication_date": "2024-07-24",
      "published": "2024-07-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about sign language recognition using video data with face swapping augmentation; it does not address discrete audio/tokenization of speech, nor analyze or propose any audio tokenization method, and the full text in English is present. Therefore it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about sign language recognition using video data with face swapping augmentation; it does not address discrete audio/tokenization of speech, nor analyze or propose any audio tokenization method, and the full text in English is present. Therefore it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition using video data and computer vision techniques such as face swapping and data augmentation, without any mention of discrete audio or speech tokenization as a core subject. It does not address converting continuous audio signals into discrete token sequences or analyze discrete audio representations, which fails the inclusion criteria and matches several exclusion criteria related to the absence of discrete audio tokenization. Therefore, it should be excluded based on the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition using video data and computer vision techniques such as face swapping and data augmentation, without any mention of discrete audio or speech tokenization as a core subject. It does not address converting continuous audio signals into discrete token sequences or analyze discrete audio representations, which fails the inclusion criteria and matches several exclusion criteria related to the absence of discrete audio tokenization. Therefore, it should be excluded based on the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural effects differ for learning highly iconic versus non-iconic signs in hearing adults",
    "abstract": "Abstract Little is known about the neural changes that accompany sign language learning by hearing adults. We used ERPs and a word-sign matching task to assess how learning impacted the N400 priming effect (reduced negativity for translations compared to unrelated trials). English monolinguals (N = 32) learned 100 ASL signs – half highly iconic (meaning was guessable), half non-iconic. In contrast to non-iconic signs, little learning was needed for the highly iconic signs as translation accuracy was similar pre- and post-learning. Prior to learning, an N400 priming effect was observed only for iconic signs. After learning, the size of the priming effect increased for non-iconic signs (replicating word learning studies) but decreased for iconic signs. For deaf ASL signers (N = 20), iconicity did not modulate the size of the N400 priming effect. We conclude that the impact of iconicity on lexico-semantic processing is reduced following learning, as signs are integrated into an emerging visual-manual lexicon.",
    "metadata": {
      "title": "Neural effects differ for learning highly iconic versus non-iconic signs in hearing adults",
      "summary": "Abstract Little is known about the neural changes that accompany sign language learning by hearing adults. We used ERPs and a word-sign matching task to assess how learning impacted the N400 priming effect (reduced negativity for translations compared to unrelated trials). English monolinguals (N = 32) learned 100 ASL signs – half highly iconic (meaning was guessable), half non-iconic. In contrast to non-iconic signs, little learning was needed for the highly iconic signs as translation accuracy was similar pre- and post-learning. Prior to learning, an N400 priming effect was observed only for iconic signs. After learning, the size of the priming effect increased for non-iconic signs (replicating word learning studies) but decreased for iconic signs. For deaf ASL signers (N = 20), iconicity did not modulate the size of the N400 priming effect. We conclude that the impact of iconicity on lexico-semantic processing is reduced following learning, as signs are integrated into an emerging visual-manual lexicon.",
      "abstract": "Abstract Little is known about the neural changes that accompany sign language learning by hearing adults. We used ERPs and a word-sign matching task to assess how learning impacted the N400 priming effect (reduced negativity for translations compared to unrelated trials). English monolinguals (N = 32) learned 100 ASL signs – half highly iconic (meaning was guessable), half non-iconic. In contrast to non-iconic signs, little learning was needed for the highly iconic signs as translation accuracy was similar pre- and post-learning. Prior to learning, an N400 priming effect was observed only for iconic signs. After learning, the size of the priming effect increased for non-iconic signs (replicating word learning studies) but decreased for iconic signs. For deaf ASL signers (N = 20), iconicity did not modulate the size of the N400 priming effect. We conclude that the impact of iconicity on lexico-semantic processing is reduced following learning, as signs are integrated into an emerging visual-manual lexicon.",
      "doi": "https://doi.org/10.1017/s1366728923000809",
      "openalex_id": "https://openalex.org/W4388928887",
      "arxiv_id": "",
      "publication_date": "2023-11-23",
      "published": "2023-11-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item examines sign language learning and neural N400 effects but does not involve discrete audio tokenization or token-based representations of speech. It focuses on sign language (visual-manual) processing rather than converting continuous audio into discrete tokens for speech tasks; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item examines sign language learning and neural N400 effects but does not involve discrete audio tokenization or token-based representations of speech. It focuses on sign language (visual-manual) processing rather than converting continuous audio into discrete tokens for speech tasks; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural effects of learning sign language in hearing adults, particularly analyzing ERP responses to iconic versus non-iconic signs, which are visual-manual language elements; it does not discuss audio or speech tokenization, discrete audio symbols, or any kind of audio signal processing or tokenization methodology relevant to audio reconstruction or understanding. Hence, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to the core audio/speech tokenization tasks specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural effects of learning sign language in hearing adults, particularly analyzing ERP responses to iconic versus non-iconic signs, which are visual-manual language elements; it does not discuss audio or speech tokenization, discrete audio symbols, or any kind of audio signal processing or tokenization methodology relevant to audio reconstruction or understanding. Hence, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to the core audio/speech tokenization tasks specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Interactive learning of natural sign language with radar",
    "abstract": "Abstract Over the past decade, there have been great advancements in radio frequency sensor technology for human–computer interaction applications, such as gesture recognition, and human activity recognition more broadly. While there is a significant amount of study on these topics, in most cases, experimental data are acquired in controlled settings by directing participants what motion to articulate. However, especially for communicative motions, such as sign language, such directed data sets do not accurately capture natural, in situ articulations. This results in a difference in the distribution of directed American Sign Language (ASL) versus natural ASL, which severely degrades natural sign language recognition in real‐world scenarios. To overcome these challenges and acquire more representative data for training deep models, the authors develop an interactive gaming environment, ChessSIGN, which records video and radar data of participants as they play the game without any external direction . The authors investigate various ways of generating synthetic samples from directed ASL data, but show that ultimately such data does not offer much improvement over just initialising using imagery from ImageNet. In contrast, an interactive learning paradigm is proposed by the authors in which model training is shown to improve as more and more natural ASL samples are acquired and augmented via synthetic samples generated from a physics‐aware generative adversarial network. The authors show that the proposed approach enables the recognition of natural ASL in a real‐world setting, achieving an accuracy of 69% for 29 ASL signs—a 60% improvement over conventional training with directed ASL data.",
    "metadata": {
      "title": "Interactive learning of natural sign language with radar",
      "summary": "Abstract Over the past decade, there have been great advancements in radio frequency sensor technology for human–computer interaction applications, such as gesture recognition, and human activity recognition more broadly. While there is a significant amount of study on these topics, in most cases, experimental data are acquired in controlled settings by directing participants what motion to articulate. However, especially for communicative motions, such as sign language, such directed data sets do not accurately capture natural, in situ articulations. This results in a difference in the distribution of directed American Sign Language (ASL) versus natural ASL, which severely degrades natural sign language recognition in real‐world scenarios. To overcome these challenges and acquire more representative data for training deep models, the authors develop an interactive gaming environment, ChessSIGN, which records video and radar data of participants as they play the game without any external direction . The authors investigate various ways of generating synthetic samples from directed ASL data, but show that ultimately such data does not offer much improvement over just initialising using imagery from ImageNet. In contrast, an interactive learning paradigm is proposed by the authors in which model training is shown to improve as more and more natural ASL samples are acquired and augmented via synthetic samples generated from a physics‐aware generative adversarial network. The authors show that the proposed approach enables the recognition of natural ASL in a real‐world setting, achieving an accuracy of 69% for 29 ASL signs—a 60% improvement over conventional training with directed ASL data.",
      "abstract": "Abstract Over the past decade, there have been great advancements in radio frequency sensor technology for human–computer interaction applications, such as gesture recognition, and human activity recognition more broadly. While there is a significant amount of study on these topics, in most cases, experimental data are acquired in controlled settings by directing participants what motion to articulate. However, especially for communicative motions, such as sign language, such directed data sets do not accurately capture natural, in situ articulations. This results in a difference in the distribution of directed American Sign Language (ASL) versus natural ASL, which severely degrades natural sign language recognition in real‐world scenarios. To overcome these challenges and acquire more representative data for training deep models, the authors develop an interactive gaming environment, ChessSIGN, which records video and radar data of participants as they play the game without any external direction . The authors investigate various ways of generating synthetic samples from directed ASL data, but show that ultimately such data does not offer much improvement over just initialising using imagery from ImageNet. In contrast, an interactive learning paradigm is proposed by the authors in which model training is shown to improve as more and more natural ASL samples are acquired and augmented via synthetic samples generated from a physics‐aware generative adversarial network. The authors show that the proposed approach enables the recognition of natural ASL in a real‐world setting, achieving an accuracy of 69% for 29 ASL signs—a 60% improvement over conventional training with directed ASL data.",
      "doi": "https://doi.org/10.1049/rsn2.12565",
      "openalex_id": "https://openalex.org/W4396829806",
      "arxiv_id": "",
      "publication_date": "2024-05-10",
      "published": "2024-05-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets radar/vision-based natural ASL recognition and synthetic data with interactive learning, not discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets radar/vision-based natural ASL recognition and synthetic data with interactive learning, not discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on recognizing natural American Sign Language using radar and video data through deep learning, and does not involve any form of discrete audio/speech tokenization nor analyze methods converting continuous audio signals to discrete symbols, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on recognizing natural American Sign Language using radar and video data through deep learning, and does not involve any form of discrete audio/speech tokenization nor analyze methods converting continuous audio signals to discrete symbols, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Literacy Instruction for Students Who are Deaf and Hard of Hearing",
    "abstract": "Abstract Responsive, high-quality literacy instruction is critical for deaf and hard-of-hearing (DHH) students as they may not be in an environment that provides full access to language and/or receive insufficient instruction. This second edition of Literacy Instruction for Students Who Are Deaf or Hard-of-Hearing updates previous findings and describes current, evidence-based practices in teaching literacy to DHH learners. In addition to application examples across chapters, a stand-alone appendix provides brief explanations of each strategy, the areas of literacy to which they relate, and references in support of each strategy. Beal, Dostal, and Easterbrooks provide educators and parents with a process for determining which literacy and language assessments are appropriate for individual DHH learners and whether an instructional practice is supported by evidence or causal factors. They describe the literacy process with an overview of related learning theories, language and literacy assessments, and evidence-based instructional strategies across the following domains of literacy: phonemic awareness, phonics, vocabulary, fluency, and comprehension. They also weave evidence-based writing strategies and case vignettes that highlight application of assessments and instructional approaches within each of these literacy areas. Finally, they review lingering questions related to literacy instruction for DHH learners. Educators and caregivers who provide literacy instruction to DHH learners will benefit from the breadth and depth of literacy content provided in this concise literacy textbook.",
    "metadata": {
      "title": "Literacy Instruction for Students Who are Deaf and Hard of Hearing",
      "summary": "Abstract Responsive, high-quality literacy instruction is critical for deaf and hard-of-hearing (DHH) students as they may not be in an environment that provides full access to language and/or receive insufficient instruction. This second edition of Literacy Instruction for Students Who Are Deaf or Hard-of-Hearing updates previous findings and describes current, evidence-based practices in teaching literacy to DHH learners. In addition to application examples across chapters, a stand-alone appendix provides brief explanations of each strategy, the areas of literacy to which they relate, and references in support of each strategy. Beal, Dostal, and Easterbrooks provide educators and parents with a process for determining which literacy and language assessments are appropriate for individual DHH learners and whether an instructional practice is supported by evidence or causal factors. They describe the literacy process with an overview of related learning theories, language and literacy assessments, and evidence-based instructional strategies across the following domains of literacy: phonemic awareness, phonics, vocabulary, fluency, and comprehension. They also weave evidence-based writing strategies and case vignettes that highlight application of assessments and instructional approaches within each of these literacy areas. Finally, they review lingering questions related to literacy instruction for DHH learners. Educators and caregivers who provide literacy instruction to DHH learners will benefit from the breadth and depth of literacy content provided in this concise literacy textbook.",
      "abstract": "Abstract Responsive, high-quality literacy instruction is critical for deaf and hard-of-hearing (DHH) students as they may not be in an environment that provides full access to language and/or receive insufficient instruction. This second edition of Literacy Instruction for Students Who Are Deaf or Hard-of-Hearing updates previous findings and describes current, evidence-based practices in teaching literacy to DHH learners. In addition to application examples across chapters, a stand-alone appendix provides brief explanations of each strategy, the areas of literacy to which they relate, and references in support of each strategy. Beal, Dostal, and Easterbrooks provide educators and parents with a process for determining which literacy and language assessments are appropriate for individual DHH learners and whether an instructional practice is supported by evidence or causal factors. They describe the literacy process with an overview of related learning theories, language and literacy assessments, and evidence-based instructional strategies across the following domains of literacy: phonemic awareness, phonics, vocabulary, fluency, and comprehension. They also weave evidence-based writing strategies and case vignettes that highlight application of assessments and instructional approaches within each of these literacy areas. Finally, they review lingering questions related to literacy instruction for DHH learners. Educators and caregivers who provide literacy instruction to DHH learners will benefit from the breadth and depth of literacy content provided in this concise literacy textbook.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.001.0001",
      "openalex_id": "https://openalex.org/W4401771534",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses literacy instruction for deaf and hard-of-hearing students and reviews teaching strategies; it does not address discrete audio/speech tokenization, making it inapplicable to tokenization-focused inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses literacy instruction for deaf and hard-of-hearing students and reviews teaching strategies; it does not address discrete audio/speech tokenization, making it inapplicable to tokenization-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on literacy instruction for deaf and hard-of-hearing students, emphasizing teaching strategies, assessments, and literacy development, with no mention or focus on discrete audio or speech tokenization methods, audio signal processing, or neural encoding techniques related to audio tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization nor does it address any tokenization-related methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on literacy instruction for deaf and hard-of-hearing students, emphasizing teaching strategies, assessments, and literacy development, with no mention or focus on discrete audio or speech tokenization methods, audio signal processing, or neural encoding techniques related to audio tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization nor does it address any tokenization-related methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The organization of the American Sign Language lexicon: Comparing one- and two-parameter ERP phonological priming effects across tasks",
    "abstract": "We used phonological priming and ERPs to investigate the organization of the lexicon in American Sign Language. Across go/no-go repetition detection and semantic categorization tasks, targets in related pairs that shared handshape and location elicited smaller N400s than targets in unrelated pairs, indicative of facilitated processing. Handshape-related targets also elicited smaller N400s than unrelated targets, but only in the repetition task. The location priming effect reversed direction across tasks, with slightlylargeramplitude N400s for targets in related versus unrelated pairs in the semantic task, indicative of interference. These patterns imply that handshape and location play different roles during sign recognition and that there is a hierarchical organization for the sign lexicon. Similar to interactive-activation models of word recognition, we argue for differentiation between sublexical facilitation and lexical competition. Lexical competition is primarily driven by the location parameter and is more engaged when identification of single lexico-semantic entries is required.",
    "metadata": {
      "title": "The organization of the American Sign Language lexicon: Comparing one- and two-parameter ERP phonological priming effects across tasks",
      "summary": "We used phonological priming and ERPs to investigate the organization of the lexicon in American Sign Language. Across go/no-go repetition detection and semantic categorization tasks, targets in related pairs that shared handshape and location elicited smaller N400s than targets in unrelated pairs, indicative of facilitated processing. Handshape-related targets also elicited smaller N400s than unrelated targets, but only in the repetition task. The location priming effect reversed direction across tasks, with slightlylargeramplitude N400s for targets in related versus unrelated pairs in the semantic task, indicative of interference. These patterns imply that handshape and location play different roles during sign recognition and that there is a hierarchical organization for the sign lexicon. Similar to interactive-activation models of word recognition, we argue for differentiation between sublexical facilitation and lexical competition. Lexical competition is primarily driven by the location parameter and is more engaged when identification of single lexico-semantic entries is required.",
      "abstract": "We used phonological priming and ERPs to investigate the organization of the lexicon in American Sign Language. Across go/no-go repetition detection and semantic categorization tasks, targets in related pairs that shared handshape and location elicited smaller N400s than targets in unrelated pairs, indicative of facilitated processing. Handshape-related targets also elicited smaller N400s than unrelated targets, but only in the repetition task. The location priming effect reversed direction across tasks, with slightlylargeramplitude N400s for targets in related versus unrelated pairs in the semantic task, indicative of interference. These patterns imply that handshape and location play different roles during sign recognition and that there is a hierarchical organization for the sign lexicon. Similar to interactive-activation models of word recognition, we argue for differentiation between sublexical facilitation and lexical competition. Lexical competition is primarily driven by the location parameter and is more engaged when identification of single lexico-semantic entries is required.",
      "doi": "https://doi.org/10.1016/j.bandl.2021.104960",
      "openalex_id": "https://openalex.org/W3157566248",
      "arxiv_id": "",
      "publication_date": "2021-04-30",
      "published": "2021-04-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study investigates sign-language lexicon organization using ERP and phonological priming in ASL, with no analysis or proposal of discrete audio tokenization of speech, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study investigates sign-language lexicon organization using ERP and phonological priming in ASL, with no analysis or proposal of discrete audio tokenization of speech, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological priming and lexical organization in American Sign Language using ERP measures, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or any form of audio signal quantization; thus, it does not meet the inclusion criteria related to discrete audio tokenization and focuses instead on sign language processing without discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological priming and lexical organization in American Sign Language using ERP measures, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or any form of audio signal quantization; thus, it does not meet the inclusion criteria related to discrete audio tokenization and focuses instead on sign language processing without discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical selection in bimodal bilinguals: ERP evidence from picture-word interference",
    "abstract": "The picture word interference (PWI) paradigm and ERPs were used to investigate whether lexical selection in deaf and hearing ASL-English bilinguals occurs via lexical competition or whether the response exclusion hypothesis (REH) for PWI effects is supported. The REH predicts that semantic interference should not occur for bimodal bilinguals because sign and word responses do not compete within an output buffer. Bimodal bilinguals named pictures in ASL, preceded by either a translation equivalent, semantically-related, or unrelated English written word. In both the translation and semantically-related conditions bimodal bilinguals showed facilitation effects: reduced RTs and N400 amplitudes for related compared to unrelated prime conditions. We also observed an unexpected focal left anterior positivity that was stronger in the translation condition, which we speculate may be due to articulatory priming. Overall, the results support the REH and models of bilingual language production that assume lexical selection occurs without competition between languages.",
    "metadata": {
      "title": "Lexical selection in bimodal bilinguals: ERP evidence from picture-word interference",
      "summary": "The picture word interference (PWI) paradigm and ERPs were used to investigate whether lexical selection in deaf and hearing ASL-English bilinguals occurs via lexical competition or whether the response exclusion hypothesis (REH) for PWI effects is supported. The REH predicts that semantic interference should not occur for bimodal bilinguals because sign and word responses do not compete within an output buffer. Bimodal bilinguals named pictures in ASL, preceded by either a translation equivalent, semantically-related, or unrelated English written word. In both the translation and semantically-related conditions bimodal bilinguals showed facilitation effects: reduced RTs and N400 amplitudes for related compared to unrelated prime conditions. We also observed an unexpected focal left anterior positivity that was stronger in the translation condition, which we speculate may be due to articulatory priming. Overall, the results support the REH and models of bilingual language production that assume lexical selection occurs without competition between languages.",
      "abstract": "The picture word interference (PWI) paradigm and ERPs were used to investigate whether lexical selection in deaf and hearing ASL-English bilinguals occurs via lexical competition or whether the response exclusion hypothesis (REH) for PWI effects is supported. The REH predicts that semantic interference should not occur for bimodal bilinguals because sign and word responses do not compete within an output buffer. Bimodal bilinguals named pictures in ASL, preceded by either a translation equivalent, semantically-related, or unrelated English written word. In both the translation and semantically-related conditions bimodal bilinguals showed facilitation effects: reduced RTs and N400 amplitudes for related compared to unrelated prime conditions. We also observed an unexpected focal left anterior positivity that was stronger in the translation condition, which we speculate may be due to articulatory priming. Overall, the results support the REH and models of bilingual language production that assume lexical selection occurs without competition between languages.",
      "doi": "https://doi.org/10.1080/23273798.2020.1821905",
      "openalex_id": "https://openalex.org/W3088249661",
      "arxiv_id": "",
      "publication_date": "2020-09-21",
      "published": "2020-09-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates lexical processing in bimodal bilinguals using ERP measures; it does not propose or analyze discrete audio/speech tokenization or discrete token representations for audio reconstruction or language modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates lexical processing in bimodal bilinguals using ERP measures; it does not propose or analyze discrete audio/speech tokenization or discrete token representations for audio reconstruction or language modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates lexical selection in bimodal bilinguals using ERP and behavioral measures but does not propose, analyze, or evaluate discrete audio/speech tokenization methods as defined by the inclusion criteria; it focuses on cognitive and linguistic processes rather than audio signal discretization or quantization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates lexical selection in bimodal bilinguals using ERP and behavioral measures but does not propose, analyze, or evaluate discrete audio/speech tokenization methods as defined by the inclusion criteria; it focuses on cognitive and linguistic processes rather than audio signal discretization or quantization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pre-output Language Monitoring in Sign Production",
    "abstract": "Abstract A domain-general monitoring mechanism is proposed to be involved in overt speech monitoring. This mechanism is reflected in a medial frontal component, the error negativity (Ne), present in both errors and correct trials (Ne-like wave) but larger in errors than correct trials. In overt speech production, this negativity starts to rise before speech onset and is therefore associated with inner speech monitoring. Here, we investigate whether the same monitoring mechanism is involved in sign language production. Twenty deaf signers (American Sign Language [ASL] dominant) and 16 hearing signers (English dominant) participated in a picture–word interference paradigm in ASL. As in previous studies, ASL naming latencies were measured using the keyboard release time. EEG results revealed a medial frontal negativity peaking within 15 msec after keyboard release in the deaf signers. This negativity was larger in errors than correct trials, as previously observed in spoken language production. No clear negativity was present in the hearing signers. In addition, the slope of the Ne was correlated with ASL proficiency (measured by the ASL Sentence Repetition Task) across signers. Our results indicate that a similar medial frontal mechanism is engaged in preoutput language monitoring in sign and spoken language production. These results suggest that the monitoring mechanism reflected by the Ne/Ne-like wave is independent of output modality (i.e., spoken or signed) and likely monitors prearticulatory representations of language. Differences between groups may be linked to several factors including differences in language proficiency or more variable lexical access to motor programming latencies for hearing than deaf signers.",
    "metadata": {
      "title": "Pre-output Language Monitoring in Sign Production",
      "summary": "Abstract A domain-general monitoring mechanism is proposed to be involved in overt speech monitoring. This mechanism is reflected in a medial frontal component, the error negativity (Ne), present in both errors and correct trials (Ne-like wave) but larger in errors than correct trials. In overt speech production, this negativity starts to rise before speech onset and is therefore associated with inner speech monitoring. Here, we investigate whether the same monitoring mechanism is involved in sign language production. Twenty deaf signers (American Sign Language [ASL] dominant) and 16 hearing signers (English dominant) participated in a picture–word interference paradigm in ASL. As in previous studies, ASL naming latencies were measured using the keyboard release time. EEG results revealed a medial frontal negativity peaking within 15 msec after keyboard release in the deaf signers. This negativity was larger in errors than correct trials, as previously observed in spoken language production. No clear negativity was present in the hearing signers. In addition, the slope of the Ne was correlated with ASL proficiency (measured by the ASL Sentence Repetition Task) across signers. Our results indicate that a similar medial frontal mechanism is engaged in preoutput language monitoring in sign and spoken language production. These results suggest that the monitoring mechanism reflected by the Ne/Ne-like wave is independent of output modality (i.e., spoken or signed) and likely monitors prearticulatory representations of language. Differences between groups may be linked to several factors including differences in language proficiency or more variable lexical access to motor programming latencies for hearing than deaf signers.",
      "abstract": "Abstract A domain-general monitoring mechanism is proposed to be involved in overt speech monitoring. This mechanism is reflected in a medial frontal component, the error negativity (Ne), present in both errors and correct trials (Ne-like wave) but larger in errors than correct trials. In overt speech production, this negativity starts to rise before speech onset and is therefore associated with inner speech monitoring. Here, we investigate whether the same monitoring mechanism is involved in sign language production. Twenty deaf signers (American Sign Language [ASL] dominant) and 16 hearing signers (English dominant) participated in a picture–word interference paradigm in ASL. As in previous studies, ASL naming latencies were measured using the keyboard release time. EEG results revealed a medial frontal negativity peaking within 15 msec after keyboard release in the deaf signers. This negativity was larger in errors than correct trials, as previously observed in spoken language production. No clear negativity was present in the hearing signers. In addition, the slope of the Ne was correlated with ASL proficiency (measured by the ASL Sentence Repetition Task) across signers. Our results indicate that a similar medial frontal mechanism is engaged in preoutput language monitoring in sign and spoken language production. These results suggest that the monitoring mechanism reflected by the Ne/Ne-like wave is independent of output modality (i.e., spoken or signed) and likely monitors prearticulatory representations of language. Differences between groups may be linked to several factors including differences in language proficiency or more variable lexical access to motor programming latencies for hearing than deaf signers.",
      "doi": "https://doi.org/10.1162/jocn_a_01542",
      "openalex_id": "https://openalex.org/W3004786620",
      "arxiv_id": "",
      "publication_date": "2020-02-06",
      "published": "2020-02-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it examines language monitoring in sign production, the study does not propose or analyze discrete audio/speech tokenization or discrete token representations, and focuses on neural monitoring rather than tokenized audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it examines language monitoring in sign production, the study does not propose or analyze discrete audio/speech tokenization or discrete token representations, and focuses on neural monitoring rather than tokenized audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates neural monitoring mechanisms in sign language production without addressing the discrete tokenization of continuous audio signals or analyzing discrete audio token representations; hence, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates neural monitoring mechanisms in sign language production without addressing the discrete tokenization of continuous audio signals or analyzing discrete audio token representations; hence, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On The Dynamics of Lexical Access In Two or More Languages",
    "abstract": "Abstract The most provocative finding about bilingualism in the last two decades is that both languages are active even when bilinguals intend to use one language alone. When bilinguals hear, read, or speak words in one language, form, or translation, relatives of those words in the other language become momentarily available. The way bilingual speakers negotiate the competition produced by cross-language interactions has profound consequences for language processing and for the cognitive and neural mechanisms that it engages. In this chapter, we review the most exciting of these new discoveries. We consider how the context of language immersion induces dynamic changes in lexical access, how the native language may change, how new learning is influenced by language experience, and how brain activity reflects these consequences. These findings expose the way that bilingualism reveals the relations between language and cognition in a manner that is impenetrable in speakers of one language alone.",
    "metadata": {
      "title": "On The Dynamics of Lexical Access In Two or More Languages",
      "summary": "Abstract The most provocative finding about bilingualism in the last two decades is that both languages are active even when bilinguals intend to use one language alone. When bilinguals hear, read, or speak words in one language, form, or translation, relatives of those words in the other language become momentarily available. The way bilingual speakers negotiate the competition produced by cross-language interactions has profound consequences for language processing and for the cognitive and neural mechanisms that it engages. In this chapter, we review the most exciting of these new discoveries. We consider how the context of language immersion induces dynamic changes in lexical access, how the native language may change, how new learning is influenced by language experience, and how brain activity reflects these consequences. These findings expose the way that bilingualism reveals the relations between language and cognition in a manner that is impenetrable in speakers of one language alone.",
      "abstract": "Abstract The most provocative finding about bilingualism in the last two decades is that both languages are active even when bilinguals intend to use one language alone. When bilinguals hear, read, or speak words in one language, form, or translation, relatives of those words in the other language become momentarily available. The way bilingual speakers negotiate the competition produced by cross-language interactions has profound consequences for language processing and for the cognitive and neural mechanisms that it engages. In this chapter, we review the most exciting of these new discoveries. We consider how the context of language immersion induces dynamic changes in lexical access, how the native language may change, how new learning is influenced by language experience, and how brain activity reflects these consequences. These findings expose the way that bilingualism reveals the relations between language and cognition in a manner that is impenetrable in speakers of one language alone.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.26",
      "openalex_id": "https://openalex.org/W4213267748",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on bilingual lexical access and cross-language influence on processing, with no discussion of discrete audio tokenization or token-based representations; it does not meet the core inclusion criterion of analyzing discrete audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on bilingual lexical access and cross-language influence on processing, with no discussion of discrete audio tokenization or token-based representations; it does not meet the core inclusion criterion of analyzing discrete audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses bilingual lexical access and cognitive mechanisms but does not mention any study related to discrete audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio tokenization and seems unrelated to the targeted topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses bilingual lexical access and cognitive mechanisms but does not mention any study related to discrete audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio tokenization and seems unrelated to the targeted topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign language learning and assessment in German Switzerland: Exploring the potential of vocabulary size tests for Swiss German Sign Language",
    "abstract": "In German Switzerland the learning and assessment of Swiss German Sign Language (Deutschschweizerische GebÃ¤rdensprache, DSGS) takes place in different contexts, for example, in tertiary education or in continuous education courses. By way of the still ongoing implementation of the Common European Framework of Reference for DSGS, different tests and assessment procedures are currently been developed and their potential are explored to support the learning and assessment of DSGS. Examples of this are two vocabulary size tests. The first is a web-delivered Yes/No Test, the second a Translation Test from written German to DSGS. For both tests, the same set of items was used. The items were sampled from DSGS teaching materials. For the development of the two vocabulary size tests, 20 DSGS adult learners of ages 24 to 55 (M = 39.3) were recruited as test takers. An item analysis of the test results yielded candidates for removal from the item set. Cronbach’s Alpha showed good results for both tests (&gt;.90), and inter-rater reliability of the translation test also indicated promising results (Cohen’s Kappa = .613, p &lt;.001). Evidence contributing to content validity was collected based on the sampling method of the test items. Due to the lack of a second DSGS vocabulary test that could be used to establish concurrent validity, external variables were identified and investigated as possible external criteria contributing to the performance of the test takers. One variable, number of courses attended, showed a significant correlation with the test results.",
    "metadata": {
      "title": "Sign language learning and assessment in German Switzerland: Exploring the potential of vocabulary size tests for Swiss German Sign Language",
      "summary": "In German Switzerland the learning and assessment of Swiss German Sign Language (Deutschschweizerische GebÃ¤rdensprache, DSGS) takes place in different contexts, for example, in tertiary education or in continuous education courses. By way of the still ongoing implementation of the Common European Framework of Reference for DSGS, different tests and assessment procedures are currently been developed and their potential are explored to support the learning and assessment of DSGS. Examples of this are two vocabulary size tests. The first is a web-delivered Yes/No Test, the second a Translation Test from written German to DSGS. For both tests, the same set of items was used. The items were sampled from DSGS teaching materials. For the development of the two vocabulary size tests, 20 DSGS adult learners of ages 24 to 55 (M = 39.3) were recruited as test takers. An item analysis of the test results yielded candidates for removal from the item set. Cronbach’s Alpha showed good results for both tests (&gt;.90), and inter-rater reliability of the translation test also indicated promising results (Cohen’s Kappa = .613, p &lt;.001). Evidence contributing to content validity was collected based on the sampling method of the test items. Due to the lack of a second DSGS vocabulary test that could be used to establish concurrent validity, external variables were identified and investigated as possible external criteria contributing to the performance of the test takers. One variable, number of courses attended, showed a significant correlation with the test results.",
      "abstract": "In German Switzerland the learning and assessment of Swiss German Sign Language (Deutschschweizerische GebÃ¤rdensprache, DSGS) takes place in different contexts, for example, in tertiary education or in continuous education courses. By way of the still ongoing implementation of the Common European Framework of Reference for DSGS, different tests and assessment procedures are currently been developed and their potential are explored to support the learning and assessment of DSGS. Examples of this are two vocabulary size tests. The first is a web-delivered Yes/No Test, the second a Translation Test from written German to DSGS. For both tests, the same set of items was used. The items were sampled from DSGS teaching materials. For the development of the two vocabulary size tests, 20 DSGS adult learners of ages 24 to 55 (M = 39.3) were recruited as test takers. An item analysis of the test results yielded candidates for removal from the item set. Cronbach’s Alpha showed good results for both tests (&gt;.90), and inter-rater reliability of the translation test also indicated promising results (Cohen’s Kappa = .613, p &lt;.001). Evidence contributing to content validity was collected based on the sampling method of the test items. Due to the lack of a second DSGS vocabulary test that could be used to establish concurrent validity, external variables were identified and investigated as possible external criteria contributing to the performance of the test takers. One variable, number of courses attended, showed a significant correlation with the test results.",
      "doi": "https://doi.org/10.29140/lea.v2n1.85",
      "openalex_id": "https://openalex.org/W2943831203",
      "arxiv_id": "",
      "publication_date": "2019-04-30",
      "published": "2019-04-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study examines vocabulary tests for Swiss German Sign Language with no discrete audio/speech tokenization analyzed, so it does not meet the inclusion criteria demanding core discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study examines vocabulary tests for Swiss German Sign Language with no discrete audio/speech tokenization analyzed, so it does not meet the inclusion criteria demanding core discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vocabulary size tests for sign language learning and assessment, with no mention or analysis of discrete audio/speech tokenization or audio signal processing, which is the core inclusion criterion; it deals with sign language and linguistic assessment rather than audio tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vocabulary size tests for sign language learning and assessment, with no mention or analysis of discrete audio/speech tokenization or audio signal processing, which is the core inclusion criterion; it deals with sign language and linguistic assessment rather than audio tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Manual and Spoken Cues in French Sign Language’s Lexical Access: Evidence From Mouthing in a Sign-Picture Priming Paradigm",
    "abstract": "Although Sign Languages are gestural languages, the fact remains that some linguistic information can also be conveyed by spoken components as mouthing. Mouthing usually tend to reproduce the more relevant phonetic part of the equivalent spoken word matching with the manual sign. Therefore, one crucial issue in sign language is to understand whether mouthing is part of the signs themselves or not, and to which extent it contributes to the construction of signs meaning. Another question is to know whether mouthing patterns constitute a phonological or a semantic cue in the lexical sign entry. This study aimed to investigate the role of mouthing on the processing of lexical signs in French Sign Language (LSF), according the type of bilingualism (intramodal vs. bimodal). For this purpose, a behavioral sign-picture lexical decision experiment was designed. Intramodal signers (native deaf adults) and Bimodal signers (fluent hearing adults) have to decide as fast as possible whether a picture matched with the sign seen just before. Five experimental conditions in which the pair sign-mouthing were congruent or incongruent were created. Our results showed a strong interference effect when the sign-mouthing matching was incongruent, reflected by higher error rates and lengthened reaction times compared with the congruent condition. This finding suggests that both groups of signers use the available lexical information contained in mouthing during accessing the sign meaning. In addition, deaf intramodal signers were strongly interfered than hearing bimodal signers. Taken together, our data indicate that mouthing is a determining factor in LSF lexical access, specifically in deaf signers.",
    "metadata": {
      "title": "Manual and Spoken Cues in French Sign Language’s Lexical Access: Evidence From Mouthing in a Sign-Picture Priming Paradigm",
      "summary": "Although Sign Languages are gestural languages, the fact remains that some linguistic information can also be conveyed by spoken components as mouthing. Mouthing usually tend to reproduce the more relevant phonetic part of the equivalent spoken word matching with the manual sign. Therefore, one crucial issue in sign language is to understand whether mouthing is part of the signs themselves or not, and to which extent it contributes to the construction of signs meaning. Another question is to know whether mouthing patterns constitute a phonological or a semantic cue in the lexical sign entry. This study aimed to investigate the role of mouthing on the processing of lexical signs in French Sign Language (LSF), according the type of bilingualism (intramodal vs. bimodal). For this purpose, a behavioral sign-picture lexical decision experiment was designed. Intramodal signers (native deaf adults) and Bimodal signers (fluent hearing adults) have to decide as fast as possible whether a picture matched with the sign seen just before. Five experimental conditions in which the pair sign-mouthing were congruent or incongruent were created. Our results showed a strong interference effect when the sign-mouthing matching was incongruent, reflected by higher error rates and lengthened reaction times compared with the congruent condition. This finding suggests that both groups of signers use the available lexical information contained in mouthing during accessing the sign meaning. In addition, deaf intramodal signers were strongly interfered than hearing bimodal signers. Taken together, our data indicate that mouthing is a determining factor in LSF lexical access, specifically in deaf signers.",
      "abstract": "Although Sign Languages are gestural languages, the fact remains that some linguistic information can also be conveyed by spoken components as mouthing. Mouthing usually tend to reproduce the more relevant phonetic part of the equivalent spoken word matching with the manual sign. Therefore, one crucial issue in sign language is to understand whether mouthing is part of the signs themselves or not, and to which extent it contributes to the construction of signs meaning. Another question is to know whether mouthing patterns constitute a phonological or a semantic cue in the lexical sign entry. This study aimed to investigate the role of mouthing on the processing of lexical signs in French Sign Language (LSF), according the type of bilingualism (intramodal vs. bimodal). For this purpose, a behavioral sign-picture lexical decision experiment was designed. Intramodal signers (native deaf adults) and Bimodal signers (fluent hearing adults) have to decide as fast as possible whether a picture matched with the sign seen just before. Five experimental conditions in which the pair sign-mouthing were congruent or incongruent were created. Our results showed a strong interference effect when the sign-mouthing matching was incongruent, reflected by higher error rates and lengthened reaction times compared with the congruent condition. This finding suggests that both groups of signers use the available lexical information contained in mouthing during accessing the sign meaning. In addition, deaf intramodal signers were strongly interfered than hearing bimodal signers. Taken together, our data indicate that mouthing is a determining factor in LSF lexical access, specifically in deaf signers.",
      "doi": "https://doi.org/10.3389/fpsyg.2021.655168",
      "openalex_id": "https://openalex.org/W3164299976",
      "arxiv_id": "",
      "publication_date": "2021-05-25",
      "published": "2021-05-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract examines mouthing as a cue in sign language processing, not discrete audio tokenization into a finite vocabulary; there is no explicit or implied analysis of tokenization for speech/audio as discrete units.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract examines mouthing as a cue in sign language processing, not discrete audio tokenization into a finite vocabulary; there is no explicit or implied analysis of tokenization for speech/audio as discrete units.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the role of mouthing (a spoken component) in French Sign Language lexical access using behavioral experiments, focusing on cognitive processing rather than proposing, analyzing, or evaluating discrete audio or speech tokenization methods or representations from continuous audio signals to discrete symbols. Additionally, there is no indication that the full text focuses on or proposes any discrete quantization/tokenization technique relevant to audio reconstruction, generation, or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it fall into a relevant exclusion category beyond that.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the role of mouthing (a spoken component) in French Sign Language lexical access using behavioral experiments, focusing on cognitive processing rather than proposing, analyzing, or evaluating discrete audio or speech tokenization methods or representations from continuous audio signals to discrete symbols. Additionally, there is no indication that the full text focuses on or proposes any discrete quantization/tokenization technique relevant to audio reconstruction, generation, or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it fall into a relevant exclusion category beyond that.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical Recognition in Deaf Children Learning American Sign Language: Activation of Semantic and Phonological Features of Signs",
    "abstract": "Abstract Children learning language efficiently process single words and activate semantic, phonological, and other features of words during recognition. We investigated lexical recognition in deaf children acquiring American Sign Language (ASL) to determine how perceiving language in the visual–spatial modality affects lexical recognition. Twenty native or early‐exposed signing deaf children (ages 4 to 8 years) participated in a visual world eye‐tracking study. Participants were presented with a single ASL sign, target picture, and three competitor pictures that varied in their phonological and semantic relationship to the target. Participants shifted gaze to the target picture shortly after sign offset. Participants showed robust evidence for activation of semantic but not phonological features of signs. However, in their behavioral responses, participants were most susceptible to phonological competitors. Results demonstrated that single word recognition in ASL is largely parallel to spoken language recognition among children who are developing a mature lexicon.",
    "metadata": {
      "title": "Lexical Recognition in Deaf Children Learning American Sign Language: Activation of Semantic and Phonological Features of Signs",
      "summary": "Abstract Children learning language efficiently process single words and activate semantic, phonological, and other features of words during recognition. We investigated lexical recognition in deaf children acquiring American Sign Language (ASL) to determine how perceiving language in the visual–spatial modality affects lexical recognition. Twenty native or early‐exposed signing deaf children (ages 4 to 8 years) participated in a visual world eye‐tracking study. Participants were presented with a single ASL sign, target picture, and three competitor pictures that varied in their phonological and semantic relationship to the target. Participants shifted gaze to the target picture shortly after sign offset. Participants showed robust evidence for activation of semantic but not phonological features of signs. However, in their behavioral responses, participants were most susceptible to phonological competitors. Results demonstrated that single word recognition in ASL is largely parallel to spoken language recognition among children who are developing a mature lexicon.",
      "abstract": "Abstract Children learning language efficiently process single words and activate semantic, phonological, and other features of words during recognition. We investigated lexical recognition in deaf children acquiring American Sign Language (ASL) to determine how perceiving language in the visual–spatial modality affects lexical recognition. Twenty native or early‐exposed signing deaf children (ages 4 to 8 years) participated in a visual world eye‐tracking study. Participants were presented with a single ASL sign, target picture, and three competitor pictures that varied in their phonological and semantic relationship to the target. Participants shifted gaze to the target picture shortly after sign offset. Participants showed robust evidence for activation of semantic but not phonological features of signs. However, in their behavioral responses, participants were most susceptible to phonological competitors. Results demonstrated that single word recognition in ASL is largely parallel to spoken language recognition among children who are developing a mature lexicon.",
      "doi": "https://doi.org/10.1111/lang.12409",
      "openalex_id": "https://openalex.org/W3033933301",
      "arxiv_id": "",
      "publication_date": "2020-06-03",
      "published": "2020-06-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study examines lexical recognition in American Sign Language (a visual–spatial language) and semantic/phonological activation in signs using eye-tracking; there is no analysis or proposal of discrete audio tokenization or audio-to-symbol tokenization, so it does not satisfy the discrete audio-tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study examines lexical recognition in American Sign Language (a visual–spatial language) and semantic/phonological activation in signs using eye-tracking; there is no analysis or proposal of discrete audio tokenization or audio-to-symbol tokenization, so it does not satisfy the discrete audio-tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical recognition in deaf children learning American Sign Language, specifically examining visual and linguistic features of signs, without any mention or analysis of discrete audio/speech tokenization or audio signal processing; therefore, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical recognition in deaf children learning American Sign Language, specifically examining visual and linguistic features of signs, without any mention or analysis of discrete audio/speech tokenization or audio signal processing; therefore, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Morphology and The Mental Lexicon",
    "abstract": "Abstract This chapter looks at the most basic question for the study of morphology and the mental lexicon: whether or not words are decomposed into constituent parts. Theories of the mental lexicon range from those that generalize decomposition as much as possible to those that posit no internal structure for words; various intermediate positions are also found. The key question at the center of this overview is what it means for a word to be decomposed in the first place; as it turns out, this is a complex notion, and there are at least three independent but related questions that have been connected with it in the literature. The goal of this chapter is to distinguish these different notions of decomposition from each other, in order to both provide points of reference for understanding prior work, and to pose specific questions to be addressed in future research.",
    "metadata": {
      "title": "Morphology and The Mental Lexicon",
      "summary": "Abstract This chapter looks at the most basic question for the study of morphology and the mental lexicon: whether or not words are decomposed into constituent parts. Theories of the mental lexicon range from those that generalize decomposition as much as possible to those that posit no internal structure for words; various intermediate positions are also found. The key question at the center of this overview is what it means for a word to be decomposed in the first place; as it turns out, this is a complex notion, and there are at least three independent but related questions that have been connected with it in the literature. The goal of this chapter is to distinguish these different notions of decomposition from each other, in order to both provide points of reference for understanding prior work, and to pose specific questions to be addressed in future research.",
      "abstract": "Abstract This chapter looks at the most basic question for the study of morphology and the mental lexicon: whether or not words are decomposed into constituent parts. Theories of the mental lexicon range from those that generalize decomposition as much as possible to those that posit no internal structure for words; various intermediate positions are also found. The key question at the center of this overview is what it means for a word to be decomposed in the first place; as it turns out, this is a complex notion, and there are at least three independent but related questions that have been connected with it in the literature. The goal of this chapter is to distinguish these different notions of decomposition from each other, in order to both provide points of reference for understanding prior work, and to pose specific questions to be addressed in future research.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.17",
      "openalex_id": "https://openalex.org/W4212952130",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses morphology and the mental lexicon with no discrete audio tokenization analysis, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses morphology and the mental lexicon with no discrete audio tokenization analysis, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on morphological decomposition in the mental lexicon related to words and language, and does not address discrete audio/speech tokenization or any discrete token quantization from continuous audio signals, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on morphological decomposition in the mental lexicon related to words and language, and does not address discrete audio/speech tokenization or any discrete token quantization from continuous audio signals, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The neural basis of word production",
    "abstract": "Abstract Traditionally, psycholinguistic models and models of speech motor control have approached the word production process from different angles and have remained, to a large extent, separate from one another. Ultimately, however, the process entails the entire pathway from semantic processing to articulation: producing a word from meaning requires activating a concept, retrieving the word, selecting its segments, ordering those segments into the right sequence, and finally articulating them with the correct timing. This chapter reviews the neural basis of word production from the complementary perspectives of the psycholinguistic literature and the speech motor control literature.",
    "metadata": {
      "title": "The neural basis of word production",
      "summary": "Abstract Traditionally, psycholinguistic models and models of speech motor control have approached the word production process from different angles and have remained, to a large extent, separate from one another. Ultimately, however, the process entails the entire pathway from semantic processing to articulation: producing a word from meaning requires activating a concept, retrieving the word, selecting its segments, ordering those segments into the right sequence, and finally articulating them with the correct timing. This chapter reviews the neural basis of word production from the complementary perspectives of the psycholinguistic literature and the speech motor control literature.",
      "abstract": "Abstract Traditionally, psycholinguistic models and models of speech motor control have approached the word production process from different angles and have remained, to a large extent, separate from one another. Ultimately, however, the process entails the entire pathway from semantic processing to articulation: producing a word from meaning requires activating a concept, retrieving the word, selecting its segments, ordering those segments into the right sequence, and finally articulating them with the correct timing. This chapter reviews the neural basis of word production from the complementary perspectives of the psycholinguistic literature and the speech motor control literature.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.12",
      "openalex_id": "https://openalex.org/W4213326696",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns neural mechanisms of word production and language processing, with no discussion of discrete audio/speech tokenization or token-level representations, so it does not meet the discrete tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns neural mechanisms of word production and language processing, with no discussion of discrete audio/speech tokenization or token-level representations, so it does not meet the discrete tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a review of the neural basis of word production focusing on psycholinguistic and speech motor control perspectives, without any mention of discrete audio/speech tokenization or related computational methods, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a review of the neural basis of word production focusing on psycholinguistic and speech motor control perspectives, without any mention of discrete audio/speech tokenization or related computational methods, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Operationalization of Sign Language Phonological Similarity and its Effects on Lexical Access",
    "abstract": "Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features-similar to the majority of extant definitions in the literature.",
    "metadata": {
      "title": "Operationalization of Sign Language Phonological Similarity and its Effects on Lexical Access",
      "summary": "Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features-similar to the majority of extant definitions in the literature.",
      "abstract": "Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features-similar to the majority of extant definitions in the literature.",
      "doi": "https://doi.org/10.1093/deafed/enx014",
      "openalex_id": "https://openalex.org/W2617407762",
      "arxiv_id": "",
      "publication_date": "2017-04-12",
      "published": "2017-04-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study examines phonological similarity and lexical access in sign language and focuses on sublexical sign features, not on discrete audio tokenization of speech into tokens; since inclusion criteria require explicit core focus on discrete audio/tokenization of speech, this item should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study examines phonological similarity and lexical access in sign language and focuses on sublexical sign features, not on discrete audio tokenization of speech into tokens; since inclusion criteria require explicit core focus on discrete audio/tokenization of speech, this item should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological similarity in sign language lexical access and does not address discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete token sequences relevant to audio reconstruction or language-model-style token sequences, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological similarity in sign language lexical access and does not address discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete token sequences relevant to audio reconstruction or language-model-style token sequences, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AfricaSign -- A Crowd-sourcing Platform for the Documentation of STEM Vocabulary in African Sign Languages",
    "abstract": "Research in sign languages, in general, is still a relatively new topic of study when compared to research into spoken languages. Most of African sign languages are endangered and severely under-studied [11]. In an attempt to (lexically) document as many endangered sign languages in Africa as possible, we have developed a low-barrier, online crowd-sourcing platform (AfricaSign) that enables the African deaf communities to document their sign languages. AfricaSign offers to users multiple input modes, accommodates regional variation in multiple sign languages and allows the use of avatar technology to describe signs. It is likely that this research will uncover typological features exhibited by African sign languages. Documentation of STEM vocabulary will also help facilitate access to education for the Deaf community.",
    "metadata": {
      "title": "AfricaSign -- A Crowd-sourcing Platform for the Documentation of STEM Vocabulary in African Sign Languages",
      "summary": "Research in sign languages, in general, is still a relatively new topic of study when compared to research into spoken languages. Most of African sign languages are endangered and severely under-studied [11]. In an attempt to (lexically) document as many endangered sign languages in Africa as possible, we have developed a low-barrier, online crowd-sourcing platform (AfricaSign) that enables the African deaf communities to document their sign languages. AfricaSign offers to users multiple input modes, accommodates regional variation in multiple sign languages and allows the use of avatar technology to describe signs. It is likely that this research will uncover typological features exhibited by African sign languages. Documentation of STEM vocabulary will also help facilitate access to education for the Deaf community.",
      "abstract": "Research in sign languages, in general, is still a relatively new topic of study when compared to research into spoken languages. Most of African sign languages are endangered and severely under-studied [11]. In an attempt to (lexically) document as many endangered sign languages in Africa as possible, we have developed a low-barrier, online crowd-sourcing platform (AfricaSign) that enables the African deaf communities to document their sign languages. AfricaSign offers to users multiple input modes, accommodates regional variation in multiple sign languages and allows the use of avatar technology to describe signs. It is likely that this research will uncover typological features exhibited by African sign languages. Documentation of STEM vocabulary will also help facilitate access to education for the Deaf community.",
      "doi": "https://doi.org/10.1145/3308561.3354592",
      "openalex_id": "https://openalex.org/W2981337119",
      "arxiv_id": "",
      "publication_date": "2019-10-24",
      "published": "2019-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes a platform for documenting sign languages and STEM vocabulary in African Sign Languages; there is no analysis or proposal of discrete audio tokenization or token-level speech representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes a platform for documenting sign languages and STEM vocabulary in African Sign Languages; there is no analysis or proposal of discrete audio tokenization or token-level speech representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on documenting African sign languages through a crowd-sourcing platform and mentions the documentation of STEM vocabulary for the Deaf community; however, it does not propose, analyze, or evaluate discrete audio/speech tokenization, nor does it address discrete token representations, audio reconstruction, or related speech processing tasks, making it irrelevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on documenting African sign languages through a crowd-sourcing platform and mentions the documentation of STEM vocabulary for the Deaf community; however, it does not propose, analyze, or evaluate discrete audio/speech tokenization, nor does it address discrete token representations, audio reconstruction, or related speech processing tasks, making it irrelevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonology Recognition in American Sign Language",
    "abstract": "Inspired by recent developments in natural language processing, we propose a novel approach to sign language processing based on phonological properties validated by American Sign Language users. By taking advantage of datasets composed of phonological data and people speaking sign language, we use a pretrained deep model based on mesh reconstruction to extract the 3D coordinates of the signers keypoints. Then, we train standard statistical and deep machine learning models in order to assign phonological classes to each temporal sequence of coordinates. Our paper introduces the idea of exploiting the phonological properties manually assigned by sign language users to classify videos of people performing signs by regressing a 3D mesh. We establish a new baseline for this problem based on the statistical distribution of 725 different signs. Our best-performing models achieve a micro-averaged F1-score of 58% for the major location class and 70% for the sign type using statistical and deep learning algorithms, compared to their corresponding baselines of 35% and 39%.",
    "metadata": {
      "title": "Phonology Recognition in American Sign Language",
      "summary": "Inspired by recent developments in natural language processing, we propose a novel approach to sign language processing based on phonological properties validated by American Sign Language users. By taking advantage of datasets composed of phonological data and people speaking sign language, we use a pretrained deep model based on mesh reconstruction to extract the 3D coordinates of the signers keypoints. Then, we train standard statistical and deep machine learning models in order to assign phonological classes to each temporal sequence of coordinates. Our paper introduces the idea of exploiting the phonological properties manually assigned by sign language users to classify videos of people performing signs by regressing a 3D mesh. We establish a new baseline for this problem based on the statistical distribution of 725 different signs. Our best-performing models achieve a micro-averaged F1-score of 58% for the major location class and 70% for the sign type using statistical and deep learning algorithms, compared to their corresponding baselines of 35% and 39%.",
      "abstract": "Inspired by recent developments in natural language processing, we propose a novel approach to sign language processing based on phonological properties validated by American Sign Language users. By taking advantage of datasets composed of phonological data and people speaking sign language, we use a pretrained deep model based on mesh reconstruction to extract the 3D coordinates of the signers keypoints. Then, we train standard statistical and deep machine learning models in order to assign phonological classes to each temporal sequence of coordinates. Our paper introduces the idea of exploiting the phonological properties manually assigned by sign language users to classify videos of people performing signs by regressing a 3D mesh. We establish a new baseline for this problem based on the statistical distribution of 725 different signs. Our best-performing models achieve a micro-averaged F1-score of 58% for the major location class and 70% for the sign type using statistical and deep learning algorithms, compared to their corresponding baselines of 35% and 39%.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747212",
      "openalex_id": "https://openalex.org/W3204590994",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates sign language phonology recognition from 3D pose data and does not involve discrete audio tokenization or a token-based representation of speech; inclusion criteria require discrete audio tokens for speech tasks, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates sign language phonology recognition from 3D pose data and does not involve discrete audio tokenization or a token-based representation of speech; inclusion criteria require discrete audio tokens for speech tasks, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonology recognition in American Sign Language using 3D mesh reconstruction and machine learning, which does not involve discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols; thus, it does not meet the inclusion criteria related to audio tokenization and involves sign language (visual modality) rather than audio signal processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonology recognition in American Sign Language using 3D mesh reconstruction and machine learning, which does not involve discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols; thus, it does not meet the inclusion criteria related to audio tokenization and involves sign language (visual modality) rather than audio signal processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Visual form of ASL verb signs predicts non-signer judgment of transitivity",
    "abstract": "Longstanding cross-linguistic work on event representations in spoken languages have argued for a robust mapping between an event’s underlying representation and its syntactic encoding, such that–for example–the agent of an event is most frequently mapped to subject position. In the same vein, sign languages have long been claimed to construct signs that visually represent their meaning, i.e., signs that are iconic. Experimental research on linguistic parameters such as plurality and aspect has recently shown some of them to be visually universal in sign, i.e. recognized by non-signers as well as signers, and have identified specific visual cues that achieve this mapping. However, little is known about what makes action representations in sign language iconic, or whether and how the mapping of underlying event representations to syntactic encoding is visually apparent in the form of a verb sign. To this end, we asked what visual cues non-signers may use in evaluating transitivity (i.e., the number of entities involved in an action). To do this, we correlated non-signer judgments about transitivity of verb signs from American Sign Language (ASL) with phonological characteristics of these signs. We found that non-signers did not accurately guess the transitivity of the signs, but that non-signer transitivity judgments can nevertheless be predicted from the signs’ visual characteristics. Further, non-signers cue in on just those features that code event representations across sign languages, despite interpreting them differently. This suggests the existence of visual biases that underlie detection of linguistic categories, such as transitivity, which may uncouple from underlying conceptual representations over time in mature sign languages due to lexicalization processes.",
    "metadata": {
      "title": "Visual form of ASL verb signs predicts non-signer judgment of transitivity",
      "summary": "Longstanding cross-linguistic work on event representations in spoken languages have argued for a robust mapping between an event’s underlying representation and its syntactic encoding, such that–for example–the agent of an event is most frequently mapped to subject position. In the same vein, sign languages have long been claimed to construct signs that visually represent their meaning, i.e., signs that are iconic. Experimental research on linguistic parameters such as plurality and aspect has recently shown some of them to be visually universal in sign, i.e. recognized by non-signers as well as signers, and have identified specific visual cues that achieve this mapping. However, little is known about what makes action representations in sign language iconic, or whether and how the mapping of underlying event representations to syntactic encoding is visually apparent in the form of a verb sign. To this end, we asked what visual cues non-signers may use in evaluating transitivity (i.e., the number of entities involved in an action). To do this, we correlated non-signer judgments about transitivity of verb signs from American Sign Language (ASL) with phonological characteristics of these signs. We found that non-signers did not accurately guess the transitivity of the signs, but that non-signer transitivity judgments can nevertheless be predicted from the signs’ visual characteristics. Further, non-signers cue in on just those features that code event representations across sign languages, despite interpreting them differently. This suggests the existence of visual biases that underlie detection of linguistic categories, such as transitivity, which may uncouple from underlying conceptual representations over time in mature sign languages due to lexicalization processes.",
      "abstract": "Longstanding cross-linguistic work on event representations in spoken languages have argued for a robust mapping between an event’s underlying representation and its syntactic encoding, such that–for example–the agent of an event is most frequently mapped to subject position. In the same vein, sign languages have long been claimed to construct signs that visually represent their meaning, i.e., signs that are iconic. Experimental research on linguistic parameters such as plurality and aspect has recently shown some of them to be visually universal in sign, i.e. recognized by non-signers as well as signers, and have identified specific visual cues that achieve this mapping. However, little is known about what makes action representations in sign language iconic, or whether and how the mapping of underlying event representations to syntactic encoding is visually apparent in the form of a verb sign. To this end, we asked what visual cues non-signers may use in evaluating transitivity (i.e., the number of entities involved in an action). To do this, we correlated non-signer judgments about transitivity of verb signs from American Sign Language (ASL) with phonological characteristics of these signs. We found that non-signers did not accurately guess the transitivity of the signs, but that non-signer transitivity judgments can nevertheless be predicted from the signs’ visual characteristics. Further, non-signers cue in on just those features that code event representations across sign languages, despite interpreting them differently. This suggests the existence of visual biases that underlie detection of linguistic categories, such as transitivity, which may uncouple from underlying conceptual representations over time in mature sign languages due to lexicalization processes.",
      "doi": "https://doi.org/10.1371/journal.pone.0262098",
      "openalex_id": "https://openalex.org/W4214745554",
      "arxiv_id": "",
      "publication_date": "2022-02-25",
      "published": "2022-02-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit analysis of discrete audio/speech tokenization (converting continuous audio into discrete tokens for language tasks). This item investigates visual iconicity in ASL signs and non-signer judgments of transitivity, with no focus on discrete audio tokenization or speech token sequences, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit analysis of discrete audio/speech tokenization (converting continuous audio into discrete tokens for language tasks). This item investigates visual iconicity in ASL signs and non-signer judgments of transitivity, with no focus on discrete audio tokenization or speech token sequences, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the visual form of ASL verb signs and non-signer judgments of transitivity, with no mention or analysis of audio/speech tokenization or discrete audio representations; thus, it does not meet any inclusion criteria related to discrete audio tokenization and falls outside the scope of audio signal processing or tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the visual form of ASL verb signs and non-signer judgments of transitivity, with no mention or analysis of audio/speech tokenization or discrete audio representations; thus, it does not meet any inclusion criteria related to discrete audio tokenization and falls outside the scope of audio signal processing or tokenization tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cleaning up the Brickyard: How Theory and Methodology Shape Experiments in Cognitive Neuroscience of Language",
    "abstract": "Abstract The capacity for language is a defining property of our species, yet despite decades of research, evidence on its neural basis is still mixed and a generalized consensus is difficult to achieve. We suggest that this is partly caused by researchers defining “language” in different ways, with focus on a wide range of phenomena, properties, and levels of investigation. Accordingly, there is very little agreement among cognitive neuroscientists of language on the operationalization of fundamental concepts to be investigated in neuroscientific experiments. Here, we review chains of derivation in the cognitive neuroscience of language, focusing on how the hypothesis under consideration is defined by a combination of theoretical and methodological assumptions. We first attempt to disentangle the complex relationship between linguistics, psychology, and neuroscience in the field. Next, we focus on how conclusions that can be drawn from any experiment are inherently constrained by auxiliary assumptions, both theoretical and methodological, on which the validity of conclusions drawn rests. These issues are discussed in the context of classical experimental manipulations as well as study designs that employ novel approaches such as naturalistic stimuli and computational modeling. We conclude by proposing that a highly interdisciplinary field such as the cognitive neuroscience of language requires researchers to form explicit statements concerning the theoretical definitions, methodological choices, and other constraining factors involved in their work.",
    "metadata": {
      "title": "Cleaning up the Brickyard: How Theory and Methodology Shape Experiments in Cognitive Neuroscience of Language",
      "summary": "Abstract The capacity for language is a defining property of our species, yet despite decades of research, evidence on its neural basis is still mixed and a generalized consensus is difficult to achieve. We suggest that this is partly caused by researchers defining “language” in different ways, with focus on a wide range of phenomena, properties, and levels of investigation. Accordingly, there is very little agreement among cognitive neuroscientists of language on the operationalization of fundamental concepts to be investigated in neuroscientific experiments. Here, we review chains of derivation in the cognitive neuroscience of language, focusing on how the hypothesis under consideration is defined by a combination of theoretical and methodological assumptions. We first attempt to disentangle the complex relationship between linguistics, psychology, and neuroscience in the field. Next, we focus on how conclusions that can be drawn from any experiment are inherently constrained by auxiliary assumptions, both theoretical and methodological, on which the validity of conclusions drawn rests. These issues are discussed in the context of classical experimental manipulations as well as study designs that employ novel approaches such as naturalistic stimuli and computational modeling. We conclude by proposing that a highly interdisciplinary field such as the cognitive neuroscience of language requires researchers to form explicit statements concerning the theoretical definitions, methodological choices, and other constraining factors involved in their work.",
      "abstract": "Abstract The capacity for language is a defining property of our species, yet despite decades of research, evidence on its neural basis is still mixed and a generalized consensus is difficult to achieve. We suggest that this is partly caused by researchers defining “language” in different ways, with focus on a wide range of phenomena, properties, and levels of investigation. Accordingly, there is very little agreement among cognitive neuroscientists of language on the operationalization of fundamental concepts to be investigated in neuroscientific experiments. Here, we review chains of derivation in the cognitive neuroscience of language, focusing on how the hypothesis under consideration is defined by a combination of theoretical and methodological assumptions. We first attempt to disentangle the complex relationship between linguistics, psychology, and neuroscience in the field. Next, we focus on how conclusions that can be drawn from any experiment are inherently constrained by auxiliary assumptions, both theoretical and methodological, on which the validity of conclusions drawn rests. These issues are discussed in the context of classical experimental manipulations as well as study designs that employ novel approaches such as naturalistic stimuli and computational modeling. We conclude by proposing that a highly interdisciplinary field such as the cognitive neuroscience of language requires researchers to form explicit statements concerning the theoretical definitions, methodological choices, and other constraining factors involved in their work.",
      "doi": "https://doi.org/10.1162/jocn_a_02058",
      "openalex_id": "https://openalex.org/W4386763304",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses theory and methodology in cognitive neuroscience of language but does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses theory and methodology in cognitive neuroscience of language but does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate a focus on theoretical and methodological issues in cognitive neuroscience of language, without mentioning any discrete audio/speech tokenization or neural encoding-based discrete token representations; the study appears to be a conceptual review rather than an experimental or methodological contribution addressing tokenization of continuous audio into discrete units relevant to language-model-style sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate a focus on theoretical and methodological issues in cognitive neuroscience of language, without mentioning any discrete audio/speech tokenization or neural encoding-based discrete token representations; the study appears to be a conceptual review rather than an experimental or methodological contribution addressing tokenization of continuous audio into discrete units relevant to language-model-style sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Role of Modality in L2 Learning: The Importance of Learners Acquiring a Second Sign Language (M2L2 and M1L2 Learners)",
    "abstract": "Abstract Second language acquisition (SLA) research offers valuable insight on how languages are learned and how they coexist and influence each other. Sign language learners offer unique perspectives on SLA, allowing researchers to test theories that are otherwise constrained by access to only one modality. Current literature on sign language learning focuses primarily on bimodal bilinguals, mostly hearing adults learning their first sign language (M2L2 learners). However, other groups of L2 signers exist, including deaf learners who have previously acquired a sign language and are learning a new one (M1L2 learners). M1L2 acquisition offers unique insights into complex interactions including multilingualism, modality, and timing of acquisition. We argue that M1L2 signers are a key comparison group for investigations of various L2 and so‐called modality effects and also represent a crucial test case for re‐examining the traditional constructs of “native speaker/signer” and the effects of initial language delay or deprivation on subsequent language acquisition.",
    "metadata": {
      "title": "The Role of Modality in L2 Learning: The Importance of Learners Acquiring a Second Sign Language (M2L2 and M1L2 Learners)",
      "summary": "Abstract Second language acquisition (SLA) research offers valuable insight on how languages are learned and how they coexist and influence each other. Sign language learners offer unique perspectives on SLA, allowing researchers to test theories that are otherwise constrained by access to only one modality. Current literature on sign language learning focuses primarily on bimodal bilinguals, mostly hearing adults learning their first sign language (M2L2 learners). However, other groups of L2 signers exist, including deaf learners who have previously acquired a sign language and are learning a new one (M1L2 learners). M1L2 acquisition offers unique insights into complex interactions including multilingualism, modality, and timing of acquisition. We argue that M1L2 signers are a key comparison group for investigations of various L2 and so‐called modality effects and also represent a crucial test case for re‐examining the traditional constructs of “native speaker/signer” and the effects of initial language delay or deprivation on subsequent language acquisition.",
      "abstract": "Abstract Second language acquisition (SLA) research offers valuable insight on how languages are learned and how they coexist and influence each other. Sign language learners offer unique perspectives on SLA, allowing researchers to test theories that are otherwise constrained by access to only one modality. Current literature on sign language learning focuses primarily on bimodal bilinguals, mostly hearing adults learning their first sign language (M2L2 learners). However, other groups of L2 signers exist, including deaf learners who have previously acquired a sign language and are learning a new one (M1L2 learners). M1L2 acquisition offers unique insights into complex interactions including multilingualism, modality, and timing of acquisition. We argue that M1L2 signers are a key comparison group for investigations of various L2 and so‐called modality effects and also represent a crucial test case for re‐examining the traditional constructs of “native speaker/signer” and the effects of initial language delay or deprivation on subsequent language acquisition.",
      "doi": "https://doi.org/10.1111/lang.12607",
      "openalex_id": "https://openalex.org/W4387575129",
      "arxiv_id": "",
      "publication_date": "2023-10-10",
      "published": "2023-10-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on sign language learning and language modality, not on discrete audio/speech tokenization, so it does not meet the inclusion criterion requiring analysis of discrete tokens for audio reconstruction or language tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on sign language learning and language modality, not on discrete audio/speech tokenization, so it does not meet the inclusion criterion requiring analysis of discrete tokens for audio reconstruction or language tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on second language acquisition of sign languages and linguistic modality effects, without discussing or proposing methods for discrete audio/speech tokenization or analyzing discrete token representations of audio signals, which does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on second language acquisition of sign languages and linguistic modality effects, without discussing or proposing methods for discrete audio/speech tokenization or analyzing discrete token representations of audio signals, which does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Do parents modify child-directed signing to emphasize iconicity?",
    "abstract": "Iconic signs are overrepresented in the vocabularies of young deaf children, but it is unclear why. It is possible that iconic signs are easier for children to learn, but it is also possible that adults use iconic signs in child-directed signing in ways that make them more learnable, either by using them more often than less iconic signs or by lengthening them. We analyzed videos of naturalistic play sessions between parents and deaf children ( n = 24 dyads) aged 9–60 months. To determine whether iconic signs are overrepresented during child-directed signing, we compared the iconicity of actual parent productions to the iconicity of simulated vocabularies designed to estimate chance levels of iconicity. For almost all dyads, parent sign types and tokens were not more iconic than the simulated vocabularies, suggesting that parents do not select more iconic signs during child-directed signing. To determine whether iconic signs are more likely to be lengthened, we ran a linear regression predicting sign duration, and found an interaction between age and iconicity: while parents of younger children produced non-iconic and iconic signs with similar durations, parents of older children produced non-iconic signs with shorter durations than iconic signs. Thus, parents sign more quickly with older children than younger children, and iconic signs appear to resist that reduction in sign length. It is possible that iconic signs are perceptually available longer, and their availability is a candidate hypothesis as to why iconic signs are overrepresented in children’s vocabularies.",
    "metadata": {
      "title": "Do parents modify child-directed signing to emphasize iconicity?",
      "summary": "Iconic signs are overrepresented in the vocabularies of young deaf children, but it is unclear why. It is possible that iconic signs are easier for children to learn, but it is also possible that adults use iconic signs in child-directed signing in ways that make them more learnable, either by using them more often than less iconic signs or by lengthening them. We analyzed videos of naturalistic play sessions between parents and deaf children ( n = 24 dyads) aged 9–60 months. To determine whether iconic signs are overrepresented during child-directed signing, we compared the iconicity of actual parent productions to the iconicity of simulated vocabularies designed to estimate chance levels of iconicity. For almost all dyads, parent sign types and tokens were not more iconic than the simulated vocabularies, suggesting that parents do not select more iconic signs during child-directed signing. To determine whether iconic signs are more likely to be lengthened, we ran a linear regression predicting sign duration, and found an interaction between age and iconicity: while parents of younger children produced non-iconic and iconic signs with similar durations, parents of older children produced non-iconic signs with shorter durations than iconic signs. Thus, parents sign more quickly with older children than younger children, and iconic signs appear to resist that reduction in sign length. It is possible that iconic signs are perceptually available longer, and their availability is a candidate hypothesis as to why iconic signs are overrepresented in children’s vocabularies.",
      "abstract": "Iconic signs are overrepresented in the vocabularies of young deaf children, but it is unclear why. It is possible that iconic signs are easier for children to learn, but it is also possible that adults use iconic signs in child-directed signing in ways that make them more learnable, either by using them more often than less iconic signs or by lengthening them. We analyzed videos of naturalistic play sessions between parents and deaf children ( n = 24 dyads) aged 9–60 months. To determine whether iconic signs are overrepresented during child-directed signing, we compared the iconicity of actual parent productions to the iconicity of simulated vocabularies designed to estimate chance levels of iconicity. For almost all dyads, parent sign types and tokens were not more iconic than the simulated vocabularies, suggesting that parents do not select more iconic signs during child-directed signing. To determine whether iconic signs are more likely to be lengthened, we ran a linear regression predicting sign duration, and found an interaction between age and iconicity: while parents of younger children produced non-iconic and iconic signs with similar durations, parents of older children produced non-iconic signs with shorter durations than iconic signs. Thus, parents sign more quickly with older children than younger children, and iconic signs appear to resist that reduction in sign length. It is possible that iconic signs are perceptually available longer, and their availability is a candidate hypothesis as to why iconic signs are overrepresented in children’s vocabularies.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.920729",
      "openalex_id": "https://openalex.org/W4293104800",
      "arxiv_id": "",
      "publication_date": "2022-08-25",
      "published": "2022-08-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns parental use of signs and iconicity in child-directed signing, not discrete audio tokenization or any analysis of discrete audio tokens for speech processing, so it does not meet the criterion of analyzing/tokenizing speech into a discrete vocabulary.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns parental use of signs and iconicity in child-directed signing, not discrete audio tokenization or any analysis of discrete audio tokens for speech processing, so it does not meet the criterion of analyzing/tokenizing speech into a discrete vocabulary.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on parent-child signing behavior and the iconicity of signs in child-directed signing, without addressing discrete audio or speech tokenization; it does not propose, analyze, or evaluate discrete audio tokenization or neural quantization methods required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on parent-child signing behavior and the iconicity of signs in child-directed signing, without addressing discrete audio or speech tokenization; it does not propose, analyze, or evaluate discrete audio tokenization or neural quantization methods required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Identifying the Correlations Between the Semantics and the Phonology of American Sign Language and British Sign Language: A Vector Space Approach",
    "abstract": "Over the history of research on sign languages, much scholarship has highlighted the pervasive presence of signs whose forms relate to their meaning in a non-arbitrary way. The presence of these forms suggests that sign language vocabularies are shaped, at least in part, by a pressure toward maintaining a link between form and meaning in wordforms. We use a vector space approach to test the ways this pressure might shape sign language vocabularies, examining how non-arbitrary forms are distributed within the lexicons of two unrelated sign languages. Vector space models situate the representations of words in a multi-dimensional space where the distance between words indexes their relatedness in meaning. Using phonological information from the vocabularies of American Sign Language (ASL) and British Sign Language (BSL), we tested whether increased similarity between the semantic representations of signs corresponds to increased phonological similarity. The results of the computational analysis showed a significant positive relationship between phonological form and semantic meaning for both sign languages, which was strongest when the sign language lexicons were organized into clusters of semantically related signs. The analysis also revealed variation in the strength of patterns across the form-meaning relationships seen between phonological parameters within each sign language, as well as between the two languages. This shows that while the connection between form and meaning is not entirely language specific, there are cross-linguistic differences in how these mappings are realized for signs in each language, suggesting that arbitrariness as well as cognitive or cultural influences may play a role in how these patterns are realized. The results of this analysis not only contribute to our understanding of the distribution of non-arbitrariness in sign language lexicons, but also demonstrate a new way that computational modeling can be harnessed in lexicon-wide investigations of sign languages.",
    "metadata": {
      "title": "Identifying the Correlations Between the Semantics and the Phonology of American Sign Language and British Sign Language: A Vector Space Approach",
      "summary": "Over the history of research on sign languages, much scholarship has highlighted the pervasive presence of signs whose forms relate to their meaning in a non-arbitrary way. The presence of these forms suggests that sign language vocabularies are shaped, at least in part, by a pressure toward maintaining a link between form and meaning in wordforms. We use a vector space approach to test the ways this pressure might shape sign language vocabularies, examining how non-arbitrary forms are distributed within the lexicons of two unrelated sign languages. Vector space models situate the representations of words in a multi-dimensional space where the distance between words indexes their relatedness in meaning. Using phonological information from the vocabularies of American Sign Language (ASL) and British Sign Language (BSL), we tested whether increased similarity between the semantic representations of signs corresponds to increased phonological similarity. The results of the computational analysis showed a significant positive relationship between phonological form and semantic meaning for both sign languages, which was strongest when the sign language lexicons were organized into clusters of semantically related signs. The analysis also revealed variation in the strength of patterns across the form-meaning relationships seen between phonological parameters within each sign language, as well as between the two languages. This shows that while the connection between form and meaning is not entirely language specific, there are cross-linguistic differences in how these mappings are realized for signs in each language, suggesting that arbitrariness as well as cognitive or cultural influences may play a role in how these patterns are realized. The results of this analysis not only contribute to our understanding of the distribution of non-arbitrariness in sign language lexicons, but also demonstrate a new way that computational modeling can be harnessed in lexicon-wide investigations of sign languages.",
      "abstract": "Over the history of research on sign languages, much scholarship has highlighted the pervasive presence of signs whose forms relate to their meaning in a non-arbitrary way. The presence of these forms suggests that sign language vocabularies are shaped, at least in part, by a pressure toward maintaining a link between form and meaning in wordforms. We use a vector space approach to test the ways this pressure might shape sign language vocabularies, examining how non-arbitrary forms are distributed within the lexicons of two unrelated sign languages. Vector space models situate the representations of words in a multi-dimensional space where the distance between words indexes their relatedness in meaning. Using phonological information from the vocabularies of American Sign Language (ASL) and British Sign Language (BSL), we tested whether increased similarity between the semantic representations of signs corresponds to increased phonological similarity. The results of the computational analysis showed a significant positive relationship between phonological form and semantic meaning for both sign languages, which was strongest when the sign language lexicons were organized into clusters of semantically related signs. The analysis also revealed variation in the strength of patterns across the form-meaning relationships seen between phonological parameters within each sign language, as well as between the two languages. This shows that while the connection between form and meaning is not entirely language specific, there are cross-linguistic differences in how these mappings are realized for signs in each language, suggesting that arbitrariness as well as cognitive or cultural influences may play a role in how these patterns are realized. The results of this analysis not only contribute to our understanding of the distribution of non-arbitrariness in sign language lexicons, but also demonstrate a new way that computational modeling can be harnessed in lexicon-wide investigations of sign languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.806471",
      "openalex_id": "https://openalex.org/W4221068510",
      "arxiv_id": "",
      "publication_date": "2022-03-16",
      "published": "2022-03-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates semantic-phonological relations in two sign languages using vector-space modeling and does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it focuses on sign-language phonology/semantics rather than discrete audio tokens for speech.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates semantic-phonological relations in two sign languages using vector-space modeling and does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it focuses on sign-language phonology/semantics rather than discrete audio tokens for speech.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic and phonological correlations in sign languages using vector space models but does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete quantization of continuous audio signals; it centers on sign language phonology and computational semantics without addressing discrete audio tokenization relevant to speech.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic and phonological correlations in sign languages using vector space models but does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete quantization of continuous audio signals; it centers on sign language phonology and computational semantics without addressing discrete audio tokenization relevant to speech.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Give Me A Sign: Using Data Gloves For Static Hand Shape Recognition",
    "abstract": "Human-to-human communication via the computer is mainly done using a keyboard or microphone. In the field of Virtual Reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g. silent commands during task force training) or simply not possible (e.g. if the user has a hearing loss). Data gloves help to increase immersion within the VR as they correspond to our natural interaction. At the same time, they offer the possibility to accurately capture hand shapes, such as those used in non-verbal communication (e.g. thumbs up, okay gesture, ...) and in sign language. In this paper, we present a hand shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an Outlier Detection and a Feature Selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial Data Augmentation, i.e., we create new artificial data from the recorded and filtered data to augment the training dataset. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. Voting Meta-Classifier (VL2) has proven to be the most accurate, albeit slowest, classifier. A good alternative is Random Forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. Outlier Detection has proven to be a effective approach, especially in improving classification time. Overall, we have shown that our hand shape recognition system using data gloves is suitable for communication within VR.",
    "metadata": {
      "title": "Give Me A Sign: Using Data Gloves For Static Hand Shape Recognition",
      "summary": "Human-to-human communication via the computer is mainly done using a keyboard or microphone. In the field of Virtual Reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g. silent commands during task force training) or simply not possible (e.g. if the user has a hearing loss). Data gloves help to increase immersion within the VR as they correspond to our natural interaction. At the same time, they offer the possibility to accurately capture hand shapes, such as those used in non-verbal communication (e.g. thumbs up, okay gesture, ...) and in sign language. In this paper, we present a hand shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an Outlier Detection and a Feature Selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial Data Augmentation, i.e., we create new artificial data from the recorded and filtered data to augment the training dataset. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. Voting Meta-Classifier (VL2) has proven to be the most accurate, albeit slowest, classifier. A good alternative is Random Forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. Outlier Detection has proven to be a effective approach, especially in improving classification time. Overall, we have shown that our hand shape recognition system using data gloves is suitable for communication within VR.",
      "abstract": "Human-to-human communication via the computer is mainly done using a keyboard or microphone. In the field of Virtual Reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g. silent commands during task force training) or simply not possible (e.g. if the user has a hearing loss). Data gloves help to increase immersion within the VR as they correspond to our natural interaction. At the same time, they offer the possibility to accurately capture hand shapes, such as those used in non-verbal communication (e.g. thumbs up, okay gesture, ...) and in sign language. In this paper, we present a hand shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an Outlier Detection and a Feature Selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial Data Augmentation, i.e., we create new artificial data from the recorded and filtered data to augment the training dataset. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. Voting Meta-Classifier (VL2) has proven to be the most accurate, albeit slowest, classifier. A good alternative is Random Forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. Outlier Detection has proven to be a effective approach, especially in improving classification time. Overall, we have shown that our hand shape recognition system using data gloves is suitable for communication within VR.",
      "doi": "https://doi.org/10.20944/preprints202311.1385.v1",
      "openalex_id": "https://openalex.org/W4388911562",
      "arxiv_id": "",
      "publication_date": "2023-11-22",
      "published": "2023-11-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on hand-gesture recognition with data gloves; it does not address discrete audio tokenization, thus it does not meet the audio/tokenization inclusion criteria (no analysis of discrete audio tokens) and does not trigger any exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on hand-gesture recognition with data gloves; it does not address discrete audio tokenization, thus it does not meet the audio/tokenization inclusion criteria (no analysis of discrete audio tokens) and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hand shape recognition using data gloves within virtual reality, without addressing discrete audio or speech tokenization, nor analyzing or proposing any discrete audio signals or related methods, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hand shape recognition using data gloves within virtual reality, without addressing discrete audio or speech tokenization, nor analyzing or proposing any discrete audio signals or related methods, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RF Micro-Doppler Classification with Multiple Spectrograms from Angular Subspace Projections",
    "abstract": "Radio Frequency (RF) sensors present distinct ad-vantages over cameras or wearables for hand gesture recognition providing high resolution radial range and velocity measurement, being able to operate in dark and through the objects with high temporal and frequency resolutions. Moreover, the flexibility of the complex formatted data allows users to develop their own algorithms to generate various data representations such as time-frequency (Micro-Doppler - &#x03BC;D) maps, or range-Doppler or - angle as a function of time. However, conventional &#x03BC;-D generation does not regard the angular information of the multiple targets existing in the RF data. Hence, multiple targets with different &#x03BC;-D signatures at various angular positions create a mixed spec-trogram output reducing recognition performance. This paper proposes an angular projection approach on radar data cubes (RDCs) to generate raw radar data for defined angular subspaces. Hence multiple &#x03BC;-D spectrograms for each angular subspace can be constructed from the projected data. The proposed approach has been tested on RF data for gross body movement and American Sign Language (ASL) recognition. It has been showed that the utilization of angular projected spectrograms increases classification accuracy for ASL and achieves recognition accuracy of 92.6&#x0025; for 20 word ASL signs.",
    "metadata": {
      "title": "RF Micro-Doppler Classification with Multiple Spectrograms from Angular Subspace Projections",
      "summary": "Radio Frequency (RF) sensors present distinct ad-vantages over cameras or wearables for hand gesture recognition providing high resolution radial range and velocity measurement, being able to operate in dark and through the objects with high temporal and frequency resolutions. Moreover, the flexibility of the complex formatted data allows users to develop their own algorithms to generate various data representations such as time-frequency (Micro-Doppler - &#x03BC;D) maps, or range-Doppler or - angle as a function of time. However, conventional &#x03BC;-D generation does not regard the angular information of the multiple targets existing in the RF data. Hence, multiple targets with different &#x03BC;-D signatures at various angular positions create a mixed spec-trogram output reducing recognition performance. This paper proposes an angular projection approach on radar data cubes (RDCs) to generate raw radar data for defined angular subspaces. Hence multiple &#x03BC;-D spectrograms for each angular subspace can be constructed from the projected data. The proposed approach has been tested on RF data for gross body movement and American Sign Language (ASL) recognition. It has been showed that the utilization of angular projected spectrograms increases classification accuracy for ASL and achieves recognition accuracy of 92.6&#x0025; for 20 word ASL signs.",
      "abstract": "Radio Frequency (RF) sensors present distinct ad-vantages over cameras or wearables for hand gesture recognition providing high resolution radial range and velocity measurement, being able to operate in dark and through the objects with high temporal and frequency resolutions. Moreover, the flexibility of the complex formatted data allows users to develop their own algorithms to generate various data representations such as time-frequency (Micro-Doppler - &#x03BC;D) maps, or range-Doppler or - angle as a function of time. However, conventional &#x03BC;-D generation does not regard the angular information of the multiple targets existing in the RF data. Hence, multiple targets with different &#x03BC;-D signatures at various angular positions create a mixed spec-trogram output reducing recognition performance. This paper proposes an angular projection approach on radar data cubes (RDCs) to generate raw radar data for defined angular subspaces. Hence multiple &#x03BC;-D spectrograms for each angular subspace can be constructed from the projected data. The proposed approach has been tested on RF data for gross body movement and American Sign Language (ASL) recognition. It has been showed that the utilization of angular projected spectrograms increases classification accuracy for ASL and achieves recognition accuracy of 92.6&#x0025; for 20 word ASL signs.",
      "doi": "https://doi.org/10.1109/radarconf2248738.2022.9763904",
      "openalex_id": "https://openalex.org/W4229371428",
      "arxiv_id": "",
      "publication_date": "2022-03-21",
      "published": "2022-03-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on RF micro-Doppler spectrograms for gesture/ASL recognition using angular subspace projections, and does not involve discrete audio tokenization or discrete token design, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on RF micro-Doppler spectrograms for gesture/ASL recognition using angular subspace projections, and does not involve discrete audio tokenization or discrete token design, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on RF sensor data processing and classification for hand gesture and American Sign Language recognition using angular subspace projections and multiple spectrograms, with no mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signal into discrete tokens, thus not meeting the inclusion criteria regarding discrete tokenization of audio signals for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on RF sensor data processing and classification for hand gesture and American Sign Language recognition using angular subspace projections and multiple spectrograms, with no mention or analysis of discrete audio/speech tokenization or conversion of continuous audio signal into discrete tokens, thus not meeting the inclusion criteria regarding discrete tokenization of audio signals for speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Units of sub-sign meaning in NGT",
    "abstract": "Abstract This paper provides an overview of all the meaningful sub-sign form units (form-meaning units; FMUs) in lexical signs in Sign Language of the Netherlands (NGT). We investigated the potential meaning of all form features that were previously established in analyses of NGT form by analyzing their distribution in lexical signs. The data set consisted of 500 NGT signs in the lexical database Global Signbank, and a set of 163 elicited newly-formed lexical signs. All features in these data sets appear to bear meaning (at least once). No completely arbitrary features were found, and some features appeared to be always associated to a specific meaning. This toolkit and the set of FMUs in NGT provides a possible basis for cross-linguistic study and for a more fine-grained approach in various research disciplines, for instance psycholinguistics and acquisition, and it may thus advance the theoretical and applied study of sign languages.",
    "metadata": {
      "title": "Units of sub-sign meaning in NGT",
      "summary": "Abstract This paper provides an overview of all the meaningful sub-sign form units (form-meaning units; FMUs) in lexical signs in Sign Language of the Netherlands (NGT). We investigated the potential meaning of all form features that were previously established in analyses of NGT form by analyzing their distribution in lexical signs. The data set consisted of 500 NGT signs in the lexical database Global Signbank, and a set of 163 elicited newly-formed lexical signs. All features in these data sets appear to bear meaning (at least once). No completely arbitrary features were found, and some features appeared to be always associated to a specific meaning. This toolkit and the set of FMUs in NGT provides a possible basis for cross-linguistic study and for a more fine-grained approach in various research disciplines, for instance psycholinguistics and acquisition, and it may thus advance the theoretical and applied study of sign languages.",
      "abstract": "Abstract This paper provides an overview of all the meaningful sub-sign form units (form-meaning units; FMUs) in lexical signs in Sign Language of the Netherlands (NGT). We investigated the potential meaning of all form features that were previously established in analyses of NGT form by analyzing their distribution in lexical signs. The data set consisted of 500 NGT signs in the lexical database Global Signbank, and a set of 163 elicited newly-formed lexical signs. All features in these data sets appear to bear meaning (at least once). No completely arbitrary features were found, and some features appeared to be always associated to a specific meaning. This toolkit and the set of FMUs in NGT provides a possible basis for cross-linguistic study and for a more fine-grained approach in various research disciplines, for instance psycholinguistics and acquisition, and it may thus advance the theoretical and applied study of sign languages.",
      "doi": "https://doi.org/10.1075/sll.20009.van",
      "openalex_id": "https://openalex.org/W4387879108",
      "arxiv_id": "",
      "publication_date": "2023-10-23",
      "published": "2023-10-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract studies form-meaning units in Sign Language of the Netherlands (NGT) using sign data, with no focus on discrete audio/speech tokenization or converting continuous audio into a discrete token sequence for speech tasks; hence it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract studies form-meaning units in Sign Language of the Netherlands (NGT) using sign data, with no focus on discrete audio/speech tokenization or converting continuous audio into a discrete token sequence for speech tasks; hence it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on meaningful sub-sign units in Dutch Sign Language, analyzing form features of signs rather than discrete audio or speech tokenization; moreover, it does not address audio signals, neural encoding, or discrete audio token representations relevant to speech, and hence does not meet the inclusion criteria focused on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on meaningful sub-sign units in Dutch Sign Language, analyzing form features of signs rather than discrete audio or speech tokenization; moreover, it does not address audio signals, neural encoding, or discrete audio token representations relevant to speech, and hence does not meet the inclusion criteria focused on audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Functional and structural brain asymmetries in sign language processing",
    "abstract": "The capacity for language constitutes a cornerstone of human cognition and distinguishes our species from other animals. Research in the cognitive sciences has demonstrated that this capacity is not bound to speech but can also be externalized in the form of sign language. Sign languages are the naturally occurring languages of the deaf and rely on movements and configurations of hands, arms, face, and torso in space. This chapter reviews the functional and structural organization of the neural substrates of sign language, as identified by neuroimaging research over the past decades. Most aspects of sign language processing in adult deaf signers markedly mirror the well-known, functional left-lateralization of spoken and written language. However, both hemispheres exhibit a certain equipotentiality for processing linguistic information and the right hemisphere seems to specifically support processing of some constructions unique to the signed modality. Crucially, the so-called \"core language network\" in the left hemisphere constitutes a functional and structural asymmetry in typically developed deaf and hearing populations alike: This network is (i) pivotal for processing complex syntax independent of the modality of language use, (ii) matures in accordance with a genetically determined biologic matrix, and (iii) may have constituted an evolutionary prerequisite for the emergence of the human capacity for language.",
    "metadata": {
      "title": "Functional and structural brain asymmetries in sign language processing",
      "summary": "The capacity for language constitutes a cornerstone of human cognition and distinguishes our species from other animals. Research in the cognitive sciences has demonstrated that this capacity is not bound to speech but can also be externalized in the form of sign language. Sign languages are the naturally occurring languages of the deaf and rely on movements and configurations of hands, arms, face, and torso in space. This chapter reviews the functional and structural organization of the neural substrates of sign language, as identified by neuroimaging research over the past decades. Most aspects of sign language processing in adult deaf signers markedly mirror the well-known, functional left-lateralization of spoken and written language. However, both hemispheres exhibit a certain equipotentiality for processing linguistic information and the right hemisphere seems to specifically support processing of some constructions unique to the signed modality. Crucially, the so-called \"core language network\" in the left hemisphere constitutes a functional and structural asymmetry in typically developed deaf and hearing populations alike: This network is (i) pivotal for processing complex syntax independent of the modality of language use, (ii) matures in accordance with a genetically determined biologic matrix, and (iii) may have constituted an evolutionary prerequisite for the emergence of the human capacity for language.",
      "abstract": "The capacity for language constitutes a cornerstone of human cognition and distinguishes our species from other animals. Research in the cognitive sciences has demonstrated that this capacity is not bound to speech but can also be externalized in the form of sign language. Sign languages are the naturally occurring languages of the deaf and rely on movements and configurations of hands, arms, face, and torso in space. This chapter reviews the functional and structural organization of the neural substrates of sign language, as identified by neuroimaging research over the past decades. Most aspects of sign language processing in adult deaf signers markedly mirror the well-known, functional left-lateralization of spoken and written language. However, both hemispheres exhibit a certain equipotentiality for processing linguistic information and the right hemisphere seems to specifically support processing of some constructions unique to the signed modality. Crucially, the so-called \"core language network\" in the left hemisphere constitutes a functional and structural asymmetry in typically developed deaf and hearing populations alike: This network is (i) pivotal for processing complex syntax independent of the modality of language use, (ii) matures in accordance with a genetically determined biologic matrix, and (iii) may have constituted an evolutionary prerequisite for the emergence of the human capacity for language.",
      "doi": "https://doi.org/10.1016/b978-0-443-15646-5.00021-x",
      "openalex_id": "https://openalex.org/W4408282267",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article reviews neural substrates of sign language processing using neuroimaging and does not involve discrete audio tokenization or analysis of discrete speech tokens, so it fails the stated inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article reviews neural substrates of sign language processing using neuroimaging and does not involve discrete audio tokenization or analysis of discrete speech tokens, so it fails the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on brain asymmetries in sign language processing using neuroimaging, with no mention of discrete audio/speech tokenization, audio signal conversion, or related methodologies, hence it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on brain asymmetries in sign language processing using neuroimaging, with no mention of discrete audio/speech tokenization, audio signal conversion, or related methodologies, hence it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Iconicity as an organizing principle of the lexicon",
    "abstract": "The view that words are arbitrary is a foundational assumption about language, used to set human languages apart from nonhuman communication. We present here a study of the alignment between the semantic and phonological structure (systematicity) of American Sign Language (ASL), and for comparison, two spoken languages—English and Spanish. Across all three languages, words that are semantically related are more likely to be phonologically related, highlighting systematic alignment between word form and word meaning. Critically, there is a significant effect of iconicity (a perceived physical resemblance between word form and word meaning) on this alignment: words are most likely to be phonologically related when they are semantically related and iconic. This phenomenon is particularly widespread in ASL: half of the signs in the ASL lexicon are iconically related to other signs, i.e., there is a nonarbitrary relationship between form and meaning that is shared across signs. Taken together, the results reveal that iconicity can act as a driving force behind the alignment between the semantic and phonological structure of spoken and signed languages, but languages may differ in the extent that iconicity structures the lexicon. Theories of language must account for iconicity as a possible organizing principle of the lexicon.",
    "metadata": {
      "title": "Iconicity as an organizing principle of the lexicon",
      "summary": "The view that words are arbitrary is a foundational assumption about language, used to set human languages apart from nonhuman communication. We present here a study of the alignment between the semantic and phonological structure (systematicity) of American Sign Language (ASL), and for comparison, two spoken languages—English and Spanish. Across all three languages, words that are semantically related are more likely to be phonologically related, highlighting systematic alignment between word form and word meaning. Critically, there is a significant effect of iconicity (a perceived physical resemblance between word form and word meaning) on this alignment: words are most likely to be phonologically related when they are semantically related and iconic. This phenomenon is particularly widespread in ASL: half of the signs in the ASL lexicon are iconically related to other signs, i.e., there is a nonarbitrary relationship between form and meaning that is shared across signs. Taken together, the results reveal that iconicity can act as a driving force behind the alignment between the semantic and phonological structure of spoken and signed languages, but languages may differ in the extent that iconicity structures the lexicon. Theories of language must account for iconicity as a possible organizing principle of the lexicon.",
      "abstract": "The view that words are arbitrary is a foundational assumption about language, used to set human languages apart from nonhuman communication. We present here a study of the alignment between the semantic and phonological structure (systematicity) of American Sign Language (ASL), and for comparison, two spoken languages—English and Spanish. Across all three languages, words that are semantically related are more likely to be phonologically related, highlighting systematic alignment between word form and word meaning. Critically, there is a significant effect of iconicity (a perceived physical resemblance between word form and word meaning) on this alignment: words are most likely to be phonologically related when they are semantically related and iconic. This phenomenon is particularly widespread in ASL: half of the signs in the ASL lexicon are iconically related to other signs, i.e., there is a nonarbitrary relationship between form and meaning that is shared across signs. Taken together, the results reveal that iconicity can act as a driving force behind the alignment between the semantic and phonological structure of spoken and signed languages, but languages may differ in the extent that iconicity structures the lexicon. Theories of language must account for iconicity as a possible organizing principle of the lexicon.",
      "doi": "https://doi.org/10.1073/pnas.2401041122",
      "openalex_id": "https://openalex.org/W4409416063",
      "arxiv_id": "",
      "publication_date": "2025-04-14",
      "published": "2025-04-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes iconicity and the relation between meaning and form in ASL and spoken languages, focusing on linguistic structure rather than discrete audio tokenization or token-level representations, so it does not meet the inclusion criterion for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes iconicity and the relation between meaning and form in ASL and spoken languages, focusing on linguistic structure rather than discrete audio tokenization or token-level representations, so it does not meet the inclusion criterion for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the relationship between semantic and phonological structure in languages, including ASL, English, and Spanish, specifically examining iconicity as an organizing principle of the lexicon; however, it does not discuss discrete audio/speech tokenization, the conversion of continuous audio signals into discrete tokens, nor does it align with any methodological contributions to discrete tokenization relevant to audio processing or language models, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the relationship between semantic and phonological structure in languages, including ASL, English, and Spanish, specifically examining iconicity as an organizing principle of the lexicon; however, it does not discuss discrete audio/speech tokenization, the conversion of continuous audio signals into discrete tokens, nor does it align with any methodological contributions to discrete tokenization relevant to audio processing or language models, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Effect of Iconicity on Weak Hand Drop in American Sign Language",
    "abstract": "The research community generally accepts that signed and spoken languages contain both iconicity and arbitrariness. Iconicity's impact on statistical distributions of motivated forms throughout signed language lexicons is clear (e.g. Occhino, 2016). However, there has been little work to determine whether the iconic links between form and meaning are relevant only to a sign's initial formation, or if these links are stored as part of lexical representations. In the present study, 40 Deaf signers of American Sign Language were asked to rate two-handed signs in their citation form and in one-handed (reduced) forms. Twelve signs were highly iconic. For each of these highly iconic sign, a less iconic but phonologically similar sign of the same grammatical category was also chosen. Signs were presented in carrier sentences and in isolation. Participants preferred one-handed forms of the highly iconic signs over one-handed forms of their phonolgogically similar but less iconic counterparts. Thus, iconicity impacted the application of a synchronic phonological process. This finding suggests that lexical representations retain iconic form-meaning links and that these links are accessible to the phonological grammar.",
    "metadata": {
      "title": "The Effect of Iconicity on Weak Hand Drop in American Sign Language",
      "summary": "The research community generally accepts that signed and spoken languages contain both iconicity and arbitrariness. Iconicity's impact on statistical distributions of motivated forms throughout signed language lexicons is clear (e.g. Occhino, 2016). However, there has been little work to determine whether the iconic links between form and meaning are relevant only to a sign's initial formation, or if these links are stored as part of lexical representations. In the present study, 40 Deaf signers of American Sign Language were asked to rate two-handed signs in their citation form and in one-handed (reduced) forms. Twelve signs were highly iconic. For each of these highly iconic sign, a less iconic but phonologically similar sign of the same grammatical category was also chosen. Signs were presented in carrier sentences and in isolation. Participants preferred one-handed forms of the highly iconic signs over one-handed forms of their phonolgogically similar but less iconic counterparts. Thus, iconicity impacted the application of a synchronic phonological process. This finding suggests that lexical representations retain iconic form-meaning links and that these links are accessible to the phonological grammar.",
      "abstract": "The research community generally accepts that signed and spoken languages contain both iconicity and arbitrariness. Iconicity's impact on statistical distributions of motivated forms throughout signed language lexicons is clear (e.g. Occhino, 2016). However, there has been little work to determine whether the iconic links between form and meaning are relevant only to a sign's initial formation, or if these links are stored as part of lexical representations. In the present study, 40 Deaf signers of American Sign Language were asked to rate two-handed signs in their citation form and in one-handed (reduced) forms. Twelve signs were highly iconic. For each of these highly iconic sign, a less iconic but phonologically similar sign of the same grammatical category was also chosen. Signs were presented in carrier sentences and in isolation. Participants preferred one-handed forms of the highly iconic signs over one-handed forms of their phonolgogically similar but less iconic counterparts. Thus, iconicity impacted the application of a synchronic phonological process. This finding suggests that lexical representations retain iconic form-meaning links and that these links are accessible to the phonological grammar.",
      "doi": "https://doi.org/10.3765/amp.v9i0.5305",
      "openalex_id": "https://openalex.org/W4289914217",
      "arxiv_id": "",
      "publication_date": "2022-08-05",
      "published": "2022-08-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article examines iconicity in American Sign Language and lexical representations but does not address discrete audio tokenization or token-level representations for speech, thus it does not meet the inclusion criteria focusing on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article examines iconicity in American Sign Language and lexical representations but does not address discrete audio tokenization or token-level representations for speech, thus it does not meet the inclusion criteria focusing on discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the impact of iconicity on phonological processes in American Sign Language, analyzing sign forms and their lexical representations rather than addressing discrete audio or speech tokenization techniques involving conversion of continuous audio signals into discrete tokens. It lacks any discussion or analysis of audio tokenization, discrete quantization, or neural encoding relevant to speech language modeling, therefore it does not meet the inclusion criteria and aligns with exclusion criteria related to absence of discrete audio token processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the impact of iconicity on phonological processes in American Sign Language, analyzing sign forms and their lexical representations rather than addressing discrete audio or speech tokenization techniques involving conversion of continuous audio signals into discrete tokens. It lacks any discussion or analysis of audio tokenization, discrete quantization, or neural encoding relevant to speech language modeling, therefore it does not meet the inclusion criteria and aligns with exclusion criteria related to absence of discrete audio token processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "W(h)ither the ASL corpus?",
    "abstract": "Abstract In this chapter, we discuss logistic and ideological practices used by signed language corpus projects around the world to develop their corpora, starting with Johnston’s early call to develop signed language corpora in 2004. We then outline a brief history of existing American Sign Language (ASL) corpora or corpus-like collections, most of which are specialized and/or inaccessible. We unpack the lessons we have learned as we, North American linguists interested in signed language research and corpora, have sought to create a national-level corpus following the standards set by earlier projects. We conclude that our contexts are somewhat different, and describe projects that we think would be better suited for our ASL communities. In other words, though our dreams of having a single national-level ASL corpus have ‘withered’, we remain optimistic that corpus methods can be used to catalog and analyze the wealth of ASL signing data currently available.",
    "metadata": {
      "title": "W(h)ither the ASL corpus?",
      "summary": "Abstract In this chapter, we discuss logistic and ideological practices used by signed language corpus projects around the world to develop their corpora, starting with Johnston’s early call to develop signed language corpora in 2004. We then outline a brief history of existing American Sign Language (ASL) corpora or corpus-like collections, most of which are specialized and/or inaccessible. We unpack the lessons we have learned as we, North American linguists interested in signed language research and corpora, have sought to create a national-level corpus following the standards set by earlier projects. We conclude that our contexts are somewhat different, and describe projects that we think would be better suited for our ASL communities. In other words, though our dreams of having a single national-level ASL corpus have ‘withered’, we remain optimistic that corpus methods can be used to catalog and analyze the wealth of ASL signing data currently available.",
      "abstract": "Abstract In this chapter, we discuss logistic and ideological practices used by signed language corpus projects around the world to develop their corpora, starting with Johnston’s early call to develop signed language corpora in 2004. We then outline a brief history of existing American Sign Language (ASL) corpora or corpus-like collections, most of which are specialized and/or inaccessible. We unpack the lessons we have learned as we, North American linguists interested in signed language research and corpora, have sought to create a national-level corpus following the standards set by earlier projects. We conclude that our contexts are somewhat different, and describe projects that we think would be better suited for our ASL communities. In other words, though our dreams of having a single national-level ASL corpus have ‘withered’, we remain optimistic that corpus methods can be used to catalog and analyze the wealth of ASL signing data currently available.",
      "doi": "https://doi.org/10.1075/scl.108.11hoc",
      "openalex_id": "https://openalex.org/W4323661081",
      "arxiv_id": "",
      "publication_date": "2023-03-09",
      "published": "2023-03-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses building and evaluating signing-language corpora and governance rather than proposing or analyzing discrete audio tokenization into a finite set of tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses building and evaluating signing-language corpora and governance rather than proposing or analyzing discrete audio tokenization into a finite set of tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses logistical and ideological aspects of developing signed language corpora focusing on American Sign Language (ASL) and does not address discrete audio/speech tokenization or any related neural encoding or discrete token representations of audio signals, which are the core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses logistical and ideological aspects of developing signed language corpora focusing on American Sign Language (ASL) and does not address discrete audio/speech tokenization or any related neural encoding or discrete token representations of audio signals, which are the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Feeling signs: Motor encoding enhances sign language learning in hearing adults",
    "abstract": "Abstract Manual production enhances learning and recall of signs by hearing second language learners; however, the mechanisms enabling this effect are unclear. We examined whether the motor encoding (somatosensory feedback) that occurs during sign production benefits learning and whether it interacts with sign iconicity, which also enhances learning. American Sign Language (ASL) signs varying in iconicity were learned either via production (repetition) with the eyes closed or via observation without production. Signs learned via production were recalled more accurately than signs learned via observation, indicating that motor encoding from manual production enriches the representations of signs. Moreover, the effect of motor encoding interacted with iconicity, suggesting that motor encoding may particularly enhance the recall of signs low in iconicity. Together, these results reveal the importance of somatosensory feedback as a key mechanism underlying the beneficial effect of production on sign learning, demonstrating that feeling one’s own signing promotes learning and recall of signs.",
    "metadata": {
      "title": "Feeling signs: Motor encoding enhances sign language learning in hearing adults",
      "summary": "Abstract Manual production enhances learning and recall of signs by hearing second language learners; however, the mechanisms enabling this effect are unclear. We examined whether the motor encoding (somatosensory feedback) that occurs during sign production benefits learning and whether it interacts with sign iconicity, which also enhances learning. American Sign Language (ASL) signs varying in iconicity were learned either via production (repetition) with the eyes closed or via observation without production. Signs learned via production were recalled more accurately than signs learned via observation, indicating that motor encoding from manual production enriches the representations of signs. Moreover, the effect of motor encoding interacted with iconicity, suggesting that motor encoding may particularly enhance the recall of signs low in iconicity. Together, these results reveal the importance of somatosensory feedback as a key mechanism underlying the beneficial effect of production on sign learning, demonstrating that feeling one’s own signing promotes learning and recall of signs.",
      "abstract": "Abstract Manual production enhances learning and recall of signs by hearing second language learners; however, the mechanisms enabling this effect are unclear. We examined whether the motor encoding (somatosensory feedback) that occurs during sign production benefits learning and whether it interacts with sign iconicity, which also enhances learning. American Sign Language (ASL) signs varying in iconicity were learned either via production (repetition) with the eyes closed or via observation without production. Signs learned via production were recalled more accurately than signs learned via observation, indicating that motor encoding from manual production enriches the representations of signs. Moreover, the effect of motor encoding interacted with iconicity, suggesting that motor encoding may particularly enhance the recall of signs low in iconicity. Together, these results reveal the importance of somatosensory feedback as a key mechanism underlying the beneficial effect of production on sign learning, demonstrating that feeling one’s own signing promotes learning and recall of signs.",
      "doi": "https://doi.org/10.1017/s0272263124000196",
      "openalex_id": "https://openalex.org/W4396228599",
      "arxiv_id": "",
      "publication_date": "2024-04-29",
      "published": "2024-04-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article examines motor encoding and somatosensory feedback in learning ASL signs, with no discrete audio/tokenization concept; inclusion requires explicit analysis of discrete speech audio tokenization, which this does not address, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article examines motor encoding and somatosensory feedback in learning ASL signs, with no discrete audio/tokenization concept; inclusion requires explicit analysis of discrete speech audio tokenization, which this does not address, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language learning and the benefits of motor encoding during manual sign production; it does not address discrete audio/speech tokenization or analyze the conversion of continuous audio signals into discrete symbols, thus failing to meet the inclusion criteria regarding discrete audio tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language learning and the benefits of motor encoding during manual sign production; it does not address discrete audio/speech tokenization or analyze the conversion of continuous audio signals into discrete symbols, thus failing to meet the inclusion criteria regarding discrete audio tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A lexical database of British Sign Language (BSL) and German Sign Language (DGS): Iconicity ratings, iconic strategies, and concreteness norms",
    "abstract": "Abstract Iconicity, understood as a resemblance relationship between meaning and form, is an important variable that has important psycholinguistic effects in lexical processing and language learning across modalities of language. With the growing interest in iconicity, clear operationalizations in terms of the different ways in which iconicity is construed and measured are critical for establishing its broader psycholinguistic profile. This study reports a normed database of iconicity ratings for the same concepts in British Sign Language (BSL) and German Sign Language (DGS). As a related dimension, we also report the type of iconic mapping strategy, i.e., a nominal variable that reflects the different ways in which signs make form-meaning associations for each sign. Finally, we include concreteness ratings for the same concepts. Data from deaf and hearing signers show that iconicity ratings are strongly correlated across both languages, with different distributions across the different strategies, and skewed towards the iconic end of the scale for all groups except German hearing non-signers. Concreteness ratings in BSL and DGS are correlated, though more weakly, and skewed towards the concrete end of the scale. Interestingly, this differs from findings for spoken languages, where concreteness ratings exhibit substantially stronger correlations and abstract concepts are more predominantly represented. We also find that iconicity and concreteness ratings have a moderate positive and strong positive correlation in BSL and DGS, respectively. These results will be useful in psycholinguistic research and highlight differences that can be attributed to the manual-visual modality of signs.",
    "metadata": {
      "title": "A lexical database of British Sign Language (BSL) and German Sign Language (DGS): Iconicity ratings, iconic strategies, and concreteness norms",
      "summary": "Abstract Iconicity, understood as a resemblance relationship between meaning and form, is an important variable that has important psycholinguistic effects in lexical processing and language learning across modalities of language. With the growing interest in iconicity, clear operationalizations in terms of the different ways in which iconicity is construed and measured are critical for establishing its broader psycholinguistic profile. This study reports a normed database of iconicity ratings for the same concepts in British Sign Language (BSL) and German Sign Language (DGS). As a related dimension, we also report the type of iconic mapping strategy, i.e., a nominal variable that reflects the different ways in which signs make form-meaning associations for each sign. Finally, we include concreteness ratings for the same concepts. Data from deaf and hearing signers show that iconicity ratings are strongly correlated across both languages, with different distributions across the different strategies, and skewed towards the iconic end of the scale for all groups except German hearing non-signers. Concreteness ratings in BSL and DGS are correlated, though more weakly, and skewed towards the concrete end of the scale. Interestingly, this differs from findings for spoken languages, where concreteness ratings exhibit substantially stronger correlations and abstract concepts are more predominantly represented. We also find that iconicity and concreteness ratings have a moderate positive and strong positive correlation in BSL and DGS, respectively. These results will be useful in psycholinguistic research and highlight differences that can be attributed to the manual-visual modality of signs.",
      "abstract": "Abstract Iconicity, understood as a resemblance relationship between meaning and form, is an important variable that has important psycholinguistic effects in lexical processing and language learning across modalities of language. With the growing interest in iconicity, clear operationalizations in terms of the different ways in which iconicity is construed and measured are critical for establishing its broader psycholinguistic profile. This study reports a normed database of iconicity ratings for the same concepts in British Sign Language (BSL) and German Sign Language (DGS). As a related dimension, we also report the type of iconic mapping strategy, i.e., a nominal variable that reflects the different ways in which signs make form-meaning associations for each sign. Finally, we include concreteness ratings for the same concepts. Data from deaf and hearing signers show that iconicity ratings are strongly correlated across both languages, with different distributions across the different strategies, and skewed towards the iconic end of the scale for all groups except German hearing non-signers. Concreteness ratings in BSL and DGS are correlated, though more weakly, and skewed towards the concrete end of the scale. Interestingly, this differs from findings for spoken languages, where concreteness ratings exhibit substantially stronger correlations and abstract concepts are more predominantly represented. We also find that iconicity and concreteness ratings have a moderate positive and strong positive correlation in BSL and DGS, respectively. These results will be useful in psycholinguistic research and highlight differences that can be attributed to the manual-visual modality of signs.",
      "doi": "https://doi.org/10.3758/s13428-025-02660-z",
      "openalex_id": "https://openalex.org/W4409309517",
      "arxiv_id": "",
      "publication_date": "2025-04-09",
      "published": "2025-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on iconicity ratings and concreteness norms for sign languages (BSL and DGS) and discusses sign-formation strategies, with no analysis or evaluation of discrete audio tokenization or speech token sequences, thus not meeting the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on iconicity ratings and concreteness norms for sign languages (BSL and DGS) and discusses sign-formation strategies, with no analysis or evaluation of discrete audio tokenization or speech token sequences, thus not meeting the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses a lexical database and ratings related to British and German Sign Language, focusing on iconicity, concreteness, and psycholinguistic aspects, with no mention of discrete audio or speech tokenization of continuous audio signals; thus, it does not meet the inclusion criteria regarding discrete audio tokenization or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses a lexical database and ratings related to British and German Sign Language, focusing on iconicity, concreteness, and psycholinguistic aspects, with no mention of discrete audio or speech tokenization of continuous audio signals; thus, it does not meet the inclusion criteria regarding discrete audio tokenization or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Are form priming effects phonological or perceptual? Electrophysiological evidence from American Sign Language",
    "abstract": "Form priming has been used to identify and demarcate the processes that underlie word and sign recognition. The facilitation that results from the prime and target being related in form is typically interpreted in terms of pre-activation of linguistic representations, with little to no consideration for the potential contributions of increased perceptual overlap between related pairs. Indeed, isolating the contribution of perceptual similarity is impossible in spoken languages; there are no listeners who can perceive speech but have not acquired a sound-based phonological system. Here, we compared the electrophysiological indices of form priming effects in American Sign Language between hearing non-signers (i.e., who had no visual-manual phonological system) and deaf signers. We reasoned that similarities in priming effects between groups would most likely be perceptual in nature, whereas priming effects that are specific to the signer group would reflect pre-activation of phonological representations. Behavior in the go/no-go repetition detection task was remarkably similar between groups. Priming in a pre-N400 window was also largely similar across groups, consistent with an early effect of perceptual similarity. However, priming effects diverged between groups during the subsequent N400 and post-N400 windows. Signers had more typical form priming effects and were especially attuned to handshape overlap, whereas non-signers did not exhibit an N400 component and were more sensitive to location overlap. We attribute this pattern to an interplay between perceptual similarity and phonological knowledge. Perceptual similarity contributes to early phonological priming effects, while phonological knowledge tunes sensitivity to linguistically relevant dimensions of perceptual similarity.",
    "metadata": {
      "title": "Are form priming effects phonological or perceptual? Electrophysiological evidence from American Sign Language",
      "summary": "Form priming has been used to identify and demarcate the processes that underlie word and sign recognition. The facilitation that results from the prime and target being related in form is typically interpreted in terms of pre-activation of linguistic representations, with little to no consideration for the potential contributions of increased perceptual overlap between related pairs. Indeed, isolating the contribution of perceptual similarity is impossible in spoken languages; there are no listeners who can perceive speech but have not acquired a sound-based phonological system. Here, we compared the electrophysiological indices of form priming effects in American Sign Language between hearing non-signers (i.e., who had no visual-manual phonological system) and deaf signers. We reasoned that similarities in priming effects between groups would most likely be perceptual in nature, whereas priming effects that are specific to the signer group would reflect pre-activation of phonological representations. Behavior in the go/no-go repetition detection task was remarkably similar between groups. Priming in a pre-N400 window was also largely similar across groups, consistent with an early effect of perceptual similarity. However, priming effects diverged between groups during the subsequent N400 and post-N400 windows. Signers had more typical form priming effects and were especially attuned to handshape overlap, whereas non-signers did not exhibit an N400 component and were more sensitive to location overlap. We attribute this pattern to an interplay between perceptual similarity and phonological knowledge. Perceptual similarity contributes to early phonological priming effects, while phonological knowledge tunes sensitivity to linguistically relevant dimensions of perceptual similarity.",
      "abstract": "Form priming has been used to identify and demarcate the processes that underlie word and sign recognition. The facilitation that results from the prime and target being related in form is typically interpreted in terms of pre-activation of linguistic representations, with little to no consideration for the potential contributions of increased perceptual overlap between related pairs. Indeed, isolating the contribution of perceptual similarity is impossible in spoken languages; there are no listeners who can perceive speech but have not acquired a sound-based phonological system. Here, we compared the electrophysiological indices of form priming effects in American Sign Language between hearing non-signers (i.e., who had no visual-manual phonological system) and deaf signers. We reasoned that similarities in priming effects between groups would most likely be perceptual in nature, whereas priming effects that are specific to the signer group would reflect pre-activation of phonological representations. Behavior in the go/no-go repetition detection task was remarkably similar between groups. Priming in a pre-N400 window was also largely similar across groups, consistent with an early effect of perceptual similarity. However, priming effects diverged between groups during the subsequent N400 and post-N400 windows. Signers had more typical form priming effects and were especially attuned to handshape overlap, whereas non-signers did not exhibit an N400 component and were more sensitive to location overlap. We attribute this pattern to an interplay between perceptual similarity and phonological knowledge. Perceptual similarity contributes to early phonological priming effects, while phonological knowledge tunes sensitivity to linguistically relevant dimensions of perceptual similarity.",
      "doi": "https://doi.org/10.1016/j.cognition.2021.104979",
      "openalex_id": "https://openalex.org/W4200149183",
      "arxiv_id": "",
      "publication_date": "2021-12-11",
      "published": "2021-12-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article investigates form priming in American Sign Language and argues about perceptual vs phonological processing; it does not involve discrete audio/speech tokenization or token quantization relevant to speech tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article investigates form priming in American Sign Language and argues about perceptual vs phonological processing; it does not involve discrete audio/speech tokenization or token quantization relevant to speech tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates form priming effects in American Sign Language using electrophysiological methods to differentiate perceptual versus phonological contributions, but it does not propose, analyze, or evaluate discrete audio or speech tokenization, nor does it convert continuous audio signals into discrete tokens for language-model-style tasks; therefore, it does not meet the inclusion criteria related to discrete tokenization of audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates form priming effects in American Sign Language using electrophysiological methods to differentiate perceptual versus phonological contributions, but it does not propose, analyze, or evaluate discrete audio or speech tokenization, nor does it convert continuous audio signals into discrete tokens for language-model-style tasks; therefore, it does not meet the inclusion criteria related to discrete tokenization of audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural substrates of sign language vocabulary processing in less-skilled hearing M2L2 signers: Evidence for difficult phonological movement perception",
    "abstract": "No previous research has investigated the neural correlates of vocabulary acquisition in second language learners of sign language. The present study investigated whether poor vocabulary knowledge engaged similar prefrontal lexico-semantic regions as seen in unimodal L2 learners. Behavioral improvements in vocabulary knowledge in a cohort of M2L2 learners were quantified. Results indicated that there is significant increase in vocabulary knowledge after one semester, but stabilized in the second semester. A longitudinal fMRI analysis was implemented for a subset of learners who were followed for the entire 10 months during initial sign language acquisition. The results indicated that learners who had poor sign vocabulary knowledge consistently showed greater activation in regions involved in motor simulation, salience, biological motion and spatial processing, and lexico-semantic retrieval. In conclusion, poor vocabulary knowledge requires greater engagement of modality-independent and modality-dependent regions, which could account for behavioral evidence of difficulty in visual phonology processing.",
    "metadata": {
      "title": "Neural substrates of sign language vocabulary processing in less-skilled hearing M2L2 signers: Evidence for difficult phonological movement perception",
      "summary": "No previous research has investigated the neural correlates of vocabulary acquisition in second language learners of sign language. The present study investigated whether poor vocabulary knowledge engaged similar prefrontal lexico-semantic regions as seen in unimodal L2 learners. Behavioral improvements in vocabulary knowledge in a cohort of M2L2 learners were quantified. Results indicated that there is significant increase in vocabulary knowledge after one semester, but stabilized in the second semester. A longitudinal fMRI analysis was implemented for a subset of learners who were followed for the entire 10 months during initial sign language acquisition. The results indicated that learners who had poor sign vocabulary knowledge consistently showed greater activation in regions involved in motor simulation, salience, biological motion and spatial processing, and lexico-semantic retrieval. In conclusion, poor vocabulary knowledge requires greater engagement of modality-independent and modality-dependent regions, which could account for behavioral evidence of difficulty in visual phonology processing.",
      "abstract": "No previous research has investigated the neural correlates of vocabulary acquisition in second language learners of sign language. The present study investigated whether poor vocabulary knowledge engaged similar prefrontal lexico-semantic regions as seen in unimodal L2 learners. Behavioral improvements in vocabulary knowledge in a cohort of M2L2 learners were quantified. Results indicated that there is significant increase in vocabulary knowledge after one semester, but stabilized in the second semester. A longitudinal fMRI analysis was implemented for a subset of learners who were followed for the entire 10 months during initial sign language acquisition. The results indicated that learners who had poor sign vocabulary knowledge consistently showed greater activation in regions involved in motor simulation, salience, biological motion and spatial processing, and lexico-semantic retrieval. In conclusion, poor vocabulary knowledge requires greater engagement of modality-independent and modality-dependent regions, which could account for behavioral evidence of difficulty in visual phonology processing.",
      "doi": "https://doi.org/10.1017/s1366728917000347",
      "openalex_id": "https://openalex.org/W2727952944",
      "arxiv_id": "",
      "publication_date": "2017-07-06",
      "published": "2017-07-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates neural substrates of sign language vocabulary processing without any discrete audio/tokenization analysis, focusing on visual-gestural language rather than discrete speech tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates neural substrates of sign language vocabulary processing without any discrete audio/tokenization analysis, focusing on visual-gestural language rather than discrete speech tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural substrates of sign language vocabulary processing and phonological movement perception in second language learners, without any mention of discrete audio or speech tokenization methods, discrete symbol sequences, or audio signal processing relevant to tokenization; thus, it does not meet the inclusion criteria requiring discrete audio tokenization as a core study object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural substrates of sign language vocabulary processing and phonological movement perception in second language learners, without any mention of discrete audio or speech tokenization methods, discrete symbol sequences, or audio signal processing relevant to tokenization; thus, it does not meet the inclusion criteria requiring discrete audio tokenization as a core study object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Earlier and more robust sensorimotor discrimination of ASL signs in deaf signers during imitation",
    "abstract": "Prior research suggests that the amount of experience an individual has with an action influences the degree to which the sensorimotor systems of their brain are involved in the subsequent perception of those actions. Less is known about how action experience and semantic knowledge impact sensorimotor involvement during imitation. To address this question, we collected electroencephalograms (EEG) while deaf signers and hearing non-signers imitated one-handed and two-handed ASL signs. During observation, deaf signers showed early differentiation in alpha/beta power between the one- and two-handed sign conditions, whereas hearing non-signers showed this discrimination only later. During sign imitation, deaf signers showed desynchronisation of alpha/beta EEG signals, while hearing non-signers showed increased power. Thus, in an imitative context, deaf signers engage anticipatory motor preparation in advance of action production, while hearing non-signers engage slower, more memory-related processes to help them complete the complex task.",
    "metadata": {
      "title": "Earlier and more robust sensorimotor discrimination of ASL signs in deaf signers during imitation",
      "summary": "Prior research suggests that the amount of experience an individual has with an action influences the degree to which the sensorimotor systems of their brain are involved in the subsequent perception of those actions. Less is known about how action experience and semantic knowledge impact sensorimotor involvement during imitation. To address this question, we collected electroencephalograms (EEG) while deaf signers and hearing non-signers imitated one-handed and two-handed ASL signs. During observation, deaf signers showed early differentiation in alpha/beta power between the one- and two-handed sign conditions, whereas hearing non-signers showed this discrimination only later. During sign imitation, deaf signers showed desynchronisation of alpha/beta EEG signals, while hearing non-signers showed increased power. Thus, in an imitative context, deaf signers engage anticipatory motor preparation in advance of action production, while hearing non-signers engage slower, more memory-related processes to help them complete the complex task.",
      "abstract": "Prior research suggests that the amount of experience an individual has with an action influences the degree to which the sensorimotor systems of their brain are involved in the subsequent perception of those actions. Less is known about how action experience and semantic knowledge impact sensorimotor involvement during imitation. To address this question, we collected electroencephalograms (EEG) while deaf signers and hearing non-signers imitated one-handed and two-handed ASL signs. During observation, deaf signers showed early differentiation in alpha/beta power between the one- and two-handed sign conditions, whereas hearing non-signers showed this discrimination only later. During sign imitation, deaf signers showed desynchronisation of alpha/beta EEG signals, while hearing non-signers showed increased power. Thus, in an imitative context, deaf signers engage anticipatory motor preparation in advance of action production, while hearing non-signers engage slower, more memory-related processes to help them complete the complex task.",
      "doi": "https://doi.org/10.1080/23273798.2021.1925712",
      "openalex_id": "https://openalex.org/W3160446378",
      "arxiv_id": "",
      "publication_date": "2021-05-12",
      "published": "2021-05-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study assesses EEG sensorimotor processing of ASL signs during imitation and does not involve discrete audio tokenization or speech-token analysis, thus it fails the inclusion criteria for tokenization-based discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study assesses EEG sensorimotor processing of ASL signs during imitation and does not involve discrete audio tokenization or speech-token analysis, thus it fails the inclusion criteria for tokenization-based discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sensorimotor discrimination of ASL signs during imitation using EEG data, without proposing, analyzing, or evaluating discrete audio or speech tokenization methods; it does not address conversion of continuous audio into discrete tokens for audio reconstruction or understanding, thus failing the inclusion criteria and fitting none of the discrete token-related focuses.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sensorimotor discrimination of ASL signs during imitation using EEG data, without proposing, analyzing, or evaluating discrete audio or speech tokenization methods; it does not address conversion of continuous audio into discrete tokens for audio reconstruction or understanding, thus failing the inclusion criteria and fitting none of the discrete token-related focuses.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attitudes toward signing human avatars vary depending on hearing status, age of signed language exposure, and avatar type",
    "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer’s preferred or native language. As development of signing avatars has gained traction in recent years, many different methods of creating signing avatars have been developed, and the resulting avatars vary widely in their appearance, the naturalness of their movements, and their facial expressions--all of which may potentially impact users’ acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars, while also examining the extent to which people’s own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by 1) a live human signer (Human), 2) an avatar made using computer-synthesized animation (CS Avatar), and 3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf (N = 83), Hard-of-Hearing (N = 34), and Hearing (N= 67) groups. Participants rated the three signers on multiple dimensions which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of, and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but this association was not seen for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users’ ratings of signing avatars, and show that signed language users with earlier age of ASL exposure are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars be considerate of retaining the fluid movement qualities which are integral to signed languages.",
    "metadata": {
      "title": "Attitudes toward signing human avatars vary depending on hearing status, age of signed language exposure, and avatar type",
      "summary": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer’s preferred or native language. As development of signing avatars has gained traction in recent years, many different methods of creating signing avatars have been developed, and the resulting avatars vary widely in their appearance, the naturalness of their movements, and their facial expressions--all of which may potentially impact users’ acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars, while also examining the extent to which people’s own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by 1) a live human signer (Human), 2) an avatar made using computer-synthesized animation (CS Avatar), and 3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf (N = 83), Hard-of-Hearing (N = 34), and Hearing (N= 67) groups. Participants rated the three signers on multiple dimensions which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of, and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but this association was not seen for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users’ ratings of signing avatars, and show that signed language users with earlier age of ASL exposure are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars be considerate of retaining the fluid movement qualities which are integral to signed languages.",
      "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer’s preferred or native language. As development of signing avatars has gained traction in recent years, many different methods of creating signing avatars have been developed, and the resulting avatars vary widely in their appearance, the naturalness of their movements, and their facial expressions--all of which may potentially impact users’ acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars, while also examining the extent to which people’s own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by 1) a live human signer (Human), 2) an avatar made using computer-synthesized animation (CS Avatar), and 3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf (N = 83), Hard-of-Hearing (N = 34), and Hearing (N= 67) groups. Participants rated the three signers on multiple dimensions which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of, and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but this association was not seen for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users’ ratings of signing avatars, and show that signed language users with earlier age of ASL exposure are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars be considerate of retaining the fluid movement qualities which are integral to signed languages.",
      "doi": "https://doi.org/10.31234/osf.io/g2wuc",
      "openalex_id": "https://openalex.org/W3174389016",
      "arxiv_id": "",
      "publication_date": "2021-06-25",
      "published": "2021-06-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on signing avatars and user attitudes; it does not analyze discrete audio tokenization or token-based representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on signing avatars and user attitudes; it does not analyze discrete audio tokenization or token-based representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on attitudes toward signing avatars and their movement qualities within signed languages and does not propose, analyze, or evaluate discrete audio/speech tokenization methods. It also has no discussion related to discrete tokenization of audio signals, continuous or discrete audio feature usage, or neural encoding for audio reconstruction or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on attitudes toward signing avatars and their movement qualities within signed languages and does not propose, analyze, or evaluate discrete audio/speech tokenization methods. It also has no discussion related to discrete tokenization of audio signals, continuous or discrete audio feature usage, or neural encoding for audio reconstruction or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Managing Semantic Norms for Cognitive Linguistics, Corpus Linguistics, and Lexicon Studies",
    "abstract": "examples where researchers commonly do not use norms to elucidate some of the problems that may arise for norm-less semantics with respect to the reproducibility of these studies (section 2); followed by an overview of some common norm data sets (section 3); and, finally, a discussion of methodological challenges of norm-based research and how norm-based linguistics fits within contemporary efforts to facilitate reproducible research (section 4). Norm-less semantics: Two examples Corpus linguistics: Semantic prosodyIt is widely known that word meaning depends on context.Take, for example, the verb to cause, which, when seen in isolation appears to be a rather neutral term.However, when using a corpus to look at the contexts this term tends to occur in (e.g., via a concordancer), it becomes apparent that by and large, only bad things get caused (Stubbs 2001), which is exemplified by the concordance lines from the Corpus of Contemporary American English (COCA; Davies 2009) shown in table 42.1.The idea that words consistently occur in certain types of attitudinal or emotional contexts has been dubbed semantic prosody in the British tradition of corpus linguistics (Hunston 2007;Louw 1993;Stewart 2010;Whitsitt 2005), and the fact that language users can so greatly mischaracterize the \"connotation\" of words when they introspect on them in isolation is a major argument for using corpus methods when looking at word meaning.In this field, other headwords that have been studied with respect to semantic prosody include to set in (Sinclair 1991) and utterly (Louw 1993), both of which tend to occur in negative contexts.As another example, consider the plural form days, for which Louw (1993) claims",
    "metadata": {
      "title": "Managing Semantic Norms for Cognitive Linguistics, Corpus Linguistics, and Lexicon Studies",
      "summary": "examples where researchers commonly do not use norms to elucidate some of the problems that may arise for norm-less semantics with respect to the reproducibility of these studies (section 2); followed by an overview of some common norm data sets (section 3); and, finally, a discussion of methodological challenges of norm-based research and how norm-based linguistics fits within contemporary efforts to facilitate reproducible research (section 4). Norm-less semantics: Two examples Corpus linguistics: Semantic prosodyIt is widely known that word meaning depends on context.Take, for example, the verb to cause, which, when seen in isolation appears to be a rather neutral term.However, when using a corpus to look at the contexts this term tends to occur in (e.g., via a concordancer), it becomes apparent that by and large, only bad things get caused (Stubbs 2001), which is exemplified by the concordance lines from the Corpus of Contemporary American English (COCA; Davies 2009) shown in table 42.1.The idea that words consistently occur in certain types of attitudinal or emotional contexts has been dubbed semantic prosody in the British tradition of corpus linguistics (Hunston 2007;Louw 1993;Stewart 2010;Whitsitt 2005), and the fact that language users can so greatly mischaracterize the \"connotation\" of words when they introspect on them in isolation is a major argument for using corpus methods when looking at word meaning.In this field, other headwords that have been studied with respect to semantic prosody include to set in (Sinclair 1991) and utterly (Louw 1993), both of which tend to occur in negative contexts.As another example, consider the plural form days, for which Louw (1993) claims",
      "abstract": "examples where researchers commonly do not use norms to elucidate some of the problems that may arise for norm-less semantics with respect to the reproducibility of these studies (section 2); followed by an overview of some common norm data sets (section 3); and, finally, a discussion of methodological challenges of norm-based research and how norm-based linguistics fits within contemporary efforts to facilitate reproducible research (section 4). Norm-less semantics: Two examples Corpus linguistics: Semantic prosodyIt is widely known that word meaning depends on context.Take, for example, the verb to cause, which, when seen in isolation appears to be a rather neutral term.However, when using a corpus to look at the contexts this term tends to occur in (e.g., via a concordancer), it becomes apparent that by and large, only bad things get caused (Stubbs 2001), which is exemplified by the concordance lines from the Corpus of Contemporary American English (COCA; Davies 2009) shown in table 42.1.The idea that words consistently occur in certain types of attitudinal or emotional contexts has been dubbed semantic prosody in the British tradition of corpus linguistics (Hunston 2007;Louw 1993;Stewart 2010;Whitsitt 2005), and the fact that language users can so greatly mischaracterize the \"connotation\" of words when they introspect on them in isolation is a major argument for using corpus methods when looking at word meaning.In this field, other headwords that have been studied with respect to semantic prosody include to set in (Sinclair 1991) and utterly (Louw 1993), both of which tend to occur in negative contexts.As another example, consider the plural form days, for which Louw (1993) claims",
      "doi": "https://doi.org/10.7551/mitpress/12200.003.0047",
      "openalex_id": "https://openalex.org/W4205719504",
      "arxiv_id": "",
      "publication_date": "2022-01-04",
      "published": "2022-01-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses semantic norms and norm data in cognitive/corpus/lexicon studies and methodological challenges, but it does not present any discrete audio/speech tokenization or evaluation of converting audio signals into discrete tokens; it therefore does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses semantic norms and norm data in cognitive/corpus/lexicon studies and methodological challenges, but it does not present any discrete audio/speech tokenization or evaluation of converting audio signals into discrete tokens; it therefore does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on semantic norms, cognitive linguistics, corpus linguistics, and lexicon studies, discussing issues related to word meaning in context and semantic prosody, without any mention of discrete audio/speech tokenization or analysis of converting continuous audio signals into discrete token sequences. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization and instead pertains purely to linguistic semantics and corpus analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on semantic norms, cognitive linguistics, corpus linguistics, and lexicon studies, discussing issues related to word meaning in context and semantic prosody, without any mention of discrete audio/speech tokenization or analysis of converting continuous audio signals into discrete token sequences. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization and instead pertains purely to linguistic semantics and corpus analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Visual Representation of Abstract Verbs: Merging Verb Classification with Iconicity in Sign Language",
    "abstract": "Theories like the picture superiority effect state that the visual modality has substantial advantage over the other human senses. This makes visual information vital in the acquisition of knowledge, such as in the learning of a language. Words can be graphically represented to illustrate the meaning of a message and facilitate its understanding. This method, however, becomes a limitation in the case of abstract words, like accept, belong, integrate and agree, which have no visual referent. The current research turns to sign languages to explore the common semantic elements that link words to each other. Such visual languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of creating clusters of iconic meanings along with their respective graphic representation. By using sign language insight and VerbNet's organisation of verb predicates, this study presents a novel organisation of 506 English abstract verbs classified by visual shape. Graphic animation was used to visually represent the 20 classes of abstract verbs developed. To build confidence on the resulting product, which can be accessed on www.vroav.online, an online survey was created to achieve judgements on the visuals' representativeness. Considerable agreement between participants was found, suggesting a positive way forward for this work, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.",
    "metadata": {
      "title": "The Visual Representation of Abstract Verbs: Merging Verb Classification with Iconicity in Sign Language",
      "summary": "Theories like the picture superiority effect state that the visual modality has substantial advantage over the other human senses. This makes visual information vital in the acquisition of knowledge, such as in the learning of a language. Words can be graphically represented to illustrate the meaning of a message and facilitate its understanding. This method, however, becomes a limitation in the case of abstract words, like accept, belong, integrate and agree, which have no visual referent. The current research turns to sign languages to explore the common semantic elements that link words to each other. Such visual languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of creating clusters of iconic meanings along with their respective graphic representation. By using sign language insight and VerbNet's organisation of verb predicates, this study presents a novel organisation of 506 English abstract verbs classified by visual shape. Graphic animation was used to visually represent the 20 classes of abstract verbs developed. To build confidence on the resulting product, which can be accessed on www.vroav.online, an online survey was created to achieve judgements on the visuals' representativeness. Considerable agreement between participants was found, suggesting a positive way forward for this work, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.",
      "abstract": "Theories like the picture superiority effect state that the visual modality has substantial advantage over the other human senses. This makes visual information vital in the acquisition of knowledge, such as in the learning of a language. Words can be graphically represented to illustrate the meaning of a message and facilitate its understanding. This method, however, becomes a limitation in the case of abstract words, like accept, belong, integrate and agree, which have no visual referent. The current research turns to sign languages to explore the common semantic elements that link words to each other. Such visual languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of creating clusters of iconic meanings along with their respective graphic representation. By using sign language insight and VerbNet's organisation of verb predicates, this study presents a novel organisation of 506 English abstract verbs classified by visual shape. Graphic animation was used to visually represent the 20 classes of abstract verbs developed. To build confidence on the resulting product, which can be accessed on www.vroav.online, an online survey was created to achieve judgements on the visuals' representativeness. Considerable agreement between participants was found, suggesting a positive way forward for this work, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.",
      "doi": "https://doi.org/10.1109/iccc.2019.00025",
      "openalex_id": "https://openalex.org/W2970711556",
      "arxiv_id": "",
      "publication_date": "2019-07-01",
      "published": "2019-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on visual representation of abstract verbs and sign language, with no analysis or proposal of discrete audio/speech tokenization or token-level representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on visual representation of abstract verbs and sign language, with no analysis or proposal of discrete audio/speech tokenization or token-level representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the visual representation of abstract verbs in sign language and does not address any aspect of audio or speech tokenization, discrete quantization of audio signals, or related audio reconstruction/generation tasks as stated in the inclusion criteria, making it irrelevant to the specified topic of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the visual representation of abstract verbs in sign language and does not address any aspect of audio or speech tokenization, discrete quantization of audio signals, or related audio reconstruction/generation tasks as stated in the inclusion criteria, making it irrelevant to the specified topic of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Infants’ Learning of Speech Sounds and Word-Forms",
    "abstract": "Abstract How do infants start learning their native language? This chapter reviews the conventional understanding of this problem, illustrated by a review of the most important studies in this area, and suggests that this conventional understanding mischaracterizes the problem infants solve and the developmental process by which they solve it. Recent experimental and modeling work from several labs suggest new ways to think about the beginnings of language learning and the emergence of the lexicon.",
    "metadata": {
      "title": "Infants’ Learning of Speech Sounds and Word-Forms",
      "summary": "Abstract How do infants start learning their native language? This chapter reviews the conventional understanding of this problem, illustrated by a review of the most important studies in this area, and suggests that this conventional understanding mischaracterizes the problem infants solve and the developmental process by which they solve it. Recent experimental and modeling work from several labs suggest new ways to think about the beginnings of language learning and the emergence of the lexicon.",
      "abstract": "Abstract How do infants start learning their native language? This chapter reviews the conventional understanding of this problem, illustrated by a review of the most important studies in this area, and suggests that this conventional understanding mischaracterizes the problem infants solve and the developmental process by which they solve it. Recent experimental and modeling work from several labs suggest new ways to think about the beginnings of language learning and the emergence of the lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.6",
      "openalex_id": "https://openalex.org/W4212928203",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a high-level theoretical review of how infants learn speech sounds and word-forms; it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite-symbol vocabulary, so it fails the inclusion criteria for tokenization-focused research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a high-level theoretical review of how infants learn speech sounds and word-forms; it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite-symbol vocabulary, so it fails the inclusion criteria for tokenization-focused research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses infants' learning of speech sounds and word-forms from a developmental and linguistic perspective, without mentioning any study, proposal, or analysis of discrete audio/speech tokenization or discrete quantization methods relevant to audio reconstruction or language modeling, thus failing to meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses infants' learning of speech sounds and word-forms from a developmental and linguistic perspective, without mentioning any study, proposal, or analysis of discrete audio/speech tokenization or discrete quantization methods relevant to audio reconstruction or language modeling, thus failing to meet the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Taboo in Sign Languages",
    "abstract": "Taboo topics in deaf communities include the usual ones found in spoken languages, as well as ones particular to deaf experiences, both in how deaf people relate to hearing people and how deaf people interact with other deaf people. Attention to these topics can help linguists understand better the consequences of field method choices and lead them to adopt better ones. Taboo expressions in American Sign Language are innovative regarding the linguistic structures they play with. This creativity is evident across the grammar in non-taboo expressions, but seems to revel in profane ones. When it comes to the syntax, however, certain types of structures occur in taboo expressions that are all but absent elsewhere, showing grammatical possibilities that might have gone unnoticed without attention to taboo. Taboo expressions are innovative, as well, in how they respond to changing culture, where lexical items that are incoherent with community sensibilities are 'corrected'.",
    "metadata": {
      "title": "Taboo in Sign Languages",
      "summary": "Taboo topics in deaf communities include the usual ones found in spoken languages, as well as ones particular to deaf experiences, both in how deaf people relate to hearing people and how deaf people interact with other deaf people. Attention to these topics can help linguists understand better the consequences of field method choices and lead them to adopt better ones. Taboo expressions in American Sign Language are innovative regarding the linguistic structures they play with. This creativity is evident across the grammar in non-taboo expressions, but seems to revel in profane ones. When it comes to the syntax, however, certain types of structures occur in taboo expressions that are all but absent elsewhere, showing grammatical possibilities that might have gone unnoticed without attention to taboo. Taboo expressions are innovative, as well, in how they respond to changing culture, where lexical items that are incoherent with community sensibilities are 'corrected'.",
      "abstract": "Taboo topics in deaf communities include the usual ones found in spoken languages, as well as ones particular to deaf experiences, both in how deaf people relate to hearing people and how deaf people interact with other deaf people. Attention to these topics can help linguists understand better the consequences of field method choices and lead them to adopt better ones. Taboo expressions in American Sign Language are innovative regarding the linguistic structures they play with. This creativity is evident across the grammar in non-taboo expressions, but seems to revel in profane ones. When it comes to the syntax, however, certain types of structures occur in taboo expressions that are all but absent elsewhere, showing grammatical possibilities that might have gone unnoticed without attention to taboo. Taboo expressions are innovative, as well, in how they respond to changing culture, where lexical items that are incoherent with community sensibilities are 'corrected'.",
      "doi": "https://doi.org/10.1017/9781009291972",
      "openalex_id": "https://openalex.org/W4387328060",
      "arxiv_id": "",
      "publication_date": "2023-10-05",
      "published": "2023-10-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns taboo expressions in sign language and linguistic analysis, but contains no discussion of discrete audio tokenization or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns taboo expressions in sign language and linguistic analysis, but contains no discussion of discrete audio tokenization or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on taboo topics in sign languages and linguistic structures but does not discuss discrete audio/speech tokenization or any related audio signal processing tasks, which are required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on taboo topics in sign languages and linguistic structures but does not discuss discrete audio/speech tokenization or any related audio signal processing tasks, which are required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Repetition Preferences in Two-Handed Balanced Signs: Vestigial Locomotor Central Pattern Generators Shape Sign Language Phonetics and Phonology",
    "abstract": "Language is produced by bodies that evolved to fulfill a variety of functions, most of them non-communicative. Vestigial influences of adaptation for quadrupedal locomotion are still affecting bimanual actions, and have consequences on manual communication systems such as sign languages of the deaf. We discuss how central pattern generators (CPGs), networks of nerve cells in the spinal cord that drive locomotion, influence bimanual actions with alternating movements to be produced with repeated motion. We demonstrate this influence with data from three unrelated sign languages, American Sign Language, British Sign Language, and Hong Kong Sign Language: in all three sign languages two-handed balanced signs produced with alternating movements have a tendency to be repeated, whereas other types of two-handed balanced signs show the opposite tendency for single movements. These tendencies cannot be fully explained by factors such as iconicity. We propose a motoric account for these results: as alternating bimanual movements are influenced by locomotor patterns, they favor repeated movements.",
    "metadata": {
      "title": "Repetition Preferences in Two-Handed Balanced Signs: Vestigial Locomotor Central Pattern Generators Shape Sign Language Phonetics and Phonology",
      "summary": "Language is produced by bodies that evolved to fulfill a variety of functions, most of them non-communicative. Vestigial influences of adaptation for quadrupedal locomotion are still affecting bimanual actions, and have consequences on manual communication systems such as sign languages of the deaf. We discuss how central pattern generators (CPGs), networks of nerve cells in the spinal cord that drive locomotion, influence bimanual actions with alternating movements to be produced with repeated motion. We demonstrate this influence with data from three unrelated sign languages, American Sign Language, British Sign Language, and Hong Kong Sign Language: in all three sign languages two-handed balanced signs produced with alternating movements have a tendency to be repeated, whereas other types of two-handed balanced signs show the opposite tendency for single movements. These tendencies cannot be fully explained by factors such as iconicity. We propose a motoric account for these results: as alternating bimanual movements are influenced by locomotor patterns, they favor repeated movements.",
      "abstract": "Language is produced by bodies that evolved to fulfill a variety of functions, most of them non-communicative. Vestigial influences of adaptation for quadrupedal locomotion are still affecting bimanual actions, and have consequences on manual communication systems such as sign languages of the deaf. We discuss how central pattern generators (CPGs), networks of nerve cells in the spinal cord that drive locomotion, influence bimanual actions with alternating movements to be produced with repeated motion. We demonstrate this influence with data from three unrelated sign languages, American Sign Language, British Sign Language, and Hong Kong Sign Language: in all three sign languages two-handed balanced signs produced with alternating movements have a tendency to be repeated, whereas other types of two-handed balanced signs show the opposite tendency for single movements. These tendencies cannot be fully explained by factors such as iconicity. We propose a motoric account for these results: as alternating bimanual movements are influenced by locomotor patterns, they favor repeated movements.",
      "doi": "https://doi.org/10.3389/fcomm.2020.612973",
      "openalex_id": "https://openalex.org/W3128753683",
      "arxiv_id": "",
      "publication_date": "2021-01-28",
      "published": "2021-01-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes sign language phonetics/phonology and motor patterns without proposing or analyzing discrete audio/speech tokenization, so it does not meet the core inclusion criterion for token-based discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes sign language phonetics/phonology and motor patterns without proposing or analyzing discrete audio/speech tokenization, so it does not meet the core inclusion criterion for token-based discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the influence of central pattern generators on two-handed signs in sign languages, analyzing manual repeated movements and their motoric basis; it does not address discrete audio or speech tokenization, nor does it involve conversion of continuous audio signals into discrete tokens, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the influence of central pattern generators on two-handed signs in sign languages, analyzing manual repeated movements and their motoric basis; it does not address discrete audio or speech tokenization, nor does it involve conversion of continuous audio signals into discrete tokens, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The distribution of handshapes in the established lexicon of Israeli Sign Language (ISL)",
    "abstract": "Abstract Our study focuses on the perception of the iconicity of handshapes – one of the formational parameters of the sign in signed language. Seventy Hebrew speakers were asked to match handshapes to Hebrew translations of 45 signs (that varied in degree of iconicity), which are specified for one of the handshapes in Israeli Sign Language (ISL). The results show that participants reliably match handshapes to corresponding sign translations for highly iconic signs, but are less accurate for less iconic signs. This demonstrates that there is a notable degree of iconicity in the lexicon of ISL, which is recognizable even to non-signers. The ability of non-signers to detect handshape to form is explained by the fact that word meanings are understood by both deaf and hearing peoples via the mental elaboration of simple iconic sources in which handshape meanings are grounded. The results suggest that while language external iconic mapping could ease the learning of direct iconic forms, it has a more limited capacity to help hearing non-signers learn indirect and opaque forms. The full semiotic distribution of handshapes in the lexicon and their use in language remain difficult for hearing non-signers to understand and depends on more specific language and cultural knowledge.",
    "metadata": {
      "title": "The distribution of handshapes in the established lexicon of Israeli Sign Language (ISL)",
      "summary": "Abstract Our study focuses on the perception of the iconicity of handshapes – one of the formational parameters of the sign in signed language. Seventy Hebrew speakers were asked to match handshapes to Hebrew translations of 45 signs (that varied in degree of iconicity), which are specified for one of the handshapes in Israeli Sign Language (ISL). The results show that participants reliably match handshapes to corresponding sign translations for highly iconic signs, but are less accurate for less iconic signs. This demonstrates that there is a notable degree of iconicity in the lexicon of ISL, which is recognizable even to non-signers. The ability of non-signers to detect handshape to form is explained by the fact that word meanings are understood by both deaf and hearing peoples via the mental elaboration of simple iconic sources in which handshape meanings are grounded. The results suggest that while language external iconic mapping could ease the learning of direct iconic forms, it has a more limited capacity to help hearing non-signers learn indirect and opaque forms. The full semiotic distribution of handshapes in the lexicon and their use in language remain difficult for hearing non-signers to understand and depends on more specific language and cultural knowledge.",
      "abstract": "Abstract Our study focuses on the perception of the iconicity of handshapes – one of the formational parameters of the sign in signed language. Seventy Hebrew speakers were asked to match handshapes to Hebrew translations of 45 signs (that varied in degree of iconicity), which are specified for one of the handshapes in Israeli Sign Language (ISL). The results show that participants reliably match handshapes to corresponding sign translations for highly iconic signs, but are less accurate for less iconic signs. This demonstrates that there is a notable degree of iconicity in the lexicon of ISL, which is recognizable even to non-signers. The ability of non-signers to detect handshape to form is explained by the fact that word meanings are understood by both deaf and hearing peoples via the mental elaboration of simple iconic sources in which handshape meanings are grounded. The results suggest that while language external iconic mapping could ease the learning of direct iconic forms, it has a more limited capacity to help hearing non-signers learn indirect and opaque forms. The full semiotic distribution of handshapes in the lexicon and their use in language remain difficult for hearing non-signers to understand and depends on more specific language and cultural knowledge.",
      "doi": "https://doi.org/10.1515/sem-2019-0049",
      "openalex_id": "https://openalex.org/W3190329100",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates handshape iconicity in Israeli Sign Language and does not address discrete audio/tokenization of speech, so it does not meet the required inclusion criteria for discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates handshape iconicity in Israeli Sign Language and does not address discrete audio/tokenization of speech, so it does not meet the required inclusion criteria for discrete audio tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the perception of handshapes in sign language, which relates to visual/manual communication rather than audio or speech tokenization; it does not propose, analyze, or evaluate discrete audio/speech tokenization from continuous audio signals into discrete tokens, and therefore does not meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the perception of handshapes in sign language, which relates to visual/manual communication rather than audio or speech tokenization; it does not propose, analyze, or evaluate discrete audio/speech tokenization from continuous audio signals into discrete tokens, and therefore does not meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Contributions of pragmatics to word learning and interpretation",
    "abstract": "Abstract This chapter focuses on the role of pragmatic reasoning in language learning and interpretation. The first half of the chapter assesses the extent to which young children use pragmatic mechanisms of intention recognition to build a mental lexicon (i.e., to learn new words). The second half discusses the extent to which children use pragmatic inference to employ their mental lexicon in conversation (i.e., to interpret known words). The evidence reviewed points to rich and massive effects of pragmatic reasoning in both domains. The sophistication of children’s pragmatic system and its relation to the mature adult system are discussed throughout the chapter.",
    "metadata": {
      "title": "Contributions of pragmatics to word learning and interpretation",
      "summary": "Abstract This chapter focuses on the role of pragmatic reasoning in language learning and interpretation. The first half of the chapter assesses the extent to which young children use pragmatic mechanisms of intention recognition to build a mental lexicon (i.e., to learn new words). The second half discusses the extent to which children use pragmatic inference to employ their mental lexicon in conversation (i.e., to interpret known words). The evidence reviewed points to rich and massive effects of pragmatic reasoning in both domains. The sophistication of children’s pragmatic system and its relation to the mature adult system are discussed throughout the chapter.",
      "abstract": "Abstract This chapter focuses on the role of pragmatic reasoning in language learning and interpretation. The first half of the chapter assesses the extent to which young children use pragmatic mechanisms of intention recognition to build a mental lexicon (i.e., to learn new words). The second half discusses the extent to which children use pragmatic inference to employ their mental lexicon in conversation (i.e., to interpret known words). The evidence reviewed points to rich and massive effects of pragmatic reasoning in both domains. The sophistication of children’s pragmatic system and its relation to the mature adult system are discussed throughout the chapter.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.21",
      "openalex_id": "https://openalex.org/W4212770460",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses pragmatic reasoning in language learning and interpretation without any focus on discrete audio/speech tokenization or token-based representations, so it does not meet the stated inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses pragmatic reasoning in language learning and interpretation without any focus on discrete audio/speech tokenization or token-based representations, so it does not meet the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the role of pragmatics in language learning and interpretation in children and do not mention or analyze discrete audio/speech tokenization or discrete token representations of continuous audio signals, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the role of pragmatics in language learning and interpretation in children and do not mention or analyze discrete audio/speech tokenization or discrete token representations of continuous audio signals, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical Properties to Aid in Generation of Technical Gestures",
    "abstract": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
    "metadata": {
      "title": "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical Properties to Aid in Generation of Technical Gestures",
      "summary": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
      "abstract": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
      "doi": "https://doi.org/10.1145/3555776.3577623",
      "openalex_id": "https://openalex.org/W4379620202",
      "arxiv_id": "",
      "publication_date": "2023-03-27",
      "published": "2023-03-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on ASL gesture generation and iconicity based on lexical/gestural properties, not on discrete audio tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on ASL gesture generation and iconicity based on lexical/gestural properties, not on discrete audio tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on assigning iconicity ratings to technical gestures in American Sign Language based on lexical properties, without any mention or analysis of discrete audio or speech tokenization where continuous audio signals are converted to discrete symbols; thus, it does not meet the inclusion criteria related to audio tokenization and primarily deals with gesture analysis rather than audio signal processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on assigning iconicity ratings to technical gestures in American Sign Language based on lexical properties, without any mention or analysis of discrete audio or speech tokenization where continuous audio signals are converted to discrete symbols; thus, it does not meet the inclusion criteria related to audio tokenization and primarily deals with gesture analysis rather than audio signal processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Winter, Woodin &amp;amp; Perlman - Defining iconicity for the cognitive sciences",
    "abstract": "Iconicity has become an increasingly hot topic in the cognitive sciences, but research is riddled with inconsistencies in the use of key terminology, including “iconicity” itself as well as other related terms. This chapter presents a precise definition of iconicity that reflects the wide range of research on the topic in cognitive science and linguistics, but also provides clarity moving into the future. We also clarify common terminological confusions with respect to related notions including sound symbolism, systematicity, and indexicality. Our discussion puts the spotlight on aspects of iconicity that are under-appreciated in cognitive science, especially its context-dependent nature, and the fact that iconicity is underpinned by multiple rather than monolithic cognitive mechanisms.",
    "metadata": {
      "title": "Winter, Woodin &amp;amp; Perlman - Defining iconicity for the cognitive sciences",
      "summary": "Iconicity has become an increasingly hot topic in the cognitive sciences, but research is riddled with inconsistencies in the use of key terminology, including “iconicity” itself as well as other related terms. This chapter presents a precise definition of iconicity that reflects the wide range of research on the topic in cognitive science and linguistics, but also provides clarity moving into the future. We also clarify common terminological confusions with respect to related notions including sound symbolism, systematicity, and indexicality. Our discussion puts the spotlight on aspects of iconicity that are under-appreciated in cognitive science, especially its context-dependent nature, and the fact that iconicity is underpinned by multiple rather than monolithic cognitive mechanisms.",
      "abstract": "Iconicity has become an increasingly hot topic in the cognitive sciences, but research is riddled with inconsistencies in the use of key terminology, including “iconicity” itself as well as other related terms. This chapter presents a precise definition of iconicity that reflects the wide range of research on the topic in cognitive science and linguistics, but also provides clarity moving into the future. We also clarify common terminological confusions with respect to related notions including sound symbolism, systematicity, and indexicality. Our discussion puts the spotlight on aspects of iconicity that are under-appreciated in cognitive science, especially its context-dependent nature, and the fact that iconicity is underpinned by multiple rather than monolithic cognitive mechanisms.",
      "doi": "https://doi.org/10.31219/osf.io/5e3rc",
      "openalex_id": "https://openalex.org/W4388931422",
      "arxiv_id": "",
      "publication_date": "2023-11-22",
      "published": "2023-11-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on defining iconicity and clarifying related terms in cognitive science, with no discussion of discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on defining iconicity and clarifying related terms in cognitive science, with no discussion of discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on defining and clarifying the notion of iconicity in cognitive sciences, without any indication that it addresses discrete audio or speech tokenization, quantization, or neural encoding models for tokenization; thus, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on defining and clarifying the notion of iconicity in cognitive sciences, without any indication that it addresses discrete audio or speech tokenization, quantization, or neural encoding models for tokenization; thus, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Classification of Handshapes in Russian Sign Language",
    "abstract": "Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as an inventory is necessary for generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, creating an inventory is a very time-consuming process, thus only a handful of sign languages have them. Therefore, in this work we firstly test an unsupervised approach with the aim to automatically generate a handshape inventory. The process includes hand detection, cropping, and clustering techniques, which we apply to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verify the data to be able to apply supervised learning to classify new data.",
    "metadata": {
      "title": "Automatic Classification of Handshapes in Russian Sign Language",
      "summary": "Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as an inventory is necessary for generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, creating an inventory is a very time-consuming process, thus only a handful of sign languages have them. Therefore, in this work we firstly test an unsupervised approach with the aim to automatically generate a handshape inventory. The process includes hand detection, cropping, and clustering techniques, which we apply to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verify the data to be able to apply supervised learning to classify new data.",
      "abstract": "Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as an inventory is necessary for generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, creating an inventory is a very time-consuming process, thus only a handful of sign languages have them. Therefore, in this work we firstly test an unsupervised approach with the aim to automatically generate a handshape inventory. The process includes hand detection, cropping, and clustering techniques, which we apply to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verify the data to be able to apply supervised learning to classify new data.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3088439434",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses visual handshape analysis in sign language and inventory construction, not discrete audio/tokenized representations of speech, thus it does not meet the inclusion criteria that require discrete audio tokenization for speech-related tasks; it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses visual handshape analysis in sign language and inventory construction, not discrete audio/tokenized representations of speech, thus it does not meet the inclusion criteria that require discrete audio tokenization for speech-related tasks; it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic classification of handshapes in Russian Sign Language, which is related to visual sign language analysis rather than audio or speech tokenization; it does not address discrete audio tokenization or quantization-based audio token representations, which are the key inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic classification of handshapes in Russian Sign Language, which is related to visual sign language analysis rather than audio or speech tokenization; it does not address discrete audio tokenization or quantization-based audio token representations, which are the key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "What the frequency list can teach us about Turkish sign language?",
    "abstract": "Abstract Recent studies on linguistics, cognitive science and psychology have shown that describing lexical frequency characteristics can answer many critical questions on language acquisition, mental lexicon and language use. Given the importance of corpus-based methodology, this study reports the preliminary findings from the objective lexical frequency list in TİD based on 103.087 sign tokens. This paper shows that frequency occurrence has a very decisive role on the linguistics categories and language in use. With respect to the multi-functionality of pointing in signed languages, the top ranked ID-gloss occurrences are mostly shaped by the pronominal references. Moreover, when compared to previous studies in terms of lexical density and lexical diversity, TİD shares both similar and different statistical features observed in other signed languages.",
    "metadata": {
      "title": "What the frequency list can teach us about Turkish sign language?",
      "summary": "Abstract Recent studies on linguistics, cognitive science and psychology have shown that describing lexical frequency characteristics can answer many critical questions on language acquisition, mental lexicon and language use. Given the importance of corpus-based methodology, this study reports the preliminary findings from the objective lexical frequency list in TİD based on 103.087 sign tokens. This paper shows that frequency occurrence has a very decisive role on the linguistics categories and language in use. With respect to the multi-functionality of pointing in signed languages, the top ranked ID-gloss occurrences are mostly shaped by the pronominal references. Moreover, when compared to previous studies in terms of lexical density and lexical diversity, TİD shares both similar and different statistical features observed in other signed languages.",
      "abstract": "Abstract Recent studies on linguistics, cognitive science and psychology have shown that describing lexical frequency characteristics can answer many critical questions on language acquisition, mental lexicon and language use. Given the importance of corpus-based methodology, this study reports the preliminary findings from the objective lexical frequency list in TİD based on 103.087 sign tokens. This paper shows that frequency occurrence has a very decisive role on the linguistics categories and language in use. With respect to the multi-functionality of pointing in signed languages, the top ranked ID-gloss occurrences are mostly shaped by the pronominal references. Moreover, when compared to previous studies in terms of lexical density and lexical diversity, TİD shares both similar and different statistical features observed in other signed languages.",
      "doi": "https://doi.org/10.1515/psicl-2021-0022",
      "openalex_id": "https://openalex.org/W4200357742",
      "arxiv_id": "",
      "publication_date": "2021-12-01",
      "published": "2021-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes lexical frequency of sign tokens in Turkish Sign Language but does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., converting a continuous audio signal into discrete symbols for speech tasks); its tokens are sign glosses, not audio-derived discrete tokens, so it does not meet the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes lexical frequency of sign tokens in Turkish Sign Language but does not propose, analyze, or evaluate discrete audio/speech tokenization (i.e., converting a continuous audio signal into discrete symbols for speech tasks); its tokens are sign glosses, not audio-derived discrete tokens, so it does not meet the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical frequency analysis of Turkish Sign Language tokens based on sign data, and does not address discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete symbols; thus, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical frequency analysis of Turkish Sign Language tokens based on sign data, and does not address discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete symbols; thus, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pragmatics and the Lexicon",
    "abstract": "Abstract The present chapter reviews how contextually driven inferences interact with the lexical encoding of meaning. A central question is whether (and to what extent) a given ingredient of meaning associated with the use of a particular expression in context should be seen as directly encoded in the lexicon, or whether (and to what extent) it is derived via general reasoning in context. The discussion focuses on three key phenomena that serve as case studies to illustrate the task of spelling out the division of labor between semantics and pragmatics: scalar implicatures, scalar adjectives, and presuppositions. While many details in the theoretical analysis of these phenomena are still up for debate, they all serve to illustrate that key choice points for proposals at the semantics-pragmatics interface concern the question of what type of information has to be included at the lexical level in order to ground pragmatic phenomena in semantics.",
    "metadata": {
      "title": "Pragmatics and the Lexicon",
      "summary": "Abstract The present chapter reviews how contextually driven inferences interact with the lexical encoding of meaning. A central question is whether (and to what extent) a given ingredient of meaning associated with the use of a particular expression in context should be seen as directly encoded in the lexicon, or whether (and to what extent) it is derived via general reasoning in context. The discussion focuses on three key phenomena that serve as case studies to illustrate the task of spelling out the division of labor between semantics and pragmatics: scalar implicatures, scalar adjectives, and presuppositions. While many details in the theoretical analysis of these phenomena are still up for debate, they all serve to illustrate that key choice points for proposals at the semantics-pragmatics interface concern the question of what type of information has to be included at the lexical level in order to ground pragmatic phenomena in semantics.",
      "abstract": "Abstract The present chapter reviews how contextually driven inferences interact with the lexical encoding of meaning. A central question is whether (and to what extent) a given ingredient of meaning associated with the use of a particular expression in context should be seen as directly encoded in the lexicon, or whether (and to what extent) it is derived via general reasoning in context. The discussion focuses on three key phenomena that serve as case studies to illustrate the task of spelling out the division of labor between semantics and pragmatics: scalar implicatures, scalar adjectives, and presuppositions. While many details in the theoretical analysis of these phenomena are still up for debate, they all serve to illustrate that key choice points for proposals at the semantics-pragmatics interface concern the question of what type of information has to be included at the lexical level in order to ground pragmatic phenomena in semantics.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.18",
      "openalex_id": "https://openalex.org/W4213186527",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This article analyzes semantics/pragmatics and lexical meaning without any discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This article analyzes semantics/pragmatics and lexical meaning without any discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on linguistic theory regarding the interaction between pragmatics and the lexicon, specifically analyzing meaning encoding and context-driven inferences; it does not address discrete audio or speech tokenization, nor any relevant methodology for audio signal processing or token quantization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on linguistic theory regarding the interaction between pragmatics and the lexicon, specifically analyzing meaning encoding and context-driven inferences; it does not address discrete audio or speech tokenization, nor any relevant methodology for audio signal processing or token quantization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Development of a Technology-Assisted Assessment for Sign Language Learning",
    "abstract": "The goal of a recently concluded project in Switzerland was to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Gebärdensprache, DSGS) that relies on automatic sign language recognition. The assessment system gives adult L2 learners of DSGS feedback on the correctness of the manual parameters of signing (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system includes automatic feedback for a subset of a DSGS vocabulary size production test consisting of approximately 100 lexical items at CEFR level A1. The paper at hand reports on the process of selecting the items for the test, compiling training data for the SLR system, and linguistically analyzing errors in the resulting video recordings.",
    "metadata": {
      "title": "Development of a Technology-Assisted Assessment for Sign Language Learning",
      "summary": "The goal of a recently concluded project in Switzerland was to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Gebärdensprache, DSGS) that relies on automatic sign language recognition. The assessment system gives adult L2 learners of DSGS feedback on the correctness of the manual parameters of signing (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system includes automatic feedback for a subset of a DSGS vocabulary size production test consisting of approximately 100 lexical items at CEFR level A1. The paper at hand reports on the process of selecting the items for the test, compiling training data for the SLR system, and linguistically analyzing errors in the resulting video recordings.",
      "abstract": "The goal of a recently concluded project in Switzerland was to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Gebärdensprache, DSGS) that relies on automatic sign language recognition. The assessment system gives adult L2 learners of DSGS feedback on the correctness of the manual parameters of signing (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system includes automatic feedback for a subset of a DSGS vocabulary size production test consisting of approximately 100 lexical items at CEFR level A1. The paper at hand reports on the process of selecting the items for the test, compiling training data for the SLR system, and linguistically analyzing errors in the resulting video recordings.",
      "doi": "https://doi.org/10.3991/ijet.v17i06.26959",
      "openalex_id": "https://openalex.org/W4221060801",
      "arxiv_id": "",
      "publication_date": "2022-03-29",
      "published": "2022-03-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on sign-language recognition from video and feedback for manual parameters, with no discrete audio tokenization or speech tokens analyzed, so it does not meet the audio/tokenization inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on sign-language recognition from video and feedback for manual parameters, with no discrete audio tokenization or speech tokens analyzed, so it does not meet the audio/tokenization inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a sign language assessment system using automatic sign language recognition, addressing visual/manual parameters of signs rather than audio or speech tokenization; it does not discuss discrete audio/speech tokenization or related quantization-based representations as required, hence it does not meet the inclusion criteria nor violate exclusion criteria related to audio tokenization specifically.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a sign language assessment system using automatic sign language recognition, addressing visual/manual parameters of signs rather than audio or speech tokenization; it does not discuss discrete audio/speech tokenization or related quantization-based representations as required, hence it does not meet the inclusion criteria nor violate exclusion criteria related to audio tokenization specifically.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Corrective Feedback to Second Language Learners of American Sign Language",
    "abstract": "This study examined the corrective feedback Deaf teachers used to target handshape, movement, and place-of-articulation errors in introductory American Sign Language (ASL) classes for hearing students. Although feedback is underresearched in bimodal second language (M2-L2) pedagogy, there is some evidence that teacher practices may differ from those observed in spoken L2 classes, notably in the more frequent use of direct corrections. Willoughby et al.'s (2015) study of Auslan (Australian Sign Language) reports that the teachers' preference for this type of correction stemmed from beliefs about the challenges of learning signed language phonology. Spoken L2 research suggests that a reliance on this type of feedback may limit students' opportunities to learn from their errors, as the nontarget form is corrected for the student and is not often followed by student \"repair\" of the original error (Panova and Lyster 2002). As student response to teacher feedback was not examined in Willoughby et al.'s (2015) study, we do not know if M2-L2 students show similar behavior. The current study was designed to address this issue, examining both teacher feedback practices and student responses to feedback.",
    "metadata": {
      "title": "Corrective Feedback to Second Language Learners of American Sign Language",
      "summary": "This study examined the corrective feedback Deaf teachers used to target handshape, movement, and place-of-articulation errors in introductory American Sign Language (ASL) classes for hearing students. Although feedback is underresearched in bimodal second language (M2-L2) pedagogy, there is some evidence that teacher practices may differ from those observed in spoken L2 classes, notably in the more frequent use of direct corrections. Willoughby et al.'s (2015) study of Auslan (Australian Sign Language) reports that the teachers' preference for this type of correction stemmed from beliefs about the challenges of learning signed language phonology. Spoken L2 research suggests that a reliance on this type of feedback may limit students' opportunities to learn from their errors, as the nontarget form is corrected for the student and is not often followed by student \"repair\" of the original error (Panova and Lyster 2002). As student response to teacher feedback was not examined in Willoughby et al.'s (2015) study, we do not know if M2-L2 students show similar behavior. The current study was designed to address this issue, examining both teacher feedback practices and student responses to feedback.",
      "abstract": "This study examined the corrective feedback Deaf teachers used to target handshape, movement, and place-of-articulation errors in introductory American Sign Language (ASL) classes for hearing students. Although feedback is underresearched in bimodal second language (M2-L2) pedagogy, there is some evidence that teacher practices may differ from those observed in spoken L2 classes, notably in the more frequent use of direct corrections. Willoughby et al.'s (2015) study of Auslan (Australian Sign Language) reports that the teachers' preference for this type of correction stemmed from beliefs about the challenges of learning signed language phonology. Spoken L2 research suggests that a reliance on this type of feedback may limit students' opportunities to learn from their errors, as the nontarget form is corrected for the student and is not often followed by student \"repair\" of the original error (Panova and Lyster 2002). As student response to teacher feedback was not examined in Willoughby et al.'s (2015) study, we do not know if M2-L2 students show similar behavior. The current study was designed to address this issue, examining both teacher feedback practices and student responses to feedback.",
      "doi": "https://doi.org/10.1353/sls.2022.0009",
      "openalex_id": "https://openalex.org/W3116954592",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on corrective feedback in ASL pedagogy and student responses, with no discrete audio tokenization or speech-token analysis, thus it does not meet the inclusion criteria for discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on corrective feedback in ASL pedagogy and student responses, with no discrete audio tokenization or speech-token analysis, thus it does not meet the inclusion criteria for discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on corrective feedback in American Sign Language learning, examining teaching practices and student responses, with no mention of discrete audio/speech tokenization or analysis of discrete audio tokens; thus, it does not meet the inclusion criteria related to tokenization of continuous audio into discrete symbols and is outside the scope of audio signal processing or token design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on corrective feedback in American Sign Language learning, examining teaching practices and student responses, with no mention of discrete audio/speech tokenization or analysis of discrete audio tokens; thus, it does not meet the inclusion criteria related to tokenization of continuous audio into discrete symbols and is outside the scope of audio signal processing or token design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Overlapping Semantic Representations of Sign and Speech in Novice Sign Language Learners",
    "abstract": "The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.",
    "metadata": {
      "title": "Overlapping Semantic Representations of Sign and Speech in Novice Sign Language Learners",
      "summary": "The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.",
      "abstract": "The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.",
      "doi": "https://doi.org/10.31234/osf.io/2tjsg",
      "openalex_id": "https://openalex.org/W4281558366",
      "arxiv_id": "",
      "publication_date": "2022-05-25",
      "published": "2022-05-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not engage discrete audio tokenization or analysis of discrete speech tokens despite cross-modal language focus, so it fails inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not engage discrete audio tokenization or analysis of discrete speech tokens despite cross-modal language focus, so it fails inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates semantic representations in neural activity related to sign and speech language learning but does not mention proposing, analyzing, or evaluating discrete audio or speech tokenization methods. It appears focused on neural patterns and language processing rather than discrete audio tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates semantic representations in neural activity related to sign and speech language learning but does not mention proposing, analyzing, or evaluating discrete audio or speech tokenization methods. It appears focused on neural patterns and language processing rather than discrete audio tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evidence of Zipfian distributions in three sign languages",
    "abstract": "One striking commonality between languages is their Zipfian distributions: A power-law distribution of word frequency. This distribution is found across languages, speech genres, and within different parts of speech. The recurrence of such distributions is thought to reflect cognitive and/or communicative pressures and to facilitate language learning. However, research on Zipfian distributions has mostly been limited to spoken languages. In this study, we ask whether Zipfian distributions are also found across signed languages, as expected if they reflect a universal property of human language. We find that sign frequencies and ranks in three sign language corpora (BSL, DGS and NGT) show a Zipfian relationship, similar to that found in spoken languages. These findings highlight the commonalities between spoken and signed languages, add to our understanding of the use of signs, and show the prevalence of Zipfian distributions across language modalities, supporting the idea that they facilitate language learning and communication.",
    "metadata": {
      "title": "Evidence of Zipfian distributions in three sign languages",
      "summary": "One striking commonality between languages is their Zipfian distributions: A power-law distribution of word frequency. This distribution is found across languages, speech genres, and within different parts of speech. The recurrence of such distributions is thought to reflect cognitive and/or communicative pressures and to facilitate language learning. However, research on Zipfian distributions has mostly been limited to spoken languages. In this study, we ask whether Zipfian distributions are also found across signed languages, as expected if they reflect a universal property of human language. We find that sign frequencies and ranks in three sign language corpora (BSL, DGS and NGT) show a Zipfian relationship, similar to that found in spoken languages. These findings highlight the commonalities between spoken and signed languages, add to our understanding of the use of signs, and show the prevalence of Zipfian distributions across language modalities, supporting the idea that they facilitate language learning and communication.",
      "abstract": "One striking commonality between languages is their Zipfian distributions: A power-law distribution of word frequency. This distribution is found across languages, speech genres, and within different parts of speech. The recurrence of such distributions is thought to reflect cognitive and/or communicative pressures and to facilitate language learning. However, research on Zipfian distributions has mostly been limited to spoken languages. In this study, we ask whether Zipfian distributions are also found across signed languages, as expected if they reflect a universal property of human language. We find that sign frequencies and ranks in three sign language corpora (BSL, DGS and NGT) show a Zipfian relationship, similar to that found in spoken languages. These findings highlight the commonalities between spoken and signed languages, add to our understanding of the use of signs, and show the prevalence of Zipfian distributions across language modalities, supporting the idea that they facilitate language learning and communication.",
      "doi": "https://doi.org/10.1075/gest.23014.kim",
      "openalex_id": "https://openalex.org/W4393435033",
      "arxiv_id": "",
      "publication_date": "2023-12-31",
      "published": "2023-12-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study is about Zipfian frequency distributions in sign languages and does not address discrete audio/speech tokenization (converting continuous audio into discrete tokens) as a core object of study, so it fails the inclusion criteria and does not fit the tokenization-focused exclusions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study is about Zipfian frequency distributions in sign languages and does not address discrete audio/speech tokenization (converting continuous audio into discrete tokens) as a core object of study, so it fails the inclusion criteria and does not fit the tokenization-focused exclusions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing Zipfian distributions in sign languages, which is linguistic and not related to discrete audio/speech tokenization or the conversion of continuous audio signals to discrete tokens; therefore, it does not meet the core inclusion criteria related to audio tokenization methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing Zipfian distributions in sign languages, which is linguistic and not related to discrete audio/speech tokenization or the conversion of continuous audio signals to discrete tokens; therefore, it does not meet the core inclusion criteria related to audio tokenization methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Defining Nodes and Edges in Other Languages in Cognitive Network Science—Moving beyond Single-Layer Networks",
    "abstract": "Cognitive network science has increased our understanding of how the mental lexicon is structured and how that structure at the micro-, meso-, and macro-levels influences language and cognitive processes. Most of the research using this approach has used single-layer networks of English words. We consider two fundamental concepts in network science—nodes and connections (or edges)—in the context of two lesser-studied languages (American Sign Language and Kaqchikel) to see if a single-layer network can model phonological similarities among words in each of those languages. The analyses of those single-layer networks revealed several differences in network architecture that may challenge the cognitive network approach. We discuss several directions for future research using different network architectures that could address these challenges and also increase our understanding of how language processing might vary across languages. Such work would also provide a common framework for research in the language sciences, despite the variation among human languages. The methodological and theoretical tools of network science may also make it easier to integrate research of various language processes, such as typical and delayed development, acquired disorders, and the interaction of phonological and semantic information. Finally, coupling the cognitive network science approach with investigations of languages other than English might further advance our understanding of cognitive processing in general.",
    "metadata": {
      "title": "Defining Nodes and Edges in Other Languages in Cognitive Network Science—Moving beyond Single-Layer Networks",
      "summary": "Cognitive network science has increased our understanding of how the mental lexicon is structured and how that structure at the micro-, meso-, and macro-levels influences language and cognitive processes. Most of the research using this approach has used single-layer networks of English words. We consider two fundamental concepts in network science—nodes and connections (or edges)—in the context of two lesser-studied languages (American Sign Language and Kaqchikel) to see if a single-layer network can model phonological similarities among words in each of those languages. The analyses of those single-layer networks revealed several differences in network architecture that may challenge the cognitive network approach. We discuss several directions for future research using different network architectures that could address these challenges and also increase our understanding of how language processing might vary across languages. Such work would also provide a common framework for research in the language sciences, despite the variation among human languages. The methodological and theoretical tools of network science may also make it easier to integrate research of various language processes, such as typical and delayed development, acquired disorders, and the interaction of phonological and semantic information. Finally, coupling the cognitive network science approach with investigations of languages other than English might further advance our understanding of cognitive processing in general.",
      "abstract": "Cognitive network science has increased our understanding of how the mental lexicon is structured and how that structure at the micro-, meso-, and macro-levels influences language and cognitive processes. Most of the research using this approach has used single-layer networks of English words. We consider two fundamental concepts in network science—nodes and connections (or edges)—in the context of two lesser-studied languages (American Sign Language and Kaqchikel) to see if a single-layer network can model phonological similarities among words in each of those languages. The analyses of those single-layer networks revealed several differences in network architecture that may challenge the cognitive network approach. We discuss several directions for future research using different network architectures that could address these challenges and also increase our understanding of how language processing might vary across languages. Such work would also provide a common framework for research in the language sciences, despite the variation among human languages. The methodological and theoretical tools of network science may also make it easier to integrate research of various language processes, such as typical and delayed development, acquired disorders, and the interaction of phonological and semantic information. Finally, coupling the cognitive network science approach with investigations of languages other than English might further advance our understanding of cognitive processing in general.",
      "doi": "https://doi.org/10.3390/info15070401",
      "openalex_id": "https://openalex.org/W4400583186",
      "arxiv_id": "",
      "publication_date": "2024-07-12",
      "published": "2024-07-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses cognitive network science across languages and network architecture but does not address discrete audio/speech tokenization or token sequences as a core object of study, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses cognitive network science across languages and network architecture but does not address discrete audio/speech tokenization or token sequences as a core object of study, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cognitive network science analyzing linguistic structures such as phonological similarities in American Sign Language and Kaqchikel, without addressing discrete audio/speech tokenization or any modern quantization-based discrete token representations; instead, it deals with network modeling of language structures rather than audio signal tokenization or token design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cognitive network science analyzing linguistic structures such as phonological similarities in American Sign Language and Kaqchikel, without addressing discrete audio/speech tokenization or any modern quantization-based discrete token representations; instead, it deals with network modeling of language structures rather than audio signal tokenization or token design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonological variation and lexical form",
    "abstract": "Abstract It is well documented that words are produced differently by different individuals (interspeaker variation) and by a single individual on different occasions (intraspeaker variation). However, exactly how this variation is processed in the brain, and therefore how to situate it in psycholinguistic models of word production and recognition remains an area of active research and debate. This chapter ties the wealth of sociolinguistic knowledge on phonological variables to the study of the mental lexicon. First, the existing experimental evidence concerning the relationship between phonological variation and lexical access is laid out. Second, to better understand conflicting results and guide further research, this chapter includes an overview of relevant structural and representational properties that differentiate phonological variables. Finally, the advantages of incorporating phonological variation into models of the mental lexicon are discussed.",
    "metadata": {
      "title": "Phonological variation and lexical form",
      "summary": "Abstract It is well documented that words are produced differently by different individuals (interspeaker variation) and by a single individual on different occasions (intraspeaker variation). However, exactly how this variation is processed in the brain, and therefore how to situate it in psycholinguistic models of word production and recognition remains an area of active research and debate. This chapter ties the wealth of sociolinguistic knowledge on phonological variables to the study of the mental lexicon. First, the existing experimental evidence concerning the relationship between phonological variation and lexical access is laid out. Second, to better understand conflicting results and guide further research, this chapter includes an overview of relevant structural and representational properties that differentiate phonological variables. Finally, the advantages of incorporating phonological variation into models of the mental lexicon are discussed.",
      "abstract": "Abstract It is well documented that words are produced differently by different individuals (interspeaker variation) and by a single individual on different occasions (intraspeaker variation). However, exactly how this variation is processed in the brain, and therefore how to situate it in psycholinguistic models of word production and recognition remains an area of active research and debate. This chapter ties the wealth of sociolinguistic knowledge on phonological variables to the study of the mental lexicon. First, the existing experimental evidence concerning the relationship between phonological variation and lexical access is laid out. Second, to better understand conflicting results and guide further research, this chapter includes an overview of relevant structural and representational properties that differentiate phonological variables. Finally, the advantages of incorporating phonological variation into models of the mental lexicon are discussed.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.15",
      "openalex_id": "https://openalex.org/W4212855166",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about discrete audio tokenization: the abstract discusses phonological variation and lexical access within psycholinguistic models, with no explicit analysis or proposal of converting continuous speech to discrete tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about discrete audio tokenization: the abstract discusses phonological variation and lexical access within psycholinguistic models, with no explicit analysis or proposal of converting continuous speech to discrete tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses phonological variation and its role in lexical access and mental lexicon models but does not mention discrete audio/speech tokenization or conversion of audio signals into discrete tokens, nor does it relate to neural encoding or token-based audio representations relevant to language models, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses phonological variation and its role in lexical access and mental lexicon models but does not mention discrete audio/speech tokenization or conversion of audio signals into discrete tokens, nor does it relate to neural encoding or token-based audio representations relevant to language models, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Disorders of Lexical Access And Production",
    "abstract": "Abstract Disorders of lexical access are characterized by inconsistent lexical access such that individuals successfully comprehend or produce a word in some contexts but fail on other occasions. Therefore, the lexical representations are thought to be intact, but their retrieval or activation is impaired and/or competing representations are not effectively managed. Lexical access deficits are most well-studied in individuals with aphasia, though some degree of lexical access difficulty can occur in a wide variety of neurogenic and developmental disorders, as well as in typical aging. This chapter focuses on the intersections of language, cognitive control, and memory: (1) how inhibition of lexical competitors and selection among competitors may explain some lexical access deficit phenomena, and (2) learning and retrieval processes in lexical access deficits from both basic research and translational application perspectives.",
    "metadata": {
      "title": "Disorders of Lexical Access And Production",
      "summary": "Abstract Disorders of lexical access are characterized by inconsistent lexical access such that individuals successfully comprehend or produce a word in some contexts but fail on other occasions. Therefore, the lexical representations are thought to be intact, but their retrieval or activation is impaired and/or competing representations are not effectively managed. Lexical access deficits are most well-studied in individuals with aphasia, though some degree of lexical access difficulty can occur in a wide variety of neurogenic and developmental disorders, as well as in typical aging. This chapter focuses on the intersections of language, cognitive control, and memory: (1) how inhibition of lexical competitors and selection among competitors may explain some lexical access deficit phenomena, and (2) learning and retrieval processes in lexical access deficits from both basic research and translational application perspectives.",
      "abstract": "Abstract Disorders of lexical access are characterized by inconsistent lexical access such that individuals successfully comprehend or produce a word in some contexts but fail on other occasions. Therefore, the lexical representations are thought to be intact, but their retrieval or activation is impaired and/or competing representations are not effectively managed. Lexical access deficits are most well-studied in individuals with aphasia, though some degree of lexical access difficulty can occur in a wide variety of neurogenic and developmental disorders, as well as in typical aging. This chapter focuses on the intersections of language, cognitive control, and memory: (1) how inhibition of lexical competitors and selection among competitors may explain some lexical access deficit phenomena, and (2) learning and retrieval processes in lexical access deficits from both basic research and translational application perspectives.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.28",
      "openalex_id": "https://openalex.org/W4212986620",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on disorders of lexical access in aphasia and language/cognitive control, with no discussion of discrete audio/speech tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on disorders of lexical access in aphasia and language/cognitive control, with no discussion of discrete audio/speech tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on cognitive and linguistic aspects of lexical access deficits rather than on discrete audio/speech tokenization or neural encoding-based audio token representations; there is no mention of discrete tokenization or audio signal processing methods relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on cognitive and linguistic aspects of lexical access deficits rather than on discrete audio/speech tokenization or neural encoding-based audio token representations; there is no mention of discrete tokenization or audio signal processing methods relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical representation and access in sign languages",
    "abstract": "Abstract The focus of this chapter is on how signed words are processed in relation to spoken ones, and whether the profound differences in how the two word types are articulated and perceived affect these processes. The form of words in sign languages, signs, is characterized by an architecture identical to that of spoken words, namely coordinated patterning between two language levels, meaning and articulatory patterning. The lexical architecture of sign differentiates it from that of gesture, which lacks this coordinated, bi-level linguistic structure. The patterning of sign phonology arises from the fine motor patterns of the fingers with the larger ones of the arms, torso, and head. Children’s lexical acquisition reveals how their analytic skills in phonological and morphological acquisition are tied to their early control of the visual field for inter-personal communication. Adult sign access and processing are governed by lexical structure and significantly modulated by prior linguistic experience.",
    "metadata": {
      "title": "Lexical representation and access in sign languages",
      "summary": "Abstract The focus of this chapter is on how signed words are processed in relation to spoken ones, and whether the profound differences in how the two word types are articulated and perceived affect these processes. The form of words in sign languages, signs, is characterized by an architecture identical to that of spoken words, namely coordinated patterning between two language levels, meaning and articulatory patterning. The lexical architecture of sign differentiates it from that of gesture, which lacks this coordinated, bi-level linguistic structure. The patterning of sign phonology arises from the fine motor patterns of the fingers with the larger ones of the arms, torso, and head. Children’s lexical acquisition reveals how their analytic skills in phonological and morphological acquisition are tied to their early control of the visual field for inter-personal communication. Adult sign access and processing are governed by lexical structure and significantly modulated by prior linguistic experience.",
      "abstract": "Abstract The focus of this chapter is on how signed words are processed in relation to spoken ones, and whether the profound differences in how the two word types are articulated and perceived affect these processes. The form of words in sign languages, signs, is characterized by an architecture identical to that of spoken words, namely coordinated patterning between two language levels, meaning and articulatory patterning. The lexical architecture of sign differentiates it from that of gesture, which lacks this coordinated, bi-level linguistic structure. The patterning of sign phonology arises from the fine motor patterns of the fingers with the larger ones of the arms, torso, and head. Children’s lexical acquisition reveals how their analytic skills in phonological and morphological acquisition are tied to their early control of the visual field for inter-personal communication. Adult sign access and processing are governed by lexical structure and significantly modulated by prior linguistic experience.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.27",
      "openalex_id": "https://openalex.org/W4213226374",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses lexical representation and processing in sign languages, focusing on sign-language structure rather than discrete audio/speech tokenization into symbols for audio reconstruction or speech tasks; it does not propose or analyze discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses lexical representation and processing in sign languages, focusing on sign-language structure rather than discrete audio/speech tokenization into symbols for audio reconstruction or speech tasks; it does not propose or analyze discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the lexical representation and processing of sign language words, emphasizing linguistic and phonological aspects rather than discrete audio/speech tokenization or conversion of continuous audio signals to discrete tokens. It does not propose, analyze, or evaluate any discrete audio tokenization method relevant to audio reconstruction or understanding, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the lexical representation and processing of sign language words, emphasizing linguistic and phonological aspects rather than discrete audio/speech tokenization or conversion of continuous audio signals to discrete tokens. It does not propose, analyze, or evaluate any discrete audio tokenization method relevant to audio reconstruction or understanding, so it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EEG analysis based on dynamic visual stimuli",
    "abstract": "This paper reviews best practices for experimental design and analysis for sign language research using neurophysiological methods, such as electroencephalography (EEG) and other methods with high temporal resolution, as well as identifies methodological challenges in neurophysiological research on natural sign language processing. In particular, we outline the considerations for generating linguistically and physically well-controlled stimuli accounting for 1) the layering of manual and non-manual information at different timescales, 2) possible unknown linguistic and non-linguistic visual cues that can affect processing, 3) variability across linguistic stimuli, and 4) predictive processing. Two specific concerns with regard to the analysis and interpretation of observed event related potential (ERP) effects for dynamic stimuli are discussed in detail. First, we discuss the “trigger/effect assignment problem”, which describes the difficulty of determining the time point for calculating ERPs. This issue is related to the problem of determining the onset of a critical sign (i.e., stimulus onset time), and the lack of clarity as to how the border between lexical (sign) and transitional movement (motion trajectory between individual signs) should be defined. Second, we discuss possible differences in the dynamics within signing that might influence ERP patterns and should be controlled for when creating natural sign language material for ERP studies. In addition, we outline alternative approaches to EEG data analyses for natural signing stimuli, such as the timestamping of continuous EEG with trigger markers for each potentially relevant cue in dynamic stimuli. Throughout the discussion, we present empirical evidence for the need to account for dynamic, multi-channel, and multi-timescale visual signal that characterizes sign languages in order to ensure the ecological validity of neurophysiological research in sign languages.",
    "metadata": {
      "title": "EEG analysis based on dynamic visual stimuli",
      "summary": "This paper reviews best practices for experimental design and analysis for sign language research using neurophysiological methods, such as electroencephalography (EEG) and other methods with high temporal resolution, as well as identifies methodological challenges in neurophysiological research on natural sign language processing. In particular, we outline the considerations for generating linguistically and physically well-controlled stimuli accounting for 1) the layering of manual and non-manual information at different timescales, 2) possible unknown linguistic and non-linguistic visual cues that can affect processing, 3) variability across linguistic stimuli, and 4) predictive processing. Two specific concerns with regard to the analysis and interpretation of observed event related potential (ERP) effects for dynamic stimuli are discussed in detail. First, we discuss the “trigger/effect assignment problem”, which describes the difficulty of determining the time point for calculating ERPs. This issue is related to the problem of determining the onset of a critical sign (i.e., stimulus onset time), and the lack of clarity as to how the border between lexical (sign) and transitional movement (motion trajectory between individual signs) should be defined. Second, we discuss possible differences in the dynamics within signing that might influence ERP patterns and should be controlled for when creating natural sign language material for ERP studies. In addition, we outline alternative approaches to EEG data analyses for natural signing stimuli, such as the timestamping of continuous EEG with trigger markers for each potentially relevant cue in dynamic stimuli. Throughout the discussion, we present empirical evidence for the need to account for dynamic, multi-channel, and multi-timescale visual signal that characterizes sign languages in order to ensure the ecological validity of neurophysiological research in sign languages.",
      "abstract": "This paper reviews best practices for experimental design and analysis for sign language research using neurophysiological methods, such as electroencephalography (EEG) and other methods with high temporal resolution, as well as identifies methodological challenges in neurophysiological research on natural sign language processing. In particular, we outline the considerations for generating linguistically and physically well-controlled stimuli accounting for 1) the layering of manual and non-manual information at different timescales, 2) possible unknown linguistic and non-linguistic visual cues that can affect processing, 3) variability across linguistic stimuli, and 4) predictive processing. Two specific concerns with regard to the analysis and interpretation of observed event related potential (ERP) effects for dynamic stimuli are discussed in detail. First, we discuss the “trigger/effect assignment problem”, which describes the difficulty of determining the time point for calculating ERPs. This issue is related to the problem of determining the onset of a critical sign (i.e., stimulus onset time), and the lack of clarity as to how the border between lexical (sign) and transitional movement (motion trajectory between individual signs) should be defined. Second, we discuss possible differences in the dynamics within signing that might influence ERP patterns and should be controlled for when creating natural sign language material for ERP studies. In addition, we outline alternative approaches to EEG data analyses for natural signing stimuli, such as the timestamping of continuous EEG with trigger markers for each potentially relevant cue in dynamic stimuli. Throughout the discussion, we present empirical evidence for the need to account for dynamic, multi-channel, and multi-timescale visual signal that characterizes sign languages in order to ensure the ecological validity of neurophysiological research in sign languages.",
      "doi": "https://doi.org/10.31299/hrri.58.si.13",
      "openalex_id": "https://openalex.org/W4304823005",
      "arxiv_id": "",
      "publication_date": "2022-10-12",
      "published": "2022-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses EEG/ERP methodologies for sign language and dynamic visual stimuli, with no focus on discrete audio tokenization or token-based language tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses EEG/ERP methodologies for sign language and dynamic visual stimuli, with no focus on discrete audio tokenization or token-based language tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on EEG analysis of dynamic visual stimuli specifically related to sign language processing, without any mention or analysis of discrete audio/speech tokenization, discrete audio token representations, or relevant discrete token design for audio signals; thus, it does not meet the inclusion criteria which require explicit focus on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on EEG analysis of dynamic visual stimuli specifically related to sign language processing, without any mention or analysis of discrete audio/speech tokenization, discrete audio token representations, or relevant discrete token design for audio signals; thus, it does not meet the inclusion criteria which require explicit focus on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Recent works in Sign Language Recognition using deep learning approach - A Survey",
    "abstract": "Across the globe, different countries have their own sign language used by hearing and speech-impaired people. The American, Arabic, British, Brazilian, and Indian Sign Languages have gained popularity and piqued researchers and engineers to develop solutions that will help in the task of Sign Language Recognition and Machine translation. Just like spoken languages, sign languages also have dialects, and hence multiple gestures can exist for one word. The task of Sign Language Recognition is complex and has been carried out for the last three decades. In this paper, we survey, compare, and summarize the proposed techniques of researchers in sign language and provide a proposed plan for our imminent work. An extensive survey of CNN networks, Transformer models, Sequence models, Keypoint-based methods, and Hardware-assisted methods applied on various sign language open datasets is presented in the paper which can form a basis of the literature survey for the research.",
    "metadata": {
      "title": "Recent works in Sign Language Recognition using deep learning approach - A Survey",
      "summary": "Across the globe, different countries have their own sign language used by hearing and speech-impaired people. The American, Arabic, British, Brazilian, and Indian Sign Languages have gained popularity and piqued researchers and engineers to develop solutions that will help in the task of Sign Language Recognition and Machine translation. Just like spoken languages, sign languages also have dialects, and hence multiple gestures can exist for one word. The task of Sign Language Recognition is complex and has been carried out for the last three decades. In this paper, we survey, compare, and summarize the proposed techniques of researchers in sign language and provide a proposed plan for our imminent work. An extensive survey of CNN networks, Transformer models, Sequence models, Keypoint-based methods, and Hardware-assisted methods applied on various sign language open datasets is presented in the paper which can form a basis of the literature survey for the research.",
      "abstract": "Across the globe, different countries have their own sign language used by hearing and speech-impaired people. The American, Arabic, British, Brazilian, and Indian Sign Languages have gained popularity and piqued researchers and engineers to develop solutions that will help in the task of Sign Language Recognition and Machine translation. Just like spoken languages, sign languages also have dialects, and hence multiple gestures can exist for one word. The task of Sign Language Recognition is complex and has been carried out for the last three decades. In this paper, we survey, compare, and summarize the proposed techniques of researchers in sign language and provide a proposed plan for our imminent work. An extensive survey of CNN networks, Transformer models, Sequence models, Keypoint-based methods, and Hardware-assisted methods applied on various sign language open datasets is presented in the paper which can form a basis of the literature survey for the research.",
      "doi": "https://doi.org/10.1109/ocit59427.2023.10430576",
      "openalex_id": "https://openalex.org/W4391936583",
      "arxiv_id": "",
      "publication_date": "2023-12-13",
      "published": "2023-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not propose or analyze discrete audio tokenization; it focuses on sign language recognition from visual gestures using deep learning, with no core discrete audio token representation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not propose or analyze discrete audio tokenization; it focuses on sign language recognition from visual gestures using deep learning, with no core discrete audio token representation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language recognition using deep learning, surveying techniques such as CNNs and Transformers applied to visual and gesture-based datasets rather than audio or speech tokenization; it does not address discrete audio/speech tokenization or the conversion of continuous audio to discrete symbols, thus it fails to meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language recognition using deep learning, surveying techniques such as CNNs and Transformers applied to visual and gesture-based datasets rather than audio or speech tokenization; it does not address discrete audio/speech tokenization or the conversion of continuous audio to discrete symbols, thus it fails to meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gamification of RF Data Acquisition for Classification of Natural Human Gestures",
    "abstract": "In recent years, there have been significant developments in radio frequency (RF) sensor technology used in human-computer interaction (HCI) applications, specifically in areas like gesture recognition and more broadly, human activity recognition. Although extensive research has been conducted on these subjects, most experiments involve controlled settings where participants are instructed on how to perform specific movements. However, when such experiments are conducted on sign language recognition they lack capturing dialectal and background-related diversities. In this work, we explore the differences in RF datasets acquired under controlled experimental settings and in free form environments where users were not constrained by the experimental instructions and limitations. We show that directed (i.e., controlled) data acquisition approaches result in over-optimistic performances which do not perform well on naturally acquired data samples in a real-world use case. We evaluate different approaches on generating synthetic samples from directed dataset, but show that such methods do not offer much benefit over collecting natural data. Therefore, we propose an interactive data acquisition paradigm through gamification. We show that the proposed approach enables the recognition of American Sign Language (ASL) in real-world settings by achieving 69% accuracy on 29 words.",
    "metadata": {
      "title": "Gamification of RF Data Acquisition for Classification of Natural Human Gestures",
      "summary": "In recent years, there have been significant developments in radio frequency (RF) sensor technology used in human-computer interaction (HCI) applications, specifically in areas like gesture recognition and more broadly, human activity recognition. Although extensive research has been conducted on these subjects, most experiments involve controlled settings where participants are instructed on how to perform specific movements. However, when such experiments are conducted on sign language recognition they lack capturing dialectal and background-related diversities. In this work, we explore the differences in RF datasets acquired under controlled experimental settings and in free form environments where users were not constrained by the experimental instructions and limitations. We show that directed (i.e., controlled) data acquisition approaches result in over-optimistic performances which do not perform well on naturally acquired data samples in a real-world use case. We evaluate different approaches on generating synthetic samples from directed dataset, but show that such methods do not offer much benefit over collecting natural data. Therefore, we propose an interactive data acquisition paradigm through gamification. We show that the proposed approach enables the recognition of American Sign Language (ASL) in real-world settings by achieving 69% accuracy on 29 words.",
      "abstract": "In recent years, there have been significant developments in radio frequency (RF) sensor technology used in human-computer interaction (HCI) applications, specifically in areas like gesture recognition and more broadly, human activity recognition. Although extensive research has been conducted on these subjects, most experiments involve controlled settings where participants are instructed on how to perform specific movements. However, when such experiments are conducted on sign language recognition they lack capturing dialectal and background-related diversities. In this work, we explore the differences in RF datasets acquired under controlled experimental settings and in free form environments where users were not constrained by the experimental instructions and limitations. We show that directed (i.e., controlled) data acquisition approaches result in over-optimistic performances which do not perform well on naturally acquired data samples in a real-world use case. We evaluate different approaches on generating synthetic samples from directed dataset, but show that such methods do not offer much benefit over collecting natural data. Therefore, we propose an interactive data acquisition paradigm through gamification. We show that the proposed approach enables the recognition of American Sign Language (ASL) in real-world settings by achieving 69% accuracy on 29 words.",
      "doi": "https://doi.org/10.1109/radarconf2458775.2024.10548148",
      "openalex_id": "https://openalex.org/W4399621143",
      "arxiv_id": "",
      "publication_date": "2024-05-06",
      "published": "2024-05-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article discusses RF data acquisition for gesture recognition and gamified data collection, but it does not involve discrete audio tokenization or analysis of discrete audio tokens required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article discusses RF data acquisition for gesture recognition and gamified data collection, but it does not involve discrete audio tokenization or analysis of discrete audio tokens required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on RF data acquisition for classification of human gestures and ASL recognition using RF sensors; it does not propose, analyze, or evaluate discrete audio/speech tokenization. The work involves gesture recognition via RF signals rather than audio signal processing or tokenization, thus not meeting the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on RF data acquisition for classification of human gestures and ASL recognition using RF sensors; it does not propose, analyze, or evaluate discrete audio/speech tokenization. The work involves gesture recognition via RF signals rather than audio signal processing or tokenization, thus not meeting the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An ERP investigation of perceptual vs motoric iconicity in sign production",
    "abstract": "The type of form-meaning mapping for iconic signs can vary. For perceptually-iconic signs there is a correspondence between visual features of a referent (e.g., the beak of a bird) and the form of the sign (e.g., extended thumb and index finger at the mouth for the American Sign Language (ASL) sign BIRD). For motorically-iconic signs there is a correspondence between how an object is held/manipulated and the form of the sign (e.g., the ASL sign FLUTE depicts how a flute is played). Previous studies have found that iconic signs are retrieved faster in picture-naming tasks, but type of iconicity has not been manipulated. We conducted an ERP study in which deaf signers and a control group of English speakers named pictures that targeted perceptually-iconic, motorically-iconic, or non-iconic ASL signs. For signers (unlike the control group), naming latencies varied by iconicity type: perceptually-iconic < motorically-iconic < non-iconic signs. A reduction in the N400 amplitude was only found for the perceptually-iconic signs, compared to both non-iconic and motorically-iconic signs. No modulations of N400 amplitudes were observed for the control group. We suggest that this pattern of results arises because pictures eliciting perceptually-iconic signs can more effectively prime lexical access due to greater alignment between features of the picture and the semantic and phonological features of the sign. We speculate that naming latencies are facilitated for motorically-iconic signs due to later processes (e.g., faster phonological encoding via cascading activation from semantic features). Overall, the results indicate that type of iconicity plays role in sign production when elicited by picture-naming tasks.",
    "metadata": {
      "title": "An ERP investigation of perceptual vs motoric iconicity in sign production",
      "summary": "The type of form-meaning mapping for iconic signs can vary. For perceptually-iconic signs there is a correspondence between visual features of a referent (e.g., the beak of a bird) and the form of the sign (e.g., extended thumb and index finger at the mouth for the American Sign Language (ASL) sign BIRD). For motorically-iconic signs there is a correspondence between how an object is held/manipulated and the form of the sign (e.g., the ASL sign FLUTE depicts how a flute is played). Previous studies have found that iconic signs are retrieved faster in picture-naming tasks, but type of iconicity has not been manipulated. We conducted an ERP study in which deaf signers and a control group of English speakers named pictures that targeted perceptually-iconic, motorically-iconic, or non-iconic ASL signs. For signers (unlike the control group), naming latencies varied by iconicity type: perceptually-iconic < motorically-iconic < non-iconic signs. A reduction in the N400 amplitude was only found for the perceptually-iconic signs, compared to both non-iconic and motorically-iconic signs. No modulations of N400 amplitudes were observed for the control group. We suggest that this pattern of results arises because pictures eliciting perceptually-iconic signs can more effectively prime lexical access due to greater alignment between features of the picture and the semantic and phonological features of the sign. We speculate that naming latencies are facilitated for motorically-iconic signs due to later processes (e.g., faster phonological encoding via cascading activation from semantic features). Overall, the results indicate that type of iconicity plays role in sign production when elicited by picture-naming tasks.",
      "abstract": "The type of form-meaning mapping for iconic signs can vary. For perceptually-iconic signs there is a correspondence between visual features of a referent (e.g., the beak of a bird) and the form of the sign (e.g., extended thumb and index finger at the mouth for the American Sign Language (ASL) sign BIRD). For motorically-iconic signs there is a correspondence between how an object is held/manipulated and the form of the sign (e.g., the ASL sign FLUTE depicts how a flute is played). Previous studies have found that iconic signs are retrieved faster in picture-naming tasks, but type of iconicity has not been manipulated. We conducted an ERP study in which deaf signers and a control group of English speakers named pictures that targeted perceptually-iconic, motorically-iconic, or non-iconic ASL signs. For signers (unlike the control group), naming latencies varied by iconicity type: perceptually-iconic < motorically-iconic < non-iconic signs. A reduction in the N400 amplitude was only found for the perceptually-iconic signs, compared to both non-iconic and motorically-iconic signs. No modulations of N400 amplitudes were observed for the control group. We suggest that this pattern of results arises because pictures eliciting perceptually-iconic signs can more effectively prime lexical access due to greater alignment between features of the picture and the semantic and phonological features of the sign. We speculate that naming latencies are facilitated for motorically-iconic signs due to later processes (e.g., faster phonological encoding via cascading activation from semantic features). Overall, the results indicate that type of iconicity plays role in sign production when elicited by picture-naming tasks.",
      "doi": "https://doi.org/10.1016/j.neuropsychologia.2024.108966",
      "openalex_id": "https://openalex.org/W4401285877",
      "arxiv_id": "",
      "publication_date": "2024-08-03",
      "published": "2024-08-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on sign language iconicity and ERP; it does not involve discrete audio tokenization or speech tokenization, so it does not meet the core inclusion criterion requiring discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on sign language iconicity and ERP; it does not involve discrete audio tokenization or speech tokenization, so it does not meet the core inclusion criterion requiring discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates electrophysiological responses (ERP) in sign language production focusing on perceptual versus motoric iconicity, without addressing discrete audio/speech tokenization or related audio signal processing techniques; thus, it neither proposes nor analyzes discrete audio tokenization methods relevant to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates electrophysiological responses (ERP) in sign language production focusing on perceptual versus motoric iconicity, without addressing discrete audio/speech tokenization or related audio signal processing techniques; thus, it neither proposes nor analyzes discrete audio tokenization methods relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploring the Benefits and Applications of Video-Span Selection and Search for Real-Time Support in Sign Language Video Comprehension among ASL Learners",
    "abstract": "People learning American Sign Language (ASL) and practicing their comprehension skills will often encounter complex ASL videos that may contain unfamiliar signs. Existing dictionary tools require users to isolate a single unknown sign before initiating a search by selecting linguistic properties or performing the sign in front of a webcam. This process presents challenges in extracting and reproducing unfamiliar signs, disrupting the video-watching experience, and requiring learners to rely on external dictionaries. We explore a technology that allows users to select and view dictionary results for one or more unfamiliar signs while watching a video. We interviewed 14 ASL learners to understand their challenges in understanding ASL videos, strategies for dealing with unfamiliar vocabulary, and expectations for an in situ dictionary system. We then conducted an in-depth analysis with eight learners to examine their interactions with a Wizard-of-Oz prototype during a video comprehension task. Finally, we conducted a comparative study with six additional ASL learners to evaluate the speed, accuracy, and workload benefits of an embedded dictionary-search feature within a video player. Our tool outperformed a baseline in the form of an existing online dictionary across all three metrics. The integration of a search tool and span selection offered advantages for video comprehension. Our findings have implications for designers, computer vision researchers, and sign language educators.",
    "metadata": {
      "title": "Exploring the Benefits and Applications of Video-Span Selection and Search for Real-Time Support in Sign Language Video Comprehension among ASL Learners",
      "summary": "People learning American Sign Language (ASL) and practicing their comprehension skills will often encounter complex ASL videos that may contain unfamiliar signs. Existing dictionary tools require users to isolate a single unknown sign before initiating a search by selecting linguistic properties or performing the sign in front of a webcam. This process presents challenges in extracting and reproducing unfamiliar signs, disrupting the video-watching experience, and requiring learners to rely on external dictionaries. We explore a technology that allows users to select and view dictionary results for one or more unfamiliar signs while watching a video. We interviewed 14 ASL learners to understand their challenges in understanding ASL videos, strategies for dealing with unfamiliar vocabulary, and expectations for an in situ dictionary system. We then conducted an in-depth analysis with eight learners to examine their interactions with a Wizard-of-Oz prototype during a video comprehension task. Finally, we conducted a comparative study with six additional ASL learners to evaluate the speed, accuracy, and workload benefits of an embedded dictionary-search feature within a video player. Our tool outperformed a baseline in the form of an existing online dictionary across all three metrics. The integration of a search tool and span selection offered advantages for video comprehension. Our findings have implications for designers, computer vision researchers, and sign language educators.",
      "abstract": "People learning American Sign Language (ASL) and practicing their comprehension skills will often encounter complex ASL videos that may contain unfamiliar signs. Existing dictionary tools require users to isolate a single unknown sign before initiating a search by selecting linguistic properties or performing the sign in front of a webcam. This process presents challenges in extracting and reproducing unfamiliar signs, disrupting the video-watching experience, and requiring learners to rely on external dictionaries. We explore a technology that allows users to select and view dictionary results for one or more unfamiliar signs while watching a video. We interviewed 14 ASL learners to understand their challenges in understanding ASL videos, strategies for dealing with unfamiliar vocabulary, and expectations for an in situ dictionary system. We then conducted an in-depth analysis with eight learners to examine their interactions with a Wizard-of-Oz prototype during a video comprehension task. Finally, we conducted a comparative study with six additional ASL learners to evaluate the speed, accuracy, and workload benefits of an embedded dictionary-search feature within a video player. Our tool outperformed a baseline in the form of an existing online dictionary across all three metrics. The integration of a search tool and span selection offered advantages for video comprehension. Our findings have implications for designers, computer vision researchers, and sign language educators.",
      "doi": "https://doi.org/10.1145/3690647",
      "openalex_id": "https://openalex.org/W4402132261",
      "arxiv_id": "",
      "publication_date": "2024-09-02",
      "published": "2024-09-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a sign-language video comprehension tool with in-video dictionary search, focusing on video/CV interactions rather than discrete audio tokenization of speech; it does not propose or analyze discrete audio tokens for speech tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a sign-language video comprehension tool with in-video dictionary search, focusing on video/CV interactions rather than discrete audio tokenization of speech; it does not propose or analyze discrete audio tokens for speech tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the development and evaluation of a sign language video comprehension tool with embedded dictionary search features and does not address discrete audio or speech tokenization, nor does it analyze or propose methods converting continuous audio signals into discrete tokens for audio tasks; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the development and evaluation of a sign language video comprehension tool with embedded dictionary search features and does not address discrete audio or speech tokenization, nor does it analyze or propose methods converting continuous audio signals into discrete tokens for audio tasks; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic processing of iconic signs is not automatic: Neural evidence from hearing non-signers",
    "abstract": "Abstract Iconicity facilitates learning signs, but it is unknown whether recognition of meaning from the sign form occurs automatically. We recorded ERPs to highly iconic (transparent) and non-iconic ASL signs presented to one group who knew they would be taught signs (learners) and another group with no such expectations (non-learners). Participants watched sign videos and detected an occasional grooming gesture (no semantic processing required). Before sign onset, learners showed a greater frontal negativity compared to non-learners for both sign types, possibly due to greater motivation to attend to signs. During the N400 window, learners showed greater negativity to iconic than non-iconic signs, indicating more semantic processing for iconic signs. The non-learners showed a later and much weaker iconicity effect. The groups did not differ in task performance or in P3 amplitude. We conclude that comprehending the form-meaning mapping of highly iconic signs is not automatic and requires motivation and attention.",
    "metadata": {
      "title": "Semantic processing of iconic signs is not automatic: Neural evidence from hearing non-signers",
      "summary": "Abstract Iconicity facilitates learning signs, but it is unknown whether recognition of meaning from the sign form occurs automatically. We recorded ERPs to highly iconic (transparent) and non-iconic ASL signs presented to one group who knew they would be taught signs (learners) and another group with no such expectations (non-learners). Participants watched sign videos and detected an occasional grooming gesture (no semantic processing required). Before sign onset, learners showed a greater frontal negativity compared to non-learners for both sign types, possibly due to greater motivation to attend to signs. During the N400 window, learners showed greater negativity to iconic than non-iconic signs, indicating more semantic processing for iconic signs. The non-learners showed a later and much weaker iconicity effect. The groups did not differ in task performance or in P3 amplitude. We conclude that comprehending the form-meaning mapping of highly iconic signs is not automatic and requires motivation and attention.",
      "abstract": "Abstract Iconicity facilitates learning signs, but it is unknown whether recognition of meaning from the sign form occurs automatically. We recorded ERPs to highly iconic (transparent) and non-iconic ASL signs presented to one group who knew they would be taught signs (learners) and another group with no such expectations (non-learners). Participants watched sign videos and detected an occasional grooming gesture (no semantic processing required). Before sign onset, learners showed a greater frontal negativity compared to non-learners for both sign types, possibly due to greater motivation to attend to signs. During the N400 window, learners showed greater negativity to iconic than non-iconic signs, indicating more semantic processing for iconic signs. The non-learners showed a later and much weaker iconicity effect. The groups did not differ in task performance or in P3 amplitude. We conclude that comprehending the form-meaning mapping of highly iconic signs is not automatic and requires motivation and attention.",
      "doi": "https://doi.org/10.1017/s1366728924001093",
      "openalex_id": "https://openalex.org/W4407294001",
      "arxiv_id": "",
      "publication_date": "2025-02-10",
      "published": "2025-02-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes sign-language semantic processing and does not involve discrete audio tokenization or token-based representations, so it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes sign-language semantic processing and does not involve discrete audio tokenization or token-based representations, so it fails the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic processing of iconic signs in sign language and neural evidence from hearing non-signers, with no mention of audio or speech tokenization or discrete audio symbol analysis; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and instead focuses on neural and semantic processing of sign language visual stimuli.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic processing of iconic signs in sign language and neural evidence from hearing non-signers, with no mention of audio or speech tokenization or discrete audio symbol analysis; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and instead focuses on neural and semantic processing of sign language visual stimuli.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Playing With Language in the Manual Modality: Which Motions Do Signers Gradiently Modify?",
    "abstract": "Abstract Language is traditionally characterized as an arbitrary, symbolic system, made up of discrete, categorical forms. But iconicity and gradience are pervasive in communication. For example, in spoken languages, word forms can be “played with” in iconic gradient ways by varying vowel length, pitch, or speed (e.g., “It's been a loooooooong day”). However, little is known about this process in sign languages. Here, we (1) explore gradient modification in three dimensions of motion in American Sign Language (ASL), and (2) ask whether the three dimensions are equally likely to be modified. We asked deaf signers of ASL ( n = 11, mean age = 49.3) to describe an event manipulated along speed, direction, or path, and observed their use of gradient modification in lexical and depicting signs. We found that signers alter the forms of both types of signs to enhance meaning. However, the three motion dimensions were not modified equally in lexical signs, suggesting constraints on gradient modification. These constraints may be linguistic in nature, found only in signers. Alternatively, the constraints could reflect difficulties in using the hands to convey particular modifications and, if so, should be found in speakers as well as signers.",
    "metadata": {
      "title": "Playing With Language in the Manual Modality: Which Motions Do Signers Gradiently Modify?",
      "summary": "Abstract Language is traditionally characterized as an arbitrary, symbolic system, made up of discrete, categorical forms. But iconicity and gradience are pervasive in communication. For example, in spoken languages, word forms can be “played with” in iconic gradient ways by varying vowel length, pitch, or speed (e.g., “It's been a loooooooong day”). However, little is known about this process in sign languages. Here, we (1) explore gradient modification in three dimensions of motion in American Sign Language (ASL), and (2) ask whether the three dimensions are equally likely to be modified. We asked deaf signers of ASL ( n = 11, mean age = 49.3) to describe an event manipulated along speed, direction, or path, and observed their use of gradient modification in lexical and depicting signs. We found that signers alter the forms of both types of signs to enhance meaning. However, the three motion dimensions were not modified equally in lexical signs, suggesting constraints on gradient modification. These constraints may be linguistic in nature, found only in signers. Alternatively, the constraints could reflect difficulties in using the hands to convey particular modifications and, if so, should be found in speakers as well as signers.",
      "abstract": "Abstract Language is traditionally characterized as an arbitrary, symbolic system, made up of discrete, categorical forms. But iconicity and gradience are pervasive in communication. For example, in spoken languages, word forms can be “played with” in iconic gradient ways by varying vowel length, pitch, or speed (e.g., “It's been a loooooooong day”). However, little is known about this process in sign languages. Here, we (1) explore gradient modification in three dimensions of motion in American Sign Language (ASL), and (2) ask whether the three dimensions are equally likely to be modified. We asked deaf signers of ASL ( n = 11, mean age = 49.3) to describe an event manipulated along speed, direction, or path, and observed their use of gradient modification in lexical and depicting signs. We found that signers alter the forms of both types of signs to enhance meaning. However, the three motion dimensions were not modified equally in lexical signs, suggesting constraints on gradient modification. These constraints may be linguistic in nature, found only in signers. Alternatively, the constraints could reflect difficulties in using the hands to convey particular modifications and, if so, should be found in speakers as well as signers.",
      "doi": "https://doi.org/10.1111/cogs.70051",
      "openalex_id": "https://openalex.org/W4409087116",
      "arxiv_id": "",
      "publication_date": "2025-04-01",
      "published": "2025-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates gradient modification of hand motions in ASL and does not address discrete audio tokenization or speech-token sequences, thus it does not meet the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates gradient modification of hand motions in ASL and does not address discrete audio tokenization or speech-token sequences, thus it does not meet the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gradient modification in motion dimensions of American Sign Language, which concerns visual-manual communication modalities rather than audio signals or discrete audio tokenization. It does not propose, analyze, or evaluate discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens, nor does it discuss relevant neural encoding or token representations for audio reconstruction or understanding. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gradient modification in motion dimensions of American Sign Language, which concerns visual-manual communication modalities rather than audio signals or discrete audio tokenization. It does not propose, analyze, or evaluate discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens, nor does it discuss relevant neural encoding or token representations for audio reconstruction or understanding. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Relationship Between Community Size and Iconicity in Sign Languages",
    "abstract": "Abstract Communication is harder in larger communities. Past research shows that this leads larger communities to create languages that are easier to learn and use. In particular, previous research suggests that spoken languages that are used by larger communities are more sound symbolic than spoken languages used by smaller communities, presumably, because sound symbolism facilitates language acquisition and use. This study tests whether the same principle extends to sign languages as the role of iconicity in the acquisition and use of sign languages is debated. Furthermore, sign languages are more iconic than spoken languages and are argued to lose their iconicity over time. Therefore, they might not show the same pattern. The paper also tests whether iconicity depends on semantic domain. Participants from five different countries guessed the meaning and rated the iconicity of signs from 11 different sign languages: five languages with &gt;500,000 signers and six languages with &lt;3000 signers. Half of the signs referred to social concepts (e.g., friend, shame) and half referred to nonsocial concepts (e.g., garlic, morning). Nonsocial signs from large sign languages were rated as more iconic than nonsocial signs from small sign languages with no difference between the languages for social signs. Results also suggest that rated iconicity and guessing accuracy are more aligned in signs from large sign languages, potentially because smaller sign languages are more likely to rely on culture‐specific iconicity that is not as easily guessed outside of context. Together, this study shows how community size can influence lexical form and how the effect of such social pressures might depend on semantic domain.",
    "metadata": {
      "title": "The Relationship Between Community Size and Iconicity in Sign Languages",
      "summary": "Abstract Communication is harder in larger communities. Past research shows that this leads larger communities to create languages that are easier to learn and use. In particular, previous research suggests that spoken languages that are used by larger communities are more sound symbolic than spoken languages used by smaller communities, presumably, because sound symbolism facilitates language acquisition and use. This study tests whether the same principle extends to sign languages as the role of iconicity in the acquisition and use of sign languages is debated. Furthermore, sign languages are more iconic than spoken languages and are argued to lose their iconicity over time. Therefore, they might not show the same pattern. The paper also tests whether iconicity depends on semantic domain. Participants from five different countries guessed the meaning and rated the iconicity of signs from 11 different sign languages: five languages with &gt;500,000 signers and six languages with &lt;3000 signers. Half of the signs referred to social concepts (e.g., friend, shame) and half referred to nonsocial concepts (e.g., garlic, morning). Nonsocial signs from large sign languages were rated as more iconic than nonsocial signs from small sign languages with no difference between the languages for social signs. Results also suggest that rated iconicity and guessing accuracy are more aligned in signs from large sign languages, potentially because smaller sign languages are more likely to rely on culture‐specific iconicity that is not as easily guessed outside of context. Together, this study shows how community size can influence lexical form and how the effect of such social pressures might depend on semantic domain.",
      "abstract": "Abstract Communication is harder in larger communities. Past research shows that this leads larger communities to create languages that are easier to learn and use. In particular, previous research suggests that spoken languages that are used by larger communities are more sound symbolic than spoken languages used by smaller communities, presumably, because sound symbolism facilitates language acquisition and use. This study tests whether the same principle extends to sign languages as the role of iconicity in the acquisition and use of sign languages is debated. Furthermore, sign languages are more iconic than spoken languages and are argued to lose their iconicity over time. Therefore, they might not show the same pattern. The paper also tests whether iconicity depends on semantic domain. Participants from five different countries guessed the meaning and rated the iconicity of signs from 11 different sign languages: five languages with &gt;500,000 signers and six languages with &lt;3000 signers. Half of the signs referred to social concepts (e.g., friend, shame) and half referred to nonsocial concepts (e.g., garlic, morning). Nonsocial signs from large sign languages were rated as more iconic than nonsocial signs from small sign languages with no difference between the languages for social signs. Results also suggest that rated iconicity and guessing accuracy are more aligned in signs from large sign languages, potentially because smaller sign languages are more likely to rely on culture‐specific iconicity that is not as easily guessed outside of context. Together, this study shows how community size can influence lexical form and how the effect of such social pressures might depend on semantic domain.",
      "doi": "https://doi.org/10.1111/cogs.70074",
      "openalex_id": "https://openalex.org/W4411134337",
      "arxiv_id": "",
      "publication_date": "2025-06-01",
      "published": "2025-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies iconicity in sign languages and community size and does not involve discrete audio tokenization or token-based representations of speech, so it fails the discrete-tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies iconicity in sign languages and community size and does not involve discrete audio tokenization or token-based representations of speech, so it fails the discrete-tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the relationship between community size and iconicity in sign languages, with no mention or analysis of discrete audio/speech tokenization, discrete quantization, or any audio signal processing relevant to token sequences; therefore, it does not meet the inclusion criteria for studies on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the relationship between community size and iconicity in sign languages, with no mention or analysis of discrete audio/speech tokenization, discrete quantization, or any audio signal processing relevant to token sequences; therefore, it does not meet the inclusion criteria for studies on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The relationship between verbal form and event structure in sign languages",
    "abstract": "Whether predicates describe events as inherently bounded (telic) or unbounded (atelic) is usually understood to be an emergent property that depends on several factors; few, if any, spoken languages have dedicated morphology to mark the distinction. It is thus surprising that sign languages have been proposed to have dedicated morphology for telicity, and moreover that it takes a form which iconically reflects the underlying event structure – this is known as the “Event Visibility Hypothesis” (EVH) (Wilbur 2008). The EVH has been extended with claims about its universality in sign languages (Wilbur 2008; Malaia &amp;amp; Wilbur 2012), its gradient nature (Kuhn 2017), and its iconic transparency (Strickland et al. 2015). However, in this paper we argue that the status of this relationship between form and meaning remains an open question due to (a) lack of independent tests for telicity, (b) lack of lexical coverage, (c) lack of demonstration that formal expressions of telicity are morphological in nature, rather than a lexical property, and (d) inability to sufficiently dissociate telicity and perfectivity. We present new data coming from verbs that alternate in both form and meaning in ASL that is in line with the EVH, and conclude that while there is evidence supporting a morphological marker, the proposed form and telicity are not isomorphic in their distribution, significantly limiting the “visibility” of the event structure. We further propose that much of the related iconicity is the result of several independent factors also found in spoken languages, so that sign languages may be more similar to spoken languages than typically implied in this domain.",
    "metadata": {
      "title": "The relationship between verbal form and event structure in sign languages",
      "summary": "Whether predicates describe events as inherently bounded (telic) or unbounded (atelic) is usually understood to be an emergent property that depends on several factors; few, if any, spoken languages have dedicated morphology to mark the distinction. It is thus surprising that sign languages have been proposed to have dedicated morphology for telicity, and moreover that it takes a form which iconically reflects the underlying event structure – this is known as the “Event Visibility Hypothesis” (EVH) (Wilbur 2008). The EVH has been extended with claims about its universality in sign languages (Wilbur 2008; Malaia &amp;amp; Wilbur 2012), its gradient nature (Kuhn 2017), and its iconic transparency (Strickland et al. 2015). However, in this paper we argue that the status of this relationship between form and meaning remains an open question due to (a) lack of independent tests for telicity, (b) lack of lexical coverage, (c) lack of demonstration that formal expressions of telicity are morphological in nature, rather than a lexical property, and (d) inability to sufficiently dissociate telicity and perfectivity. We present new data coming from verbs that alternate in both form and meaning in ASL that is in line with the EVH, and conclude that while there is evidence supporting a morphological marker, the proposed form and telicity are not isomorphic in their distribution, significantly limiting the “visibility” of the event structure. We further propose that much of the related iconicity is the result of several independent factors also found in spoken languages, so that sign languages may be more similar to spoken languages than typically implied in this domain.",
      "abstract": "Whether predicates describe events as inherently bounded (telic) or unbounded (atelic) is usually understood to be an emergent property that depends on several factors; few, if any, spoken languages have dedicated morphology to mark the distinction. It is thus surprising that sign languages have been proposed to have dedicated morphology for telicity, and moreover that it takes a form which iconically reflects the underlying event structure – this is known as the “Event Visibility Hypothesis” (EVH) (Wilbur 2008). The EVH has been extended with claims about its universality in sign languages (Wilbur 2008; Malaia &amp;amp; Wilbur 2012), its gradient nature (Kuhn 2017), and its iconic transparency (Strickland et al. 2015). However, in this paper we argue that the status of this relationship between form and meaning remains an open question due to (a) lack of independent tests for telicity, (b) lack of lexical coverage, (c) lack of demonstration that formal expressions of telicity are morphological in nature, rather than a lexical property, and (d) inability to sufficiently dissociate telicity and perfectivity. We present new data coming from verbs that alternate in both form and meaning in ASL that is in line with the EVH, and conclude that while there is evidence supporting a morphological marker, the proposed form and telicity are not isomorphic in their distribution, significantly limiting the “visibility” of the event structure. We further propose that much of the related iconicity is the result of several independent factors also found in spoken languages, so that sign languages may be more similar to spoken languages than typically implied in this domain.",
      "doi": "https://doi.org/10.5334/gjgl.924",
      "openalex_id": "https://openalex.org/W2989221848",
      "arxiv_id": "",
      "publication_date": "2019-11-11",
      "published": "2019-11-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper deals with telicity and event structure in sign languages and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-level representations, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper deals with telicity and event structure in sign languages and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-level representations, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the linguistic analysis of sign languages, particularly the morphological marking of telicity and event structure, with no mention of discrete audio or speech tokenization, neural encoding, or token-based audio representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor does it address discrete token representations for audio reconstruction or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the linguistic analysis of sign languages, particularly the morphological marking of telicity and event structure, with no mention of discrete audio or speech tokenization, neural encoding, or token-based audio representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor does it address discrete token representations for audio reconstruction or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Construals of iconicity: experimental approaches to form-meaning resemblances in language",
    "abstract": "While speculations on form-meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well-defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well-equipped to contribute to a comprehensive science of language.",
    "metadata": {
      "title": "Construals of iconicity: experimental approaches to form-meaning resemblances in language",
      "summary": "While speculations on form-meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well-defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well-equipped to contribute to a comprehensive science of language.",
      "abstract": "While speculations on form-meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well-defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well-equipped to contribute to a comprehensive science of language.",
      "doi": "https://doi.org/10.31234/osf.io/9qb6a",
      "openalex_id": "https://openalex.org/W4254548727",
      "arxiv_id": "",
      "publication_date": "2019-12-03",
      "published": "2019-12-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article is a conceptual/theoretical/experimental review of iconicity in language and does not focus on discrete audio/speech tokenization or token quantization; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article is a conceptual/theoretical/experimental review of iconicity in language and does not focus on discrete audio/speech tokenization or token quantization; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on experimental approaches to iconicity and form-meaning resemblances in language, without explicit mention or analysis of discrete audio/speech tokenization, discrete quantization, or neural encoding-based token representations relevant to speech tasks; therefore, it does not meet the inclusion criteria and does not address the core topic of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on experimental approaches to iconicity and form-meaning resemblances in language, without explicit mention or analysis of discrete audio/speech tokenization, discrete quantization, or neural encoding-based token representations relevant to speech tasks; therefore, it does not meet the inclusion criteria and does not address the core topic of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The effect of bilingualism on lexical learning and memory across two language modalities: some evidence for a domain-specific, but not general, advantage",
    "abstract": "The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material – no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults.",
    "metadata": {
      "title": "The effect of bilingualism on lexical learning and memory across two language modalities: some evidence for a domain-specific, but not general, advantage",
      "summary": "The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material – no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults.",
      "abstract": "The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material – no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults.",
      "doi": "https://doi.org/10.1080/20445911.2019.1634080",
      "openalex_id": "https://openalex.org/W2953716057",
      "arxiv_id": "",
      "publication_date": "2019-06-24",
      "published": "2019-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates bilingual advantages in memory for speech-like material and sign language, but there is no explicit focus on discrete audio/speech tokenization or converting continuous audio into a sequence of discrete tokens for language tasks; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates bilingual advantages in memory for speech-like material and sign language, but there is no explicit focus on discrete audio/speech tokenization or converting continuous audio into a sequence of discrete tokens for language tasks; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cognitive advantages of bilingualism in memory and word learning but does not investigate discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens as required by the inclusion criteria. It primarily examines cognitive and linguistic performance without methodological contribution to or evaluation of discrete audio tokenization techniques.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cognitive advantages of bilingualism in memory and word learning but does not investigate discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens as required by the inclusion criteria. It primarily examines cognitive and linguistic performance without methodological contribution to or evaluation of discrete audio tokenization techniques.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Children’s Use of Syntax In Word Learning",
    "abstract": "Abstract This chapter investigates the role that syntax plays in guiding the acquisition of word meaning. It reviews data that reveal how children can use the syntactic distribution of a word as evidence for its meaning and discusses the principles of grammar that license such inferences. We delineate the role of thematic linking generalizations in the acquisition of action verbs, arguing that children use specific links between subject and agent and between object and patient to guide initial verb learning. In the domain of attitude verbs, we show that children’s knowledge of abstract links between subclasses of attitude verbs and their syntactic distribution enable learners to identify the meanings of their initial attitude verbs, such as think and want. Finally, we show that syntactic bootstrapping effects are not limited to verb learning but extend across the lexicon.",
    "metadata": {
      "title": "Children’s Use of Syntax In Word Learning",
      "summary": "Abstract This chapter investigates the role that syntax plays in guiding the acquisition of word meaning. It reviews data that reveal how children can use the syntactic distribution of a word as evidence for its meaning and discusses the principles of grammar that license such inferences. We delineate the role of thematic linking generalizations in the acquisition of action verbs, arguing that children use specific links between subject and agent and between object and patient to guide initial verb learning. In the domain of attitude verbs, we show that children’s knowledge of abstract links between subclasses of attitude verbs and their syntactic distribution enable learners to identify the meanings of their initial attitude verbs, such as think and want. Finally, we show that syntactic bootstrapping effects are not limited to verb learning but extend across the lexicon.",
      "abstract": "Abstract This chapter investigates the role that syntax plays in guiding the acquisition of word meaning. It reviews data that reveal how children can use the syntactic distribution of a word as evidence for its meaning and discusses the principles of grammar that license such inferences. We delineate the role of thematic linking generalizations in the acquisition of action verbs, arguing that children use specific links between subject and agent and between object and patient to guide initial verb learning. In the domain of attitude verbs, we show that children’s knowledge of abstract links between subclasses of attitude verbs and their syntactic distribution enable learners to identify the meanings of their initial attitude verbs, such as think and want. Finally, we show that syntactic bootstrapping effects are not limited to verb learning but extend across the lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.10",
      "openalex_id": "https://openalex.org/W4212781277",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on syntax-driven word learning and bootstrapping, with no discrete audio tokenization or token-based representations analyzed; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on syntax-driven word learning and bootstrapping, with no discrete audio tokenization or token-based representations analyzed; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on children's use of syntax in word learning and do not mention discrete audio/speech tokenization, discrete symbol sequences, or any form of audio signal quantization, which are required by the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to audio or speech tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on children's use of syntax in word learning and do not mention discrete audio/speech tokenization, discrete symbol sequences, or any form of audio signal quantization, which are required by the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to audio or speech tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Real-time lexical comprehension in young children learning American Sign Language",
    "abstract": "When children interpret spoken language in real time, linguistic information drives rapidshifts in visual attention to objects in the visual world. This language-vision interaction canprovide insights into children's developing efficiency in language comprehension. But how doeslanguage influence visual attention when the linguistic signal and the visual world are bothprocessed via the visual channel? Here, we measured eye movements during real-timecomprehension of a visual-manual language, American Sign Language (ASL), by 29 nativeASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. Allsigners showed evidence of rapid, incremental language comprehension, tending to initiate aneye movement before sign offset. Deaf and hearing ASL-learners showed similar gaze patterns,suggesting that the in-the-moment dynamics of eye movements during ASL processing areshaped by the constraints of processing a visual language in real time and not by differentialaccess to auditory information in day-to-day life. Finally, variation in children’s ASL processingwas positively correlated with age and vocabulary size. Thus, despite competition for attentionwithin a single modality, the timing and accuracy of visual fixations during ASL comprehensionreflect information processing skills that are fundamental for language acquisition regardless oflanguage modality.",
    "metadata": {
      "title": "Real-time lexical comprehension in young children learning American Sign Language",
      "summary": "When children interpret spoken language in real time, linguistic information drives rapidshifts in visual attention to objects in the visual world. This language-vision interaction canprovide insights into children's developing efficiency in language comprehension. But how doeslanguage influence visual attention when the linguistic signal and the visual world are bothprocessed via the visual channel? Here, we measured eye movements during real-timecomprehension of a visual-manual language, American Sign Language (ASL), by 29 nativeASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. Allsigners showed evidence of rapid, incremental language comprehension, tending to initiate aneye movement before sign offset. Deaf and hearing ASL-learners showed similar gaze patterns,suggesting that the in-the-moment dynamics of eye movements during ASL processing areshaped by the constraints of processing a visual language in real time and not by differentialaccess to auditory information in day-to-day life. Finally, variation in children’s ASL processingwas positively correlated with age and vocabulary size. Thus, despite competition for attentionwithin a single modality, the timing and accuracy of visual fixations during ASL comprehensionreflect information processing skills that are fundamental for language acquisition regardless oflanguage modality.",
      "abstract": "When children interpret spoken language in real time, linguistic information drives rapidshifts in visual attention to objects in the visual world. This language-vision interaction canprovide insights into children's developing efficiency in language comprehension. But how doeslanguage influence visual attention when the linguistic signal and the visual world are bothprocessed via the visual channel? Here, we measured eye movements during real-timecomprehension of a visual-manual language, American Sign Language (ASL), by 29 nativeASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. Allsigners showed evidence of rapid, incremental language comprehension, tending to initiate aneye movement before sign offset. Deaf and hearing ASL-learners showed similar gaze patterns,suggesting that the in-the-moment dynamics of eye movements during ASL processing areshaped by the constraints of processing a visual language in real time and not by differentialaccess to auditory information in day-to-day life. Finally, variation in children’s ASL processingwas positively correlated with age and vocabulary size. Thus, despite competition for attentionwithin a single modality, the timing and accuracy of visual fixations during ASL comprehensionreflect information processing skills that are fundamental for language acquisition regardless oflanguage modality.",
      "doi": "https://doi.org/10.31234/osf.io/zht6g",
      "openalex_id": "https://openalex.org/W4240668972",
      "arxiv_id": "",
      "publication_date": "2017-10-12",
      "published": "2017-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on real-time ASL processing and eye movements in a visual-gestural language; it does not involve discrete audio tokenization or analysis of discrete audio tokens, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on real-time ASL processing and eye movements in a visual-gestural language; it does not involve discrete audio tokenization or analysis of discrete audio tokens, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates real-time lexical comprehension in American Sign Language by analyzing eye movements in children and adults but does not address audio/speech tokenization or discrete token representation of continuous audio signals, which is the core inclusion criterion; it focuses on visual-manual language processing rather than audio signal processing or tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates real-time lexical comprehension in American Sign Language by analyzing eye movements in children and adults but does not address audio/speech tokenization or discrete token representation of continuous audio signals, which is the core inclusion criterion; it focuses on visual-manual language processing rather than audio signal processing or tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perception and production of language in the visual modality",
    "abstract": "Abstract Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
    "metadata": {
      "title": "Perception and production of language in the visual modality",
      "summary": "Abstract Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "abstract": "Abstract Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "doi": "https://doi.org/10.1075/tilar.25.08dye",
      "openalex_id": "https://openalex.org/W4245659432",
      "arxiv_id": "",
      "publication_date": "2020-01-30",
      "published": "2020-01-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on sign language and visual language processing, not discrete audio tokenization or analysis of speech token sequences, so it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on sign language and visual language processing, not discrete audio tokenization or analysis of speech token sequences, so it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the perception and production of sign language in the visual modality, discussing psycholinguistic aspects and language acquisition in visual language systems, but it does not mention any discrete audio/speech tokenization, audio signal processing, or analysis related to discrete audio tokens as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the perception and production of sign language in the visual modality, discussing psycholinguistic aspects and language acquisition in visual language systems, but it does not mention any discrete audio/speech tokenization, audio signal processing, or analysis related to discrete audio tokens as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical processing in sign language comprehension and production – experimental perspectives",
    "abstract": "The study of sign language has received increasing interest in the last decades. Within this growing field, research on sign language processing – including both comprehension and production – has also received a remarkable boost in recent years. At initial stages of research, efforts were concentrated on demonstrating universal aspects of language processing; thus, little attention was paid to the differences between modalities or to the specific aspects of the sign-modality. However, the wide recognition of sign languages as natural languages has supported a greater interest in furthering our understanding of modality specific factors (e.g., the use of proprioceptive and spatial information for phonological encoding or the greater potential for iconicity). This chapter offers a comprehensive overview of the most relevant studies of sign language comprehension and production that focus on the lexical level of processing. Results from behavioural studies, as well as evidence of similar neural substrates underlying speech and sign processing, have led to the widely accepted assumption that universal language processing principles can explain lexical access in both signed and spoken languages. However, although psycholinguistic and cognitive mechanisms as well as neural networks underlying speech and sign processing are strikingly similar, they are not identical. We propose that the study of the differences in processing of speech and signs can lead to a more complete picture of human language processing. Acknowledging these differences can also point researchers to factors influencing spoken language processing that might have been under-researched so far.",
    "metadata": {
      "title": "Lexical processing in sign language comprehension and production – experimental perspectives",
      "summary": "The study of sign language has received increasing interest in the last decades. Within this growing field, research on sign language processing – including both comprehension and production – has also received a remarkable boost in recent years. At initial stages of research, efforts were concentrated on demonstrating universal aspects of language processing; thus, little attention was paid to the differences between modalities or to the specific aspects of the sign-modality. However, the wide recognition of sign languages as natural languages has supported a greater interest in furthering our understanding of modality specific factors (e.g., the use of proprioceptive and spatial information for phonological encoding or the greater potential for iconicity). This chapter offers a comprehensive overview of the most relevant studies of sign language comprehension and production that focus on the lexical level of processing. Results from behavioural studies, as well as evidence of similar neural substrates underlying speech and sign processing, have led to the widely accepted assumption that universal language processing principles can explain lexical access in both signed and spoken languages. However, although psycholinguistic and cognitive mechanisms as well as neural networks underlying speech and sign processing are strikingly similar, they are not identical. We propose that the study of the differences in processing of speech and signs can lead to a more complete picture of human language processing. Acknowledging these differences can also point researchers to factors influencing spoken language processing that might have been under-researched so far.",
      "abstract": "The study of sign language has received increasing interest in the last decades. Within this growing field, research on sign language processing – including both comprehension and production – has also received a remarkable boost in recent years. At initial stages of research, efforts were concentrated on demonstrating universal aspects of language processing; thus, little attention was paid to the differences between modalities or to the specific aspects of the sign-modality. However, the wide recognition of sign languages as natural languages has supported a greater interest in furthering our understanding of modality specific factors (e.g., the use of proprioceptive and spatial information for phonological encoding or the greater potential for iconicity). This chapter offers a comprehensive overview of the most relevant studies of sign language comprehension and production that focus on the lexical level of processing. Results from behavioural studies, as well as evidence of similar neural substrates underlying speech and sign processing, have led to the widely accepted assumption that universal language processing principles can explain lexical access in both signed and spoken languages. However, although psycholinguistic and cognitive mechanisms as well as neural networks underlying speech and sign processing are strikingly similar, they are not identical. We propose that the study of the differences in processing of speech and signs can lead to a more complete picture of human language processing. Acknowledging these differences can also point researchers to factors influencing spoken language processing that might have been under-researched so far.",
      "doi": "https://doi.org/10.31219/osf.io/qr769",
      "openalex_id": "https://openalex.org/W4248506213",
      "arxiv_id": "",
      "publication_date": "2019-09-23",
      "published": "2019-09-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article reviews sign language lexical processing and does not propose or analyze discrete audio/speech tokenization, so it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article reviews sign language lexical processing and does not propose or analyze discrete audio/speech tokenization, so it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical processing in sign language comprehension and production, analyzing linguistic and cognitive aspects rather than discrete audio/speech tokenization or quantization of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio/speech tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical processing in sign language comprehension and production, analyzing linguistic and cognitive aspects rather than discrete audio/speech tokenization or quantization of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio/speech tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Communication and The Organization of The Lexicon",
    "abstract": "Abstract This chapter applies a language design perspective to the lexicon. It reviews and synthesizes a body of work in cognitive science and linguistics that uses ideas from computer science, specifically information theory, to explore how structural features of lexicons can be explained by principles of efficient communication. It pays particular attention to four major properties of lexicons. The first is the structure of word frequency distributions, particularly the Zipfian structure of these distributions and the way that individual semantic spaces are carved up so as to be maximally efficient. The second is the relationship between word frequency and properties like word length and phonotactic probability. The third concerns lexical arbitrariness: the extent to which word forms contain information about their meanings. Finally, the chapter considers how lexicons are structured for child language learning.",
    "metadata": {
      "title": "Efficient Communication and The Organization of The Lexicon",
      "summary": "Abstract This chapter applies a language design perspective to the lexicon. It reviews and synthesizes a body of work in cognitive science and linguistics that uses ideas from computer science, specifically information theory, to explore how structural features of lexicons can be explained by principles of efficient communication. It pays particular attention to four major properties of lexicons. The first is the structure of word frequency distributions, particularly the Zipfian structure of these distributions and the way that individual semantic spaces are carved up so as to be maximally efficient. The second is the relationship between word frequency and properties like word length and phonotactic probability. The third concerns lexical arbitrariness: the extent to which word forms contain information about their meanings. Finally, the chapter considers how lexicons are structured for child language learning.",
      "abstract": "Abstract This chapter applies a language design perspective to the lexicon. It reviews and synthesizes a body of work in cognitive science and linguistics that uses ideas from computer science, specifically information theory, to explore how structural features of lexicons can be explained by principles of efficient communication. It pays particular attention to four major properties of lexicons. The first is the structure of word frequency distributions, particularly the Zipfian structure of these distributions and the way that individual semantic spaces are carved up so as to be maximally efficient. The second is the relationship between word frequency and properties like word length and phonotactic probability. The third concerns lexical arbitrariness: the extent to which word forms contain information about their meanings. Finally, the chapter considers how lexicons are structured for child language learning.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.19",
      "openalex_id": "https://openalex.org/W4289716311",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokenization or token-based representations for speech; focuses on lexicon structure and information-theory without discrete speech tokens, so should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokenization or token-based representations for speech; focuses on lexicon structure and information-theory without discrete speech tokens, so should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on principles of efficient communication in the organization of the lexicon and does not mention discrete audio or speech tokenization, neural encoding, or any analysis related to converting continuous audio into discrete tokens, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on principles of efficient communication in the organization of the lexicon and does not mention discrete audio or speech tokenization, neural encoding, or any analysis related to converting continuous audio into discrete tokens, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Wifi Sensing And Networking With Channel State Information",
    "abstract": "In recent years, WiFi has a very rapid growth due to its high throughput, high efficiency, and low costs. Multiple-Input Multiple-Output (MIMO) and Orthogonal Frequency-Division Multiplexing (OFDM) are two key technologies for providing high throughput and efficiency for WiFi systems. MIMO-OFDM provides Channel State Information (CSI) which represents the amplitude attenuation and phase shift of each transmit-receiver antenna pair of each carrier frequency. CSI helps WiFi achieve high throughput to meet the growing demands of wireless data traffic. CSI captures how wireless signals travel through the surrounding environment, so it can also be used for wireless sensing purposes. This dissertation presents how to improve WiFi sensing and networking with CSI. More specifically, this dissertation proposes deep learning models to improve the performance and capability of WiFi sensing and presents network protocols to reduce CSI feedback overhead for high efficiency WiFi networking. For WiFi sensing, there are many wireless sensing applications using CSI as the input in recent years. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this dissertation presents a survey of signal processing techniques, algorithms, applications, performance results, challenges, and future trends of CSI-based WiFi sensing. CSI is widely used for gesture recognition and sign language recognition. Existing methods for WiFi-based sign language recognition have low accuracy and high costs when there are more than 200 sign gestures. The dissertation presents SignFi for sign language recognition using CSI and Convolutional Neural Networks (CNNs). SignFi provides high accuracy and low costs for run-time testing for 276 sign gestures in the lab and home environments. For WiFi networking, although CSI provides high throughput for WiFi networks, it also introduces high overhead. WiFi transmitters need CSI feedback for transmit beamforming and rate adaptation. The size of CSI packets is very large and it grows very fast with respect to the number of antennas and channel width. CSI feedback introduces high overhead which reduces the performance and efficiency of WiFi systems, especially mobile and hand-held WiFi devices. This dissertation presents RoFi to reduce CSI feedback overhead based on the mobility status of WiFi receivers. CSI feedback compression reduces overhead, but WiFi receivers still need to send CSI feedback to the WiFi transmitter. The dissertation presents EliMO for eliminating CSI feedback without sacrificing beamforming gains.",
    "metadata": {
      "title": "Improving Wifi Sensing And Networking With Channel State Information",
      "summary": "In recent years, WiFi has a very rapid growth due to its high throughput, high efficiency, and low costs. Multiple-Input Multiple-Output (MIMO) and Orthogonal Frequency-Division Multiplexing (OFDM) are two key technologies for providing high throughput and efficiency for WiFi systems. MIMO-OFDM provides Channel State Information (CSI) which represents the amplitude attenuation and phase shift of each transmit-receiver antenna pair of each carrier frequency. CSI helps WiFi achieve high throughput to meet the growing demands of wireless data traffic. CSI captures how wireless signals travel through the surrounding environment, so it can also be used for wireless sensing purposes. This dissertation presents how to improve WiFi sensing and networking with CSI. More specifically, this dissertation proposes deep learning models to improve the performance and capability of WiFi sensing and presents network protocols to reduce CSI feedback overhead for high efficiency WiFi networking. For WiFi sensing, there are many wireless sensing applications using CSI as the input in recent years. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this dissertation presents a survey of signal processing techniques, algorithms, applications, performance results, challenges, and future trends of CSI-based WiFi sensing. CSI is widely used for gesture recognition and sign language recognition. Existing methods for WiFi-based sign language recognition have low accuracy and high costs when there are more than 200 sign gestures. The dissertation presents SignFi for sign language recognition using CSI and Convolutional Neural Networks (CNNs). SignFi provides high accuracy and low costs for run-time testing for 276 sign gestures in the lab and home environments. For WiFi networking, although CSI provides high throughput for WiFi networks, it also introduces high overhead. WiFi transmitters need CSI feedback for transmit beamforming and rate adaptation. The size of CSI packets is very large and it grows very fast with respect to the number of antennas and channel width. CSI feedback introduces high overhead which reduces the performance and efficiency of WiFi systems, especially mobile and hand-held WiFi devices. This dissertation presents RoFi to reduce CSI feedback overhead based on the mobility status of WiFi receivers. CSI feedback compression reduces overhead, but WiFi receivers still need to send CSI feedback to the WiFi transmitter. The dissertation presents EliMO for eliminating CSI feedback without sacrificing beamforming gains.",
      "abstract": "In recent years, WiFi has a very rapid growth due to its high throughput, high efficiency, and low costs. Multiple-Input Multiple-Output (MIMO) and Orthogonal Frequency-Division Multiplexing (OFDM) are two key technologies for providing high throughput and efficiency for WiFi systems. MIMO-OFDM provides Channel State Information (CSI) which represents the amplitude attenuation and phase shift of each transmit-receiver antenna pair of each carrier frequency. CSI helps WiFi achieve high throughput to meet the growing demands of wireless data traffic. CSI captures how wireless signals travel through the surrounding environment, so it can also be used for wireless sensing purposes. This dissertation presents how to improve WiFi sensing and networking with CSI. More specifically, this dissertation proposes deep learning models to improve the performance and capability of WiFi sensing and presents network protocols to reduce CSI feedback overhead for high efficiency WiFi networking. For WiFi sensing, there are many wireless sensing applications using CSI as the input in recent years. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this dissertation presents a survey of signal processing techniques, algorithms, applications, performance results, challenges, and future trends of CSI-based WiFi sensing. CSI is widely used for gesture recognition and sign language recognition. Existing methods for WiFi-based sign language recognition have low accuracy and high costs when there are more than 200 sign gestures. The dissertation presents SignFi for sign language recognition using CSI and Convolutional Neural Networks (CNNs). SignFi provides high accuracy and low costs for run-time testing for 276 sign gestures in the lab and home environments. For WiFi networking, although CSI provides high throughput for WiFi networks, it also introduces high overhead. WiFi transmitters need CSI feedback for transmit beamforming and rate adaptation. The size of CSI packets is very large and it grows very fast with respect to the number of antennas and channel width. CSI feedback introduces high overhead which reduces the performance and efficiency of WiFi systems, especially mobile and hand-held WiFi devices. This dissertation presents RoFi to reduce CSI feedback overhead based on the mobility status of WiFi receivers. CSI feedback compression reduces overhead, but WiFi receivers still need to send CSI feedback to the WiFi transmitter. The dissertation presents EliMO for eliminating CSI feedback without sacrificing beamforming gains.",
      "doi": "https://doi.org/10.21220/s2-dwgg-3j27",
      "openalex_id": "https://openalex.org/W3080097142",
      "arxiv_id": "",
      "publication_date": "2020-06-25",
      "published": "2020-06-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on improving WiFi sensing and networking using Channel State Information (CSI), including gesture/sign language recognition via CSI, but it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or language tasks; therefore it does not meet the inclusion criteria and fails the exclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on improving WiFi sensing and networking using Channel State Information (CSI), including gesture/sign language recognition via CSI, but it does not propose, analyze, or evaluate discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or language tasks; therefore it does not meet the inclusion criteria and fails the exclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on WiFi sensing and networking using Channel State Information (CSI) and does not involve any discrete audio/speech tokenization or related audio signal processing tasks; therefore, it does not meet the inclusion criteria centered on audio tokenization and is irrelevant to the tokenization-based audio analysis described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on WiFi sensing and networking using Channel State Information (CSI) and does not involve any discrete audio/speech tokenization or related audio signal processing tasks; therefore, it does not meet the inclusion criteria centered on audio tokenization and is irrelevant to the tokenization-based audio analysis described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Systematicity and Arbitrariness In Language",
    "abstract": "Abstract This chapter reassesses two traditional ideas about the nature of language—Saussurean arbitrariness and Humboldtian infinity—in light of the modern study of linguistic productivity, language acquisition, and cognitive science. It reviews evidence from child language that productivity is categorical, which enables the language user to overcome the arbitrariness of words with systematic generalizations. Such a conception of productivity place severe constraints on the language learning mechanism that children use to form productive generalizations from a finite number of examples. Special focus is given to the Tolerance Principle, which appears to underlie the acquisition of rules in phonology, morphology, and syntax. The connection between a learning-theoretic approach to language and the structural theory of language is discussed, which may also elucidate the role of language acquisition in children’s conceptual development.",
    "metadata": {
      "title": "Systematicity and Arbitrariness In Language",
      "summary": "Abstract This chapter reassesses two traditional ideas about the nature of language—Saussurean arbitrariness and Humboldtian infinity—in light of the modern study of linguistic productivity, language acquisition, and cognitive science. It reviews evidence from child language that productivity is categorical, which enables the language user to overcome the arbitrariness of words with systematic generalizations. Such a conception of productivity place severe constraints on the language learning mechanism that children use to form productive generalizations from a finite number of examples. Special focus is given to the Tolerance Principle, which appears to underlie the acquisition of rules in phonology, morphology, and syntax. The connection between a learning-theoretic approach to language and the structural theory of language is discussed, which may also elucidate the role of language acquisition in children’s conceptual development.",
      "abstract": "Abstract This chapter reassesses two traditional ideas about the nature of language—Saussurean arbitrariness and Humboldtian infinity—in light of the modern study of linguistic productivity, language acquisition, and cognitive science. It reviews evidence from child language that productivity is categorical, which enables the language user to overcome the arbitrariness of words with systematic generalizations. Such a conception of productivity place severe constraints on the language learning mechanism that children use to form productive generalizations from a finite number of examples. Special focus is given to the Tolerance Principle, which appears to underlie the acquisition of rules in phonology, morphology, and syntax. The connection between a learning-theoretic approach to language and the structural theory of language is discussed, which may also elucidate the role of language acquisition in children’s conceptual development.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.9",
      "openalex_id": "https://openalex.org/W4212864350",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on theoretical linguistics and cognitive aspects of language without any discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on theoretical linguistics and cognitive aspects of language without any discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on linguistic theory related to language productivity, acquisition, and cognitive aspects, without any mention or analysis of discrete audio/speech tokenization or related methods, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on linguistic theory related to language productivity, acquisition, and cognitive aspects, without any mention or analysis of discrete audio/speech tokenization or related methods, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Language and thought",
    "abstract": "Abstract This chapter explores the relationship between language and thought, focusing on whether changes in non-linguistic thought follow from learning a particular language (e.g., English vs. Greek) and/or from learning any natural language at all. The chapter considers several different hypotheses about the possible effects of language on our non-linguistic representation, including classical and neo-classical versions of Whorf’s hypothesis as well as more recent views. The latter include one proposal suggesting that learning a language causes massive reorganization of our non-linguistic thought, and one suggesting that language more likely causes pervasive momentary on-line effects of linguistic understanding while leaving non-linguistic representations intact. These views are considered across a number of different domains including color, space, number, and theory of mind.",
    "metadata": {
      "title": "Language and thought",
      "summary": "Abstract This chapter explores the relationship between language and thought, focusing on whether changes in non-linguistic thought follow from learning a particular language (e.g., English vs. Greek) and/or from learning any natural language at all. The chapter considers several different hypotheses about the possible effects of language on our non-linguistic representation, including classical and neo-classical versions of Whorf’s hypothesis as well as more recent views. The latter include one proposal suggesting that learning a language causes massive reorganization of our non-linguistic thought, and one suggesting that language more likely causes pervasive momentary on-line effects of linguistic understanding while leaving non-linguistic representations intact. These views are considered across a number of different domains including color, space, number, and theory of mind.",
      "abstract": "Abstract This chapter explores the relationship between language and thought, focusing on whether changes in non-linguistic thought follow from learning a particular language (e.g., English vs. Greek) and/or from learning any natural language at all. The chapter considers several different hypotheses about the possible effects of language on our non-linguistic representation, including classical and neo-classical versions of Whorf’s hypothesis as well as more recent views. The latter include one proposal suggesting that learning a language causes massive reorganization of our non-linguistic thought, and one suggesting that language more likely causes pervasive momentary on-line effects of linguistic understanding while leaving non-linguistic representations intact. These views are considered across a number of different domains including color, space, number, and theory of mind.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.5",
      "openalex_id": "https://openalex.org/W4213305108",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses language and thought theories (e.g., Whorfian hypotheses) and their impact on non-linguistic cognition, but it does not propose, analyze, or evaluate discrete audio tokenization or token-based representations for speech, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses language and thought theories (e.g., Whorfian hypotheses) and their impact on non-linguistic cognition, but it does not propose, analyze, or evaluate discrete audio tokenization or token-based representations for speech, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The chapter focuses on the relationship between language and thought, discussing hypotheses about language effects on non-linguistic representation, but does not address discrete audio/speech tokenization or any form of audio signal processing or tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The chapter focuses on the relationship between language and thought, discussing hypotheses about language effects on non-linguistic representation, but does not address discrete audio/speech tokenization or any form of audio signal processing or tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language",
    "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
    "metadata": {
      "title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language",
      "summary": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
      "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
      "doi": "https://doi.org/10.48550/arxiv.2203.06096",
      "openalex_id": "https://openalex.org/W4221167884",
      "arxiv_id": "",
      "publication_date": "2022-03-11",
      "published": "2022-03-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on a dataset and phonology of American Sign Language and discusses graph-based methods for recognizing phonological properties, not discrete audio/speech tokenization; thus it does not meet the inclusion criteria focused on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on a dataset and phonology of American Sign Language and discusses graph-based methods for recognizing phonological properties, not discrete audio/speech tokenization; thus it does not meet the inclusion criteria focused on discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on recognizing phonological properties in American Sign Language using a dataset and graph-based neural networks analyzing visual skeleton features, without addressing discrete audio/speech tokenization or discrete tokenization of audio signals as required by the inclusion criteria. Since there is no indication of audio signal processing or discrete audio token analysis, it fails to meet the inclusion criteria and falls under exclusion criteria related to absence of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on recognizing phonological properties in American Sign Language using a dataset and graph-based neural networks analyzing visual skeleton features, without addressing discrete audio/speech tokenization or discrete tokenization of audio signals as required by the inclusion criteria. Since there is no indication of audio signal processing or discrete audio token analysis, it fails to meet the inclusion criteria and falls under exclusion criteria related to absence of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Effects of Adjuvant Chemotherapy on Insulin Resistance in Patients with Early Breast Cancer",
    "abstract": "Objective: To assess the effect of adjuvant chemotherapy on insulin resistance in patients with early breast cancer. Material and Methods: Twenty-three non-diabetic patients were included. Patients were prospectively evaluated before, during, and after chemotherapy. Demographic, anthropometric, histopathological features, and treatment data were recorded. Blood samples were taken to evaluate fasting blood glucose, fasting insulin levels, and HbA1c. Homeostatic model assessment for insulin resistance (HOMA-IR) score measured using fasting blood glucose and fasting insulin levels. Results: Overall, pre- and post-chemotherapy mean weights were comparable (70.17 kg vs. 71.43). Prechemotherapy mean HOMA-IR was 4.99 and significantly higher than the control group of the healthy population (p=0.008). The mean values of the HOMA-IR score before, during, and after chemotherapy were 4.99, 3.47, and 3.13, respectively. Although the mean HOMA-IR decreased after chemotherapy, these decreases were not statistically significant (p=0.089). The mean fasting glucose levels before, during, and after chemotherapy were 95.5, 101.9, and 94.1 mg/dL, respectively. Before, during, and after chemotherapy, the mean fasting insulin levels were 21.43, 13.32, and 13.28 &#956;IU/mL, respectively. Conclusion: In the study, we observed a higher rate of insulin resistance in patients with breast cancer. The mean values of the HOMA-IR score decreased during and after chemotherapy.",
    "metadata": {
      "title": "Effects of Adjuvant Chemotherapy on Insulin Resistance in Patients with Early Breast Cancer",
      "summary": "Objective: To assess the effect of adjuvant chemotherapy on insulin resistance in patients with early breast cancer. Material and Methods: Twenty-three non-diabetic patients were included. Patients were prospectively evaluated before, during, and after chemotherapy. Demographic, anthropometric, histopathological features, and treatment data were recorded. Blood samples were taken to evaluate fasting blood glucose, fasting insulin levels, and HbA1c. Homeostatic model assessment for insulin resistance (HOMA-IR) score measured using fasting blood glucose and fasting insulin levels. Results: Overall, pre- and post-chemotherapy mean weights were comparable (70.17 kg vs. 71.43). Prechemotherapy mean HOMA-IR was 4.99 and significantly higher than the control group of the healthy population (p=0.008). The mean values of the HOMA-IR score before, during, and after chemotherapy were 4.99, 3.47, and 3.13, respectively. Although the mean HOMA-IR decreased after chemotherapy, these decreases were not statistically significant (p=0.089). The mean fasting glucose levels before, during, and after chemotherapy were 95.5, 101.9, and 94.1 mg/dL, respectively. Before, during, and after chemotherapy, the mean fasting insulin levels were 21.43, 13.32, and 13.28 &#956;IU/mL, respectively. Conclusion: In the study, we observed a higher rate of insulin resistance in patients with breast cancer. The mean values of the HOMA-IR score decreased during and after chemotherapy.",
      "abstract": "Objective: To assess the effect of adjuvant chemotherapy on insulin resistance in patients with early breast cancer. Material and Methods: Twenty-three non-diabetic patients were included. Patients were prospectively evaluated before, during, and after chemotherapy. Demographic, anthropometric, histopathological features, and treatment data were recorded. Blood samples were taken to evaluate fasting blood glucose, fasting insulin levels, and HbA1c. Homeostatic model assessment for insulin resistance (HOMA-IR) score measured using fasting blood glucose and fasting insulin levels. Results: Overall, pre- and post-chemotherapy mean weights were comparable (70.17 kg vs. 71.43). Prechemotherapy mean HOMA-IR was 4.99 and significantly higher than the control group of the healthy population (p=0.008). The mean values of the HOMA-IR score before, during, and after chemotherapy were 4.99, 3.47, and 3.13, respectively. Although the mean HOMA-IR decreased after chemotherapy, these decreases were not statistically significant (p=0.089). The mean fasting glucose levels before, during, and after chemotherapy were 95.5, 101.9, and 94.1 mg/dL, respectively. Before, during, and after chemotherapy, the mean fasting insulin levels were 21.43, 13.32, and 13.28 &#956;IU/mL, respectively. Conclusion: In the study, we observed a higher rate of insulin resistance in patients with breast cancer. The mean values of the HOMA-IR score decreased during and after chemotherapy.",
      "doi": "https://doi.org/10.4274/csmedj.galenos.2022.2022-1-7",
      "openalex_id": "https://openalex.org/W4225502224",
      "arxiv_id": "",
      "publication_date": "2022-04-08",
      "published": "2022-04-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not involve discrete audio/speech tokenization or tokenization analysis; it is a clinical study on chemotherapy and insulin resistance with no speech/audio tokenization contribution.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not involve discrete audio/speech tokenization or tokenization analysis; it is a clinical study on chemotherapy and insulin resistance with no speech/audio tokenization contribution.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the medical effects of adjuvant chemotherapy on insulin resistance in breast cancer patients and does not discuss any aspect of audio/speech tokenization, discrete audio representations, or related computational methods, thus failing entirely to meet the inclusion criteria for studies on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the medical effects of adjuvant chemotherapy on insulin resistance in breast cancer patients and does not discuss any aspect of audio/speech tokenization, discrete audio representations, or related computational methods, thus failing entirely to meet the inclusion criteria for studies on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bi-directional cross-language activation in Chinese Sign Language (CSL)-Chinese bimodal bilinguals",
    "abstract": "In bilingual word recognition, cross-language activation has been found in unimodal bilinguals (e.g., Chinese-English bilinguals) and bimodal bilinguals (e.g., American Sign language-English bilinguals). However, it remains unclear how signs' phonological parameters, spoken words' orthographic and phonological representation, and language proficiency affect cross-language activation in bimodal bilinguals. To resolve the issues, we recruited deaf Chinese sign language (CSL)-Chinese bimodal bilinguals as participants. We conducted two experiments with the implicit priming paradigm and the semantic relatedness decision task. Experiment 1 first showed cross-language activation from Chinese to CSL, and the CSL words' phonological parameter affected the cross-language activation. Experiment 2 further revealed inverse cross-language activation from CSL to Chinese. The Chinese words' orthographic and phonological representation played a similar role in the cross-language activation. Moreover, a comparison between Experiments 1 and 2 indicated that language proficiency influenced cross-language activation. The findings were further discussed with the Bilingual Interactive Activation Plus (BIA+) model, the deaf BIA+ model, and the Bilingual Language Interaction Network for Comprehension of Speech (BLINCS) model.",
    "metadata": {
      "title": "Bi-directional cross-language activation in Chinese Sign Language (CSL)-Chinese bimodal bilinguals",
      "summary": "In bilingual word recognition, cross-language activation has been found in unimodal bilinguals (e.g., Chinese-English bilinguals) and bimodal bilinguals (e.g., American Sign language-English bilinguals). However, it remains unclear how signs' phonological parameters, spoken words' orthographic and phonological representation, and language proficiency affect cross-language activation in bimodal bilinguals. To resolve the issues, we recruited deaf Chinese sign language (CSL)-Chinese bimodal bilinguals as participants. We conducted two experiments with the implicit priming paradigm and the semantic relatedness decision task. Experiment 1 first showed cross-language activation from Chinese to CSL, and the CSL words' phonological parameter affected the cross-language activation. Experiment 2 further revealed inverse cross-language activation from CSL to Chinese. The Chinese words' orthographic and phonological representation played a similar role in the cross-language activation. Moreover, a comparison between Experiments 1 and 2 indicated that language proficiency influenced cross-language activation. The findings were further discussed with the Bilingual Interactive Activation Plus (BIA+) model, the deaf BIA+ model, and the Bilingual Language Interaction Network for Comprehension of Speech (BLINCS) model.",
      "abstract": "In bilingual word recognition, cross-language activation has been found in unimodal bilinguals (e.g., Chinese-English bilinguals) and bimodal bilinguals (e.g., American Sign language-English bilinguals). However, it remains unclear how signs' phonological parameters, spoken words' orthographic and phonological representation, and language proficiency affect cross-language activation in bimodal bilinguals. To resolve the issues, we recruited deaf Chinese sign language (CSL)-Chinese bimodal bilinguals as participants. We conducted two experiments with the implicit priming paradigm and the semantic relatedness decision task. Experiment 1 first showed cross-language activation from Chinese to CSL, and the CSL words' phonological parameter affected the cross-language activation. Experiment 2 further revealed inverse cross-language activation from CSL to Chinese. The Chinese words' orthographic and phonological representation played a similar role in the cross-language activation. Moreover, a comparison between Experiments 1 and 2 indicated that language proficiency influenced cross-language activation. The findings were further discussed with the Bilingual Interactive Activation Plus (BIA+) model, the deaf BIA+ model, and the Bilingual Language Interaction Network for Comprehension of Speech (BLINCS) model.",
      "doi": "https://doi.org/10.1016/j.actpsy.2022.103693",
      "openalex_id": "https://openalex.org/W4289778574",
      "arxiv_id": "",
      "publication_date": "2022-08-04",
      "published": "2022-08-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates cross-language activation in CSL-Chinese bimodal bilinguals and analyzes phonological/orthographic representations, but it does not involve discrete audio/speech tokenization as a core objective (no analysis of converting continuous audio into discrete tokens), so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates cross-language activation in CSL-Chinese bimodal bilinguals and analyzes phonological/orthographic representations, but it does not involve discrete audio/speech tokenization as a core objective (no analysis of converting continuous audio into discrete tokens), so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cross-language activation in bimodal bilinguals involving sign language and Chinese spoken/written words, analyzing language processing and phonological parameters rather than proposing or evaluating discrete audio/speech tokenization methods converting continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to discrete audio tokenization or neural encoding-based discrete token representations relevant to language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cross-language activation in bimodal bilinguals involving sign language and Chinese spoken/written words, analyzing language processing and phonological parameters rather than proposing or evaluating discrete audio/speech tokenization methods converting continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to discrete audio tokenization or neural encoding-based discrete token representations relevant to language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign learning of hearing children in inclusive day care centers—does iconicity matter?",
    "abstract": "An increasing number of experimental studies suggest that signs and gestures can scaffold vocabulary learning for children with and without special educational needs and/or disabilities (SEND). However, little research has been done on the extent to which iconicity plays a role in sign learning, particularly in inclusive day care centers. This current study investigated the role of iconicity in the sign learning of 145 hearing children (2;1 to 6;3 years) from inclusive day care centers with educators who started using sign-supported speech after a training module. Children’s sign use was assessed via a questionnaire completed by their educators. We found that older children were more likely to learn signs with a higher degree of iconicity, whereas the learning of signs by younger children was less affected by iconicity. Children with SEND did not benefit more from iconicity than children without SEND. These results suggest that whether iconicity plays a role in sign learning depends on the age of the children.",
    "metadata": {
      "title": "Sign learning of hearing children in inclusive day care centers—does iconicity matter?",
      "summary": "An increasing number of experimental studies suggest that signs and gestures can scaffold vocabulary learning for children with and without special educational needs and/or disabilities (SEND). However, little research has been done on the extent to which iconicity plays a role in sign learning, particularly in inclusive day care centers. This current study investigated the role of iconicity in the sign learning of 145 hearing children (2;1 to 6;3 years) from inclusive day care centers with educators who started using sign-supported speech after a training module. Children’s sign use was assessed via a questionnaire completed by their educators. We found that older children were more likely to learn signs with a higher degree of iconicity, whereas the learning of signs by younger children was less affected by iconicity. Children with SEND did not benefit more from iconicity than children without SEND. These results suggest that whether iconicity plays a role in sign learning depends on the age of the children.",
      "abstract": "An increasing number of experimental studies suggest that signs and gestures can scaffold vocabulary learning for children with and without special educational needs and/or disabilities (SEND). However, little research has been done on the extent to which iconicity plays a role in sign learning, particularly in inclusive day care centers. This current study investigated the role of iconicity in the sign learning of 145 hearing children (2;1 to 6;3 years) from inclusive day care centers with educators who started using sign-supported speech after a training module. Children’s sign use was assessed via a questionnaire completed by their educators. We found that older children were more likely to learn signs with a higher degree of iconicity, whereas the learning of signs by younger children was less affected by iconicity. Children with SEND did not benefit more from iconicity than children without SEND. These results suggest that whether iconicity plays a role in sign learning depends on the age of the children.",
      "doi": "https://doi.org/10.3389/fpsyg.2023.1196114",
      "openalex_id": "https://openalex.org/W4385950012",
      "arxiv_id": "",
      "publication_date": "2023-08-16",
      "published": "2023-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on sign learning and the role of iconicity in sign-supported speech, with no discrete audio/tokenized representations of speech or analysis of tokenization, so it does not meet the tokenization-centric inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on sign learning and the role of iconicity in sign-supported speech, with no discrete audio/tokenized representations of speech or analysis of tokenization, so it does not meet the tokenization-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign learning and iconicity in hearing children within inclusive day care centers and does not address discrete audio/speech tokenization, nor does it discuss any discrete quantization or token design related to audio signals; thus, it does not meet the inclusion criteria and falls outside the scope defined for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign learning and iconicity in hearing children within inclusive day care centers and does not address discrete audio/speech tokenization, nor does it discuss any discrete quantization or token design related to audio signals; thus, it does not meet the inclusion criteria and falls outside the scope defined for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign Language Production: A Review",
    "abstract": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
    "metadata": {
      "title": "Sign Language Production: A Review",
      "summary": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
      "abstract": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
      "doi": "https://doi.org/10.1109/cvprw53098.2021.00384",
      "openalex_id": "https://openalex.org/W3141790718",
      "arxiv_id": "",
      "publication_date": "2021-06-01",
      "published": "2021-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item is a survey on Sign Language Production using deep learning and does not explicitly propose, analyze, or evaluate discrete audio/tokenization of speech into a finite vocabulary; it focuses on sign language production/recognition systems rather than discrete audio token representations, so it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item is a survey on Sign Language Production using deep learning and does not explicitly propose, analyze, or evaluate discrete audio/tokenization of speech into a finite vocabulary; it focuses on sign language production/recognition systems rather than discrete audio token representations, so it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language production using deep learning and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbols; hence it does not meet the inclusion criteria related to discrete audio tokenization and falls into exclusion categories.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language production using deep learning and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete symbols; hence it does not meet the inclusion criteria related to discrete audio tokenization and falls into exclusion categories.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language",
    "abstract": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
    "metadata": {
      "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language",
      "summary": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
      "abstract": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
      "doi": "https://doi.org/10.48550/arxiv.2003.08759",
      "openalex_id": "https://openalex.org/W3084410123",
      "arxiv_id": "",
      "publication_date": "2020-03-03",
      "published": "2020-03-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about a facial expression dataset for sign language and does not involve discrete audio tokenization or analysis of discrete audio tokens, so it fails the inclusion criteria focused on quantizing audio into a finite vocabulary; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about a facial expression dataset for sign language and does not involve discrete audio tokenization or analysis of discrete audio tokens, so it fails the inclusion criteria focused on quantizing audio into a finite vocabulary; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an annotated facial expression dataset in sign language and does not involve any discrete audio or speech tokenization, nor does it analyze discrete token representations from audio signals; therefore, it does not meet the inclusion criteria related to audio tokenization and discrete audio symbol sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an annotated facial expression dataset in sign language and does not involve any discrete audio or speech tokenization, nor does it analyze discrete token representations from audio signals; therefore, it does not meet the inclusion criteria related to audio tokenization and discrete audio symbol sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonological Abstraction in The Mental Lexicon",
    "abstract": "Abstract In this chapter, we examine the nature of the long-term memory representation of the pronunciations of words. A fundamental question concerns how abstract these representations are vis à vis the physical manifestation of words, both as gestures and as physical percepts. We consider this question and related issues within the traditions of linguistic cognition and generative phonology. We first explore the general nature of abstraction, and then review the arguments in generative phonology for positing that the units of speech stored in long-term memory (so called ‘underlying forms’) abstract away from many phonetic details. Motivations for concepts such as phonemes and distinctive phonological features are given. We then visit the open question regarding how abstract underlying forms may be allowed to be. We conclude by highlighting the contributions that evidence from neuroscience and sign language linguistics brings to these issues of phonological abstraction in the mental lexicon.",
    "metadata": {
      "title": "Phonological Abstraction in The Mental Lexicon",
      "summary": "Abstract In this chapter, we examine the nature of the long-term memory representation of the pronunciations of words. A fundamental question concerns how abstract these representations are vis à vis the physical manifestation of words, both as gestures and as physical percepts. We consider this question and related issues within the traditions of linguistic cognition and generative phonology. We first explore the general nature of abstraction, and then review the arguments in generative phonology for positing that the units of speech stored in long-term memory (so called ‘underlying forms’) abstract away from many phonetic details. Motivations for concepts such as phonemes and distinctive phonological features are given. We then visit the open question regarding how abstract underlying forms may be allowed to be. We conclude by highlighting the contributions that evidence from neuroscience and sign language linguistics brings to these issues of phonological abstraction in the mental lexicon.",
      "abstract": "Abstract In this chapter, we examine the nature of the long-term memory representation of the pronunciations of words. A fundamental question concerns how abstract these representations are vis à vis the physical manifestation of words, both as gestures and as physical percepts. We consider this question and related issues within the traditions of linguistic cognition and generative phonology. We first explore the general nature of abstraction, and then review the arguments in generative phonology for positing that the units of speech stored in long-term memory (so called ‘underlying forms’) abstract away from many phonetic details. Motivations for concepts such as phonemes and distinctive phonological features are given. We then visit the open question regarding how abstract underlying forms may be allowed to be. We conclude by highlighting the contributions that evidence from neuroscience and sign language linguistics brings to these issues of phonological abstraction in the mental lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.14",
      "openalex_id": "https://openalex.org/W4212983170",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses abstract phonological representations and underlying forms, not discrete audio tokenization or token-level quantization of speech; it does not propose, analyze, or evaluate discrete tokens for audio reconstruction or speech tasks, thus failing inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses abstract phonological representations and underlying forms, not discrete audio tokenization or token-level quantization of speech; it does not propose, analyze, or evaluate discrete tokens for audio reconstruction or speech tasks, thus failing inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological abstraction in the mental lexicon, examining linguistic and cognitive representations of speech sounds rather than proposing, analyzing, or evaluating discrete audio/speech tokenization methods that convert continuous audio into discrete symbols for computational tasks. Consequently, it does not meet the inclusion criteria centered on discrete tokenization of continuous audio signals for computational audio reconstruction or related purposes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological abstraction in the mental lexicon, examining linguistic and cognitive representations of speech sounds rather than proposing, analyzing, or evaluating discrete audio/speech tokenization methods that convert continuous audio into discrete symbols for computational tasks. Consequently, it does not meet the inclusion criteria centered on discrete tokenization of continuous audio signals for computational audio reconstruction or related purposes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Folk Definitions as a Model for Sign Language Dictionary Definitions: A User-Focused Study of the Online Dictionary of New Zealand Sign Language",
    "abstract": "&lt;p&gt;This thesis addresses the question whether signed definitions, made possible by advances in electronic lexicography, should be introduced to sign language dictionaries. The thesis comprises four interrelated studies investigating different aspects of this question through a user-focused case study of the Online Dictionary of New Zealand Sign Language (ODNZSL). A preliminary study investigated current use of the ODNZSL in order to identify what user needs signed definitions might fulfil. The study drew on two data sets: website log data for the ODNZSL, and a think-aloud protocol and interview with representatives of user groups. Results showed that in addition to a large volume of casual browsers, the most frequent and intensive users of the dictionary are beginner and intermediate students of New Zealand Sign Language (NZSL). These (hearing) language learners mostly search for frequent vocabulary with the aims of language production and vocabulary learning. Findings also identified reasons for unsuccessful dictionary consultations that may impact on the effectiveness of definitions. In the second study, a review of ODNZSL entries highlighted categories of lexical items for which the current description through English glosses, examples, and usage notes is inadequate. A test was developed to assess whether these categories of signs were problematic for the user group identified in the first study: hearing intermediate learners of NZSL. Twenty-one participants took a computer-based error correction test with both comprehension and production sections comprising fifty items in six different categories: culture-bound; idiomatic; polysemous; metaphoric/metonymic; vocabulary type / word class; and other. Quantitative results indicated that a small number of test items were problematic, but that none of the test categories were good predictors of the difficulties learners experienced. A qualitative examination identified linguistic factors and issues with the current dictionary information that may be improved by the addition of signed definitions. The central proposition tested in the third study was that folk definitions—informal explanations of sign meaning by Deaf sign language users—can be applied as a template for dictionary definitions. This study took fifteen of the signs that were identified as problematic for learners in the previous study, and asked thirteen Deaf NZSL users to explain the meaning of these signs. A qualitative analysis found that the folk definitions by different NZSL users shared common semantic categories and embedded information about situational and sociolinguistic variation as well as grammatical structures. Some semantic relationships that occur frequently in spoken language folk definitions, such as exemplification and synonymy, were also common in signed folk definitions. Other semantic relationships such as attribution, function, operation, and spatial relationships occurred less frequently because they were inherent in the sign construction. Due to the bilingual status of the participants, many folk definitions included reference to English words in the form of mouth patterns and fingerspelling. In the fourth study, twelve pilot dictionary definitions were created on the basis of common features found in the folk definitions and an evaluation of definition formats by Deaf NZSL users. The error correction test from the second study was repeated with a new cohort of intermediate NZSL learners. This time twelve test items were accompanied by a pilot definition; for the remaining items participants were shown a video example sentence from the ODNZSL entry. Results showed no significant improvements in scores for the test items with definitions. However, feedback from test participants showed that the definitions were comprehensible and perceived as valuable for language learning. The overall conclusion of these studies is that a selective approach should be taken to introducing signed definitions in existing multifunctional sign language dictionaries. For hearing learners of sign language, signed definitions do not meet immediate communicative (comprehension and production) needs, but they may contribute to wider vocabulary learning goals. The main contribution of this thesis is that it suggests a user-focused methodology for creating signed definitions, driven by evidence from the first empirical user study of an online sign language dictionary and therefore taking into account the particular challenges of sign language lexicography. Furthermore, the analysis of features of signed folk definitions contributes to the semantic description of sign languages.&lt;/p&gt;",
    "metadata": {
      "title": "Folk Definitions as a Model for Sign Language Dictionary Definitions: A User-Focused Study of the Online Dictionary of New Zealand Sign Language",
      "summary": "&lt;p&gt;This thesis addresses the question whether signed definitions, made possible by advances in electronic lexicography, should be introduced to sign language dictionaries. The thesis comprises four interrelated studies investigating different aspects of this question through a user-focused case study of the Online Dictionary of New Zealand Sign Language (ODNZSL). A preliminary study investigated current use of the ODNZSL in order to identify what user needs signed definitions might fulfil. The study drew on two data sets: website log data for the ODNZSL, and a think-aloud protocol and interview with representatives of user groups. Results showed that in addition to a large volume of casual browsers, the most frequent and intensive users of the dictionary are beginner and intermediate students of New Zealand Sign Language (NZSL). These (hearing) language learners mostly search for frequent vocabulary with the aims of language production and vocabulary learning. Findings also identified reasons for unsuccessful dictionary consultations that may impact on the effectiveness of definitions. In the second study, a review of ODNZSL entries highlighted categories of lexical items for which the current description through English glosses, examples, and usage notes is inadequate. A test was developed to assess whether these categories of signs were problematic for the user group identified in the first study: hearing intermediate learners of NZSL. Twenty-one participants took a computer-based error correction test with both comprehension and production sections comprising fifty items in six different categories: culture-bound; idiomatic; polysemous; metaphoric/metonymic; vocabulary type / word class; and other. Quantitative results indicated that a small number of test items were problematic, but that none of the test categories were good predictors of the difficulties learners experienced. A qualitative examination identified linguistic factors and issues with the current dictionary information that may be improved by the addition of signed definitions. The central proposition tested in the third study was that folk definitions—informal explanations of sign meaning by Deaf sign language users—can be applied as a template for dictionary definitions. This study took fifteen of the signs that were identified as problematic for learners in the previous study, and asked thirteen Deaf NZSL users to explain the meaning of these signs. A qualitative analysis found that the folk definitions by different NZSL users shared common semantic categories and embedded information about situational and sociolinguistic variation as well as grammatical structures. Some semantic relationships that occur frequently in spoken language folk definitions, such as exemplification and synonymy, were also common in signed folk definitions. Other semantic relationships such as attribution, function, operation, and spatial relationships occurred less frequently because they were inherent in the sign construction. Due to the bilingual status of the participants, many folk definitions included reference to English words in the form of mouth patterns and fingerspelling. In the fourth study, twelve pilot dictionary definitions were created on the basis of common features found in the folk definitions and an evaluation of definition formats by Deaf NZSL users. The error correction test from the second study was repeated with a new cohort of intermediate NZSL learners. This time twelve test items were accompanied by a pilot definition; for the remaining items participants were shown a video example sentence from the ODNZSL entry. Results showed no significant improvements in scores for the test items with definitions. However, feedback from test participants showed that the definitions were comprehensible and perceived as valuable for language learning. The overall conclusion of these studies is that a selective approach should be taken to introducing signed definitions in existing multifunctional sign language dictionaries. For hearing learners of sign language, signed definitions do not meet immediate communicative (comprehension and production) needs, but they may contribute to wider vocabulary learning goals. The main contribution of this thesis is that it suggests a user-focused methodology for creating signed definitions, driven by evidence from the first empirical user study of an online sign language dictionary and therefore taking into account the particular challenges of sign language lexicography. Furthermore, the analysis of features of signed folk definitions contributes to the semantic description of sign languages.&lt;/p&gt;",
      "abstract": "&lt;p&gt;This thesis addresses the question whether signed definitions, made possible by advances in electronic lexicography, should be introduced to sign language dictionaries. The thesis comprises four interrelated studies investigating different aspects of this question through a user-focused case study of the Online Dictionary of New Zealand Sign Language (ODNZSL). A preliminary study investigated current use of the ODNZSL in order to identify what user needs signed definitions might fulfil. The study drew on two data sets: website log data for the ODNZSL, and a think-aloud protocol and interview with representatives of user groups. Results showed that in addition to a large volume of casual browsers, the most frequent and intensive users of the dictionary are beginner and intermediate students of New Zealand Sign Language (NZSL). These (hearing) language learners mostly search for frequent vocabulary with the aims of language production and vocabulary learning. Findings also identified reasons for unsuccessful dictionary consultations that may impact on the effectiveness of definitions. In the second study, a review of ODNZSL entries highlighted categories of lexical items for which the current description through English glosses, examples, and usage notes is inadequate. A test was developed to assess whether these categories of signs were problematic for the user group identified in the first study: hearing intermediate learners of NZSL. Twenty-one participants took a computer-based error correction test with both comprehension and production sections comprising fifty items in six different categories: culture-bound; idiomatic; polysemous; metaphoric/metonymic; vocabulary type / word class; and other. Quantitative results indicated that a small number of test items were problematic, but that none of the test categories were good predictors of the difficulties learners experienced. A qualitative examination identified linguistic factors and issues with the current dictionary information that may be improved by the addition of signed definitions. The central proposition tested in the third study was that folk definitions—informal explanations of sign meaning by Deaf sign language users—can be applied as a template for dictionary definitions. This study took fifteen of the signs that were identified as problematic for learners in the previous study, and asked thirteen Deaf NZSL users to explain the meaning of these signs. A qualitative analysis found that the folk definitions by different NZSL users shared common semantic categories and embedded information about situational and sociolinguistic variation as well as grammatical structures. Some semantic relationships that occur frequently in spoken language folk definitions, such as exemplification and synonymy, were also common in signed folk definitions. Other semantic relationships such as attribution, function, operation, and spatial relationships occurred less frequently because they were inherent in the sign construction. Due to the bilingual status of the participants, many folk definitions included reference to English words in the form of mouth patterns and fingerspelling. In the fourth study, twelve pilot dictionary definitions were created on the basis of common features found in the folk definitions and an evaluation of definition formats by Deaf NZSL users. The error correction test from the second study was repeated with a new cohort of intermediate NZSL learners. This time twelve test items were accompanied by a pilot definition; for the remaining items participants were shown a video example sentence from the ODNZSL entry. Results showed no significant improvements in scores for the test items with definitions. However, feedback from test participants showed that the definitions were comprehensible and perceived as valuable for language learning. The overall conclusion of these studies is that a selective approach should be taken to introducing signed definitions in existing multifunctional sign language dictionaries. For hearing learners of sign language, signed definitions do not meet immediate communicative (comprehension and production) needs, but they may contribute to wider vocabulary learning goals. The main contribution of this thesis is that it suggests a user-focused methodology for creating signed definitions, driven by evidence from the first empirical user study of an online sign language dictionary and therefore taking into account the particular challenges of sign language lexicography. Furthermore, the analysis of features of signed folk definitions contributes to the semantic description of sign languages.&lt;/p&gt;",
      "doi": "https://doi.org/10.26686/wgtn.17060042.v1",
      "openalex_id": "https://openalex.org/W2613542438",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The thesis investigates signed dictionary definitions and user learning; it does not involve discrete audio/speech tokenization or token-quantization analysis, so it does not meet the inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The thesis investigates signed dictionary definitions and user learning; it does not involve discrete audio/speech tokenization or token-quantization analysis, so it does not meet the inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language dictionary definitions, user needs, and linguistic analysis related to sign language lexicography, without addressing discrete audio/speech tokenization or audio signal processing aspects, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language dictionary definitions, user needs, and linguistic analysis related to sign language lexicography, without addressing discrete audio/speech tokenization or audio signal processing aspects, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "5. Childhood Autism and Sign Communication",
    "abstract": "Chapter 5 provides a comprehensive, historical overview of the defining characteristics of autism spectrum disorder (ASD) along with the various language therapies employed to improve the communicative success of minimally verbal individuals. The various strengths and weaknesses of such approaches are analyzed as a basis for helping to determine which methods are likely to be the most successful. The history of signing in persons with ASD is examined, with a special focus on the relatively recent realization that motor skills (both gross and fine) and imitation abilities in such individuals may be severely impaired, thus limiting the effectiveness of sign interventions that do not take into consideration the motor complexity of the signs used. Since many parents and caregivers may also express reluctance to adopt a strategy that uses signs out of fear that this will prevent their child’s acquisition of speech, research dispelling this myth is provided. In addition to coverage of sign-communication interventions and strategies for promoting spontaneous communication and generalization of sign use to multiple settings, other non-oral approaches employed with persons with disabilities are presented as options, including the use of real objects, the Picture Exchange Communication System (PECS), Blissymbols, speech-generating devices, and software applications.",
    "metadata": {
      "title": "5. Childhood Autism and Sign Communication",
      "summary": "Chapter 5 provides a comprehensive, historical overview of the defining characteristics of autism spectrum disorder (ASD) along with the various language therapies employed to improve the communicative success of minimally verbal individuals. The various strengths and weaknesses of such approaches are analyzed as a basis for helping to determine which methods are likely to be the most successful. The history of signing in persons with ASD is examined, with a special focus on the relatively recent realization that motor skills (both gross and fine) and imitation abilities in such individuals may be severely impaired, thus limiting the effectiveness of sign interventions that do not take into consideration the motor complexity of the signs used. Since many parents and caregivers may also express reluctance to adopt a strategy that uses signs out of fear that this will prevent their child’s acquisition of speech, research dispelling this myth is provided. In addition to coverage of sign-communication interventions and strategies for promoting spontaneous communication and generalization of sign use to multiple settings, other non-oral approaches employed with persons with disabilities are presented as options, including the use of real objects, the Picture Exchange Communication System (PECS), Blissymbols, speech-generating devices, and software applications.",
      "abstract": "Chapter 5 provides a comprehensive, historical overview of the defining characteristics of autism spectrum disorder (ASD) along with the various language therapies employed to improve the communicative success of minimally verbal individuals. The various strengths and weaknesses of such approaches are analyzed as a basis for helping to determine which methods are likely to be the most successful. The history of signing in persons with ASD is examined, with a special focus on the relatively recent realization that motor skills (both gross and fine) and imitation abilities in such individuals may be severely impaired, thus limiting the effectiveness of sign interventions that do not take into consideration the motor complexity of the signs used. Since many parents and caregivers may also express reluctance to adopt a strategy that uses signs out of fear that this will prevent their child’s acquisition of speech, research dispelling this myth is provided. In addition to coverage of sign-communication interventions and strategies for promoting spontaneous communication and generalization of sign use to multiple settings, other non-oral approaches employed with persons with disabilities are presented as options, including the use of real objects, the Picture Exchange Communication System (PECS), Blissymbols, speech-generating devices, and software applications.",
      "doi": "https://doi.org/10.11647/obp.0205.05",
      "openalex_id": "https://openalex.org/W3037410390",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses autism and sign communication but does not involve discrete audio/speech tokenization or analysis of discrete tokens; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses autism and sign communication but does not involve discrete audio/speech tokenization or analysis of discrete tokens; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on language therapies and communication methods for individuals with autism spectrum disorder, discussing sign communication and various non-oral approaches, but it does not involve any analysis, proposal, or evaluation of discrete audio/speech tokenization or discrete token representations from continuous audio signals, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on language therapies and communication methods for individuals with autism spectrum disorder, discussing sign communication and various non-oral approaches, but it does not involve any analysis, proposal, or evaluation of discrete audio/speech tokenization or discrete token representations from continuous audio signals, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "2. Use of Manual Signs and Gestures by Hearing Persons",
    "abstract": "Chapter 2 presents multiple accounts of the widespread use of manual signs by hearing persons in diverse settings throughout history. From an initial theoretical focus on the origins of language in humans, and the potential that language first emerged from gestural or manual communication, the reader is introduced to the views of various historical scholars who believed that signs and gestures are a natural means of communication and could potentially even be a universal form of communication. Such a universal form of communication, however, meets with a substantial obstacle in that gestures may vary widely in meaning and usage cross-culturally. Nevertheless, such a system was developed once before by the Indigenous peoples of North America, who spoke hundreds of different languages. Native Americans used signs as a lingua franca across a wide geographical area to overcome the numerous spoken language barriers they encountered. Also covered in this chapter are the use of signs in early contact situations and interactions between Native Americans and Europeans, and the development of signs by various monastic orders in Europe.",
    "metadata": {
      "title": "2. Use of Manual Signs and Gestures by Hearing Persons",
      "summary": "Chapter 2 presents multiple accounts of the widespread use of manual signs by hearing persons in diverse settings throughout history. From an initial theoretical focus on the origins of language in humans, and the potential that language first emerged from gestural or manual communication, the reader is introduced to the views of various historical scholars who believed that signs and gestures are a natural means of communication and could potentially even be a universal form of communication. Such a universal form of communication, however, meets with a substantial obstacle in that gestures may vary widely in meaning and usage cross-culturally. Nevertheless, such a system was developed once before by the Indigenous peoples of North America, who spoke hundreds of different languages. Native Americans used signs as a lingua franca across a wide geographical area to overcome the numerous spoken language barriers they encountered. Also covered in this chapter are the use of signs in early contact situations and interactions between Native Americans and Europeans, and the development of signs by various monastic orders in Europe.",
      "abstract": "Chapter 2 presents multiple accounts of the widespread use of manual signs by hearing persons in diverse settings throughout history. From an initial theoretical focus on the origins of language in humans, and the potential that language first emerged from gestural or manual communication, the reader is introduced to the views of various historical scholars who believed that signs and gestures are a natural means of communication and could potentially even be a universal form of communication. Such a universal form of communication, however, meets with a substantial obstacle in that gestures may vary widely in meaning and usage cross-culturally. Nevertheless, such a system was developed once before by the Indigenous peoples of North America, who spoke hundreds of different languages. Native Americans used signs as a lingua franca across a wide geographical area to overcome the numerous spoken language barriers they encountered. Also covered in this chapter are the use of signs in early contact situations and interactions between Native Americans and Europeans, and the development of signs by various monastic orders in Europe.",
      "doi": "https://doi.org/10.11647/obp.0205.02",
      "openalex_id": "https://openalex.org/W3037624045",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses historical use of manual signs and gestures and their cultural context, but it does not address discrete audio/speech tokenization or token-level representations necessary for audio reconstruction/generation/understanding.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses historical use of manual signs and gestures and their cultural context, but it does not address discrete audio/speech tokenization or token-level representations necessary for audio reconstruction/generation/understanding.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discussed manual signs and gestures by hearing persons from a historical and anthropological perspective, with no mention of audio/speech tokenization or discrete audio token representations related to language modeling or neural encoding; it clearly fails to meet the inclusion criteria and instead falls outside the relevant field.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discussed manual signs and gestures by hearing persons from a historical and anthropological perspective, with no mention of audio/speech tokenization or discrete audio token representations related to language modeling or neural encoding; it clearly fails to meet the inclusion criteria and instead falls outside the relevant field.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "9. Application and Use of the Simplified Sign System with Persons with Disabilities",
    "abstract": "In Chapter 9, various approaches to teaching signs to non-speaking or minimally verbal persons are examined, including general exposure, an incidental or milieu approach, games and group activities, and specific training sessions. Learning goals are identified not only for the main or primary user of the system, but also for that person’s communication partners (family members, caregivers, friends) and persons in the wider environment. Guidelines for using the Simplified Sign System with the target populations are provided in order to establish good and consistent communication practices that will help to maximize users’ success with the system. Such guidelines or strategies include ensuring a positive signing environment, establishing visual contact, using key word signing, accepting errors in sign formation from the main user, rewarding progress, using facial expressions and environmental cues or contextual information to enhance vocabulary acquisition, and adapting the rate and frequency of signing. Finally, the authors anticipate and address many of the questions or concerns that teachers or caregivers may have as they embark on a program of using Simplified Signs.",
    "metadata": {
      "title": "9. Application and Use of the Simplified Sign System with Persons with Disabilities",
      "summary": "In Chapter 9, various approaches to teaching signs to non-speaking or minimally verbal persons are examined, including general exposure, an incidental or milieu approach, games and group activities, and specific training sessions. Learning goals are identified not only for the main or primary user of the system, but also for that person’s communication partners (family members, caregivers, friends) and persons in the wider environment. Guidelines for using the Simplified Sign System with the target populations are provided in order to establish good and consistent communication practices that will help to maximize users’ success with the system. Such guidelines or strategies include ensuring a positive signing environment, establishing visual contact, using key word signing, accepting errors in sign formation from the main user, rewarding progress, using facial expressions and environmental cues or contextual information to enhance vocabulary acquisition, and adapting the rate and frequency of signing. Finally, the authors anticipate and address many of the questions or concerns that teachers or caregivers may have as they embark on a program of using Simplified Signs.",
      "abstract": "In Chapter 9, various approaches to teaching signs to non-speaking or minimally verbal persons are examined, including general exposure, an incidental or milieu approach, games and group activities, and specific training sessions. Learning goals are identified not only for the main or primary user of the system, but also for that person’s communication partners (family members, caregivers, friends) and persons in the wider environment. Guidelines for using the Simplified Sign System with the target populations are provided in order to establish good and consistent communication practices that will help to maximize users’ success with the system. Such guidelines or strategies include ensuring a positive signing environment, establishing visual contact, using key word signing, accepting errors in sign formation from the main user, rewarding progress, using facial expressions and environmental cues or contextual information to enhance vocabulary acquisition, and adapting the rate and frequency of signing. Finally, the authors anticipate and address many of the questions or concerns that teachers or caregivers may have as they embark on a program of using Simplified Signs.",
      "doi": "https://doi.org/10.11647/obp.0205.09",
      "openalex_id": "https://openalex.org/W3037687057",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on teaching a sign language system and communication guidelines for signers, not on discrete audio tokenization or speech tokenization analysis, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on teaching a sign language system and communication guidelines for signers, not on discrete audio tokenization or speech tokenization analysis, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on teaching and using a simplified sign system for communication with persons with disabilities but does not mention any aspect of discrete audio/speech tokenization or the related evaluation criteria specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on teaching and using a simplified sign system for communication with persons with disabilities but does not mention any aspect of discrete audio/speech tokenization or the related evaluation criteria specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "6. Sign-Communication Intervention in Adults and Children with Aphasia",
    "abstract": "Chapter 6 focuses on the language and communication impairments of adults and children that may be acquired after suffering a head injury, stroke, brain infection, tumor, or other similar trauma. Such persons may have had intact language abilities before the trauma, but often present with varying degrees of severity of aphasia that may temporarily or permanently affect their receptive understanding of language and/or their production of language. Although most of the literature examines such deficits in hearing persons and the resulting impact on spoken language, deaf persons may also experience aphasic impairments to their production and understanding of sign language. An examination of apraxia, which often co-occurs with aphasia, provides another dimension that needs to be analyzed when addressing strategies for language rehabilitation. The outcomes of sign-communication interventions in persons with aphasia are presented, along with a focus on Amer-Ind, pantomime skills, and the use of signing to facilitate speech. Finally, the authors address speech and language disorders such as acquired childhood aphasia (Landau-Kleffner syndrome), developmental language disorder (formerly known as specific language impairment) in both hearing and deaf children, and childhood apraxia of speech.",
    "metadata": {
      "title": "6. Sign-Communication Intervention in Adults and Children with Aphasia",
      "summary": "Chapter 6 focuses on the language and communication impairments of adults and children that may be acquired after suffering a head injury, stroke, brain infection, tumor, or other similar trauma. Such persons may have had intact language abilities before the trauma, but often present with varying degrees of severity of aphasia that may temporarily or permanently affect their receptive understanding of language and/or their production of language. Although most of the literature examines such deficits in hearing persons and the resulting impact on spoken language, deaf persons may also experience aphasic impairments to their production and understanding of sign language. An examination of apraxia, which often co-occurs with aphasia, provides another dimension that needs to be analyzed when addressing strategies for language rehabilitation. The outcomes of sign-communication interventions in persons with aphasia are presented, along with a focus on Amer-Ind, pantomime skills, and the use of signing to facilitate speech. Finally, the authors address speech and language disorders such as acquired childhood aphasia (Landau-Kleffner syndrome), developmental language disorder (formerly known as specific language impairment) in both hearing and deaf children, and childhood apraxia of speech.",
      "abstract": "Chapter 6 focuses on the language and communication impairments of adults and children that may be acquired after suffering a head injury, stroke, brain infection, tumor, or other similar trauma. Such persons may have had intact language abilities before the trauma, but often present with varying degrees of severity of aphasia that may temporarily or permanently affect their receptive understanding of language and/or their production of language. Although most of the literature examines such deficits in hearing persons and the resulting impact on spoken language, deaf persons may also experience aphasic impairments to their production and understanding of sign language. An examination of apraxia, which often co-occurs with aphasia, provides another dimension that needs to be analyzed when addressing strategies for language rehabilitation. The outcomes of sign-communication interventions in persons with aphasia are presented, along with a focus on Amer-Ind, pantomime skills, and the use of signing to facilitate speech. Finally, the authors address speech and language disorders such as acquired childhood aphasia (Landau-Kleffner syndrome), developmental language disorder (formerly known as specific language impairment) in both hearing and deaf children, and childhood apraxia of speech.",
      "doi": "https://doi.org/10.11647/obp.0205.06",
      "openalex_id": "https://openalex.org/W3037714746",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses sign-communication interventions for aphasia and sign language, with no focus on discrete audio/speech tokenization or token design, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses sign-communication interventions for aphasia and sign language, with no focus on discrete audio/speech tokenization or token design, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign-communication interventions for aphasia affecting spoken and sign language modalities, without any mention or analysis of discrete audio/speech tokenization methods or discrete token-based audio representations relevant to language-model-style tasks, thus failing the core inclusion criteria and meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign-communication interventions for aphasia affecting spoken and sign language modalities, without any mention or analysis of discrete audio/speech tokenization methods or discrete token-based audio representations relevant to language-model-style tasks, thus failing the core inclusion criteria and meeting exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Simplified Signs: A Manual Sign-Communication System for Special Populations, Volume 1",
    "abstract": "Simplified Signs presents a system of manual sign communication intended for special populations who have had limited success mastering spoken or full sign languages. It is the culmination of over twenty years of research and development by the authors. The Simplified Sign System has been developed and tested for ease of sign comprehension, memorization, and formation by limiting the complexity of the motor skills required to form each sign, and by ensuring that each sign visually resembles the meaning it conveys. Volume 1 outlines the research underpinning and informing the project, and places the Simplified Sign System in a wider context of sign usage, historically and by different populations. Volume 2 presents the lexicon of signs, totalling approximately 1000 signs, each with a clear illustration and a written description of how the sign is formed, as well as a memory aid that connects the sign visually to the meaning that it conveys. While the Simplified Sign System originally was developed to meet the needs of persons with intellectual disabilities, cerebral palsy, autism, or aphasia, it may also assist the communication needs of a wider audience – such as healthcare professionals, aid workers, military personnel, travellers or parents, and children who have not yet mastered spoken language. The system also has been shown to enhance learning for individuals studying a foreign language. Lucid and comprehensive, this work constitutes a valuable resource that will enhance the communicative interactions of many different people, and will be of great interest to researchers and educators alike.",
    "metadata": {
      "title": "Simplified Signs: A Manual Sign-Communication System for Special Populations, Volume 1",
      "summary": "Simplified Signs presents a system of manual sign communication intended for special populations who have had limited success mastering spoken or full sign languages. It is the culmination of over twenty years of research and development by the authors. The Simplified Sign System has been developed and tested for ease of sign comprehension, memorization, and formation by limiting the complexity of the motor skills required to form each sign, and by ensuring that each sign visually resembles the meaning it conveys. Volume 1 outlines the research underpinning and informing the project, and places the Simplified Sign System in a wider context of sign usage, historically and by different populations. Volume 2 presents the lexicon of signs, totalling approximately 1000 signs, each with a clear illustration and a written description of how the sign is formed, as well as a memory aid that connects the sign visually to the meaning that it conveys. While the Simplified Sign System originally was developed to meet the needs of persons with intellectual disabilities, cerebral palsy, autism, or aphasia, it may also assist the communication needs of a wider audience – such as healthcare professionals, aid workers, military personnel, travellers or parents, and children who have not yet mastered spoken language. The system also has been shown to enhance learning for individuals studying a foreign language. Lucid and comprehensive, this work constitutes a valuable resource that will enhance the communicative interactions of many different people, and will be of great interest to researchers and educators alike.",
      "abstract": "Simplified Signs presents a system of manual sign communication intended for special populations who have had limited success mastering spoken or full sign languages. It is the culmination of over twenty years of research and development by the authors. The Simplified Sign System has been developed and tested for ease of sign comprehension, memorization, and formation by limiting the complexity of the motor skills required to form each sign, and by ensuring that each sign visually resembles the meaning it conveys. Volume 1 outlines the research underpinning and informing the project, and places the Simplified Sign System in a wider context of sign usage, historically and by different populations. Volume 2 presents the lexicon of signs, totalling approximately 1000 signs, each with a clear illustration and a written description of how the sign is formed, as well as a memory aid that connects the sign visually to the meaning that it conveys. While the Simplified Sign System originally was developed to meet the needs of persons with intellectual disabilities, cerebral palsy, autism, or aphasia, it may also assist the communication needs of a wider audience – such as healthcare professionals, aid workers, military personnel, travellers or parents, and children who have not yet mastered spoken language. The system also has been shown to enhance learning for individuals studying a foreign language. Lucid and comprehensive, this work constitutes a valuable resource that will enhance the communicative interactions of many different people, and will be of great interest to researchers and educators alike.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3037808925",
      "arxiv_id": "",
      "publication_date": "2020-06-22",
      "published": "2020-06-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item describes a manual sign-communication system and its research basis, with no focus on discrete audio/speech tokenization or conversion of audio into a finite vocabulary for speech tasks; it does not meet the required core tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item describes a manual sign-communication system and its research basis, with no focus on discrete audio/speech tokenization or conversion of audio into a finite vocabulary for speech tasks; it does not meet the required core tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a manual sign communication system for special populations and does not involve discrete audio/speech tokenization or any related audio signal processing as defined in the inclusion criteria. Therefore, it does not meet the core inclusion requirements related to audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a manual sign communication system for special populations and does not involve discrete audio/speech tokenization or any related audio signal processing as defined in the inclusion criteria. Therefore, it does not meet the core inclusion requirements related to audio tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "4. Sign Communication in Persons with an Intellectual Disability or with Cerebral Palsy",
    "abstract": "In Chapter 4, the authors begin an in-depth discussion of the use of signs with special populations, including an early study that occurred in the West of England in the 1840s with deaf students with intellectual disabilities. Various types of intellectual disability are identified, including fragile X syndrome, Williams syndrome, Down syndrome, and Angelman syndrome. The successes and failures of speech-based and sign-based interventions are covered for individuals with these syndromes (particularly the latter two) as well as in persons with multiple disabilities. The authors next move on to a discussion of the relatively sparse research related to teaching signs to children who have cerebral palsy. Recommendations for enhancing the sign-learning environment are provided so that all persons who use signs as an augmentative or alternative means of communication may derive the greatest benefit from their communicative interactions. In addition to maximizing the positive atmosphere in which signing individuals interact with others at school, at home, and in public, the authors suggest that the types of signs employed may also have an impact on whether or not signing is successful.",
    "metadata": {
      "title": "4. Sign Communication in Persons with an Intellectual Disability or with Cerebral Palsy",
      "summary": "In Chapter 4, the authors begin an in-depth discussion of the use of signs with special populations, including an early study that occurred in the West of England in the 1840s with deaf students with intellectual disabilities. Various types of intellectual disability are identified, including fragile X syndrome, Williams syndrome, Down syndrome, and Angelman syndrome. The successes and failures of speech-based and sign-based interventions are covered for individuals with these syndromes (particularly the latter two) as well as in persons with multiple disabilities. The authors next move on to a discussion of the relatively sparse research related to teaching signs to children who have cerebral palsy. Recommendations for enhancing the sign-learning environment are provided so that all persons who use signs as an augmentative or alternative means of communication may derive the greatest benefit from their communicative interactions. In addition to maximizing the positive atmosphere in which signing individuals interact with others at school, at home, and in public, the authors suggest that the types of signs employed may also have an impact on whether or not signing is successful.",
      "abstract": "In Chapter 4, the authors begin an in-depth discussion of the use of signs with special populations, including an early study that occurred in the West of England in the 1840s with deaf students with intellectual disabilities. Various types of intellectual disability are identified, including fragile X syndrome, Williams syndrome, Down syndrome, and Angelman syndrome. The successes and failures of speech-based and sign-based interventions are covered for individuals with these syndromes (particularly the latter two) as well as in persons with multiple disabilities. The authors next move on to a discussion of the relatively sparse research related to teaching signs to children who have cerebral palsy. Recommendations for enhancing the sign-learning environment are provided so that all persons who use signs as an augmentative or alternative means of communication may derive the greatest benefit from their communicative interactions. In addition to maximizing the positive atmosphere in which signing individuals interact with others at school, at home, and in public, the authors suggest that the types of signs employed may also have an impact on whether or not signing is successful.",
      "doi": "https://doi.org/10.11647/obp.0205.04",
      "openalex_id": "https://openalex.org/W3037841061",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses sign communication and augmentative signing in intellectual disability and cerebral palsy, with no analysis or use of discrete audio tokenization or speech token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses sign communication and augmentative signing in intellectual disability and cerebral palsy, with no analysis or use of discrete audio tokenization or speech token sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on sign communication with intellectual disabilities and cerebral palsy, discussing speech-based and sign-based interventions, but it does not address discrete audio/speech tokenization or any audio signal quantization and token design relevant to language-model-style sequences, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on sign communication with intellectual disabilities and cerebral palsy, discussing speech-based and sign-based interventions, but it does not address discrete audio/speech tokenization or any audio signal quantization and token design relevant to language-model-style sequences, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "3. Deaf Persons and Sign Languages",
    "abstract": "Chapter 3 introduces the reader to various aspects of sign languages, including their historical development and use within educational contexts by Deaf communities in Europe and the United States. Also covered is the initiation of the field of sign language linguistics by William C. Stokoe, a linguist who systematically proved that American Sign Language (ASL) was indeed a language with its own distinct structure and properties that differed from any spoken language. The phonological parameters of signs receive considerable attention, highlighting ways in which the unique properties of sign languages allow them to represent meaning in ways that are more consistently transparent and iconic than similar phenomena in the speech modality. Despite these similarities across sign languages, the differences among the sign languages of the world led Deaf persons to create and develop the lingua franca of International Sign (previously Gestuno) for use at international conventions. Finally, the similarities and distinctions between the processes of language development and acquisition across the modalities of speech and sign are discussed, as well as how signing benefits the learning of spoken language vocabulary by hearing children.",
    "metadata": {
      "title": "3. Deaf Persons and Sign Languages",
      "summary": "Chapter 3 introduces the reader to various aspects of sign languages, including their historical development and use within educational contexts by Deaf communities in Europe and the United States. Also covered is the initiation of the field of sign language linguistics by William C. Stokoe, a linguist who systematically proved that American Sign Language (ASL) was indeed a language with its own distinct structure and properties that differed from any spoken language. The phonological parameters of signs receive considerable attention, highlighting ways in which the unique properties of sign languages allow them to represent meaning in ways that are more consistently transparent and iconic than similar phenomena in the speech modality. Despite these similarities across sign languages, the differences among the sign languages of the world led Deaf persons to create and develop the lingua franca of International Sign (previously Gestuno) for use at international conventions. Finally, the similarities and distinctions between the processes of language development and acquisition across the modalities of speech and sign are discussed, as well as how signing benefits the learning of spoken language vocabulary by hearing children.",
      "abstract": "Chapter 3 introduces the reader to various aspects of sign languages, including their historical development and use within educational contexts by Deaf communities in Europe and the United States. Also covered is the initiation of the field of sign language linguistics by William C. Stokoe, a linguist who systematically proved that American Sign Language (ASL) was indeed a language with its own distinct structure and properties that differed from any spoken language. The phonological parameters of signs receive considerable attention, highlighting ways in which the unique properties of sign languages allow them to represent meaning in ways that are more consistently transparent and iconic than similar phenomena in the speech modality. Despite these similarities across sign languages, the differences among the sign languages of the world led Deaf persons to create and develop the lingua franca of International Sign (previously Gestuno) for use at international conventions. Finally, the similarities and distinctions between the processes of language development and acquisition across the modalities of speech and sign are discussed, as well as how signing benefits the learning of spoken language vocabulary by hearing children.",
      "doi": "https://doi.org/10.11647/obp.0205.03",
      "openalex_id": "https://openalex.org/W3037863649",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses sign languages and linguistic aspects, with no discrete audio tokenization methodology or evaluation of discrete tokens, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses sign languages and linguistic aspects, with no discrete audio tokenization methodology or evaluation of discrete tokens, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the historical, linguistic, and educational aspects of sign languages, without discussing any discrete audio or speech tokenization methods, discrete token representations, or related technical contributions required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the historical, linguistic, and educational aspects of sign languages, without discussing any discrete audio or speech tokenization methods, discrete token representations, or related technical contributions required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "8. Development of the Simplified Sign System",
    "abstract": "Chapter 8 provides background information on the development of the Simplified Sign System. These steps are included so that investigators may replicate research findings and/or develop additional signs for their own sign-intervention programs. The authors first discuss efforts to find highly iconic or representative gestures in the dictionaries of various sign languages and sign systems from around the world. If necessary, signs were then modified to make them easier to produce based on the results of prior studies of signing errors made by students with autism, the sign-learning children of Deaf parents, and undergraduate students unfamiliar with any sign language. These potential signs were then tested with different undergraduate students to determine whether the signs were sufficiently memorable and accurately formed. Signs that did not meet criterion were either dropped from the system or subsequently modified and re-tested. Initial results from comparison studies between Simplified Signs and ASL signs and between Simplified Signs and Amer-Ind signs are presented as well. Finally, feedback from users influenced the course of the project. Memory aids were developed, especially for those persons who have less familiarity with sign languages, to help explain the ties between each sign and its referent in case that relationship is not readily or immediately apparent to a potential learner.",
    "metadata": {
      "title": "8. Development of the Simplified Sign System",
      "summary": "Chapter 8 provides background information on the development of the Simplified Sign System. These steps are included so that investigators may replicate research findings and/or develop additional signs for their own sign-intervention programs. The authors first discuss efforts to find highly iconic or representative gestures in the dictionaries of various sign languages and sign systems from around the world. If necessary, signs were then modified to make them easier to produce based on the results of prior studies of signing errors made by students with autism, the sign-learning children of Deaf parents, and undergraduate students unfamiliar with any sign language. These potential signs were then tested with different undergraduate students to determine whether the signs were sufficiently memorable and accurately formed. Signs that did not meet criterion were either dropped from the system or subsequently modified and re-tested. Initial results from comparison studies between Simplified Signs and ASL signs and between Simplified Signs and Amer-Ind signs are presented as well. Finally, feedback from users influenced the course of the project. Memory aids were developed, especially for those persons who have less familiarity with sign languages, to help explain the ties between each sign and its referent in case that relationship is not readily or immediately apparent to a potential learner.",
      "abstract": "Chapter 8 provides background information on the development of the Simplified Sign System. These steps are included so that investigators may replicate research findings and/or develop additional signs for their own sign-intervention programs. The authors first discuss efforts to find highly iconic or representative gestures in the dictionaries of various sign languages and sign systems from around the world. If necessary, signs were then modified to make them easier to produce based on the results of prior studies of signing errors made by students with autism, the sign-learning children of Deaf parents, and undergraduate students unfamiliar with any sign language. These potential signs were then tested with different undergraduate students to determine whether the signs were sufficiently memorable and accurately formed. Signs that did not meet criterion were either dropped from the system or subsequently modified and re-tested. Initial results from comparison studies between Simplified Signs and ASL signs and between Simplified Signs and Amer-Ind signs are presented as well. Finally, feedback from users influenced the course of the project. Memory aids were developed, especially for those persons who have less familiarity with sign languages, to help explain the ties between each sign and its referent in case that relationship is not readily or immediately apparent to a potential learner.",
      "doi": "https://doi.org/10.11647/obp.0205.08",
      "openalex_id": "https://openalex.org/W3037943431",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes the development of a Simplified Sign System for gestures and compares signs, with no discussion of discrete audio tokenization or speech token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes the development of a Simplified Sign System for gestures and compares signs, with no discussion of discrete audio tokenization or speech token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the development of a Simplified Sign System for sign languages and does not discuss discrete audio or speech tokenization, continuous audio signals, or neural discrete token representations, which are the core of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the development of a Simplified Sign System for sign languages and does not discuss discrete audio or speech tokenization, continuous audio signals, or neural discrete token representations, which are the core of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "7. Use of Manual Signs and Gestures by Hearing Persons",
    "abstract": "In Chapter 7, the authors change focus from the use of signs by deaf persons and with individuals with disabilities to how signing may enhance the learning and processing of spoken language by typically developing hearing children and adults. The first topic examined is the use of signs to foster infants’ and young children’s acquisition of their principal spoken language. Signs may further serve as an effective intervention strategy in academic settings for children with ADHD or as a means to improving vocabulary acquisition and reading comprehension for children who lag behind their age group on various language performance measures. Iconic signs and representative gestures may also be used to facilitate the acquisition of foreign language vocabulary when the signs are paired with the to-be-learned words. Finally, various studies concerning the positive benefits of learning to sign promote the possibility that using the visual-gestural modality may confer increased skills in various cognitive domains such as spatial memory, mental rotation, and facial discrimination.",
    "metadata": {
      "title": "7. Use of Manual Signs and Gestures by Hearing Persons",
      "summary": "In Chapter 7, the authors change focus from the use of signs by deaf persons and with individuals with disabilities to how signing may enhance the learning and processing of spoken language by typically developing hearing children and adults. The first topic examined is the use of signs to foster infants’ and young children’s acquisition of their principal spoken language. Signs may further serve as an effective intervention strategy in academic settings for children with ADHD or as a means to improving vocabulary acquisition and reading comprehension for children who lag behind their age group on various language performance measures. Iconic signs and representative gestures may also be used to facilitate the acquisition of foreign language vocabulary when the signs are paired with the to-be-learned words. Finally, various studies concerning the positive benefits of learning to sign promote the possibility that using the visual-gestural modality may confer increased skills in various cognitive domains such as spatial memory, mental rotation, and facial discrimination.",
      "abstract": "In Chapter 7, the authors change focus from the use of signs by deaf persons and with individuals with disabilities to how signing may enhance the learning and processing of spoken language by typically developing hearing children and adults. The first topic examined is the use of signs to foster infants’ and young children’s acquisition of their principal spoken language. Signs may further serve as an effective intervention strategy in academic settings for children with ADHD or as a means to improving vocabulary acquisition and reading comprehension for children who lag behind their age group on various language performance measures. Iconic signs and representative gestures may also be used to facilitate the acquisition of foreign language vocabulary when the signs are paired with the to-be-learned words. Finally, various studies concerning the positive benefits of learning to sign promote the possibility that using the visual-gestural modality may confer increased skills in various cognitive domains such as spatial memory, mental rotation, and facial discrimination.",
      "doi": "https://doi.org/10.11647/obp.0205.07",
      "openalex_id": "https://openalex.org/W3037999947",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on using signs and gestures by hearing individuals to aid spoken language learning, with no discussion of discrete audio tokenization or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on using signs and gestures by hearing individuals to aid spoken language learning, with no discussion of discrete audio tokenization or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the use of manual signs and gestures to enhance spoken language learning and cognitive skills, without mentioning any aspect of discrete audio or speech tokenization or analysis of audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor address any exclusion criteria relevant to that domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the use of manual signs and gestures to enhance spoken language learning and cognitive skills, without mentioning any aspect of discrete audio or speech tokenization or analysis of audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor address any exclusion criteria relevant to that domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sign Language Phonological Variation and Change",
    "abstract": "This chapter covers factors that affect variation in phonological or prosodic form synchronically and diachronically. Such factors can include grammatical context, phonetic factors such as ease of articulation and perception, as well as a range of sociolinguistic factors, such as age, gender, region, and ethnicity.",
    "metadata": {
      "title": "Sign Language Phonological Variation and Change",
      "summary": "This chapter covers factors that affect variation in phonological or prosodic form synchronically and diachronically. Such factors can include grammatical context, phonetic factors such as ease of articulation and perception, as well as a range of sociolinguistic factors, such as age, gender, region, and ethnicity.",
      "abstract": "This chapter covers factors that affect variation in phonological or prosodic form synchronically and diachronically. Such factors can include grammatical context, phonetic factors such as ease of articulation and perception, as well as a range of sociolinguistic factors, such as age, gender, region, and ethnicity.",
      "doi": "https://doi.org/10.1017/9781316286401.008",
      "openalex_id": "https://openalex.org/W3161496272",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses sociolinguistic factors in sign language phonology and does not address discrete audio/speech tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses sociolinguistic factors in sign language phonology and does not address discrete audio/speech tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological variation in sign language, discussing factors influencing phonological or prosodic form, but does not propose, analyze, or evaluate discrete audio/speech tokenization or any method converting continuous audio to discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological variation in sign language, discussing factors influencing phonological or prosodic form, but does not propose, analyze, or evaluate discrete audio/speech tokenization or any method converting continuous audio to discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Design of Signed Language Tests for Deaf L1 Children",
    "abstract": "Abstract Recent changes in the earlier diagnosis of deafness and improved amplification options have meant that deaf children increasingly have better opportunities to develop spoken language. Nevertheless, a significant proportion of children continue to use signed language as a first language (L1), including deaf and hearing children in deaf signing families and deaf children in hearing families where families use signed language in the home. For both groups, mastery of sign language as an L1 is important because it paves the way to communication and also because it provides the basis for development of spoken language, in either its oral or written form, as a second language (L2). It is crucial that signed language development proceeds in an age-appropriate manner, and assessments of signed language are therefore important to ensure that this is the case. However, the development of effective tests of signed language acquisition is not without challenges. This chapter presents these challenges and other issues and gives examples of how available tests seek to overcome them.",
    "metadata": {
      "title": "Design of Signed Language Tests for Deaf L1 Children",
      "summary": "Abstract Recent changes in the earlier diagnosis of deafness and improved amplification options have meant that deaf children increasingly have better opportunities to develop spoken language. Nevertheless, a significant proportion of children continue to use signed language as a first language (L1), including deaf and hearing children in deaf signing families and deaf children in hearing families where families use signed language in the home. For both groups, mastery of sign language as an L1 is important because it paves the way to communication and also because it provides the basis for development of spoken language, in either its oral or written form, as a second language (L2). It is crucial that signed language development proceeds in an age-appropriate manner, and assessments of signed language are therefore important to ensure that this is the case. However, the development of effective tests of signed language acquisition is not without challenges. This chapter presents these challenges and other issues and gives examples of how available tests seek to overcome them.",
      "abstract": "Abstract Recent changes in the earlier diagnosis of deafness and improved amplification options have meant that deaf children increasingly have better opportunities to develop spoken language. Nevertheless, a significant proportion of children continue to use signed language as a first language (L1), including deaf and hearing children in deaf signing families and deaf children in hearing families where families use signed language in the home. For both groups, mastery of sign language as an L1 is important because it paves the way to communication and also because it provides the basis for development of spoken language, in either its oral or written form, as a second language (L2). It is crucial that signed language development proceeds in an age-appropriate manner, and assessments of signed language are therefore important to ensure that this is the case. However, the development of effective tests of signed language acquisition is not without challenges. This chapter presents these challenges and other issues and gives examples of how available tests seek to overcome them.",
      "doi": "https://doi.org/10.1093/oso/9780190885052.003.0003",
      "openalex_id": "https://openalex.org/W4200391881",
      "arxiv_id": "",
      "publication_date": "2021-12-21",
      "published": "2021-12-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on the design of signed language tests for Deaf L1 children and discusses issues in signed language assessment; there is no explicit analysis or evaluation of discrete audio/speech tokenization or token design, which is the central inclusion criterion. Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on the design of signed language tests for Deaf L1 children and discusses issues in signed language assessment; there is no explicit analysis or evaluation of discrete audio/speech tokenization or token design, which is the central inclusion criterion. Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the design of signed language tests for deaf children, emphasizing sign language acquisition assessment rather than audio/speech tokenization or discrete audio token representations. It does not address any core aspects of discrete audio tokenization or neural quantization relevant to audio signal processing, which are required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the design of signed language tests for deaf children, emphasizing sign language acquisition assessment rather than audio/speech tokenization or discrete audio token representations. It does not address any core aspects of discrete audio tokenization or neural quantization relevant to audio signal processing, which are required inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Emergence of Phonology",
    "abstract": "In this chapter, issues concerning the emergence of phonology will be addressed by tracing the paths of phonology and morphophonology as they move from gesture, to homesign, and across multiple stages in cohorts (or generations) of young sign languages. The material covered in the first four chapters of this volume will provide theoretical context for the emergence of phonology. Relevant work on spoken languages, which has observed and modeled processes of emergence or mapped them typologically, will be discussed, and some of the principles of phonological systems will be articulated, such as paradigm uniformity, conventionalization, symmetry of the phonological inventory, and well-formedness constraints on phonological constituents. Based on ongoing work we can also address some of the social factors that may be important in different rates of emergence in different social contexts or \"language ecologies.\"",
    "metadata": {
      "title": "The Emergence of Phonology",
      "summary": "In this chapter, issues concerning the emergence of phonology will be addressed by tracing the paths of phonology and morphophonology as they move from gesture, to homesign, and across multiple stages in cohorts (or generations) of young sign languages. The material covered in the first four chapters of this volume will provide theoretical context for the emergence of phonology. Relevant work on spoken languages, which has observed and modeled processes of emergence or mapped them typologically, will be discussed, and some of the principles of phonological systems will be articulated, such as paradigm uniformity, conventionalization, symmetry of the phonological inventory, and well-formedness constraints on phonological constituents. Based on ongoing work we can also address some of the social factors that may be important in different rates of emergence in different social contexts or \"language ecologies.\"",
      "abstract": "In this chapter, issues concerning the emergence of phonology will be addressed by tracing the paths of phonology and morphophonology as they move from gesture, to homesign, and across multiple stages in cohorts (or generations) of young sign languages. The material covered in the first four chapters of this volume will provide theoretical context for the emergence of phonology. Relevant work on spoken languages, which has observed and modeled processes of emergence or mapped them typologically, will be discussed, and some of the principles of phonological systems will be articulated, such as paradigm uniformity, conventionalization, symmetry of the phonological inventory, and well-formedness constraints on phonological constituents. Based on ongoing work we can also address some of the social factors that may be important in different rates of emergence in different social contexts or \"language ecologies.\"",
      "doi": "https://doi.org/10.1017/9781316286401.005",
      "openalex_id": "https://openalex.org/W4211179059",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on the emergence of phonology in sign languages and gesture-based communication without discussing discrete audio tokenization or token sequences for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on the emergence of phonology in sign languages and gesture-based communication without discussing discrete audio tokenization or token sequences for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the emergence of phonology through linguistic and social analysis of sign languages and spoken language phonological principles, without any mention of discrete audio or speech tokenization methods, quantization, or related neural encoding techniques; hence, it does not meet the inclusion criteria centered on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the emergence of phonology through linguistic and social analysis of sign languages and spoken language phonological principles, without any mention of discrete audio or speech tokenization methods, quantization, or related neural encoding techniques; hence, it does not meet the inclusion criteria centered on audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Structure of The Lexical Item and Sentence Meaning Composition",
    "abstract": "Abstract This chapter examines the full-entry model according to which lexical meaning is a generalization that results from, and is maintained by, continued exposure to a conceptual structure through linguistics means. It focuses on the iteration reading in “durative” for, as in “Sam jumped for an hour,” and the observation that such iteration has no overt morphophonological support, raising the question of its source. Composition of the for-adverbial exerts greater computational load than the non-iterative counterpart. The root of this cost is explained as the real-time search for a partition measure demanded by for’s meaning. Hence, for’s meaning determines the context that is relevant for the utterance construal. In this way, the lexical item built in a full-entry fashion captures the semantic combinatorial and generative burden by capitalizing on a simplified lexically-driven constraint-satisfaction dynamic for the processor and a lexicon-based grammar.",
    "metadata": {
      "title": "The Structure of The Lexical Item and Sentence Meaning Composition",
      "summary": "Abstract This chapter examines the full-entry model according to which lexical meaning is a generalization that results from, and is maintained by, continued exposure to a conceptual structure through linguistics means. It focuses on the iteration reading in “durative” for, as in “Sam jumped for an hour,” and the observation that such iteration has no overt morphophonological support, raising the question of its source. Composition of the for-adverbial exerts greater computational load than the non-iterative counterpart. The root of this cost is explained as the real-time search for a partition measure demanded by for’s meaning. Hence, for’s meaning determines the context that is relevant for the utterance construal. In this way, the lexical item built in a full-entry fashion captures the semantic combinatorial and generative burden by capitalizing on a simplified lexically-driven constraint-satisfaction dynamic for the processor and a lexicon-based grammar.",
      "abstract": "Abstract This chapter examines the full-entry model according to which lexical meaning is a generalization that results from, and is maintained by, continued exposure to a conceptual structure through linguistics means. It focuses on the iteration reading in “durative” for, as in “Sam jumped for an hour,” and the observation that such iteration has no overt morphophonological support, raising the question of its source. Composition of the for-adverbial exerts greater computational load than the non-iterative counterpart. The root of this cost is explained as the real-time search for a partition measure demanded by for’s meaning. Hence, for’s meaning determines the context that is relevant for the utterance construal. In this way, the lexical item built in a full-entry fashion captures the semantic combinatorial and generative burden by capitalizing on a simplified lexically-driven constraint-satisfaction dynamic for the processor and a lexicon-based grammar.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.13",
      "openalex_id": "https://openalex.org/W4212814378",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses lexical semantics and processing dynamics, with no discussion of discrete audio/speech tokenization or token-level representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses lexical semantics and processing dynamics, with no discussion of discrete audio/speech tokenization or token-level representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical meaning and semantic composition within linguistics and does not mention any analysis, proposal, or evaluation related to discrete audio or speech tokenization; it also lacks any indication of dealing with audio signals or token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical meaning and semantic composition within linguistics and does not mention any analysis, proposal, or evaluation related to discrete audio or speech tokenization; it also lacks any indication of dealing with audio signals or token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning and Using Written Word Forms",
    "abstract": "Abstract Over most of human history, knowing a word has involved knowing its phonological form. Nowadays, for people who are literate, knowing a word means knowing its written form as well. The goal of this chapter is to discuss how people learn and use these forms. The chapter begins by considering how writing systems represent language in a visual form. The next section of the chapter takes up the processes that are involved in skilled reading, considering how experienced readers perform the secondary linguistic task of reading as well and quickly as they do. How children learn to read and spell is also considered. The final section of the chapter discusses how the learning of orthographic representations can affect the mental lexicon.",
    "metadata": {
      "title": "Learning and Using Written Word Forms",
      "summary": "Abstract Over most of human history, knowing a word has involved knowing its phonological form. Nowadays, for people who are literate, knowing a word means knowing its written form as well. The goal of this chapter is to discuss how people learn and use these forms. The chapter begins by considering how writing systems represent language in a visual form. The next section of the chapter takes up the processes that are involved in skilled reading, considering how experienced readers perform the secondary linguistic task of reading as well and quickly as they do. How children learn to read and spell is also considered. The final section of the chapter discusses how the learning of orthographic representations can affect the mental lexicon.",
      "abstract": "Abstract Over most of human history, knowing a word has involved knowing its phonological form. Nowadays, for people who are literate, knowing a word means knowing its written form as well. The goal of this chapter is to discuss how people learn and use these forms. The chapter begins by considering how writing systems represent language in a visual form. The next section of the chapter takes up the processes that are involved in skilled reading, considering how experienced readers perform the secondary linguistic task of reading as well and quickly as they do. How children learn to read and spell is also considered. The final section of the chapter discusses how the learning of orthographic representations can affect the mental lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.24",
      "openalex_id": "https://openalex.org/W4212849484",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on literacy, orthography, and reading processes rather than discrete speech tokenization into a finite set of tokens for audio tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on literacy, orthography, and reading processes rather than discrete speech tokenization into a finite set of tokens for audio tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning and using written word forms related to literacy and reading processes, without discussing discrete audio/speech tokenization, quantization, or any related audio signal processing; therefore, it does not meet the inclusion criteria and matches the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning and using written word forms related to literacy and reading processes, without discussing discrete audio/speech tokenization, quantization, or any related audio signal processing; therefore, it does not meet the inclusion criteria and matches the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural encoding of speech and word forms",
    "abstract": "Abstract Speech perception is a collection of procedures that make possible the mapping from continuously varying acoustic input to discrete mental representations that form the basis for stored words in the mental lexicon. The chapter focuses on experiments supporting abstractionist versus episodic approaches to the representation of speech and words. Studies using electrophysiological approaches are reviewed that are consistent with both the generation of abstract categories and the maintenance of episodic information, suggesting that both types of representations are carried forward during the recognition process. The functional neuroanatomy that underpins speech recognition is reviewed, and both neurophysiological and neuroimaging data are discussed that, jointly, are consistent with a perspective that privileges abstract representations but allows for the concurrent incorporation of episodic or indexical information in speech perception. The integrative, neurally inspired model that emerges accommodates both abstractionist and episodicist approaches.",
    "metadata": {
      "title": "Neural encoding of speech and word forms",
      "summary": "Abstract Speech perception is a collection of procedures that make possible the mapping from continuously varying acoustic input to discrete mental representations that form the basis for stored words in the mental lexicon. The chapter focuses on experiments supporting abstractionist versus episodic approaches to the representation of speech and words. Studies using electrophysiological approaches are reviewed that are consistent with both the generation of abstract categories and the maintenance of episodic information, suggesting that both types of representations are carried forward during the recognition process. The functional neuroanatomy that underpins speech recognition is reviewed, and both neurophysiological and neuroimaging data are discussed that, jointly, are consistent with a perspective that privileges abstract representations but allows for the concurrent incorporation of episodic or indexical information in speech perception. The integrative, neurally inspired model that emerges accommodates both abstractionist and episodicist approaches.",
      "abstract": "Abstract Speech perception is a collection of procedures that make possible the mapping from continuously varying acoustic input to discrete mental representations that form the basis for stored words in the mental lexicon. The chapter focuses on experiments supporting abstractionist versus episodic approaches to the representation of speech and words. Studies using electrophysiological approaches are reviewed that are consistent with both the generation of abstract categories and the maintenance of episodic information, suggesting that both types of representations are carried forward during the recognition process. The functional neuroanatomy that underpins speech recognition is reviewed, and both neurophysiological and neuroimaging data are discussed that, jointly, are consistent with a perspective that privileges abstract representations but allows for the concurrent incorporation of episodic or indexical information in speech perception. The integrative, neurally inspired model that emerges accommodates both abstractionist and episodicist approaches.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.16",
      "openalex_id": "https://openalex.org/W4212906825",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses neural representations and distinctions between abstractionist and episodic processing in speech perception but does not explicitly address discrete audio tokenization into a finite vocabulary or analysis of discrete tokenization as a core object of study, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses neural representations and distinctions between abstractionist and episodic processing in speech perception but does not explicitly address discrete audio tokenization into a finite vocabulary or analysis of discrete tokenization as a core object of study, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses neural and neurophysiological studies of speech perception focusing on abstract and episodic representations but does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks; therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses neural and neurophysiological studies of speech perception focusing on abstract and episodic representations but does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks; therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Dynamics of Word Production",
    "abstract": "Abstract The common sense notion of lexicon as a dictionary implies a static, fixed repository of information about the properties of individual words. This chapter discusses evidence from speech production suggesting that the lexicon in production is best characterized as a process: lexical access. This process involves the dynamic interaction of information from multiple lexical representations, resulting in the production of variable word forms. The corresponding theoretical framework is outlined within the context of single word production. This chapter then discusses a relatively less-well explored area: how lexical access changes when speakers plan and produce multiple words in connected speech. The conclusion points to open theoretical issues raised by new findings in connected speech.",
    "metadata": {
      "title": "The Dynamics of Word Production",
      "summary": "Abstract The common sense notion of lexicon as a dictionary implies a static, fixed repository of information about the properties of individual words. This chapter discusses evidence from speech production suggesting that the lexicon in production is best characterized as a process: lexical access. This process involves the dynamic interaction of information from multiple lexical representations, resulting in the production of variable word forms. The corresponding theoretical framework is outlined within the context of single word production. This chapter then discusses a relatively less-well explored area: how lexical access changes when speakers plan and produce multiple words in connected speech. The conclusion points to open theoretical issues raised by new findings in connected speech.",
      "abstract": "Abstract The common sense notion of lexicon as a dictionary implies a static, fixed repository of information about the properties of individual words. This chapter discusses evidence from speech production suggesting that the lexicon in production is best characterized as a process: lexical access. This process involves the dynamic interaction of information from multiple lexical representations, resulting in the production of variable word forms. The corresponding theoretical framework is outlined within the context of single word production. This chapter then discusses a relatively less-well explored area: how lexical access changes when speakers plan and produce multiple words in connected speech. The conclusion points to open theoretical issues raised by new findings in connected speech.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.25",
      "openalex_id": "https://openalex.org/W4212957703",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on theoretical lexical access in speech production and connected speech, with no discussion of discrete audio tokenization, token sequences, or discrete speech representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on theoretical lexical access in speech production and connected speech, with no discussion of discrete audio tokenization, token sequences, or discrete speech representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the lexical access process in speech production, particularly on the cognitive and linguistic aspects of word production and connected speech, without discussing or analyzing discrete audio/speech tokenization methods or discrete representations for audio reconstruction, generation, or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the lexical access process in speech production, particularly on the cognitive and linguistic aspects of word production and connected speech, without discussing or analyzing discrete audio/speech tokenization methods or discrete representations for audio reconstruction, generation, or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How Learners Move From Sound To Morphology",
    "abstract": "Abstract This chapter investigates the phenomenon of how children acquire grammatical morphology, including both function words and inflectional morphemes. In particular, it shows that the phonology and prosodic structure of a language interact with how and when grammatical morphemes are perceived/comprehended and produced. With respect to function words such as articles, it shows that those that can be prosodified as part of a foot/prosodic word tend to be produced first, as do inflectional morphemes occurring at the ends of phrases/utterances. The fact that similar patterns of prosodic interactions between the perception/production of grammatical morphology and the lexicon appear crosslinguistically suggests that these are robust phenomena. This has both theoretical implications for understanding the interactions between children’s developing linguistic competencies at the phonology/syntax interface, as well as practical implications for clinicians working with children with language delay.",
    "metadata": {
      "title": "How Learners Move From Sound To Morphology",
      "summary": "Abstract This chapter investigates the phenomenon of how children acquire grammatical morphology, including both function words and inflectional morphemes. In particular, it shows that the phonology and prosodic structure of a language interact with how and when grammatical morphemes are perceived/comprehended and produced. With respect to function words such as articles, it shows that those that can be prosodified as part of a foot/prosodic word tend to be produced first, as do inflectional morphemes occurring at the ends of phrases/utterances. The fact that similar patterns of prosodic interactions between the perception/production of grammatical morphology and the lexicon appear crosslinguistically suggests that these are robust phenomena. This has both theoretical implications for understanding the interactions between children’s developing linguistic competencies at the phonology/syntax interface, as well as practical implications for clinicians working with children with language delay.",
      "abstract": "Abstract This chapter investigates the phenomenon of how children acquire grammatical morphology, including both function words and inflectional morphemes. In particular, it shows that the phonology and prosodic structure of a language interact with how and when grammatical morphemes are perceived/comprehended and produced. With respect to function words such as articles, it shows that those that can be prosodified as part of a foot/prosodic word tend to be produced first, as do inflectional morphemes occurring at the ends of phrases/utterances. The fact that similar patterns of prosodic interactions between the perception/production of grammatical morphology and the lexicon appear crosslinguistically suggests that these are robust phenomena. This has both theoretical implications for understanding the interactions between children’s developing linguistic competencies at the phonology/syntax interface, as well as practical implications for clinicians working with children with language delay.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.8",
      "openalex_id": "https://openalex.org/W4212986196",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses how children acquire grammatical morphology and its interaction with phonology/prosody but does not discuss discrete audio tokenization or token-level representations; there is no analysis or proposal of converting continuous audio into discrete tokens for speech tasks, so it fails inclusion and would be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses how children acquire grammatical morphology and its interaction with phonology/prosody but does not discuss discrete audio tokenization or token-level representations; there is no analysis or proposal of converting continuous audio into discrete tokens for speech tasks, so it fails inclusion and would be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on children's acquisition of grammatical morphology with emphasis on phonology and prosodic structure, but it does not address discrete audio/speech tokenization, quantization, or neural encoding relevant to speech token sequences as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on children's acquisition of grammatical morphology with emphasis on phonology and prosodic structure, but it does not address discrete audio/speech tokenization, quantization, or neural encoding relevant to speech token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Early Logic and Language",
    "abstract": "Abstract The present chapter charts the course of the acquisition of logical expressions in child language. The focus is on the meanings that child language learners initially assign to logical expressions, how children’s meanings compare with the meanings assigned by adults, and how the meanings of the logical expressions of natural language correspond to the meanings that are assigned to the corresponding vocabulary of classical logic. We review the findings of several cross-linguistic experimental studies investigating children’s interpretation of sentences that contain different combinations of logical expressions. In general, the findings of research indicate a strong overlap in the meanings that children assign to logical expressions and the meanings assigned to the corresponding expressions in classical logic.",
    "metadata": {
      "title": "Early Logic and Language",
      "summary": "Abstract The present chapter charts the course of the acquisition of logical expressions in child language. The focus is on the meanings that child language learners initially assign to logical expressions, how children’s meanings compare with the meanings assigned by adults, and how the meanings of the logical expressions of natural language correspond to the meanings that are assigned to the corresponding vocabulary of classical logic. We review the findings of several cross-linguistic experimental studies investigating children’s interpretation of sentences that contain different combinations of logical expressions. In general, the findings of research indicate a strong overlap in the meanings that children assign to logical expressions and the meanings assigned to the corresponding expressions in classical logic.",
      "abstract": "Abstract The present chapter charts the course of the acquisition of logical expressions in child language. The focus is on the meanings that child language learners initially assign to logical expressions, how children’s meanings compare with the meanings assigned by adults, and how the meanings of the logical expressions of natural language correspond to the meanings that are assigned to the corresponding vocabulary of classical logic. We review the findings of several cross-linguistic experimental studies investigating children’s interpretation of sentences that contain different combinations of logical expressions. In general, the findings of research indicate a strong overlap in the meanings that children assign to logical expressions and the meanings assigned to the corresponding expressions in classical logic.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.11",
      "openalex_id": "https://openalex.org/W4213062342",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns acquisition and meanings of logical expressions in child language and compares to classical logic; it contains no discrete audio/speech tokenization or analysis of token sequences, so it fails the inclusion criteria and meets an exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns acquisition and meanings of logical expressions in child language and compares to classical logic; it contains no discrete audio/speech tokenization or analysis of token sequences, so it fails the inclusion criteria and meets an exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on child language acquisition of logical expressions and does not address discrete audio/speech tokenization or neural encoding methods related to audio signals, therefore it does not meet the inclusion criteria related to audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on child language acquisition of logical expressions and does not address discrete audio/speech tokenization or neural encoding methods related to audio signals, therefore it does not meet the inclusion criteria related to audio tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Syntax and The Lexicon",
    "abstract": "Abstract Taking as point of departure Chomsky’s Remarks, this chapter discusses the relationship between syntax and the lexicon and how this has changed in the last 50 years. It then examines the syntax-lexicon relationship from the perspective of Distributed Morphology, focusing on transitivity alternations, nominalization, and adjectival passives. The main question addressed is the locus of introduction of arguments, focusing on external and internal arguments as well as on different types of external arguments. The Distributed Morphology perspective is compared to Ramchand’s (2008, 2018) First Phase Syntax framework, and Borer’s (2005a, b, 2013) Exoskeletal model. The chapter is structured from the perspective of a theory of grammar that dispenses with the lexicon component and discusses the consequences of this choice for argument introduction.",
    "metadata": {
      "title": "Syntax and The Lexicon",
      "summary": "Abstract Taking as point of departure Chomsky’s Remarks, this chapter discusses the relationship between syntax and the lexicon and how this has changed in the last 50 years. It then examines the syntax-lexicon relationship from the perspective of Distributed Morphology, focusing on transitivity alternations, nominalization, and adjectival passives. The main question addressed is the locus of introduction of arguments, focusing on external and internal arguments as well as on different types of external arguments. The Distributed Morphology perspective is compared to Ramchand’s (2008, 2018) First Phase Syntax framework, and Borer’s (2005a, b, 2013) Exoskeletal model. The chapter is structured from the perspective of a theory of grammar that dispenses with the lexicon component and discusses the consequences of this choice for argument introduction.",
      "abstract": "Abstract Taking as point of departure Chomsky’s Remarks, this chapter discusses the relationship between syntax and the lexicon and how this has changed in the last 50 years. It then examines the syntax-lexicon relationship from the perspective of Distributed Morphology, focusing on transitivity alternations, nominalization, and adjectival passives. The main question addressed is the locus of introduction of arguments, focusing on external and internal arguments as well as on different types of external arguments. The Distributed Morphology perspective is compared to Ramchand’s (2008, 2018) First Phase Syntax framework, and Borer’s (2005a, b, 2013) Exoskeletal model. The chapter is structured from the perspective of a theory of grammar that dispenses with the lexicon component and discusses the consequences of this choice for argument introduction.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.2",
      "openalex_id": "https://openalex.org/W4213124799",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a theoretical linguistics chapter on syntax-lexicon and distributed morphology, with no discussion of discrete audio tokenization or speech token representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a theoretical linguistics chapter on syntax-lexicon and distributed morphology, with no discussion of discrete audio tokenization or speech token representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on theoretical linguistic analysis regarding syntax and the lexicon, discussing models of argument introduction and grammatical theory, without any mention of audio processing, discrete audio tokenization, or related computational methods for speech signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on theoretical linguistic analysis regarding syntax and the lexicon, discussing models of argument introduction and grammatical theory, without any mention of audio processing, discrete audio tokenization, or related computational methods for speech signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Differences in vocabulary growth across groups and individuals",
    "abstract": "Abstract This chapter describes causes and consequences of individual differences in young children’s word learning. For decades, research has documented qualitative and quantitative differences in children’s language input, and it has been convincingly demonstrated that across different communities, children’s vocabulary growth can be linked to their language experiences. However, children also actively shape their own learning environment, and it is important to consider how their cognitive abilities, as well as their interests, shape their language experiences and their learning. Only by examining children with a wide range of experiences (e.g., children growing up in multilingual communities) and abilities (e.g., children with developmental delays or disorders) will it be possible to develop theories that adequately capture and explain differences in children’s word learning and vocabulary growth.",
    "metadata": {
      "title": "Differences in vocabulary growth across groups and individuals",
      "summary": "Abstract This chapter describes causes and consequences of individual differences in young children’s word learning. For decades, research has documented qualitative and quantitative differences in children’s language input, and it has been convincingly demonstrated that across different communities, children’s vocabulary growth can be linked to their language experiences. However, children also actively shape their own learning environment, and it is important to consider how their cognitive abilities, as well as their interests, shape their language experiences and their learning. Only by examining children with a wide range of experiences (e.g., children growing up in multilingual communities) and abilities (e.g., children with developmental delays or disorders) will it be possible to develop theories that adequately capture and explain differences in children’s word learning and vocabulary growth.",
      "abstract": "Abstract This chapter describes causes and consequences of individual differences in young children’s word learning. For decades, research has documented qualitative and quantitative differences in children’s language input, and it has been convincingly demonstrated that across different communities, children’s vocabulary growth can be linked to their language experiences. However, children also actively shape their own learning environment, and it is important to consider how their cognitive abilities, as well as their interests, shape their language experiences and their learning. Only by examining children with a wide range of experiences (e.g., children growing up in multilingual communities) and abilities (e.g., children with developmental delays or disorders) will it be possible to develop theories that adequately capture and explain differences in children’s word learning and vocabulary growth.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.22",
      "openalex_id": "https://openalex.org/W4213154137",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on factors affecting vocabulary growth and individual differences without addressing discrete audio tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on factors affecting vocabulary growth and individual differences without addressing discrete audio tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on individual differences in children's vocabulary growth and language experiences, without discussing discrete audio/speech tokenization or any related analysis or methodology involving converting audio into discrete symbols; hence, it does not meet the inclusion criteria regarding discrete tokenization of audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on individual differences in children's vocabulary growth and language experiences, without discussing discrete audio/speech tokenization or any related analysis or methodology involving converting audio into discrete symbols; hence, it does not meet the inclusion criteria regarding discrete tokenization of audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Logic and The Lexicon",
    "abstract": "Abstract This chapter focuses on a special instance of logical vocabulary, namely modal words, like “might” or “must,” which express possibility and necessity. Modal statements involve a complex interplay of morphology, syntax, semantics, and pragmatics, which make it particularly challenging to identify what lexical meanings the modal words encode. This chapter surveys how possibilities and necessities are expressed in natural language, with an eye toward cross-linguistic similarity and variation, and introduces the framework that formal semantics inherits from modal logic to analyze modal statements. It then turns to the challenges—for both the semanticist and for the child learner—of figuring out the right division of labor between semantics and pragmatics for modal statements, and the exact lexical contributions of the modal words themselves.",
    "metadata": {
      "title": "Logic and The Lexicon",
      "summary": "Abstract This chapter focuses on a special instance of logical vocabulary, namely modal words, like “might” or “must,” which express possibility and necessity. Modal statements involve a complex interplay of morphology, syntax, semantics, and pragmatics, which make it particularly challenging to identify what lexical meanings the modal words encode. This chapter surveys how possibilities and necessities are expressed in natural language, with an eye toward cross-linguistic similarity and variation, and introduces the framework that formal semantics inherits from modal logic to analyze modal statements. It then turns to the challenges—for both the semanticist and for the child learner—of figuring out the right division of labor between semantics and pragmatics for modal statements, and the exact lexical contributions of the modal words themselves.",
      "abstract": "Abstract This chapter focuses on a special instance of logical vocabulary, namely modal words, like “might” or “must,” which express possibility and necessity. Modal statements involve a complex interplay of morphology, syntax, semantics, and pragmatics, which make it particularly challenging to identify what lexical meanings the modal words encode. This chapter surveys how possibilities and necessities are expressed in natural language, with an eye toward cross-linguistic similarity and variation, and introduces the framework that formal semantics inherits from modal logic to analyze modal statements. It then turns to the challenges—for both the semanticist and for the child learner—of figuring out the right division of labor between semantics and pragmatics for modal statements, and the exact lexical contributions of the modal words themselves.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.4",
      "openalex_id": "https://openalex.org/W4213190168",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract investigates modal semantics in linguistics and does not address discrete audio/speech tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract investigates modal semantics in linguistics and does not address discrete audio/speech tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus entirely on linguistic theory pertaining to modal words and their semantics and pragmatics, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing; thus, this work does not meet the inclusion criteria and is unrelated to discrete audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus entirely on linguistic theory pertaining to modal words and their semantics and pragmatics, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing; thus, this work does not meet the inclusion criteria and is unrelated to discrete audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Compositionality of concepts",
    "abstract": "Abstract This chapter focuses on the problem of concept composition: to obtain a complex concept such as RED SQUARE, the mind has to be able to combine simple concepts, RED and SQUARE. It is argued here that compositionality constraint is a necessary element of any cognitively plausible theory of concepts. The chapter provides an overview of the theories of concepts that have been particularly influential in cognitive science, such as the Inferential Roles Semantics and the Prototype theory. At the same time, it aims to show how these theories still fall short of providing a satisfying solution for compositionality constraint. The chapter concludes by emphasizing the importance of compositionality constraint by stating that it should be regarded as a critical (rather than a secondary) concern that guides both theoretical and empirical research on concept representations.",
    "metadata": {
      "title": "Compositionality of concepts",
      "summary": "Abstract This chapter focuses on the problem of concept composition: to obtain a complex concept such as RED SQUARE, the mind has to be able to combine simple concepts, RED and SQUARE. It is argued here that compositionality constraint is a necessary element of any cognitively plausible theory of concepts. The chapter provides an overview of the theories of concepts that have been particularly influential in cognitive science, such as the Inferential Roles Semantics and the Prototype theory. At the same time, it aims to show how these theories still fall short of providing a satisfying solution for compositionality constraint. The chapter concludes by emphasizing the importance of compositionality constraint by stating that it should be regarded as a critical (rather than a secondary) concern that guides both theoretical and empirical research on concept representations.",
      "abstract": "Abstract This chapter focuses on the problem of concept composition: to obtain a complex concept such as RED SQUARE, the mind has to be able to combine simple concepts, RED and SQUARE. It is argued here that compositionality constraint is a necessary element of any cognitively plausible theory of concepts. The chapter provides an overview of the theories of concepts that have been particularly influential in cognitive science, such as the Inferential Roles Semantics and the Prototype theory. At the same time, it aims to show how these theories still fall short of providing a satisfying solution for compositionality constraint. The chapter concludes by emphasizing the importance of compositionality constraint by stating that it should be regarded as a critical (rather than a secondary) concern that guides both theoretical and empirical research on concept representations.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.20",
      "openalex_id": "https://openalex.org/W4213352237",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses cognitive compositionality of concepts and theories, not discrete audio/speech tokenization or token-level representations, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses cognitive compositionality of concepts and theories, not discrete audio/speech tokenization or token-level representations, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on concept compositionality in cognitive science, discussing conceptual theories without any mention of discrete audio/speech tokenization, neural encoding, or token-based audio representations relevant to speech processing tasks, hence it does not meet any inclusion criteria and is unrelated to the discrete audio tokenization domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on concept compositionality in cognitive science, discussing conceptual theories without any mention of discrete audio/speech tokenization, neural encoding, or token-based audio representations relevant to speech processing tasks, hence it does not meet any inclusion criteria and is unrelated to the discrete audio tokenization domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Preface and Acknowledgments",
    "abstract": "The Preface and Acknowledgements section provides a personal look by the primary author into the initial request for a simplified form of signing to use with persons with autism at the Grafton School in Virginia in the 1980s. Since little research into the development of sign language skills in children had been done at that time, the primary author felt that investigators first needed to document sign language acquisition by the typically developing signing children of Deaf parents. After this initial foundation of knowledge was established, then the focus could turn to sign language acquisition by atypical populations. The onset of the actual Simplified Sign project in 1998, and its subsequent development over the decades since, are covered with a focus on the contributions made by the authors as well as by the many University of Virginia students involved in the sign testing and by various persons who provided feedback on earlier versions of the chapters in this manuscript. Finally, a short Postscript by the primary author’s brother summarizes the work done on the Simplified Sign project since the passing of Dr. John Bonvillian in 2018.",
    "metadata": {
      "title": "Preface and Acknowledgments",
      "summary": "The Preface and Acknowledgements section provides a personal look by the primary author into the initial request for a simplified form of signing to use with persons with autism at the Grafton School in Virginia in the 1980s. Since little research into the development of sign language skills in children had been done at that time, the primary author felt that investigators first needed to document sign language acquisition by the typically developing signing children of Deaf parents. After this initial foundation of knowledge was established, then the focus could turn to sign language acquisition by atypical populations. The onset of the actual Simplified Sign project in 1998, and its subsequent development over the decades since, are covered with a focus on the contributions made by the authors as well as by the many University of Virginia students involved in the sign testing and by various persons who provided feedback on earlier versions of the chapters in this manuscript. Finally, a short Postscript by the primary author’s brother summarizes the work done on the Simplified Sign project since the passing of Dr. John Bonvillian in 2018.",
      "abstract": "The Preface and Acknowledgements section provides a personal look by the primary author into the initial request for a simplified form of signing to use with persons with autism at the Grafton School in Virginia in the 1980s. Since little research into the development of sign language skills in children had been done at that time, the primary author felt that investigators first needed to document sign language acquisition by the typically developing signing children of Deaf parents. After this initial foundation of knowledge was established, then the focus could turn to sign language acquisition by atypical populations. The onset of the actual Simplified Sign project in 1998, and its subsequent development over the decades since, are covered with a focus on the contributions made by the authors as well as by the many University of Virginia students involved in the sign testing and by various persons who provided feedback on earlier versions of the chapters in this manuscript. Finally, a short Postscript by the primary author’s brother summarizes the work done on the Simplified Sign project since the passing of Dr. John Bonvillian in 2018.",
      "doi": "https://doi.org/10.11647/obp.0205.10",
      "openalex_id": "https://openalex.org/W4230164465",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokenization; the abstract discusses sign language development and project history instead.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokenization; the abstract discusses sign language development and project history instead.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a historical and developmental overview of sign language acquisition research and the Simplified Sign project; there is no mention of discrete audio/speech tokenization or related technical contributions involving audio signal processing or discrete token representations, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a historical and developmental overview of sign language acquisition research and the Simplified Sign project; there is no mention of discrete audio/speech tokenization or related technical contributions involving audio signal processing or discrete token representations, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Case Study of Suspected Childhood Apraxia of Sign",
    "abstract": "Purpose: We provide a case report of “Zoe,” a 4-year-old deaf child from a deaf signing family, who presented with a possible case of Childhood Apraxia of Sign (CASign). Method: The description is based on reports from the child's speech-language pathologist, her Individualized Education Program report, and a clinician-created sign imitation task. Results: Zoe's sign articulation errors in American Sign Language differed from those reported for typically developing deaf children and were parallel to the types of errors observed for hearing children with childhood apraxia of speech. Specifically, Zoe produced inconsistent errors across signs, substituted more complex handshapes for simple handshapes, made errors on both unmarked (common) and marked (less common) forms, produced articulatory distortions (rather than substitutions), and exhibited “groping” behaviors (a sequence of attempts to move her hands into the correct position). In addition, Zoe sometimes self-corrected her errors by manipulating her own hands, for example, using her left hand to move the thumb of her right hand into the correct position. Conclusion: Zoe's pattern of sign errors is consistent with an underlying deficit in motor planning and/or programming and may constitute the first reported case of CASign.",
    "metadata": {
      "title": "A Case Study of Suspected Childhood Apraxia of Sign",
      "summary": "Purpose: We provide a case report of “Zoe,” a 4-year-old deaf child from a deaf signing family, who presented with a possible case of Childhood Apraxia of Sign (CASign). Method: The description is based on reports from the child's speech-language pathologist, her Individualized Education Program report, and a clinician-created sign imitation task. Results: Zoe's sign articulation errors in American Sign Language differed from those reported for typically developing deaf children and were parallel to the types of errors observed for hearing children with childhood apraxia of speech. Specifically, Zoe produced inconsistent errors across signs, substituted more complex handshapes for simple handshapes, made errors on both unmarked (common) and marked (less common) forms, produced articulatory distortions (rather than substitutions), and exhibited “groping” behaviors (a sequence of attempts to move her hands into the correct position). In addition, Zoe sometimes self-corrected her errors by manipulating her own hands, for example, using her left hand to move the thumb of her right hand into the correct position. Conclusion: Zoe's pattern of sign errors is consistent with an underlying deficit in motor planning and/or programming and may constitute the first reported case of CASign.",
      "abstract": "Purpose: We provide a case report of “Zoe,” a 4-year-old deaf child from a deaf signing family, who presented with a possible case of Childhood Apraxia of Sign (CASign). Method: The description is based on reports from the child's speech-language pathologist, her Individualized Education Program report, and a clinician-created sign imitation task. Results: Zoe's sign articulation errors in American Sign Language differed from those reported for typically developing deaf children and were parallel to the types of errors observed for hearing children with childhood apraxia of speech. Specifically, Zoe produced inconsistent errors across signs, substituted more complex handshapes for simple handshapes, made errors on both unmarked (common) and marked (less common) forms, produced articulatory distortions (rather than substitutions), and exhibited “groping” behaviors (a sequence of attempts to move her hands into the correct position). In addition, Zoe sometimes self-corrected her errors by manipulating her own hands, for example, using her left hand to move the thumb of her right hand into the correct position. Conclusion: Zoe's pattern of sign errors is consistent with an underlying deficit in motor planning and/or programming and may constitute the first reported case of CASign.",
      "doi": "https://doi.org/10.1044/2024_persp-24-00042",
      "openalex_id": "https://openalex.org/W4401386165",
      "arxiv_id": "",
      "publication_date": "2024-08-07",
      "published": "2024-08-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a case study of suspected Childhood Apraxia of Sign focusing on motor planning and sign articulation errors; it does not involve discrete audio/speech tokenization, token design, or analysis of discrete audio representations for speech tasks, therefore it fails the inclusion criteria and meets at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a case study of suspected Childhood Apraxia of Sign focusing on motor planning and sign articulation errors; it does not involve discrete audio/speech tokenization, token design, or analysis of discrete audio representations for speech tasks, therefore it fails the inclusion criteria and meets at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study is a case report on Childhood Apraxia of Sign focusing on sign language motor planning and articulation errors, with no mention of audio/speech tokenization or discrete audio signal processing. Thus, it does not meet the inclusion criteria related to discrete audio tokenization, and does not address any relevant tokenization methodology or experiments.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study is a case report on Childhood Apraxia of Sign focusing on sign language motor planning and articulation errors, with no mention of audio/speech tokenization or discrete audio signal processing. Thus, it does not meet the inclusion criteria related to discrete audio tokenization, and does not address any relevant tokenization methodology or experiments.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Text Comprehension and Grammar",
    "abstract": "Abstract Chapter 5 provides an overview of grammar knowledge and its relation to reading comprehension. A structure for reading instruction is provided, including use of all languages across guided, shared, and independent reading and writing. Evidence-based strategies for all areas of instruction are embedded across the chapter, such as explicit instruction, the language experience approach, directed reading and thinking, writer’s workshop, narrative story grammar, story maps, visualization, and reflections. Case vignettes include application of prediction, visualization, and summarizing during reading with diverse deaf/hard-of-hearing (DHH) learners. Tables include examples of complex grammatical structures, story grammar elements, sentence-parsing strategies, strategies used within multilingual deaf education classrooms, how to teach strategies, and a review of story signing and story reading.",
    "metadata": {
      "title": "Text Comprehension and Grammar",
      "summary": "Abstract Chapter 5 provides an overview of grammar knowledge and its relation to reading comprehension. A structure for reading instruction is provided, including use of all languages across guided, shared, and independent reading and writing. Evidence-based strategies for all areas of instruction are embedded across the chapter, such as explicit instruction, the language experience approach, directed reading and thinking, writer’s workshop, narrative story grammar, story maps, visualization, and reflections. Case vignettes include application of prediction, visualization, and summarizing during reading with diverse deaf/hard-of-hearing (DHH) learners. Tables include examples of complex grammatical structures, story grammar elements, sentence-parsing strategies, strategies used within multilingual deaf education classrooms, how to teach strategies, and a review of story signing and story reading.",
      "abstract": "Abstract Chapter 5 provides an overview of grammar knowledge and its relation to reading comprehension. A structure for reading instruction is provided, including use of all languages across guided, shared, and independent reading and writing. Evidence-based strategies for all areas of instruction are embedded across the chapter, such as explicit instruction, the language experience approach, directed reading and thinking, writer’s workshop, narrative story grammar, story maps, visualization, and reflections. Case vignettes include application of prediction, visualization, and summarizing during reading with diverse deaf/hard-of-hearing (DHH) learners. Tables include examples of complex grammatical structures, story grammar elements, sentence-parsing strategies, strategies used within multilingual deaf education classrooms, how to teach strategies, and a review of story signing and story reading.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0005",
      "openalex_id": "https://openalex.org/W4401771202",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "No explicit focus on discrete audio/speech tokenization; the abstract centers on grammar and reading instruction for deaf/hard-of-hearing learners, not on tokenization or discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "No explicit focus on discrete audio/speech tokenization; the abstract centers on grammar and reading instruction for deaf/hard-of-hearing learners, not on tokenization or discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus entirely on grammar knowledge and reading comprehension instruction, especially for deaf/hard-of-hearing learners, without any mention of audio/speech tokenization, discrete token analysis, or related neural encoding techniques; thus, it does not meet the inclusion criteria nor relate to any of the discrete audio tokenization aspects required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus entirely on grammar knowledge and reading comprehension instruction, especially for deaf/hard-of-hearing learners, without any mention of audio/speech tokenization, discrete token analysis, or related neural encoding techniques; thus, it does not meet the inclusion criteria nor relate to any of the discrete audio tokenization aspects required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Introduction and Overview",
    "abstract": "Abstract Chapter 1 provides an overview of the diversity of DHH learners. This range of diversity is wide and includes language(s), listening technology, literacy outcomes, early intervention, and learning environments. The chapter then shows how literacy theories and models are applied to DHH learners and it goes on to present considerations for literacy instruction, such as learner motivation, learning standards, accommodations and modifications, and learning environment. A discussion of criteria that define evidence-based instructional practices and causal factors is presented. Finally, a review of curricula for DHH learners and how to make informed choices during instruction with DHH learners closes the chapter.",
    "metadata": {
      "title": "Introduction and Overview",
      "summary": "Abstract Chapter 1 provides an overview of the diversity of DHH learners. This range of diversity is wide and includes language(s), listening technology, literacy outcomes, early intervention, and learning environments. The chapter then shows how literacy theories and models are applied to DHH learners and it goes on to present considerations for literacy instruction, such as learner motivation, learning standards, accommodations and modifications, and learning environment. A discussion of criteria that define evidence-based instructional practices and causal factors is presented. Finally, a review of curricula for DHH learners and how to make informed choices during instruction with DHH learners closes the chapter.",
      "abstract": "Abstract Chapter 1 provides an overview of the diversity of DHH learners. This range of diversity is wide and includes language(s), listening technology, literacy outcomes, early intervention, and learning environments. The chapter then shows how literacy theories and models are applied to DHH learners and it goes on to present considerations for literacy instruction, such as learner motivation, learning standards, accommodations and modifications, and learning environment. A discussion of criteria that define evidence-based instructional practices and causal factors is presented. Finally, a review of curricula for DHH learners and how to make informed choices during instruction with DHH learners closes the chapter.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0001",
      "openalex_id": "https://openalex.org/W4401775482",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on literacy development for DHH learners and instructional considerations; it does not involve discrete audio tokenization research, token design, or analysis, hence fails the inclusion criterion for discrete tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on literacy development for DHH learners and instructional considerations; it does not involve discrete audio tokenization research, token design, or analysis, hence fails the inclusion criterion for discrete tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a chapter focused on literacy and education for Deaf and Hard of Hearing (DHH) learners, without any mention of audio or speech tokenization, discrete token design, or neural encoding relevant to audio signal processing, which are core to the inclusion criteria; therefore, the study does not meet inclusion needs and falls outside the scope of discrete audio/speech tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a chapter focused on literacy and education for Deaf and Hard of Hearing (DHH) learners, without any mention of audio or speech tokenization, discrete token design, or neural encoding relevant to audio signal processing, which are core to the inclusion criteria; therefore, the study does not meet inclusion needs and falls outside the scope of discrete audio/speech tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Early Production of Imperceptible Words by Infants and Toddlers Born Deaf or Blind",
    "abstract": "Abstract We investigate the roles of linguistic and sensory experience in the early-produced visual, auditory, and abstract words of congenitally-blind toddlers, deaf toddlers, and typically-sighted/hearing peers. We also assess the role of language access by comparing early word production in children learning English or American Sign Language (ASL) from birth, versus at a delay. Using parental report data on child word production from the MacArthur-Bates Communicative Development Inventory, we found evidence that while children produced words referring to imperceptible referents before age 2, such words were less likely to be produced relative to words with perceptible referents. For instance, blind (vs. sighted) children said fewer highly visual words like “blue” or “see”; deaf signing (vs. hearing) children produced fewer auditory signs like hear. Additionally, in spoken English and ASL, children who received delayed language access were less likely to produce words overall. These results demonstrate and begin to quantify how linguistic and sensory access may influence which words young children produce.",
    "metadata": {
      "title": "Early Production of Imperceptible Words by Infants and Toddlers Born Deaf or Blind",
      "summary": "Abstract We investigate the roles of linguistic and sensory experience in the early-produced visual, auditory, and abstract words of congenitally-blind toddlers, deaf toddlers, and typically-sighted/hearing peers. We also assess the role of language access by comparing early word production in children learning English or American Sign Language (ASL) from birth, versus at a delay. Using parental report data on child word production from the MacArthur-Bates Communicative Development Inventory, we found evidence that while children produced words referring to imperceptible referents before age 2, such words were less likely to be produced relative to words with perceptible referents. For instance, blind (vs. sighted) children said fewer highly visual words like “blue” or “see”; deaf signing (vs. hearing) children produced fewer auditory signs like hear. Additionally, in spoken English and ASL, children who received delayed language access were less likely to produce words overall. These results demonstrate and begin to quantify how linguistic and sensory access may influence which words young children produce.",
      "abstract": "Abstract We investigate the roles of linguistic and sensory experience in the early-produced visual, auditory, and abstract words of congenitally-blind toddlers, deaf toddlers, and typically-sighted/hearing peers. We also assess the role of language access by comparing early word production in children learning English or American Sign Language (ASL) from birth, versus at a delay. Using parental report data on child word production from the MacArthur-Bates Communicative Development Inventory, we found evidence that while children produced words referring to imperceptible referents before age 2, such words were less likely to be produced relative to words with perceptible referents. For instance, blind (vs. sighted) children said fewer highly visual words like “blue” or “see”; deaf signing (vs. hearing) children produced fewer auditory signs like hear. Additionally, in spoken English and ASL, children who received delayed language access were less likely to produce words overall. These results demonstrate and begin to quantify how linguistic and sensory access may influence which words young children produce.",
      "doi": "https://doi.org/10.1162/opmi_a_00197",
      "openalex_id": "https://openalex.org/W4409260542",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract investigates early word production in deaf/blind/typical children and the effect of sensory access, but it does not propose or analyze discrete audio/speech tokenization or token-level representations, so it fails the inclusion criteria requiring explicit tokenization of audio.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract investigates early word production in deaf/blind/typical children and the effect of sensory access, but it does not propose or analyze discrete audio/speech tokenization or token-level representations, so it fails the inclusion criteria requiring explicit tokenization of audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on early language production in infants and toddlers with sensory impairments and does not address discrete audio or speech tokenization methods, nor does it analyze or propose tokenization of continuous audio signals into discrete tokens for language-model tasks, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on early language production in infants and toddlers with sensory impairments and does not address discrete audio or speech tokenization methods, nor does it analyze or propose tokenization of continuous audio signals into discrete tokens for language-model tasks, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Initial signs of learning: Decoding newly-learned vocabulary from neural patterns in novice sign language learners",
    "abstract": "How do novice language learners represent semantic information in their new language? The extent to which multiple languages are supported by divergent or overlapping semantic representations in bilinguals has been well-studied, but less is known about how new knowledge is integrated into established representational networks at the earliest stages of acquisition. Furthermore, examining language across modality (sign vs. speech) can provide unique insight into language unconfounded by perceptual features. We present two experiments in which hearing non-signers underwent brief training in American Sign Language (ASL) followed by fMRI scanning. Across both datasets (N=50), we use representational similarity analysis (RSA) to identify brain regions where neural patterns reflect semantic relationships between stimuli. In Study 2 (N=40) we show that multivariate neural measures of semantic representation in several frontal, temporal, and occipital regions reflect individual participant-level comprehension. These results demonstrate the role of frontal and temporal regions, especially bilateral superior temporal sulcus, in representing semantic content across language and modality in novice learners.",
    "metadata": {
      "title": "Initial signs of learning: Decoding newly-learned vocabulary from neural patterns in novice sign language learners",
      "summary": "How do novice language learners represent semantic information in their new language? The extent to which multiple languages are supported by divergent or overlapping semantic representations in bilinguals has been well-studied, but less is known about how new knowledge is integrated into established representational networks at the earliest stages of acquisition. Furthermore, examining language across modality (sign vs. speech) can provide unique insight into language unconfounded by perceptual features. We present two experiments in which hearing non-signers underwent brief training in American Sign Language (ASL) followed by fMRI scanning. Across both datasets (N=50), we use representational similarity analysis (RSA) to identify brain regions where neural patterns reflect semantic relationships between stimuli. In Study 2 (N=40) we show that multivariate neural measures of semantic representation in several frontal, temporal, and occipital regions reflect individual participant-level comprehension. These results demonstrate the role of frontal and temporal regions, especially bilateral superior temporal sulcus, in representing semantic content across language and modality in novice learners.",
      "abstract": "How do novice language learners represent semantic information in their new language? The extent to which multiple languages are supported by divergent or overlapping semantic representations in bilinguals has been well-studied, but less is known about how new knowledge is integrated into established representational networks at the earliest stages of acquisition. Furthermore, examining language across modality (sign vs. speech) can provide unique insight into language unconfounded by perceptual features. We present two experiments in which hearing non-signers underwent brief training in American Sign Language (ASL) followed by fMRI scanning. Across both datasets (N=50), we use representational similarity analysis (RSA) to identify brain regions where neural patterns reflect semantic relationships between stimuli. In Study 2 (N=40) we show that multivariate neural measures of semantic representation in several frontal, temporal, and occipital regions reflect individual participant-level comprehension. These results demonstrate the role of frontal and temporal regions, especially bilateral superior temporal sulcus, in representing semantic content across language and modality in novice learners.",
      "doi": "https://doi.org/10.1101/2025.04.11.648265",
      "openalex_id": "https://openalex.org/W4409723404",
      "arxiv_id": "",
      "publication_date": "2025-04-17",
      "published": "2025-04-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates semantic representations in novice ASL learners using fMRI and RSA, but it does not involve discrete audio tokenization or analysis of discrete tokens, which is a core inclusion criterion; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates semantic representations in novice ASL learners using fMRI and RSA, but it does not involve discrete audio tokenization or analysis of discrete tokens, which is a core inclusion criterion; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the neural representation of semantic information in novice sign language learners using fMRI and representational similarity analysis, without any mention of discrete audio or speech tokenization or conversion of continuous audio signals into discrete symbols; hence, it does not meet the inclusion criteria related to audio tokenization, nor does it address discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the neural representation of semantic information in novice sign language learners using fMRI and representational similarity analysis, without any mention of discrete audio or speech tokenization or conversion of continuous audio signals into discrete symbols; hence, it does not meet the inclusion criteria related to audio tokenization, nor does it address discrete audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Methods to study evolution of iconicity in sign languages",
    "abstract": "Abstract Sign languages—the conventional languages of deaf communities—have been considered to provide a window into answering some questions regarding language emergence and evolution. In particular, iconicity, defined as the ‘existence of a structure-preserving mapping between mental models of linguistic form and meaning’, is generally regarded as a precursor to the arbitrary and segmental categorical structures found in spoken languages. However, iconic structures are omnipresent in sign languages at all levels of linguistic organization. Thus, there is a necessity for a more nuanced understanding of iconicity and its trajectory in language evolution. In this chapter, we outline different quantitative and qualitative methods to study iconicity and how one can operationalize them at lexical and discourse levels to investigate the role of iconicity in the evolution of sign languages.",
    "metadata": {
      "title": "Methods to study evolution of iconicity in sign languages",
      "summary": "Abstract Sign languages—the conventional languages of deaf communities—have been considered to provide a window into answering some questions regarding language emergence and evolution. In particular, iconicity, defined as the ‘existence of a structure-preserving mapping between mental models of linguistic form and meaning’, is generally regarded as a precursor to the arbitrary and segmental categorical structures found in spoken languages. However, iconic structures are omnipresent in sign languages at all levels of linguistic organization. Thus, there is a necessity for a more nuanced understanding of iconicity and its trajectory in language evolution. In this chapter, we outline different quantitative and qualitative methods to study iconicity and how one can operationalize them at lexical and discourse levels to investigate the role of iconicity in the evolution of sign languages.",
      "abstract": "Abstract Sign languages—the conventional languages of deaf communities—have been considered to provide a window into answering some questions regarding language emergence and evolution. In particular, iconicity, defined as the ‘existence of a structure-preserving mapping between mental models of linguistic form and meaning’, is generally regarded as a precursor to the arbitrary and segmental categorical structures found in spoken languages. However, iconic structures are omnipresent in sign languages at all levels of linguistic organization. Thus, there is a necessity for a more nuanced understanding of iconicity and its trajectory in language evolution. In this chapter, we outline different quantitative and qualitative methods to study iconicity and how one can operationalize them at lexical and discourse levels to investigate the role of iconicity in the evolution of sign languages.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780192886491.013.11",
      "openalex_id": "https://openalex.org/W4410593440",
      "arxiv_id": "",
      "publication_date": "2025-05-22",
      "published": "2025-05-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses iconicity in sign languages and outlines methods to study it, but it does not propose or analyze discrete audio tokenization or a sequence of discrete audio symbols for speech-related tasks, which is required by the inclusion criteria; thus it does not meet the core criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses iconicity in sign languages and outlines methods to study it, but it does not propose or analyze discrete audio tokenization or a sequence of discrete audio symbols for speech-related tasks, which is required by the inclusion criteria; thus it does not meet the core criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on methods to study evolution of iconicity in sign languages and does not address discrete audio/speech tokenization or related topics involving converting continuous audio signals to discrete tokens; hence it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on methods to study evolution of iconicity in sign languages and does not address discrete audio/speech tokenization or related topics involving converting continuous audio signals to discrete tokens; hence it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perceptual-semantic features of words differentially shape early vocabulary in American Sign Language and English",
    "abstract": "Abstract How do sensory experiences shape the words we learn first? Most studies of language have focused on hearing children learning spoken languages, making it challenging to know how sound and language modality might contribute to language learning. This study investigates how perceptual and semantic features influence early vocabulary acquisition in deaf children learning American Sign Language and hearing children learning spoken English. Using vocabulary data from parent-report inventories, we analyzed 214 nouns common to both languages to compare the types of meanings associated with earlier Age of Acquisition. Results revealed that while children in both groups were earlier to acquire words that were more strongly related to the senses, the specific types of sensory meaning varied by language modality. Hearing children learned words with sound-related features earlier than other words, while deaf children learned words with visual and touch-related features earlier. This suggests that the easiest words to learn are words with meanings that children can experience first-hand, which varies based on children’s own sensory access and experience. Studying the diverse ways children acquire language, in this case deaf children, is key to developing language learning theories that reflect all learners.",
    "metadata": {
      "title": "Perceptual-semantic features of words differentially shape early vocabulary in American Sign Language and English",
      "summary": "Abstract How do sensory experiences shape the words we learn first? Most studies of language have focused on hearing children learning spoken languages, making it challenging to know how sound and language modality might contribute to language learning. This study investigates how perceptual and semantic features influence early vocabulary acquisition in deaf children learning American Sign Language and hearing children learning spoken English. Using vocabulary data from parent-report inventories, we analyzed 214 nouns common to both languages to compare the types of meanings associated with earlier Age of Acquisition. Results revealed that while children in both groups were earlier to acquire words that were more strongly related to the senses, the specific types of sensory meaning varied by language modality. Hearing children learned words with sound-related features earlier than other words, while deaf children learned words with visual and touch-related features earlier. This suggests that the easiest words to learn are words with meanings that children can experience first-hand, which varies based on children’s own sensory access and experience. Studying the diverse ways children acquire language, in this case deaf children, is key to developing language learning theories that reflect all learners.",
      "abstract": "Abstract How do sensory experiences shape the words we learn first? Most studies of language have focused on hearing children learning spoken languages, making it challenging to know how sound and language modality might contribute to language learning. This study investigates how perceptual and semantic features influence early vocabulary acquisition in deaf children learning American Sign Language and hearing children learning spoken English. Using vocabulary data from parent-report inventories, we analyzed 214 nouns common to both languages to compare the types of meanings associated with earlier Age of Acquisition. Results revealed that while children in both groups were earlier to acquire words that were more strongly related to the senses, the specific types of sensory meaning varied by language modality. Hearing children learned words with sound-related features earlier than other words, while deaf children learned words with visual and touch-related features earlier. This suggests that the easiest words to learn are words with meanings that children can experience first-hand, which varies based on children’s own sensory access and experience. Studying the diverse ways children acquire language, in this case deaf children, is key to developing language learning theories that reflect all learners.",
      "doi": "https://doi.org/10.1017/s0142716425100210",
      "openalex_id": "https://openalex.org/W4414844883",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses perceptual and semantic features in ASL and English vocabulary learning but does not involve discrete audio/speech tokenization or analysis of discrete token representations for speech tasks, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses perceptual and semantic features in ASL and English vocabulary learning but does not involve discrete audio/speech tokenization or analysis of discrete token representations for speech tasks, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on how perceptual and semantic features influence early vocabulary acquisition in American Sign Language and English, without any mention or analysis of discrete audio/speech tokenization of continuous audio signals into discrete tokens, nor any discussion related to neural encoding or modern quantization-based discrete token representations. Therefore, it does not meet the core inclusion criterion related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on how perceptual and semantic features influence early vocabulary acquisition in American Sign Language and English, without any mention or analysis of discrete audio/speech tokenization of continuous audio signals into discrete tokens, nor any discussion related to neural encoding or modern quantization-based discrete token representations. Therefore, it does not meet the core inclusion criterion related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone",
    "abstract": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
    "metadata": {
      "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone",
      "summary": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "abstract": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "doi": "https://doi.org/10.48550/arxiv.2112.02418",
      "openalex_id": "https://openalex.org/W4200631896",
      "arxiv_id": "",
      "publication_date": "2021-12-04",
      "published": "2021-12-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on zero-shot multi-speaker TTS and voice conversion without any explicit discrete audio tokenization or tokenization analysis, which is required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on zero-shot multi-speaker TTS and voice conversion without any explicit discrete audio tokenization or tokenization analysis, which is required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot multi-speaker TTS and voice conversion using a modified VITS model, but the abstract does not mention any discrete audio/speech tokenization or conversion of continuous signals into discrete token sequences, which is required by the inclusion criteria. It appears to rely on continuous feature representations without discrete token analysis, and thus does not meet the core inclusion requirement.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot multi-speaker TTS and voice conversion using a modified VITS model, but the abstract does not mention any discrete audio/speech tokenization or conversion of continuous signals into discrete token sequences, which is required by the inclusion criteria. It appears to rely on continuous feature representations without discrete token analysis, and thus does not meet the core inclusion requirement.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
    "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
    "metadata": {
      "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
      "summary": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2022-489",
      "openalex_id": "https://openalex.org/W4226132755",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete vector-quantized (VQ) acoustic features as the core representation for TTS, i.e., a tokenized discrete audio representation, not just continuous mel-spectrograms, so it meets inclusion and should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete vector-quantized (VQ) acoustic features as the core representation for TTS, i.e., a tokenized discrete audio representation, not just continuous mel-spectrograms, so it meets inclusion and should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel TTS system, VQTTS, that uses a self-supervised vector-quantized acoustic feature as a discrete tokenization method instead of continuous mel-spectrograms, explicitly focusing on quantized acoustic features for speech synthesis and demonstrating state-of-the-art performance, hence meeting the inclusion criteria without violating any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel TTS system, VQTTS, that uses a self-supervised vector-quantized acoustic feature as a discrete tokenization method instead of continuous mel-spectrograms, explicitly focusing on quantized acoustic features for speech synthesis and demonstrating state-of-the-art performance, hence meeting the inclusion criteria without violating any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A pitch extraction algorithm tuned for automatic speech recognition",
    "abstract": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
    "metadata": {
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "summary": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "abstract": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854049",
      "openalex_id": "https://openalex.org/W2085628288",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper develops a pitch tracking method and a voicing probability feature for ASR; it treats pitch as a continuous feature rather than discretizing audio into a finite-symbol token sequence, thus not meeting the discrete-token inclusion criterion and meeting the exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper develops a pitch tracking method and a voicing probability feature for ASR; it treats pitch as a continuous feature rather than discretizing audio into a finite-symbol token sequence, thus not meeting the discrete-token inclusion criterion and meeting the exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an algorithm for pitch extraction as a continuous feature set (pitch and probability-of-voicing) to improve automatic speech recognition, without proposing or analyzing discrete audio/speech tokenization or quantization into finite vocabularies; the paper is more about feature extraction and engineering improvements rather than discrete tokenization methods for audio reconstruction or language-model-style token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an algorithm for pitch extraction as a continuous feature set (pitch and probability-of-voicing) to improve automatic speech recognition, without proposing or analyzing discrete audio/speech tokenization or quantization into finite vocabularies; the paper is more about feature extraction and engineering improvements rather than discrete tokenization methods for audio reconstruction or language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
    "abstract": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
    "metadata": {
      "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
      "summary": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "abstract": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "doi": "https://doi.org/10.48550/arxiv.2111.14822",
      "openalex_id": "https://openalex.org/W3217345456",
      "arxiv_id": "",
      "publication_date": "2021-11-29",
      "published": "2021-11-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work concerns discrete latent tokens in a VQ-VAE for image generation, not discrete audio/speech tokenization as a core object, so it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work concerns discrete latent tokens in a VQ-VAE for image generation, not discrete audio/speech tokenization as a core object, so it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a vector quantized diffusion model for text-to-image synthesis and does not discuss discrete audio/speech tokenization or any audio-related tokenization approach, which is the core inclusion criterion; therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a vector quantized diffusion model for text-to-image synthesis and does not discuss discrete audio/speech tokenization or any audio-related tokenization approach, which is the core inclusion criterion; therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
    "metadata": {
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "doi": "https://doi.org/10.48550/arxiv.2005.08100",
      "openalex_id": "https://openalex.org/W3025165719",
      "arxiv_id": "",
      "publication_date": "2020-05-16",
      "published": "2020-05-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: Although the paper targets ASR using Conformer architecture, it does not present or analyze discrete audio tokenization (converting audio into a discrete symbol sequence) as a core object, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Reasoning: Although the paper targets ASR using Conformer architecture, it does not present or analyze discrete audio tokenization (converting audio into a discrete symbol sequence) as a core object, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on combining convolutional and transformer models for speech recognition and reports on performance improvements but does not mention any discrete audio/speech tokenization process or analysis as a core part of the work; it mainly deals with continuous feature-based modeling and does not explicitly address discrete tokenization or quantization methods relevant to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on combining convolutional and transformer models for speech recognition and reports on performance improvements but does not mention any discrete audio/speech tokenization process or analysis as a core part of the work; it mainly deals with continuous feature-based modeling and does not explicitly address discrete tokenization or quantization methods relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
    "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
    "metadata": {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "summary": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "doi": "https://doi.org/10.1109/taslp.2021.3122291",
      "openalex_id": "https://openalex.org/W3169320628",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "HuBERT relies on discrete target units obtained by offline clustering and trains to predict masked discrete tokens, making discrete audio tokenization central to the study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "HuBERT relies on discrete target units obtained by offline clustering and trains to predict masked discrete tokens, making discrete audio tokenization central to the study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes HuBERT, which utilizes an offline clustering step to generate discrete target labels (hidden units) for self-supervised speech representation learning, effectively converting continuous audio into discrete sequences; this fits the inclusion criterion of discrete audio tokenization as a core study object, with methodological contributions and experimental validation on speech tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes HuBERT, which utilizes an offline clustering step to generate discrete target labels (hidden units) for self-supervised speech representation learning, effectively converting continuous audio into discrete sequences; this fits the inclusion criterion of discrete audio tokenization as a core study object, with methodological contributions and experimental validation on speech tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
    "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
    "metadata": {
      "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
      "summary": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "doi": "https://doi.org/10.48550/arxiv.2005.11129",
      "openalex_id": "https://openalex.org/W3026874504",
      "arxiv_id": "",
      "publication_date": "2020-05-22",
      "published": "2020-05-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a flow-based TTS model using mel-spectrograms and a learned latent representation with monotonic alignment; it does not introduce or analyze discrete audio tokenization or quantized token representations, so it fails to meet the inclusion criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a flow-based TTS model using mel-spectrograms and a learned latent representation with monotonic alignment; it does not introduce or analyze discrete audio tokenization or quantized token representations, so it fails to meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Glow-TTS, a flow-based generative model for text-to-speech that directly generates continuous mel-spectrograms without involving discrete audio tokenization or quantization; hence, it does not analyze or propose discrete audio tokenization as required by the inclusion criteria and instead uses continuous audio features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes Glow-TTS, a flow-based generative model for text-to-speech that directly generates continuous mel-spectrograms without involving discrete audio tokenization or quantization; hence, it does not analyze or propose discrete audio tokenization as required by the inclusion criteria and instead uses continuous audio features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
    "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
    "metadata": {
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "summary": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "doi": "https://doi.org/10.48550/arxiv.2010.05646",
      "openalex_id": "https://openalex.org/W3092028330",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: It focuses on generating continuous waveforms from mel-spectrograms using a GAN-based vocoder and does not analyze or quantify discrete audio tokenization, thus not meeting the inclusion criteria and meeting an exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Reasoning: It focuses on generating continuous waveforms from mel-spectrograms using a GAN-based vocoder and does not analyze or quantify discrete audio tokenization, thus not meeting the inclusion criteria and meeting an exclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using generative adversarial networks for direct waveform generation from mel-spectrograms without mention of discrete audio tokenization or converting continuous audio to sequences of discrete tokens, thus it does not meet the core inclusion criterion of analyzing or proposing discrete audio tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using generative adversarial networks for direct waveform generation from mel-spectrograms without mention of discrete audio tokenization or converting continuous audio to sequences of discrete tokens, thus it does not meet the core inclusion criterion of analyzing or proposing discrete audio tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
    "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
    "metadata": {
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "summary": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-475",
      "openalex_id": "https://openalex.org/W3140429000",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly uses discrete self-supervised representations for speech resynthesis and evaluates discrete token-like representations (content, prosody, speaker), aligning with the inclusion criterion and not matching the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly uses discrete self-supervised representations for speech resynthesis and evaluates discrete token-like representations (content, prosody, speaker), aligning with the inclusion criterion and not matching the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes discrete self-supervised representations for speech resynthesis, focusing on converting continuous audio into discrete symbols and evaluating their impact on reconstruction and generation tasks, which satisfies the inclusion criteria without violating any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes discrete self-supervised representations for speech resynthesis, focusing on converting continuous audio into discrete symbols and evaluating their impact on reconstruction and generation tasks, which satisfies the inclusion criteria without violating any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
    "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
    "metadata": {
      "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
      "summary": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "doi": "https://doi.org/10.48550/arxiv.2105.06337",
      "openalex_id": "https://openalex.org/W3162673269",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This Grad-TTS paper uses continuous mel-spectrograms with diffusion-based generation and does not analyze or propose discrete audio tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This Grad-TTS paper uses continuous mel-spectrograms with diffusion-based generation and does not analyze or propose discrete audio tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Grad-TTS, focusing on generating mel-spectrograms (continuous audio features) from text input using diffusion probabilistic models; it does not address or analyze discrete audio/speech tokenization where audio signals are converted into discrete tokens as the core object of study, thus not meeting the inclusion criteria and falling under exclusion criteria related to relying only on continuous audio features without discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents Grad-TTS, focusing on generating mel-spectrograms (continuous audio features) from text input using diffusion probabilistic models; it does not address or analyze discrete audio/speech tokenization where audio signals are converted into discrete tokens as the core object of study, thus not meeting the inclusion criteria and falling under exclusion criteria related to relying only on continuous audio features without discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition",
    "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
    "metadata": {
      "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition",
      "summary": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461375",
      "openalex_id": "https://openalex.org/W2890964092",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses continuous embedding methods (x-vectors) for speaker recognition and compares them to i-vectors, with no discrete audio tokenization or quantization analyzed, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses continuous embedding methods (x-vectors) for speaker recognition and compares them to i-vectors, with no discrete audio tokenization or quantization analyzed, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using deep neural network embeddings (x-vectors) for speaker recognition, which involves continuous representation learning for speaker identity, but it does not propose or analyze discrete audio/speech tokenization converting continuous audio signals into sequences of discrete symbols, nor does it discuss discrete token design or quantization relevant to the given inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using deep neural network embeddings (x-vectors) for speaker recognition, which involves continuous representation learning for speaker identity, but it does not propose or analyze discrete audio/speech tokenization converting continuous audio signals into sequences of discrete symbols, nor does it discuss discrete token design or quantization relevant to the given inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
    "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
    "metadata": {
      "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
      "summary": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02584",
      "openalex_id": "https://openalex.org/W3204550533",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes editing of speech using diffusion-based methods on continuous audio representations, with no focus on discrete audio tokenization or quantization into a finite vocabulary, thus it fails the inclusion criteria and aligns with the exclusion criteria for continuous features.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes editing of speech using diffusion-based methods on continuous audio representations, with no focus on discrete audio tokenization or quantization into a finite vocabulary, thus it fails the inclusion criteria and aligns with the exclusion criteria for continuous features.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech editing method using score-based generative modeling for text-to-speech synthesis and does not indicate any proposal, analysis, or evaluation of discrete audio/speech tokenization where continuous audio is converted into discrete symbols; instead, it appears to operate on continuous audio features without discrete token quantization relevant to token-based audio modeling as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech editing method using score-based generative modeling for text-to-speech synthesis and does not indicate any proposal, analysis, or evaluation of discrete audio/speech tokenization where continuous audio is converted into discrete symbols; instead, it appears to operate on continuous audio features without discrete token quantization relevant to token-based audio modeling as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
    "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
    "metadata": {
      "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
      "summary": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "doi": "https://doi.org/10.21437/interspeech.2022-245",
      "openalex_id": "https://openalex.org/W4283722828",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions using tokens to represent global/local speech factors, but it does not clearly describe discrete audio tokenization into a finite vocabulary for audio reconstruction/generation; the tokens are likely learned continuous representations rather than explicit discrete quantized tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions using tokens to represent global/local speech factors, but it does not clearly describe discrete audio tokenization into a finite vocabulary for audio reconstruction/generation; the tokens are likely learned continuous representations rather than explicit discrete quantized tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a method for speech insertion involving manipulation of global and local speech factors and does not explicitly discuss discrete audio/speech tokenization converting continuous audio into discrete tokens, which is the core inclusion criterion; it focuses on speech factors representation and prosody continuity without clear analysis or evaluation of discrete tokenization, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a method for speech insertion involving manipulation of global and local speech factors and does not explicitly discuss discrete audio/speech tokenization converting continuous audio into discrete tokens, which is the core inclusion criterion; it focuses on speech factors representation and prosody continuity without clear analysis or evaluation of discrete tokenization, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
    "abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
    "metadata": {
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "summary": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1910.05453",
      "openalex_id": "https://openalex.org/W2979476256",
      "arxiv_id": "",
      "publication_date": "2019-10-12",
      "published": "2019-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly learns discrete speech representations by quantizing dense audio features via Gumbel-Softmax or online k-means and demonstrates improvements on speech tasks (TIMIT/WSJ), which directly aligns with the discrete tokenization criterion and does not fall into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly learns discrete speech representations by quantizing dense audio features via Gumbel-Softmax or online k-means and demonstrates improvements on speech tasks (TIMIT/WSJ), which directly aligns with the discrete tokenization criterion and does not fall into any exclusion category.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete representations of speech audio through self-supervised learning and quantization methods, directly addressing discrete audio tokenization for speech tasks as required by the inclusion criteria, with no indication of falling into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete representations of speech audio through self-supervised learning and quantization methods, directly addressing discrete audio tokenization for speech tasks as required by the inclusion criteria, with no indication of falling into any exclusion category.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ESPnet: End-to-End Speech Processing Toolkit",
    "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
    "metadata": {
      "title": "ESPnet: End-to-End Speech Processing Toolkit",
      "summary": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1456",
      "openalex_id": "https://openalex.org/W2962780374",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes ESPnet as a toolkit for end-to-end speech processing and ASR but does not explicitly analyze or evaluate discrete audio tokenization as a core object; it focuses on software platform, architecture, and benchmarks rather than token-level discretization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes ESPnet as a toolkit for end-to-end speech processing and ASR but does not explicitly analyze or evaluate discrete audio tokenization as a core object; it focuses on software platform, architecture, and benchmarks rather than token-level discretization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes the ESPnet toolkit, focusing on end-to-end speech recognition with neural networks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization mechanisms converting continuous signals into discrete tokens, instead focusing on an open-source engineering platform without emphasizing discrete token methodological contributions.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper describes the ESPnet toolkit, focusing on end-to-end speech recognition with neural networks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization mechanisms converting continuous signals into discrete tokens, instead focusing on an open-source engineering platform without emphasizing discrete token methodological contributions.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
    "abstract": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
    "metadata": {
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "abstract": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095100",
      "openalex_id": "https://openalex.org/W4375869198",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a TTS model that uses a latent continuous representation with diffusion and mentions a phoneme-rate latent representation, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (a finite vocabulary of discrete tokens) as a core object of study, so it does not meet the discrete tokenization inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a TTS model that uses a latent continuous representation with diffusion and mentions a phoneme-rate latent representation, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (a finite vocabulary of discrete tokens) as a core object of study, so it does not meet the discrete tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses encoding speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, which implies a form of discrete or structured tokenization of audio for TTS. However, it does not explicitly clarify that the latent representation is a discrete token sequence from a finite vocabulary, as required by the inclusion criteria, nor does it clearly focus on analyzing or evaluating discrete audio tokenization as the core topic. The focus is more on a diffusion model for TTS and speech editing rather than discrete token design or systematic analysis of tokenization.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study discusses encoding speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, which implies a form of discrete or structured tokenization of audio for TTS. However, it does not explicitly clarify that the latent representation is a discrete token sequence from a finite vocabulary, as required by the inclusion criteria, nor does it clearly focus on analyzing or evaluating discrete audio tokenization as the core topic. The focus is more on a diffusion model for TTS and speech editing rather than discrete token design or systematic analysis of tokenization.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged that the abstract describes a phoneme-rate latent representation and latent diffusion TTS but does not explicitly propose or evaluate a discrete-token (finite-vocabulary) audio tokenization—there is no mention of quantization, codebooks, discrete token sequences, or analysis of discrete token design. The work appears to use continuous latent representations from a VAE (even with adversarial training) and diffusion modeling for generation/editing/zero-shot adaptation, which matches the exclusion criterion of studies that only use continuous audio features without discrete quantization. Given the required inclusion criterion (explicit discrete audio/speech tokenization as a core object of study) is not met, this item should be excluded. I rate this a 2 (better to exclude) because the abstract lacks explicit discrete-tokenization proposal, analysis, or evaluation.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged that the abstract describes a phoneme-rate latent representation and latent diffusion TTS but does not explicitly propose or evaluate a discrete-token (finite-vocabulary) audio tokenization—there is no mention of quantization, codebooks, discrete token sequences, or analysis of discrete token design. The work appears to use continuous latent representations from a VAE (even with adversarial training) and diffusion modeling for generation/editing/zero-shot adaptation, which matches the exclusion criterion of studies that only use continuous audio features without discrete quantization. Given the required inclusion criterion (explicit discrete audio/speech tokenization as a core object of study) is not met, this item should be excluded. I rate this a 2 (better to exclude) because the abstract lacks explicit discrete-tokenization proposal, analysis, or evaluation.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
    "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
    "metadata": {
      "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
      "summary": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "doi": "https://doi.org/10.48550/arxiv.2304.09116",
      "openalex_id": "https://openalex.org/W4366460484",
      "arxiv_id": "",
      "publication_date": "2023-04-18",
      "published": "2023-04-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on discrete audio tokenization via residual vector quantizers for quantized latent speech representations and models them with diffusion, making discrete tokenization the core object of study and meeting the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper centers on discrete audio tokenization via residual vector quantizers for quantized latent speech representations and models them with diffusion, making discrete tokenization the core object of study and meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents NaturalSpeech 2 which uses a neural audio codec with residual vector quantizers to obtain quantized latent vectors from speech, indicating the study involves discrete audio tokenization for speech synthesis tasks. It also explicitly discusses quantization and discrete latent representation as central components of the methodology, meeting inclusion criteria while not falling into any exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents NaturalSpeech 2 which uses a neural audio codec with residual vector quantizers to obtain quantized latent vectors from speech, indicating the study involves discrete audio tokenization for speech synthesis tasks. It also explicitly discusses quantization and discrete latent representation as central components of the methodology, meeting inclusion criteria while not falling into any exclusion categories.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adam: A Method for Stochastic Optimization",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "metadata": {
      "title": "Adam: A Method for Stochastic Optimization",
      "summary": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "doi": "https://doi.org/10.48550/arxiv.1412.6980",
      "openalex_id": "https://openalex.org/W1522301498",
      "arxiv_id": "",
      "publication_date": "2014-12-22",
      "published": "2014-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses the Adam optimization algorithm for stochastic gradient methods and does not address discrete audio/speech tokenization into a finite vocabulary, so it does not meet the inclusion criteria nor the exclusion criteria related to tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses the Adam optimization algorithm for stochastic gradient methods and does not address discrete audio/speech tokenization into a finite vocabulary, so it does not meet the inclusion criteria nor the exclusion criteria related to tokenization studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Adam, a general-purpose stochastic optimization algorithm without any mention or focus on discrete audio/speech tokenization or analysis relevant to the inclusion criteria; it neither proposes nor evaluates discrete audio tokenization methods and does not address continuous audio quantization or related tasks, thus fails the inclusion requirements and meets exclusion criteria implicitly.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents Adam, a general-purpose stochastic optimization algorithm without any mention or focus on discrete audio/speech tokenization or analysis relevant to the inclusion criteria; it neither proposes nor evaluates discrete audio tokenization methods and does not address continuous audio quantization or related tasks, thus fails the inclusion requirements and meets exclusion criteria implicitly.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
    "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
    "metadata": {
      "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
      "summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "doi": "https://doi.org/10.48550/arxiv.2005.05957",
      "openalex_id": "https://openalex.org/W3025528898",
      "arxiv_id": "",
      "publication_date": "2020-05-12",
      "published": "2020-05-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a flow-based TTS model using continuous mel-spectrogram representations and does not address discrete audio tokenization or token-based speech representations, thus failing the inclusion criteria and meeting an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a flow-based TTS model using continuous mel-spectrogram representations and does not address discrete audio tokenization or token-based speech representations, thus failing the inclusion criteria and meeting an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Flowtron, a generative model for text-to-speech synthesis focusing on continuous feature synthesis like mel-spectrograms, without explicit mention or analysis of discrete audio/speech tokenization or discrete symbol sequences as a core component. It mainly uses continuous audio features for synthesis and does not discuss discrete token quantization or token design, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes Flowtron, a generative model for text-to-speech synthesis focusing on continuous feature synthesis like mel-spectrograms, without explicit mention or analysis of discrete audio/speech tokenization or discrete symbol sequences as a core component. It mainly uses continuous audio features for synthesis and does not discuss discrete token quantization or token design, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
    "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
    "metadata": {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "summary": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2006.11477",
      "openalex_id": "https://openalex.org/W3036601975",
      "arxiv_id": "",
      "publication_date": "2020-06-20",
      "published": "2020-06-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly employs a discrete quantization of latent speech representations to form a sequence of discrete tokens (tokenization) used in a contrastive objective, which aligns with the inclusion criteria and does not meet any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly employs a discrete quantization of latent speech representations to form a sequence of discrete tokens (tokenization) used in a contrastive objective, which aligns with the inclusion criteria and does not meet any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly discusses a contrastive task defined over quantized latent speech representations, indicating the use of discrete audio tokenization as a core method. The focus is on learning discrete representations from raw speech signals, which aligns with the inclusion criteria. The abstract is in English and appears to present a methodological contribution with experimental validation relevant to discrete tokenization in speech. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly discusses a contrastive task defined over quantized latent speech representations, indicating the use of discrete audio tokenization as a core method. The focus is on learning discrete representations from raw speech signals, which aligns with the inclusion criteria. The abstract is in English and appears to present a methodological contribution with experimental validation relevant to discrete tokenization in speech. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
    "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
    "metadata": {
      "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "summary": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688253",
      "openalex_id": "https://openalex.org/W4226033575",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses discrete speech tokens obtained from continuous speech and trains with masked language modeling on those discretized tokens, fulfilling the core discrete-token objective and evaluating against speech benchmarks.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses discrete speech tokens obtained from continuous speech and trains with masked language modeling on those discretized tokens, fulfilling the core discrete-token objective and evaluating against speech benchmarks.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete speech tokenization within the w2v-BERT framework, converting continuous speech into discriminative discrete tokens, fulfilling the inclusion criteria by analyzing discrete audio tokenization methods for self-supervised speech pre-training with demonstrated experimental validation on speech benchmarks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete speech tokenization within the w2v-BERT framework, converting continuous speech into discriminative discrete tokens, fulfilling the inclusion criteria by analyzing discrete audio tokenization methods for self-supervised speech pre-training with demonstrated experimental validation on speech benchmarks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech",
    "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
    "metadata": {
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech",
      "summary": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "doi": "https://doi.org/10.57702/v7a23pbp",
      "openalex_id": "https://openalex.org/W3033411150",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This FastSpeech 2 paper focuses on non-autoregressive TTS and waveform generation using continuous features; it does not analyze or propose discrete audio tokens or a discrete tokenization scheme for speech.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This FastSpeech 2 paper focuses on non-autoregressive TTS and waveform generation using continuous features; it does not analyze or propose discrete audio tokens or a discrete tokenization scheme for speech.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on FastSpeech 2, an end-to-end text-to-speech model that improves training speed and voice quality by using continuous speech features (pitch, energy, duration) as conditional inputs and does not mention any discrete audio/speech tokenization or conversion of continuous audio signals into sequences of discrete symbols as a core part of the study. The work emphasizes modeling and generating continuous acoustic features (mel-spectrograms, waveform) rather than analyzing or proposing discrete tokenization strategies, thus not meeting the inclusion criteria and falling under the exclusion criteria concerning continuous feature use without discrete tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on FastSpeech 2, an end-to-end text-to-speech model that improves training speed and voice quality by using continuous speech features (pitch, energy, duration) as conditional inputs and does not mention any discrete audio/speech tokenization or conversion of continuous audio signals into sequences of discrete symbols as a core part of the study. The work emphasizes modeling and generating continuous acoustic features (mel-spectrograms, waveform) rather than analyzing or proposing discrete tokenization strategies, thus not meeting the inclusion criteria and falling under the exclusion criteria concerning continuous feature use without discrete tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "AudioLM: A Language Modeling Approach to Audio Generation",
    "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
    "metadata": {
      "title": "AudioLM: A Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "doi": "https://doi.org/10.1109/taslp.2023.3288409",
      "openalex_id": "https://openalex.org/W4381786045",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly proposes a discrete tokenization approach for audio (mapping audio to a sequence of discrete tokens) and uses language-model style generation, with evaluation on speech and music, meeting the core inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly proposes a discrete tokenization approach for audio (mapping audio to a sequence of discrete tokens) and uses language-model style generation, with evaluation on speech and music, meeting the core inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on a discrete audio tokenization approach by mapping input audio into sequences of discrete tokens and analyzing the trade-offs between tokenizers for reconstruction quality and long-term structure. It involves neural encoding and tokenization in the context of audio generation, including speech, and demonstrates methodological contributions beyond engineering reports. The work also evaluates discrete tokenization for speech tasks and includes experimental validation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on a discrete audio tokenization approach by mapping input audio into sequences of discrete tokens and analyzing the trade-offs between tokenizers for reconstruction quality and long-term structure. It involves neural encoding and tokenization in the context of audio generation, including speech, and demonstrates methodological contributions beyond engineering reports. The work also evaluates discrete tokenization for speech tasks and includes experimental validation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
    "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
    "metadata": {
      "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
      "summary": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461368",
      "openalex_id": "https://openalex.org/W2964243274",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study uses continuous mel-spectrogram representations and a WaveNet vocoder, with no analysis or use of discrete audio tokens, so it does not meet the discrete-tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study uses continuous mel-spectrogram representations and a WaveNet vocoder, with no analysis or use of discrete audio tokens, so it does not meet the discrete-tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a neural TTS system synthesizing speech from text by predicting mel-spectrograms followed by WaveNet vocoding, relying entirely on continuous acoustic features (mel-spectrograms) without proposing or analyzing any discrete audio tokenization approach or discrete symbol sequences for audio representation, and thus does not meet the inclusion criteria focused on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a neural TTS system synthesizing speech from text by predicting mel-spectrograms followed by WaveNet vocoding, relying entirely on continuous acoustic features (mel-spectrograms) without proposing or analyzing any discrete audio tokenization approach or discrete symbol sequences for audio representation, and thus does not meet the inclusion criteria focused on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
    "metadata": {
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
      "summary": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2441",
      "openalex_id": "https://openalex.org/W2972359262",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper presents LibriTTS as a dataset for text-to-speech and does not propose, analyze, or evaluate discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria for discrete tokenization and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This paper presents LibriTTS as a dataset for text-to-speech and does not propose, analyze, or evaluate discrete audio tokenization into a finite vocabulary, so it does not meet the inclusion criteria for discrete tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a speech corpus for text-to-speech and demonstrates evaluations of TTS models, but it does not explicitly propose or analyze discrete audio/speech tokenization methods, nor does it focus on converting continuous audio signals into discrete tokens; thus, it does not meet the core inclusion criteria related to discrete audio tokenization as the main study object.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a speech corpus for text-to-speech and demonstrates evaluations of TTS models, but it does not explicitly propose or analyze discrete audio/speech tokenization methods, nor does it focus on converting continuous audio signals into discrete tokens; thus, it does not meet the core inclusion criteria related to discrete audio tokenization as the main study object.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "An audiovisual and contextual approach for categorical and continuous\\n emotion recognition in-the-wild",
    "abstract": "In this work we tackle the task of video-based audio-visual emotion\\nrecognition, within the premises of the 2nd Workshop and Competition on\\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\\nhead/body orientation and low image resolution constitute factors that can\\npotentially hinder performance in case of methodologies that solely rely on the\\nextraction and analysis of facial features. In order to alleviate this problem,\\nwe leverage both bodily and contextual features, as part of a broader emotion\\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\\nApart from learning through the RGB input modality, we construct an aural\\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\\nof the human body and scene context, as aspects of the emotion recognition\\nprocess that have been left relatively unexplored up to this point. All the\\ncode was implemented using PyTorch and is publicly available.\\n",
    "metadata": {
      "title": "An audiovisual and contextual approach for categorical and continuous\\n emotion recognition in-the-wild",
      "summary": "In this work we tackle the task of video-based audio-visual emotion\\nrecognition, within the premises of the 2nd Workshop and Competition on\\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\\nhead/body orientation and low image resolution constitute factors that can\\npotentially hinder performance in case of methodologies that solely rely on the\\nextraction and analysis of facial features. In order to alleviate this problem,\\nwe leverage both bodily and contextual features, as part of a broader emotion\\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\\nApart from learning through the RGB input modality, we construct an aural\\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\\nof the human body and scene context, as aspects of the emotion recognition\\nprocess that have been left relatively unexplored up to this point. All the\\ncode was implemented using PyTorch and is publicly available.\\n",
      "abstract": "In this work we tackle the task of video-based audio-visual emotion\\nrecognition, within the premises of the 2nd Workshop and Competition on\\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\\nhead/body orientation and low image resolution constitute factors that can\\npotentially hinder performance in case of methodologies that solely rely on the\\nextraction and analysis of facial features. In order to alleviate this problem,\\nwe leverage both bodily and contextual features, as part of a broader emotion\\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\\nApart from learning through the RGB input modality, we construct an aural\\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\\nof the human body and scene context, as aspects of the emotion recognition\\nprocess that have been left relatively unexplored up to this point. All the\\ncode was implemented using PyTorch and is publicly available.\\n",
      "doi": "https://doi.org/10.1109/iccvw54120.2021.00407",
      "openalex_id": "https://openalex.org/W3209058072",
      "arxiv_id": "",
      "publication_date": "2021-07-07",
      "published": "2021-07-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not involve discrete audio tokenization; uses continuous mel-spectrogram features in a multi-modal emotion recognition setup, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not involve discrete audio tokenization; uses continuous mel-spectrogram features in a multi-modal emotion recognition setup, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audiovisual emotion recognition using continuous audio features such as mel-spectrograms and leverages CNN-RNN architectures; it does not explicitly discuss discrete audio/speech tokenization, discrete quantization, or token design as a core research component. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor does it propose any novel discrete token representations for audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audiovisual emotion recognition using continuous audio features such as mel-spectrograms and leverages CNN-RNN architectures; it does not explicitly discuss discrete audio/speech tokenization, discrete quantization, or token design as a core research component. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization nor does it propose any novel discrete token representations for audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Influence of lips on the production of vowels based on finite element simulations and experiments",
    "abstract": "Three-dimensional (3-D) numerical approaches for voice production are currently being investigated and developed. Radiation losses produced when sound waves emanate from the mouth aperture are one of the key aspects to be modeled. When doing so, the lips are usually removed from the vocal tract geometry in order to impose a radiation impedance on a closed cross-section, which speeds up the numerical simulations compared to free-field radiation solutions. However, lips may play a significant role. In this work, the lips' effects on vowel sounds are investigated by using 3-D vocal tract geometries generated from magnetic resonance imaging. To this aim, two configurations for the vocal tract exit are considered: with lips and without lips. The acoustic behavior of each is analyzed and compared by means of time-domain finite element simulations that allow free-field wave propagation and experiments performed using 3-D-printed mechanical replicas. The results show that the lips should be included in order to correctly model vocal tract acoustics not only at high frequencies, as commonly accepted, but also in the low frequency range below 4 kHz, where plane wave propagation occurs.",
    "metadata": {
      "title": "Influence of lips on the production of vowels based on finite element simulations and experiments",
      "summary": "Three-dimensional (3-D) numerical approaches for voice production are currently being investigated and developed. Radiation losses produced when sound waves emanate from the mouth aperture are one of the key aspects to be modeled. When doing so, the lips are usually removed from the vocal tract geometry in order to impose a radiation impedance on a closed cross-section, which speeds up the numerical simulations compared to free-field radiation solutions. However, lips may play a significant role. In this work, the lips' effects on vowel sounds are investigated by using 3-D vocal tract geometries generated from magnetic resonance imaging. To this aim, two configurations for the vocal tract exit are considered: with lips and without lips. The acoustic behavior of each is analyzed and compared by means of time-domain finite element simulations that allow free-field wave propagation and experiments performed using 3-D-printed mechanical replicas. The results show that the lips should be included in order to correctly model vocal tract acoustics not only at high frequencies, as commonly accepted, but also in the low frequency range below 4 kHz, where plane wave propagation occurs.",
      "abstract": "Three-dimensional (3-D) numerical approaches for voice production are currently being investigated and developed. Radiation losses produced when sound waves emanate from the mouth aperture are one of the key aspects to be modeled. When doing so, the lips are usually removed from the vocal tract geometry in order to impose a radiation impedance on a closed cross-section, which speeds up the numerical simulations compared to free-field radiation solutions. However, lips may play a significant role. In this work, the lips' effects on vowel sounds are investigated by using 3-D vocal tract geometries generated from magnetic resonance imaging. To this aim, two configurations for the vocal tract exit are considered: with lips and without lips. The acoustic behavior of each is analyzed and compared by means of time-domain finite element simulations that allow free-field wave propagation and experiments performed using 3-D-printed mechanical replicas. The results show that the lips should be included in order to correctly model vocal tract acoustics not only at high frequencies, as commonly accepted, but also in the low frequency range below 4 kHz, where plane wave propagation occurs.",
      "doi": "https://doi.org/10.1121/1.4950698",
      "openalex_id": "https://openalex.org/W2404836908",
      "arxiv_id": "",
      "publication_date": "2016-05-01",
      "published": "2016-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper studies lip influence on vowel production using simulations and experiments, but does not involve discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper studies lip influence on vowel production using simulations and experiments, but does not involve discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the modeling of vowel production using finite element simulations to assess the role of lips in vocal tract acoustics; it does not discuss any discrete audio/speech tokenization process or the conversion of continuous audio signals into discrete tokens, which is a mandatory inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the modeling of vowel production using finite element simulations to assess the role of lips in vocal tract acoustics; it does not discuss any discrete audio/speech tokenization process or the conversion of continuous audio signals into discrete tokens, which is a mandatory inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
    "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
    "metadata": {
      "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "summary": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "doi": "https://doi.org/10.48550/arxiv.2202.03555",
      "openalex_id": "https://openalex.org/W4221145109",
      "arxiv_id": "",
      "publication_date": "2022-02-07",
      "published": "2022-02-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes a general self-supervised framework across modalities using latent representations and does not introduce or analyze discrete speech tokenization (no sequence of discrete audio symbols for speech reconstruction or understanding).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work describes a general self-supervised framework across modalities using latent representations and does not introduce or analyze discrete speech tokenization (no sequence of discrete audio symbols for speech reconstruction or understanding).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces data2vec, a general self-supervised learning framework applied to speech, vision, and NLP, focusing on predicting latent representations rather than explicitly proposing or analyzing discrete audio tokenization; it does not discuss converting continuous audio signals into discrete tokens as a core focus and primarily uses contextually predicted latent representations without discrete quantization/token design for audio.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces data2vec, a general self-supervised learning framework applied to speech, vision, and NLP, focusing on predicting latent representations rather than explicitly proposing or analyzing discrete audio tokenization; it does not discuss converting continuous audio signals into discrete tokens as a core focus and primarily uses contextually predicted latent representations without discrete quantization/token design for audio.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "How Far are We from Solving the 2D &amp; 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)",
    "abstract": "This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from https://www.adrianbulat.com/face-alignment/\\n",
    "metadata": {
      "title": "How Far are We from Solving the 2D &amp; 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)",
      "summary": "This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from https://www.adrianbulat.com/face-alignment/\\n",
      "abstract": "This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from https://www.adrianbulat.com/face-alignment/\\n",
      "doi": "https://doi.org/10.1109/iccv.2017.116",
      "openalex_id": "https://openalex.org/W2949662773",
      "arxiv_id": "",
      "publication_date": "2017-10-01",
      "published": "2017-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study targets 2D/3D face alignment and datasets, with no discrete audio tokenization; therefore it should be excluded under the audio/tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study targets 2D/3D face alignment and datasets, with no discrete audio tokenization; therefore it should be excluded under the audio/tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 2D and 3D face alignment using deep neural networks and facial landmark datasets; it does not discuss any form of discrete audio or speech tokenization, nor does it involve converting continuous audio signals into sequences of discrete tokens. Therefore, it does not meet any inclusion criteria related to audio tokenization and is irrelevant to the area of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 2D and 3D face alignment using deep neural networks and facial landmark datasets; it does not discuss any form of discrete audio or speech tokenization, nor does it involve converting continuous audio signals into sequences of discrete tokens. Therefore, it does not meet any inclusion criteria related to audio tokenization and is irrelevant to the area of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Simple Framework for Contrastive Learning of Visual Representations",
    "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
    "metadata": {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "summary": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "doi": "https://doi.org/10.48550/arxiv.2002.05709",
      "openalex_id": "https://openalex.org/W3005680577",
      "arxiv_id": "",
      "publication_date": "2020-02-13",
      "published": "2020-02-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on visual contrastive learning (SimCLR) with image representations and has no discrete audio/speech tokenization component, thus it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on visual contrastive learning (SimCLR) with image representations and has no discrete audio/speech tokenization component, thus it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents SimCLR, a framework for contrastive learning of visual representations, focusing on image-based self-supervised learning with no mention of audio or discrete audio tokenization; it does not address the inclusion criteria regarding discrete audio/speech tokenization, nor does it analyze discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents SimCLR, a framework for contrastive learning of visual representations, focusing on image-based self-supervised learning with no mention of audio or discrete audio tokenization; it does not address the inclusion criteria regarding discrete audio/speech tokenization, nor does it analyze discrete audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
    "abstract": "In this paper, we consider the problem of multi-modal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.",
    "metadata": {
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "summary": "In this paper, we consider the problem of multi-modal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.",
      "abstract": "In this paper, we consider the problem of multi-modal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.",
      "doi": "https://doi.org/10.1109/icpr56361.2022.9956592",
      "openalex_id": "https://openalex.org/W4312292725",
      "arxiv_id": "",
      "publication_date": "2022-08-21",
      "published": "2022-08-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on multimodal (audio-visual) emotion recognition with robustness to missing data and modality dropout, and does not discuss discrete audio tokenization or conversion of audio into a finite vocabulary; thus it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on multimodal (audio-visual) emotion recognition with robustness to missing data and modality dropout, and does not discuss discrete audio tokenization or conversion of audio into a finite vocabulary; thus it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audiovisual emotion recognition with modality fusion techniques and robustness to missing or noisy modalities. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object; rather, it works on raw data fusion and modality dropout without a discrete tokenization framework. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization as the main focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audiovisual emotion recognition with modality fusion techniques and robustness to missing or noisy modalities. However, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object; rather, it works on raw data fusion and modality dropout without a discrete tokenization framework. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization as the main focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis",
    "abstract": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.",
    "metadata": {
      "title": "Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis",
      "summary": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.",
      "abstract": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.",
      "doi": "https://doi.org/10.1162/neco.2008.04-08-771",
      "openalex_id": "https://openalex.org/W2039844283",
      "arxiv_id": "",
      "publication_date": "2008-09-11",
      "published": "2008-09-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on NMF with IS divergence applied to music spectrograms (continuous audio features) and denoising/upmix, with no analysis or development of discrete audio tokenization or tokens; thus it does not meet the discrete-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on NMF with IS divergence applied to music spectrograms (continuous audio features) and denoising/upmix, with no analysis or development of discrete audio tokenization or tokens; thus it does not meet the discrete-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on nonnegative matrix factorization techniques applied to music analysis, including denoising and upmixing audio signals, but it does not propose or analyze discrete audio tokenization or discrete quantization methods converting continuous signals into discrete symbol sequences; thus, it does not meet the inclusion criteria and aligns with exclusion criteria regarding the absence of discrete token design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on nonnegative matrix factorization techniques applied to music analysis, including denoising and upmixing audio signals, but it does not propose or analyze discrete audio tokenization or discrete quantization methods converting continuous signals into discrete symbol sequences; thus, it does not meet the inclusion criteria and aligns with exclusion criteria regarding the absence of discrete token design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Co-Separating Sounds of Visual Objects",
    "abstract": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
    "metadata": {
      "title": "Co-Separating Sounds of Visual Objects",
      "summary": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
      "abstract": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
      "doi": "https://doi.org/10.1109/iccv.2019.00398",
      "openalex_id": "https://openalex.org/W2988200020",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract does not involve any discrete audio tokenization or token-based representations; it describes continuous-audio separation learning, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract does not involve any discrete audio tokenization or token-based representations; it describes continuous-audio separation learning, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on visually-guided audio source separation to disentangle sounds in videos, but it does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols; it primarily deals with audio source separation using continuous audio features without discussing discrete tokenization approaches relevant to language-model-style token sequences, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on visually-guided audio source separation to disentangle sounds in videos, but it does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols; it primarily deals with audio source separation using continuous audio features without discussing discrete tokenization approaches relevant to language-model-style token sequences, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multimodal Attention-Mechanism For Temporal Emotion Recognition",
    "abstract": "Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition.",
    "metadata": {
      "title": "Multimodal Attention-Mechanism For Temporal Emotion Recognition",
      "summary": "Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition.",
      "abstract": "Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition.",
      "doi": "https://doi.org/10.1109/icip40778.2020.9191019",
      "openalex_id": "https://openalex.org/W3090471441",
      "arxiv_id": "",
      "publication_date": "2020-09-30",
      "published": "2020-09-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study deals with multimodal attention for continuous audio-visual emotion recognition and does not involve discrete audio tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study deals with multimodal attention for continuous audio-visual emotion recognition and does not involve discrete audio tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal attention mechanisms for temporal emotion recognition using audio-visual data, but it does not discuss discrete audio/speech tokenization or the conversion of continuous audio into discrete symbols as a core object of study; it primarily leverages continuous audio features for emotion recognition without discrete token design or analysis relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal attention mechanisms for temporal emotion recognition using audio-visual data, but it does not discuss discrete audio/speech tokenization or the conversion of continuous audio into discrete symbols as a core object of study; it primarily leverages continuous audio features for emotion recognition without discrete token design or analysis relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
    "abstract": "In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-art performance on both datasets, and showing a significant impact of multimodal and temporal emotion perception.",
    "metadata": {
      "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
      "summary": "In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-art performance on both datasets, and showing a significant impact of multimodal and temporal emotion perception.",
      "abstract": "In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-art performance on both datasets, and showing a significant impact of multimodal and temporal emotion perception.",
      "doi": "https://doi.org/10.1109/acii.2019.8925444",
      "openalex_id": "https://openalex.org/W2977259558",
      "arxiv_id": "",
      "publication_date": "2019-09-01",
      "published": "2019-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study discusses multimodal audio-visual emotion recognition and temporal modeling but does not propose or analyze discrete audio tokenization; thus it does not meet the discrete-token inclusion requirement and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study discusses multimodal audio-visual emotion recognition and temporal modeling but does not propose or analyze discrete audio tokenization; thus it does not meet the discrete-token inclusion requirement and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal and temporal perception in audio-visual emotion recognition, primarily using continuous audio-visual features for emotion classification, and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods for audio reconstruction, generation, or understanding tasks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal and temporal perception in audio-visual emotion recognition, primarily using continuous audio-visual features for emotion classification, and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods for audio reconstruction, generation, or understanding tasks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised Representation Learning by Predicting Image Rotations",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
    "metadata": {
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "summary": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
      "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
      "doi": "https://doi.org/10.48550/arxiv.1803.07728",
      "openalex_id": "https://openalex.org/W2785325870",
      "arxiv_id": "",
      "publication_date": "2018-03-21",
      "published": "2018-03-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work concerns unsupervised visual representation learning via predicting image rotations, with no discrete audio/speech tokenization or tokenization analysis, so it does not meet the audio-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work concerns unsupervised visual representation learning via predicting image rotations, with no discrete audio/speech tokenization or tokenization analysis, so it does not meet the audio-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised representation learning for images by predicting rotations, without any mention of audio or speech tokenization or converting continuous audio signals into discrete tokens. It does not meet any inclusion criteria related to discrete audio/speech tokenization, and it is unrelated to audio or speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised representation learning for images by predicting rotations, without any mention of audio or speech tokenization or converting continuous audio signals into discrete tokens. It does not meet any inclusion criteria related to discrete audio/speech tokenization, and it is unrelated to audio or speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Robust Audiovisual Emotion Recognition: Aligning Modalities, Capturing Temporal Information, and Handling Missing Features",
    "abstract": "Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.",
    "metadata": {
      "title": "Robust Audiovisual Emotion Recognition: Aligning Modalities, Capturing Temporal Information, and Handling Missing Features",
      "summary": "Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.",
      "abstract": "Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.",
      "doi": "https://doi.org/10.1109/taffc.2022.3216993",
      "openalex_id": "https://openalex.org/W4312976151",
      "arxiv_id": "",
      "publication_date": "2022-10-01",
      "published": "2022-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets continuous audiovisual features and multimodal fusion with transformer-based modeling, and there is no discrete audio tokenization or token-quantization analysis presented, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets continuous audiovisual features and multimodal fusion with transformer-based modeling, and there is no discrete audio tokenization or token-quantization analysis presented, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on audiovisual emotion recognition using multimodal fusion and transformer architectures to handle challenges like modality misalignment and missing features; however, it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, which is a core inclusion criterion. Instead, it likely uses continuous audio features without discrete token design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on audiovisual emotion recognition using multimodal fusion and transformer architectures to handle challenges like modality misalignment and missing features; however, it does not mention discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, which is a core inclusion criterion. Instead, it likely uses continuous audio features without discrete token design, so it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gradient Descent Ascent for Minimax Problems on Riemannian Manifolds",
    "abstract": "In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of O(κ<sup>2</sup>ϵ<sup>-2</sup>) for finding an ϵ-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where κ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of O(κ<sup>4</sup>ϵ<sup>-4</sup>) for finding an ϵ-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of ~O(κ<sup>4</sup>ϵ<sup>-3</sup>) in searching for an ϵ-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.",
    "metadata": {
      "title": "Gradient Descent Ascent for Minimax Problems on Riemannian Manifolds",
      "summary": "In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of O(κ<sup>2</sup>ϵ<sup>-2</sup>) for finding an ϵ-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where κ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of O(κ<sup>4</sup>ϵ<sup>-4</sup>) for finding an ϵ-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of ~O(κ<sup>4</sup>ϵ<sup>-3</sup>) in searching for an ϵ-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.",
      "abstract": "In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of O(κ<sup>2</sup>ϵ<sup>-2</sup>) for finding an ϵ-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where κ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of O(κ<sup>4</sup>ϵ<sup>-4</sup>) for finding an ϵ-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of ~O(κ<sup>4</sup>ϵ<sup>-3</sup>) in searching for an ϵ-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.",
      "doi": "https://doi.org/10.1109/tpami.2023.3234160",
      "openalex_id": "https://openalex.org/W4313591702",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns optimization algorithms on Riemannian manifolds with no discrete audio tokenization or speech-related token design, so it does not meet the audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns optimization algorithms on Riemannian manifolds with no discrete audio tokenization or speech-related token design, so it does not meet the audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Riemannian manifold optimization algorithms for minimax problems, particularly applied to robust distributional optimization and robust deep neural network training on Stiefel manifold; however, it does not discuss discrete audio or speech tokenization, nor does it concern converting continuous audio signals into discrete tokens, which are central to the inclusion criteria, and thus does not meet the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Riemannian manifold optimization algorithms for minimax problems, particularly applied to robust distributional optimization and robust deep neural network training on Stiefel manifold; however, it does not discuss discrete audio or speech tokenization, nor does it concern converting continuous audio signals into discrete tokens, which are central to the inclusion criteria, and thus does not meet the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video",
    "abstract": "This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.",
    "metadata": {
      "title": "SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video",
      "summary": "This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.",
      "abstract": "This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.",
      "doi": "https://doi.org/10.1109/fg57933.2023.10042638",
      "openalex_id": "https://openalex.org/W4321020767",
      "arxiv_id": "",
      "publication_date": "2023-01-05",
      "published": "2023-01-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on video-based apparent emotional reaction recognition with self-supervised learning and does not involve discretizing or analyzing discrete audio/speech tokens, thus failing the required audio-tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on video-based apparent emotional reaction recognition with self-supervised learning and does not involve discretizing or analyzing discrete audio/speech tokens, thus failing the required audio-tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on apparent emotional reaction recognition from video-only inputs using self-supervised learning, with no mention of audio tokenization, discrete audio token design, or analysis; thus, it does not meet the inclusion criteria related to discrete audio tokenization, and there is no indication of dealing with audio signals or token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on apparent emotional reaction recognition from video-only inputs using self-supervised learning, with no mention of audio tokenization, discrete audio token design, or analysis; thus, it does not meet the inclusion criteria related to discrete audio tokenization, and there is no indication of dealing with audio signals or token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
    "abstract": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "summary": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "abstract": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "doi": "https://doi.org/10.1371/journal.pone.0196391",
      "openalex_id": "https://openalex.org/W2803193013",
      "arxiv_id": "",
      "publication_date": "2018-05-16",
      "published": "2018-05-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a multimodal emotional speech/singing database, with no explicit discussion of discrete audio/tokenization methods or token-level representations, thus not meeting the inclusion criteria centered on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a multimodal emotional speech/singing database, with no explicit discussion of discrete audio/tokenization methods or token-level representations, thus not meeting the inclusion criteria centered on discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a multimodal database of emotional speech and song, focusing on emotional expression and validity ratings, without any mention of discrete audio/speech tokenization, discrete symbol sequences, or tokenization methods central to audio reconstruction or generation, thus not meeting the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes a multimodal database of emotional speech and song, focusing on emotional expression and validity ratings, without any mention of discrete audio/speech tokenization, discrete symbol sequences, or tokenization methods central to audio reconstruction or generation, thus not meeting the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deep Multimodal Learning: A Survey on Recent Advances and Trends",
    "abstract": "The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.",
    "metadata": {
      "title": "Deep Multimodal Learning: A Survey on Recent Advances and Trends",
      "summary": "The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.",
      "abstract": "The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.",
      "doi": "https://doi.org/10.1109/msp.2017.2738401",
      "openalex_id": "https://openalex.org/W2767290858",
      "arxiv_id": "",
      "publication_date": "2017-11-01",
      "published": "2017-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses broad deep multimodal learning topics and architectures, with no explicit focus on discrete audio tokenization, quantized token sequences, or analysis of discrete speech tokens; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses broad deep multimodal learning topics and architectures, with no explicit focus on discrete audio tokenization, quantized token sequences, or analysis of discrete speech tokens; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a survey of deep multimodal learning advances including fusion strategies, but do not explicitly mention discrete audio/speech tokenization or analysis thereof, which is a core inclusion criterion; furthermore, there is no evidence of discrete token methods or neural quantization-based representations specific to audio discussed.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a survey of deep multimodal learning advances including fusion strategies, but do not explicitly mention discrete audio/speech tokenization or analysis thereof, which is a core inclusion criterion; furthermore, there is no evidence of discrete token methods or neural quantization-based representations specific to audio discussed.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Asymmetric Loss For Multi-Label Classification",
    "abstract": "In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (\"ASL\"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",
    "metadata": {
      "title": "Asymmetric Loss For Multi-Label Classification",
      "summary": "In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (\"ASL\"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",
      "abstract": "In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (\"ASL\"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",
      "doi": "https://doi.org/10.1109/iccv48922.2021.00015",
      "openalex_id": "https://openalex.org/W4214673031",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a loss function for multi-label image classification with no discrete audio/speech tokenization or token-based discrete representation; it does not meet the audio tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a loss function for multi-label image classification with no discrete audio/speech tokenization or token-based discrete representation; it does not meet the audio tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-label classification using asymmetric loss and does not involve discrete audio/speech tokenization or the conversion of continuous audio to discrete symbols, which are central to the inclusion criteria. Therefore, it does not meet the key inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-label classification using asymmetric loss and does not involve discrete audio/speech tokenization or the conversion of continuous audio to discrete symbols, which are central to the inclusion criteria. Therefore, it does not meet the key inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
    "abstract": "High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and an inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal or dynamical (i.e., sequential). In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audiovisual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
    "metadata": {
      "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
      "summary": "High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and an inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal or dynamical (i.e., sequential). In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audiovisual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
      "abstract": "High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and an inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal or dynamical (i.e., sequential). In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audiovisual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106120",
      "openalex_id": "https://openalex.org/W4390742209",
      "arxiv_id": "",
      "publication_date": "2024-01-11",
      "published": "2024-01-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper employs a vector-quantized VAE (VQ-VAE) that yields discrete latent codes, the work's primary focus is on a multimodal and dynamical VAE architecture for audiovisual speech representation rather than a core analysis or evaluation of discrete audio tokenization for speech tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper employs a vector-quantized VAE (VQ-VAE) that yields discrete latent codes, the work's primary focus is on a multimodal and dynamical VAE architecture for audiovisual speech representation rather than a core analysis or evaluation of discrete audio tokenization for speech tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study involves a two-stage training process for audiovisual speech representation, where the first stage uses a vector quantized VAE (VQ-VAE) independently for each modality, which involves discrete quantization of continuous audio representations into discrete tokens; this meets the inclusion criterion of explicitly proposing and evaluating discrete audio/speech tokenization. The study further applies and evaluates this in speech-related tasks such as emotion recognition. Therefore, it aligns well with the inclusion criteria and does not fall into any exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study involves a two-stage training process for audiovisual speech representation, where the first stage uses a vector quantized VAE (VQ-VAE) independently for each modality, which involves discrete quantization of continuous audio representations into discrete tokens; this meets the inclusion criterion of explicitly proposing and evaluating discrete audio/speech tokenization. The study further applies and evaluates this in speech-related tasks such as emotion recognition. Therefore, it aligns well with the inclusion criteria and does not fall into any exclusion categories.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the presence of VQ-VAE: JuniorNano argued the work’s primary focus is a multimodal dynamical VAE rather than discrete tokenization, while JuniorMini emphasized that VQ-VAE implies discrete tokenization. The abstract, however, states the MDVAE is trained on the intermediate representation of the VQ-VAEs “before quantization,” and the experiments focus on audiovisual manipulation, denoising, and emotion recognition rather than proposing, analyzing, or evaluating discrete audio/speech tokenization as the core contribution. Therefore this study does not meet the inclusion criterion requiring discrete audio/speech tokenization to be an explicit core object of study; I recommend exclusion (leaning toward exclusion rather than absolute).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the presence of VQ-VAE: JuniorNano argued the work’s primary focus is a multimodal dynamical VAE rather than discrete tokenization, while JuniorMini emphasized that VQ-VAE implies discrete tokenization. The abstract, however, states the MDVAE is trained on the intermediate representation of the VQ-VAEs “before quantization,” and the experiments focus on audiovisual manipulation, denoising, and emotion recognition rather than proposing, analyzing, or evaluating discrete audio/speech tokenization as the core contribution. Therefore this study does not meet the inclusion criterion requiring discrete audio/speech tokenization to be an explicit core object of study; I recommend exclusion (leaning toward exclusion rather than absolute).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder",
    "abstract": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.",
    "metadata": {
      "title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder",
      "summary": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.",
      "abstract": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.",
      "doi": "https://doi.org/10.1109/icassp49660.2025.10887856",
      "openalex_id": "https://openalex.org/W4406273203",
      "arxiv_id": "",
      "publication_date": "2025-03-12",
      "published": "2025-03-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a masked autoencoder for analysis, control, and generation of speech but does not discuss discrete audio tokenization or a finite vocabulary for audio reconstruction/generation; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a masked autoencoder for analysis, control, and generation of speech but does not discuss discrete audio tokenization or a finite vocabulary for audio reconstruction/generation; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a masked autoencoder model for speech analysis, control, and generation but does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences; it mainly references continuous attributes such as speaker identity, pitch, and signal-to-noise ratio, with no indication of discrete tokenization as a core focus, which does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a masked autoencoder model for speech analysis, control, and generation but does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences; it mainly references continuous attributes such as speaker identity, pitch, and signal-to-noise ratio, with no indication of discrete tokenization as a core focus, which does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Vector Quantized Masked Autoencoder for Speech Emotion Recognition",
    "abstract": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
    "metadata": {
      "title": "A Vector Quantized Masked Autoencoder for Speech Emotion Recognition",
      "summary": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
      "abstract": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193151",
      "openalex_id": "https://openalex.org/W4385484923",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a model (VQ-MAE-S) that operates in a discrete latent space via a vector-quantized VAE, i.e., it relies on discrete audio tokens for SER, satisfying the inclusion criterion, and it does not fall under exclusion criteria since it moves beyond continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a model (VQ-MAE-S) that operates in a discrete latent space via a vector-quantized VAE, i.e., it relies on discrete audio tokens for SER, satisfying the inclusion criterion, and it does not fall under exclusion criteria since it moves beyond continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a vector quantized masked autoencoder that operates in the discrete latent space of a vector quantized variational autoencoder, indicating the use and analysis of discrete audio tokenization for speech emotion recognition, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a vector quantized masked autoencoder that operates in the discrete latent space of a vector quantized variational autoencoder, indicating the use and analysis of discrete audio tokenization for speech emotion recognition, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Pre-Trained Audio-Visual Transformer for Emotion Recognition",
    "abstract": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
    "metadata": {
      "title": "A Pre-Trained Audio-Visual Transformer for Emotion Recognition",
      "summary": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
      "abstract": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747278",
      "openalex_id": "https://openalex.org/W4225959162",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a pre-trained audio-visual Transformer for emotion recognition but does not discuss discrete audio tokenization or converting audio into a sequence of discrete symbols; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a pre-trained audio-visual Transformer for emotion recognition but does not discuss discrete audio tokenization or converting audio into a sequence of discrete symbols; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an audio-visual Transformer pretrained for emotion recognition and fine-tunes on speech datasets; however, it does not mention proposing, analyzing, or evaluating discrete audio tokenization or transforming continuous audio into discrete token sequences, nor does it analyze discrete token representations as a core object of study. Hence, it mainly uses continuous audio features rather than discrete tokenization as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an audio-visual Transformer pretrained for emotion recognition and fine-tunes on speech datasets; however, it does not mention proposing, analyzing, or evaluating discrete audio tokenization or transforming continuous audio into discrete token sequences, nor does it analyze discrete token representations as a core object of study. Hence, it mainly uses continuous audio features rather than discrete tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "SimMIM: a Simple Framework for Masked Image Modeling",
    "abstract": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.",
    "metadata": {
      "title": "SimMIM: a Simple Framework for Masked Image Modeling",
      "summary": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.",
      "abstract": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.00943",
      "openalex_id": "https://openalex.org/W4312804044",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns masked image modeling in computer vision and does not involve discrete audio/speech tokenization, so it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns masked image modeling in computer vision and does not involve discrete audio/speech tokenization, so it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on masked image modeling using a simple framework called SimMIM, analyzing components and achieving strong image representation performance; it neither addresses audio or speech tokenization nor discusses discrete audio tokens or their quantization, which are central to the inclusion criteria. Therefore, this study does not meet the inclusion criteria related to discrete audio/speech tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on masked image modeling using a simple framework called SimMIM, analyzing components and achieving strong image representation performance; it neither addresses audio or speech tokenization nor discusses discrete audio tokens or their quantization, which are central to the inclusion criteria. Therefore, this study does not meet the inclusion criteria related to discrete audio/speech tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Sound of Motions",
    "abstract": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
    "metadata": {
      "title": "The Sound of Motions",
      "summary": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
      "abstract": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
      "doi": "https://doi.org/10.1109/iccv.2019.00182",
      "openalex_id": "https://openalex.org/W2981851635",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work proposes a motion-based audio-visual system for sound separation, it does not involve discrete audio tokenization or quantization, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the work proposes a motion-based audio-visual system for sound separation, it does not involve discrete audio tokenization or quantization, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual sound localization and separation by leveraging motion cues, without mentioning any discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; hence it does not meet the inclusion criterion and aligns with the exclusion of using only continuous audio features without discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual sound localization and separation by leveraging motion cues, without mentioning any discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; hence it does not meet the inclusion criterion and aligns with the exclusion of using only continuous audio features without discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition",
    "abstract": "Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale \"in-the-wild\" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.",
    "metadata": {
      "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition",
      "summary": "Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale \"in-the-wild\" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.",
      "abstract": "Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale \"in-the-wild\" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.",
      "doi": "https://doi.org/10.48550/arxiv.1811.07770",
      "openalex_id": "https://openalex.org/W2901836079",
      "arxiv_id": "",
      "publication_date": "2018-11-11",
      "published": "2018-11-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on extending a video-based affect recognition database and uses visual deep learning methods; there is no discrete audio tokenization or analysis of discrete audio representations as a core contribution, so it does not meet the audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on extending a video-based affect recognition database and uses visual deep learning methods; there is no discrete audio tokenization or analysis of discrete audio representations as a core contribution, so it does not meet the audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on visual analysis of human affect using video data and deep neural architectures for emotion recognition. There is no mention of audio/speech tokenization or discrete audio token representations; the work is based on visual signals and continuous emotion dimensions without any discrete audio token design or analysis, thus failing to meet the inclusion criteria and matching exclusion criteria related to lack of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on visual analysis of human affect using video data and deep neural architectures for emotion recognition. There is no mention of audio/speech tokenization or discrete audio token representations; the work is based on visual signals and continuous emotion dimensions without any discrete audio token design or analysis, thus failing to meet the inclusion criteria and matching exclusion criteria related to lack of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition",
    "abstract": "Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.",
    "metadata": {
      "title": "A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition",
      "summary": "Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.",
      "abstract": "Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.",
      "doi": "https://doi.org/10.48550/arxiv.2107.04187",
      "openalex_id": "https://openalex.org/W3180874665",
      "arxiv_id": "",
      "publication_date": "2021-07-09",
      "published": "2021-07-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a multi-modal learning method for action unit and facial expression recognition using visual and audio data, but there is no mention of discrete audio tokenization or token-based representations; it does not analyze discrete audio tokens, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a multi-modal learning method for action unit and facial expression recognition using visual and audio data, but there is no mention of discrete audio tokenization or token-based representations; it does not analyze discrete audio tokens, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-modal and multi-task learning for action unit and expression recognition using both visual and audio information, but it does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it analyze or evaluate such tokenization methods as a core object of study; therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for only using audio as a modality without discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-modal and multi-task learning for action unit and expression recognition using both visual and audio information, but it does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it analyze or evaluate such tokenization methods as a core object of study; therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for only using audio as a modality without discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emotion Recognition for In-the-wild Videos",
    "abstract": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
    "metadata": {
      "title": "Emotion Recognition for In-the-wild Videos",
      "summary": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
      "abstract": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
      "doi": "https://doi.org/10.48550/arxiv.2002.05447",
      "openalex_id": "https://openalex.org/W3005997049",
      "arxiv_id": "",
      "publication_date": "2020-02-13",
      "published": "2020-02-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on emotion recognition in in-the-wild videos using ResNet+BLSTM; it does not propose or analyze discrete audio tokenization or discrete audio representations, and relies on continuous features, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on emotion recognition in in-the-wild videos using ResNet+BLSTM; it does not propose or analyze discrete audio tokenization or discrete audio representations, and relies on continuous features, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on emotion recognition from videos using deep learning, with no mention or analysis of discrete audio/speech tokenization as a core subject, and does not propose any method related to converting continuous audio into discrete symbols; thus, it fails to meet the inclusion criteria and matches the exclusion criteria regarding lack of discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on emotion recognition from videos using deep learning, with no mention or analysis of discrete audio/speech tokenization as a core subject, and does not propose any method related to converting continuous audio into discrete symbols; thus, it fails to meet the inclusion criteria and matches the exclusion criteria regarding lack of discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Augmenting Convolutional networks with attention-based aggregation",
    "abstract": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
    "metadata": {
      "title": "Augmenting Convolutional networks with attention-based aggregation",
      "summary": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
      "abstract": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
      "doi": "https://doi.org/10.48550/arxiv.2112.13692",
      "openalex_id": "https://openalex.org/W4226487397",
      "arxiv_id": "",
      "publication_date": "2021-12-27",
      "published": "2021-12-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on attention-based aggregation for CNNs in computer vision and does not involve discrete audio/speech tokenization or any analysis of discrete audio tokens, so it fails the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on attention-based aggregation for CNNs in computer vision and does not involve discrete audio/speech tokenization or any analysis of discrete audio tokens, so it fails the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on augmenting convolutional neural networks with attention-based aggregation for computer vision tasks such as object classification, image segmentation, and detection, without any mention of discrete audio or speech tokenization, nor any relevant audio processing components. Therefore, it does not meet the inclusion criteria related to audio tokenization and is unrelated to the domain specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on augmenting convolutional neural networks with attention-based aggregation for computer vision tasks such as object classification, image segmentation, and detection, without any mention of discrete audio or speech tokenization, nor any relevant audio processing components. Therefore, it does not meet the inclusion criteria related to audio tokenization and is unrelated to the domain specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
    "abstract": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
    "metadata": {
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "summary": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
      "abstract": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
      "doi": "https://doi.org/10.48550/arxiv.2111.02172",
      "openalex_id": "https://openalex.org/W3209494353",
      "arxiv_id": "",
      "publication_date": "2021-11-03",
      "published": "2021-11-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work relies on continuous audio features and does not analyze or propose discrete audio tokenization, so it should be excluded under the discrete-token criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work relies on continuous audio features and does not analyze or propose discrete audio tokenization, so it should be excluded under the discrete-token criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal emotion recognition using continuous audio and video features with self-attention and residual structures but does not mention any discrete audio tokenization or quantization methods for audio signals as a core component of the work, which is a key inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal emotion recognition using continuous audio and video features with self-attention and residual structures but does not mention any discrete audio tokenization or quantization methods for audio signals as a core component of the work, which is a key inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild",
    "abstract": "Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.",
    "metadata": {
      "title": "Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild",
      "summary": "Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.",
      "abstract": "Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.",
      "doi": "https://doi.org/10.1109/tcsvt.2023.3312858",
      "openalex_id": "https://openalex.org/W4386699383",
      "arxiv_id": "",
      "publication_date": "2023-09-13",
      "published": "2023-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on transformer-based fusion for multimodal emotion recognition across audio, image, and text but does not introduce or evaluate discrete audio tokenization; it relies on continuous audio features, triggering the exclusion criterion for tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on transformer-based fusion for multimodal emotion recognition across audio, image, and text but does not introduce or evaluate discrete audio tokenization; it relies on continuous audio features, triggering the exclusion criterion for tokenization studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Transformer-based multimodal framework that includes audio, image, and text modalities for dynamic facial expression recognition; however, there is no indication that it proposes, analyzes, or evaluates discrete audio/speech tokenization for audio reconstruction, generation, or understanding. The audio features appear to be used as continuous modalities without discrete tokenization design or analysis, and the contribution centers on multimodal fusion rather than discrete audio tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Transformer-based multimodal framework that includes audio, image, and text modalities for dynamic facial expression recognition; however, there is no indication that it proposes, analyzes, or evaluates discrete audio/speech tokenization for audio reconstruction, generation, or understanding. The audio features appear to be used as continuous modalities without discrete tokenization design or analysis, and the contribution centers on multimodal fusion rather than discrete audio tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
    "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
    "metadata": {
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "summary": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
      "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
      "doi": "https://doi.org/10.48550/arxiv.2104.11178",
      "openalex_id": "https://openalex.org/W3154596443",
      "arxiv_id": "",
      "publication_date": "2021-04-22",
      "published": "2021-04-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on multimodal self-supervised learning from raw video, audio, and text using transformers; it does not propose or analyze discrete audio tokenization or a codebook, instead using raw signals, so it does not meet the discrete audio tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multimodal self-supervised learning from raw video, audio, and text using transformers; it does not propose or analyze discrete audio tokenization or a codebook, instead using raw signals, so it does not meet the discrete audio tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a Transformer-based multimodal learning framework but does not explicitly focus on discrete audio/speech tokenization; it mainly discusses end-to-end learning from raw audio signals without mentioning discrete token conversion or analysis vital for the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a Transformer-based multimodal learning framework but does not explicitly focus on discrete audio/speech tokenization; it mainly discusses end-to-end learning from raw audio signals without mentioning discrete token conversion or analysis vital for the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
    "abstract": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
    "metadata": {
      "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
      "summary": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
      "abstract": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
      "doi": "https://doi.org/10.48550/arxiv.2111.02735",
      "openalex_id": "https://openalex.org/W3211224152",
      "arxiv_id": "",
      "publication_date": "2021-11-04",
      "published": "2021-11-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on fine-tuning wav2vec 2.0/HuBERT for downstream speech tasks and does not explicitly analyze or evaluate discrete audio tokenization/quantization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on fine-tuning wav2vec 2.0/HuBERT for downstream speech tasks and does not explicitly analyze or evaluate discrete audio tokenization/quantization as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-tuning wav2vec 2.0 and HuBERT models for speech tasks but does not explicitly discuss the proposal, analysis, or evaluation of discrete audio/speech tokenization methods; thus, it lacks a core focus on discrete tokenization as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning wav2vec 2.0 and HuBERT models for speech tasks but does not explicitly discuss the proposal, analysis, or evaluation of discrete audio/speech tokenization methods; thus, it lacks a core focus on discrete tokenization as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MizAR 60 for Mizar 50",
    "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
    "metadata": {
      "title": "MizAR 60 for Mizar 50",
      "summary": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "doi": "https://doi.org/10.4230/lipics.itp.2023.19",
      "openalex_id": "https://openalex.org/W4385245566",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes AI-driven automated theorem proving for Mizar; it contains no discrete audio tokenization analysis or speech-tokenization contributions, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes AI-driven automated theorem proving for Mizar; it contains no discrete audio tokenization analysis or speech-tokenization contributions, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an AI/TP system for theorem proving in the Mizar system and do not mention any work related to discrete audio/speech tokenization or audio signal processing, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an AI/TP system for theorem proving in the Mizar system and do not mention any work related to discrete audio/speech tokenization or audio signal processing, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
    "abstract": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",
    "metadata": {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "summary": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",
      "abstract": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",
      "doi": "https://doi.org/10.48550/arxiv.2203.12602",
      "openalex_id": "https://openalex.org/W4221167396",
      "arxiv_id": "",
      "publication_date": "2022-03-23",
      "published": "2022-03-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns video masked autoencoders (VideoMAE) for self-supervised video pre-training and does not involve discrete audio tokenization or analysis of speech/relevant discrete audio representations, thus failing the specified audio/tokenization inclusion criterion and not meeting the exclusion criteria to exclude on non-audio basis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns video masked autoencoders (VideoMAE) for self-supervised video pre-training and does not involve discrete audio tokenization or analysis of speech/relevant discrete audio representations, thus failing the specified audio/tokenization inclusion criterion and not meeting the exclusion criteria to exclude on non-audio basis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised video pre-training using a masked autoencoder approach, with no mention or analysis of discrete audio/speech tokenization or any discrete quantization of audio signals. It primarily treats video data and does not address discrete token representations for audio or speech tasks, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised video pre-training using a masked autoencoder approach, with no mention or analysis of discrete audio/speech tokenization or any discrete quantization of audio signals. It primarily treats video data and does not address discrete token representations for audio or speech tasks, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Masked Autoencoders that Listen",
    "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
    "metadata": {
      "title": "Masked Autoencoders that Listen",
      "summary": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "doi": "https://doi.org/10.48550/arxiv.2207.06405",
      "openalex_id": "https://openalex.org/W4285483774",
      "arxiv_id": "",
      "publication_date": "2022-07-13",
      "published": "2022-07-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes Audio-MAE applied to spectrograms and reconstructs continuous spectrograms without introducing discrete audio tokenization or a finite symbol vocabulary; thus it does not meet the inclusion criterion and triggers the exclusion criterion about using discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes Audio-MAE applied to spectrograms and reconstructs continuous spectrograms without introducing discrete audio tokenization or a finite symbol vocabulary; thus it does not meet the inclusion criterion and triggers the exclusion criterion about using discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a self-supervised learning method on audio spectrograms using masked autoencoders, but it does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; it focuses on features (spectrogram patches) rather than discrete token representations, so it does not meet the inclusion criteria regarding discrete tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a self-supervised learning method on audio spectrograms using masked autoencoders, but it does not explicitly mention discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; it focuses on features (spectrogram patches) rather than discrete token representations, so it does not meet the inclusion criteria regarding discrete tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond",
    "abstract": "Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
    "metadata": {
      "title": "A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond",
      "summary": "Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
      "abstract": "Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
      "doi": "https://doi.org/10.48550/arxiv.2208.00173",
      "openalex_id": "https://openalex.org/W4289644862",
      "arxiv_id": "",
      "publication_date": "2022-07-30",
      "published": "2022-07-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on masked autoencoders for vision and self-supervised learning, with no discrete audio tokenization or speech-token analysis, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on masked autoencoders for vision and self-supervised learning, with no discrete audio tokenization or speech-token analysis, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on masked autoencoders for self-supervised learning in vision and beyond but do not explicitly discuss discrete audio/speech tokenization as a core study object, nor do they mention audio signal discretization or evaluation related to audio tasks. The study appears to center around vision applications and does not satisfy the inclusion criteria related to audio tokenization nor does it address or analyze discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on masked autoencoders for self-supervised learning in vision and beyond but do not explicitly discuss discrete audio/speech tokenization as a core study object, nor do they mention audio signal discretization or evaluation related to audio tasks. The study appears to center around vision applications and does not satisfy the inclusion criteria related to audio tokenization nor does it address or analyze discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Masked Autoencoders As Spatiotemporal Learners",
    "abstract": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
    "metadata": {
      "title": "Masked Autoencoders As Spatiotemporal Learners",
      "summary": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
      "abstract": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
      "doi": "https://doi.org/10.48550/arxiv.2205.09113",
      "openalex_id": "https://openalex.org/W4280490805",
      "arxiv_id": "",
      "publication_date": "2022-05-18",
      "published": "2022-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on video masked autoencoders for spatiotemporal learning and does not address discrete audio tokenization or speech token vocabularies, thus it does not meet the audio-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on video masked autoencoders for spatiotemporal learning and does not address discrete audio tokenization or speech token vocabularies, thus it does not meet the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on spatiotemporal learning using Masked Autoencoders applied to video representation, with no mention or analysis of discrete audio/speech tokenization or any conversion of continuous audio signals into discrete symbols. It neither addresses discrete tokenization for audio/speech nor evaluates its application in speech-related tasks, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on spatiotemporal learning using Masked Autoencoders applied to video representation, with no mention or analysis of discrete audio/speech tokenization or any conversion of continuous audio signals into discrete symbols. It neither addresses discrete tokenization for audio/speech nor evaluates its application in speech-related tasks, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Baselines with Momentum Contrastive Learning",
    "abstract": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
    "metadata": {
      "title": "Improved Baselines with Momentum Contrastive Learning",
      "summary": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
      "abstract": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
      "doi": "https://doi.org/10.48550/arxiv.2003.04297",
      "openalex_id": "https://openalex.org/W3009561768",
      "arxiv_id": "",
      "publication_date": "2020-03-09",
      "published": "2020-03-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on momentum contrastive learning for image-based unsupervised learning and does not involve discrete audio/speech tokenization or token-based analysis; thus it fails the audio-tokenization inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on momentum contrastive learning for image-based unsupervised learning and does not involve discrete audio/speech tokenization or token-based analysis; thus it fails the audio-tokenization inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving baselines for contrastive unsupervised learning methods in general (MoCo and SimCLR), primarily targeting representation learning without explicit mention or analysis of discrete audio/speech tokenization or quantization into finite vocabularies. It does not meet the inclusion criteria about discrete audio tokenization as a core subject, nor does it analyze or propose discrete audio tokens relevant to speech-specific tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving baselines for contrastive unsupervised learning methods in general (MoCo and SimCLR), primarily targeting representation learning without explicit mention or analysis of discrete audio/speech tokenization or quantization into finite vocabularies. It does not meet the inclusion criteria about discrete audio tokenization as a core subject, nor does it analyze or propose discrete audio tokens relevant to speech-specific tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
    "abstract": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
    "metadata": {
      "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
      "summary": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
      "abstract": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
      "doi": "https://doi.org/10.48550/arxiv.2107.10834",
      "openalex_id": "https://openalex.org/W3184087575",
      "arxiv_id": "",
      "publication_date": "2021-07-22",
      "published": "2021-07-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper is about vision-based multi-label classification using Transformer decoders and does not involve discrete audio/speech tokenization, so it does not meet the inclusion criteria for discrete tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper is about vision-based multi-label classification using Transformer decoders and does not involve discrete audio/speech tokenization, so it does not meet the inclusion criteria for discrete tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Transformer-based approach for multi-label image classification and does not involve audio or speech tokenization, discrete audio symbols, or analysis related to audio signal processing; therefore, it does not meet any inclusion criteria and falls under exclusion criteria due to absence of audio-related discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Transformer-based approach for multi-label image classification and does not involve audio or speech tokenization, discrete audio symbols, or analysis related to audio signal processing; therefore, it does not meet any inclusion criteria and falls under exclusion criteria due to absence of audio-related discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
    "abstract": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
    "metadata": {
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "summary": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
      "abstract": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
      "doi": "https://doi.org/10.48550/arxiv.1706.02677",
      "openalex_id": "https://openalex.org/W2622263826",
      "arxiv_id": "",
      "publication_date": "2017-06-08",
      "published": "2017-06-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper is about large minibatch SGD for image classification and has no discrete audio/speech tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper is about large minibatch SGD for image classification and has no discrete audio/speech tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient training of large minibatch SGD for ImageNet image classification; it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, nor does it involve audio data at all.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient training of large minibatch SGD for ImageNet image classification; it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, nor does it involve audio data at all.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
    "abstract": "In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.",
    "metadata": {
      "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "summary": "In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.",
      "abstract": "In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639081",
      "openalex_id": "https://openalex.org/W2025198378",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a multilingual deep neural network with shared hidden layers for speech recognition, but does not propose, analyze, or evaluate discrete audio tokenization or discrete token representations; it relies on continuous features and language-dependent softmax layers, thus not satisfying the discrete-tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a multilingual deep neural network with shared hidden layers for speech recognition, but does not propose, analyze, or evaluate discrete audio tokenization or discrete token representations; it relies on continuous features and language-dependent softmax layers, thus not satisfying the discrete-tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multilingual deep neural network architecture for speech recognition by sharing hidden layers across languages; however, it does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens. Instead, it emphasizes acoustic modeling and error reduction in recognition, without analysis or proposal of discrete tokenization methods, which is central to the inclusion criteria. Therefore, it does not meet the core criterion of studying discrete audio tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multilingual deep neural network architecture for speech recognition by sharing hidden layers across languages; however, it does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens. Instead, it emphasizes acoustic modeling and error reduction in recognition, without analysis or proposal of discrete tokenization methods, which is central to the inclusion criteria. Therefore, it does not meet the core criterion of studying discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Spoken Knowledge Organization by Semantic Structuring and a Prototype Course Lecture System for Personalized Learning",
    "abstract": "It takes very long time to go through a complete online course. Without proper background, it is also difficult to understand retrieved spoken paragraphs. This paper therefore presents a new approach of spoken knowledge organization for course lectures for efficient personalized learning. Automatically extracted key terms are taken as the fundamental elements of the semantics of the course. Key term graph constructed by connecting related key terms forms the backbone of the global semantic structure. Audio/video signals are divided into multi-layer temporal structure including paragraphs, sections and chapters, each of which includes a summary as the local semantic structure. The interconnection between semantic structure and temporal structure together with spoken term detection jointly offer to the learners efficient ways to navigate across the course knowledge with personalized learning paths considering their personal interests, available time and background knowledge. A preliminary prototype system has also been successfully developed.",
    "metadata": {
      "title": "Spoken Knowledge Organization by Semantic Structuring and a Prototype Course Lecture System for Personalized Learning",
      "summary": "It takes very long time to go through a complete online course. Without proper background, it is also difficult to understand retrieved spoken paragraphs. This paper therefore presents a new approach of spoken knowledge organization for course lectures for efficient personalized learning. Automatically extracted key terms are taken as the fundamental elements of the semantics of the course. Key term graph constructed by connecting related key terms forms the backbone of the global semantic structure. Audio/video signals are divided into multi-layer temporal structure including paragraphs, sections and chapters, each of which includes a summary as the local semantic structure. The interconnection between semantic structure and temporal structure together with spoken term detection jointly offer to the learners efficient ways to navigate across the course knowledge with personalized learning paths considering their personal interests, available time and background knowledge. A preliminary prototype system has also been successfully developed.",
      "abstract": "It takes very long time to go through a complete online course. Without proper background, it is also difficult to understand retrieved spoken paragraphs. This paper therefore presents a new approach of spoken knowledge organization for course lectures for efficient personalized learning. Automatically extracted key terms are taken as the fundamental elements of the semantics of the course. Key term graph constructed by connecting related key terms forms the backbone of the global semantic structure. Audio/video signals are divided into multi-layer temporal structure including paragraphs, sections and chapters, each of which includes a summary as the local semantic structure. The interconnection between semantic structure and temporal structure together with spoken term detection jointly offer to the learners efficient ways to navigate across the course knowledge with personalized learning paths considering their personal interests, available time and background knowledge. A preliminary prototype system has also been successfully developed.",
      "doi": "https://doi.org/10.1109/taslp.2014.2310993",
      "openalex_id": "https://openalex.org/W2121842316",
      "arxiv_id": "",
      "publication_date": "2014-03-11",
      "published": "2014-03-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses semantic structuring and spoken term detection for lectures and a prototype system, but it does not explicitly propose, analyze, or evaluate discrete speech tokenization into a finite vocabulary as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses semantic structuring and spoken term detection for lectures and a prototype system, but it does not explicitly propose, analyze, or evaluate discrete speech tokenization into a finite vocabulary as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on organizing spoken knowledge and personalized learning paths using key terms extracted from audio/video lectures, but it does not explicitly discuss the discrete tokenization of continuous audio signals into finite symbol sequences for audio reconstruction, generation, or understanding tasks. It also lacks mention of neural encoding or quantization-based discrete token representations, which are central to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on organizing spoken knowledge and personalized learning paths using key terms extracted from audio/video lectures, but it does not explicitly discuss the discrete tokenization of continuous audio signals into finite symbol sequences for audio reconstruction, generation, or understanding tasks. It also lacks mention of neural encoding or quantization-based discrete token representations, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription",
    "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third-from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%-using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",
    "metadata": {
      "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription",
      "summary": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third-from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%-using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",
      "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third-from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%-using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",
      "doi": "https://doi.org/10.1109/asru.2011.6163899",
      "openalex_id": "https://openalex.org/W2160306971",
      "arxiv_id": "",
      "publication_date": "2011-12-01",
      "published": "2011-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses CD-DNN-HMMs and feature engineering with continuous acoustic features; it does not explicitly analyze or evaluate discrete audio/tokenization of speech into a finite vocabulary, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses CD-DNN-HMMs and feature engineering with continuous acoustic features; it does not explicitly analyze or evaluate discrete audio/tokenization of speech into a finite vocabulary, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Context-Dependent Deep Neural Networks applied to speech transcription and discusses word error rate reductions using continuous features (e.g., HLDA features) and deep neural network models, but it does not explicitly analyze or propose discrete audio/speech tokenization converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria related to discrete tokenization and seems more focused on continuous feature modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Context-Dependent Deep Neural Networks applied to speech transcription and discusses word error rate reductions using continuous features (e.g., HLDA features) and deep neural network models, but it does not explicitly analyze or propose discrete audio/speech tokenization converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria related to discrete tokenization and seems more focused on continuous feature modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Adaptation of context-dependent deep neural networks for automatic speech recognition",
    "abstract": "In this paper, we evaluate the effectiveness of adaptation methods for context-dependent deep-neural-network hidden Markov models (CD-DNN-HMMs) for automatic speech recognition. We investigate the affine transformation and several of its variants for adapting the top hidden layer. We compare the affine transformations against direct adaptation of the softmax layer weights. The feature-space discriminative linear regression (fDLR) method with the affine transformations on the input layer is also evaluated. On a large vocabulary speech recognition task, a stochastic gradient ascent implementation of the fDLR and the top hidden layer adaptation is shown to reduce word error rates (WERs) by 17% and 14%, respectively, compared to the baseline DNN performances. With a batch update implementation, the softmax layer adaptation technique reduces WERs by 10%. We observe that using bias shift performs as well as doing scaling plus bias shift.",
    "metadata": {
      "title": "Adaptation of context-dependent deep neural networks for automatic speech recognition",
      "summary": "In this paper, we evaluate the effectiveness of adaptation methods for context-dependent deep-neural-network hidden Markov models (CD-DNN-HMMs) for automatic speech recognition. We investigate the affine transformation and several of its variants for adapting the top hidden layer. We compare the affine transformations against direct adaptation of the softmax layer weights. The feature-space discriminative linear regression (fDLR) method with the affine transformations on the input layer is also evaluated. On a large vocabulary speech recognition task, a stochastic gradient ascent implementation of the fDLR and the top hidden layer adaptation is shown to reduce word error rates (WERs) by 17% and 14%, respectively, compared to the baseline DNN performances. With a batch update implementation, the softmax layer adaptation technique reduces WERs by 10%. We observe that using bias shift performs as well as doing scaling plus bias shift.",
      "abstract": "In this paper, we evaluate the effectiveness of adaptation methods for context-dependent deep-neural-network hidden Markov models (CD-DNN-HMMs) for automatic speech recognition. We investigate the affine transformation and several of its variants for adapting the top hidden layer. We compare the affine transformations against direct adaptation of the softmax layer weights. The feature-space discriminative linear regression (fDLR) method with the affine transformations on the input layer is also evaluated. On a large vocabulary speech recognition task, a stochastic gradient ascent implementation of the fDLR and the top hidden layer adaptation is shown to reduce word error rates (WERs) by 17% and 14%, respectively, compared to the baseline DNN performances. With a batch update implementation, the softmax layer adaptation technique reduces WERs by 10%. We observe that using bias shift performs as well as doing scaling plus bias shift.",
      "doi": "https://doi.org/10.1109/slt.2012.6424251",
      "openalex_id": "https://openalex.org/W2087006792",
      "arxiv_id": "",
      "publication_date": "2012-12-01",
      "published": "2012-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper investigates adaptation methods for CD-DNN-HMM acoustic models using continuous features and does not address discrete audio tokenization or token-level representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper investigates adaptation methods for CD-DNN-HMM acoustic models using continuous features and does not address discrete audio tokenization or token-level representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adaptation methods for deep neural networks in automatic speech recognition, primarily dealing with continuous feature adaptation and not on discrete audio/speech tokenization or analysis of discrete token representations as a core subject, thus it does not meet the inclusion criteria and fits the exclusion criteria related to continuous audio features without discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adaptation methods for deep neural networks in automatic speech recognition, primarily dealing with continuous feature adaptation and not on discrete audio/speech tokenization or analysis of discrete token representations as a core subject, thus it does not meet the inclusion criteria and fits the exclusion criteria related to continuous audio features without discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hermitian based hidden activation functions for adaptation of hybrid HMM/ANN models",
    "abstract": "This work is concerned with speaker adaptation techniques for artificial neural network (ANN) implemented as feed-forward multi-layer perceptrons (MLPs) in the context of large vocabulary continuous speech recognition (LVCSR). Most successful speaker adaptation techniques for MLPs consist of augmenting the neural architecture with a linear transformation network connected to either the input or the output layer. The weights of this additional linear layer are learned during the adaptation phase while all of the other weights are kept frozen in order to avoid over-fitting. In doing so, the structure of the speaker-dependent (SD) and speaker-independent (Si) architecture differs and the number of adaptation parameters depends upon the dimension of either the input or output layers. We propose an alternative neural architecture for speaker-adaptation to overcome the limits of current approaches. This neural architecture adopts hidden activation functions that can be learned directly from the adaptation data. This adaptive capability of the hidden activation function is achieved through the use of orthonormal Hermite polynomials. Experimental evidence gathered on the Wall Street Journal Nov92 task demonstrates the viability of the proposed technique.",
    "metadata": {
      "title": "Hermitian based hidden activation functions for adaptation of hybrid HMM/ANN models",
      "summary": "This work is concerned with speaker adaptation techniques for artificial neural network (ANN) implemented as feed-forward multi-layer perceptrons (MLPs) in the context of large vocabulary continuous speech recognition (LVCSR). Most successful speaker adaptation techniques for MLPs consist of augmenting the neural architecture with a linear transformation network connected to either the input or the output layer. The weights of this additional linear layer are learned during the adaptation phase while all of the other weights are kept frozen in order to avoid over-fitting. In doing so, the structure of the speaker-dependent (SD) and speaker-independent (Si) architecture differs and the number of adaptation parameters depends upon the dimension of either the input or output layers. We propose an alternative neural architecture for speaker-adaptation to overcome the limits of current approaches. This neural architecture adopts hidden activation functions that can be learned directly from the adaptation data. This adaptive capability of the hidden activation function is achieved through the use of orthonormal Hermite polynomials. Experimental evidence gathered on the Wall Street Journal Nov92 task demonstrates the viability of the proposed technique.",
      "abstract": "This work is concerned with speaker adaptation techniques for artificial neural network (ANN) implemented as feed-forward multi-layer perceptrons (MLPs) in the context of large vocabulary continuous speech recognition (LVCSR). Most successful speaker adaptation techniques for MLPs consist of augmenting the neural architecture with a linear transformation network connected to either the input or the output layer. The weights of this additional linear layer are learned during the adaptation phase while all of the other weights are kept frozen in order to avoid over-fitting. In doing so, the structure of the speaker-dependent (SD) and speaker-independent (Si) architecture differs and the number of adaptation parameters depends upon the dimension of either the input or output layers. We propose an alternative neural architecture for speaker-adaptation to overcome the limits of current approaches. This neural architecture adopts hidden activation functions that can be learned directly from the adaptation data. This adaptive capability of the hidden activation function is achieved through the use of orthonormal Hermite polynomials. Experimental evidence gathered on the Wall Street Journal Nov92 task demonstrates the viability of the proposed technique.",
      "doi": "https://doi.org/10.21437/interspeech.2012-13",
      "openalex_id": "https://openalex.org/W2397860787",
      "arxiv_id": "",
      "publication_date": "2012-09-09",
      "published": "2012-09-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on speaker adaptation and novel hidden activation functions for ANN-based LVCSR models; it does not address discrete audio tokenization or token sequences, and seems based on continuous features, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on speaker adaptation and novel hidden activation functions for ANN-based LVCSR models; it does not address discrete audio tokenization or token sequences, and seems based on continuous features, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker adaptation techniques using neural networks with adaptive hidden activation functions, without addressing the discrete tokenization of continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead uses continuous audio features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker adaptation techniques using neural networks with adaptive hidden activation functions, without addressing the discrete tokenization of continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead uses continuous audio features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised training of acoustic models for large vocabulary continuous speech recognition",
    "abstract": "For large vocabulary continuous speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were thus recorded from various sources and had to be transcribed manually. It is thus desirable to train a recognizer with as little manually transcribed acoustic data as possible. Since untranscribed speech is available in various forms nowadays, the unsupervised training of a speech recognizer on recognized transcriptions is studied in this paper. A low-cost recognizer trained with between one and six h of manually transcribed speech is used to recognize 72 h of untranscribed acoustic data. These transcriptions are then used in combination with a confidence measure to train an improved recognizer. The effect of the confidence measure which is used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Starting with only one h of transcribed acoustic data, a recognition system is trained fully automatically. With this iterative training procedure, the word error rates are reduced from 71.3% to 38.3% on the Broadcast News'96 evaluation test set and from 65.6% to 29.3% on the Broadcast News'98 evaluation test set. In comparison with an optimized system trained with the manually generated transcriptions of the complete 72 h training corpus, the word error rates increase by 14.3% relative and 18.6% relative, respectively.",
    "metadata": {
      "title": "Unsupervised training of acoustic models for large vocabulary continuous speech recognition",
      "summary": "For large vocabulary continuous speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were thus recorded from various sources and had to be transcribed manually. It is thus desirable to train a recognizer with as little manually transcribed acoustic data as possible. Since untranscribed speech is available in various forms nowadays, the unsupervised training of a speech recognizer on recognized transcriptions is studied in this paper. A low-cost recognizer trained with between one and six h of manually transcribed speech is used to recognize 72 h of untranscribed acoustic data. These transcriptions are then used in combination with a confidence measure to train an improved recognizer. The effect of the confidence measure which is used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Starting with only one h of transcribed acoustic data, a recognition system is trained fully automatically. With this iterative training procedure, the word error rates are reduced from 71.3% to 38.3% on the Broadcast News'96 evaluation test set and from 65.6% to 29.3% on the Broadcast News'98 evaluation test set. In comparison with an optimized system trained with the manually generated transcriptions of the complete 72 h training corpus, the word error rates increase by 14.3% relative and 18.6% relative, respectively.",
      "abstract": "For large vocabulary continuous speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were thus recorded from various sources and had to be transcribed manually. It is thus desirable to train a recognizer with as little manually transcribed acoustic data as possible. Since untranscribed speech is available in various forms nowadays, the unsupervised training of a speech recognizer on recognized transcriptions is studied in this paper. A low-cost recognizer trained with between one and six h of manually transcribed speech is used to recognize 72 h of untranscribed acoustic data. These transcriptions are then used in combination with a confidence measure to train an improved recognizer. The effect of the confidence measure which is used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Starting with only one h of transcribed acoustic data, a recognition system is trained fully automatically. With this iterative training procedure, the word error rates are reduced from 71.3% to 38.3% on the Broadcast News'96 evaluation test set and from 65.6% to 29.3% on the Broadcast News'98 evaluation test set. In comparison with an optimized system trained with the manually generated transcriptions of the complete 72 h training corpus, the word error rates increase by 14.3% relative and 18.6% relative, respectively.",
      "doi": "https://doi.org/10.1109/tsa.2004.838537",
      "openalex_id": "https://openalex.org/W2056786202",
      "arxiv_id": "",
      "publication_date": "2004-12-20",
      "published": "2004-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses semi-supervised/unsupervised ASR training using limited transcripts and confidence-based iterative labeling; it does not analyze or propose discrete audio tokenization (quantization/tokens) as a core object, so it does not meet the primary inclusion criterion.\n",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper discusses semi-supervised/unsupervised ASR training using limited transcripts and confidence-based iterative labeling; it does not analyze or propose discrete audio tokenization (quantization/tokens) as a core object, so it does not meet the primary inclusion criterion.\n",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised training of acoustic models using recognized transcriptions to improve large vocabulary continuous speech recognition but does not explicitly discuss or analyze discrete audio/speech tokenization or converting continuous signals into discrete symbolic sequences as a core object; it mainly deals with model training with confidence measures and error rates rather than discrete token design or quantization relevant to language-model-style token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised training of acoustic models using recognized transcriptions to improve large vocabulary continuous speech recognition but does not explicitly discuss or analyze discrete audio/speech tokenization or converting continuous signals into discrete symbolic sequences as a core object; it mainly deals with model training with confidence measures and error rates rather than discrete token design or quantization relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns",
    "abstract": "Query expansion techniques were originally developed for text information retrieval in order to retrieve the documents not containing the query terms but semantically related to the query. This is achieved by assuming the terms frequently occurring in the top-ranked documents in the first-pass retrieval results to be query-related and using them to expand the query to do the second-pass retrieval. However, when this approach was used for spoken content retrieval, the inevitable recognition errors and the OOV problems in ASR make it difficult for many query-related terms to be included in the expanded query, and much of the information carried by the speech signal is lost during recognition and not recoverable. In this paper, we propose to use a second ASR engine based on acoustic patterns automatically discovered from the spoken archive used for retrieval. These acoustic patterns are discovered directly based on the signal characteristics, and therefore can compensate for the information lost during recognition to a good extent. When a text query is entered, the system generates the first-pass retrieval results based on the transcriptions of the spoken segments obtained via the conventional ASR. The acoustic patterns frequently occurring in the spoken segments ranked on top of the first-pass results are considered as query-related, and the spoken segments containing these query-related acoustic patterns are retrieved. In this way, even though some query-related terms are OOV or incorrectly recognized, the segments including these terms can still be retrieved by acoustic patterns corresponding to these terms. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
    "metadata": {
      "title": "Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns",
      "summary": "Query expansion techniques were originally developed for text information retrieval in order to retrieve the documents not containing the query terms but semantically related to the query. This is achieved by assuming the terms frequently occurring in the top-ranked documents in the first-pass retrieval results to be query-related and using them to expand the query to do the second-pass retrieval. However, when this approach was used for spoken content retrieval, the inevitable recognition errors and the OOV problems in ASR make it difficult for many query-related terms to be included in the expanded query, and much of the information carried by the speech signal is lost during recognition and not recoverable. In this paper, we propose to use a second ASR engine based on acoustic patterns automatically discovered from the spoken archive used for retrieval. These acoustic patterns are discovered directly based on the signal characteristics, and therefore can compensate for the information lost during recognition to a good extent. When a text query is entered, the system generates the first-pass retrieval results based on the transcriptions of the spoken segments obtained via the conventional ASR. The acoustic patterns frequently occurring in the spoken segments ranked on top of the first-pass results are considered as query-related, and the spoken segments containing these query-related acoustic patterns are retrieved. In this way, even though some query-related terms are OOV or incorrectly recognized, the segments including these terms can still be retrieved by acoustic patterns corresponding to these terms. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
      "abstract": "Query expansion techniques were originally developed for text information retrieval in order to retrieve the documents not containing the query terms but semantically related to the query. This is achieved by assuming the terms frequently occurring in the top-ranked documents in the first-pass retrieval results to be query-related and using them to expand the query to do the second-pass retrieval. However, when this approach was used for spoken content retrieval, the inevitable recognition errors and the OOV problems in ASR make it difficult for many query-related terms to be included in the expanded query, and much of the information carried by the speech signal is lost during recognition and not recoverable. In this paper, we propose to use a second ASR engine based on acoustic patterns automatically discovered from the spoken archive used for retrieval. These acoustic patterns are discovered directly based on the signal characteristics, and therefore can compensate for the information lost during recognition to a good extent. When a text query is entered, the system generates the first-pass retrieval results based on the transcriptions of the spoken segments obtained via the conventional ASR. The acoustic patterns frequently occurring in the spoken segments ranked on top of the first-pass results are considered as query-related, and the spoken segments containing these query-related acoustic patterns are retrieved. In this way, even though some query-related terms are OOV or incorrectly recognized, the segments including these terms can still be retrieved by acoustic patterns corresponding to these terms. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639283",
      "openalex_id": "https://openalex.org/W1991727948",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses using acoustic patterns for retrieval but does not explicitly propose, analyze, or evaluate discrete audio tokenization into a finite-symbol sequence for speech reconstruction/understanding; its core contribution appears to be pattern-based representations rather than discrete tokenization suitable for the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses using acoustic patterns for retrieval but does not explicitly propose, analyze, or evaluate discrete audio tokenization into a finite-symbol sequence for speech reconstruction/understanding; its core contribution appears to be pattern-based representations rather than discrete tokenization suitable for the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes to enhance spoken content retrieval by using a second ASR engine based on acoustic patterns automatically discovered from the spoken archive; however, these acoustic patterns are derived based on signal characteristics and used for retrieval rather than explicitly quantizing continuous audio into a sequence of discrete tokens for audio reconstruction or generation. The focus is more on semantic retrieval via discovered acoustic patterns than on discrete audio tokenization as defined in the inclusion criteria. Therefore, it does not clearly meet the core requirement of analyzing or evaluating discrete audio/speech tokenization methods for language-model-style discrete token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes to enhance spoken content retrieval by using a second ASR engine based on acoustic patterns automatically discovered from the spoken archive; however, these acoustic patterns are derived based on signal characteristics and used for retrieval rather than explicitly quantizing continuous audio into a sequence of discrete tokens for audio reconstruction or generation. The focus is more on semantic retrieval via discovered acoustic patterns than on discrete audio tokenization as defined in the inclusion criteria. Therefore, it does not clearly meet the core requirement of analyzing or evaluating discrete audio/speech tokenization methods for language-model-style discrete token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Improving DNN speaker independence with I-vector inputs",
    "abstract": "We propose providing additional utterance-level features as inputs to a deep neural network (DNN) to facilitate speaker, channel and background normalization. Modifications of the basic algorithm are developed which result in significant reductions in word error rates (WERs). The algorithms are shown to combine well with speaker adaptation by backpropagation, resulting in a 9% relative WER reduction. We address implementation of the algorithm for a streaming task.",
    "metadata": {
      "title": "Improving DNN speaker independence with I-vector inputs",
      "summary": "We propose providing additional utterance-level features as inputs to a deep neural network (DNN) to facilitate speaker, channel and background normalization. Modifications of the basic algorithm are developed which result in significant reductions in word error rates (WERs). The algorithms are shown to combine well with speaker adaptation by backpropagation, resulting in a 9% relative WER reduction. We address implementation of the algorithm for a streaming task.",
      "abstract": "We propose providing additional utterance-level features as inputs to a deep neural network (DNN) to facilitate speaker, channel and background normalization. Modifications of the basic algorithm are developed which result in significant reductions in word error rates (WERs). The algorithms are shown to combine well with speaker adaptation by backpropagation, resulting in a 9% relative WER reduction. We address implementation of the algorithm for a streaming task.",
      "doi": "https://doi.org/10.1109/icassp.2014.6853591",
      "openalex_id": "https://openalex.org/W1985371235",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on using continuous features (e.g., i-vectors) for DNN-based speech recognition without any discrete audio tokenization or token-level quantization, thus not meeting the inclusion criteria and aligning with the exclusion criterion regarding continuous features.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on using continuous features (e.g., i-vectors) for DNN-based speech recognition without any discrete audio tokenization or token-level quantization, thus not meeting the inclusion criteria and aligning with the exclusion criterion regarding continuous features.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speaker independence in DNN-based speech recognition using I-vector inputs for normalization and adaptation and does not explicitly address discrete audio or speech tokenization as a core subject; it primarily deals with continuous feature enhancement rather than discrete tokenization or quantization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speaker independence in DNN-based speech recognition using I-vector inputs for normalization and adaptation and does not explicitly address discrete audio or speech tokenization as a core subject; it primarily deals with continuous feature enhancement rather than discrete tokenization or quantization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards unsupervised semantic retrieval of spoken content with query expansion based on automatically discovered acoustic patterns",
    "abstract": "This paper presents an initial effort to retrieve semantically related spoken content in a completely unsupervised way. Unsupervised approaches of spoken content retrieval is attractive because the need for annotated data reasonably matched to the spoken content for training acoustic and language models can be bypassed. However, almost all such unsupervised approaches focus on spoken term detection, or returning the spoken segments containing the query, using either template matching techniques such as dynamic time warping (DTW) or model-based approaches. However, users usually prefer to retrieve all objects semantically related to the query, but not necessarily including the query terms. This paper proposes a different approach. We transcribe the spoken segments in the archive to be retrieved through into sequences of acoustic patterns automatically discovered in an unsupervised method. For an input query in spoken form, the top-N spoken segments from the archive obtained with the first-pass retrieval with DTW are taken as pseudo-relevant. The acoustic patterns frequently occurring in these segments are therefore considered as query-related and used for query expansion. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
    "metadata": {
      "title": "Towards unsupervised semantic retrieval of spoken content with query expansion based on automatically discovered acoustic patterns",
      "summary": "This paper presents an initial effort to retrieve semantically related spoken content in a completely unsupervised way. Unsupervised approaches of spoken content retrieval is attractive because the need for annotated data reasonably matched to the spoken content for training acoustic and language models can be bypassed. However, almost all such unsupervised approaches focus on spoken term detection, or returning the spoken segments containing the query, using either template matching techniques such as dynamic time warping (DTW) or model-based approaches. However, users usually prefer to retrieve all objects semantically related to the query, but not necessarily including the query terms. This paper proposes a different approach. We transcribe the spoken segments in the archive to be retrieved through into sequences of acoustic patterns automatically discovered in an unsupervised method. For an input query in spoken form, the top-N spoken segments from the archive obtained with the first-pass retrieval with DTW are taken as pseudo-relevant. The acoustic patterns frequently occurring in these segments are therefore considered as query-related and used for query expansion. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
      "abstract": "This paper presents an initial effort to retrieve semantically related spoken content in a completely unsupervised way. Unsupervised approaches of spoken content retrieval is attractive because the need for annotated data reasonably matched to the spoken content for training acoustic and language models can be bypassed. However, almost all such unsupervised approaches focus on spoken term detection, or returning the spoken segments containing the query, using either template matching techniques such as dynamic time warping (DTW) or model-based approaches. However, users usually prefer to retrieve all objects semantically related to the query, but not necessarily including the query terms. This paper proposes a different approach. We transcribe the spoken segments in the archive to be retrieved through into sequences of acoustic patterns automatically discovered in an unsupervised method. For an input query in spoken form, the top-N spoken segments from the archive obtained with the first-pass retrieval with DTW are taken as pseudo-relevant. The acoustic patterns frequently occurring in these segments are therefore considered as query-related and used for query expansion. Preliminary experiments performed on Mandarin broadcast news offered very encouraging results.",
      "doi": "https://doi.org/10.1109/asru.2013.6707729",
      "openalex_id": "https://openalex.org/W2014012135",
      "arxiv_id": "",
      "publication_date": "2013-12-01",
      "published": "2013-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes transcribing spoken content into sequences of acoustic patterns (discrete tokens) discovered unsupervisedly and using these patterns for query expansion, indicating a discrete audio tokenization approach as the core object of study.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes transcribing spoken content into sequences of acoustic patterns (discrete tokens) discovered unsupervisedly and using these patterns for query expansion, indicating a discrete audio tokenization approach as the core object of study.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses unsupervised spoken content retrieval using sequences of acoustic patterns automatically discovered, which implies a tokenization approach converting continuous audio into discrete units; however, it does not explicitly analyze or evaluate discrete audio tokenization as a core study focus beyond retrieval application, and it is unclear if these acoustic patterns meet the criteria for discrete tokens analogous to language-model token sequences.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study discusses unsupervised spoken content retrieval using sequences of acoustic patterns automatically discovered, which implies a tokenization approach converting continuous audio into discrete units; however, it does not explicitly analyze or evaluate discrete audio tokenization as a core study focus beyond retrieval application, and it is unclear if these acoustic patterns meet the criteria for discrete tokens analogous to language-model token sequences.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted that the paper uses unsupervisedly discovered ‘acoustic patterns’ (discrete tokens) in a spoken-content retrieval pipeline; JuniorNano favored inclusion (4) seeing tokenization as core, while JuniorMini was more cautious (3) because the work focuses on retrieval and does not explicitly analyze or evaluate the discrete-tokenization method itself. After reviewing the abstract against the inclusion/exclusion criteria, this study appears to use discrete acoustic patterns as an intermediate representation but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the primary methodological contribution (no evidence of token-design, quantization analysis, or token-focused experiments for reconstruction/generation). Therefore it does not meet the requirement that tokenization be a core object of study and should not be included.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted that the paper uses unsupervisedly discovered ‘acoustic patterns’ (discrete tokens) in a spoken-content retrieval pipeline; JuniorNano favored inclusion (4) seeing tokenization as core, while JuniorMini was more cautious (3) because the work focuses on retrieval and does not explicitly analyze or evaluate the discrete-tokenization method itself. After reviewing the abstract against the inclusion/exclusion criteria, this study appears to use discrete acoustic patterns as an intermediate representation but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the primary methodological contribution (no evidence of token-design, quantization analysis, or token-focused experiments for reconstruction/generation). Therefore it does not meet the requirement that tokenization be a core object of study and should not be included.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition",
    "abstract": "We propose a novel regularized adaptation technique for context dependent deep neural network hidden Markov models (CD-DNN-HMMs). The CD-DNN-HMM has a large output layer and many large hidden layers, each with thousands of neurons. The huge number of parameters in the CD-DNN-HMM makes adaptation a challenging task, esp. when the adaptation set is small. The technique developed in this paper adapts the model conservatively by forcing the senone distribution estimated from the adapted model to be close to that from the unadapted model. This constraint is realized by adding Kullback-Leibler divergence (KLD) regularization to the adaptation criterion. We show that applying this regularization is equivalent to changing the target distribution in the conventional backpropagation algorithm. Experiments on Xbox voice search, short message dictation, and Switchboard and lecture speech transcription tasks demonstrate that the proposed adaptation technique can provide 2%-30% relative error reduction against the already very strong speaker independent CD-DNN-HMM systems using different adaptation sets under both supervised and unsupervised adaptation setups.",
    "metadata": {
      "title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition",
      "summary": "We propose a novel regularized adaptation technique for context dependent deep neural network hidden Markov models (CD-DNN-HMMs). The CD-DNN-HMM has a large output layer and many large hidden layers, each with thousands of neurons. The huge number of parameters in the CD-DNN-HMM makes adaptation a challenging task, esp. when the adaptation set is small. The technique developed in this paper adapts the model conservatively by forcing the senone distribution estimated from the adapted model to be close to that from the unadapted model. This constraint is realized by adding Kullback-Leibler divergence (KLD) regularization to the adaptation criterion. We show that applying this regularization is equivalent to changing the target distribution in the conventional backpropagation algorithm. Experiments on Xbox voice search, short message dictation, and Switchboard and lecture speech transcription tasks demonstrate that the proposed adaptation technique can provide 2%-30% relative error reduction against the already very strong speaker independent CD-DNN-HMM systems using different adaptation sets under both supervised and unsupervised adaptation setups.",
      "abstract": "We propose a novel regularized adaptation technique for context dependent deep neural network hidden Markov models (CD-DNN-HMMs). The CD-DNN-HMM has a large output layer and many large hidden layers, each with thousands of neurons. The huge number of parameters in the CD-DNN-HMM makes adaptation a challenging task, esp. when the adaptation set is small. The technique developed in this paper adapts the model conservatively by forcing the senone distribution estimated from the adapted model to be close to that from the unadapted model. This constraint is realized by adding Kullback-Leibler divergence (KLD) regularization to the adaptation criterion. We show that applying this regularization is equivalent to changing the target distribution in the conventional backpropagation algorithm. Experiments on Xbox voice search, short message dictation, and Switchboard and lecture speech transcription tasks demonstrate that the proposed adaptation technique can provide 2%-30% relative error reduction against the already very strong speaker independent CD-DNN-HMM systems using different adaptation sets under both supervised and unsupervised adaptation setups.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639201",
      "openalex_id": "https://openalex.org/W1989549063",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses KL-divergence regularized adaptation for CD-DNN-HMM speech recognition, focusing on model adaptation rather than discrete audio/tokenization of the input into a finite set of tokens; there is no explicit analysis or evaluation of discrete audio tokenization as a core object, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses KL-divergence regularized adaptation for CD-DNN-HMM speech recognition, focusing on model adaptation rather than discrete audio/tokenization of the input into a finite set of tokens; there is no explicit analysis or evaluation of discrete audio tokenization as a core object, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a regularized adaptation technique for deep neural networks in speech recognition, emphasizing model adaptation using continuous representations and does not propose, analyze, or evaluate discrete audio tokenization where continuous audio is converted into discrete symbols. Therefore, it does not meet the inclusion criteria regarding discrete tokenization and uses continuous audio features without discrete quantization design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a regularized adaptation technique for deep neural networks in speech recognition, emphasizing model adaptation using continuous representations and does not propose, analyze, or evaluate discrete audio tokenization where continuous audio is converted into discrete symbols. Therefore, it does not meet the inclusion criteria regarding discrete tokenization and uses continuous audio features without discrete quantization design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Factorized adaptation for deep neural network",
    "abstract": "In this paper, we propose a novel method to adapt context-dependent deep neural network hidden Markov model (CD-DNN-HMM) with only limited number of parameters by taking into account the underlying factors that contribute to the distorted speech signal. We derive this factorized adaptation method from the perspectives of joint factor analysis and vector Taylor series expansion, respectively. Evaluated on Aurora 4, the proposed method can get 19.0% and 10.6% relative word error rate reduction on test set B and D with only 20 adaptation utterances, and can have decent improvement with as few as two adaptation utterances. We also show that the proposed method is better than feature discriminative linear regression (fDLR), an existing DNN adaptation method. Its small number of parameters and short training time offer an attractive solution to low-footprint speech applications.",
    "metadata": {
      "title": "Factorized adaptation for deep neural network",
      "summary": "In this paper, we propose a novel method to adapt context-dependent deep neural network hidden Markov model (CD-DNN-HMM) with only limited number of parameters by taking into account the underlying factors that contribute to the distorted speech signal. We derive this factorized adaptation method from the perspectives of joint factor analysis and vector Taylor series expansion, respectively. Evaluated on Aurora 4, the proposed method can get 19.0% and 10.6% relative word error rate reduction on test set B and D with only 20 adaptation utterances, and can have decent improvement with as few as two adaptation utterances. We also show that the proposed method is better than feature discriminative linear regression (fDLR), an existing DNN adaptation method. Its small number of parameters and short training time offer an attractive solution to low-footprint speech applications.",
      "abstract": "In this paper, we propose a novel method to adapt context-dependent deep neural network hidden Markov model (CD-DNN-HMM) with only limited number of parameters by taking into account the underlying factors that contribute to the distorted speech signal. We derive this factorized adaptation method from the perspectives of joint factor analysis and vector Taylor series expansion, respectively. Evaluated on Aurora 4, the proposed method can get 19.0% and 10.6% relative word error rate reduction on test set B and D with only 20 adaptation utterances, and can have decent improvement with as few as two adaptation utterances. We also show that the proposed method is better than feature discriminative linear regression (fDLR), an existing DNN adaptation method. Its small number of parameters and short training time offer an attractive solution to low-footprint speech applications.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854662",
      "openalex_id": "https://openalex.org/W2117239706",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a DNN-HMM speech adaptation method without any discussion of discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria and should be excluded per the stated criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a DNN-HMM speech adaptation method without any discussion of discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria and should be excluded per the stated criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adaptation methods for deep neural network acoustic models in speech recognition and reports on word error rate improvements, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete symbolic sequences, which is a core inclusion criterion; thus, it does not meet the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adaptation methods for deep neural network acoustic models in speech recognition and reports on word error rate improvements, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete symbolic sequences, which is a core inclusion criterion; thus, it does not meet the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speaker adaptation of neural network acoustic models using i-vectors",
    "abstract": "We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.",
    "metadata": {
      "title": "Speaker adaptation of neural network acoustic models using i-vectors",
      "summary": "We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.",
      "abstract": "We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.",
      "doi": "https://doi.org/10.1109/asru.2013.6707705",
      "openalex_id": "https://openalex.org/W2079623482",
      "arxiv_id": "",
      "publication_date": "2013-12-01",
      "published": "2013-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work discusses speaker adaptation using i-vectors with continuous acoustic features and does not involve discrete audio tokenization or finite-vocabulary token representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work discusses speaker adaptation using i-vectors with continuous acoustic features and does not involve discrete audio tokenization or finite-vocabulary token representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker adaptation in DNN acoustic models using continuous speaker identity vectors (i-vectors) concatenated with acoustic features, without proposing or analyzing discrete audio/speech tokenization methods; it utilizes continuous feature representations and does not involve discrete quantization or token design for audio reconstruction or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker adaptation in DNN acoustic models using continuous speaker identity vectors (i-vectors) concatenated with acoustic features, without proposing or analyzing discrete audio/speech tokenization methods; it utilizes continuous feature representations and does not involve discrete quantization or token design for audio reconstruction or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code",
    "abstract": "In this paper, we propose a new fast speaker adaptation method for the hybrid NN-HMM speech recognition model. The adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes (one per speaker). The joint training method uses all training data along with speaker labels to update adaptation NN weights and speaker codes based on the standard back-propagation algorithm. In this way, the learned adaptation NN is capable of transforming each speaker features into a generic speaker-independent feature space when a small speaker code is given. Adaptation to a new speaker can be simply done by learning a new speaker code using the same back-propagation algorithm without changing any NN weights. In this method, a separate speaker code is learned for each speaker while the large adaptation NN is learned from the whole training set. The main advantage of this method is that the size of speaker codes is very small. As a result, it is possible to conduct a very fast adaptation of the hybrid NN/HMM model for each speaker based on only a small amount of adaptation data (i.e., just a few utterances). Experimental results on TIMIT have shown that it can achieve over 10% relative reduction in phone error rate by using only seven utterances for adaptation.",
    "metadata": {
      "title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code",
      "summary": "In this paper, we propose a new fast speaker adaptation method for the hybrid NN-HMM speech recognition model. The adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes (one per speaker). The joint training method uses all training data along with speaker labels to update adaptation NN weights and speaker codes based on the standard back-propagation algorithm. In this way, the learned adaptation NN is capable of transforming each speaker features into a generic speaker-independent feature space when a small speaker code is given. Adaptation to a new speaker can be simply done by learning a new speaker code using the same back-propagation algorithm without changing any NN weights. In this method, a separate speaker code is learned for each speaker while the large adaptation NN is learned from the whole training set. The main advantage of this method is that the size of speaker codes is very small. As a result, it is possible to conduct a very fast adaptation of the hybrid NN/HMM model for each speaker based on only a small amount of adaptation data (i.e., just a few utterances). Experimental results on TIMIT have shown that it can achieve over 10% relative reduction in phone error rate by using only seven utterances for adaptation.",
      "abstract": "In this paper, we propose a new fast speaker adaptation method for the hybrid NN-HMM speech recognition model. The adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes (one per speaker). The joint training method uses all training data along with speaker labels to update adaptation NN weights and speaker codes based on the standard back-propagation algorithm. In this way, the learned adaptation NN is capable of transforming each speaker features into a generic speaker-independent feature space when a small speaker code is given. Adaptation to a new speaker can be simply done by learning a new speaker code using the same back-propagation algorithm without changing any NN weights. In this method, a separate speaker code is learned for each speaker while the large adaptation NN is learned from the whole training set. The main advantage of this method is that the size of speaker codes is very small. As a result, it is possible to conduct a very fast adaptation of the hybrid NN/HMM model for each speaker based on only a small amount of adaptation data (i.e., just a few utterances). Experimental results on TIMIT have shown that it can achieve over 10% relative reduction in phone error rate by using only seven utterances for adaptation.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639211",
      "openalex_id": "https://openalex.org/W1993409002",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on fast speaker adaptation for NN/HMM speech recognition and does not discuss discrete audio tokenization into a finite vocabulary.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on fast speaker adaptation for NN/HMM speech recognition and does not discuss discrete audio tokenization into a finite vocabulary.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker adaptation in a hybrid NN-HMM speech recognition model using speaker codes within a neural network adaptation framework. However, it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it mainly deals with continuous features and adaptation without quantization or discrete token design. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and falls under the exclusion criteria regarding the absence of discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker adaptation in a hybrid NN-HMM speech recognition model using speaker codes within a neural network adaptation framework. However, it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it mainly deals with continuous features and adaptation without quantization or discrete token design. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and falls under the exclusion criteria regarding the absence of discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sequence summarizing neural network for speaker adaptation",
    "abstract": "In this paper, we propose a DNN adaptation technique, where the i-vector extractor is replaced by a Sequence Summarizing Neural Network (SSNN). Similarly to i-vector extractor, the SSNN produces a \"summary vector\", representing an acoustic summary of an utterance. Such vector is then appended to the input of main network, while both networks are trained together optimizing single loss function. Both the i-vector and SSNN speaker adaptation methods are compared on AMI meeting data. The results show comparable performance of both techniques on FBANK system with frame-classification training. Moreover, appending both the i-vector and \"summary vector\" to the FBANK features leads to additional improvement comparable to the performance of FMLLR adapted DNN system.",
    "metadata": {
      "title": "Sequence summarizing neural network for speaker adaptation",
      "summary": "In this paper, we propose a DNN adaptation technique, where the i-vector extractor is replaced by a Sequence Summarizing Neural Network (SSNN). Similarly to i-vector extractor, the SSNN produces a \"summary vector\", representing an acoustic summary of an utterance. Such vector is then appended to the input of main network, while both networks are trained together optimizing single loss function. Both the i-vector and SSNN speaker adaptation methods are compared on AMI meeting data. The results show comparable performance of both techniques on FBANK system with frame-classification training. Moreover, appending both the i-vector and \"summary vector\" to the FBANK features leads to additional improvement comparable to the performance of FMLLR adapted DNN system.",
      "abstract": "In this paper, we propose a DNN adaptation technique, where the i-vector extractor is replaced by a Sequence Summarizing Neural Network (SSNN). Similarly to i-vector extractor, the SSNN produces a \"summary vector\", representing an acoustic summary of an utterance. Such vector is then appended to the input of main network, while both networks are trained together optimizing single loss function. Both the i-vector and SSNN speaker adaptation methods are compared on AMI meeting data. The results show comparable performance of both techniques on FBANK system with frame-classification training. Moreover, appending both the i-vector and \"summary vector\" to the FBANK features leads to additional improvement comparable to the performance of FMLLR adapted DNN system.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472692",
      "openalex_id": "https://openalex.org/W2398776621",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses a neural network-based speaker adaptation technique using continuous acoustic features and a summarization vector, with no discrete audio tokenization or token sequence analysis presented, so it does not meet the discrete tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses a neural network-based speaker adaptation technique using continuous acoustic features and a summarization vector, with no discrete audio tokenization or token sequence analysis presented, so it does not meet the discrete tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker adaptation using a Sequence Summarizing Neural Network to generate continuous summary vectors appended to DNN inputs; there is no indication of discrete audio/speech tokenization or quantized token representations being proposed, analyzed, or evaluated, thus failing the inclusion criteria that require discrete tokenization as the core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker adaptation using a Sequence Summarizing Neural Network to generate continuous summary vectors appended to DNN inputs; there is no indication of discrete audio/speech tokenization or quantized token representations being proposed, analyzed, or evaluated, thus failing the inclusion criteria that require discrete tokenization as the core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Direct adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code",
    "abstract": "Recently an effective fast speaker adaptation method using discriminative speaker code (SC) has been proposed for the hybrid DNN-HMM models in speech recognition [1]. This adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes using the standard back-propagation algorithm. In this paper, we propose an alternative direct adaptation in model space, where speaker codes are directly connected to the original DNN models through a set of new connection weights, which can be estimated very efficiently from all or part of training data. As a result, the proposed method is more suitable for large scale speech recognition tasks since it eliminates the time-consuming training process to estimate another adaptation neural networks. In this work, we have evaluated the proposed direct SC-based adaptation method in the large scale 320-hr Switchboard task. Experimental results have shown that the proposed SC-based rapid adaptation method is very effective not only for small recognition tasks but also for very large scale tasks. For example, it has shown that the proposed method leads to up to 8% relative reduction in word error rate in Switchboard by using only a very small number of adaptation utterances per speaker (from 10 to a few dozens). Moreover, the extra training time required for adaptation is also significantly reduced from the method in [1].",
    "metadata": {
      "title": "Direct adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code",
      "summary": "Recently an effective fast speaker adaptation method using discriminative speaker code (SC) has been proposed for the hybrid DNN-HMM models in speech recognition [1]. This adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes using the standard back-propagation algorithm. In this paper, we propose an alternative direct adaptation in model space, where speaker codes are directly connected to the original DNN models through a set of new connection weights, which can be estimated very efficiently from all or part of training data. As a result, the proposed method is more suitable for large scale speech recognition tasks since it eliminates the time-consuming training process to estimate another adaptation neural networks. In this work, we have evaluated the proposed direct SC-based adaptation method in the large scale 320-hr Switchboard task. Experimental results have shown that the proposed SC-based rapid adaptation method is very effective not only for small recognition tasks but also for very large scale tasks. For example, it has shown that the proposed method leads to up to 8% relative reduction in word error rate in Switchboard by using only a very small number of adaptation utterances per speaker (from 10 to a few dozens). Moreover, the extra training time required for adaptation is also significantly reduced from the method in [1].",
      "abstract": "Recently an effective fast speaker adaptation method using discriminative speaker code (SC) has been proposed for the hybrid DNN-HMM models in speech recognition [1]. This adaptation method depends on a joint learning of a large generic adaptation neural network for all speakers as well as multiple small speaker codes using the standard back-propagation algorithm. In this paper, we propose an alternative direct adaptation in model space, where speaker codes are directly connected to the original DNN models through a set of new connection weights, which can be estimated very efficiently from all or part of training data. As a result, the proposed method is more suitable for large scale speech recognition tasks since it eliminates the time-consuming training process to estimate another adaptation neural networks. In this work, we have evaluated the proposed direct SC-based adaptation method in the large scale 320-hr Switchboard task. Experimental results have shown that the proposed SC-based rapid adaptation method is very effective not only for small recognition tasks but also for very large scale tasks. For example, it has shown that the proposed method leads to up to 8% relative reduction in word error rate in Switchboard by using only a very small number of adaptation utterances per speaker (from 10 to a few dozens). Moreover, the extra training time required for adaptation is also significantly reduced from the method in [1].",
      "doi": "https://doi.org/10.1109/icassp.2014.6854824",
      "openalex_id": "https://openalex.org/W2090320273",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on direct model-space adaptation for DNN/HMM-based ASR and speaker codes, with no discussion of converting audio into discrete tokens or discrete tokenization analyses, thus it does not meet the inclusion criterion for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on direct model-space adaptation for DNN/HMM-based ASR and speaker codes, with no discussion of converting audio into discrete tokens or discrete tokenization analyses, thus it does not meet the inclusion criterion for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fast speaker adaptation techniques for hybrid DNN/HMM models in speech recognition, specifically leveraging speaker codes and direct model adaptation; however, it does not involve the proposal, analysis, or evaluation of discrete audio/speech tokenization, nor does it discuss converting continuous audio signals into sequences of discrete tokens, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fast speaker adaptation techniques for hybrid DNN/HMM models in speech recognition, specifically leveraging speaker codes and direct model adaptation; however, it does not involve the proposal, analysis, or evaluation of discrete audio/speech tokenization, nor does it discuss converting continuous audio signals into sequences of discrete tokens, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Two-Stage Speaker Adaptation of Hybrid Tied-Posterior Acoustic Models",
    "abstract": "In this paper, strategies are explored to adapt hybrid neural network/HMM systems based on the tied-posterior paradigm. We investigate the retraining of selected important parts of the neural network and a gradient based adaptation strategy for the HMM mixture coefficients based on maximizing the scaled likelihood. The paper presents the following innovations: first, it introduces one of the first adaptation methods for hybrid systems where the HMM component contributes significantly to the adaptation success; second, it presents a novel approach to the neural network's adaptation, based on the selection of suitable neurons for adaptation. Results on the WSJ speaker adaptation test show the capability of our methods to adapt to new speakers, especially in the case of adapting the neural net, and that both methods can be combined to achieve additional improvement of the word error rate in most cases.",
    "metadata": {
      "title": "Two-Stage Speaker Adaptation of Hybrid Tied-Posterior Acoustic Models",
      "summary": "In this paper, strategies are explored to adapt hybrid neural network/HMM systems based on the tied-posterior paradigm. We investigate the retraining of selected important parts of the neural network and a gradient based adaptation strategy for the HMM mixture coefficients based on maximizing the scaled likelihood. The paper presents the following innovations: first, it introduces one of the first adaptation methods for hybrid systems where the HMM component contributes significantly to the adaptation success; second, it presents a novel approach to the neural network's adaptation, based on the selection of suitable neurons for adaptation. Results on the WSJ speaker adaptation test show the capability of our methods to adapt to new speakers, especially in the case of adapting the neural net, and that both methods can be combined to achieve additional improvement of the word error rate in most cases.",
      "abstract": "In this paper, strategies are explored to adapt hybrid neural network/HMM systems based on the tied-posterior paradigm. We investigate the retraining of selected important parts of the neural network and a gradient based adaptation strategy for the HMM mixture coefficients based on maximizing the scaled likelihood. The paper presents the following innovations: first, it introduces one of the first adaptation methods for hybrid systems where the HMM component contributes significantly to the adaptation success; second, it presents a novel approach to the neural network's adaptation, based on the selection of suitable neurons for adaptation. Results on the WSJ speaker adaptation test show the capability of our methods to adapt to new speakers, especially in the case of adapting the neural net, and that both methods can be combined to achieve additional improvement of the word error rate in most cases.",
      "doi": "https://doi.org/10.1109/icassp.2005.1415279",
      "openalex_id": "https://openalex.org/W2117824967",
      "arxiv_id": "",
      "publication_date": "2006-10-11",
      "published": "2006-10-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on adapting hybrid NN/HMM acoustic models and does not treat discrete audio tokenization (mapping continuous audio to a finite vocabulary of tokens) as the core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on adapting hybrid NN/HMM acoustic models and does not treat discrete audio tokenization (mapping continuous audio to a finite vocabulary of tokens) as the core object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker adaptation for hybrid neural network/HMM acoustic models but does not explicitly address discrete audio/speech tokenization where continuous audio is converted into sequences of discrete symbols; instead, it deals mainly with adaptation techniques on continuous model parameters without proposing or analyzing discrete tokenization approaches.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker adaptation for hybrid neural network/HMM acoustic models but does not explicitly address discrete audio/speech tokenization where continuous audio is converted into sequences of discrete symbols; instead, it deals mainly with adaptation techniques on continuous model parameters without proposing or analyzing discrete tokenization approaches.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Data-selective transfer learning for multi-domain speech recognition",
    "abstract": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.",
    "metadata": {
      "title": "Data-selective transfer learning for multi-domain speech recognition",
      "summary": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.",
      "abstract": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.",
      "doi": "https://doi.org/10.21437/interspeech.2015-609",
      "openalex_id": "https://openalex.org/W2228017765",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses data selection for acoustic model training and evaluates improvements using PLP and DNN features; it does not involve discrete audio tokenization or token-based representations, thus fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses data selection for acoustic model training and evaluates improvements using PLP and DNN features; it does not involve discrete audio tokenization or token-based representations, thus fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data selection for acoustic model training in speech recognition using continuous acoustic features (likelihood ratios on acoustic similarity) without proposing or analyzing discrete audio tokenization or discrete symbol sequences; hence, it does not meet the inclusion criteria that require discrete audio/speech tokenization as a core focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data selection for acoustic model training in speech recognition using continuous acoustic features (likelihood ratios on acoustic similarity) without proposing or analyzing discrete audio tokenization or discrete symbol sequences; hence, it does not meet the inclusion criteria that require discrete audio/speech tokenization as a core focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data selection from multiple ASR systems' hypotheses for unsupervised acoustic model training",
    "abstract": "This paper addresses unsupervised training of DNN acoustic model, by exploiting a large amount of unlabeled data with CRF-based classifiers. In the proposed scheme, we obtain ASR hypotheses by complementary GMM and DNN based ASR systems. Then, a set of dedicated classifiers are designed and trained to select the better hypothesis and verify the selected data. It is demonstrated that the classifiers can effectively filter usable data from unlabeled data for acoustic model training. The proposed method achieved significant improvement in the ASR accuracy from the baseline system, and it outperformed the models trained from the data selected based on the confidence measure scores (CMS) and also from the simple ROVER-based system combination.",
    "metadata": {
      "title": "Data selection from multiple ASR systems' hypotheses for unsupervised acoustic model training",
      "summary": "This paper addresses unsupervised training of DNN acoustic model, by exploiting a large amount of unlabeled data with CRF-based classifiers. In the proposed scheme, we obtain ASR hypotheses by complementary GMM and DNN based ASR systems. Then, a set of dedicated classifiers are designed and trained to select the better hypothesis and verify the selected data. It is demonstrated that the classifiers can effectively filter usable data from unlabeled data for acoustic model training. The proposed method achieved significant improvement in the ASR accuracy from the baseline system, and it outperformed the models trained from the data selected based on the confidence measure scores (CMS) and also from the simple ROVER-based system combination.",
      "abstract": "This paper addresses unsupervised training of DNN acoustic model, by exploiting a large amount of unlabeled data with CRF-based classifiers. In the proposed scheme, we obtain ASR hypotheses by complementary GMM and DNN based ASR systems. Then, a set of dedicated classifiers are designed and trained to select the better hypothesis and verify the selected data. It is demonstrated that the classifiers can effectively filter usable data from unlabeled data for acoustic model training. The proposed method achieved significant improvement in the ASR accuracy from the baseline system, and it outperformed the models trained from the data selected based on the confidence measure scores (CMS) and also from the simple ROVER-based system combination.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472804",
      "openalex_id": "https://openalex.org/W2408234571",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on selecting hypotheses from ASR systems for unsupervised acoustic model training and data filtering, not on proposing or analyzing discrete audio tokenization into a finite vocabulary as the core object.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on selecting hypotheses from ASR systems for unsupervised acoustic model training and data filtering, not on proposing or analyzing discrete audio tokenization into a finite vocabulary as the core object.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic model training by selecting better ASR hypotheses from multiple systems, without explicitly proposing, analyzing, or evaluating discrete audio tokenization or discrete symbol sequences from continuous audio signals, which is the primary inclusion criterion; thus, it does not meet the requirement of focusing on discrete audio/speech tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic model training by selecting better ASR hypotheses from multiple systems, without explicitly proposing, analyzing, or evaluating discrete audio tokenization or discrete symbol sequences from continuous audio signals, which is the primary inclusion criterion; thus, it does not meet the requirement of focusing on discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity",
    "abstract": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.",
    "metadata": {
      "title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity",
      "summary": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.",
      "abstract": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.",
      "doi": "https://doi.org/10.1109/icassp.2014.6855121",
      "openalex_id": "https://openalex.org/W2105016867",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly centers on discrete audio tokenization by discovering and using sequences of acoustic patterns (tokenized representations) for unsupervised spoken term detection, satisfying the inclusion criteria and not falling into purely continuous features or non-speech token discussions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly centers on discrete audio tokenization by discovering and using sequences of acoustic patterns (tokenized representations) for unsupervised spoken term detection, satisfying the inclusion criteria and not falling into purely continuous features or non-speech token discussions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised spoken term detection using multiple sets of acoustic patterns, which are discrete sequences representing the spoken content; this aligns with discrete audio tokenization as it converts continuous speech signals into sequences of acoustic tokens for understanding tasks. Moreover, the study includes methodological contributions and experimental evaluation showing performance improvements over baselines. Therefore, it meets the inclusion criteria and does not violate any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised spoken term detection using multiple sets of acoustic patterns, which are discrete sequences representing the spoken content; this aligns with discrete audio tokenization as it converts continuous speech signals into sequences of acoustic tokens for understanding tasks. Moreover, the study includes methodological contributions and experimental evaluation showing performance improvements over baselines. Therefore, it meets the inclusion criteria and does not violate any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization",
    "abstract": "Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subword-like and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems.",
    "metadata": {
      "title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization",
      "summary": "Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subword-like and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems.",
      "abstract": "Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subword-like and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639239",
      "openalex_id": "https://openalex.org/W2116422968",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly targets discrete acoustic tokenization by learning subword-like and word-like patterns (HMMs) and a lexicon, enabling token sequences for ASR-like tasks, thus meeting the inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly targets discrete acoustic tokenization by learning subword-like and word-like patterns (HMMs) and a lexicon, enabling token sequences for ASR-like tasks, thus meeting the inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on unsupervised discovery of linguistic structure from raw speech data by converting continuous acoustic signals into discrete two-level patterns (subword-like and word-like), effectively performing discrete audio tokenization for speech understanding tasks, and experimentally validates this approach in Mandarin broadcast news data; therefore, it meets all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on unsupervised discovery of linguistic structure from raw speech data by converting continuous acoustic signals into discrete two-level patterns (subword-like and word-like), effectively performing discrete audio tokenization for speech understanding tasks, and experimentally validates this approach in Mandarin broadcast news data; therefore, it meets all inclusion criteria and none of the exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards unsupervised training of speaker independent acoustic models",
    "abstract": "Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.",
    "metadata": {
      "title": "Towards unsupervised training of speaker independent acoustic models",
      "summary": "Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.",
      "abstract": "Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.",
      "doi": "https://doi.org/10.21437/interspeech.2011-184",
      "openalex_id": "https://openalex.org/W2401464865",
      "arxiv_id": "",
      "publication_date": "2011-08-27",
      "published": "2011-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly studies unsupervised discovery of speaker-independent subword units and treats the discovered subword sequences as discrete tokens for acoustic modeling, satisfying the core discrete tokenization criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly studies unsupervised discovery of speaker-independent subword units and treats the discovered subword sequences as discrete tokens for acoustic modeling, satisfying the core discrete tokenization criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discovering phonemelike subword units and training acoustic models using discrete sequence representations (whole-word HMMs with subword units) inferred from speech, aligning well with discrete audio tokenization for speech understanding, thus meeting inclusion criteria without falling into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discovering phonemelike subword units and training acoustic models using discrete sequence representations (whole-word HMMs with subword units) inferred from speech, aligning well with discrete audio tokenization for speech understanding, thus meeting inclusion criteria without falling into any exclusion category.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Pattern Discovery in Speech",
    "abstract": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
    "metadata": {
      "title": "Unsupervised Pattern Discovery in Speech",
      "summary": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "abstract": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "doi": "https://doi.org/10.1109/tasl.2007.909282",
      "openalex_id": "https://openalex.org/W2114347655",
      "arxiv_id": "",
      "publication_date": "2007-12-20",
      "published": "2007-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract clearly targets unsupervised discovery of discrete lexical tokens from speech (words/short phrases) and evaluates the resulting lexical identities, fitting the inclusion criteria for discrete tokenization; not merely continuous features or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract clearly targets unsupervised discovery of discrete lexical tokens from speech (words/short phrases) and evaluates the resulting lexical identities, fitting the inclusion criteria for discrete tokenization; not merely continuous features or non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discovering lexical units from speech in an unsupervised manner, clustering acoustic sequences into discrete groups corresponding to words and phrases, effectively treating speech segmentation and lexical entity extraction as discrete tokenization processes; it involves analysis and validation on academic lecture audio, meeting the inclusion criteria of proposing and evaluating discrete audio tokenization, and does not fall under any exclusion criteria such as solely using continuous features without discrete tokenization or lacking methodological contribution.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discovering lexical units from speech in an unsupervised manner, clustering acoustic sequences into discrete groups corresponding to words and phrases, effectively treating speech segmentation and lexical entity extraction as discrete tokenization processes; it involves analysis and validation on academic lecture audio, meeting the inclusion criteria of proposing and evaluating discrete audio tokenization, and does not fall under any exclusion criteria such as solely using continuous features without discrete tokenization or lacking methodological contribution.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards spoken term discovery at scale with zero resources",
    "abstract": "The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots",
    "metadata": {
      "title": "Towards spoken term discovery at scale with zero resources",
      "summary": "The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots",
      "abstract": "The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots",
      "doi": "https://doi.org/10.21437/interspeech.2010-483",
      "openalex_id": "https://openalex.org/W30845872",
      "arxiv_id": "",
      "publication_date": "2010-09-26",
      "published": "2010-09-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes zero-resource spoken term discovery and pattern-based retrieval without first emitting discrete tokenized representations; there is no explicit proposal/analysis of converting continuous audio into discrete symbols/vocabulary, so it does not meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes zero-resource spoken term discovery and pattern-based retrieval without first emitting discrete tokenized representations; there is no explicit proposal/analysis of converting continuous audio into discrete symbols/vocabulary, so it does not meet the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study aims at spoken term discovery from continuous speech without using training data or dictionaries, focusing on finding repeated spoken terms through an algorithm exploiting sparse feature representations, but it does not explicitly propose or analyze discrete audio tokenization converting continuous audio into finite discrete symbols for reconstruction or generation. Thus, the core topic of discrete tokenization as defined in the inclusion criteria is not clearly met.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study aims at spoken term discovery from continuous speech without using training data or dictionaries, focusing on finding repeated spoken terms through an algorithm exploiting sparse feature representations, but it does not explicitly propose or analyze discrete audio tokenization converting continuous audio into finite discrete symbols for reconstruction or generation. Thus, the core topic of discrete tokenization as defined in the inclusion criteria is not clearly met.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Regularized Adaptation of Discriminative Classifiers",
    "abstract": "We introduce a novel method for adapting discriminative classifiers (multi-layer perceptrons (MLPs) and support vector machines (SVMs)). Ourmethod is based on the idea of regularization, whereby an optimization cost criterion to be minimized includes a penalty in accordance to how \"complex\" the system is. Specifically, our regularization term penalizes depending on how different an adapted system is from an unadapted system, thus avoiding the problem of overtraining when only a small amount of adaptation data is available. We justify this approach using a max-margin argument. We apply this technique to MLPs and produce a working real-time System for rapid adaptation of vowel classifiers in the context of the Vocal Joystick project. Overall, we find that our method outperforms all other MLP-based adaptation methods we are aware of. Our technique, however, is quite general and can be used whenever rapid adaptation of MLP or SVM classifiers are needed (e.g., from a speaker-independent to a speaker-dependent classifier in a hybrid MLP/HMM or SVM/HMM speech-recognition system).",
    "metadata": {
      "title": "Regularized Adaptation of Discriminative Classifiers",
      "summary": "We introduce a novel method for adapting discriminative classifiers (multi-layer perceptrons (MLPs) and support vector machines (SVMs)). Ourmethod is based on the idea of regularization, whereby an optimization cost criterion to be minimized includes a penalty in accordance to how \"complex\" the system is. Specifically, our regularization term penalizes depending on how different an adapted system is from an unadapted system, thus avoiding the problem of overtraining when only a small amount of adaptation data is available. We justify this approach using a max-margin argument. We apply this technique to MLPs and produce a working real-time System for rapid adaptation of vowel classifiers in the context of the Vocal Joystick project. Overall, we find that our method outperforms all other MLP-based adaptation methods we are aware of. Our technique, however, is quite general and can be used whenever rapid adaptation of MLP or SVM classifiers are needed (e.g., from a speaker-independent to a speaker-dependent classifier in a hybrid MLP/HMM or SVM/HMM speech-recognition system).",
      "abstract": "We introduce a novel method for adapting discriminative classifiers (multi-layer perceptrons (MLPs) and support vector machines (SVMs)). Ourmethod is based on the idea of regularization, whereby an optimization cost criterion to be minimized includes a penalty in accordance to how \"complex\" the system is. Specifically, our regularization term penalizes depending on how different an adapted system is from an unadapted system, thus avoiding the problem of overtraining when only a small amount of adaptation data is available. We justify this approach using a max-margin argument. We apply this technique to MLPs and produce a working real-time System for rapid adaptation of vowel classifiers in the context of the Vocal Joystick project. Overall, we find that our method outperforms all other MLP-based adaptation methods we are aware of. Our technique, however, is quite general and can be used whenever rapid adaptation of MLP or SVM classifiers are needed (e.g., from a speaker-independent to a speaker-dependent classifier in a hybrid MLP/HMM or SVM/HMM speech-recognition system).",
      "doi": "https://doi.org/10.1109/icassp.2006.1660001",
      "openalex_id": "https://openalex.org/W2162042984",
      "arxiv_id": "",
      "publication_date": "2006-08-03",
      "published": "2006-08-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokenization or token sequence design; focuses on classifier adaptation with (likely) continuous features rather than discrete tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokenization or token sequence design; focuses on classifier adaptation with (likely) continuous features rather than discrete tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adapting discriminative classifiers for speech tasks but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; it deals primarily with classifier adaptation rather than discrete token representations from continuous audio signals, thus not meeting the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adapting discriminative classifiers for speech tasks but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; it deals primarily with classifier adaptation rather than discrete token representations from continuous audio signals, thus not meeting the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A big data approach to acoustic model training corpus selection",
    "abstract": "Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.",
    "metadata": {
      "title": "A big data approach to acoustic model training corpus selection",
      "summary": "Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.",
      "abstract": "Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.",
      "doi": "https://doi.org/10.21437/interspeech.2014-473",
      "openalex_id": "https://openalex.org/W2294962864",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes data curation and transcript refinement for training acoustic models, not a study of discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes data curation and transcript refinement for training acoustic models, not a study of discrete audio tokenization or token-level representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on constructing large high-quality unsupervised training sets for DNN acoustic models in speech recognition, using confidence scores and heuristics to select data, but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete token design; instead, it mainly discusses improving training data and acoustic models without involving discrete token sequences or quantization-based representations as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on constructing large high-quality unsupervised training sets for DNN acoustic models in speech recognition, using confidence scores and heuristics to select data, but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete token design; instead, it mainly discusses improving training data and acoustic models without involving discrete token sequences or quantization-based representations as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Is Learning The n-th Thing Any Easier Than Learning The First?",
    "abstract": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.",
    "metadata": {
      "title": "Is Learning The n-th Thing Any Easier Than Learning The First?",
      "summary": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.",
      "abstract": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2133013156",
      "arxiv_id": "",
      "publication_date": "1995-11-27",
      "published": "1995-11-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on lifelong learning and transfer across tasks in an object recognition domain and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on lifelong learning and transfer across tasks in an object recognition domain and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on lifelong learning approaches applied to object recognition tasks and discusses knowledge transfer between tasks; there is no mention of discrete audio/speech tokenization, discrete symbol sequences from audio signals, or any relevant audio representation methods. It does not address the core inclusion criteria related to discrete audio tokenization nor the exclusion criteria concerning continuous features or non-relevant signal processing methods, but rather it is unrelated to the audio domain entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on lifelong learning approaches applied to object recognition tasks and discusses knowledge transfer between tasks; there is no mention of discrete audio/speech tokenization, discrete symbol sequences from audio signals, or any relevant audio representation methods. It does not address the core inclusion criteria related to discrete audio tokenization nor the exclusion criteria concerning continuous features or non-relevant signal processing methods, but rather it is unrelated to the audio domain entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The JPEG still picture compression standard",
    "abstract": "article Free Access Share on The JPEG still picture compression standard Author: Gregory K. Wallace Digital Equipment Corp., Maynard, MA Digital Equipment Corp., Maynard, MAView Profile Authors Info & Claims Communications of the ACMVolume 34Issue 4April 1991 pp 30–44https://doi.org/10.1145/103085.103089Published:01 April 1991Publication History 2,125citation8,762DownloadsMetricsTotal Citations2,125Total Downloads8,762Last 12 Months417Last 6 weeks101 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "metadata": {
      "title": "The JPEG still picture compression standard",
      "summary": "article Free Access Share on The JPEG still picture compression standard Author: Gregory K. Wallace Digital Equipment Corp., Maynard, MA Digital Equipment Corp., Maynard, MAView Profile Authors Info & Claims Communications of the ACMVolume 34Issue 4April 1991 pp 30–44https://doi.org/10.1145/103085.103089Published:01 April 1991Publication History 2,125citation8,762DownloadsMetricsTotal Citations2,125Total Downloads8,762Last 12 Months417Last 6 weeks101 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "abstract": "article Free Access Share on The JPEG still picture compression standard Author: Gregory K. Wallace Digital Equipment Corp., Maynard, MA Digital Equipment Corp., Maynard, MAView Profile Authors Info & Claims Communications of the ACMVolume 34Issue 4April 1991 pp 30–44https://doi.org/10.1145/103085.103089Published:01 April 1991Publication History 2,125citation8,762DownloadsMetricsTotal Citations2,125Total Downloads8,762Last 12 Months417Last 6 weeks101 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "doi": "https://doi.org/10.1145/103085.103089",
      "openalex_id": "https://openalex.org/W2040903332",
      "arxiv_id": "",
      "publication_date": "1991-04-01",
      "published": "1991-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses the JPEG still image compression standard and does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses the JPEG still image compression standard and does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article centers on the JPEG still picture compression standard, which pertains to image data compression rather than audio or speech tokenization; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization and falls under exclusion criteria focusing away from audio-based tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article centers on the JPEG still picture compression standard, which pertains to image data compression rather than audio or speech tokenization; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization and falls under exclusion criteria focusing away from audio-based tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Instability of Successive Deep Image Compression",
    "abstract": "Successive image compression refers to the process of repeated encoding and decoding of an image. It frequently occurs during sharing, manipulation, and re-distribution of images. While deep learning-based methods have made significant progress for single-step compression, thorough analysis of their performance under successive compression has not been conducted. In this paper, we conduct comprehensive analysis of successive deep image compression. First, we introduce a new observation, instability of successive deep image compression, which is not observed in JPEG, and discuss causes of the instability. Then, we conduct a successive image compression benchmark for the state-of-the-art deep learning-based methods, and analyze the factors that affect the instability in a comparative manner. Finally, we propose a new loss function for training deep compression models, called feature identity loss, to mitigate the instability of successive deep image compression.",
    "metadata": {
      "title": "Instability of Successive Deep Image Compression",
      "summary": "Successive image compression refers to the process of repeated encoding and decoding of an image. It frequently occurs during sharing, manipulation, and re-distribution of images. While deep learning-based methods have made significant progress for single-step compression, thorough analysis of their performance under successive compression has not been conducted. In this paper, we conduct comprehensive analysis of successive deep image compression. First, we introduce a new observation, instability of successive deep image compression, which is not observed in JPEG, and discuss causes of the instability. Then, we conduct a successive image compression benchmark for the state-of-the-art deep learning-based methods, and analyze the factors that affect the instability in a comparative manner. Finally, we propose a new loss function for training deep compression models, called feature identity loss, to mitigate the instability of successive deep image compression.",
      "abstract": "Successive image compression refers to the process of repeated encoding and decoding of an image. It frequently occurs during sharing, manipulation, and re-distribution of images. While deep learning-based methods have made significant progress for single-step compression, thorough analysis of their performance under successive compression has not been conducted. In this paper, we conduct comprehensive analysis of successive deep image compression. First, we introduce a new observation, instability of successive deep image compression, which is not observed in JPEG, and discuss causes of the instability. Then, we conduct a successive image compression benchmark for the state-of-the-art deep learning-based methods, and analyze the factors that affect the instability in a comparative manner. Finally, we propose a new loss function for training deep compression models, called feature identity loss, to mitigate the instability of successive deep image compression.",
      "doi": "https://doi.org/10.1145/3394171.3413680",
      "openalex_id": "https://openalex.org/W3093249525",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes image compression and instability in successive deep image compression, with no discrete audio/speech tokenization or token-based representations, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes image compression and instability in successive deep image compression, with no discrete audio/speech tokenization or token-based representations, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on successive deep image compression, analyzing the instability issues in repeated compression of images using deep learning methods. It does not address discrete audio/speech tokenization or any audio-related discrete tokenization concepts, nor does it meet any inclusion criteria related to audio tokenization. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on successive deep image compression, analyzing the instability issues in repeated compression of images using deep learning methods. It does not address discrete audio/speech tokenization or any audio-related discrete tokenization concepts, nor does it meet any inclusion criteria related to audio tokenization. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs",
    "abstract": "Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.",
    "metadata": {
      "title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs",
      "summary": "Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.",
      "abstract": "Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.",
      "doi": "https://doi.org/10.1109/icassp.2001.941023",
      "openalex_id": "https://openalex.org/W1552314771",
      "arxiv_id": "",
      "publication_date": "2002-11-13",
      "published": "2002-11-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes PESQ, a perceptual speech quality model; it does not propose, analyze, or evaluate discrete tokenization of speech into a finite vocabulary, and thus does not meet the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes PESQ, a perceptual speech quality model; it does not propose, analyze, or evaluate discrete tokenization of speech into a finite vocabulary, and thus does not meet the inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study primarily focuses on speech quality assessment methods and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; it discusses perceptual evaluation metrics for speech quality rather than discrete token design or analysis relevant to neural or language-model-style representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study primarily focuses on speech quality assessment methods and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; it discusses perceptual evaluation metrics for speech quality rather than discrete token design or analysis relevant to neural or language-model-style representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Librispeech: An ASR corpus based on public domain audio books",
    "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
    "metadata": {
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "summary": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178964",
      "openalex_id": "https://openalex.org/W1494198834",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper introduces a speech corpus and ASR results but does not analyze/discuss discrete audio tokenization or token-based representations, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a speech corpus and ASR results but does not analyze/discuss discrete audio tokenization or token-based representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces the LibriSpeech corpus and provides resources for speech recognition system training and evaluation, but it does not describe any discrete audio/speech tokenization method or analysis as a core contribution; it mainly provides a dataset and tools rather than methodological advancements in discrete tokenization relevant for audio reconstruction or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces the LibriSpeech corpus and provides resources for speech recognition system training and evaluation, but it does not describe any discrete audio/speech tokenization method or analysis as a core contribution; it mainly provides a dataset and tools rather than methodological advancements in discrete tokenization relevant for audio reconstruction or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The role of syllables in sign language production",
    "abstract": "The aim of the present study was to investigate the functional role of syllables in sign language and how the different phonological combinations influence sign production. Moreover, the influence of age of acquisition was evaluated. Deaf signers (native and non-native) of Catalan Signed Language (LSC) were asked in a picture-sign interference task to sign picture names while ignoring distractor-signs with which they shared two phonological parameters (out of three of the main sign parameters: Location, Movement, and Handshape). The results revealed a different impact of the three phonological combinations. While no effect was observed for the phonological combination Handshape-Location, the combination Handshape-Movement slowed down signing latencies, but only in the non-native group. A facilitatory effect was observed for both groups when pictures and distractors shared Location-Movement. Importantly, linguistic models have considered this phonological combination to be a privileged unit in the composition of signs, as syllables are in spoken languages. Thus, our results support the functional role of syllable units during phonological articulation in sign language production.",
    "metadata": {
      "title": "The role of syllables in sign language production",
      "summary": "The aim of the present study was to investigate the functional role of syllables in sign language and how the different phonological combinations influence sign production. Moreover, the influence of age of acquisition was evaluated. Deaf signers (native and non-native) of Catalan Signed Language (LSC) were asked in a picture-sign interference task to sign picture names while ignoring distractor-signs with which they shared two phonological parameters (out of three of the main sign parameters: Location, Movement, and Handshape). The results revealed a different impact of the three phonological combinations. While no effect was observed for the phonological combination Handshape-Location, the combination Handshape-Movement slowed down signing latencies, but only in the non-native group. A facilitatory effect was observed for both groups when pictures and distractors shared Location-Movement. Importantly, linguistic models have considered this phonological combination to be a privileged unit in the composition of signs, as syllables are in spoken languages. Thus, our results support the functional role of syllable units during phonological articulation in sign language production.",
      "abstract": "The aim of the present study was to investigate the functional role of syllables in sign language and how the different phonological combinations influence sign production. Moreover, the influence of age of acquisition was evaluated. Deaf signers (native and non-native) of Catalan Signed Language (LSC) were asked in a picture-sign interference task to sign picture names while ignoring distractor-signs with which they shared two phonological parameters (out of three of the main sign parameters: Location, Movement, and Handshape). The results revealed a different impact of the three phonological combinations. While no effect was observed for the phonological combination Handshape-Location, the combination Handshape-Movement slowed down signing latencies, but only in the non-native group. A facilitatory effect was observed for both groups when pictures and distractors shared Location-Movement. Importantly, linguistic models have considered this phonological combination to be a privileged unit in the composition of signs, as syllables are in spoken languages. Thus, our results support the functional role of syllable units during phonological articulation in sign language production.",
      "doi": "https://doi.org/10.3389/fpsyg.2014.01254",
      "openalex_id": "https://openalex.org/W1994683737",
      "arxiv_id": "",
      "publication_date": "2014-11-13",
      "published": "2014-11-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on sign-language phonology and syllable units with no discrete audio-tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on sign-language phonology and syllable units with no discrete audio-tokenization into a finite vocabulary for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the role of syllables in sign language production without dealing with audio/speech signal tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the role of syllables in sign language production without dealing with audio/speech signal tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Inflected words in production: Evidence for a morphologically rich lexicon",
    "abstract": "Current evidence suggests that there is a difference between the representations of multimorphemic words in production and perception. In perception, it is widely believed that both whole-word and root representations exist, while in production there is little evidence for whole-word representations. The present investigation demonstrates that whole-word and root frequency independently predict the duration of words suffixed with –ing, –ed, and –s, which reveals that both root and word representations play a role in the production of inflected English words. In a second line of analysis, we find that the number of inflected phonological neighbours independently predicts the duration of monomorphemic words, which extends these results and suggests that whole-word representations exist at the lexical level. Together these results suggest that both root and word representations of inflected words are stored in the lexicon and are relevant for the production of both monomorphemic and multimorphemic words.",
    "metadata": {
      "title": "Inflected words in production: Evidence for a morphologically rich lexicon",
      "summary": "Current evidence suggests that there is a difference between the representations of multimorphemic words in production and perception. In perception, it is widely believed that both whole-word and root representations exist, while in production there is little evidence for whole-word representations. The present investigation demonstrates that whole-word and root frequency independently predict the duration of words suffixed with –ing, –ed, and –s, which reveals that both root and word representations play a role in the production of inflected English words. In a second line of analysis, we find that the number of inflected phonological neighbours independently predicts the duration of monomorphemic words, which extends these results and suggests that whole-word representations exist at the lexical level. Together these results suggest that both root and word representations of inflected words are stored in the lexicon and are relevant for the production of both monomorphemic and multimorphemic words.",
      "abstract": "Current evidence suggests that there is a difference between the representations of multimorphemic words in production and perception. In perception, it is widely believed that both whole-word and root representations exist, while in production there is little evidence for whole-word representations. The present investigation demonstrates that whole-word and root frequency independently predict the duration of words suffixed with –ing, –ed, and –s, which reveals that both root and word representations play a role in the production of inflected English words. In a second line of analysis, we find that the number of inflected phonological neighbours independently predicts the duration of monomorphemic words, which extends these results and suggests that whole-word representations exist at the lexical level. Together these results suggest that both root and word representations of inflected words are stored in the lexicon and are relevant for the production of both monomorphemic and multimorphemic words.",
      "doi": "https://doi.org/10.1080/17470218.2015.1054847",
      "openalex_id": "https://openalex.org/W1606571944",
      "arxiv_id": "",
      "publication_date": "2015-05-27",
      "published": "2015-05-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns lexical representations in spoken production and word inflection; it does not analyze or propose discrete audio/tokenization schemes, so it fails the discrete tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns lexical representations in spoken production and word inflection; it does not analyze or propose discrete audio/tokenization schemes, so it fails the discrete tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on morphological representation and production of inflected words in English, analyzing root and whole-word frequency effects on word duration; however, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it pertain to neural encoding or quantization-based discrete token representations relevant to audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on morphological representation and production of inflected words in English, analyzing root and whole-word frequency effects on word duration; however, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it pertain to neural encoding or quantization-based discrete token representations relevant to audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical access in sign language: a computational model",
    "abstract": "PSYCHOLINGUISTIC THEORIES HAVE PREDOMINANTLY BEEN BUILT UPON DATA FROM SPOKEN LANGUAGE, WHICH LEAVES OPEN THE QUESTION: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either (1) the time course of sign perception or (2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.",
    "metadata": {
      "title": "Lexical access in sign language: a computational model",
      "summary": "PSYCHOLINGUISTIC THEORIES HAVE PREDOMINANTLY BEEN BUILT UPON DATA FROM SPOKEN LANGUAGE, WHICH LEAVES OPEN THE QUESTION: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either (1) the time course of sign perception or (2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.",
      "abstract": "PSYCHOLINGUISTIC THEORIES HAVE PREDOMINANTLY BEEN BUILT UPON DATA FROM SPOKEN LANGUAGE, WHICH LEAVES OPEN THE QUESTION: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either (1) the time course of sign perception or (2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.",
      "doi": "https://doi.org/10.3389/fpsyg.2014.00428",
      "openalex_id": "https://openalex.org/W2111112092",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a computational model of lexical access in sign language and cross-modality generalization; it does not center on discrete audio tokenization or converting continuous audio into discrete tokens, which is required by inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a computational model of lexical access in sign language and cross-modality generalization; it does not center on discrete audio tokenization or converting continuous audio into discrete tokens, which is required by inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on computational modeling of lexical access in sign language and spoken language, addressing cognitive architectures and psycholinguistic theories, but does not discuss discrete audio or speech tokenization or the conversion of continuous audio signals into discrete tokens, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on computational modeling of lexical access in sign language and spoken language, addressing cognitive architectures and psycholinguistic theories, but does not discuss discrete audio or speech tokenization or the conversion of continuous audio signals into discrete tokens, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Abstract and Lexically Specific Information in Sound Patterns: Evidence from /r/-sandhi in Rhotic and Non-rhotic Varieties of English",
    "abstract": "Phonological theories differ as to whether phonological knowledge is abstract (e.g., phonemic), concrete (e.g., exemplar-based), or some combination of the two. The abstractness/concreteness of phonological knowledge was examined by analyzing the process of /r/-sandhi in two corpora of spoken English. Two predictions of exemplar-based theories were examined: the extent to which a word manifests a particular sound pattern like /r/-deletion should be influenced by (1) its lexical frequency and (2) its distribution in the language with respect to the sound pattern’s conditioning environment. Lexical frequency was found to influence /r/-sandhi in a corpus of rhotic American English but not in a corpus of predominantly non-rhotic British English. No effect of a word’s long-term distribution was found in either corpus. These results support theories proposing that phonological knowledge is both word-specific and abstract and indicate that speakers do not store all phonetic detail that is in principle available to them. The factors that may favor the use of word-specific versus abstract representations are discussed.",
    "metadata": {
      "title": "Abstract and Lexically Specific Information in Sound Patterns: Evidence from /r/-sandhi in Rhotic and Non-rhotic Varieties of English",
      "summary": "Phonological theories differ as to whether phonological knowledge is abstract (e.g., phonemic), concrete (e.g., exemplar-based), or some combination of the two. The abstractness/concreteness of phonological knowledge was examined by analyzing the process of /r/-sandhi in two corpora of spoken English. Two predictions of exemplar-based theories were examined: the extent to which a word manifests a particular sound pattern like /r/-deletion should be influenced by (1) its lexical frequency and (2) its distribution in the language with respect to the sound pattern’s conditioning environment. Lexical frequency was found to influence /r/-sandhi in a corpus of rhotic American English but not in a corpus of predominantly non-rhotic British English. No effect of a word’s long-term distribution was found in either corpus. These results support theories proposing that phonological knowledge is both word-specific and abstract and indicate that speakers do not store all phonetic detail that is in principle available to them. The factors that may favor the use of word-specific versus abstract representations are discussed.",
      "abstract": "Phonological theories differ as to whether phonological knowledge is abstract (e.g., phonemic), concrete (e.g., exemplar-based), or some combination of the two. The abstractness/concreteness of phonological knowledge was examined by analyzing the process of /r/-sandhi in two corpora of spoken English. Two predictions of exemplar-based theories were examined: the extent to which a word manifests a particular sound pattern like /r/-deletion should be influenced by (1) its lexical frequency and (2) its distribution in the language with respect to the sound pattern’s conditioning environment. Lexical frequency was found to influence /r/-sandhi in a corpus of rhotic American English but not in a corpus of predominantly non-rhotic British English. No effect of a word’s long-term distribution was found in either corpus. These results support theories proposing that phonological knowledge is both word-specific and abstract and indicate that speakers do not store all phonetic detail that is in principle available to them. The factors that may favor the use of word-specific versus abstract representations are discussed.",
      "doi": "https://doi.org/10.1177/0023830914567168",
      "openalex_id": "https://openalex.org/W2036289461",
      "arxiv_id": "",
      "publication_date": "2015-02-18",
      "published": "2015-02-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract investigates phonological representations and exposure to /r/-sandhi in spoken English via corpus analyses; it does not propose, analyze, or evaluate discrete audio tokenization into a finite set of symbols, which is required by the inclusion criteria. It focuses on theoretical representation (exemplar vs abstract) rather than discrete tokenization for speech reconstruction or NLP tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract investigates phonological representations and exposure to /r/-sandhi in spoken English via corpus analyses; it does not propose, analyze, or evaluate discrete audio tokenization into a finite set of symbols, which is required by the inclusion criteria. It focuses on theoretical representation (exemplar vs abstract) rather than discrete tokenization for speech reconstruction or NLP tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates phonological knowledge related to /r/-sandhi in English varieties and analyzes phonological patterns with respect to lexical frequency and distribution in spoken corpora, but it does not address discrete audio or speech tokenization processes where continuous audio signals are converted into discrete symbols for tasks such as reconstruction, generation, or understanding; thus, it does not meet the inclusion criteria focused on discrete audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates phonological knowledge related to /r/-sandhi in English varieties and analyzes phonological patterns with respect to lexical frequency and distribution in spoken corpora, but it does not address discrete audio or speech tokenization processes where continuous audio signals are converted into discrete symbols for tasks such as reconstruction, generation, or understanding; thus, it does not meet the inclusion criteria focused on discrete audio tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The MRC Psycholinguistic Database",
    "abstract": "This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",
    "metadata": {
      "title": "The MRC Psycholinguistic Database",
      "summary": "This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",
      "abstract": "This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",
      "doi": "https://doi.org/10.1080/14640748108400805",
      "openalex_id": "https://openalex.org/W1985449126",
      "arxiv_id": "",
      "publication_date": "1981-11-01",
      "published": "1981-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a psycholinguistic database of word properties and associations, not a discrete audio/speech tokenization approach; it does not propose or analyze tokenization into discrete symbols for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a psycholinguistic database of word properties and associations, not a discrete audio/speech tokenization approach; it does not propose or analyze tokenization into discrete symbols for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a psycholinguistic database with linguistic and word-association data, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study, nor does it mention audio signal processing or tokenization methods relevant to audio reconstruction or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a psycholinguistic database with linguistic and word-association data, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study, nor does it mention audio signal processing or tokenization methods relevant to audio reconstruction or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Psycholinguistic investigations of phonological structure in ASL",
    "abstract": "Linguistic categories (e.g. segment, syllable, etc.) have long enabled cogent descriptions of the systematic patterns apparent in spoken languages. Beginning with the seminal work of William Stokoe (1960; 1965), research on the structure of American Sign Language (ASL) has demonstrated that linguistic categories are useful in capturing extant patterns found in a signed language. For example, recognition of a syllable unit permits accounts of morphophonological processes and places constraints on sign forms (Brentari 1990; Perlmutter 1993; Sandler 1993; Corina 1996). Acknowledgment of Movement and Location segments permits descriptions of infixation processes (Liddell and Johnson 1985; Sandler 1986). Feature hierarchies provide accounts of assimilations that are observed in the language and also help to explain those that do not occur (Corina and Sandler 1993). These investigations of linguistic structure have led to a better understanding of both the similarities and differences between signed and spoken language.",
    "metadata": {
      "title": "Psycholinguistic investigations of phonological structure in ASL",
      "summary": "Linguistic categories (e.g. segment, syllable, etc.) have long enabled cogent descriptions of the systematic patterns apparent in spoken languages. Beginning with the seminal work of William Stokoe (1960; 1965), research on the structure of American Sign Language (ASL) has demonstrated that linguistic categories are useful in capturing extant patterns found in a signed language. For example, recognition of a syllable unit permits accounts of morphophonological processes and places constraints on sign forms (Brentari 1990; Perlmutter 1993; Sandler 1993; Corina 1996). Acknowledgment of Movement and Location segments permits descriptions of infixation processes (Liddell and Johnson 1985; Sandler 1986). Feature hierarchies provide accounts of assimilations that are observed in the language and also help to explain those that do not occur (Corina and Sandler 1993). These investigations of linguistic structure have led to a better understanding of both the similarities and differences between signed and spoken language.",
      "abstract": "Linguistic categories (e.g. segment, syllable, etc.) have long enabled cogent descriptions of the systematic patterns apparent in spoken languages. Beginning with the seminal work of William Stokoe (1960; 1965), research on the structure of American Sign Language (ASL) has demonstrated that linguistic categories are useful in capturing extant patterns found in a signed language. For example, recognition of a syllable unit permits accounts of morphophonological processes and places constraints on sign forms (Brentari 1990; Perlmutter 1993; Sandler 1993; Corina 1996). Acknowledgment of Movement and Location segments permits descriptions of infixation processes (Liddell and Johnson 1985; Sandler 1986). Feature hierarchies provide accounts of assimilations that are observed in the language and also help to explain those that do not occur (Corina and Sandler 1993). These investigations of linguistic structure have led to a better understanding of both the similarities and differences between signed and spoken language.",
      "doi": "https://doi.org/10.1017/cbo9780511486777.005",
      "openalex_id": "https://openalex.org/W2198101096",
      "arxiv_id": "",
      "publication_date": "2002-10-24",
      "published": "2002-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses ASL phonology in a visual-manual modality and does not propose, analyze, or evaluate discrete audio/speech tokenization, which is required for inclusion; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses ASL phonology in a visual-manual modality and does not propose, analyze, or evaluate discrete audio/speech tokenization, which is required for inclusion; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological structure in American Sign Language and linguistic categories related to signed language, without any mention of discrete audio/speech tokenization or processing of continuous audio signals into discrete tokens, which is required by the inclusion criteria. It also does not describe any neural encoding or token design relevant to audio/speech reconstruction or understanding tasks, thus not meeting the inclusion criteria and fitting none of the exclusion criteria focused on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological structure in American Sign Language and linguistic categories related to signed language, without any mention of discrete audio/speech tokenization or processing of continuous audio signals into discrete tokens, which is required by the inclusion criteria. It also does not describe any neural encoding or token design relevant to audio/speech reconstruction or understanding tasks, thus not meeting the inclusion criteria and fitting none of the exclusion criteria focused on audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A constraint -based account of handshape contrast in sign languages",
    "abstract": "The main goal of this dissertation is to explore the nature of contrast in sign language handshapes. I first demonstrate that the distribution of handshape contrasts is not homogeneous, either within or across sign languages. By using a variety of methodologies (examination of dictionary data, elicited data, and psycholinguistic experimentation), I present examples of differences related to type of contrast (distinctive, active, and prominent—following Clements, 2001); position in the lexical substrata (following the work of Ito and Mester, 1995a, and Brentari and Padden, 2001); iconic relationships (e.g. shape, size, arrangement of parts); and cross-linguistic variation (comparing American Sign Language, Swiss German Sign Language, and Hong Kong Sign Language). I also propose that the distributional differences in handshape contrasts can be explained in terms of a confluence of pressures on language. Using the tenets of Optimality Theory (OT), these differences can be explained by determining how various languages—or lexical components within languages—rank constraints related to those pressures. Specifically, I follow Flemming's (2002) version of OT (Dispersion Theory) in which grammars balance the pressures of articulatory ease and perceptual distinctiveness, as well as the desire to maximize the number of contrasts available for word formation. To this, I propose an additional pressure—one to maintain contrasts borrowed into the language from external sources. These external contrasts can be borrowed from other languages (directly from other sign languages, or indirectly from spoken languages via systems such as fingerspelling), or they can be borrowed from visual aspects of the real world.",
    "metadata": {
      "title": "A constraint -based account of handshape contrast in sign languages",
      "summary": "The main goal of this dissertation is to explore the nature of contrast in sign language handshapes. I first demonstrate that the distribution of handshape contrasts is not homogeneous, either within or across sign languages. By using a variety of methodologies (examination of dictionary data, elicited data, and psycholinguistic experimentation), I present examples of differences related to type of contrast (distinctive, active, and prominent—following Clements, 2001); position in the lexical substrata (following the work of Ito and Mester, 1995a, and Brentari and Padden, 2001); iconic relationships (e.g. shape, size, arrangement of parts); and cross-linguistic variation (comparing American Sign Language, Swiss German Sign Language, and Hong Kong Sign Language). I also propose that the distributional differences in handshape contrasts can be explained in terms of a confluence of pressures on language. Using the tenets of Optimality Theory (OT), these differences can be explained by determining how various languages—or lexical components within languages—rank constraints related to those pressures. Specifically, I follow Flemming's (2002) version of OT (Dispersion Theory) in which grammars balance the pressures of articulatory ease and perceptual distinctiveness, as well as the desire to maximize the number of contrasts available for word formation. To this, I propose an additional pressure—one to maintain contrasts borrowed into the language from external sources. These external contrasts can be borrowed from other languages (directly from other sign languages, or indirectly from spoken languages via systems such as fingerspelling), or they can be borrowed from visual aspects of the real world.",
      "abstract": "The main goal of this dissertation is to explore the nature of contrast in sign language handshapes. I first demonstrate that the distribution of handshape contrasts is not homogeneous, either within or across sign languages. By using a variety of methodologies (examination of dictionary data, elicited data, and psycholinguistic experimentation), I present examples of differences related to type of contrast (distinctive, active, and prominent—following Clements, 2001); position in the lexical substrata (following the work of Ito and Mester, 1995a, and Brentari and Padden, 2001); iconic relationships (e.g. shape, size, arrangement of parts); and cross-linguistic variation (comparing American Sign Language, Swiss German Sign Language, and Hong Kong Sign Language). I also propose that the distributional differences in handshape contrasts can be explained in terms of a confluence of pressures on language. Using the tenets of Optimality Theory (OT), these differences can be explained by determining how various languages—or lexical components within languages—rank constraints related to those pressures. Specifically, I follow Flemming's (2002) version of OT (Dispersion Theory) in which grammars balance the pressures of articulatory ease and perceptual distinctiveness, as well as the desire to maximize the number of contrasts available for word formation. To this, I propose an additional pressure—one to maintain contrasts borrowed into the language from external sources. These external contrasts can be borrowed from other languages (directly from other sign languages, or indirectly from spoken languages via systems such as fingerspelling), or they can be borrowed from visual aspects of the real world.",
      "doi": "",
      "openalex_id": "https://openalex.org/W1485561458",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates handshape contrasts in sign languages using OT, with no core focus on discrete audio tokenization or speech token sequences; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates handshape contrasts in sign languages using OT, with no core focus on discrete audio tokenization or speech token sequences; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on handshape contrasts in sign languages and uses linguistic and psycholinguistic methodologies but does not discuss audio/speech tokenization, discrete audio representations, or neural quantization-based tokens for speech tasks; thus, it does not meet the inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on handshape contrasts in sign languages and uses linguistic and psycholinguistic methodologies but does not discuss audio/speech tokenization, discrete audio representations, or neural quantization-based tokens for speech tasks; thus, it does not meet the inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bimodal Bilingualism and the Frequency-Lag Hypothesis",
    "abstract": "The frequency-lag hypothesis proposes that bilinguals have slowed lexical retrieval relative to monolinguals and in their nondominant language relative to their dominant language, particularly for low-frequency words. These effects arise because bilinguals divide their language use between 2 languages and use their nondominant language less frequently. We conducted a picture-naming study with hearing American Sign Language (ASL)-English bilinguals (bimodal bilinguals), deaf signers, and English-speaking monolinguals. As predicted by the frequency-lag hypothesis, bimodal bilinguals were slower, less accurate, and exhibited a larger frequency effect when naming pictures in ASL as compared with English (their dominant language) and as compared with deaf signers. For English there was no difference in naming latencies, error rates, or frequency effects for bimodal bilinguals as compared with monolinguals. Neither age of ASL acquisition nor interpreting experience affected the results; picture-naming accuracy and frequency effects were equivalent for deaf signers and English monolinguals. Larger frequency effects in ASL relative to English for bimodal bilinguals suggests that they are affected by a frequency lag in ASL. The absence of a lag for English could reflect the use of mouthing and/or code-blending, which may shield bimodal bilinguals from the lexical slowing observed for spoken language bilinguals in the dominant language.",
    "metadata": {
      "title": "Bimodal Bilingualism and the Frequency-Lag Hypothesis",
      "summary": "The frequency-lag hypothesis proposes that bilinguals have slowed lexical retrieval relative to monolinguals and in their nondominant language relative to their dominant language, particularly for low-frequency words. These effects arise because bilinguals divide their language use between 2 languages and use their nondominant language less frequently. We conducted a picture-naming study with hearing American Sign Language (ASL)-English bilinguals (bimodal bilinguals), deaf signers, and English-speaking monolinguals. As predicted by the frequency-lag hypothesis, bimodal bilinguals were slower, less accurate, and exhibited a larger frequency effect when naming pictures in ASL as compared with English (their dominant language) and as compared with deaf signers. For English there was no difference in naming latencies, error rates, or frequency effects for bimodal bilinguals as compared with monolinguals. Neither age of ASL acquisition nor interpreting experience affected the results; picture-naming accuracy and frequency effects were equivalent for deaf signers and English monolinguals. Larger frequency effects in ASL relative to English for bimodal bilinguals suggests that they are affected by a frequency lag in ASL. The absence of a lag for English could reflect the use of mouthing and/or code-blending, which may shield bimodal bilinguals from the lexical slowing observed for spoken language bilinguals in the dominant language.",
      "abstract": "The frequency-lag hypothesis proposes that bilinguals have slowed lexical retrieval relative to monolinguals and in their nondominant language relative to their dominant language, particularly for low-frequency words. These effects arise because bilinguals divide their language use between 2 languages and use their nondominant language less frequently. We conducted a picture-naming study with hearing American Sign Language (ASL)-English bilinguals (bimodal bilinguals), deaf signers, and English-speaking monolinguals. As predicted by the frequency-lag hypothesis, bimodal bilinguals were slower, less accurate, and exhibited a larger frequency effect when naming pictures in ASL as compared with English (their dominant language) and as compared with deaf signers. For English there was no difference in naming latencies, error rates, or frequency effects for bimodal bilinguals as compared with monolinguals. Neither age of ASL acquisition nor interpreting experience affected the results; picture-naming accuracy and frequency effects were equivalent for deaf signers and English monolinguals. Larger frequency effects in ASL relative to English for bimodal bilinguals suggests that they are affected by a frequency lag in ASL. The absence of a lag for English could reflect the use of mouthing and/or code-blending, which may shield bimodal bilinguals from the lexical slowing observed for spoken language bilinguals in the dominant language.",
      "doi": "https://doi.org/10.1093/deafed/ens034",
      "openalex_id": "https://openalex.org/W2135389530",
      "arxiv_id": "",
      "publication_date": "2012-10-16",
      "published": "2012-10-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates lexical retrieval and frequency effects in bimodal bilinguals (ASL-English) and deaf signers; it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates lexical retrieval and frequency effects in bimodal bilinguals (ASL-English) and deaf signers; it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations, which is the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates lexical retrieval in bilinguals using a picture-naming task and does not discuss discrete audio or speech tokenization, neural encoding, or discrete quantization-based token representations, nor does it analyze or propose tokenization methods for audio signals; thus, it does not meet the inclusion criteria focusing on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates lexical retrieval in bilinguals using a picture-naming task and does not discuss discrete audio or speech tokenization, neural encoding, or discrete quantization-based token representations, nor does it analyze or propose tokenization methods for audio signals; thus, it does not meet the inclusion criteria focusing on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Building BSL SignBank: The Lemma Dilemma Revisited",
    "abstract": "One key criterion when creating a representation of the lexicon of any language within a dictionary or lexical database is that it must be decided which groups of idiosyncratic and systematically modified variants together form a lexeme. Few researchers have, however, attempted to outline such principles as they might apply to sign languages. As a consequence, some sign language dictionaries and lexical databases appear to be mixed collections of phonetic, phonological, morphological, and lexical variants of lexical signs (e.g. Brien 1992) which have not addressed what may be termed as the lemma dilemma . In this paper, we outline the lemmatisation practices used in the creation of BSL SignBank (Fenlon, Cormier et al. 2014), a lexical database and dictionary of British Sign Language based on signs identified within the British Sign Language Corpus ( http://www.bslcorpusproject.org ). We argue that the principles outlined here should be considered in the creation of any sign language lexical database and ultimately any sign language dictionary and reference grammar.",
    "metadata": {
      "title": "Building BSL SignBank: The Lemma Dilemma Revisited",
      "summary": "One key criterion when creating a representation of the lexicon of any language within a dictionary or lexical database is that it must be decided which groups of idiosyncratic and systematically modified variants together form a lexeme. Few researchers have, however, attempted to outline such principles as they might apply to sign languages. As a consequence, some sign language dictionaries and lexical databases appear to be mixed collections of phonetic, phonological, morphological, and lexical variants of lexical signs (e.g. Brien 1992) which have not addressed what may be termed as the lemma dilemma . In this paper, we outline the lemmatisation practices used in the creation of BSL SignBank (Fenlon, Cormier et al. 2014), a lexical database and dictionary of British Sign Language based on signs identified within the British Sign Language Corpus ( http://www.bslcorpusproject.org ). We argue that the principles outlined here should be considered in the creation of any sign language lexical database and ultimately any sign language dictionary and reference grammar.",
      "abstract": "One key criterion when creating a representation of the lexicon of any language within a dictionary or lexical database is that it must be decided which groups of idiosyncratic and systematically modified variants together form a lexeme. Few researchers have, however, attempted to outline such principles as they might apply to sign languages. As a consequence, some sign language dictionaries and lexical databases appear to be mixed collections of phonetic, phonological, morphological, and lexical variants of lexical signs (e.g. Brien 1992) which have not addressed what may be termed as the lemma dilemma . In this paper, we outline the lemmatisation practices used in the creation of BSL SignBank (Fenlon, Cormier et al. 2014), a lexical database and dictionary of British Sign Language based on signs identified within the British Sign Language Corpus ( http://www.bslcorpusproject.org ). We argue that the principles outlined here should be considered in the creation of any sign language lexical database and ultimately any sign language dictionary and reference grammar.",
      "doi": "https://doi.org/10.1093/ijl/ecv008",
      "openalex_id": "https://openalex.org/W2073421084",
      "arxiv_id": "",
      "publication_date": "2015-04-24",
      "published": "2015-04-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses lemmatisation practices for a sign language lexical database (BSL SignBank), focusing on lexicon representation rather than discrete audio/speech tokenization or conversion of continuous audio to discrete tokens; it does not meet the inclusion criterion of analyzing discrete audio tokenization, and relates to sign-language data rather than speech/audio token sequences. Therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses lemmatisation practices for a sign language lexical database (BSL SignBank), focusing on lexicon representation rather than discrete audio/speech tokenization or conversion of continuous audio to discrete tokens; it does not meet the inclusion criterion of analyzing discrete audio tokenization, and relates to sign-language data rather than speech/audio token sequences. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses lemmatisation and lexical database construction for British Sign Language, focusing on lexical variants and representation principles for sign languages, with no mention or analysis of discrete audio/speech tokenization or conversion of audio signals into discrete tokens; hence it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study addresses lemmatisation and lexical database construction for British Sign Language, focusing on lexical variants and representation principles for sign languages, with no mention or analysis of discrete audio/speech tokenization or conversion of audio signals into discrete tokens; hence it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Arbitrariness and Iconicity: Historical Change in American Sign Language",
    "abstract": "Grammarians since Saussure have insisted that language symbols are arbitrary, though conventionalized, in form. Sign languages in general, however, and American Sign Language (ASL) in particular, have been noted for their pantomimic or iconic nature. This paper examines some historical processes in ASL, and shows that there is a strong tendency for signs to change in the direction of arbitrariness, rather than maintaining a level of iconicity. Changes at the formational level can be seen as contributing to language-internal consistency, at the expense of transparency.*",
    "metadata": {
      "title": "Arbitrariness and Iconicity: Historical Change in American Sign Language",
      "summary": "Grammarians since Saussure have insisted that language symbols are arbitrary, though conventionalized, in form. Sign languages in general, however, and American Sign Language (ASL) in particular, have been noted for their pantomimic or iconic nature. This paper examines some historical processes in ASL, and shows that there is a strong tendency for signs to change in the direction of arbitrariness, rather than maintaining a level of iconicity. Changes at the formational level can be seen as contributing to language-internal consistency, at the expense of transparency.*",
      "abstract": "Grammarians since Saussure have insisted that language symbols are arbitrary, though conventionalized, in form. Sign languages in general, however, and American Sign Language (ASL) in particular, have been noted for their pantomimic or iconic nature. This paper examines some historical processes in ASL, and shows that there is a strong tendency for signs to change in the direction of arbitrariness, rather than maintaining a level of iconicity. Changes at the formational level can be seen as contributing to language-internal consistency, at the expense of transparency.*",
      "doi": "https://doi.org/10.2307/412894",
      "openalex_id": "https://openalex.org/W2037715905",
      "arxiv_id": "",
      "publication_date": "1975-09-01",
      "published": "1975-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses historical changes in ASL and its iconicity/arbitrariness, which does not involve discrete audio/speech tokenization or analysis of token sequences, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses historical changes in ASL and its iconicity/arbitrariness, which does not involve discrete audio/speech tokenization or analysis of token sequences, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on historical linguistic changes in American Sign Language related to arbitrariness and iconicity and does not address audio or speech tokenization, discrete audio symbols, or related neural quantization methods as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on historical linguistic changes in American Sign Language related to arbitrariness and iconicity and does not address audio or speech tokenization, discrete audio symbols, or related neural quantization methods as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Time and Thyme Are not Homophones: The Effect of Lemma Frequency on Word Durations in Spontaneous Speech",
    "abstract": "Frequent words tend to shorten. But do homophone pairs, such as time and thyme , shorten equally if one member of the pair is frequent? This study reports an analysis of roughly 90,000 tokens of homophones in the Switchboard corpus of American English telephone conversations, in which it was found that high-frequency words like time are significantly shorter than their low-frequency homophones like thyme . The effect of lemma frequency persisted when local speaking rate, predictability from neighboring words, position relative to pauses, syntactic category, and orthographic regularity were brought under statistical control. These findings have theoretical implications for the locus of frequency information in linguistic competence and in models of language production, and for the role of articulatory routinization in shortening.",
    "metadata": {
      "title": "Time and Thyme Are not Homophones: The Effect of Lemma Frequency on Word Durations in Spontaneous Speech",
      "summary": "Frequent words tend to shorten. But do homophone pairs, such as time and thyme , shorten equally if one member of the pair is frequent? This study reports an analysis of roughly 90,000 tokens of homophones in the Switchboard corpus of American English telephone conversations, in which it was found that high-frequency words like time are significantly shorter than their low-frequency homophones like thyme . The effect of lemma frequency persisted when local speaking rate, predictability from neighboring words, position relative to pauses, syntactic category, and orthographic regularity were brought under statistical control. These findings have theoretical implications for the locus of frequency information in linguistic competence and in models of language production, and for the role of articulatory routinization in shortening.",
      "abstract": "Frequent words tend to shorten. But do homophone pairs, such as time and thyme , shorten equally if one member of the pair is frequent? This study reports an analysis of roughly 90,000 tokens of homophones in the Switchboard corpus of American English telephone conversations, in which it was found that high-frequency words like time are significantly shorter than their low-frequency homophones like thyme . The effect of lemma frequency persisted when local speaking rate, predictability from neighboring words, position relative to pauses, syntactic category, and orthographic regularity were brought under statistical control. These findings have theoretical implications for the locus of frequency information in linguistic competence and in models of language production, and for the role of articulatory routinization in shortening.",
      "doi": "https://doi.org/10.1353/lan.0.0035",
      "openalex_id": "https://openalex.org/W2094008654",
      "arxiv_id": "",
      "publication_date": "2008-09-01",
      "published": "2008-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work examines word durations in spontaneous speech and word tokens, but does not involve discrete audio tokenization or token representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work examines word durations in spontaneous speech and word tokens, but does not involve discrete audio tokenization or token representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study examines the effect of lemma frequency on word durations in spontaneous speech, focusing on phonetic and linguistic analysis of homophones, but does not propose, analyze, or evaluate discrete audio/speech tokenization as defined by the inclusion criteria (i.e., converting continuous audio signals into sequences of discrete tokens for audio reconstruction, generation or understanding). The analysis is based on duration and frequency effects rather than any discrete tokenization method or discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study examines the effect of lemma frequency on word durations in spontaneous speech, focusing on phonetic and linguistic analysis of homophones, but does not propose, analyze, or evaluate discrete audio/speech tokenization as defined by the inclusion criteria (i.e., converting continuous audio signals into sequences of discrete tokens for audio reconstruction, generation or understanding). The analysis is based on duration and frequency effects rather than any discrete tokenization method or discrete audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Transition From Fingerspelling to English Print: Facilitating English Decoding",
    "abstract": "Fingerspelling is an integral part of American Sign Language (ASL) and it is also an important aspect of becoming bilingual in English and ASL. Even though fingerspelling is based on English orthography, the development of fingerspelling does not parallel the development of reading in hearing children. Research reveals that deaf children may initially treat fingerspelled words as lexical items rather than a series of letters that represent English orthography and only later begin to learn to link handshapes to English graphemes. The purpose of this study is to determine whether a training method that uses fingerspelling and phonological patterns that resemble those found in lexicalized fingerspelling to teach deaf students unknown English vocabulary would increase their ability to learn the fingerspelled and orthographic version of a word. There were 21 deaf students (aged 4-14 years) who participated. Results show that students were better able to recognize and write the printed English word as well as fingerspell the word, when training incorporated fingerspelling that is more lexicalized. The discussion focuses on the degree to which fingerspelling can serve as a visual phonological bridge as an aid to decode English print.",
    "metadata": {
      "title": "The Transition From Fingerspelling to English Print: Facilitating English Decoding",
      "summary": "Fingerspelling is an integral part of American Sign Language (ASL) and it is also an important aspect of becoming bilingual in English and ASL. Even though fingerspelling is based on English orthography, the development of fingerspelling does not parallel the development of reading in hearing children. Research reveals that deaf children may initially treat fingerspelled words as lexical items rather than a series of letters that represent English orthography and only later begin to learn to link handshapes to English graphemes. The purpose of this study is to determine whether a training method that uses fingerspelling and phonological patterns that resemble those found in lexicalized fingerspelling to teach deaf students unknown English vocabulary would increase their ability to learn the fingerspelled and orthographic version of a word. There were 21 deaf students (aged 4-14 years) who participated. Results show that students were better able to recognize and write the printed English word as well as fingerspell the word, when training incorporated fingerspelling that is more lexicalized. The discussion focuses on the degree to which fingerspelling can serve as a visual phonological bridge as an aid to decode English print.",
      "abstract": "Fingerspelling is an integral part of American Sign Language (ASL) and it is also an important aspect of becoming bilingual in English and ASL. Even though fingerspelling is based on English orthography, the development of fingerspelling does not parallel the development of reading in hearing children. Research reveals that deaf children may initially treat fingerspelled words as lexical items rather than a series of letters that represent English orthography and only later begin to learn to link handshapes to English graphemes. The purpose of this study is to determine whether a training method that uses fingerspelling and phonological patterns that resemble those found in lexicalized fingerspelling to teach deaf students unknown English vocabulary would increase their ability to learn the fingerspelled and orthographic version of a word. There were 21 deaf students (aged 4-14 years) who participated. Results show that students were better able to recognize and write the printed English word as well as fingerspell the word, when training incorporated fingerspelling that is more lexicalized. The discussion focuses on the degree to which fingerspelling can serve as a visual phonological bridge as an aid to decode English print.",
      "doi": "https://doi.org/10.1093/deafed/enm003",
      "openalex_id": "https://openalex.org/W2142638134",
      "arxiv_id": "",
      "publication_date": "2007-02-25",
      "published": "2007-02-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on fingerspelling within ASL to aid decoding English print and does not address discrete audio tokenization or token-level speech representations for audio processing tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on fingerspelling within ASL to aid decoding English print and does not address discrete audio tokenization or token-level speech representations for audio processing tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fingerspelling and English decoding in deaf children, with no mention or analysis of discrete audio/speech tokenization or any neural quantization-based discrete token representations; thus, it does not meet the inclusion criteria related to discrete audio tokenization and does not address discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fingerspelling and English decoding in deaf children, with no mention or analysis of discrete audio/speech tokenization or any neural quantization-based discrete token representations; thus, it does not meet the inclusion criteria related to discrete audio tokenization and does not address discrete audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Calculating Frequency of Occurrence of ASL handshapes",
    "abstract": "&lt;p&gt;Here we discuss an investigation of handshape markedness based on frequency of occurrence in an ASL&lt;br /&gt;database. Using a database of the most frequently used signs in a corpus of child language and other&lt;br /&gt;early-acquired signs we examined the handshapes of approximately 1000 signs by using two annotation&lt;br /&gt;systems, BTS and Stokoe annotation. Results indicate that the distribution of handshape use on the&lt;br /&gt;dominant and non-dominant hands is consistent with the predictions set forth by previous researchers&lt;br /&gt;in their examinations of handshape markedness. Our findings are also consistent with investigations&lt;br /&gt;handshape frequency in other sign languages, suggesting some cross-linguistic comparability in handshape&lt;br /&gt;markedness.&lt;/p&gt;",
    "metadata": {
      "title": "Calculating Frequency of Occurrence of ASL handshapes",
      "summary": "&lt;p&gt;Here we discuss an investigation of handshape markedness based on frequency of occurrence in an ASL&lt;br /&gt;database. Using a database of the most frequently used signs in a corpus of child language and other&lt;br /&gt;early-acquired signs we examined the handshapes of approximately 1000 signs by using two annotation&lt;br /&gt;systems, BTS and Stokoe annotation. Results indicate that the distribution of handshape use on the&lt;br /&gt;dominant and non-dominant hands is consistent with the predictions set forth by previous researchers&lt;br /&gt;in their examinations of handshape markedness. Our findings are also consistent with investigations&lt;br /&gt;handshape frequency in other sign languages, suggesting some cross-linguistic comparability in handshape&lt;br /&gt;markedness.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Here we discuss an investigation of handshape markedness based on frequency of occurrence in an ASL&lt;br /&gt;database. Using a database of the most frequently used signs in a corpus of child language and other&lt;br /&gt;early-acquired signs we examined the handshapes of approximately 1000 signs by using two annotation&lt;br /&gt;systems, BTS and Stokoe annotation. Results indicate that the distribution of handshape use on the&lt;br /&gt;dominant and non-dominant hands is consistent with the predictions set forth by previous researchers&lt;br /&gt;in their examinations of handshape markedness. Our findings are also consistent with investigations&lt;br /&gt;handshape frequency in other sign languages, suggesting some cross-linguistic comparability in handshape&lt;br /&gt;markedness.&lt;/p&gt;",
      "doi": "https://doi.org/10.3765/exabs.v0i0.764",
      "openalex_id": "https://openalex.org/W2425058634",
      "arxiv_id": "",
      "publication_date": "2013-05-07",
      "published": "2013-05-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study analyzes handshape frequency in ASL using sign-language annotations; it does not involve discrete audio tokenization or speech-token sequence modeling, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study analyzes handshape frequency in ASL using sign-language annotations; it does not involve discrete audio tokenization or speech-token sequence modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing handshape frequencies in ASL signs, not on discrete audio/speech tokenization of continuous audio signals; hence, it does not meet the inclusion criteria related to audio tokenization and discrete symbol sequences for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing handshape frequencies in ASL signs, not on discrete audio/speech tokenization of continuous audio signals; hence, it does not meet the inclusion criteria related to audio tokenization and discrete symbol sequences for speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexical Frequency in Sign Languages",
    "abstract": "Measures of lexical frequency presuppose the existence of corpora, but true machine-readable corpora of sign languages (SLs) are only now being created. Lexical frequency ratings for SLs are needed because there has been a heavy reliance on the interpretation of results of psycholinguistic and neurolinguistic experiments in the SL research literature; yet, these experiments have been conducted without the benefit of such measures. In addition, measures of lexical frequency can also guide SL teachers by identifying which signs would be prioritized in early language instruction. I begin by a discussion of lexicalization and sign types in order to explain what constitutes a lexical sign in SLs. I then present the annotation method and results. In the discussion, I raise the potential limitations of previous studies of lexical frequency in terms of the discrimination of lexical signs from other kinds of signs, consistent lemma glossing, part of speech tagging, and the systematic treatment of depicting signs. I conclude in cautioning that descriptions of SL grammars that do not accommodate typical mixtures and sequences of signs as shown in data are likely to be unreliable.",
    "metadata": {
      "title": "Lexical Frequency in Sign Languages",
      "summary": "Measures of lexical frequency presuppose the existence of corpora, but true machine-readable corpora of sign languages (SLs) are only now being created. Lexical frequency ratings for SLs are needed because there has been a heavy reliance on the interpretation of results of psycholinguistic and neurolinguistic experiments in the SL research literature; yet, these experiments have been conducted without the benefit of such measures. In addition, measures of lexical frequency can also guide SL teachers by identifying which signs would be prioritized in early language instruction. I begin by a discussion of lexicalization and sign types in order to explain what constitutes a lexical sign in SLs. I then present the annotation method and results. In the discussion, I raise the potential limitations of previous studies of lexical frequency in terms of the discrimination of lexical signs from other kinds of signs, consistent lemma glossing, part of speech tagging, and the systematic treatment of depicting signs. I conclude in cautioning that descriptions of SL grammars that do not accommodate typical mixtures and sequences of signs as shown in data are likely to be unreliable.",
      "abstract": "Measures of lexical frequency presuppose the existence of corpora, but true machine-readable corpora of sign languages (SLs) are only now being created. Lexical frequency ratings for SLs are needed because there has been a heavy reliance on the interpretation of results of psycholinguistic and neurolinguistic experiments in the SL research literature; yet, these experiments have been conducted without the benefit of such measures. In addition, measures of lexical frequency can also guide SL teachers by identifying which signs would be prioritized in early language instruction. I begin by a discussion of lexicalization and sign types in order to explain what constitutes a lexical sign in SLs. I then present the annotation method and results. In the discussion, I raise the potential limitations of previous studies of lexical frequency in terms of the discrimination of lexical signs from other kinds of signs, consistent lemma glossing, part of speech tagging, and the systematic treatment of depicting signs. I conclude in cautioning that descriptions of SL grammars that do not accommodate typical mixtures and sequences of signs as shown in data are likely to be unreliable.",
      "doi": "https://doi.org/10.1093/deafed/enr036",
      "openalex_id": "https://openalex.org/W2153317408",
      "arxiv_id": "",
      "publication_date": "2011-08-12",
      "published": "2011-08-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses lexical frequency in sign languages and annotation methods but does not propose or analyze discrete audio tokenization or a token-based representation of speech; it concerns sign-language corpora and linguistic annotation rather than discrete audio tokens for speech tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses lexical frequency in sign languages and annotation methods but does not propose or analyze discrete audio tokenization or a token-based representation of speech; it concerns sign-language corpora and linguistic annotation rather than discrete audio tokens for speech tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical frequency in sign languages, discussing linguistic annotation and psycholinguistic research implications, but it does not involve any discrete audio/speech tokenization or analysis of audio signals. Therefore, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical frequency in sign languages, discussing linguistic annotation and psycholinguistic research implications, but it does not involve any discrete audio/speech tokenization or analysis of audio signals. Therefore, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The reluctant oracle: using strategic annotations to add value to, and extract value from, a signed language corpus",
    "abstract": "In this paper, I discuss the ways in which multimedia annotation software is being used to transform an archive of Auslan recordings into a true machine-readable language corpus. After the basic structure of the annotation files in the Auslan corpus is described and the exercise differentiated from transcription, the glossing and annotation conventions are explained. Following this, I exemplify the searching and pattern-matching at different levels of linguistic organisation that these annotations make possible. The paper shows how, in the creation of signed language corpora, it is important to be clear about the difference between transcription and annotation. Without an awareness of this distinction – and despite time consuming and expensive processing of the video recordings – we may not be able to discern the types of patterns in our corpora that we hope to. The conventions are designed to ensure that the annotations really do enable researchers to identify regularities at different levels of linguistic organisation in the corpus and, thus, to test, or build on, existing descriptions of the language.",
    "metadata": {
      "title": "The reluctant oracle: using strategic annotations to add value to, and extract value from, a signed language corpus",
      "summary": "In this paper, I discuss the ways in which multimedia annotation software is being used to transform an archive of Auslan recordings into a true machine-readable language corpus. After the basic structure of the annotation files in the Auslan corpus is described and the exercise differentiated from transcription, the glossing and annotation conventions are explained. Following this, I exemplify the searching and pattern-matching at different levels of linguistic organisation that these annotations make possible. The paper shows how, in the creation of signed language corpora, it is important to be clear about the difference between transcription and annotation. Without an awareness of this distinction – and despite time consuming and expensive processing of the video recordings – we may not be able to discern the types of patterns in our corpora that we hope to. The conventions are designed to ensure that the annotations really do enable researchers to identify regularities at different levels of linguistic organisation in the corpus and, thus, to test, or build on, existing descriptions of the language.",
      "abstract": "In this paper, I discuss the ways in which multimedia annotation software is being used to transform an archive of Auslan recordings into a true machine-readable language corpus. After the basic structure of the annotation files in the Auslan corpus is described and the exercise differentiated from transcription, the glossing and annotation conventions are explained. Following this, I exemplify the searching and pattern-matching at different levels of linguistic organisation that these annotations make possible. The paper shows how, in the creation of signed language corpora, it is important to be clear about the difference between transcription and annotation. Without an awareness of this distinction – and despite time consuming and expensive processing of the video recordings – we may not be able to discern the types of patterns in our corpora that we hope to. The conventions are designed to ensure that the annotations really do enable researchers to identify regularities at different levels of linguistic organisation in the corpus and, thus, to test, or build on, existing descriptions of the language.",
      "doi": "https://doi.org/10.3366/cor.2014.0056",
      "openalex_id": "https://openalex.org/W2012075454",
      "arxiv_id": "",
      "publication_date": "2014-11-01",
      "published": "2014-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on annotation vs transcription in a signed-language corpus rather than on discretizing audio into tokenized discrete units for speech tasks, so it does not meet the discrete audio tokenization inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on annotation vs transcription in a signed-language corpus rather than on discretizing audio into tokenized discrete units for speech tasks, so it does not meet the discrete audio tokenization inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on annotation and transcription of signed language corpora and does not involve discrete audio or speech tokenization; thus, it does not meet the inclusion criteria related to audio signal tokenization or discrete audio token representations relevant to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on annotation and transcription of signed language corpora and does not involve discrete audio or speech tokenization; thus, it does not meet the inclusion criteria related to audio signal tokenization or discrete audio token representations relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Segmental Framework for Representing Signs Phonetically",
    "abstract": "The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments.",
    "metadata": {
      "title": "A Segmental Framework for Representing Signs Phonetically",
      "summary": "The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments.",
      "abstract": "The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments.",
      "doi": "https://doi.org/10.1353/sls.2011.0002",
      "openalex_id": "https://openalex.org/W2054973881",
      "arxiv_id": "",
      "publication_date": "2011-03-01",
      "published": "2011-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article proposes a segmental framework for representing signs in sign language (postural and transforming segments) and focuses on phonetic representation of visual signs; it does not address discrete audio/speech tokenization into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This article proposes a segmental framework for representing signs in sign language (postural and transforming segments) and focuses on phonetic representation of visual signs; it does not address discrete audio/speech tokenization into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonetic segmentation of signed languages, analyzing the visual and articulatory features of sign language segments, and does not address discrete audio or speech tokenization from continuous audio signals as required by the inclusion criteria. Furthermore, it does not discuss any audio-based tokenization, neural encoding, or discrete audio token representations relevant to speech tasks. Therefore, it does not meet the inclusion criteria and falls outside the scope of discrete audio tokenization studies specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonetic segmentation of signed languages, analyzing the visual and articulatory features of sign language segments, and does not address discrete audio or speech tokenization from continuous audio signals as required by the inclusion criteria. Furthermore, it does not discuss any audio-based tokenization, neural encoding, or discrete audio token representations relevant to speech tasks. Therefore, it does not meet the inclusion criteria and falls outside the scope of discrete audio tokenization studies specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "American Sign Language: The Phonological Base",
    "abstract": "This paper has the ambitious goal of outlining the phonological structures and processes we have analyzed in American Sign Language (ASL). In order to do this we have divided the paper into five parts. In section 1 we detail the types of sequential phenomena found in the production of individual signs, allowing us to argue that ASL signs are composed of sequences of phonological segments, just as are words in spoken languages. Section 2 provides the details of a segmental phonetic transcription system. Using the descriptions made available by the transcription system, Section 3 briefly discusses both paradigmatic and syntagmatic contrast in ASL signs. Section 4 deals with the various types of phonological processes at work in the language, processes remarkable in their similarity to phonological processes found in spoken languages. We conclude the paper with an overview of the major typed of phonological effects of ASL’s rich system of morphological processes. We realize that the majority of readers will come to this paper with neither sign language proficiency nor a knowledge of sign language structure. As a result, many will encounter reference to ASL signs without knowing their form. Although we have been unable to illustrate all the examples, we hope we have provided sufficient illustrations to make the paper more accessible.",
    "metadata": {
      "title": "American Sign Language: The Phonological Base",
      "summary": "This paper has the ambitious goal of outlining the phonological structures and processes we have analyzed in American Sign Language (ASL). In order to do this we have divided the paper into five parts. In section 1 we detail the types of sequential phenomena found in the production of individual signs, allowing us to argue that ASL signs are composed of sequences of phonological segments, just as are words in spoken languages. Section 2 provides the details of a segmental phonetic transcription system. Using the descriptions made available by the transcription system, Section 3 briefly discusses both paradigmatic and syntagmatic contrast in ASL signs. Section 4 deals with the various types of phonological processes at work in the language, processes remarkable in their similarity to phonological processes found in spoken languages. We conclude the paper with an overview of the major typed of phonological effects of ASL’s rich system of morphological processes. We realize that the majority of readers will come to this paper with neither sign language proficiency nor a knowledge of sign language structure. As a result, many will encounter reference to ASL signs without knowing their form. Although we have been unable to illustrate all the examples, we hope we have provided sufficient illustrations to make the paper more accessible.",
      "abstract": "This paper has the ambitious goal of outlining the phonological structures and processes we have analyzed in American Sign Language (ASL). In order to do this we have divided the paper into five parts. In section 1 we detail the types of sequential phenomena found in the production of individual signs, allowing us to argue that ASL signs are composed of sequences of phonological segments, just as are words in spoken languages. Section 2 provides the details of a segmental phonetic transcription system. Using the descriptions made available by the transcription system, Section 3 briefly discusses both paradigmatic and syntagmatic contrast in ASL signs. Section 4 deals with the various types of phonological processes at work in the language, processes remarkable in their similarity to phonological processes found in spoken languages. We conclude the paper with an overview of the major typed of phonological effects of ASL’s rich system of morphological processes. We realize that the majority of readers will come to this paper with neither sign language proficiency nor a knowledge of sign language structure. As a result, many will encounter reference to ASL signs without knowing their form. Although we have been unable to illustrate all the examples, we hope we have provided sufficient illustrations to make the paper more accessible.",
      "doi": "https://doi.org/10.1353/sls.1989.0027",
      "openalex_id": "https://openalex.org/W1965498301",
      "arxiv_id": "",
      "publication_date": "1989-09-01",
      "published": "1989-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper studies American Sign Language phonology and sign-language segment transcription; it does not analyze discrete audio/tokenization of speech or convert audio signals into a discrete token sequence, so it fails the inclusion criterion for discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper studies American Sign Language phonology and sign-language segment transcription; it does not analyze discrete audio/tokenization of speech or convert audio signals into a discrete token sequence, so it fails the inclusion criterion for discrete audio tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on phonological structures and processes in American Sign Language, specifically analyzing sequences of phonological segments and their transcription, with no mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for audio reconstruction, generation, or understanding; thus it does not meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on phonological structures and processes in American Sign Language, specifically analyzing sequences of phonological segments and their transcription, with no mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens for audio reconstruction, generation, or understanding; thus it does not meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Recognizing Spoken Words: The Neighborhood Activation Model",
    "abstract": "The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.",
    "metadata": {
      "title": "Recognizing Spoken Words: The Neighborhood Activation Model",
      "summary": "The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.",
      "abstract": "The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.",
      "doi": "https://doi.org/10.1097/00003446-199802000-00001",
      "openalex_id": "https://openalex.org/W2026992087",
      "arxiv_id": "",
      "publication_date": "1998-02-01",
      "published": "1998-02-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a cognitive model for spoken word recognition based on phonological neighborhood activation, not a study of discrete audio tokenization into a finite vocabulary; it does not analyze or propose discrete tokenization as a core object.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a cognitive model for spoken word recognition based on phonological neighborhood activation, not a study of discrete audio tokenization into a finite vocabulary; it does not analyze or propose discrete tokenization as a core object.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on auditory word recognition and models the effects of similarity neighborhood structure on discriminating acoustic-phonetic word representations, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on auditory word recognition and models the effects of similarity neighborhood structure on discriminating acoustic-phonetic word representations, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as converting continuous audio into discrete tokens; thus, it does not meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "The Distribution of Signs in New Zealand Sign Language",
    "abstract": "Until now, teachers and learners of NZSL have not had access to information on the most frequently used signs in the Deaf community. This article describes the first study of the distribution of signs in New Zealand Sign Language (NZSL). We hope that it will help teachers of NZSL make decisions about which signs to teach first and suggest questions for investigation into other signed languages using a corpus analysis approach.",
    "metadata": {
      "title": "The Distribution of Signs in New Zealand Sign Language",
      "summary": "Until now, teachers and learners of NZSL have not had access to information on the most frequently used signs in the Deaf community. This article describes the first study of the distribution of signs in New Zealand Sign Language (NZSL). We hope that it will help teachers of NZSL make decisions about which signs to teach first and suggest questions for investigation into other signed languages using a corpus analysis approach.",
      "abstract": "Until now, teachers and learners of NZSL have not had access to information on the most frequently used signs in the Deaf community. This article describes the first study of the distribution of signs in New Zealand Sign Language (NZSL). We hope that it will help teachers of NZSL make decisions about which signs to teach first and suggest questions for investigation into other signed languages using a corpus analysis approach.",
      "doi": "https://doi.org/10.1353/sls.2006.0027",
      "openalex_id": "https://openalex.org/W2154212878",
      "arxiv_id": "",
      "publication_date": "2006-06-01",
      "published": "2006-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes the distribution of signs in New Zealand Sign Language using a corpus approach, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete quantization of audio signals for reconstruction/generation/understanding tasks; it concerns a visual-manual language modality rather than speech audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes the distribution of signs in New Zealand Sign Language using a corpus approach, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete quantization of audio signals for reconstruction/generation/understanding tasks; it concerns a visual-manual language modality rather than speech audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the distribution of visual sign language signs rather than audio or speech tokenization, and makes no mention of discrete audio tokenization or analysis thereof, which is the key inclusion criterion; thus, it does not meet the inclusion requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the distribution of visual sign language signs rather than audio or speech tokenization, and makes no mention of discrete audio tokenization or analysis thereof, which is the key inclusion criterion; thus, it does not meet the inclusion requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bilingual Deaf Students’ Phonological Awareness in ASL and Reading Skills in English",
    "abstract": "The sources of knowledge that individuals use to make similarity judgments about words are thought to tap underlying phonological representations. This study addresses the issue of segmental representation by investigating bilingual deaf students’ (a) awareness of American Sign Language (ASL) phonological structure; (b) the relationships between ASL phonological awareness (ASL-PA) and written English word recognition and reading comprehension skill, and (c) the question of whether age and/or reading ability would differentially affect performance on an ASL-PA task in fifty bilingual deaf children (ages 7–18) attending schools for deaf children in Western Canada. In the ASL-PA task, minimal contrasts between ASL parameters (handshape, movement, and location; H, M, and L, respectively) were systematically manipulated. The results show significant differences in deaf students’ ASL phonological awareness, with discrimination accuracy improving with age and reading ability. Significant relationships between children’s second language (L2) reading skills and first language (L1) phonological awareness skills were found. Evidence of rich metalinguistic knowledge that children with developing L1 phonological skills bring to the acquisition of L2 reading skills may have practical implications for the education of bilingual deaf children.",
    "metadata": {
      "title": "Bilingual Deaf Students’ Phonological Awareness in ASL and Reading Skills in English",
      "summary": "The sources of knowledge that individuals use to make similarity judgments about words are thought to tap underlying phonological representations. This study addresses the issue of segmental representation by investigating bilingual deaf students’ (a) awareness of American Sign Language (ASL) phonological structure; (b) the relationships between ASL phonological awareness (ASL-PA) and written English word recognition and reading comprehension skill, and (c) the question of whether age and/or reading ability would differentially affect performance on an ASL-PA task in fifty bilingual deaf children (ages 7–18) attending schools for deaf children in Western Canada. In the ASL-PA task, minimal contrasts between ASL parameters (handshape, movement, and location; H, M, and L, respectively) were systematically manipulated. The results show significant differences in deaf students’ ASL phonological awareness, with discrimination accuracy improving with age and reading ability. Significant relationships between children’s second language (L2) reading skills and first language (L1) phonological awareness skills were found. Evidence of rich metalinguistic knowledge that children with developing L1 phonological skills bring to the acquisition of L2 reading skills may have practical implications for the education of bilingual deaf children.",
      "abstract": "The sources of knowledge that individuals use to make similarity judgments about words are thought to tap underlying phonological representations. This study addresses the issue of segmental representation by investigating bilingual deaf students’ (a) awareness of American Sign Language (ASL) phonological structure; (b) the relationships between ASL phonological awareness (ASL-PA) and written English word recognition and reading comprehension skill, and (c) the question of whether age and/or reading ability would differentially affect performance on an ASL-PA task in fifty bilingual deaf children (ages 7–18) attending schools for deaf children in Western Canada. In the ASL-PA task, minimal contrasts between ASL parameters (handshape, movement, and location; H, M, and L, respectively) were systematically manipulated. The results show significant differences in deaf students’ ASL phonological awareness, with discrimination accuracy improving with age and reading ability. Significant relationships between children’s second language (L2) reading skills and first language (L1) phonological awareness skills were found. Evidence of rich metalinguistic knowledge that children with developing L1 phonological skills bring to the acquisition of L2 reading skills may have practical implications for the education of bilingual deaf children.",
      "doi": "https://doi.org/10.1353/sls.2013.0028",
      "openalex_id": "https://openalex.org/W2030728229",
      "arxiv_id": "",
      "publication_date": "2013-09-01",
      "published": "2013-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on ASL phonological awareness and English reading skills without analyzing any discrete audio/speech tokenization or tokenized speech representations, so it does not meet the inclusion criterion for discrete audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on ASL phonological awareness and English reading skills without analyzing any discrete audio/speech tokenization or tokenized speech representations, so it does not meet the inclusion criterion for discrete audio tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological awareness and the relationships between ASL phonological skills and English reading skills among bilingual deaf students, without any indication of discrete audio or speech tokenization analysis, nor any mention of audio signal processing or neural encoding methods; hence, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological awareness and the relationships between ASL phonological skills and English reading skills among bilingual deaf students, without any indication of discrete audio or speech tokenization analysis, nor any mention of audio signal processing or neural encoding methods; hence, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Introduction to WordNet: An On-line Lexical Database<sup>*</sup>",
    "abstract": "WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",
    "metadata": {
      "title": "Introduction to WordNet: An On-line Lexical Database<sup>*</sup>",
      "summary": "WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",
      "abstract": "WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",
      "doi": "https://doi.org/10.1093/ijl/3.4.235",
      "openalex_id": "https://openalex.org/W2102381086",
      "arxiv_id": "",
      "publication_date": "1990-01-01",
      "published": "1990-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes WordNet as a lexical database; there is no focus on discrete audio/speech tokenization or converting continuous audio into a discrete token sequence for audio reconstruction, generation, or understanding, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes WordNet as a lexical database; there is no focus on discrete audio/speech tokenization or converting continuous audio into a discrete token sequence for audio reconstruction, generation, or understanding, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces WordNet, an online lexical database focused on organizing lexical concepts and synonym sets in language, without any mention of audio/speech tokenization or discrete audio symbol sequences relevant to audio reconstruction or understanding; therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces WordNet, an online lexical database focused on organizing lexical concepts and synonym sets in language, without any mention of audio/speech tokenization or discrete audio symbol sequences relevant to audio reconstruction or understanding; therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Frequency Characteristics of American Sign Language",
    "abstract": "When signers communicate with one another, they use some signs, such as finish, more frequently than others, such as eagle. The frequency of occurrence affects both the way that languages are processed and the way they change over time. It is important to be aware of the frequency characteristics of a language when pursuing either psycholinguistic or linguistic analyses. This article reports the findings of a pilot study of sign frequency in American Sign Language. A corpus of over four thousand signs was analyzed, and some of the frequency characteristics that were uncovered are reported here. Appendix 1 lists the most frequent signs in the database.",
    "metadata": {
      "title": "Frequency Characteristics of American Sign Language",
      "summary": "When signers communicate with one another, they use some signs, such as finish, more frequently than others, such as eagle. The frequency of occurrence affects both the way that languages are processed and the way they change over time. It is important to be aware of the frequency characteristics of a language when pursuing either psycholinguistic or linguistic analyses. This article reports the findings of a pilot study of sign frequency in American Sign Language. A corpus of over four thousand signs was analyzed, and some of the frequency characteristics that were uncovered are reported here. Appendix 1 lists the most frequent signs in the database.",
      "abstract": "When signers communicate with one another, they use some signs, such as finish, more frequently than others, such as eagle. The frequency of occurrence affects both the way that languages are processed and the way they change over time. It is important to be aware of the frequency characteristics of a language when pursuing either psycholinguistic or linguistic analyses. This article reports the findings of a pilot study of sign frequency in American Sign Language. A corpus of over four thousand signs was analyzed, and some of the frequency characteristics that were uncovered are reported here. Appendix 1 lists the most frequent signs in the database.",
      "doi": "https://doi.org/10.1353/sls.2003.0003",
      "openalex_id": "https://openalex.org/W1966925283",
      "arxiv_id": "",
      "publication_date": "2003-12-01",
      "published": "2003-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Investigates frequency of discrete sign tokens in ASL, but does not involve discrete audio/speech tokenization or conversion of continuous audio into a discrete token sequence for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Investigates frequency of discrete sign tokens in ASL, but does not involve discrete audio/speech tokenization or conversion of continuous audio into a discrete token sequence for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the frequency characteristics of signs in American Sign Language based on a corpus analysis, without proposing or analyzing discrete audio/speech tokenization from continuous audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the frequency characteristics of signs in American Sign Language based on a corpus analysis, without proposing or analyzing discrete audio/speech tokenization from continuous audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Role of Iconicity in Early Sign Language Acquisition",
    "abstract": "A longitudinal study of sign language acquisition was conducted with 13 very young children (median age 10 months at outset of study) of deaf parents. The children's sign language lexicons were examined for their percentages of iconic signs at two early stages of vocabulary development. Iconic signs are those that clearly resemble the action, object, or characteristic they represent. Analysis of the subjects' vocabularies revealed that iconic signs comprised 30.8% of the first 10 signs they acquired. At age 18 months, the proportion of iconic signs was found to be 33.7%. The finding that a majority of signs in the subjects' early vocabularies were not iconic suggests that the role of iconicity in young children's acquisition of signs may have been overrated by some investigators, and that other formational features may be of greater importance in influencing young children's ability to acquire signs.",
    "metadata": {
      "title": "The Role of Iconicity in Early Sign Language Acquisition",
      "summary": "A longitudinal study of sign language acquisition was conducted with 13 very young children (median age 10 months at outset of study) of deaf parents. The children's sign language lexicons were examined for their percentages of iconic signs at two early stages of vocabulary development. Iconic signs are those that clearly resemble the action, object, or characteristic they represent. Analysis of the subjects' vocabularies revealed that iconic signs comprised 30.8% of the first 10 signs they acquired. At age 18 months, the proportion of iconic signs was found to be 33.7%. The finding that a majority of signs in the subjects' early vocabularies were not iconic suggests that the role of iconicity in young children's acquisition of signs may have been overrated by some investigators, and that other formational features may be of greater importance in influencing young children's ability to acquire signs.",
      "abstract": "A longitudinal study of sign language acquisition was conducted with 13 very young children (median age 10 months at outset of study) of deaf parents. The children's sign language lexicons were examined for their percentages of iconic signs at two early stages of vocabulary development. Iconic signs are those that clearly resemble the action, object, or characteristic they represent. Analysis of the subjects' vocabularies revealed that iconic signs comprised 30.8% of the first 10 signs they acquired. At age 18 months, the proportion of iconic signs was found to be 33.7%. The finding that a majority of signs in the subjects' early vocabularies were not iconic suggests that the role of iconicity in young children's acquisition of signs may have been overrated by some investigators, and that other formational features may be of greater importance in influencing young children's ability to acquire signs.",
      "doi": "https://doi.org/10.1044/jshd.4903.287",
      "openalex_id": "https://openalex.org/W2011095659",
      "arxiv_id": "",
      "publication_date": "1984-08-01",
      "published": "1984-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates iconicity in early sign language acquisition and does not involve discrete audio tokenization or analysis of transforming continuous audio signals into discrete tokens; therefore it does not meet the specified inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates iconicity in early sign language acquisition and does not involve discrete audio tokenization or analysis of transforming continuous audio signals into discrete tokens; therefore it does not meet the specified inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the acquisition of iconic signs in sign language by young children and does not involve any analysis, proposal, or evaluation of discrete audio or speech tokenization, nor does it deal with continuous audio signals or their discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the acquisition of iconic signs in sign language by young children and does not involve any analysis, proposal, or evaluation of discrete audio or speech tokenization, nor does it deal with continuous audio signals or their discrete token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Patterned iconicity in sign language lexicons",
    "abstract": "Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity . In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.",
    "metadata": {
      "title": "Patterned iconicity in sign language lexicons",
      "summary": "Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity . In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.",
      "abstract": "Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity . In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.",
      "doi": "https://doi.org/10.1075/gest.13.3.03pad",
      "openalex_id": "https://openalex.org/W2019268934",
      "arxiv_id": "",
      "publication_date": "2013-12-31",
      "published": "2013-12-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study examines iconic patterning in sign language and gesture without any discrete audio/speech tokenization or discrete token design relevant to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study examines iconic patterning in sign language and gesture without any discrete audio/speech tokenization or discrete token design relevant to speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on iconicity in sign language lexicons and gestures and does not involve audio/speech tokenization or discrete audio representations as defined in the inclusion criteria. It neither analyzes discrete tokenization of audio signals nor addresses related speech or audio tasks relevant to the criteria specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on iconicity in sign language lexicons and gestures and does not involve audio/speech tokenization or discrete audio representations as defined in the inclusion criteria. It neither analyzes discrete tokenization of audio signals nor addresses related speech or audio tasks relevant to the criteria specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Language from the Body",
    "abstract": "What is the role of meaning in linguistic theory? Generative linguists have severely limited the influence of meaning, claiming that language is not affected by other cognitive processes and that semantics does not influence linguistic form. Conversely, cognitivist and functionalist linguists believe that meaning pervades and motivates all levels of linguistic structure. This dispute can be resolved conclusively by evidence from signed languages. Signed languages are full of iconic linguistic items: words, inflections, and even syntactic constructions with structural similarities between their physical form and their referents' form. Iconic items can have concrete meanings and also abstract meanings through conceptual metaphors. Language from the Body rebuts the generativist linguistic theories which separate form and meaning and asserts that iconicity can only be described in a cognitivist framework where meaning can influence form.",
    "metadata": {
      "title": "Language from the Body",
      "summary": "What is the role of meaning in linguistic theory? Generative linguists have severely limited the influence of meaning, claiming that language is not affected by other cognitive processes and that semantics does not influence linguistic form. Conversely, cognitivist and functionalist linguists believe that meaning pervades and motivates all levels of linguistic structure. This dispute can be resolved conclusively by evidence from signed languages. Signed languages are full of iconic linguistic items: words, inflections, and even syntactic constructions with structural similarities between their physical form and their referents' form. Iconic items can have concrete meanings and also abstract meanings through conceptual metaphors. Language from the Body rebuts the generativist linguistic theories which separate form and meaning and asserts that iconicity can only be described in a cognitivist framework where meaning can influence form.",
      "abstract": "What is the role of meaning in linguistic theory? Generative linguists have severely limited the influence of meaning, claiming that language is not affected by other cognitive processes and that semantics does not influence linguistic form. Conversely, cognitivist and functionalist linguists believe that meaning pervades and motivates all levels of linguistic structure. This dispute can be resolved conclusively by evidence from signed languages. Signed languages are full of iconic linguistic items: words, inflections, and even syntactic constructions with structural similarities between their physical form and their referents' form. Iconic items can have concrete meanings and also abstract meanings through conceptual metaphors. Language from the Body rebuts the generativist linguistic theories which separate form and meaning and asserts that iconicity can only be described in a cognitivist framework where meaning can influence form.",
      "doi": "https://doi.org/10.1017/cbo9780511509629",
      "openalex_id": "https://openalex.org/W4247720119",
      "arxiv_id": "",
      "publication_date": "2001-02-26",
      "published": "2001-02-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses the role of meaning and iconicity in linguistic theory using signed languages; it does not propose or analyze discrete audio/speech tokenization or token-level representations, and there is no focus on speech tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses the role of meaning and iconicity in linguistic theory using signed languages; it does not propose or analyze discrete audio/speech tokenization or token-level representations, and there is no focus on speech tokenization tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the linguistic theory of meaning and iconicity in signed languages without addressing discrete audio or speech tokenization, quantization, or neural encoding relevant to speech; thus, it does not meet any inclusion criteria related to audio tokenization nor violates any exclusion criteria, but it is clearly outside the scope of audio/speech discrete tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the linguistic theory of meaning and iconicity in signed languages without addressing discrete audio or speech tokenization, quantization, or neural encoding relevant to speech; thus, it does not meet any inclusion criteria related to audio tokenization nor violates any exclusion criteria, but it is clearly outside the scope of audio/speech discrete tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The link between form and meaning in American Sign Language: Lexical processing effects.",
    "abstract": "Signed languages exploit iconicity (the transparent relationship between meaning and form) to a greater extent than spoken languages. where it is largely limited to onomatopoeia. In a picture-sign matching experiment measuring reaction times, the authors examined the potential advantage of iconicity both for 1st- and 2nd-language learners of American Sign Language (ASL). The results show that native ASL signers are faster to respond when a specific property iconically represented in a sign is made salient in the corresponding picture, thus providing evidence that a closer mapping between meaning and form can aid in lexical retrieval. While late 2nd-language learners appear to use iconicity as an aid to learning sign (R. Campbell, P. Martin, & T. White, 1992), they did not show the same facilitation effect as native ASL signers, suggesting that the task tapped into more automatic language processes. Overall, the findings suggest that completely arbitrary mappings between meaning and form may not be more advantageous in language and that, rather, arbitrariness may simply be an accident of modality.",
    "metadata": {
      "title": "The link between form and meaning in American Sign Language: Lexical processing effects.",
      "summary": "Signed languages exploit iconicity (the transparent relationship between meaning and form) to a greater extent than spoken languages. where it is largely limited to onomatopoeia. In a picture-sign matching experiment measuring reaction times, the authors examined the potential advantage of iconicity both for 1st- and 2nd-language learners of American Sign Language (ASL). The results show that native ASL signers are faster to respond when a specific property iconically represented in a sign is made salient in the corresponding picture, thus providing evidence that a closer mapping between meaning and form can aid in lexical retrieval. While late 2nd-language learners appear to use iconicity as an aid to learning sign (R. Campbell, P. Martin, & T. White, 1992), they did not show the same facilitation effect as native ASL signers, suggesting that the task tapped into more automatic language processes. Overall, the findings suggest that completely arbitrary mappings between meaning and form may not be more advantageous in language and that, rather, arbitrariness may simply be an accident of modality.",
      "abstract": "Signed languages exploit iconicity (the transparent relationship between meaning and form) to a greater extent than spoken languages. where it is largely limited to onomatopoeia. In a picture-sign matching experiment measuring reaction times, the authors examined the potential advantage of iconicity both for 1st- and 2nd-language learners of American Sign Language (ASL). The results show that native ASL signers are faster to respond when a specific property iconically represented in a sign is made salient in the corresponding picture, thus providing evidence that a closer mapping between meaning and form can aid in lexical retrieval. While late 2nd-language learners appear to use iconicity as an aid to learning sign (R. Campbell, P. Martin, & T. White, 1992), they did not show the same facilitation effect as native ASL signers, suggesting that the task tapped into more automatic language processes. Overall, the findings suggest that completely arbitrary mappings between meaning and form may not be more advantageous in language and that, rather, arbitrariness may simply be an accident of modality.",
      "doi": "https://doi.org/10.1037/a0014547",
      "openalex_id": "https://openalex.org/W2108026270",
      "arxiv_id": "",
      "publication_date": "2009-03-01",
      "published": "2009-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates ASL lexical processing and iconicity via reaction-time experiments, but it does not analyze discrete audio tokenization or token sequences; there is no discrete speech tokenization aspect, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates ASL lexical processing and iconicity via reaction-time experiments, but it does not analyze discrete audio tokenization or token sequences; there is no discrete speech tokenization aspect, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical processing and iconicity in American Sign Language without any mention of audio or speech tokenization, discrete audio token design, or analysis of audio-based representation; thus, it does not meet the inclusion criteria regarding discrete audio tokenization and falls outside the scope of audio/speech tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical processing and iconicity in American Sign Language without any mention of audio or speech tokenization, discrete audio token design, or analysis of audio-based representation; thus, it does not meet the inclusion criteria regarding discrete audio tokenization and falls outside the scope of audio/speech tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Prevalence of Poor Reading in Dutch Special Elementary Education",
    "abstract": "The relative frequency of poor readers in Dutch general elementary education (GEE) and special elementary education (SEE) and the characteristics of their reading performance were investigated using a lexical decision procedure. According to the same norms that identified 9% of students as poor readers in GEE, no less than 73% of the students in SEE were classified as poor readers. On average, the GEE poor readers were better readers than those in SEE, but the findings do not point to substantial differences in reading processes between the two reader groups. Hypotheses about the nature of the referral process that may cause this surprisingly strong relation between poor reading ability and SEE placement are advanced.",
    "metadata": {
      "title": "The Prevalence of Poor Reading in Dutch Special Elementary Education",
      "summary": "The relative frequency of poor readers in Dutch general elementary education (GEE) and special elementary education (SEE) and the characteristics of their reading performance were investigated using a lexical decision procedure. According to the same norms that identified 9% of students as poor readers in GEE, no less than 73% of the students in SEE were classified as poor readers. On average, the GEE poor readers were better readers than those in SEE, but the findings do not point to substantial differences in reading processes between the two reader groups. Hypotheses about the nature of the referral process that may cause this surprisingly strong relation between poor reading ability and SEE placement are advanced.",
      "abstract": "The relative frequency of poor readers in Dutch general elementary education (GEE) and special elementary education (SEE) and the characteristics of their reading performance were investigated using a lexical decision procedure. According to the same norms that identified 9% of students as poor readers in GEE, no less than 73% of the students in SEE were classified as poor readers. On average, the GEE poor readers were better readers than those in SEE, but the findings do not point to substantial differences in reading processes between the two reader groups. Hypotheses about the nature of the referral process that may cause this surprisingly strong relation between poor reading ability and SEE placement are advanced.",
      "doi": "https://doi.org/10.1177/00222194060390060101",
      "openalex_id": "https://openalex.org/W2106794954",
      "arxiv_id": "",
      "publication_date": "2006-11-01",
      "published": "2006-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study concerns reading prevalence and uses a lexical decision task; it does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations relevant to speech models, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study concerns reading prevalence and uses a lexical decision task; it does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations relevant to speech models, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the prevalence of poor reading in Dutch special elementary education using a lexical decision procedure, focusing on reading performance characteristics, without any mention or analysis of discrete audio/speech tokenization or related audio signal processing methods, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the prevalence of poor reading in Dutch special elementary education using a lexical decision procedure, focusing on reading performance characteristics, without any mention or analysis of discrete audio/speech tokenization or related audio signal processing methods, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Units in the analysis of signs",
    "abstract": "The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on ‘spoken phonetics’.",
    "metadata": {
      "title": "Units in the analysis of signs",
      "summary": "The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on ‘spoken phonetics’.",
      "abstract": "The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on ‘spoken phonetics’.",
      "doi": "https://doi.org/10.1017/s095267570000004x",
      "openalex_id": "https://openalex.org/W2117522905",
      "arxiv_id": "",
      "publication_date": "1993-08-01",
      "published": "1993-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses sign-language structure and representation theory rather than discrete audio tokenization or token-level discretization; it does not propose, analyze, or evaluate discrete audio tokens for reconstruction or language tasks, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses sign-language structure and representation theory rather than discrete audio tokenization or token-level discretization; it does not propose, analyze, or evaluate discrete audio tokens for reconstruction or language tasks, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses phonological theories related to sign languages rather than audio or speech tokenization involving discrete audio signals, and does not address conversion of continuous audio into discrete tokens, or neural quantization relevant to speech tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses phonological theories related to sign languages rather than audio or speech tokenization involving discrete audio signals, and does not address conversion of continuous audio into discrete tokens, or neural quantization relevant to speech tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Cognitive Neuropsychological Approach to Assessment and Intervention in Aphasia",
    "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
    "metadata": {
      "title": "A Cognitive Neuropsychological Approach to Assessment and Intervention in Aphasia",
      "summary": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "doi": "https://doi.org/10.4324/9781315852447",
      "openalex_id": "https://openalex.org/W4254658607",
      "arxiv_id": "",
      "publication_date": "2014-01-03",
      "published": "2014-01-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a clinical book on cognitive neuropsychological assessment/intervention for aphasia and does not address discrete audio/speech tokenization or token-level representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a clinical book on cognitive neuropsychological assessment/intervention for aphasia and does not address discrete audio/speech tokenization or token-level representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a clinical and theoretical resource for assessment and intervention in aphasia using cognitive neuropsychological approaches, with no mention of audio/speech tokenization or discrete token analysis relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a clinical and theoretical resource for assessment and intervention in aphasia using cognitive neuropsychological approaches, with no mention of audio/speech tokenization or discrete token analysis relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A cognitive neuropsychological approach to assessment and intervention in aphasia: A clinician's guide",
    "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
    "metadata": {
      "title": "A cognitive neuropsychological approach to assessment and intervention in aphasia: A clinician's guide",
      "summary": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. \n\nThe opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention.\n\nThis new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. \n\nThe book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. \n\nThe opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention.\n\nThis new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. \n\nThe book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "doi": "https://doi.org/10.1080/02687030600742046",
      "openalex_id": "https://openalex.org/W1549850236",
      "arxiv_id": "",
      "publication_date": "2008-01-11",
      "published": "2008-01-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a clinical book on aphasia assessment/intervention; it does not propose or evaluate discrete audio/speech tokenization or discrete token representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a clinical book on aphasia assessment/intervention; it does not propose or evaluate discrete audio/speech tokenization or discrete token representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study described is a clinical guide focused on aphasia assessment and intervention from a cognitive neuropsychological perspective; it does not mention any work on discrete audio/speech tokenization, neural encoding, or quantitative methods related to audio signal tokenization, thus it does not meet the inclusion criteria for studies on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study described is a clinical guide focused on aphasia assessment and intervention from a cognitive neuropsychological perspective; it does not mention any work on discrete audio/speech tokenization, neural encoding, or quantitative methods related to audio signal tokenization, thus it does not meet the inclusion criteria for studies on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Access to the Internal Lexicon",
    "abstract": "In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the \"no\" response latency, but not the \"yes\" response latency, and it is argued that this result favors the view that lexical access is \"direct,\" rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say \"no\" to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.",
    "metadata": {
      "title": "Access to the Internal Lexicon",
      "summary": "In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the \"no\" response latency, but not the \"yes\" response latency, and it is argued that this result favors the view that lexical access is \"direct,\" rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say \"no\" to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.",
      "abstract": "In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the \"no\" response latency, but not the \"yes\" response latency, and it is argued that this result favors the view that lexical access is \"direct,\" rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say \"no\" to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.",
      "doi": "https://doi.org/10.4324/9781003309734-29",
      "openalex_id": "https://openalex.org/W1519165391",
      "arxiv_id": "",
      "publication_date": "2022-05-19",
      "published": "2022-05-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates lexical access in reading and phonology using a visual lexical-decision task and does not address discrete audio tokenization or token-based representations; it lacks any discussion of converting continuous audio into discrete tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates lexical access in reading and phonology using a visual lexical-decision task and does not address discrete audio tokenization or token-based representations; it lacks any discussion of converting continuous audio into discrete tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical access in reading and cognitive processing of printed words, with experiments involving lexical-decision tasks; it does not discuss audio or speech tokenization, discrete audio signals, or any form of neural encoding or quantization of audio relevant to language-model-style token sequences, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical access in reading and cognitive processing of printed words, with experiments involving lexical-decision tasks; it does not discuss audio or speech tokenization, discrete audio signals, or any form of neural encoding or quantization of audio relevant to language-model-style token sequences, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phonological priming in British Sign Language",
    "abstract": "Models of lexical access seek to explain how incoming language data is mapped onto long-term lexical representations. The experiment reported here aims to provide insight into which elements of language input are used for mapping onto a sign language lexicon. Rather than using the organs of the vocal tract, sign languages use the arms, hands, body and face to create meaning, combining handshapes, locations and movements to create meaningful words (signs). This study aims to determine whether these parameters are also used in lexical access processes. Twelve deaf native and twelve deaf non-native signers of British Sign Language (BSL) were presented with a primed lexical decision task. They were required to make a lexical decision about a target sign after viewing a preceding prime that was phonologically related to the target. Analysis of the data suggests that native signers use phonological information in signs in order to access their mental lexicon. Moreover, it appears that the salient parameter in the input is a combination of location and movement – only when both these parameters are shared by prime and target is facilitatory priming observed. There was no evidence that non-native (deaf) signers used phonological parameters to access their lexicon, despite a high degree of success in the lexical decision task at a speed comparable to that of native signers. These findings are discussed in relation to sign language acquisition and the development of phonological theories of signed languages.",
    "metadata": {
      "title": "Phonological priming in British Sign Language",
      "summary": "Models of lexical access seek to explain how incoming language data is mapped onto long-term lexical representations. The experiment reported here aims to provide insight into which elements of language input are used for mapping onto a sign language lexicon. Rather than using the organs of the vocal tract, sign languages use the arms, hands, body and face to create meaning, combining handshapes, locations and movements to create meaningful words (signs). This study aims to determine whether these parameters are also used in lexical access processes. Twelve deaf native and twelve deaf non-native signers of British Sign Language (BSL) were presented with a primed lexical decision task. They were required to make a lexical decision about a target sign after viewing a preceding prime that was phonologically related to the target. Analysis of the data suggests that native signers use phonological information in signs in order to access their mental lexicon. Moreover, it appears that the salient parameter in the input is a combination of location and movement – only when both these parameters are shared by prime and target is facilitatory priming observed. There was no evidence that non-native (deaf) signers used phonological parameters to access their lexicon, despite a high degree of success in the lexical decision task at a speed comparable to that of native signers. These findings are discussed in relation to sign language acquisition and the development of phonological theories of signed languages.",
      "abstract": "Models of lexical access seek to explain how incoming language data is mapped onto long-term lexical representations. The experiment reported here aims to provide insight into which elements of language input are used for mapping onto a sign language lexicon. Rather than using the organs of the vocal tract, sign languages use the arms, hands, body and face to create meaning, combining handshapes, locations and movements to create meaningful words (signs). This study aims to determine whether these parameters are also used in lexical access processes. Twelve deaf native and twelve deaf non-native signers of British Sign Language (BSL) were presented with a primed lexical decision task. They were required to make a lexical decision about a target sign after viewing a preceding prime that was phonologically related to the target. Analysis of the data suggests that native signers use phonological information in signs in order to access their mental lexicon. Moreover, it appears that the salient parameter in the input is a combination of location and movement – only when both these parameters are shared by prime and target is facilitatory priming observed. There was no evidence that non-native (deaf) signers used phonological parameters to access their lexicon, despite a high degree of success in the lexical decision task at a speed comparable to that of native signers. These findings are discussed in relation to sign language acquisition and the development of phonological theories of signed languages.",
      "doi": "https://doi.org/10.1515/9783110197211.1.241",
      "openalex_id": "https://openalex.org/W129019996",
      "arxiv_id": "",
      "publication_date": "2006-06-20",
      "published": "2006-06-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates phonological processing in sign language and lexical access using primed lexical decisions, focusing on sign parameters rather than discrete audio tokenization; it does not analyze converting continuous audio into discrete speech tokens, so it fails the inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates phonological processing in sign language and lexical access using primed lexical decisions, focusing on sign parameters rather than discrete audio tokenization; it does not analyze converting continuous audio into discrete speech tokens, so it fails the inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological priming in British Sign Language and the mental lexicon access of signers, without addressing discrete audio/speech tokenization or any method of converting continuous audio signals into discrete tokens; the research centers on sign language parameters rather than audio or speech tokenization, thus not meeting the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological priming in British Sign Language and the mental lexicon access of signers, without addressing discrete audio/speech tokenization or any method of converting continuous audio signals into discrete tokens; the research centers on sign language parameters rather than audio or speech tokenization, thus not meeting the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Signs of Language",
    "abstract": "Introduction PART I: The Two Faces of Sign 1. Iconicity in Signs and Signing 2. Properties of Symbols in a Silent Language 3. Historical Change: From Iconic to Arbitrary PART II: The Structure of the Sign 4. Remembering without Words: Manual Memory 5. Slips of the Hands 6. A Comparison of Chinese and American Signs 7. A Feature Analysis of Handshapes 8. The Rate of Speaking and Signing PART III: Grammatical Processes 9. On the Creation of New Lexical Items by Compounding 10. Linguistic Expression of Category Levels 11. Aspectual Modulations on Adjectival Predicates 12. The Structured Use of Space and Movement: Morphological Processes PART IV: The Heightened Use of Language 13. Wit and Plays on Signs 14. Poetry and Song in a Language without Sound Appendix A: Notation Appendix B: Conventions Employed in Illustrations Notes References Index",
    "metadata": {
      "title": "The Signs of Language",
      "summary": "Introduction PART I: The Two Faces of Sign 1. Iconicity in Signs and Signing 2. Properties of Symbols in a Silent Language 3. Historical Change: From Iconic to Arbitrary PART II: The Structure of the Sign 4. Remembering without Words: Manual Memory 5. Slips of the Hands 6. A Comparison of Chinese and American Signs 7. A Feature Analysis of Handshapes 8. The Rate of Speaking and Signing PART III: Grammatical Processes 9. On the Creation of New Lexical Items by Compounding 10. Linguistic Expression of Category Levels 11. Aspectual Modulations on Adjectival Predicates 12. The Structured Use of Space and Movement: Morphological Processes PART IV: The Heightened Use of Language 13. Wit and Plays on Signs 14. Poetry and Song in a Language without Sound Appendix A: Notation Appendix B: Conventions Employed in Illustrations Notes References Index",
      "abstract": "Introduction PART I: The Two Faces of Sign 1. Iconicity in Signs and Signing 2. Properties of Symbols in a Silent Language 3. Historical Change: From Iconic to Arbitrary PART II: The Structure of the Sign 4. Remembering without Words: Manual Memory 5. Slips of the Hands 6. A Comparison of Chinese and American Signs 7. A Feature Analysis of Handshapes 8. The Rate of Speaking and Signing PART III: Grammatical Processes 9. On the Creation of New Lexical Items by Compounding 10. Linguistic Expression of Category Levels 11. Aspectual Modulations on Adjectival Predicates 12. The Structured Use of Space and Movement: Morphological Processes PART IV: The Heightened Use of Language 13. Wit and Plays on Signs 14. Poetry and Song in a Language without Sound Appendix A: Notation Appendix B: Conventions Employed in Illustrations Notes References Index",
      "doi": "https://doi.org/10.2307/413507",
      "openalex_id": "https://openalex.org/W1980502471",
      "arxiv_id": "",
      "publication_date": "1980-12-01",
      "published": "1980-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses sign language linguistics (signs, grammar, and structure) and does not propose or analyze discrete audio tokenization into a finite vocabulary for audio reconstruction/understanding, so it should be excluded (score = 1).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses sign language linguistics (signs, grammar, and structure) and does not propose or analyze discrete audio tokenization into a finite vocabulary for audio reconstruction/understanding, so it should be excluded (score = 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a work focused on the linguistic and structural aspects of sign language, without any mention of audio or speech tokenization, discrete symbol sequences derived from continuous audio signals, or relevant modern quantization approaches; thus, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a work focused on the linguistic and structural aspects of sign language, without any mention of audio or speech tokenization, discrete symbol sequences derived from continuous audio signals, or relevant modern quantization approaches; thus, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Prosodic Model of Sign Language Phonology",
    "abstract": "Superior to any other book on the subject that I have seen. I can see it being used as a class text or reference for current theory in sign language phonology. This book is intended in part to provide linguists and cognitive scientists who do not know sign language with a point of entry into the study of sign language phonology. At the same time, it presents a comprehensive theory of American Sign Language (ASL) phonology, while reviewing and building on alternative theories. One claim of this theoretical framework is that, because of sign language's visual/gestural phonetic basis, the consonant-like units and vowel-like units are expressed simultaneously with one another, rather than sequentially as in spoken languages. A second claim is that movements operate as the most basic prosodic units of the language. The author is concerned to show both the similarities and differences between signed and spoken languages, and to indicate some directions for future work in cognitive science that can be derived from her phonological model. Bradford Books imprint",
    "metadata": {
      "title": "A Prosodic Model of Sign Language Phonology",
      "summary": "Superior to any other book on the subject that I have seen. I can see it being used as a class text or reference for current theory in sign language phonology. This book is intended in part to provide linguists and cognitive scientists who do not know sign language with a point of entry into the study of sign language phonology. At the same time, it presents a comprehensive theory of American Sign Language (ASL) phonology, while reviewing and building on alternative theories. One claim of this theoretical framework is that, because of sign language's visual/gestural phonetic basis, the consonant-like units and vowel-like units are expressed simultaneously with one another, rather than sequentially as in spoken languages. A second claim is that movements operate as the most basic prosodic units of the language. The author is concerned to show both the similarities and differences between signed and spoken languages, and to indicate some directions for future work in cognitive science that can be derived from her phonological model. Bradford Books imprint",
      "abstract": "Superior to any other book on the subject that I have seen. I can see it being used as a class text or reference for current theory in sign language phonology. This book is intended in part to provide linguists and cognitive scientists who do not know sign language with a point of entry into the study of sign language phonology. At the same time, it presents a comprehensive theory of American Sign Language (ASL) phonology, while reviewing and building on alternative theories. One claim of this theoretical framework is that, because of sign language's visual/gestural phonetic basis, the consonant-like units and vowel-like units are expressed simultaneously with one another, rather than sequentially as in spoken languages. A second claim is that movements operate as the most basic prosodic units of the language. The author is concerned to show both the similarities and differences between signed and spoken languages, and to indicate some directions for future work in cognitive science that can be derived from her phonological model. Bradford Books imprint",
      "doi": "https://doi.org/10.7551/mitpress/5644.001.0001",
      "openalex_id": "https://openalex.org/W1566439858",
      "arxiv_id": "",
      "publication_date": "1999-02-16",
      "published": "1999-02-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a prosodic model of sign language phonology and gestural prosody, with no explicit discussion of discrete audio/tokenization of speech; it does not propose/analyze discrete audio tokens or their sequences, failing the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a prosodic model of sign language phonology and gestural prosody, with no explicit discussion of discrete audio/tokenization of speech; it does not propose/analyze discrete audio tokens or their sequences, failing the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language phonology, discussing theoretical frameworks for American Sign Language and prosodic units, without addressing discrete audio or speech tokenization, nor analyzing discrete audio representations or neural quantization methods as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language phonology, discussing theoretical frameworks for American Sign Language and prosodic units, without addressing discrete audio or speech tokenization, nor analyzing discrete audio representations or neural quantization methods as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Verbal and signed languages : comparing structures, constructs and methodologies",
    "abstract": "This book is the first to explore how much of knowledge based on research on spoken languages needs to be refined in the light of the growing field of sign linguistics. Drawing upon a broad cross-linguistic perspective, the contributors focus on topics of general theoretical interest: linearity and arbitrariness principles, definition of units and levels of analysis, expression of grammatical categories, semantic relations, and cohesion mechanisms. The book is of interest to language typologists, theoretical and descriptive linguists, scholars in the fields of semiotics, anthropology, gesture studies, and cognitive sciences at large.",
    "metadata": {
      "title": "Verbal and signed languages : comparing structures, constructs and methodologies",
      "summary": "This book is the first to explore how much of knowledge based on research on spoken languages needs to be refined in the light of the growing field of sign linguistics. Drawing upon a broad cross-linguistic perspective, the contributors focus on topics of general theoretical interest: linearity and arbitrariness principles, definition of units and levels of analysis, expression of grammatical categories, semantic relations, and cohesion mechanisms. The book is of interest to language typologists, theoretical and descriptive linguists, scholars in the fields of semiotics, anthropology, gesture studies, and cognitive sciences at large.",
      "abstract": "This book is the first to explore how much of knowledge based on research on spoken languages needs to be refined in the light of the growing field of sign linguistics. Drawing upon a broad cross-linguistic perspective, the contributors focus on topics of general theoretical interest: linearity and arbitrariness principles, definition of units and levels of analysis, expression of grammatical categories, semantic relations, and cohesion mechanisms. The book is of interest to language typologists, theoretical and descriptive linguists, scholars in the fields of semiotics, anthropology, gesture studies, and cognitive sciences at large.",
      "doi": "",
      "openalex_id": "https://openalex.org/W614258797",
      "arxiv_id": "",
      "publication_date": "2007-01-01",
      "published": "2007-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract is about sign languages and general theory, not about discrete audio/speech tokenization into tokens for reconstruction/generation/understanding tasks; it does not meet inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract is about sign languages and general theory, not about discrete audio/speech tokenization into tokens for reconstruction/generation/understanding tasks; it does not meet inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate that the book focuses on linguistic comparison between verbal and signed languages, analyzing theoretical linguistic constructs rather than proposing or analyzing discrete audio/speech tokenization or neural discrete representation methods; therefore, it does not meet the inclusion criteria related to discrete tokenization of continuous audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate that the book focuses on linguistic comparison between verbal and signed languages, analyzing theoretical linguistic constructs rather than proposing or analyzing discrete audio/speech tokenization or neural discrete representation methods; therefore, it does not meet the inclusion criteria related to discrete tokenization of continuous audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
    "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
    "metadata": {
      "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
      "summary": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446933",
      "openalex_id": "https://openalex.org/W4392931281",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work focuses on multimodal inputs including speech and sentiment, it does not propose or analyze discrete audio/token quantization into a finite vocabulary; it relies on continuous speech embeddings, so it does not meet the discrete audio tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the work focuses on multimodal inputs including speech and sentiment, it does not propose or analyze discrete audio/token quantization into a finite vocabulary; it relies on continuous speech embeddings, so it does not meet the discrete audio tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multimodal LLM incorporating text and speech embeddings to improve spoken dialogue understanding, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; it uses speech embeddings without clear evidence of discrete quantization or token design as a core focus, thus not meeting the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multimodal LLM incorporating text and speech embeddings to improve spoken dialogue understanding, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; it uses speech embeddings without clear evidence of discrete quantization or token design as a core focus, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
    "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
    "metadata": {
      "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
      "summary": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094723",
      "openalex_id": "https://openalex.org/W4372259964",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on learning disentangled speech features for real-time neural coding and cites VQ-VAE, which uses discrete latent codes; however, it does not clearly state that discrete tokenization is the core object of the study—its contribution appears to be disentangled feature representations with potential tokenization historically via VQ-VAE, but this is not explicitly framed as the main focus.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on learning disentangled speech features for real-time neural coding and cites VQ-VAE, which uses discrete latent codes; however, it does not clearly state that discrete tokenization is the core object of the study—its contribution appears to be disentangled feature representations with potential tokenization historically via VQ-VAE, but this is not explicitly framed as the main focus.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes learning disentangled features for real-time neural speech coding following a VQ-VAE paradigm, which involves vector quantization and discrete feature representation; hence it explicitly involves discrete audio tokenization relevant to speech coding and transformation tasks, meeting the inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes learning disentangled features for real-time neural speech coding following a VQ-VAE paradigm, which involves vector quantization and discrete feature representation; hence it explicitly involves discrete audio tokenization relevant to speech coding and transformation tasks, meeting the inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly references the VQ-VAE paradigm and vector-quantization for neural speech coding and proposes learning disentangled (discrete-quantized) feature representations for real-time coding and editing, so discrete tokenization/quantization is a core object of study (the junior reviewers disagreed—one uncertain about explicitness, the other included—but the explicit mention of vector quantization and coding in the abstract supports inclusion).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly references the VQ-VAE paradigm and vector-quantization for neural speech coding and proposes learning disentangled (discrete-quantized) feature representations for real-time coding and editing, so discrete tokenization/quantization is a core object of study (the junior reviewers disagreed—one uncertain about explicitness, the other included—but the explicit mention of vector quantization and coding in the abstract supports inclusion).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Textless Speech-to-Speech Translation on Real Data",
    "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "metadata": {
      "title": "Textless Speech-to-Speech Translation on Real Data",
      "summary": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.63",
      "openalex_id": "https://openalex.org/W4287854499",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract text is not available, only the title indicates textless S2S translation; it is unclear whether discrete audio tokenization is a core focus of the study, so we cannot confirm it meets the inclusion criteria requiring explicit discrete tokenization analysis. ",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract text is not available, only the title indicates textless S2S translation; it is unclear whether discrete audio tokenization is a core focus of the study, so we cannot confirm it meets the inclusion criteria requiring explicit discrete tokenization analysis. ",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract mention a speech-to-speech translation study but do not explicitly indicate that the work focuses on discrete audio tokenization or analyze discrete token representations as core components; without clear evidence of discrete audio tokenization being a central methodological feature, the inclusion criteria are not clearly met.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The title and abstract mention a speech-to-speech translation study but do not explicitly indicate that the work focuses on discrete audio tokenization or analyze discrete token representations as core components; without clear evidence of discrete audio tokenization being a central methodological feature, the inclusion criteria are not clearly met.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly note the provided item lacks an abstract/body text; while the title strongly suggests a textless S2S approach (which in related work often uses discrete speech units), there is no explicit evidence here that the paper proposes, analyzes, or evaluates discrete audio tokenization as the core object—therefore inclusion cannot be confirmed without the full text.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly note the provided item lacks an abstract/body text; while the title strongly suggests a textless S2S approach (which in related work often uses discrete speech units), there is no explicit evidence here that the paper proposes, analyzes, or evaluates discrete audio tokenization as the core object—therefore inclusion cannot be confirmed without the full text.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Direct Speech-to-Speech Translation With Discrete Units",
    "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "metadata": {
      "title": "Direct Speech-to-Speech Translation With Discrete Units",
      "summary": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.235",
      "openalex_id": "https://openalex.org/W3180374548",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper's title indicates use of discrete speech units as the core representation for speech-to-speech translation, aligning with the inclusion criteria on discrete audio tokenization; there is no evidence it is merely continuous features or a non-speech tokenization issue, and it is presumably in English as an ACL long paper.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper's title indicates use of discrete speech units as the core representation for speech-to-speech translation, aligning with the inclusion criteria on discrete audio tokenization; there is no evidence it is merely continuous features or a non-speech tokenization issue, and it is presumably in English as an ACL long paper.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "Based on the title, the study likely involves discrete units in speech translation, which matches the inclusion criteria of discrete audio tokenization for speech tasks; the source is a reputable conference proceeding in English, and there is no indication that the study falls into exclusion criteria; however, without access to the full text, absolute certainty is limited.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "Based on the title, the study likely involves discrete units in speech translation, which matches the inclusion criteria of discrete audio tokenization for speech tasks; the source is a reputable conference proceeding in English, and there is no indication that the study falls into exclusion criteria; however, without access to the full text, absolute certainty is limited.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
    "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "metadata": {
      "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
      "summary": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.872",
      "openalex_id": "https://openalex.org/W4385570550",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title 'UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units' explicitly centers on discrete speech tokens as the core representation in a speech-to-speech translation task, aligning with the inclusion criteria for discrete audio tokenization; it's an ACL 2023 paper (English, publicly evaluable form), so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title 'UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units' explicitly centers on discrete speech tokens as the core representation in a speech-to-speech translation task, aligning with the inclusion criteria for discrete audio tokenization; it's an ACL 2023 paper (English, publicly evaluable form), so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" suggests it deals with discrete units in speech-to-speech translation, implying discrete audio tokenization is a core object of study; it is published in ACL 2023, a reputable venue, ensuring the full text is in English and academically accessible. There is no indication that it only uses continuous features or focuses on traditional codecs, and given the title and venue, it likely involves methodological contributions or analysis related to discrete speech tokens rather than purely engineering or unrelated tokenization. Therefore, it meets the inclusion criteria without evident exclusion criteria triggers.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper titled \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" suggests it deals with discrete units in speech-to-speech translation, implying discrete audio tokenization is a core object of study; it is published in ACL 2023, a reputable venue, ensuring the full text is in English and academically accessible. There is no indication that it only uses continuous features or focuses on traditional codecs, and given the title and venue, it likely involves methodological contributions or analysis related to discrete speech tokens rather than purely engineering or unrelated tokenization. Therefore, it meets the inclusion criteria without evident exclusion criteria triggers.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
    "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
    "metadata": {
      "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
      "summary": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9052942",
      "openalex_id": "https://openalex.org/W2995181338",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a large ASR dataset and evaluation under limited supervision, but does not propose or analyze discrete speech tokenization (discrete audio tokens) as a core object, thus failing the tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a large ASR dataset and evaluation under limited supervision, but does not propose or analyze discrete speech tokenization (discrete audio tokens) as a core object, thus failing the tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a large speech corpus and evaluates ASR systems under limited supervision, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods converting continuous audio into discrete symbols, which is required by the inclusion criteria; it mainly focuses on ASR benchmarking using speech audio and aligns text resources, thus it does not meet the core requirement for discrete tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces a large speech corpus and evaluates ASR systems under limited supervision, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods converting continuous audio into discrete symbols, which is required by the inclusion criteria; it mainly focuses on ASR benchmarking using speech audio and aligns text resources, thus it does not meet the core requirement for discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Looking to listen at the cocktail party",
    "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
    "metadata": {
      "title": "Looking to listen at the cocktail party",
      "summary": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "doi": "https://doi.org/10.1145/3197517.3201357",
      "openalex_id": "https://openalex.org/W4289665794",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents an audio-visual speech separation model but does not discuss discrete audio tokenization or token quantization as a core method, thus it fails the inclusion criterion focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents an audio-visual speech separation model but does not discuss discrete audio tokenization or token quantization as a core method, thus it fails the inclusion criterion focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a joint audio-visual speech separation model focusing on isolating speech from mixed audio signals but does not mention any discrete audio/speech tokenization methods or analysis; it mainly deals with continuous audio features and visual signals for speech separation without proposing or evaluating discrete token sequences, which is a core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a joint audio-visual speech separation model focusing on isolating speech from mixed audio signals but does not mention any discrete audio/speech tokenization methods or analysis; it mainly deals with continuous audio features and visual signals for speech separation without proposing or evaluating discrete token sequences, which is a core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
    "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
    "metadata": {
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "summary": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2826",
      "openalex_id": "https://openalex.org/W3095410713",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a multilingual speech dataset and baseline ASR models but does not address discrete audio tokenization, so it does not meet the tokenization criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a multilingual speech dataset and baseline ASR models but does not address discrete audio tokenization, so it does not meet the tokenization criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a large-scale multilingual speech dataset and baseline ASR and language models but does not explicitly discuss any discrete audio tokenization methods, analysis, or evaluation; it mainly provides a dataset and baseline models without methodological contribution on discrete token design or neural quantization, thus not meeting the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a large-scale multilingual speech dataset and baseline ASR and language models but does not explicitly discuss any discrete audio tokenization methods, analysis, or evaluation; it mainly provides a dataset and baseline models without methodological contribution on discrete token design or neural quantization, thus not meeting the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
    "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "summary": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "doi": "https://doi.org/10.5281/zenodo.1188976",
      "openalex_id": "https://openalex.org/W3081192838",
      "arxiv_id": "",
      "publication_date": "2018-04-05",
      "published": "2018-04-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a dataset description (RAVDESS) and does not propose or analyze discrete audio tokenization or a token vocabulary, thus it does not meet the inclusion criteria and falls under the exclusion criteria for tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a dataset description (RAVDESS) and does not propose or analyze discrete audio tokenization or a token vocabulary, thus it does not meet the inclusion criteria and falls under the exclusion criteria for tokenization research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes the RAVDESS dataset containing emotional speech and song with detailed metadata but does not propose, analyze, or evaluate any discrete audio/speech tokenization method; instead, it is a resource description without methodological contribution on discrete tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes the RAVDESS dataset containing emotional speech and song with detailed metadata but does not propose, analyze, or evaluate any discrete audio/speech tokenization method; instead, it is a resource description without methodological contribution on discrete tokenization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
    "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
    "metadata": {
      "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
      "summary": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683277",
      "openalex_id": "https://openalex.org/W2935711438",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses VQ-VAE with a discrete latent (quantized) representation for speech coding, i.e., explicit discrete audio tokens, aligning with the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work uses VQ-VAE with a discrete latent (quantized) representation for speech coding, i.e., explicit discrete audio tokens, aligning with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network-based low bit-rate speech coding method using VQ-VAE, which inherently involves vector quantization—a form of discrete tokenization of speech signal into discrete codes for reconstruction, fitting the inclusion criteria; it is not a traditional codec and includes experimental validation relevant to discrete speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network-based low bit-rate speech coding method using VQ-VAE, which inherently involves vector quantization—a form of discrete tokenization of speech signal into discrete codes for reconstruction, fitting the inclusion criteria; it is not a traditional codec and includes experimental validation relevant to discrete speech tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
    "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
    "metadata": {
      "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "summary": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "doi": "https://doi.org/10.1109/icassp.2009.4960497",
      "openalex_id": "https://openalex.org/W2107740512",
      "arxiv_id": "",
      "publication_date": "2009-04-01",
      "published": "2009-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work addresses F0 tracking and voicing classification using continuous audio features and evaluation metrics, without proposing or analyzing a discrete audio tokenization scheme (i.e., converting audio into a finite vocabulary of discrete tokens for speech reconstruction/understanding). Thus it does not meet the inclusion criteria focused on discrete tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work addresses F0 tracking and voicing classification using continuous audio features and evaluation metrics, without proposing or analyzing a discrete audio tokenization scheme (i.e., converting audio into a finite vocabulary of discrete tokens for speech reconstruction/understanding). Thus it does not meet the inclusion criteria focused on discrete tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving F0 tracking under noisy conditions using a voiced/unvoiced classification frontend but does not discuss discrete audio/speech tokenization or any form of discrete quantization/token design that converts continuous audio into discrete symbols for reconstruction, generation, or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving F0 tracking under noisy conditions using a voiced/unvoiced classification frontend but does not discuss discrete audio/speech tokenization or any form of discrete quantization/token design that converts continuous audio into discrete symbols for reconstruction, generation, or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
    "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
    "metadata": {
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "summary": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "doi": "https://doi.org/10.21437/interspeech.2022-439",
      "openalex_id": "https://openalex.org/W4225956675",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on predicting MOS from SSL/continuous audio features with ensemble learning and does not involve any discrete audio tokenization or token design, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on predicting MOS from SSL/continuous audio features with ensemble learning and does not involve any discrete audio tokenization or token design, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting MOS values using ensemble learning and SSL features but does not mention proposing, analyzing, or evaluating discrete audio/speech tokenization or discrete token representations as required; it appears to rely on continuous features and lacks methodological contributions on discrete tokenization, thus not meeting inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting MOS values using ensemble learning and SSL features but does not mention proposing, analyzing, or evaluating discrete audio/speech tokenization or discrete token representations as required; it appears to rely on continuous features and lacks methodological contributions on discrete tokenization, thus not meeting inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
    "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
    "metadata": {
      "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
      "summary": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "doi": "https://doi.org/10.1109/qomex48832.2020.9123150",
      "openalex_id": "https://openalex.org/W3037038648",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns ViSQOL v3, an objective speech/audio quality metric paper; it does not propose or analyze discrete tokenization of audio into a finite vocabulary, and instead relies on continuous audio representations for quality assessment, so it does not meet the inclusion criteria and should be excluded under the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns ViSQOL v3, an objective speech/audio quality metric paper; it does not propose or analyze discrete tokenization of audio into a finite vocabulary, and instead relies on continuous audio representations for quality assessment, so it does not meet the inclusion criteria and should be excluded under the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an open-source objective speech and audio metric presented at a conference, but do not mention any discrete audio/speech tokenization, quantization, or token design, nor do they imply methodological contribution or analysis related to discrete audio tokens; thus, the study does not meet the inclusion criteria and falls under the exclusion criteria of engineering-only reports without methodological contribution to discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an open-source objective speech and audio metric presented at a conference, but do not mention any discrete audio/speech tokenization, quantization, or token design, nor do they imply methodological contribution or analysis related to discrete audio tokens; thus, the study does not meet the inclusion criteria and falls under the exclusion criteria of engineering-only reports without methodological contribution to discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluation of Objective Quality Measures for Speech Enhancement",
    "abstract": "In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques.",
    "metadata": {
      "title": "Evaluation of Objective Quality Measures for Speech Enhancement",
      "summary": "In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques.",
      "abstract": "In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques.",
      "doi": "https://doi.org/10.1109/tasl.2007.911054",
      "openalex_id": "https://openalex.org/W2144404214",
      "arxiv_id": "",
      "publication_date": "2007-12-20",
      "published": "2007-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study evaluates objective quality measures for speech enhancement using continuous audio features and does not involve discrete audio tokenization into tokens, hence it does not meet the tokenization-focused inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study evaluates objective quality measures for speech enhancement using continuous audio features and does not involve discrete audio tokenization into tokens, hence it does not meet the tokenization-focused inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating objective quality measures for speech enhancement algorithms using subjective quality ratings; it does not propose or analyze discrete audio/speech tokenization involving converting continuous audio into discrete symbols, nor does it involve neural or modern quantization-based token design relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating objective quality measures for speech enhancement algorithms using subjective quality ratings; it does not propose or analyze discrete audio/speech tokenization involving converting continuous audio into discrete symbols, nor does it involve neural or modern quantization-based token design relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence",
    "abstract": "Over the past few decades, computational methods have been developed to\\nestimate perceptual audio quality. These methods, also referred to as objective\\nquality measures, are usually developed and intended for a specific application\\ndomain. Because of their convenience, they are often used outside their\\noriginal intended domain, even if it is unclear whether they provide reliable\\nquality estimates in this case. This work studies the correlation of well-known\\nstate-of-the-art objective measures with human perceptual scores in two\\ndifferent domains: audio coding and source separation. The following objective\\nmeasures are considered: fwSNRseg, dLLR, PESQ, PEAQ, POLQA, PEMO-Q,\\nViSQOLAudio, (SI-)BSSEval, PEASS, LKR-PI, 2f-model, and HAAQI. Additionally, a\\nnovel measure (SI-SA2f) is presented, based on the 2f-model and a BSSEval-based\\nsignal decomposition. We use perceptual scores from 7 listening tests about\\naudio coding and 7 listening tests about source separation as ground-truth data\\nfor the correlation analysis. The results show that one method (2f-model)\\nperforms significantly better than the others on both domains and indicate that\\nthe dataset for training the method and a robust underlying auditory model are\\ncrucial factors towards a universal, domain-independent objective measure.\\n",
    "metadata": {
      "title": "Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence",
      "summary": "Over the past few decades, computational methods have been developed to\\nestimate perceptual audio quality. These methods, also referred to as objective\\nquality measures, are usually developed and intended for a specific application\\ndomain. Because of their convenience, they are often used outside their\\noriginal intended domain, even if it is unclear whether they provide reliable\\nquality estimates in this case. This work studies the correlation of well-known\\nstate-of-the-art objective measures with human perceptual scores in two\\ndifferent domains: audio coding and source separation. The following objective\\nmeasures are considered: fwSNRseg, dLLR, PESQ, PEAQ, POLQA, PEMO-Q,\\nViSQOLAudio, (SI-)BSSEval, PEASS, LKR-PI, 2f-model, and HAAQI. Additionally, a\\nnovel measure (SI-SA2f) is presented, based on the 2f-model and a BSSEval-based\\nsignal decomposition. We use perceptual scores from 7 listening tests about\\naudio coding and 7 listening tests about source separation as ground-truth data\\nfor the correlation analysis. The results show that one method (2f-model)\\nperforms significantly better than the others on both domains and indicate that\\nthe dataset for training the method and a robust underlying auditory model are\\ncrucial factors towards a universal, domain-independent objective measure.\\n",
      "abstract": "Over the past few decades, computational methods have been developed to\\nestimate perceptual audio quality. These methods, also referred to as objective\\nquality measures, are usually developed and intended for a specific application\\ndomain. Because of their convenience, they are often used outside their\\noriginal intended domain, even if it is unclear whether they provide reliable\\nquality estimates in this case. This work studies the correlation of well-known\\nstate-of-the-art objective measures with human perceptual scores in two\\ndifferent domains: audio coding and source separation. The following objective\\nmeasures are considered: fwSNRseg, dLLR, PESQ, PEAQ, POLQA, PEMO-Q,\\nViSQOLAudio, (SI-)BSSEval, PEASS, LKR-PI, 2f-model, and HAAQI. Additionally, a\\nnovel measure (SI-SA2f) is presented, based on the 2f-model and a BSSEval-based\\nsignal decomposition. We use perceptual scores from 7 listening tests about\\naudio coding and 7 listening tests about source separation as ground-truth data\\nfor the correlation analysis. The results show that one method (2f-model)\\nperforms significantly better than the others on both domains and indicate that\\nthe dataset for training the method and a robust underlying auditory model are\\ncrucial factors towards a universal, domain-independent objective measure.\\n",
      "doi": "https://doi.org/10.1109/taslp.2021.3069302",
      "openalex_id": "https://openalex.org/W3145029257",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article evaluates objective perceptual audio quality measures across domains and does not propose or analyze discrete audio tokenization; no tokenization or discrete symbols are involved, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article evaluates objective perceptual audio quality measures across domains and does not propose or analyze discrete audio tokenization; no tokenization or discrete symbols are involved, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study evaluates objective perceptual audio quality measures across different domains, focusing on their correlation with human perceptual scores, but it does not propose, analyze, or evaluate discrete audio/speech tokenization where continuous audio is converted into discrete tokens; there is no mention of any discrete tokenization or related neural quantization-based methods, thus not meeting the inclusion criteria and falling under exclusion criteria regarding absence of discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study evaluates objective perceptual audio quality measures across different domains, focusing on their correlation with human perceptual scores, but it does not propose, analyze, or evaluate discrete audio/speech tokenization where continuous audio is converted into discrete tokens; there is no mention of any discrete tokenization or related neural quantization-based methods, thus not meeting the inclusion criteria and falling under exclusion criteria regarding absence of discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A short-time objective intelligibility measure for time-frequency weighted noisy speech",
    "abstract": "Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.",
    "metadata": {
      "title": "A short-time objective intelligibility measure for time-frequency weighted noisy speech",
      "summary": "Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.",
      "abstract": "Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.",
      "doi": "https://doi.org/10.1109/icassp.2010.5495701",
      "openalex_id": "https://openalex.org/W2067295501",
      "arxiv_id": "",
      "publication_date": "2010-03-01",
      "published": "2010-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes an objective intelligibility measure for time-frequency weighted speech and does not address discrete audio tokenization or token-based representations, hence it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes an objective intelligibility measure for time-frequency weighted speech and does not address discrete audio tokenization or token-based representations, hence it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an objective intelligibility measure for noisy and time-frequency weighted speech without addressing discrete audio or speech tokenization, discrete quantization, or token design related to language-model-style sequences, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes an objective intelligibility measure for noisy and time-frequency weighted speech without addressing discrete audio or speech tokenization, discrete quantization, or token design related to language-model-style sequences, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers",
    "abstract": "Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.",
    "metadata": {
      "title": "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers",
      "summary": "Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.",
      "abstract": "Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.",
      "doi": "https://doi.org/10.1109/taslp.2016.2585878",
      "openalex_id": "https://openalex.org/W2516001803",
      "arxiv_id": "",
      "publication_date": "2016-08-10",
      "published": "2016-08-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper develops an intelligibility prediction algorithm using spectrograms (continuous audio features) with reference to STOI/ESTOI, and does not propose or analyze any discrete audio tokenization or a token-based representation, so it fails the inclusion criteria and meets an exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper develops an intelligibility prediction algorithm using spectrograms (continuous audio features) with reference to STOI/ESTOI, and does not propose or analyze any discrete audio tokenization or a token-based representation, so it fails the inclusion criteria and meets an exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an intelligibility prediction algorithm based on continuous spectrogram analysis for speech masked by noise, without any mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbol sequences for reconstruction or understanding tasks. Hence, it does not meet the inclusion criteria of proposing or analyzing discrete tokenization methods but rather deals with continuous feature analysis and intelligibility prediction, which fits none of the inclusion criteria and aligns with exclusion criteria regarding continuous features without discrete quantization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an intelligibility prediction algorithm based on continuous spectrogram analysis for speech masked by noise, without any mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbol sequences for reconstruction or understanding tasks. Hence, it does not meet the inclusion criteria of proposing or analyzing discrete tokenization methods but rather deals with continuous feature analysis and intelligibility prediction, which fits none of the inclusion criteria and aligns with exclusion criteria regarding continuous features without discrete quantization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Efficient Model for Estimating Subjective Quality of Separated Audio Source Signals",
    "abstract": "Audio source separation, i.e. the separation of one or more target sources from a given audio signal mixture, has been a vivid and growing research field in recent years. Applications emerge which allow users to manipulate a given music recording to create a personal mix of a music recording or to adapt the audio level of the sports commentator and the atmosphere in sports broadcast to their own preference or hearing abilities. The perceived quality of the produced audio signals is an important key factor to rate these separation systems. In order to optimize them, an efficient, perceptually based measurement scheme to predict the perceived audio quality would be highly beneficial. Existing evaluation models, such as BSSEval or PEASS suffer from poor prediction of perceived quality or excessive computational complexity. In this paper a model for prediction of the perceived audio quality of separated audio source signals is presented, solely based on two timbre features and demanding less computational effort than current perceptual measurement schemes for audio source separation. High correlation of the model output with perceived quality is demonstrated.",
    "metadata": {
      "title": "An Efficient Model for Estimating Subjective Quality of Separated Audio Source Signals",
      "summary": "Audio source separation, i.e. the separation of one or more target sources from a given audio signal mixture, has been a vivid and growing research field in recent years. Applications emerge which allow users to manipulate a given music recording to create a personal mix of a music recording or to adapt the audio level of the sports commentator and the atmosphere in sports broadcast to their own preference or hearing abilities. The perceived quality of the produced audio signals is an important key factor to rate these separation systems. In order to optimize them, an efficient, perceptually based measurement scheme to predict the perceived audio quality would be highly beneficial. Existing evaluation models, such as BSSEval or PEASS suffer from poor prediction of perceived quality or excessive computational complexity. In this paper a model for prediction of the perceived audio quality of separated audio source signals is presented, solely based on two timbre features and demanding less computational effort than current perceptual measurement schemes for audio source separation. High correlation of the model output with perceived quality is demonstrated.",
      "abstract": "Audio source separation, i.e. the separation of one or more target sources from a given audio signal mixture, has been a vivid and growing research field in recent years. Applications emerge which allow users to manipulate a given music recording to create a personal mix of a music recording or to adapt the audio level of the sports commentator and the atmosphere in sports broadcast to their own preference or hearing abilities. The perceived quality of the produced audio signals is an important key factor to rate these separation systems. In order to optimize them, an efficient, perceptually based measurement scheme to predict the perceived audio quality would be highly beneficial. Existing evaluation models, such as BSSEval or PEASS suffer from poor prediction of perceived quality or excessive computational complexity. In this paper a model for prediction of the perceived audio quality of separated audio source signals is presented, solely based on two timbre features and demanding less computational effort than current perceptual measurement schemes for audio source separation. High correlation of the model output with perceived quality is demonstrated.",
      "doi": "https://doi.org/10.1109/waspaa.2019.8937179",
      "openalex_id": "https://openalex.org/W2998733532",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on predicting perceived quality of separated audio using continuous features; it does not propose or analyze discrete tokenization of audio into tokens, failing the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on predicting perceived quality of separated audio using continuous features; it does not propose or analyze discrete tokenization of audio into tokens, failing the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting perceived audio quality of separated audio source signals using timbre features and does not involve discrete audio/speech tokenization or analysis of converting continuous audio into discrete tokens from a finite vocabulary. Therefore, it does not meet the inclusion criteria regarding discrete tokenization methods, nor does it address neural encoding or modern quantization-based discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting perceived audio quality of separated audio source signals using timbre features and does not involve discrete audio/speech tokenization or analysis of converting continuous audio into discrete tokens from a finite vocabulary. Therefore, it does not meet the inclusion criteria regarding discrete tokenization methods, nor does it address neural encoding or modern quantization-based discrete token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Performance measurement in blind audio source separation",
    "abstract": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels",
    "metadata": {
      "title": "Performance measurement in blind audio source separation",
      "summary": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels",
      "abstract": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels",
      "doi": "https://doi.org/10.1109/tsa.2005.858005",
      "openalex_id": "https://openalex.org/W2127851351",
      "arxiv_id": "",
      "publication_date": "2006-06-21",
      "published": "2006-06-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work concerns evaluating blind audio source separation metrics and does not involve any discrete tokenization or token-based audio representations; thus it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work concerns evaluating blind audio source separation metrics and does not involve any discrete tokenization or token-based audio representations; thus it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on performance measurement for blind audio source separation algorithms, discussing distortions and error decompositions without addressing discrete audio/speech tokenization or quantization, which are required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on performance measurement for blind audio source separation algorithms, discussing distortions and error decompositions without addressing discrete audio/speech tokenization or quantization, which are required inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec",
    "abstract": "A good audio codec for live applications such as telecommunication is\\ncharacterized by three key properties: (1) compression, i.e.\\\\ the bitrate that\\nis required to transmit the signal should be as low as possible; (2) latency,\\ni.e.\\\\ encoding and decoding the signal needs to be fast enough to enable\\ncommunication without or with only minimal noticeable delay; and (3)\\nreconstruction quality of the signal. In this work, we propose an open-source,\\nstreamable, and real-time neural audio codec that achieves strong performance\\nalong all three axes: it can reconstruct highly natural sounding 48~kHz speech\\nsignals while operating at only 12~kbps and running with less than 6~ms\\n(GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated\\nfor developing such neural audio codecs for real-world scenarios. Both\\nobjective and subjective evaluations using the VCTK corpus are provided. To sum\\nup, AudioDec is a well-developed plug-and-play benchmark for audio codec\\napplications.\\n",
    "metadata": {
      "title": "Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec",
      "summary": "A good audio codec for live applications such as telecommunication is\\ncharacterized by three key properties: (1) compression, i.e.\\\\ the bitrate that\\nis required to transmit the signal should be as low as possible; (2) latency,\\ni.e.\\\\ encoding and decoding the signal needs to be fast enough to enable\\ncommunication without or with only minimal noticeable delay; and (3)\\nreconstruction quality of the signal. In this work, we propose an open-source,\\nstreamable, and real-time neural audio codec that achieves strong performance\\nalong all three axes: it can reconstruct highly natural sounding 48~kHz speech\\nsignals while operating at only 12~kbps and running with less than 6~ms\\n(GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated\\nfor developing such neural audio codecs for real-world scenarios. Both\\nobjective and subjective evaluations using the VCTK corpus are provided. To sum\\nup, AudioDec is a well-developed plug-and-play benchmark for audio codec\\napplications.\\n",
      "abstract": "A good audio codec for live applications such as telecommunication is\\ncharacterized by three key properties: (1) compression, i.e.\\\\ the bitrate that\\nis required to transmit the signal should be as low as possible; (2) latency,\\ni.e.\\\\ encoding and decoding the signal needs to be fast enough to enable\\ncommunication without or with only minimal noticeable delay; and (3)\\nreconstruction quality of the signal. In this work, we propose an open-source,\\nstreamable, and real-time neural audio codec that achieves strong performance\\nalong all three axes: it can reconstruct highly natural sounding 48~kHz speech\\nsignals while operating at only 12~kbps and running with less than 6~ms\\n(GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated\\nfor developing such neural audio codecs for real-world scenarios. Both\\nobjective and subjective evaluations using the VCTK corpus are provided. To sum\\nup, AudioDec is a well-developed plug-and-play benchmark for audio codec\\napplications.\\n",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096509",
      "openalex_id": "https://openalex.org/W4372270198",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a neural audio codec but does not mention discrete tokenization or a discrete token-based representation as a core component, so it does not meet the inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural audio codec but does not mention discrete tokenization or a discrete token-based representation as a core component, so it does not meet the inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents an open-source neural audio codec that operates with low latency and high quality, but the abstract does not explicitly mention the use, analysis, or evaluation of discrete audio/speech tokenization as a core object of study; hence, it is unclear whether the method involves converting continuous audio signals into discrete token sequences as required by the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents an open-source neural audio codec that operates with low latency and high quality, but the abstract does not explicitly mention the use, analysis, or evaluation of discrete audio/speech tokenization as a core object of study; hence, it is unclear whether the method involves converting continuous audio signals into discrete token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted absence of explicit discrete-token discussion (JuniorNano: excludes; JuniorMini: uncertain). The abstract describes an open-source neural audio codec with low bitrate and latency but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete symbol sequences—so it fails the requirement that discrete tokenization be a core object of study and appears more like an engineering/benchmarking codec paper. Explanation for score: The paper should be excluded because it does not explicitly address discrete audio tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted absence of explicit discrete-token discussion (JuniorNano: excludes; JuniorMini: uncertain). The abstract describes an open-source neural audio codec with low bitrate and latency but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete symbol sequences—so it fails the requirement that discrete tokenization be a core object of study and appears more like an engineering/benchmarking codec paper. Explanation for score: The paper should be excluded because it does not explicitly address discrete audio tokenization as required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences",
    "abstract": "Many audio processing tasks require perceptual assessment.The \"gold standard\" of obtaining human judgments is timeconsuming, expensive, and cannot be used as an optimization criterion.On the other hand, automated metrics are efficient to compute but often correlate poorly with human judgment, particularly for audio differences at the threshold of human detection.In this work, we construct a metric by fitting a deep neural network to a new large dataset of crowdsourced human judgments.Subjects are prompted to answer a straightforward, objective question: are two recordings identical or not?These pairs are algorithmically generated under a variety of perturbations, including noise, reverb, and compression artifacts; the perturbation space is probed with the goal of efficiently identifying the just-noticeable difference (JND) level of the subject.We show that the resulting learned metric is well-calibrated with human judgments, outperforming baseline methods.Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks.Thus, simply replacing an existing loss (e.g., deep feature loss) with our metric yields significant improvement in a denoising network, as measured by subjective pairwise comparison.",
    "metadata": {
      "title": "A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences",
      "summary": "Many audio processing tasks require perceptual assessment.The \"gold standard\" of obtaining human judgments is timeconsuming, expensive, and cannot be used as an optimization criterion.On the other hand, automated metrics are efficient to compute but often correlate poorly with human judgment, particularly for audio differences at the threshold of human detection.In this work, we construct a metric by fitting a deep neural network to a new large dataset of crowdsourced human judgments.Subjects are prompted to answer a straightforward, objective question: are two recordings identical or not?These pairs are algorithmically generated under a variety of perturbations, including noise, reverb, and compression artifacts; the perturbation space is probed with the goal of efficiently identifying the just-noticeable difference (JND) level of the subject.We show that the resulting learned metric is well-calibrated with human judgments, outperforming baseline methods.Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks.Thus, simply replacing an existing loss (e.g., deep feature loss) with our metric yields significant improvement in a denoising network, as measured by subjective pairwise comparison.",
      "abstract": "Many audio processing tasks require perceptual assessment.The \"gold standard\" of obtaining human judgments is timeconsuming, expensive, and cannot be used as an optimization criterion.On the other hand, automated metrics are efficient to compute but often correlate poorly with human judgment, particularly for audio differences at the threshold of human detection.In this work, we construct a metric by fitting a deep neural network to a new large dataset of crowdsourced human judgments.Subjects are prompted to answer a straightforward, objective question: are two recordings identical or not?These pairs are algorithmically generated under a variety of perturbations, including noise, reverb, and compression artifacts; the perturbation space is probed with the goal of efficiently identifying the just-noticeable difference (JND) level of the subject.We show that the resulting learned metric is well-calibrated with human judgments, outperforming baseline methods.Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks.Thus, simply replacing an existing loss (e.g., deep feature loss) with our metric yields significant improvement in a denoising network, as measured by subjective pairwise comparison.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1191",
      "openalex_id": "https://openalex.org/W3097934054",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on a differentiable perceptual audio metric learned from JND judgments and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations; hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on a differentiable perceptual audio metric learned from JND judgments and does not propose, analyze, or evaluate discrete audio/speech tokenization or token-based representations; hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on constructing a perceptual audio metric using deep neural networks trained on human judgments of audio similarity and differences, with an emphasis on just-noticeable differences. However, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences; rather, it focuses on audio quality assessment and metric learning without proposing or analyzing discrete tokenization methods. Therefore, it does not meet the inclusion criteria related to discrete tokenization and relates more to continuous audio feature evaluation and metric learning.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on constructing a perceptual audio metric using deep neural networks trained on human judgments of audio similarity and differences, with an emphasis on just-noticeable differences. However, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences; rather, it focuses on audio quality assessment and metric learning without proposing or analyzing discrete tokenization methods. Therefore, it does not meet the inclusion criteria related to discrete tokenization and relates more to continuous audio feature evaluation and metric learning.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  }
]