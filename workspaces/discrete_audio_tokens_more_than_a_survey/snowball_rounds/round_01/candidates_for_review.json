[
  {
    "metadata": {
      "title": "Wavefit: an Iterative and Non-Autoregressive Neural Vocoder Based on Fixed-Point Iteration",
      "summary": "Denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) are popular generative models for neural vocoders. The DDPMs and GANs can be characterized by the iterative denoising framework and adversarial training, respectively. This study proposes a fast and high-quality neural vocoder called WaveFit, which integrates the essence of GANs into a DDPM-like iterative framework based on fixed-point iteration. WaveFit iteratively denoises an input signal, and trains a deep neural network (DNN) for minimizing an adversarial loss calculated from intermediate outputs at all iterations. Subjective (side-by-side) listening tests showed no statistically significant differences in naturalness between human natural speech and those synthesized by WaveFit with five iterations. Furthermore, the inference speed of WaveFit was more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/.",
      "abstract": "Denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) are popular generative models for neural vocoders. The DDPMs and GANs can be characterized by the iterative denoising framework and adversarial training, respectively. This study proposes a fast and high-quality neural vocoder called WaveFit, which integrates the essence of GANs into a DDPM-like iterative framework based on fixed-point iteration. WaveFit iteratively denoises an input signal, and trains a deep neural network (DNN) for minimizing an adversarial loss calculated from intermediate outputs at all iterations. Subjective (side-by-side) listening tests showed no statistically significant differences in naturalness between human natural speech and those synthesized by WaveFit with five iterations. Furthermore, the inference speed of WaveFit was more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10022496",
      "openalex_id": "https://openalex.org/W4319862245",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning Disentangled Speech Representations with Contrastive Learning and Time-Invariant Retrieval",
      "summary": "Voice conversion refers to transferring speaker identity with well-preserved content. Better disentanglement of speech representations leads to better voice conversion. Recent studies have found that phonetic information from input audio has the potential ability to well represent content. Besides, the speaker-style modeling with pre-trained models making the process more complex. To tackle these issues, we introduce an new method named \"CTVC\" which utilizes disen-tangled speech representations with contrastive learning and time-invariant retrieval.Specifically, a similarity-based compression module is used to facilitate a more intimate connection between the frame-level hidden features and linguistic information at phoneme-level. Additionally, a time-invariant retrieval is proposed for timbre extraction based on multiple segmentation and mutual information. Experimental results demonstrate that \"CTVC\" outperforms previous studies and improves the sound quality and similarity of converted results.",
      "abstract": "Voice conversion refers to transferring speaker identity with well-preserved content. Better disentanglement of speech representations leads to better voice conversion. Recent studies have found that phonetic information from input audio has the potential ability to well represent content. Besides, the speaker-style modeling with pre-trained models making the process more complex. To tackle these issues, we introduce an new method named \"CTVC\" which utilizes disen-tangled speech representations with contrastive learning and time-invariant retrieval.Specifically, a similarity-based compression module is used to facilitate a more intimate connection between the frame-level hidden features and linguistic information at phoneme-level. Additionally, a time-invariant retrieval is proposed for timbre extraction based on multiple segmentation and mutual information. Experimental results demonstrate that \"CTVC\" outperforms previous studies and improves the sound quality and similarity of converted results.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447283",
      "openalex_id": "https://openalex.org/W4392931065",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion",
      "summary": "Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.",
      "abstract": "Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.",
      "doi": "https://doi.org/10.1109/apsipaasc58517.2023.10317592",
      "openalex_id": "https://openalex.org/W4388821525",
      "arxiv_id": "",
      "publication_date": "2023-10-31",
      "published": "2023-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Anonymization: Disentangling Speaker Features from Pre-Trained Speech Embeddings for Voice Conversion",
      "summary": "Speech is a crucial source of personal information, and the risk of attackers using such information increases day by day. Speaker privacy protection is crucial, and various approaches have been proposed to hide the speaker’s identity. One approach is voice anonymization, which aims to safeguard speaker identity while maintaining speech content through techniques such as voice conversion or spectral feature alteration. The significance of voice anonymization has grown due to the necessity to protect personal information in applications such as voice assistants, authentication, and customer support. Building upon the S3PRL-VC toolkit and on pre-trained speech and speaker representation models, this paper introduces a feature disentanglement approach to improve the de-identification performance of the state-of-the-art anonymization approaches based on voice conversion. The proposed approach achieves state-of-the-art speaker de-identification and causes minimal impact on the intelligibility of the signal after conversion.",
      "abstract": "Speech is a crucial source of personal information, and the risk of attackers using such information increases day by day. Speaker privacy protection is crucial, and various approaches have been proposed to hide the speaker’s identity. One approach is voice anonymization, which aims to safeguard speaker identity while maintaining speech content through techniques such as voice conversion or spectral feature alteration. The significance of voice anonymization has grown due to the necessity to protect personal information in applications such as voice assistants, authentication, and customer support. Building upon the S3PRL-VC toolkit and on pre-trained speech and speaker representation models, this paper introduces a feature disentanglement approach to improve the de-identification performance of the state-of-the-art anonymization approaches based on voice conversion. The proposed approach achieves state-of-the-art speaker de-identification and causes minimal impact on the intelligibility of the signal after conversion.",
      "doi": "https://doi.org/10.3390/app14093876",
      "openalex_id": "https://openalex.org/W4396611751",
      "arxiv_id": "",
      "publication_date": "2024-04-30",
      "published": "2024-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Editorial Editorial of Special Issue on Self-Supervised Learning for Speech and Audio Processing",
      "summary": "The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.",
      "abstract": "The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.",
      "doi": "https://doi.org/10.1109/jstsp.2022.3205434",
      "openalex_id": "https://openalex.org/W4308480316",
      "arxiv_id": "",
      "publication_date": "2022-10-01",
      "published": "2022-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Posterior Variance-Parameterised Gaussian Dropout: Improving Disentangled Sequential Autoencoders for Zero-Shot Voice Conversion",
      "summary": "The class of disentangled sequential auto-encoders factorises speech into time-invariant (global) and time-variant (local) representations for speaker identity and linguistic content, respectively. Many of the existing models employ this assumption to tackle zero-shot voice conversion (VC), which converts speaker characteristics of any given utterance to any novel speakers while preserving the linguistic content. However, balancing capacity between the two representations is intricate, as the global representation tends to collapse due to its lower information capacity along the time axis than that of the local representation. We propose a simple and effective dropout technique that applies an information bottleneck to the local representation via multiplicative Gaussian noise, in order to encourage the usage of the global one. We endow existing zero-shot VC models with the proposed method and show significant improvements in speaker conversion in terms of speaker verification acceptance rate and comparable or better intelligibility measured in character error rate.",
      "abstract": "The class of disentangled sequential auto-encoders factorises speech into time-invariant (global) and time-variant (local) representations for speaker identity and linguistic content, respectively. Many of the existing models employ this assumption to tackle zero-shot voice conversion (VC), which converts speaker characteristics of any given utterance to any novel speakers while preserving the linguistic content. However, balancing capacity between the two representations is intricate, as the global representation tends to collapse due to its lower information capacity along the time axis than that of the local representation. We propose a simple and effective dropout technique that applies an information bottleneck to the local representation via multiplicative Gaussian noise, in order to encourage the usage of the global one. We endow existing zero-shot VC models with the proposed method and show significant improvements in speaker conversion in terms of speaker verification acceptance rate and comparable or better intelligibility measured in character error rate.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447835",
      "openalex_id": "https://openalex.org/W4392909504",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mapping the Audio Landscape for Innovative Music Sample Generation",
      "summary": "This paper introduces the Generative Sample Map (GESAM), a novel two-stage unsupervised learning framework capable of generating high-quality and expressive audio samples for music production. Recent generative approaches based on language models rely on text prompts as conditions. However, fine nuances in musical audio samples can hardly be described in the modality of text. For addressing this shortcoming, we propose to learn a highly descriptive latent 2D audio map by a Variational Autoencoder (VAE) which is then utilized for conditioning a Transformer model. We demonstrate the Transformer model's ability to achieve high generation quality and compare its performance against two baseline models. By selecting points on the map that compresses the manifold of the audio training set into 2D, we enable a more natural interaction with the model. We showcase this capability through an interactive demo interface, which is accessible on our website https://limchr.github.io/gesam/",
      "abstract": "This paper introduces the Generative Sample Map (GESAM), a novel two-stage unsupervised learning framework capable of generating high-quality and expressive audio samples for music production. Recent generative approaches based on language models rely on text prompts as conditions. However, fine nuances in musical audio samples can hardly be described in the modality of text. For addressing this shortcoming, we propose to learn a highly descriptive latent 2D audio map by a Variational Autoencoder (VAE) which is then utilized for conditioning a Transformer model. We demonstrate the Transformer model's ability to achieve high generation quality and compare its performance against two baseline models. By selecting points on the map that compresses the manifold of the audio training set into 2D, we enable a more natural interaction with the model. We showcase this capability through an interactive demo interface, which is accessible on our website https://limchr.github.io/gesam/",
      "doi": "https://doi.org/10.1145/3652583.3657586",
      "openalex_id": "https://openalex.org/W4399418461",
      "arxiv_id": "",
      "publication_date": "2024-05-30",
      "published": "2024-05-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BeatNet+: Real‑Time Rhythm Analysis for Diverse Music Audio",
      "summary": "This paper presents a comprehensive study on real-time music rhythm analysis, covering joint beat and downbeat tracking for diverse kinds of music signals. We introduce BeatNet+, a two-stage approach to real-time rhythm analysis built on a previous state-of-the-art method named BeatNet. The main innovation of the proposed method is the auxiliary training strategy that helps the neural network model to learn a representation invariant to the amount of percussive components in the music. Together with other architectural improvements, this strategy significantly improves the model performance for generic music. Another innovation is on the adaptation strategies that help develop real-time rhythm analysis models for challenging music scenarios, including isolated singing voices and non-percussive music. Two adaptation strategies are proposed and experimented with using different neural architectures and training schemes. Comprehensive experiments and comparisons with multiple baselines are conducted, and results show that BeatNet+ achieves superior beat tracking and downbeat tracking F1 scores for generic music, isolated singing voices, and non-percussive audio, with competitive latency and computational complexity. Finally, we release beat and downbeat annotations for two datasets that are designed for other tasks, and revised annotations of three existing datasets. We also release the code repository and pre-trained models on GitHub.",
      "abstract": "This paper presents a comprehensive study on real-time music rhythm analysis, covering joint beat and downbeat tracking for diverse kinds of music signals. We introduce BeatNet+, a two-stage approach to real-time rhythm analysis built on a previous state-of-the-art method named BeatNet. The main innovation of the proposed method is the auxiliary training strategy that helps the neural network model to learn a representation invariant to the amount of percussive components in the music. Together with other architectural improvements, this strategy significantly improves the model performance for generic music. Another innovation is on the adaptation strategies that help develop real-time rhythm analysis models for challenging music scenarios, including isolated singing voices and non-percussive music. Two adaptation strategies are proposed and experimented with using different neural architectures and training schemes. Comprehensive experiments and comparisons with multiple baselines are conducted, and results show that BeatNet+ achieves superior beat tracking and downbeat tracking F1 scores for generic music, isolated singing voices, and non-percussive audio, with competitive latency and computational complexity. Finally, we release beat and downbeat annotations for two datasets that are designed for other tasks, and revised annotations of three existing datasets. We also release the code repository and pre-trained models on GitHub.",
      "doi": "https://doi.org/10.5334/tismir.198",
      "openalex_id": "https://openalex.org/W4405115893",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization",
      "summary": "Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability. Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering. In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression. Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks. In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory.",
      "abstract": "Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability. Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering. In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression. Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks. In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory.",
      "doi": "https://doi.org/10.1609/aaai.v38i3.28056",
      "openalex_id": "https://openalex.org/W4393159110",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey",
      "summary": "Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth exploration of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area. Additionally, we provide a GitHub link https://github.com/hanyang1999/Preference-Tuning-with-Human-Feedback.",
      "abstract": "Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth exploration of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area. Additionally, we provide a GitHub link https://github.com/hanyang1999/Preference-Tuning-with-Human-Feedback.",
      "doi": "https://doi.org/10.1613/jair.1.17541",
      "openalex_id": "https://openalex.org/W4409996147",
      "arxiv_id": "",
      "publication_date": "2025-04-30",
      "published": "2025-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
      "summary": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "doi": "https://doi.org/10.1609/aaai.v38i7.28486",
      "openalex_id": "https://openalex.org/W4393147998",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Audio Teacher-Student Transformer for Both Clip-Level and Frame-Level Tasks",
      "summary": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "abstract": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "doi": "https://doi.org/10.1109/taslp.2024.3352248",
      "openalex_id": "https://openalex.org/W4390738640",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CED: Consistent Ensemble Distillation for Audio Tagging",
      "summary": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "abstract": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446348",
      "openalex_id": "https://openalex.org/W4392904001",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the Use of an Acoustic Sensor in the Tasks of Determining Defects in the Roadway",
      "summary": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "abstract": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "doi": "https://doi.org/10.1109/ieeeconf60226.2024.10496721",
      "openalex_id": "https://openalex.org/W4394860713",
      "arxiv_id": "",
      "publication_date": "2024-03-12",
      "published": "2024-03-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
      "summary": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "doi": "https://doi.org/10.1109/tmm.2024.3405622",
      "openalex_id": "https://openalex.org/W4399146361",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SFC-Sup: Robust Two-Stage Underwater Acoustic Target Recognition Method Based on Supervised Contrastive Learning",
      "summary": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "abstract": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tgrs.2023.3329653",
      "openalex_id": "https://openalex.org/W4388240219",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast ship radiated noise recognition using three-dimensional mel-spectrograms with an additive attention based transformer",
      "summary": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "abstract": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "doi": "https://doi.org/10.3389/fmars.2023.1280708",
      "openalex_id": "https://openalex.org/W4388973434",
      "arxiv_id": "",
      "publication_date": "2023-11-24",
      "published": "2023-11-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Audio Captioning Models with Fine-Grained Audio Features, Text Embedding Supervision, and LLM Mix-Up Augmentation",
      "summary": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447215",
      "openalex_id": "https://openalex.org/W4392903033",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Self-supervised Contrastive Learning of Spatial Sound Event Representation",
      "summary": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "abstract": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447391",
      "openalex_id": "https://openalex.org/W4392903751",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Foundation Models for Video Understanding: A Survey",
      "summary": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "abstract": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "doi": "https://doi.org/10.36227/techrxiv.171769139.99464428/v1",
      "openalex_id": "https://openalex.org/W4399393261",
      "arxiv_id": "",
      "publication_date": "2024-06-06",
      "published": "2024-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SSL-Net: A Synergistic Spectral and Learning-Based Network for Efficient Bird Sound Classification",
      "summary": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "abstract": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445889",
      "openalex_id": "https://openalex.org/W4392904623",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DTF-AT: Decoupled Time-Frequency Audio Transformer for Event Classification",
      "summary": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "abstract": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29716",
      "openalex_id": "https://openalex.org/W4393160941",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends",
      "summary": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "abstract": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "doi": "https://doi.org/10.1109/bigdata59044.2023.10386933",
      "openalex_id": "https://openalex.org/W4391092942",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Audio Generation Diversity with Visual Information",
      "summary": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "abstract": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447384",
      "openalex_id": "https://openalex.org/W4392909638",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Paᗧ-HuBERT: Self-Supervised Music Source Separation Via Primitive Auditory Clustering And Hidden-Unit Bert",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193575",
      "openalex_id": "https://openalex.org/W4385478423",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weak Supervised Sound Event Detection Based on Puzzle CAM",
      "summary": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "abstract": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "doi": "https://doi.org/10.1109/access.2023.3305633",
      "openalex_id": "https://openalex.org/W4385834290",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers",
      "summary": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "abstract": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10289774",
      "openalex_id": "https://openalex.org/W4388183682",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification",
      "summary": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "abstract": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "doi": "https://doi.org/10.48550/arxiv.2211.13189",
      "openalex_id": "https://openalex.org/W4309957268",
      "arxiv_id": "",
      "publication_date": "2022-11-23",
      "published": "2022-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
      "summary": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "doi": "https://doi.org/10.48550/arxiv.2303.17395",
      "openalex_id": "https://openalex.org/W4361864998",
      "arxiv_id": "",
      "publication_date": "2023-03-30",
      "published": "2023-03-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
      "summary": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "doi": "https://doi.org/10.48550/arxiv.2306.00561",
      "openalex_id": "https://openalex.org/W4379256313",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures",
      "summary": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "abstract": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446342",
      "openalex_id": "https://openalex.org/W4392902846",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Impact of Parroting Mode on Cross-Lingual Speaker Recognition",
      "summary": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "abstract": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00035",
      "openalex_id": "https://openalex.org/W4392980346",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weakly Labeled Sound Event Detection using Attention Mechanism with Teacher-Student Model",
      "summary": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "abstract": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00034",
      "openalex_id": "https://openalex.org/W4392980966",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech2Face3D: A Two‐Stage Transfer‐Learning Framework for Speech‐Driven 3D Facial Animation",
      "summary": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "abstract": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "doi": "https://doi.org/10.1049/ipr2.70155",
      "openalex_id": "https://openalex.org/W4412613010",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Realistic and broad-scope learning simulations: first results and challenges",
      "summary": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "abstract": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "doi": "https://doi.org/10.1017/s0305000923000272",
      "openalex_id": "https://openalex.org/W4378619943",
      "arxiv_id": "",
      "publication_date": "2023-05-29",
      "published": "2023-05-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-Training and Multi-Modal Tokens",
      "summary": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ.",
      "abstract": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446888",
      "openalex_id": "https://openalex.org/W4392904292",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Models of Speech Infer Universal Articulatory Kinematics",
      "summary": "Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun \"probing\" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show \"inference of articulatory kinematics\" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.",
      "abstract": "Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun \"probing\" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show \"inference of articulatory kinematics\" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447345",
      "openalex_id": "https://openalex.org/W4392902939",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in Hubert",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and speech units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and speech units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446062",
      "openalex_id": "https://openalex.org/W4392904409",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards Matching Phones and Speech Representations",
      "summary": "Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.",
      "abstract": "Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389757",
      "openalex_id": "https://openalex.org/W4391021372",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Artificial neural networks to analyze and simulate language acquisition in children",
      "summary": "Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.",
      "abstract": "Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.",
      "doi": "https://doi.org/10.31234/osf.io/5p8ge",
      "openalex_id": "https://openalex.org/W4387877468",
      "arxiv_id": "",
      "publication_date": "2023-10-22",
      "published": "2023-10-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Artificial Intelligence &amp; Creativity: A Manifesto for Collaboration",
      "summary": "ABSTRACT With the advent of artificial intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human–machine collaboration on creative tasks and proposes “fundamental laws of generative AI” to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and “Shut down,” each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI‐generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "abstract": "ABSTRACT With the advent of artificial intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human–machine collaboration on creative tasks and proposes “fundamental laws of generative AI” to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and “Shut down,” each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI‐generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "doi": "https://doi.org/10.1002/jocb.597",
      "openalex_id": "https://openalex.org/W4380988459",
      "arxiv_id": "",
      "publication_date": "2023-06-15",
      "published": "2023-06-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.1055",
      "openalex_id": "https://openalex.org/W4389524500",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Character-LLM: A Trainable Agent for Role-Playing",
      "summary": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
      "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-main.814",
      "openalex_id": "https://openalex.org/W4389519488",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Combined scaling for zero-shot transfer learning",
      "summary": "Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.",
      "abstract": "Recent developments in multimodal training methodologies, including CLIP and ALIGN, obviate the necessity for individual data labeling. These approaches utilize pairs of data and corresponding textual information found online as a form of weak supervision signal. However, models employing this kind of weak supervision are not as competitive as their supervised and semi-supervised counterparts when sufficient labeled data is accessible. This performance gap constrains the applicability of weekly supervised models. In this paper, we narrow the gap by proposing a combined scaling method, named BASIC, that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models, CLIP and ALIGN, by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we first develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as CLIP and ALIGN. Based on this theoretical result, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions (data size, model size, and batch size) by proposing a new method using gradient checkpointing and model parallelism. As a result, our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.",
      "doi": "https://doi.org/10.1016/j.neucom.2023.126658",
      "openalex_id": "https://openalex.org/W3214803981",
      "arxiv_id": "",
      "publication_date": "2023-08-05",
      "published": "2023-08-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
      "summary": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389705",
      "openalex_id": "https://openalex.org/W4391021666",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
      "summary": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447523",
      "openalex_id": "https://openalex.org/W4392903389",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
      "summary": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "doi": "https://doi.org/10.1109/taslp.2024.3451951",
      "openalex_id": "https://openalex.org/W4402301063",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447112",
      "openalex_id": "https://openalex.org/W4392904805",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation",
      "summary": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447553",
      "openalex_id": "https://openalex.org/W4392903872",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparison of the Ability of Neural Network Model and Humans to Detect a Cloned Voice",
      "summary": "The vulnerability of the speaker identity verification system to attacks using voice cloning was examined. The research project assumed creating a model for verifying the speaker’s identity based on voice biometrics and then testing its resistance to potential attacks using voice cloning. The Deep Speaker Neural Speaker Embedding System was trained, and the Real-Time Voice Cloning system was employed based on the SV2TTS, Tacotron, WaveRNN, and GE2E neural networks. The results of attacks using voice cloning were analyzed and discussed in the context of a subjective assessment of cloned voice fidelity. Subjective test results and attempts to authenticate speakers proved that the tested biometric identity verification system might resist voice cloning attacks even if humans cannot distinguish cloned samples from original ones.",
      "abstract": "The vulnerability of the speaker identity verification system to attacks using voice cloning was examined. The research project assumed creating a model for verifying the speaker’s identity based on voice biometrics and then testing its resistance to potential attacks using voice cloning. The Deep Speaker Neural Speaker Embedding System was trained, and the Real-Time Voice Cloning system was employed based on the SV2TTS, Tacotron, WaveRNN, and GE2E neural networks. The results of attacks using voice cloning were analyzed and discussed in the context of a subjective assessment of cloned voice fidelity. Subjective test results and attempts to authenticate speakers proved that the tested biometric identity verification system might resist voice cloning attacks even if humans cannot distinguish cloned samples from original ones.",
      "doi": "https://doi.org/10.3390/electronics12214458",
      "openalex_id": "https://openalex.org/W4388017642",
      "arxiv_id": "",
      "publication_date": "2023-10-30",
      "published": "2023-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AutoPrep: An Automatic Preprocessing Framework for In-The-Wild Speech Data",
      "summary": "Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447759",
      "openalex_id": "https://openalex.org/W4392903247",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection",
      "summary": "© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "abstract": "© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "doi": "https://doi.org/10.1109/cvprw59228.2023.00112",
      "openalex_id": "https://openalex.org/W4385800906",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems",
      "summary": "This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions with, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multi-modal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.",
      "abstract": "This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions with, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multi-modal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.",
      "doi": "https://doi.org/10.1109/itsc57777.2023.10422003",
      "openalex_id": "https://openalex.org/W4391770514",
      "arxiv_id": "",
      "publication_date": "2023-09-24",
      "published": "2023-09-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Ubiq-Genie: Leveraging External Frameworks for Enhanced Social VR Experiences",
      "summary": "This paper describes the Ubiq-Genie framework for integrating external frameworks with the Ubiq social VR platform. The proposed architecture is modular, allowing for easy integration of services and providing mechanisms to offload computationally intensive processes to a server. To showcase the capabilities of the framework, we present two prototype applications: 1) a voice- and gesture-controlled texture generation method based on Stable Diffusion 2.0 and 2) an embodied conversational agent based on ChatGPT. This work aims to demonstrate the potential of integrating external frameworks into social VR for the creation of new types of collaborative experiences.",
      "abstract": "This paper describes the Ubiq-Genie framework for integrating external frameworks with the Ubiq social VR platform. The proposed architecture is modular, allowing for easy integration of services and providing mechanisms to offload computationally intensive processes to a server. To showcase the capabilities of the framework, we present two prototype applications: 1) a voice- and gesture-controlled texture generation method based on Stable Diffusion 2.0 and 2) an embodied conversational agent based on ChatGPT. This work aims to demonstrate the potential of integrating external frameworks into social VR for the creation of new types of collaborative experiences.",
      "doi": "https://doi.org/10.1109/vrw58643.2023.00108",
      "openalex_id": "https://openalex.org/W4367662705",
      "arxiv_id": "",
      "publication_date": "2023-03-01",
      "published": "2023-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study",
      "summary": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447929",
      "openalex_id": "https://openalex.org/W4392909068",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Artificial Intelligence &amp;amp; Creativity: A manifesto for collaboration",
      "summary": "Creativity is a phenomenon that emerges in the human-sociocultural and machine-artificial layers. With the advent of Artificial Intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human-machine collaboration on creative tasks and proposes \"fundamental laws of generative AI\" to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: \"Co-Cre-AI-tion\", \"Organic\", \"Plagiarism 3.0\", and “Shut down”, each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI-generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "abstract": "Creativity is a phenomenon that emerges in the human-sociocultural and machine-artificial layers. With the advent of Artificial Intelligence (AI), the field of creativity faces new opportunities and challenges. This manifesto explores several scenarios of human-machine collaboration on creative tasks and proposes \"fundamental laws of generative AI\" to reinforce the responsible and ethical use of AI in the creativity field. Four scenarios are proposed and discussed: \"Co-Cre-AI-tion\", \"Organic\", \"Plagiarism 3.0\", and “Shut down”, each illustrating different possible futures based on the collaboration between humans and machines. In addition, we have incorporated an AI-generated manifesto that also highlights important themes, ranging from accessibility and ethics to cultural sensitivity. The fundamental laws proposed aim to prevent AIs from generating harmful content and competing directly with humans. Creating labels and laws are also highlighted to ensure responsible use of AIs. The positive future of creativity and AI lies in a harmonious collaboration that can benefit everyone, potentially leading to a new level of creative productivity respecting ethical considerations and human values during the creative process.",
      "doi": "https://doi.org/10.31234/osf.io/ukqc9",
      "openalex_id": "https://openalex.org/W4368370051",
      "arxiv_id": "",
      "publication_date": "2023-05-03",
      "published": "2023-05-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection",
      "summary": "Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the cross-dataset scenario.",
      "abstract": "Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the cross-dataset scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448196",
      "openalex_id": "https://openalex.org/W4392903271",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MM-TTS: Multi-Modal Prompt Based Style Transfer for Expressive Text-to-Speech Synthesis",
      "summary": "The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.",
      "abstract": "The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29769",
      "openalex_id": "https://openalex.org/W4393152865",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
      "summary": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
      "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
      "doi": "https://doi.org/10.1109/taslp.2024.3393714",
      "openalex_id": "https://openalex.org/W4395447377",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speaker Adaptive Text-to-Speech With Timbre-Normalized Vector-Quantized Feature",
      "summary": "Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.",
      "abstract": "Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.",
      "doi": "https://doi.org/10.1109/taslp.2023.3308374",
      "openalex_id": "https://openalex.org/W4386133927",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fewer-Token Neural Speech Codec with Time-Invariant Codes",
      "summary": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.",
      "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448454",
      "openalex_id": "https://openalex.org/W4392903006",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding",
      "summary": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446203",
      "openalex_id": "https://openalex.org/W4392903524",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "TextrolSpeech: A Text Style Control Speech Corpus with Codec Language Text-to-Speech Models",
      "summary": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.",
      "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445879",
      "openalex_id": "https://openalex.org/W4392904245",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446998",
      "openalex_id": "https://openalex.org/W4392904154",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Yet Another Generative Model for Room Impulse Response Estimation",
      "summary": "Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.",
      "abstract": "Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.",
      "doi": "https://doi.org/10.1109/waspaa58266.2023.10248189",
      "openalex_id": "https://openalex.org/W4386764631",
      "arxiv_id": "",
      "publication_date": "2023-09-15",
      "published": "2023-09-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vision + Language Applications: A Survey",
      "summary": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
      "abstract": "Text-to-image generation has attracted significant interest from researchers and practitioners in recent years due to its widespread and diverse applications across various industries. Despite the progress made in the domain of vision and language research, the existing literature remains relatively limited, particularly with regard to advancements and applications in this field. This paper explores a relevant research track within multimodal applications, including text, vision, audio, and others. In addition to the studies discussed in this paper, we are also committed to continually updating the latest relevant papers, datasets, application projects and corresponding information at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image.",
      "doi": "https://doi.org/10.1109/cvprw59228.2023.00090",
      "openalex_id": "https://openalex.org/W4385801730",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Toward Joint Language Modeling for Speech Units and Text",
      "summary": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
      "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model’s learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.438",
      "openalex_id": "https://openalex.org/W4389518827",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023",
      "summary": "In this paper, we present MuLanTTS, the Microsoft end-toend neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023.About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness.Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora.For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker finetuning.MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment.The excellent quality and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.",
      "abstract": "In this paper, we present MuLanTTS, the Microsoft end-toend neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023.About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness.Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora.For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker finetuning.MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment.The excellent quality and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.",
      "doi": "https://doi.org/10.21437/blizzard.2023-5",
      "openalex_id": "https://openalex.org/W4387941753",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data",
      "summary": "Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.",
      "abstract": "Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448074",
      "openalex_id": "https://openalex.org/W4392903365",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative De-Quantization for Neural Speech Codec Via Latent Diffusion",
      "summary": "End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "End-to-end speech coding models achieve high coding gains by learning compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens. Instead of using its decoder, we employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. We investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions in ablation studies. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. We open-source the project for reproducibility <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446556",
      "openalex_id": "https://openalex.org/W4392931975",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation",
      "summary": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
      "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446214",
      "openalex_id": "https://openalex.org/W4392902773",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
      "summary": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447926",
      "openalex_id": "https://openalex.org/W4392902778",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Personalized Aging-in-Place Support Through Fine-Tuning of Generative AI Models",
      "summary": "Artificial intelligence (AI) has long been proposed as an enabling technology for addressing the challenges of the impending demographic shifts in western countries. While AI tools for identifying, monitoring, and assessing adherence to best health practices have been previously proposed, recent developments in generative AI models offer unique capabilities to address compliance-related challenges. This manuscript proposes a framework for enhancing AI-assistive technology through personalization enabled by generative AI. Namely, emerging generative models are fine-tuned using training data mined from text, audio, and visual records of target individuals to produce customized user experiences which can be integrated within the technology ecosystem available to the end-user. In addition to proposing this framework, this work-in-progress manuscript also reviews recent advancements in technologies which enable its implementation.",
      "abstract": "Artificial intelligence (AI) has long been proposed as an enabling technology for addressing the challenges of the impending demographic shifts in western countries. While AI tools for identifying, monitoring, and assessing adherence to best health practices have been previously proposed, recent developments in generative AI models offer unique capabilities to address compliance-related challenges. This manuscript proposes a framework for enhancing AI-assistive technology through personalization enabled by generative AI. Namely, emerging generative models are fine-tuned using training data mined from text, audio, and visual records of target individuals to produce customized user experiences which can be integrated within the technology ecosystem available to the end-user. In addition to proposing this framework, this work-in-progress manuscript also reviews recent advancements in technologies which enable its implementation.",
      "doi": "https://doi.org/10.1109/mobisecserv58080.2023.10329224",
      "openalex_id": "https://openalex.org/W4389230249",
      "arxiv_id": "",
      "publication_date": "2023-11-04",
      "published": "2023-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
      "summary": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447938",
      "openalex_id": "https://openalex.org/W4392909867",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An AI red team playbook",
      "summary": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing <i>An AI Blue Team Playbook</i>, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.",
      "abstract": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing <i>An AI Blue Team Playbook</i>, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.",
      "doi": "https://doi.org/10.1117/12.3021906",
      "openalex_id": "https://openalex.org/W4399422814",
      "arxiv_id": "",
      "publication_date": "2024-06-07",
      "published": "2024-06-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Practical Study of Deep Learning Models for Speech Synthesis",
      "summary": "Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.",
      "abstract": "Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.",
      "doi": "https://doi.org/10.1145/3594806.3596536",
      "openalex_id": "https://openalex.org/W4385732548",
      "arxiv_id": "",
      "publication_date": "2023-07-05",
      "published": "2023-07-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Latent Filling: Latent Space Data Augmentation for Zero-Shot Speech Synthesis",
      "summary": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.",
      "abstract": "Previous works in zero-shot text-to-speech (ZS-TTS) have attempted to enhance its systems by enlarging the training data through crowd-sourcing or augmenting existing speech data. However, the use of low-quality data has led to a decline in the overall system performance. To avoid such degradation, instead of directly augmenting the input data, we propose a latent filling (LF) method that adopts simple but effective latent space data augmentation in the speaker embedding space of the ZS-TTS system. By incorporating a consistency loss, LF can be seamlessly integrated into existing ZS-TTS systems without the need for additional training stages. Experimental results show that LF significantly improves speaker similarity while preserving speech quality.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446098",
      "openalex_id": "https://openalex.org/W4392902644",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GR0: Self-Supervised Global Representation Learning for Zero-Shot Voice Conversion",
      "summary": "Research in generative self-supervised learning (SSL) has largely focused on local embeddings for tokenized sequences. We introduce a generative SSL framework that learns a global representation that is disentangled from local embeddings. We apply this technique to jointly learn a global speaker embedding and a zero-shot voice converter. The converter modifies recorded speech to sound as if it were spoken by a different person while preserving the content, using only a short reference clip unavailable to the model during training. Listening experiments conducted on an unseen dataset show that our models significantly outperform SOTA baselines in both quality and speaker similarity for various datasets and unseen languages.",
      "abstract": "Research in generative self-supervised learning (SSL) has largely focused on local embeddings for tokenized sequences. We introduce a generative SSL framework that learns a global representation that is disentangled from local embeddings. We apply this technique to jointly learn a global speaker embedding and a zero-shot voice converter. The converter modifies recorded speech to sound as if it were spoken by a different person while preserving the content, using only a short reference clip unavailable to the model during training. Listening experiments conducted on an unseen dataset show that our models significantly outperform SOTA baselines in both quality and speaker similarity for various datasets and unseen languages.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448232",
      "openalex_id": "https://openalex.org/W4392903903",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DSVAE: Disentangled Representation Learning for Synthetic Speech Detection",
      "summary": "Tools to generate high quality synthetic speech that is perceptually indistinguishable from speech recorded from hu-man speakers are easily available. Many incidents report misuse of synthetic speech for spreading misinformation and committing financial fraud. Several approaches have been proposed for detecting synthetic speech. Many of these approaches use deep learning methods without providing reasoning for the decisions they make. This limits the explainability of these approaches. In this paper, we use disentangled representation learning for developing a synthetic speech detector. We propose Disentangled Spectrogram Variational Auto Encoder (DSVAE) which is a two stage trained variational autoencoder that processes spectrograms of speech to generate features that disentangle synthetic and bona fide speech. We evaluated DSVAE using the ASVspoof2019 dataset. Our experimental results show high accuracy (&amp;gt; 98%) on detecting synthetic speech from 6 known and 10 unknown speech synthesizers. Further, the visualization of disentangled features obtained from DSVAE provides rea-soning behind the working principle of DSVAE, improving its explainability. DSVAE performs well compared to several existing methods. Additionally, DSVAE works in practical scenarios such as detecting synthetic speech uploaded on social platforms and against simple attacks such as removing silence regions.",
      "abstract": "Tools to generate high quality synthetic speech that is perceptually indistinguishable from speech recorded from hu-man speakers are easily available. Many incidents report misuse of synthetic speech for spreading misinformation and committing financial fraud. Several approaches have been proposed for detecting synthetic speech. Many of these approaches use deep learning methods without providing reasoning for the decisions they make. This limits the explainability of these approaches. In this paper, we use disentangled representation learning for developing a synthetic speech detector. We propose Disentangled Spectrogram Variational Auto Encoder (DSVAE) which is a two stage trained variational autoencoder that processes spectrograms of speech to generate features that disentangle synthetic and bona fide speech. We evaluated DSVAE using the ASVspoof2019 dataset. Our experimental results show high accuracy (&amp;gt; 98%) on detecting synthetic speech from 6 known and 10 unknown speech synthesizers. Further, the visualization of disentangled features obtained from DSVAE provides rea-soning behind the working principle of DSVAE, improving its explainability. DSVAE performs well compared to several existing methods. Additionally, DSVAE works in practical scenarios such as detecting synthetic speech uploaded on social platforms and against simple attacks such as removing silence regions.",
      "doi": "https://doi.org/10.1109/icmla58977.2023.00072",
      "openalex_id": "https://openalex.org/W4392942164",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision",
      "summary": "Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.",
      "abstract": "Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.",
      "doi": "https://doi.org/10.1109/taslp.2023.3306716",
      "openalex_id": "https://openalex.org/W4385975838",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
      "summary": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "doi": "https://doi.org/10.21437/blizzard.2023-13",
      "openalex_id": "https://openalex.org/W4388272705",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The concept of deepfake in Russian law, classification of deepfake and issues of their legal regulation",
      "summary": "The article deals with the issues of legal regulation of deepfake in the Russian Federation. Legal regulation of deepfake does not keep up with the pace of development of artificial intelligence technologies. The authors emphasize that there is no definition of deepfake in the current legislation, and the existing formulations in scientific works are extremely contradictory in nature. Taking into account the pace of development of artificial intelligence technologies, it is necessary to legislate the definition of deepfake. The authors note that the classification of deepfakes is fundamentally important for the legal regulation of these technologies. According to the results of the analysis of modern neural networks the species classification of deepfakes is offered. Taking into account the authors' proposed definition of the concept of \"deepfake\" and taking into account the lack of legal mechanisms to regulate social relations in the sphere of use and distribution of deepfake, which cause the development of digital transformation, it is important to form mechanisms to adapt the legal system to the challenges associated with the development of deepfake technologies.",
      "abstract": "The article deals with the issues of legal regulation of deepfake in the Russian Federation. Legal regulation of deepfake does not keep up with the pace of development of artificial intelligence technologies. The authors emphasize that there is no definition of deepfake in the current legislation, and the existing formulations in scientific works are extremely contradictory in nature. Taking into account the pace of development of artificial intelligence technologies, it is necessary to legislate the definition of deepfake. The authors note that the classification of deepfakes is fundamentally important for the legal regulation of these technologies. According to the results of the analysis of modern neural networks the species classification of deepfakes is offered. Taking into account the authors' proposed definition of the concept of \"deepfake\" and taking into account the lack of legal mechanisms to regulate social relations in the sphere of use and distribution of deepfake, which cause the development of digital transformation, it is important to form mechanisms to adapt the legal system to the challenges associated with the development of deepfake technologies.",
      "doi": "https://doi.org/10.25136/2409-7136.2023.11.69014",
      "openalex_id": "https://openalex.org/W4388870810",
      "arxiv_id": "",
      "publication_date": "2023-11-01",
      "published": "2023-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mapache: Masked Parallel Transformer for Advanced Speech Editing and Synthesis",
      "summary": "Recent advancements in Generative AI, such as scaled Transformer large language models (LLM) and diffusion decoders, have revolutionized speech synthesis. With speech encompassing the complexities of natural language and audio dimensionality, many recent models have relied on autoregressive modeling of quantized speech tokens. Such an approach limits speech synthesis to left-to-right generation, making these models unsuitable for speech edits free from audio discontinuities. We introduce Mapache, a novel architecture that combines a non-autoregressive masked speech language model with acoustic diffusion modeling, offering a unique, fully parallel pipeline. Mapache excels in precise speech editing that is indiscernible to human listeners, exhibiting inpainting and zero-shot synthesis capabilities that either surpass or rival those of other state-of-the-art models that specialize in just one of these tasks. This paper also sheds light on optimizing the decoding process for such non-autoregressive models.",
      "abstract": "Recent advancements in Generative AI, such as scaled Transformer large language models (LLM) and diffusion decoders, have revolutionized speech synthesis. With speech encompassing the complexities of natural language and audio dimensionality, many recent models have relied on autoregressive modeling of quantized speech tokens. Such an approach limits speech synthesis to left-to-right generation, making these models unsuitable for speech edits free from audio discontinuities. We introduce Mapache, a novel architecture that combines a non-autoregressive masked speech language model with acoustic diffusion modeling, offering a unique, fully parallel pipeline. Mapache excels in precise speech editing that is indiscernible to human listeners, exhibiting inpainting and zero-shot synthesis capabilities that either surpass or rival those of other state-of-the-art models that specialize in just one of these tasks. This paper also sheds light on optimizing the decoding process for such non-autoregressive models.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448121",
      "openalex_id": "https://openalex.org/W4392902860",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
      "summary": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448495",
      "openalex_id": "https://openalex.org/W4392931282",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Redaction from Conditional Generative Models",
      "summary": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "doi": "https://doi.org/10.1109/satml59370.2024.00035",
      "openalex_id": "https://openalex.org/W4396815655",
      "arxiv_id": "",
      "publication_date": "2024-04-09",
      "published": "2024-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stack-and-Delay: A New Codebook Pattern for Music Generation",
      "summary": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "abstract": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447392",
      "openalex_id": "https://openalex.org/W4392909680",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
      "summary": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446991",
      "openalex_id": "https://openalex.org/W4392931320",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification",
      "summary": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.",
      "abstract": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.",
      "doi": "https://doi.org/10.1609/aaai.v38i5.28249",
      "openalex_id": "https://openalex.org/W4393153727",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
      "summary": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "doi": "https://doi.org/10.48550/arxiv.2306.07691",
      "openalex_id": "https://openalex.org/W4380714711",
      "arxiv_id": "",
      "publication_date": "2023-06-13",
      "published": "2023-06-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Creativity and Machine Learning: A Survey",
      "summary": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
      "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
      "doi": "https://doi.org/10.48550/arxiv.2104.02726",
      "openalex_id": "https://openalex.org/W4287239377",
      "arxiv_id": "",
      "publication_date": "2021-04-06",
      "published": "2021-04-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Voice Cloning Quality through Data Selection and Alignment-based Metrics",
      "summary": "Voice cloning, an emerging field in the speech processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigate the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also use two high-quality corpora for comparative analysis. We conduct exhaustive evaluations of the quality of the gathered corpora in order to select the most suitable audios for the training of a Voice Cloning system. Following these measurements, we conduct a series of ablations by removing audios with lower SNR and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduce a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 Text-to-Speech (TTS) system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increase the quality of synthesised audios for the challenging low-quality corpus. Notably, our findings indicate that models trained on a 3-hour corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.",
      "abstract": "Voice cloning, an emerging field in the speech processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigate the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also use two high-quality corpora for comparative analysis. We conduct exhaustive evaluations of the quality of the gathered corpora in order to select the most suitable audios for the training of a Voice Cloning system. Following these measurements, we conduct a series of ablations by removing audios with lower SNR and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduce a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 Text-to-Speech (TTS) system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increase the quality of synthesised audios for the challenging low-quality corpus. Notably, our findings indicate that models trained on a 3-hour corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.",
      "doi": "https://doi.org/10.20944/preprints202306.0223.v1",
      "openalex_id": "https://openalex.org/W4379387359",
      "arxiv_id": "",
      "publication_date": "2023-06-05",
      "published": "2023-06-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Proceedings of the 4th International Workshop on Distributed Infrastructure for the Common Good",
      "summary": "In contrast to many online services based on client-server infrastructure, peer-to-peer systems are usually designed as open commons. This is partly because, by design, peer-to-peer systems replicate data on end-user devices and typically use open ...",
      "abstract": "In contrast to many online services based on client-server infrastructure, peer-to-peer systems are usually designed as open commons. This is partly because, by design, peer-to-peer systems replicate data on end-user devices and typically use open ...",
      "doi": "https://doi.org/10.1145/3631310",
      "openalex_id": "https://openalex.org/W4391054691",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
      "summary": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily altering phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation.",
      "abstract": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily altering phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation.",
      "doi": "https://doi.org/10.48550/arxiv.2306.03389",
      "openalex_id": "https://openalex.org/W4379919722",
      "arxiv_id": "",
      "publication_date": "2023-06-06",
      "published": "2023-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Context, Perception, Production: A Model of Vocal Persona",
      "summary": "We present a contextualized production-perception model of vocal persona based on a deductive thematic analysis of interviews with voice and performance experts. The model formalizes how the vocal persona frames and bounds expressive vocal interactions (both biological and synthesized), and centers a person's agency over their communicative role. This article provides insights into opportunities for improvement in Voice User Interfaces (VUI) and Augmentative \\&amp;amp; Assistive Communications (AAC) technologies based on the study's results. The proposed contextualized production-perception model fills an important gap in the literature on expressive and interactive speech technologies by identifying a key missing mechanism in contextualized vocal communication models and providing a resource for more nuanced approaches to expressive speech synthesis methods. Incorporating vocal persona into expressive vocal synthesis has the potential to significantly enhance the level of agency and embodiment experienced by VUI and AAC users during communication, resulting in a heightened sense of authenticity and an improved relationship with their environment.",
      "abstract": "We present a contextualized production-perception model of vocal persona based on a deductive thematic analysis of interviews with voice and performance experts. The model formalizes how the vocal persona frames and bounds expressive vocal interactions (both biological and synthesized), and centers a person's agency over their communicative role. This article provides insights into opportunities for improvement in Voice User Interfaces (VUI) and Augmentative \\&amp;amp; Assistive Communications (AAC) technologies based on the study's results. The proposed contextualized production-perception model fills an important gap in the literature on expressive and interactive speech technologies by identifying a key missing mechanism in contextualized vocal communication models and providing a resource for more nuanced approaches to expressive speech synthesis methods. Incorporating vocal persona into expressive vocal synthesis has the potential to significantly enhance the level of agency and embodiment experienced by VUI and AAC users during communication, resulting in a heightened sense of authenticity and an improved relationship with their environment.",
      "doi": "https://doi.org/10.31234/osf.io/ebh2s",
      "openalex_id": "https://openalex.org/W4385381365",
      "arxiv_id": "",
      "publication_date": "2023-07-28",
      "published": "2023-07-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neutral TTS Female Voice Corpus in Brazilian Portuguese",
      "summary": "This paper introduces a new dataset designed to address the limitations in high-quality, diverse and representative datasets for training text-to-speech (TTS) models, specifically for female voices in Brazilian Portuguese.The dataset features a female voice recorded in a professional and controlled environment with neutral emotion and comprises more than 20 hours of recordings.The goal is to facilitate transfer learning and enable the development of more natural-sounding, high-quality, and gender-balanced TTS systems.Alongside the dataset, genderaware voice transfer experiments are performed to understand the impact of utilizing gender-specific pretrained models for speech synthesis.The results obtained show that same-gender voice transfer yields better speech similarity and intelligibility when compared to cross-gender transfer, emphasizing the importance of gender-aware training procedures and highlighting the need for balanced gender data.",
      "abstract": "This paper introduces a new dataset designed to address the limitations in high-quality, diverse and representative datasets for training text-to-speech (TTS) models, specifically for female voices in Brazilian Portuguese.The dataset features a female voice recorded in a professional and controlled environment with neutral emotion and comprises more than 20 hours of recordings.The goal is to facilitate transfer learning and enable the development of more natural-sounding, high-quality, and gender-balanced TTS systems.Alongside the dataset, genderaware voice transfer experiments are performed to understand the impact of utilizing gender-specific pretrained models for speech synthesis.The results obtained show that same-gender voice transfer yields better speech similarity and intelligibility when compared to cross-gender transfer, emphasizing the importance of gender-aware training procedures and highlighting the need for balanced gender data.",
      "doi": "https://doi.org/10.14209/sbrt.2023.1570917697",
      "openalex_id": "https://openalex.org/W4388272221",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Current and Evolving Applications to Speech Processing",
      "summary": "Automatic speech recognition (ASR) and text-to-speech (TTS) synthesis have been around for several decades. Although ASR and TTS deal with basic voice processing, including traditional vocoding mechanisms, artificial intelligence, especially machine learning/deep learning techniques, is increasingly being used to enhance the accuracy of recognition in ASR, and to enhance sentence fluency and intonation in TTS. The last decade has also seen major breakthroughs in speech synthesis by applying neural network methods and end-to-end modeling. ASR entails methods for converting the acoustic speech waveform into a sequence of textual words. This is traditionally done using statistical modeling of the speech signal. Noise cancellation mechanisms are typically used to deal with noise. In training of a dense neural network architecture used in ASR, a regression mechanism often includes a minimization of a pre-defined cost function.",
      "abstract": "Automatic speech recognition (ASR) and text-to-speech (TTS) synthesis have been around for several decades. Although ASR and TTS deal with basic voice processing, including traditional vocoding mechanisms, artificial intelligence, especially machine learning/deep learning techniques, is increasingly being used to enhance the accuracy of recognition in ASR, and to enhance sentence fluency and intonation in TTS. The last decade has also seen major breakthroughs in speech synthesis by applying neural network methods and end-to-end modeling. ASR entails methods for converting the acoustic speech waveform into a sequence of textual words. This is traditionally done using statistical modeling of the speech signal. Noise cancellation mechanisms are typically used to deal with noise. In training of a dense neural network architecture used in ASR, a regression mechanism often includes a minimization of a pre-defined cost function.",
      "doi": "https://doi.org/10.1002/9781394190034.ch3",
      "openalex_id": "https://openalex.org/W4388558441",
      "arxiv_id": "",
      "publication_date": "2023-11-10",
      "published": "2023-11-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ELEGANT: End-to-end Language Grounded Speech Denoiser for Efficient Generation of Talking Face",
      "summary": "Existing speech driven talking face generation methods (a.k.a Speech2Face models) provide realistic-looking talking avatars. Yet, they are not appropriate if (1) the input speech signals are from the wild which could contain background noise; and (2) the input signal contains hate speech. In the presence of in-the-wild audio signals, the Speech2Face models do a bad lip-sync, unwanted facial movements, and sudden jitters on the head movements. While on the other hand, the Speech2Face models do not perform any reasoning on language understanding of input speech signal which could enable malicious users to translate hateful speech to a synthetic talking face promoting internet, social, and political threats. In this paper, we serve dual objectives on a single go. To the best of our knowledge, our method ELEGANT is the first Speech2Face generative model that performs a language grounding on the input speech that eliminates the transfer of spurious features originating from audio noise. Subsequently, the text embedding is associated with the speech style and passed on to a generative model with a view to learn the phoneme-viseme correspondence. In this way, our proposed ELEGANT model suppresses negative and hateful words using text embedding and also suppresses audio-specific noises using text embedding since noise-to-phoneme mapping would be random. Our experiments show that adopting the speech-denoising technique through text grounding eliminates the transfer of spurious features originating from audio noise to the vision domain. Consequently, a good phoneme-viseme correspondence leads to a comparable performances of SSIM and PSNR scores w.r.t state of the art methods.",
      "abstract": "Existing speech driven talking face generation methods (a.k.a Speech2Face models) provide realistic-looking talking avatars. Yet, they are not appropriate if (1) the input speech signals are from the wild which could contain background noise; and (2) the input signal contains hate speech. In the presence of in-the-wild audio signals, the Speech2Face models do a bad lip-sync, unwanted facial movements, and sudden jitters on the head movements. While on the other hand, the Speech2Face models do not perform any reasoning on language understanding of input speech signal which could enable malicious users to translate hateful speech to a synthetic talking face promoting internet, social, and political threats. In this paper, we serve dual objectives on a single go. To the best of our knowledge, our method ELEGANT is the first Speech2Face generative model that performs a language grounding on the input speech that eliminates the transfer of spurious features originating from audio noise. Subsequently, the text embedding is associated with the speech style and passed on to a generative model with a view to learn the phoneme-viseme correspondence. In this way, our proposed ELEGANT model suppresses negative and hateful words using text embedding and also suppresses audio-specific noises using text embedding since noise-to-phoneme mapping would be random. Our experiments show that adopting the speech-denoising technique through text grounding eliminates the transfer of spurious features originating from audio noise to the vision domain. Consequently, a good phoneme-viseme correspondence leads to a comparable performances of SSIM and PSNR scores w.r.t state of the art methods.",
      "doi": "https://doi.org/10.1109/apsipaasc58517.2023.10317456",
      "openalex_id": "https://openalex.org/W4388821451",
      "arxiv_id": "",
      "publication_date": "2023-10-31",
      "published": "2023-10-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Invert-Classify: Recovering Discrete Prosody Inputs for Text-To-Speech",
      "summary": "Modeling prosody in Text-to-Speech (TTS) is challenging due to ambiguous orthography and the high cost of annotating prosodic events. This study focuses on the modeling of contrastive focus, the emphasis of a word to contrast it to presuppositions held by an interlocutor. Modeling of contrastive focus can be done in TTS by using binary, symbolic inputs at the word level in a supervised setting. To address the absence of annotated data, we propose the Invert-Classify method, which leverages a frozen TTS model and unlabeled parallel text-speech data to recover missing contrastive focus inputs. Our approach achieves a binary F-score of up to 0.71 for contrastive focus annotation recovery, utilizing only 5-10 % of annotated training data. Furthermore, subjective listening tests show that training on additional data labeled via Invert-Classify enhances overall synthesis quality, as well as providing good control and plausible-sounding contrastive focus.",
      "abstract": "Modeling prosody in Text-to-Speech (TTS) is challenging due to ambiguous orthography and the high cost of annotating prosodic events. This study focuses on the modeling of contrastive focus, the emphasis of a word to contrast it to presuppositions held by an interlocutor. Modeling of contrastive focus can be done in TTS by using binary, symbolic inputs at the word level in a supervised setting. To address the absence of annotated data, we propose the Invert-Classify method, which leverages a frozen TTS model and unlabeled parallel text-speech data to recover missing contrastive focus inputs. Our approach achieves a binary F-score of up to 0.71 for contrastive focus annotation recovery, utilizing only 5-10 % of annotated training data. Furthermore, subjective listening tests show that training on additional data labeled via Invert-Classify enhances overall synthesis quality, as well as providing good control and plausible-sounding contrastive focus.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389773",
      "openalex_id": "https://openalex.org/W4391021539",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generation of Synthetic Echocardiograms Using Video Diffusion Models",
      "summary": "An echocardiogram is a video sequence of a human heart captured using ultrasound imaging, which helps in diagnosis of cardiovascular diseases. Deep learning methods, which require large amounts of training data, have shown success in using echocardiograms to detect cardiovascular disorders. Large datasets of echocardiograms that can be used for machine learning training are scarce. This problem can be addressed by generating synthetic echocardiograms that can be used for machine learning training. In this paper, we propose a video diffusion method for echocardiograms generation. We show that our method generates better echocardiograms with higher resolution as compared to existing methods.",
      "abstract": "An echocardiogram is a video sequence of a human heart captured using ultrasound imaging, which helps in diagnosis of cardiovascular diseases. Deep learning methods, which require large amounts of training data, have shown success in using echocardiograms to detect cardiovascular disorders. Large datasets of echocardiograms that can be used for machine learning training are scarce. This problem can be addressed by generating synthetic echocardiograms that can be used for machine learning training. In this paper, we propose a video diffusion method for echocardiograms generation. We show that our method generates better echocardiograms with higher resolution as compared to existing methods.",
      "doi": "https://doi.org/10.1109/ssiai59505.2024.10508643",
      "openalex_id": "https://openalex.org/W4396215619",
      "arxiv_id": "",
      "publication_date": "2024-03-17",
      "published": "2024-03-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Notes on artificial intelligence: concepts, applications and techniques",
      "summary": "Artificial Intelligence (AI) has been increasingly present in the contemporary world, with applicability in various fields of knowledge. AI-based solutions, implemented with different techniques, are present in different systems. The purpose of this article is to (1) present a brief history and the main concepts related to Artificial Intelligence - with a focus on machine learning (ML) techniques -, (2) discuss its nuances, techniques and (3) present some application examples in several areas. The intention is to approach the subject in an accessible way to the non-specialized public, in order to promote the understanding of its main concepts, but without the need to resort to very technical details. For this, a narrative review of the literature on AI is carried out, briefly addressing its history, concepts, intersections with other areas and applications. The machine learning segment receives special attention, with emphasis on supervised learning and its application in classification problems. The article also highlights the capabilities presented by the most modern machine learning techniques which, in some cases, present better results than those obtained by humans. Given these promising results, the future of AI points to the creation of systems with capabilities that will increasingly resemble those of the human intellect. However, the creation of truly thinking machines, and not of thinking simulators, should still remain for a long time as an objective to be achieved.",
      "abstract": "Artificial Intelligence (AI) has been increasingly present in the contemporary world, with applicability in various fields of knowledge. AI-based solutions, implemented with different techniques, are present in different systems. The purpose of this article is to (1) present a brief history and the main concepts related to Artificial Intelligence - with a focus on machine learning (ML) techniques -, (2) discuss its nuances, techniques and (3) present some application examples in several areas. The intention is to approach the subject in an accessible way to the non-specialized public, in order to promote the understanding of its main concepts, but without the need to resort to very technical details. For this, a narrative review of the literature on AI is carried out, briefly addressing its history, concepts, intersections with other areas and applications. The machine learning segment receives special attention, with emphasis on supervised learning and its application in classification problems. The article also highlights the capabilities presented by the most modern machine learning techniques which, in some cases, present better results than those obtained by humans. Given these promising results, the future of AI points to the creation of systems with capabilities that will increasingly resemble those of the human intellect. However, the creation of truly thinking machines, and not of thinking simulators, should still remain for a long time as an objective to be achieved.",
      "doi": "https://doi.org/10.61411/rsc202457217",
      "openalex_id": "https://openalex.org/W4400268946",
      "arxiv_id": "",
      "publication_date": "2024-07-03",
      "published": "2024-07-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ConvAtt Network: A Low Parameter Approach For Sign Language Recognition",
      "summary": "Despite recent advances in Large Language Models in text processing, Sign Language Recognition (SLR) remains an unresolved task. This is, in part, due to limitations in the available data. In this paper, we investigate combining 1D convolutions with transformer layers to capture local features and global interactions in a low-parameter SLR model. We experimented using multiple data augmentation and regularization techniques to categorize signs of the French Belgian Sign Language. We achieved a top-1 accuracy of 42.7% and a top-10 accuracy of 81.9% in 600 different signs. This model is competitive with the current state of the art while using a significantly lower number of parameters.",
      "abstract": "Despite recent advances in Large Language Models in text processing, Sign Language Recognition (SLR) remains an unresolved task. This is, in part, due to limitations in the available data. In this paper, we investigate combining 1D convolutions with transformer layers to capture local features and global interactions in a low-parameter SLR model. We experimented using multiple data augmentation and regularization techniques to categorize signs of the French Belgian Sign Language. We achieved a top-1 accuracy of 42.7% and a top-10 accuracy of 81.9% in 600 different signs. This model is competitive with the current state of the art while using a significantly lower number of parameters.",
      "doi": "https://doi.org/10.24215/16666038.24.e10",
      "openalex_id": "https://openalex.org/W4403524969",
      "arxiv_id": "",
      "publication_date": "2024-10-18",
      "published": "2024-10-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparing human-labeled and AI-labeled speech datasets for TTS",
      "summary": "As the output quality of neural networks in the fields of automatic speech recognition (ASR) and text-to-speech (TTS) continues to improve, new opportunities are becoming available to train models in a weakly supervised fashion, thus minimizing the manual effort required to annotate new audio data for supervised training. While weak supervision has recently shown very promising results in the domain of ASR, speech synthesis has not yet been thoroughly investigated regarding this technique despite requiring the equivalent training dataset structure of aligned audio-transcript pairs. In this work, we compare the performance of TTS models trained using a well-curated and manually labeled training dataset to others trained on the same audio data with text labels generated using both grapheme- and phoneme-based ASR models. Phoneme-based approaches seem especially promising, since even for wrongly predicted phonemes, the resulting word is more likely to sound similar to the originally spoken word than for grapheme-based predictions. For evaluation and ranking, we generate synthesized audio outputs from all previously trained models using input texts sourced from a selection of speech recognition datasets covering a wide range of application domains. These synthesized outputs are subsequently fed into multiple state-of-the-art ASR models with their output text predictions being compared to the initial TTS model input texts. This comparison enables an objective assessment of the intelligibility of the audio outputs from all TTS models, by utilizing metrics like word error rate and character error rate. Our results not only show that models trained on data generated with weak supervision achieve comparable quality to models trained on manually labeled datasets, but can outperform the latter, even for small, well-curated speech datasets. These findings suggest that the future creation of labeled datasets for supervised training of TTS models may not require any manual annotation but can be fully automated.",
      "abstract": "As the output quality of neural networks in the fields of automatic speech recognition (ASR) and text-to-speech (TTS) continues to improve, new opportunities are becoming available to train models in a weakly supervised fashion, thus minimizing the manual effort required to annotate new audio data for supervised training. While weak supervision has recently shown very promising results in the domain of ASR, speech synthesis has not yet been thoroughly investigated regarding this technique despite requiring the equivalent training dataset structure of aligned audio-transcript pairs. In this work, we compare the performance of TTS models trained using a well-curated and manually labeled training dataset to others trained on the same audio data with text labels generated using both grapheme- and phoneme-based ASR models. Phoneme-based approaches seem especially promising, since even for wrongly predicted phonemes, the resulting word is more likely to sound similar to the originally spoken word than for grapheme-based predictions. For evaluation and ranking, we generate synthesized audio outputs from all previously trained models using input texts sourced from a selection of speech recognition datasets covering a wide range of application domains. These synthesized outputs are subsequently fed into multiple state-of-the-art ASR models with their output text predictions being compared to the initial TTS model input texts. This comparison enables an objective assessment of the intelligibility of the audio outputs from all TTS models, by utilizing metrics like word error rate and character error rate. Our results not only show that models trained on data generated with weak supervision achieve comparable quality to models trained on manually labeled datasets, but can outperform the latter, even for small, well-curated speech datasets. These findings suggest that the future creation of labeled datasets for supervised training of TTS models may not require any manual annotation but can be fully automated.",
      "doi": "https://doi.org/10.34190/icair.5.1.3030",
      "openalex_id": "https://openalex.org/W4405087680",
      "arxiv_id": "",
      "publication_date": "2024-12-04",
      "published": "2024-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Development of Arabic Reading Materials Based on Moderation with Artificial Intelligence/Tathwir Mawad al-Qira’ah ‘ala Asas al-Wasathiyah bi Istikhdam al-Dzaka’ al-Ishthina’iy",
      "summary": "Qira’ah learning materials often fail to effectively integrate the values of moderation. Additionally, conventional learning methods sometimes struggle to engage students and promote independent learning. Therefore, there is a need for innovative qira’ah materials that not only focus on Arabic reading skills but also instill the values of moderation. Artificial Intelligence (AI) technology offers a solution through interactive features, automated feedback, and adaptive learning, which can enrich the learning experience and deepen students’ understanding of moderation values in Arabic texts. This article explored the development of qira’ah learning materials based on moderation, supported by AI technology, at STAI Darul Ulum Banyuanyar Pamekasan. This research employed the ADDIE development model (Analysis, Design, Development, Implementation, Evaluation). Data collection techniques included observation, interviews, questionnaires, documentation, and tests. The qualitative data analysis technique used was classification analysis, while the quantitative analysis applies the gain score formula: d=T2−T1d = T2 - T1d=T2−T1. The findings indicate that qira’ah learning materials based on moderation, supported by AI, can enhance students' understanding of moderation values and boost their learning motivation. With interactive features, learning becomes more adaptive and responsive to students’ needs, allowing them to study more independently and effectively. This research offered a substantial contribution to the advancement of Arabic language education that integrates both technological innovation and the principles of moderation. It highlighted how AI integration in qira’ah materials can significantly improve learning effectiveness, promote learner autonomy, and embed moderation values in a more profound and contextualized way.",
      "abstract": "Qira’ah learning materials often fail to effectively integrate the values of moderation. Additionally, conventional learning methods sometimes struggle to engage students and promote independent learning. Therefore, there is a need for innovative qira’ah materials that not only focus on Arabic reading skills but also instill the values of moderation. Artificial Intelligence (AI) technology offers a solution through interactive features, automated feedback, and adaptive learning, which can enrich the learning experience and deepen students’ understanding of moderation values in Arabic texts. This article explored the development of qira’ah learning materials based on moderation, supported by AI technology, at STAI Darul Ulum Banyuanyar Pamekasan. This research employed the ADDIE development model (Analysis, Design, Development, Implementation, Evaluation). Data collection techniques included observation, interviews, questionnaires, documentation, and tests. The qualitative data analysis technique used was classification analysis, while the quantitative analysis applies the gain score formula: d=T2−T1d = T2 - T1d=T2−T1. The findings indicate that qira’ah learning materials based on moderation, supported by AI, can enhance students' understanding of moderation values and boost their learning motivation. With interactive features, learning becomes more adaptive and responsive to students’ needs, allowing them to study more independently and effectively. This research offered a substantial contribution to the advancement of Arabic language education that integrates both technological innovation and the principles of moderation. It highlighted how AI integration in qira’ah materials can significantly improve learning effectiveness, promote learner autonomy, and embed moderation values in a more profound and contextualized way.",
      "doi": "https://doi.org/10.24042/c1qfz450",
      "openalex_id": "https://openalex.org/W4411350491",
      "arxiv_id": "",
      "publication_date": "2025-06-06",
      "published": "2025-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring fairness in service robotics",
      "summary": "Abstract Fairness in service robotics is a complex and multidimensional concept shaped by legal, social and technical considerations. As service robots increasingly operate in personal and professional domains, questions of fairness – ranging from legal certainty and anti-discrimination to user protection and algorithmic transparency – require systematic and interdisciplinary engagement. This paper develops a working definition of fairness tailored to the domain of service robotics based on a doctrinal analysis of how fairness is understood across different fields. It identifies four key dimensions essential to fair service robotics: (i) furthering legal certainty, (ii) preventing bias and discrimination, (iii) protecting users from exploitation and (iv) ensuring transparency and accountability. The paper explores how developers, policymakers and researchers can contribute to these goals. While fairness may resist universal definition, articulating its core components offers a foundation for guiding more equitable and trustworthy human–robot interactions.",
      "abstract": "Abstract Fairness in service robotics is a complex and multidimensional concept shaped by legal, social and technical considerations. As service robots increasingly operate in personal and professional domains, questions of fairness – ranging from legal certainty and anti-discrimination to user protection and algorithmic transparency – require systematic and interdisciplinary engagement. This paper develops a working definition of fairness tailored to the domain of service robotics based on a doctrinal analysis of how fairness is understood across different fields. It identifies four key dimensions essential to fair service robotics: (i) furthering legal certainty, (ii) preventing bias and discrimination, (iii) protecting users from exploitation and (iv) ensuring transparency and accountability. The paper explores how developers, policymakers and researchers can contribute to these goals. While fairness may resist universal definition, articulating its core components offers a foundation for guiding more equitable and trustworthy human–robot interactions.",
      "doi": "https://doi.org/10.1017/cfl.2025.10012",
      "openalex_id": "https://openalex.org/W4411650237",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Expressiveness in Vocal Conversational Agents through Large Language Model-Generated Speech Synthesis Markup Language",
      "summary": "Advancements in speech synthesis have enabled more natural and engaging conversational agents, including neural text-to-speech models that can adjust speech inflections to produce distinct vocal styles. For example, Azure Neural Voices can adjust speech using Speech Synthesis Markup Language (SSML) style tags, such as “affectionate,” “cheerful,” and “hopeful.” However, determining when to apply these tags in real-time interactions can be challenging and time-consuming. In this paper, we present a prompt-based approach that enables large language models (LLMs) to dynamically stylize their responses with appropriate SSML tags, enhancing synthesized speech expressiveness across 34 different styles. Using targeted probes designed to elicit specific speech styles, we demonstrate that LLM-generated responses are syntactically well-formed and correctly apply style tags to enhance expressiveness. This simple, customizable approach facilitates the rapid development of expressive vocal conversational agents.",
      "abstract": "Advancements in speech synthesis have enabled more natural and engaging conversational agents, including neural text-to-speech models that can adjust speech inflections to produce distinct vocal styles. For example, Azure Neural Voices can adjust speech using Speech Synthesis Markup Language (SSML) style tags, such as “affectionate,” “cheerful,” and “hopeful.” However, determining when to apply these tags in real-time interactions can be challenging and time-consuming. In this paper, we present a prompt-based approach that enables large language models (LLMs) to dynamically stylize their responses with appropriate SSML tags, enhancing synthesized speech expressiveness across 34 different styles. Using targeted probes designed to elicit specific speech styles, we demonstrate that LLM-generated responses are syntactically well-formed and correctly apply style tags to enhance expressiveness. This simple, customizable approach facilitates the rapid development of expressive vocal conversational agents.",
      "doi": "https://doi.org/10.32473/flairs.38.1.138814",
      "openalex_id": "https://openalex.org/W4410398715",
      "arxiv_id": "",
      "publication_date": "2025-05-14",
      "published": "2025-05-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
      "summary": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "abstract": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.327",
      "openalex_id": "https://openalex.org/W4389519824",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
      "summary": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "doi": "https://doi.org/10.1051/itmconf/20246901003",
      "openalex_id": "https://openalex.org/W4405379347",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
      "summary": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "doi": "https://doi.org/10.18653/v1/2023.iwslt-1.33",
      "openalex_id": "https://openalex.org/W4385571610",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep clustering analysis via variational autoencoder with Gamma mixture latent embeddings",
      "summary": "This article proposes a novel deep clustering model based on the variational autoencoder (VAE), named GamMM-VAE, which can learn latent representations of training data for clustering in an unsupervised manner. Most existing VAE-based deep clustering methods use the Gaussian mixture model (GMM) as a prior on the latent space. We employ a more flexible asymmetric Gamma mixture model to achieve higher quality embeddings of the data latent space. Second, since the Gamma is defined for strictly positive variables, in order to exploit the reparameterization trick of VAE, we propose a transformation method from Gaussian distribution to Gamma distribution. This method can also be considered a Gamma distribution reparameterization trick, allows gradients to be backpropagated through the sampling process in the VAE. Finally, we derive the evidence lower bound (ELBO) based on the Gamma mixture model in an effective way for the stochastic gradient variational Bayesian (SGVB) estimator to optimize the proposed model. ELBO, a variational inference objective, ensures the maximization of the approximation of the posterior distribution, while SGVB is a method used to perform efficient inference and learning in VAEs. We validate the effectiveness of our model through quantitative comparisons with other state-of-the-art deep clustering models on six benchmark datasets. Moreover, due to the generative nature of VAEs, the proposed model can generate highly realistic samples of specific classes without supervised information.",
      "abstract": "This article proposes a novel deep clustering model based on the variational autoencoder (VAE), named GamMM-VAE, which can learn latent representations of training data for clustering in an unsupervised manner. Most existing VAE-based deep clustering methods use the Gaussian mixture model (GMM) as a prior on the latent space. We employ a more flexible asymmetric Gamma mixture model to achieve higher quality embeddings of the data latent space. Second, since the Gamma is defined for strictly positive variables, in order to exploit the reparameterization trick of VAE, we propose a transformation method from Gaussian distribution to Gamma distribution. This method can also be considered a Gamma distribution reparameterization trick, allows gradients to be backpropagated through the sampling process in the VAE. Finally, we derive the evidence lower bound (ELBO) based on the Gamma mixture model in an effective way for the stochastic gradient variational Bayesian (SGVB) estimator to optimize the proposed model. ELBO, a variational inference objective, ensures the maximization of the approximation of the posterior distribution, while SGVB is a method used to perform efficient inference and learning in VAEs. We validate the effectiveness of our model through quantitative comparisons with other state-of-the-art deep clustering models on six benchmark datasets. Moreover, due to the generative nature of VAEs, the proposed model can generate highly realistic samples of specific classes without supervised information.",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106979",
      "openalex_id": "https://openalex.org/W4405010156",
      "arxiv_id": "",
      "publication_date": "2024-12-04",
      "published": "2024-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Variational Graph Convolutional Networks for Dynamic Graph Representation Learning",
      "summary": "The ubiquitous and ever-evolving nature of cyber threats demands innovative approaches that can adapt to the dynamic relationships and structures within network data. Traditional models struggle to adapt to the constantly changing nature of network traffic, where both structural dependencies and temporal evolution must be accurately captured to detect anomalies and predict future threats. To address the challenges, this research introduces V-GCN (Variational Graph Convolutional Network), a new model that integrates the probabilistic latent space modelling of Variational Autoencoders (VAEs) with the structural learning capabilities of Graph Convolutional Networks (GCNs). The proposed model is designed to capture both temporal dependencies and uncertainties inherent in dynamic networks, and as such, it is highly suitable for tasks such as link prediction and node classification. The proposed hybrid model encodes node features into a probabilistic latent space using a VAE encoder and refine the representations using GCN layers, that aggregates structural information from neighbouring nodes. The integration of variational inference with graph convolution enables V-GCN to adapt to the dynamic evolution of network traffic and measure the uncertainties in node and edge relationships. The DynKDD dataset, a dynamic adaptation of the NSL-KDD dataset, is developed in this research to evaluate the model performance. The dataset introduces temporal dynamics into the conventional NSL-KDD dataset, enabling the application of advanced graph-based learning models such as V-GCN. Experimental evaluation indicates that V-GCN significantly outperforms baseline models such as GCNs, Graph Sample and Aggregation (GraphSAGE), and Graph Attention Networks (GATs). In node classification, V-GCN achieved a 10% higher F1-score (0.845), with precision reaching 83.7%, and a balanced accuracy of 84.2%, underscoring its ability to handle uncertainty and adapt to changing network structures in dynamic environments. V-GCN achieved a 15% improvement in AUC-ROC (0.98), a 12% increase in average precision (0.9357), and a 14% higher F1-score (0.8196) in link prediction tasks compared to baseline models. The V-GCN&#x2019;s integration of probabilistic modelling and graph convolution sets a new benchmark for dynamic network traffic analysis, providing a superior solution to real-world challenges in cybersecurity, social network analysis and beyond.",
      "abstract": "The ubiquitous and ever-evolving nature of cyber threats demands innovative approaches that can adapt to the dynamic relationships and structures within network data. Traditional models struggle to adapt to the constantly changing nature of network traffic, where both structural dependencies and temporal evolution must be accurately captured to detect anomalies and predict future threats. To address the challenges, this research introduces V-GCN (Variational Graph Convolutional Network), a new model that integrates the probabilistic latent space modelling of Variational Autoencoders (VAEs) with the structural learning capabilities of Graph Convolutional Networks (GCNs). The proposed model is designed to capture both temporal dependencies and uncertainties inherent in dynamic networks, and as such, it is highly suitable for tasks such as link prediction and node classification. The proposed hybrid model encodes node features into a probabilistic latent space using a VAE encoder and refine the representations using GCN layers, that aggregates structural information from neighbouring nodes. The integration of variational inference with graph convolution enables V-GCN to adapt to the dynamic evolution of network traffic and measure the uncertainties in node and edge relationships. The DynKDD dataset, a dynamic adaptation of the NSL-KDD dataset, is developed in this research to evaluate the model performance. The dataset introduces temporal dynamics into the conventional NSL-KDD dataset, enabling the application of advanced graph-based learning models such as V-GCN. Experimental evaluation indicates that V-GCN significantly outperforms baseline models such as GCNs, Graph Sample and Aggregation (GraphSAGE), and Graph Attention Networks (GATs). In node classification, V-GCN achieved a 10% higher F1-score (0.845), with precision reaching 83.7%, and a balanced accuracy of 84.2%, underscoring its ability to handle uncertainty and adapt to changing network structures in dynamic environments. V-GCN achieved a 15% improvement in AUC-ROC (0.98), a 12% increase in average precision (0.9357), and a 14% higher F1-score (0.8196) in link prediction tasks compared to baseline models. The V-GCN&#x2019;s integration of probabilistic modelling and graph convolution sets a new benchmark for dynamic network traffic analysis, providing a superior solution to real-world challenges in cybersecurity, social network analysis and beyond.",
      "doi": "https://doi.org/10.1109/access.2024.3483839",
      "openalex_id": "https://openalex.org/W4403598640",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "STEMGEN: A Music Generation Model That Listens",
      "summary": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "abstract": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446088",
      "openalex_id": "https://openalex.org/W4392904237",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446160",
      "openalex_id": "https://openalex.org/W4392902857",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generation-Based Target Speech Extraction with Speech Discretization and Vocoder",
      "summary": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "abstract": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446418",
      "openalex_id": "https://openalex.org/W4392903977",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "V2Meow: Meowing to the Visual Beat via Video-to-Music Generation",
      "summary": "Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.",
      "abstract": "Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.",
      "doi": "https://doi.org/10.1609/aaai.v38i5.28299",
      "openalex_id": "https://openalex.org/W4393148499",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models",
      "summary": "Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.",
      "abstract": "Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447122",
      "openalex_id": "https://openalex.org/W4392904047",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CPTGZ: Generating Chinese Guzheng Music From Chinese Paintings Based on Diffusion Model",
      "summary": "In the context of rapid advancements in artificial intelligence technology, AI-powered music composition has demonstrated remarkable creative capabilities. However, no existing music generation model has been able to produce authentic waveform-level traditional Chinese music. To explore the potential of this field and address the limitations of current technologies in generating traditional Chinese music, this study introduces CPTGZ (Chinese Painting to Guzheng Music), a music generation model based on latent diffusion and Transformer architectures. CPTGZ aims to achieve automatic generation of waveform-level Guzheng music from Chinese paintings, thereby addressing the inability of existing music generation models to produce traditional Chinese music.To support the development and training of the model, we constructed a large-scale dataset of paired Chinese paintings and Guzheng music, consisting of 22,103 sample pairs. Through experimental evaluation, we found that CPTGZ exhibits excellent performance in terms of music quality and Guzheng-specific characteristics. The results demonstrate that our model can generate Chinese Guzheng music pieces highly correlated in style and semantics with the input Chinese paintings. Furthermore, the musical qualities of the generated Guzheng compositions demonstrate the characteristics of traditional Chinese music, thus validating the feasibility and effectiveness of our model.This research contributes to the field of AI-driven music generation by addressing the specific challenges of creating authentic traditional Chinese music, particularly Guzheng compositions, based on visual art inputs. The successful implementation of CPTGZ not only opens new avenues for cross-modal generation in the domain of culturally specific art forms, but also demonstrates the potential for AI to preserve and innovate within traditional art forms.",
      "abstract": "In the context of rapid advancements in artificial intelligence technology, AI-powered music composition has demonstrated remarkable creative capabilities. However, no existing music generation model has been able to produce authentic waveform-level traditional Chinese music. To explore the potential of this field and address the limitations of current technologies in generating traditional Chinese music, this study introduces CPTGZ (Chinese Painting to Guzheng Music), a music generation model based on latent diffusion and Transformer architectures. CPTGZ aims to achieve automatic generation of waveform-level Guzheng music from Chinese paintings, thereby addressing the inability of existing music generation models to produce traditional Chinese music.To support the development and training of the model, we constructed a large-scale dataset of paired Chinese paintings and Guzheng music, consisting of 22,103 sample pairs. Through experimental evaluation, we found that CPTGZ exhibits excellent performance in terms of music quality and Guzheng-specific characteristics. The results demonstrate that our model can generate Chinese Guzheng music pieces highly correlated in style and semantics with the input Chinese paintings. Furthermore, the musical qualities of the generated Guzheng compositions demonstrate the characteristics of traditional Chinese music, thus validating the feasibility and effectiveness of our model.This research contributes to the field of AI-driven music generation by addressing the specific challenges of creating authentic traditional Chinese music, particularly Guzheng compositions, based on visual art inputs. The successful implementation of CPTGZ not only opens new avenues for cross-modal generation in the domain of culturally specific art forms, but also demonstrates the potential for AI to preserve and innovate within traditional art forms.",
      "doi": "https://doi.org/10.1109/access.2024.3476998",
      "openalex_id": "https://openalex.org/W4403277663",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
      "summary": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.704",
      "openalex_id": "https://openalex.org/W4389519009",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
      "summary": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "doi": "https://doi.org/10.1109/ic2pct60090.2024.10486323",
      "openalex_id": "https://openalex.org/W4394583388",
      "arxiv_id": "",
      "publication_date": "2024-02-09",
      "published": "2024-02-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Study Experience using Handwritten Character and Digit Recognition and Text Summarization",
      "summary": "The integration of Handwritten characters and, Digit Recognition and Deep Learning in education heralds a transformative era in learning methodologies. This abstract delves into the multifaceted benefits derived from the amalgamation of these technologies, redefining the educational landscape. Handwritten characters and Digit Recognition technology facilitates the seamless digitization of handwritten content, transcending the limitations of manual note-taking. Its introduction into educational frameworks enhances accessibility, promotes organization, and augments the searchability of diverse educational materials. Deep Learning, acting in tandem with Handwriting Recognition, amplifies these advantages manifold. Deep learning powered study assistants offer personalized learning experiences tailored to individual needs, adapting to varied learning styles. Additionally, these assistants facilitate collaborative opportunities, providing real time feedback and evaluation tools that revolutionize the learning process. Key Words: Image recognition, CNN, RNN, LSTM, Neural Networks, SVM, Deep Learning, Convolutional layer, PreLU, ReLU, Text Summarization, Extractive text summarization, Tokenization.",
      "abstract": "The integration of Handwritten characters and, Digit Recognition and Deep Learning in education heralds a transformative era in learning methodologies. This abstract delves into the multifaceted benefits derived from the amalgamation of these technologies, redefining the educational landscape. Handwritten characters and Digit Recognition technology facilitates the seamless digitization of handwritten content, transcending the limitations of manual note-taking. Its introduction into educational frameworks enhances accessibility, promotes organization, and augments the searchability of diverse educational materials. Deep Learning, acting in tandem with Handwriting Recognition, amplifies these advantages manifold. Deep learning powered study assistants offer personalized learning experiences tailored to individual needs, adapting to varied learning styles. Additionally, these assistants facilitate collaborative opportunities, providing real time feedback and evaluation tools that revolutionize the learning process. Key Words: Image recognition, CNN, RNN, LSTM, Neural Networks, SVM, Deep Learning, Convolutional layer, PreLU, ReLU, Text Summarization, Extractive text summarization, Tokenization.",
      "doi": "https://doi.org/10.55041/ijsrem30665",
      "openalex_id": "https://openalex.org/W4394785933",
      "arxiv_id": "",
      "publication_date": "2024-04-13",
      "published": "2024-04-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting",
      "summary": "We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.",
      "abstract": "We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.",
      "doi": "https://doi.org/10.1109/taslp.2024.3463395",
      "openalex_id": "https://openalex.org/W4402592728",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Foundation Models for Low-Resource Language Education (Vision Paper)",
      "summary": "Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.",
      "abstract": "Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.",
      "doi": "https://doi.org/10.32388/iqu339",
      "openalex_id": "https://openalex.org/W4406247986",
      "arxiv_id": "",
      "publication_date": "2025-01-10",
      "published": "2025-01-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "“Wild West” of Evaluating Speech‐Driven 3D Facial Animation Synthesis: A Benchmark Study",
      "summary": "Abstract Recent advancements in the field of audio‐driven 3D facial animation have accelerated rapidly, with numerous papers being published in a short span of time. This surge in research has garnered significant attention from both academia and industry with its potential applications on digital humans. Various approaches, both deterministic and non‐deterministic, have been explored based on foundational advancements in deep learning algorithms. However, there remains no consensus among researchers on standardized methods for evaluating these techniques. Additionally, rather than converging on a common set of datasets and objective metrics suited for specific methods, recent works exhibit considerable variation in experimental setups. This inconsistency complicates the research landscape, making it difficult to establish a streamlined evaluation process and rendering many cross‐paper comparisons challenging. Moreover, the common practice of A/B testing in perceptual studies focus only on two common metrics and not sufficient for non‐deterministic and emotion‐enabled approaches. The lack of correlations between subjective and objective metrics points out that there is a need for critical analysis in this space. In this study, we address these issues by benchmarking state‐of‐the‐art deterministic and non‐deterministic models, utilizing a consistent experimental setup across a carefully curated set of objective metrics and datasets. We also conduct a perceptual user study to assess whether subjective perceptual metrics align with the objective metrics. Our findings indicate that model rankings do not necessarily generalize across datasets, and subjective metric ratings are not always consistent with their corresponding objective metrics. The supplementary video, edited code scripts for training on different datasets and documentation related to this benchmark study are made publicly available‐ https://galib360.github.io/face-benchmark-project/ .",
      "abstract": "Abstract Recent advancements in the field of audio‐driven 3D facial animation have accelerated rapidly, with numerous papers being published in a short span of time. This surge in research has garnered significant attention from both academia and industry with its potential applications on digital humans. Various approaches, both deterministic and non‐deterministic, have been explored based on foundational advancements in deep learning algorithms. However, there remains no consensus among researchers on standardized methods for evaluating these techniques. Additionally, rather than converging on a common set of datasets and objective metrics suited for specific methods, recent works exhibit considerable variation in experimental setups. This inconsistency complicates the research landscape, making it difficult to establish a streamlined evaluation process and rendering many cross‐paper comparisons challenging. Moreover, the common practice of A/B testing in perceptual studies focus only on two common metrics and not sufficient for non‐deterministic and emotion‐enabled approaches. The lack of correlations between subjective and objective metrics points out that there is a need for critical analysis in this space. In this study, we address these issues by benchmarking state‐of‐the‐art deterministic and non‐deterministic models, utilizing a consistent experimental setup across a carefully curated set of objective metrics and datasets. We also conduct a perceptual user study to assess whether subjective perceptual metrics align with the objective metrics. Our findings indicate that model rankings do not necessarily generalize across datasets, and subjective metric ratings are not always consistent with their corresponding objective metrics. The supplementary video, edited code scripts for training on different datasets and documentation related to this benchmark study are made publicly available‐ https://galib360.github.io/face-benchmark-project/ .",
      "doi": "https://doi.org/10.1111/cgf.70073",
      "openalex_id": "https://openalex.org/W4409565106",
      "arxiv_id": "",
      "publication_date": "2025-04-18",
      "published": "2025-04-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison",
      "summary": "Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/WLASL/.",
      "abstract": "Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/WLASL/.",
      "doi": "https://doi.org/10.1109/wacv45572.2020.9093512",
      "openalex_id": "https://openalex.org/W3009828227",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish",
      "summary": "Considerable evidence now shows that all languages, signed and spoken, exhibit a significant amount of iconicity. We examined how the visual-gestural modality of signed languages facilitates iconicity for different kinds of lexical meanings compared to the auditory-vocal modality of spoken languages. We used iconicity ratings of hundreds of signs and words to compare iconicity across the vocabularies of two signed languages - American Sign Language and British Sign Language, and two spoken languages - English and Spanish. We examined (1) the correlation in iconicity ratings between the languages; (2) the relationship between iconicity and an array of semantic variables (ratings of concreteness, sensory experience, imageability, perceptual strength of vision, audition, touch, smell and taste); (3) how iconicity varies between broad lexical classes (nouns, verbs, adjectives, grammatical words and adverbs); and (4) between more specific semantic categories (e.g., manual actions, clothes, colors). The results show several notable patterns that characterize how iconicity is spread across the four vocabularies. There were significant correlations in the iconicity ratings between the four languages, including English with ASL, BSL, and Spanish. The highest correlation was between ASL and BSL, suggesting iconicity may be more transparent in signs than words. In each language, iconicity was distributed according to the semantic variables in ways that reflect the semiotic affordances of the modality (e.g., more concrete meanings more iconic in signs, not words; more auditory meanings more iconic in words, not signs; more tactile meanings more iconic in both signs and words). Analysis of the 220 meanings with ratings in all four languages further showed characteristic patterns of iconicity across broad and specific semantic domains, including those that distinguished between signed and spoken languages (e.g., verbs more iconic in ASL, BSL, and English, but not Spanish; manual actions especially iconic in ASL and BSL; adjectives more iconic in English and Spanish; color words especially low in iconicity in ASL and BSL). These findings provide the first quantitative account of how iconicity is spread across the lexicons of signed languages in comparison to spoken languages.",
      "abstract": "Considerable evidence now shows that all languages, signed and spoken, exhibit a significant amount of iconicity. We examined how the visual-gestural modality of signed languages facilitates iconicity for different kinds of lexical meanings compared to the auditory-vocal modality of spoken languages. We used iconicity ratings of hundreds of signs and words to compare iconicity across the vocabularies of two signed languages - American Sign Language and British Sign Language, and two spoken languages - English and Spanish. We examined (1) the correlation in iconicity ratings between the languages; (2) the relationship between iconicity and an array of semantic variables (ratings of concreteness, sensory experience, imageability, perceptual strength of vision, audition, touch, smell and taste); (3) how iconicity varies between broad lexical classes (nouns, verbs, adjectives, grammatical words and adverbs); and (4) between more specific semantic categories (e.g., manual actions, clothes, colors). The results show several notable patterns that characterize how iconicity is spread across the four vocabularies. There were significant correlations in the iconicity ratings between the four languages, including English with ASL, BSL, and Spanish. The highest correlation was between ASL and BSL, suggesting iconicity may be more transparent in signs than words. In each language, iconicity was distributed according to the semantic variables in ways that reflect the semiotic affordances of the modality (e.g., more concrete meanings more iconic in signs, not words; more auditory meanings more iconic in words, not signs; more tactile meanings more iconic in both signs and words). Analysis of the 220 meanings with ratings in all four languages further showed characteristic patterns of iconicity across broad and specific semantic domains, including those that distinguished between signed and spoken languages (e.g., verbs more iconic in ASL, BSL, and English, but not Spanish; manual actions especially iconic in ASL and BSL; adjectives more iconic in English and Spanish; color words especially low in iconicity in ASL and BSL). These findings provide the first quantitative account of how iconicity is spread across the lexicons of signed languages in comparison to spoken languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2018.01433",
      "openalex_id": "https://openalex.org/W2885539092",
      "arxiv_id": "",
      "publication_date": "2018-08-14",
      "published": "2018-08-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Construals of iconicity: experimental approaches to form–meaning resemblances in language",
      "summary": "abstract While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.",
      "abstract": "abstract While speculations on form–meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well equipped to contribute to a comprehensive science of language.",
      "doi": "https://doi.org/10.1017/langcog.2019.48",
      "openalex_id": "https://openalex.org/W2988385843",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Road to Language Learning Is Not Entirely Iconic: Iconicity, Neighborhood Density, and Frequency Facilitate Acquisition of Sign Language",
      "summary": "Iconic mappings between words and their meanings are far more prevalent than once estimated and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows two other factors known to support the acquisition of spoken vocabulary: neighborhood density (the number of lexical items phonologically similar to the target) and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s productive acquisition of 332 signs in American Sign Language (ASL; Anderson &amp; Reilly, 2002) and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure between signed and spoken language, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage this information to expand their vocabulary.",
      "abstract": "Iconic mappings between words and their meanings are far more prevalent than once estimated and seem to support children’s acquisition of new words, spoken or signed. We asked whether iconicity’s prevalence in sign language overshadows two other factors known to support the acquisition of spoken vocabulary: neighborhood density (the number of lexical items phonologically similar to the target) and lexical frequency. Using mixed-effects logistic regressions, we reanalyzed 58 parental reports of native-signing deaf children’s productive acquisition of 332 signs in American Sign Language (ASL; Anderson &amp; Reilly, 2002) and found that iconicity, neighborhood density, and lexical frequency independently facilitated vocabulary acquisition. Despite differences in iconicity and phonological structure between signed and spoken language, signing children, like children learning a spoken language, track statistical information about lexical items and their phonological properties and leverage this information to expand their vocabulary.",
      "doi": "https://doi.org/10.1177/0956797617700498",
      "openalex_id": "https://openalex.org/W2619923810",
      "arxiv_id": "",
      "publication_date": "2017-05-30",
      "published": "2017-05-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The ASL-LEX 2.0 Project: A Database of Lexical and Phonological Properties for 2,723 Signs in American Sign Language",
      "summary": "Abstract ASL-LEX is a publicly available, large-scale lexical database for American Sign Language (ASL). We report on the expanded database (ASL-LEX 2.0) that contains 2,723 ASL signs. For each sign, ASL-LEX now includes a more detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (“guessability”) ratings (from non-signers), sign and videoclip durations, lexical class, and more. We document the steps used to create ASL-LEX 2.0 and describe the distributional characteristics for sign properties across the lexicon and examine the relationships among lexical and phonological properties of signs. Correlation analyses revealed that frequent signs were less iconic and phonologically simpler than infrequent signs and iconic signs tended to be phonologically simpler than less iconic signs. The complete ASL-LEX dataset and supplementary materials are available at https://osf.io/zpha4/ and an interactive visualization of the entire lexicon can be accessed on the ASL-LEX page: http://asl-lex.org/.",
      "abstract": "Abstract ASL-LEX is a publicly available, large-scale lexical database for American Sign Language (ASL). We report on the expanded database (ASL-LEX 2.0) that contains 2,723 ASL signs. For each sign, ASL-LEX now includes a more detailed phonological description, phonological density and complexity measures, frequency ratings (from deaf signers), iconicity ratings (from hearing non-signers and deaf signers), transparency (“guessability”) ratings (from non-signers), sign and videoclip durations, lexical class, and more. We document the steps used to create ASL-LEX 2.0 and describe the distributional characteristics for sign properties across the lexicon and examine the relationships among lexical and phonological properties of signs. Correlation analyses revealed that frequent signs were less iconic and phonologically simpler than infrequent signs and iconic signs tended to be phonologically simpler than less iconic signs. The complete ASL-LEX dataset and supplementary materials are available at https://osf.io/zpha4/ and an interactive visualization of the entire lexicon can be accessed on the ASL-LEX page: http://asl-lex.org/.",
      "doi": "https://doi.org/10.1093/deafed/enaa038",
      "openalex_id": "https://openalex.org/W3132179798",
      "arxiv_id": "",
      "publication_date": "2021-02-10",
      "published": "2021-02-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Assessment for Reading Instruction",
      "summary": "Abstract Chapter 2 provides an extensive overview of language and literacy assessments and how to use assessment results to guide literacy instruction for deaf/hard-of-hearing (DHH) students. A myriad of assessment purposes is covered, including documentation of eligibility, vocabulary, language(s), emergent reading skills, print awareness, phonological skills, fluency, comprehension, writing, spelling, and motivation. Administration and application of assessment results is addressed through seven case vignettes with DHH learners. Assessment accommodations and modifications, considerations when creating assessments, and the roles of educators in the assessment process are discussed. A summary of recommended assessment practices concludes this chapter.",
      "abstract": "Abstract Chapter 2 provides an extensive overview of language and literacy assessments and how to use assessment results to guide literacy instruction for deaf/hard-of-hearing (DHH) students. A myriad of assessment purposes is covered, including documentation of eligibility, vocabulary, language(s), emergent reading skills, print awareness, phonological skills, fluency, comprehension, writing, spelling, and motivation. Administration and application of assessment results is addressed through seven case vignettes with DHH learners. Assessment accommodations and modifications, considerations when creating assessments, and the roles of educators in the assessment process are discussed. A summary of recommended assessment practices concludes this chapter.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0002",
      "openalex_id": "https://openalex.org/W4401771242",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Oxford Handbook of the Mental Lexicon",
      "summary": "Abstract The present handbook is a state-of-the-art compilation of papers from leading scholars on the mental lexicon—the representation of language in the mind/brain at the level of individual words and meaningful sub-word units. In recent years, the study of words as mental objects has grown rapidly across several fields including linguistics, psychology, philosophy, neuroscience, education, and computational cognitive science. This comprehensive collection spans multiple disciplines, topics, theories, and methods, to highlight important advances in the study of the mental lexicon, identify areas of debate, and inspire innovation in the field from present and future generations of scholars. The book is divided into three parts. Part I presents modern linguistic and cognitive theories of how the mind/brain represents words at the phonological, morphological, syntactic, semantic, and pragmatic levels. This part also discusses broad architectural issues pertaining to the organization of the lexicon, the relation between words and concepts, and the role of compositionality. Part II discusses how children learn the form and meaning of words in their native language drawing from the key domains of phonology, morphology, syntax, semantics, and pragmatics. Multiple approaches to lexical learning are introduced to explain how learner- and environment-driven factors contribute to both the stability and the variability of lexical learning across both individual learners and communities. Part III examines how the mental lexicon contributes to language use during listening, speaking, and conversation, and includes perspectives from bilingualism, sign languages, and disorders of lexical access and production.",
      "abstract": "Abstract The present handbook is a state-of-the-art compilation of papers from leading scholars on the mental lexicon—the representation of language in the mind/brain at the level of individual words and meaningful sub-word units. In recent years, the study of words as mental objects has grown rapidly across several fields including linguistics, psychology, philosophy, neuroscience, education, and computational cognitive science. This comprehensive collection spans multiple disciplines, topics, theories, and methods, to highlight important advances in the study of the mental lexicon, identify areas of debate, and inspire innovation in the field from present and future generations of scholars. The book is divided into three parts. Part I presents modern linguistic and cognitive theories of how the mind/brain represents words at the phonological, morphological, syntactic, semantic, and pragmatic levels. This part also discusses broad architectural issues pertaining to the organization of the lexicon, the relation between words and concepts, and the role of compositionality. Part II discusses how children learn the form and meaning of words in their native language drawing from the key domains of phonology, morphology, syntax, semantics, and pragmatics. Multiple approaches to lexical learning are introduced to explain how learner- and environment-driven factors contribute to both the stability and the variability of lexical learning across both individual learners and communities. Part III examines how the mental lexicon contributes to language use during listening, speaking, and conversation, and includes perspectives from bilingualism, sign languages, and disorders of lexical access and production.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.001.0001",
      "openalex_id": "https://openalex.org/W3215506352",
      "arxiv_id": "",
      "publication_date": "2022-01-07",
      "published": "2022-01-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Real‐time lexical comprehension in young children learning American Sign Language",
      "summary": "Abstract When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world. This language–vision interaction can provide insights into children's developing efficiency in language comprehension. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? Here, we measured eye movements during real‐time comprehension of a visual‐manual language, American Sign Language ( ASL ), by 29 native ASL ‐learning children (16–53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset. Deaf and hearing ASL ‐learners showed similar gaze patterns, suggesting that the in‐the‐moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day‐to‐day life. Finally, variation in children's ASL processing was positively correlated with age and vocabulary size. Thus, despite competition for attention within a single modality, the timing and accuracy of visual fixations during ASL comprehension reflect information processing skills that are important for language acquisition regardless of language modality.",
      "abstract": "Abstract When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world. This language–vision interaction can provide insights into children's developing efficiency in language comprehension. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? Here, we measured eye movements during real‐time comprehension of a visual‐manual language, American Sign Language ( ASL ), by 29 native ASL ‐learning children (16–53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset. Deaf and hearing ASL ‐learners showed similar gaze patterns, suggesting that the in‐the‐moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day‐to‐day life. Finally, variation in children's ASL processing was positively correlated with age and vocabulary size. Thus, despite competition for attention within a single modality, the timing and accuracy of visual fixations during ASL comprehension reflect information processing skills that are important for language acquisition regardless of language modality.",
      "doi": "https://doi.org/10.1111/desc.12672",
      "openalex_id": "https://openalex.org/W2797842776",
      "arxiv_id": "",
      "publication_date": "2018-04-16",
      "published": "2018-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Gesture recognition algorithm based on multi‐scale feature fusion in RGB‐D images",
      "summary": "Abstract With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human‐computer interaction more natural and flexible, bringing richer interactive experience to teaching, on‐board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi‐level feature fusion of two‐stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB‐D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi‐level feature fusion of two‐stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single‐channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1‐miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.",
      "abstract": "Abstract With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human‐computer interaction more natural and flexible, bringing richer interactive experience to teaching, on‐board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi‐level feature fusion of two‐stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB‐D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi‐level feature fusion of two‐stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single‐channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1‐miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.",
      "doi": "https://doi.org/10.1049/ipr2.12712",
      "openalex_id": "https://openalex.org/W4312105654",
      "arxiv_id": "",
      "publication_date": "2022-12-23",
      "published": "2022-12-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-modal translation priming and iconicity effects in deaf signers and hearing learners of American Sign Language",
      "summary": "Abstract This study used ERPs to a) assess the neural correlates of cross-linguistic, cross-modal translation priming in hearing beginning learners of American Sign Language (ASL) and deaf highly proficient signers and b) examine whether sign iconicity modulates these priming effects. Hearing learners exhibited translation priming for ASL signs preceded by English words (greater negativity for unrelated than translation primes) later in the ERP waveform than deaf signers and exhibited earlier and greater priming for iconic than non-iconic signs. Iconicity did not modulate translation priming effects either behaviorally or in the ERPs for deaf signers (except in a 800–1000 ms time window). Because deaf signers showed early translation priming effects (beginning at 400ms-600ms), we suggest that iconicity did not facilitate lexical access, but deaf signers may have recognized sign iconicity later in processing. Overall, the results indicate that iconicity speeds lexical access for L2 sign language learners, but not for proficient signers.",
      "abstract": "Abstract This study used ERPs to a) assess the neural correlates of cross-linguistic, cross-modal translation priming in hearing beginning learners of American Sign Language (ASL) and deaf highly proficient signers and b) examine whether sign iconicity modulates these priming effects. Hearing learners exhibited translation priming for ASL signs preceded by English words (greater negativity for unrelated than translation primes) later in the ERP waveform than deaf signers and exhibited earlier and greater priming for iconic than non-iconic signs. Iconicity did not modulate translation priming effects either behaviorally or in the ERPs for deaf signers (except in a 800–1000 ms time window). Because deaf signers showed early translation priming effects (beginning at 400ms-600ms), we suggest that iconicity did not facilitate lexical access, but deaf signers may have recognized sign iconicity later in processing. Overall, the results indicate that iconicity speeds lexical access for L2 sign language learners, but not for proficient signers.",
      "doi": "https://doi.org/10.1017/s1366728919000889",
      "openalex_id": "https://openalex.org/W3004227195",
      "arxiv_id": "",
      "publication_date": "2020-01-31",
      "published": "2020-01-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effects of Video Reversal on Gaze Patterns during Signed Narrative Comprehension",
      "summary": "Abstract Language knowledge, age of acquisition (AoA), and stimulus intelligibility all affect gaze behavior for reading print, but it is unknown how these factors affect “sign-watching” among signers. This study investigated how these factors affect gaze behavior during sign language comprehension in 52 adult signers who acquired American Sign Language (ASL) at different ages. We examined gaze patterns and story comprehension in four subject groups who differ in hearing status and when they learned ASL (i.e. Deaf Early, Deaf Late, Hearing Late, and Hearing Novice). Participants watched signed stories in normal (high intelligibility) and video-reversed (low intelligibility) conditions. This video manipulation was used because it distorts word order and thus disrupts the syntax and semantic content of narratives, while preserving most surface phonological features of individual signs. Video reversal decreased story comprehension accuracy, and this effect was greater for those who learned ASL later in life. Reversal also was associated with more dispersed gaze behavior. Although each subject group had unique gaze patterns, the effect of video reversal on gaze measures was similar across all groups. Among fluent signers, gaze behavior was not correlated with AoA, suggesting that “efficient” sign watching can be quickly learnt even among signers exposed to signed language later in life.",
      "abstract": "Abstract Language knowledge, age of acquisition (AoA), and stimulus intelligibility all affect gaze behavior for reading print, but it is unknown how these factors affect “sign-watching” among signers. This study investigated how these factors affect gaze behavior during sign language comprehension in 52 adult signers who acquired American Sign Language (ASL) at different ages. We examined gaze patterns and story comprehension in four subject groups who differ in hearing status and when they learned ASL (i.e. Deaf Early, Deaf Late, Hearing Late, and Hearing Novice). Participants watched signed stories in normal (high intelligibility) and video-reversed (low intelligibility) conditions. This video manipulation was used because it distorts word order and thus disrupts the syntax and semantic content of narratives, while preserving most surface phonological features of individual signs. Video reversal decreased story comprehension accuracy, and this effect was greater for those who learned ASL later in life. Reversal also was associated with more dispersed gaze behavior. Although each subject group had unique gaze patterns, the effect of video reversal on gaze measures was similar across all groups. Among fluent signers, gaze behavior was not correlated with AoA, suggesting that “efficient” sign watching can be quickly learnt even among signers exposed to signed language later in life.",
      "doi": "https://doi.org/10.1093/deafed/enaa007",
      "openalex_id": "https://openalex.org/W3028570272",
      "arxiv_id": "",
      "publication_date": "2020-02-24",
      "published": "2020-02-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASL Trigger Recognition in Mixed Activity/Signing Sequences for RF Sensor-Based User Interfaces",
      "summary": "The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.",
      "abstract": "The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This article considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RF-sensor-based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and three gross motor activities.",
      "doi": "https://doi.org/10.1109/thms.2021.3131675",
      "openalex_id": "https://openalex.org/W3213624573",
      "arxiv_id": "",
      "publication_date": "2021-12-22",
      "published": "2021-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Syntax through the looking glass: A review on two-word linguistic processing across behavioral, neuroimaging and neurostimulation studies",
      "summary": "In recent years a growing number of studies on syntactic processing has employed basic two-word constructions (e.g., \"the tree\") to characterize the fundamental aspects of linguistic composition. This large body of evidence allows, for the first time, to closely examine which cognitive processes and neural substrates support the combination of two syntactic units into a more complex one, mirroring the nature of combinatory operations described in theoretical linguistics. The present review comprehensively examines behavioral, neuroimaging and neurostimulation studies investigating basic syntactic composition, covering more than forty years of psycho- and neuro-linguistic research. Across several paradigms, four key features of syntactic composition have emerged: (1) the rule-based and (2) automatic nature of the combinatorial process, (3) a central role of Broca's area and the posterior temporal lobe in representing and combining syntactic features, and (4) the reliance on efficient bottom-up integration rather than top-down prediction.",
      "abstract": "In recent years a growing number of studies on syntactic processing has employed basic two-word constructions (e.g., \"the tree\") to characterize the fundamental aspects of linguistic composition. This large body of evidence allows, for the first time, to closely examine which cognitive processes and neural substrates support the combination of two syntactic units into a more complex one, mirroring the nature of combinatory operations described in theoretical linguistics. The present review comprehensively examines behavioral, neuroimaging and neurostimulation studies investigating basic syntactic composition, covering more than forty years of psycho- and neuro-linguistic research. Across several paradigms, four key features of syntactic composition have emerged: (1) the rule-based and (2) automatic nature of the combinatorial process, (3) a central role of Broca's area and the posterior temporal lobe in representing and combining syntactic features, and (4) the reliance on efficient bottom-up integration rather than top-down prediction.",
      "doi": "https://doi.org/10.1016/j.neubiorev.2022.104881",
      "openalex_id": "https://openalex.org/W4296380331",
      "arxiv_id": "",
      "publication_date": "2022-09-19",
      "published": "2022-09-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign Language Recognition: A Comprehensive Review of Traditional and Deep Learning Approaches, Datasets, and Challenges",
      "summary": "The Deaf are a large social group in society. Their unique way of communicating through sign language is often confined within their community due to limited understanding by individuals outside of this demographic. This is where sign language recognition (SLR) comes in to help people without hearing impairments understand the meaning of sign language. In recent years, new methods of sign language recognition have been developed and achieved good results, so it is necessary to make a summary. This review mainly focuses on the introduction of sign language recognition techniques based on algorithms especially in recent years, including the recognition models based on traditional methods and deep learning approaches, sign language datasets, challenges and future directions in SLR. To make the method structure clearer, this article explains and compares the basic principles of different methods from the perspectives of feature extraction and temporal modelling. We hope that this review will provide some reference and help for future research in sign language recognition.",
      "abstract": "The Deaf are a large social group in society. Their unique way of communicating through sign language is often confined within their community due to limited understanding by individuals outside of this demographic. This is where sign language recognition (SLR) comes in to help people without hearing impairments understand the meaning of sign language. In recent years, new methods of sign language recognition have been developed and achieved good results, so it is necessary to make a summary. This review mainly focuses on the introduction of sign language recognition techniques based on algorithms especially in recent years, including the recognition models based on traditional methods and deep learning approaches, sign language datasets, challenges and future directions in SLR. To make the method structure clearer, this article explains and compares the basic principles of different methods from the perspectives of feature extraction and temporal modelling. We hope that this review will provide some reference and help for future research in sign language recognition.",
      "doi": "https://doi.org/10.1109/access.2024.3398806",
      "openalex_id": "https://openalex.org/W4396753522",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A survey on sign language literature",
      "summary": "Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.",
      "abstract": "Individuals with hearing impairment encounter various types and levels of difficulties, highlighting the need for more research to provide effective support. One significant difficulty is communication and interaction with others. Given that these individuals employ sign language as their primary mode of communication, there exists a notable information void among those who can hear in comprehending and interpreting sign language. Consequently, to bridge this gap, the field of sign language research has seen significant growth. In this study, we emphasize the importance of sign language recognition and translation and provide a comprehensive review of relevant research conducted in this field. Our examination encompasses multiple perspectives, including sign language recognition, translation, and the availability of datasets. By exploring these aspects, we aim to contribute to the advancement of sign language literature and its practical applications.",
      "doi": "https://doi.org/10.1016/j.mlwa.2023.100504",
      "openalex_id": "https://openalex.org/W4387924637",
      "arxiv_id": "",
      "publication_date": "2023-10-24",
      "published": "2023-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign2Pose: A Pose-Based Approach for Gloss Prediction Using a Transformer Model",
      "summary": "Word-level sign language recognition (WSLR) is the backbone for continuous sign language recognition (CSLR) that infers glosses from sign videos. Finding the relevant gloss from the sign sequence and detecting explicit boundaries of the glosses from sign videos is a persistent challenge. In this paper, we propose a systematic approach for gloss prediction in WLSR using the Sign2Pose Gloss prediction transformer model. The primary goal of this work is to enhance WLSR’s gloss prediction accuracy with reduced time and computational overhead. The proposed approach uses hand-crafted features rather than automated feature extraction, which is computationally expensive and less accurate. A modified key frame extraction technique is proposed that uses histogram difference and Euclidean distance metrics to select and drop redundant frames. To enhance the model’s generalization ability, pose vector augmentation using perspective transformation along with joint angle rotation is performed. Further, for normalization, we employed YOLOv3 (You Only Look Once) to detect the signing space and track the hand gestures of the signers in the frames. The proposed model experiments on WLASL datasets achieved the top 1% recognition accuracy of 80.9% in WLASL100 and 64.21% in WLASL300. The performance of the proposed model surpasses state-of-the-art approaches. The integration of key frame extraction, augmentation, and pose estimation improved the performance of the proposed gloss prediction model by increasing the model’s precision in locating minor variations in their body posture. We observed that introducing YOLOv3 improved gloss prediction accuracy and helped prevent model overfitting. Overall, the proposed model showed 17% improved performance in the WLASL 100 dataset.",
      "abstract": "Word-level sign language recognition (WSLR) is the backbone for continuous sign language recognition (CSLR) that infers glosses from sign videos. Finding the relevant gloss from the sign sequence and detecting explicit boundaries of the glosses from sign videos is a persistent challenge. In this paper, we propose a systematic approach for gloss prediction in WLSR using the Sign2Pose Gloss prediction transformer model. The primary goal of this work is to enhance WLSR’s gloss prediction accuracy with reduced time and computational overhead. The proposed approach uses hand-crafted features rather than automated feature extraction, which is computationally expensive and less accurate. A modified key frame extraction technique is proposed that uses histogram difference and Euclidean distance metrics to select and drop redundant frames. To enhance the model’s generalization ability, pose vector augmentation using perspective transformation along with joint angle rotation is performed. Further, for normalization, we employed YOLOv3 (You Only Look Once) to detect the signing space and track the hand gestures of the signers in the frames. The proposed model experiments on WLASL datasets achieved the top 1% recognition accuracy of 80.9% in WLASL100 and 64.21% in WLASL300. The performance of the proposed model surpasses state-of-the-art approaches. The integration of key frame extraction, augmentation, and pose estimation improved the performance of the proposed gloss prediction model by increasing the model’s precision in locating minor variations in their body posture. We observed that introducing YOLOv3 improved gloss prediction accuracy and helped prevent model overfitting. Overall, the proposed model showed 17% improved performance in the WLASL 100 dataset.",
      "doi": "https://doi.org/10.3390/s23052853",
      "openalex_id": "https://openalex.org/W4323314027",
      "arxiv_id": "",
      "publication_date": "2023-03-06",
      "published": "2023-03-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Activation of ASL signs during sentence reading for deaf readers: evidence from eye-tracking",
      "summary": "Abstract Bilinguals activate both of their languages as they process written words, regardless of modality (spoken or signed); these effects have primarily been documented in single word reading paradigms. We used eye-tracking to determine whether deaf bilingual readers ( n = 23) activate American Sign Language (ASL) translations as they read English sentences. Sentences contained a target word and one of the two possible prime words: a related prime which shared phonological parameters (location, handshape or movement) with the target when translated into ASL or an unrelated prime. The results revealed that first fixation durations and gaze durations (early processing measures) were shorter when target words were preceded by ASL-related primes, but prime condition did not impact later processing measures (e.g., regressions). Further, less-skilled readers showed a larger ASL co-activation effect. Together, the results indicate that ASL co-activation impacts early lexical access and can facilitate reading, particularly for less-skilled deaf readers.",
      "abstract": "Abstract Bilinguals activate both of their languages as they process written words, regardless of modality (spoken or signed); these effects have primarily been documented in single word reading paradigms. We used eye-tracking to determine whether deaf bilingual readers ( n = 23) activate American Sign Language (ASL) translations as they read English sentences. Sentences contained a target word and one of the two possible prime words: a related prime which shared phonological parameters (location, handshape or movement) with the target when translated into ASL or an unrelated prime. The results revealed that first fixation durations and gaze durations (early processing measures) were shorter when target words were preceded by ASL-related primes, but prime condition did not impact later processing measures (e.g., regressions). Further, less-skilled readers showed a larger ASL co-activation effect. Together, the results indicate that ASL co-activation impacts early lexical access and can facilitate reading, particularly for less-skilled deaf readers.",
      "doi": "https://doi.org/10.1017/s1366728924000336",
      "openalex_id": "https://openalex.org/W4395673765",
      "arxiv_id": "",
      "publication_date": "2024-04-26",
      "published": "2024-04-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Turning languages on and off: Switching into and out of code-blends reveals the nature of bilingual language control.",
      "summary": "When spoken language (unimodal) bilinguals switch between languages, they must simultaneously inhibit 1 language and activate the other language. Because American Sign Language (ASL)-English (bimodal) bilinguals can switch into and out of code-blends (simultaneous production of a sign and a word), we can tease apart the cost of inhibition (turning a language off) and activation (turning a language on). Results from a cued picture-naming task with 43 bimodal bilinguals revealed a significant cost to turn off a language (switching out of a code-blend), but no cost to turn on a language (switching into a code-blend). Switching from single to dual lexical retrieval (adding a language) was also not costly. These patterns held for both languages regardless of default language, that is, whether switching between speaking and code-blending (English default) or between signing and code-blending (ASL default). Overall, the results support models of bilingual language control that assume a primary role for inhibitory control and indicate that disengaging from producing a language is more difficult than engaging a new language. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",
      "abstract": "When spoken language (unimodal) bilinguals switch between languages, they must simultaneously inhibit 1 language and activate the other language. Because American Sign Language (ASL)-English (bimodal) bilinguals can switch into and out of code-blends (simultaneous production of a sign and a word), we can tease apart the cost of inhibition (turning a language off) and activation (turning a language on). Results from a cued picture-naming task with 43 bimodal bilinguals revealed a significant cost to turn off a language (switching out of a code-blend), but no cost to turn on a language (switching into a code-blend). Switching from single to dual lexical retrieval (adding a language) was also not costly. These patterns held for both languages regardless of default language, that is, whether switching between speaking and code-blending (English default) or between signing and code-blending (ASL default). Overall, the results support models of bilingual language control that assume a primary role for inhibitory control and indicate that disengaging from producing a language is more difficult than engaging a new language. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",
      "doi": "https://doi.org/10.1037/xlm0000734",
      "openalex_id": "https://openalex.org/W2955094614",
      "arxiv_id": "",
      "publication_date": "2019-06-27",
      "published": "2019-06-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Picture-naming in American Sign Language: an electrophysiological study of the effects of iconicity and structured alignment",
      "summary": "A picture-naming task and ERPs were used to investigate effects of iconicity and visual alignment between signs and pictures in American Sign Language (ASL). For iconic signs, half the pictures visually overlapped with phonological features of the sign (e.g., the fingers of CAT align with a picture of a cat with prominent whiskers), while half did not (whiskers are not shown). Iconic signs were produced numerically faster than non-iconic signs and were associated with larger N400 amplitudes, akin to concreteness effects. Pictures aligned with iconic signs were named faster than non-aligned pictures, and there was a reduction in N400 amplitude. No behavioral effects were observed for the control group (English speakers). We conclude that sensory-motoric semantic features are represented more robustly for iconic than non-iconic signs (eliciting a concreteness-like N400 effect) and visual overlap between pictures and the phonological form of iconic signs facilitates lexical retrieval (eliciting a reduced N400).",
      "abstract": "A picture-naming task and ERPs were used to investigate effects of iconicity and visual alignment between signs and pictures in American Sign Language (ASL). For iconic signs, half the pictures visually overlapped with phonological features of the sign (e.g., the fingers of CAT align with a picture of a cat with prominent whiskers), while half did not (whiskers are not shown). Iconic signs were produced numerically faster than non-iconic signs and were associated with larger N400 amplitudes, akin to concreteness effects. Pictures aligned with iconic signs were named faster than non-aligned pictures, and there was a reduction in N400 amplitude. No behavioral effects were observed for the control group (English speakers). We conclude that sensory-motoric semantic features are represented more robustly for iconic than non-iconic signs (eliciting a concreteness-like N400 effect) and visual overlap between pictures and the phonological form of iconic signs facilitates lexical retrieval (eliciting a reduced N400).",
      "doi": "https://doi.org/10.1080/23273798.2020.1804601",
      "openalex_id": "https://openalex.org/W3063240284",
      "arxiv_id": "",
      "publication_date": "2020-08-19",
      "published": "2020-08-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Give Me a Sign: Using Data Gloves for Static Hand-Shape Recognition",
      "summary": "Human-to-human communication via the computer is mainly carried out using a keyboard or microphone. In the field of virtual reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g., silent commands during task-force training) or simply not possible (e.g., if the user has hearing loss). Data gloves help to increase immersion within VR, as they correspond to our natural interaction. At the same time, they offer the possibility of accurately capturing hand shapes, such as those used in non-verbal communication (e.g., thumbs up, okay gesture, …) and in sign language. In this paper, we present a hand-shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an outlier detection and a feature selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial data augmentation, i.e., we created new artificial data from the recorded and filtered data to augment the training data set. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. The voting meta-classifier (VL2) proved to be the most accurate, albeit slowest, classifier. A good alternative is random forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. outlier detection was proven to be an effective approach, especially in improving the classification time. Overall, we have shown that our hand-shape recognition system using data gloves is suitable for communication within VR.",
      "abstract": "Human-to-human communication via the computer is mainly carried out using a keyboard or microphone. In the field of virtual reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g., silent commands during task-force training) or simply not possible (e.g., if the user has hearing loss). Data gloves help to increase immersion within VR, as they correspond to our natural interaction. At the same time, they offer the possibility of accurately capturing hand shapes, such as those used in non-verbal communication (e.g., thumbs up, okay gesture, …) and in sign language. In this paper, we present a hand-shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an outlier detection and a feature selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial data augmentation, i.e., we created new artificial data from the recorded and filtered data to augment the training data set. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. The voting meta-classifier (VL2) proved to be the most accurate, albeit slowest, classifier. A good alternative is random forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. outlier detection was proven to be an effective approach, especially in improving the classification time. Overall, we have shown that our hand-shape recognition system using data gloves is suitable for communication within VR.",
      "doi": "https://doi.org/10.3390/s23249847",
      "openalex_id": "https://openalex.org/W4389922996",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Morphology and The Mental Lexicon",
      "summary": "Abstract This chapter looks at the most basic question for the study of morphology and the mental lexicon: whether or not words are decomposed into constituent parts. Theories of the mental lexicon range from those that generalize decomposition as much as possible to those that posit no internal structure for words; various intermediate positions are also found. The key question at the center of this overview is what it means for a word to be decomposed in the first place; as it turns out, this is a complex notion, and there are at least three independent but related questions that have been connected with it in the literature. The goal of this chapter is to distinguish these different notions of decomposition from each other, in order to both provide points of reference for understanding prior work, and to pose specific questions to be addressed in future research.",
      "abstract": "Abstract This chapter looks at the most basic question for the study of morphology and the mental lexicon: whether or not words are decomposed into constituent parts. Theories of the mental lexicon range from those that generalize decomposition as much as possible to those that posit no internal structure for words; various intermediate positions are also found. The key question at the center of this overview is what it means for a word to be decomposed in the first place; as it turns out, this is a complex notion, and there are at least three independent but related questions that have been connected with it in the literature. The goal of this chapter is to distinguish these different notions of decomposition from each other, in order to both provide points of reference for understanding prior work, and to pose specific questions to be addressed in future research.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.17",
      "openalex_id": "https://openalex.org/W4212952130",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recent Issues in the Use of Signed Language Assessments for Diagnosis of Language Disorders in Signing Deaf and Hard of Hearing Children",
      "summary": "In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are limitations in their application, particularly for the diagnosis of language disorders, and learning disabilities. Here, we discuss some of the available normed, signed language assessments and some of their limitations. We have also provided information related to practices that should lead to improvement in the quality of signed language assessments.",
      "abstract": "In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are limitations in their application, particularly for the diagnosis of language disorders, and learning disabilities. Here, we discuss some of the available normed, signed language assessments and some of their limitations. We have also provided information related to practices that should lead to improvement in the quality of signed language assessments.",
      "doi": "https://doi.org/10.1093/deafed/eny014",
      "openalex_id": "https://openalex.org/W2803946838",
      "arxiv_id": "",
      "publication_date": "2018-04-11",
      "published": "2018-04-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The perceived mapping between form and meaning in American Sign Language depends on linguistic knowledge and task: evidence from iconicity and transparency judgments",
      "summary": "abstract Iconicity is often defined as the resemblance between a form and a given meaning, while transparency is defined as the ability to infer a given meaning based on the form. This study examined the influence of knowledge of American Sign Language (ASL) on the perceived iconicity of signs and the relationship between iconicity, transparency (correctly guessed signs), ‘perceived transparency’ (transparency ratings of the guesses), and ‘semantic potential’ (the diversity (H index) of guesses). Experiment 1 compared iconicity ratings by deaf ASL signers and hearing non-signers for 991 signs from the ASL-LEX database. Signers and non-signers’ ratings were highly correlated; however, the groups provided different iconicity ratings for subclasses of signs: nouns vs. verbs, handling vs. entity, and one- vs. two-handed signs. In Experiment 2, non-signers guessed the meaning of 430 signs and rated them for how transparent their guessed meaning would be for others. Only 10% of guesses were correct. Iconicity ratings correlated with transparency (correct guesses), perceived transparency ratings, and semantic potential (H index). Further, some iconic signs were perceived as non-transparent and vice versa. The study demonstrates that linguistic knowledge mediates perceived iconicity distinctly from gesture and highlights critical distinctions between iconicity, transparency (perceived and objective), and semantic potential.",
      "abstract": "abstract Iconicity is often defined as the resemblance between a form and a given meaning, while transparency is defined as the ability to infer a given meaning based on the form. This study examined the influence of knowledge of American Sign Language (ASL) on the perceived iconicity of signs and the relationship between iconicity, transparency (correctly guessed signs), ‘perceived transparency’ (transparency ratings of the guesses), and ‘semantic potential’ (the diversity (H index) of guesses). Experiment 1 compared iconicity ratings by deaf ASL signers and hearing non-signers for 991 signs from the ASL-LEX database. Signers and non-signers’ ratings were highly correlated; however, the groups provided different iconicity ratings for subclasses of signs: nouns vs. verbs, handling vs. entity, and one- vs. two-handed signs. In Experiment 2, non-signers guessed the meaning of 430 signs and rated them for how transparent their guessed meaning would be for others. Only 10% of guesses were correct. Iconicity ratings correlated with transparency (correct guesses), perceived transparency ratings, and semantic potential (H index). Further, some iconic signs were perceived as non-transparent and vice versa. The study demonstrates that linguistic knowledge mediates perceived iconicity distinctly from gesture and highlights critical distinctions between iconicity, transparency (perceived and objective), and semantic potential.",
      "doi": "https://doi.org/10.1017/langcog.2019.18",
      "openalex_id": "https://openalex.org/W2958944944",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The iconicity toolbox: empirical approaches to measuring iconicity",
      "summary": "abstract Growing evidence from across the cognitive sciences indicates that iconicity plays an important role in a number of fundamental language processes, spanning learning, comprehension, and online use. One benefit of this recent upsurge in empirical work is the diversification of methods available for measuring iconicity. In this paper, we provide an overview of methods in the form of a ‘toolbox’. We lay out empirical methods for measuring iconicity at a behavioural level, in the perception, production, and comprehension of iconic forms. We also discuss large-scale studies that look at iconicity on a system-wide level, based on objective measures of similarity between signals and meanings. We give a detailed overview of how different measures of iconicity can better address specific hypotheses, providing greater clarity when choosing testing methods.",
      "abstract": "abstract Growing evidence from across the cognitive sciences indicates that iconicity plays an important role in a number of fundamental language processes, spanning learning, comprehension, and online use. One benefit of this recent upsurge in empirical work is the diversification of methods available for measuring iconicity. In this paper, we provide an overview of methods in the form of a ‘toolbox’. We lay out empirical methods for measuring iconicity at a behavioural level, in the perception, production, and comprehension of iconic forms. We also discuss large-scale studies that look at iconicity on a system-wide level, based on objective measures of similarity between signals and meanings. We give a detailed overview of how different measures of iconicity can better address specific hypotheses, providing greater clarity when choosing testing methods.",
      "doi": "https://doi.org/10.1017/langcog.2019.14",
      "openalex_id": "https://openalex.org/W2946721775",
      "arxiv_id": "",
      "publication_date": "2019-06-01",
      "published": "2019-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Degree and not type of iconicity affects sign language vocabulary acquisition.",
      "summary": "Lexical iconicity-signs or words that resemble their meaning-is overrepresented in children's early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child's firsthand experiences. As such, pantomimic iconic signs, which use the signer's body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the structure mapping theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in American Sign Language (ASL), we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We reanalyzed a previously published set of ASL Communicative Development Inventory (CDI) reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a structure mapping theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "abstract": "Lexical iconicity-signs or words that resemble their meaning-is overrepresented in children's early vocabularies. Embodied theories of language acquisition predict that symbols are more learnable when they are grounded in a child's firsthand experiences. As such, pantomimic iconic signs, which use the signer's body to represent a body, might be more readily learned than other types of iconic signs. Alternatively, the structure mapping theory of iconicity predicts that learners are sensitive to the amount of overlap between form and meaning. In this exploratory study of early vocabulary development in American Sign Language (ASL), we asked whether type of iconicity predicts sign acquisition above and beyond degree of iconicity. We also controlled for concreteness and relevance to babies, two possible confounding factors. Highly concrete referents and concepts that are germane to babies may be amenable to iconic mappings. We reanalyzed a previously published set of ASL Communicative Development Inventory (CDI) reports from 58 deaf children learning ASL from their deaf parents (Anderson & Reilly, 2002). Pantomimic signs were more iconic than other types of iconic signs (perceptual, both pantomimic and perceptual, or arbitrary), but type of iconicity had no effect on acquisition. Children may not make use of the special status of pantomimic elements of signs. Their vocabularies are, however, shaped by degree of iconicity, which aligns with a structure mapping theory of iconicity, though other explanations are also compatible (e.g., iconicity in child-directed signing). Previously demonstrated effects of type of iconicity may be an artifact of the increased degree of iconicity among pantomimic signs. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
      "doi": "https://doi.org/10.1037/xlm0000713",
      "openalex_id": "https://openalex.org/W2945201830",
      "arxiv_id": "",
      "publication_date": "2019-05-16",
      "published": "2019-05-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ERP Evidence for Co-Activation of English Words during Recognition of American Sign Language Signs",
      "summary": "Event-related potentials (ERPs) were used to investigate co-activation of English words during recognition of American Sign Language (ASL) signs. Deaf and hearing signers viewed pairs of ASL signs and judged their semantic relatedness. Half of the semantically unrelated signs had English translations that shared an orthographic and phonological rime (e.g., BAR–STAR) and half did not (e.g., NURSE–STAR). Classic N400 and behavioral semantic priming effects were observed in both groups. For hearing signers, targets in sign pairs with English rime translations elicited a smaller N400 compared to targets in pairs with unrelated English translations. In contrast, a reversed N400 effect was observed for deaf signers: target signs in English rime translation pairs elicited a larger N400 compared to targets in pairs with unrelated English translations. This reversed effect was overtaken by a later, more typical ERP priming effect for deaf signers who were aware of the manipulation. These findings provide evidence that implicit language co-activation in bimodal bilinguals is bidirectional. However, the distinct pattern of effects in deaf and hearing signers suggests that it may be modulated by differences in language proficiency and dominance as well as by asymmetric reliance on orthographic versus phonological representations.",
      "abstract": "Event-related potentials (ERPs) were used to investigate co-activation of English words during recognition of American Sign Language (ASL) signs. Deaf and hearing signers viewed pairs of ASL signs and judged their semantic relatedness. Half of the semantically unrelated signs had English translations that shared an orthographic and phonological rime (e.g., BAR–STAR) and half did not (e.g., NURSE–STAR). Classic N400 and behavioral semantic priming effects were observed in both groups. For hearing signers, targets in sign pairs with English rime translations elicited a smaller N400 compared to targets in pairs with unrelated English translations. In contrast, a reversed N400 effect was observed for deaf signers: target signs in English rime translation pairs elicited a larger N400 compared to targets in pairs with unrelated English translations. This reversed effect was overtaken by a later, more typical ERP priming effect for deaf signers who were aware of the manipulation. These findings provide evidence that implicit language co-activation in bimodal bilinguals is bidirectional. However, the distinct pattern of effects in deaf and hearing signers suggests that it may be modulated by differences in language proficiency and dominance as well as by asymmetric reliance on orthographic versus phonological representations.",
      "doi": "https://doi.org/10.3390/brainsci9060148",
      "openalex_id": "https://openalex.org/W2950142550",
      "arxiv_id": "",
      "publication_date": "2019-06-21",
      "published": "2019-06-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neurophysiological Correlates of Frequency, Concreteness, and Iconicity in American Sign Language",
      "summary": "To investigate possible universal and modality-specific factors that influence the neurophysiological response during lexical processing, we recorded event-related potentials while a large group of deaf adults ( n = 40) viewed 404 signs in American Sign Language (ASL) that varied in ASL frequency, concreteness, and iconicity. Participants performed a go/no-go semantic categorization task (does the sign refer to people?) to videoclips of ASL signs (clips began with the signer’s hands at rest). Linear mixed-effects regression models were fit with per-participant, per-trial, and per-electrode data, allowing us to identify unique effects of each lexical variable. We observed an early effect of frequency (greater negativity for less frequent signs) beginning at 400 ms postvideo onset at anterior sites, which we interpreted as reflecting form-based lexical processing. This effect was followed by a more widely distributed posterior response that we interpreted as reflecting lexical-semantic processing. Paralleling spoken language, more concrete signs elicited greater negativities, beginning 600 ms postvideo onset with a wide scalp distribution. Finally, there were no effects of iconicity (except for a weak effect in the latest epochs; 1,000–1,200 ms), suggesting that iconicity does not modulate the neural response during sign recognition. Despite the perceptual and sensorimotoric differences between signed and spoken languages, the overall results indicate very similar neurophysiological processes underlie lexical access for both signs and words.",
      "abstract": "To investigate possible universal and modality-specific factors that influence the neurophysiological response during lexical processing, we recorded event-related potentials while a large group of deaf adults ( n = 40) viewed 404 signs in American Sign Language (ASL) that varied in ASL frequency, concreteness, and iconicity. Participants performed a go/no-go semantic categorization task (does the sign refer to people?) to videoclips of ASL signs (clips began with the signer’s hands at rest). Linear mixed-effects regression models were fit with per-participant, per-trial, and per-electrode data, allowing us to identify unique effects of each lexical variable. We observed an early effect of frequency (greater negativity for less frequent signs) beginning at 400 ms postvideo onset at anterior sites, which we interpreted as reflecting form-based lexical processing. This effect was followed by a more widely distributed posterior response that we interpreted as reflecting lexical-semantic processing. Paralleling spoken language, more concrete signs elicited greater negativities, beginning 600 ms postvideo onset with a wide scalp distribution. Finally, there were no effects of iconicity (except for a weak effect in the latest epochs; 1,000–1,200 ms), suggesting that iconicity does not modulate the neural response during sign recognition. Despite the perceptual and sensorimotoric differences between signed and spoken languages, the overall results indicate very similar neurophysiological processes underlie lexical access for both signs and words.",
      "doi": "https://doi.org/10.1162/nol_a_00012",
      "openalex_id": "https://openalex.org/W3017459497",
      "arxiv_id": "",
      "publication_date": "2020-04-24",
      "published": "2020-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The signed mental lexicon: Effects of phonological neighborhood density, iconicity, and childhood language experience",
      "summary": "Most of what is known about the mental lexicon comes from studies of spoken language and their written forms. Signs differ from spoken/written words in two important ways that may affect lexical recognition: their phonological composition is unique (e.g., more simultaneous than serial structure; few minimal pairs) and many signs are iconic. Using an unprimed lexical decision task in American Sign Language (ASL) and the first available estimates of phonological neighborhood density for any sign language, we found that phonological neighborhood density had an inhibitory effect on latency among low frequency signs. This is the first clear evidence that phonological neighbors spontaneously compete during sign recognition. Iconicity negatively affected accuracy but not reaction times, suggesting that iconicity plays a role in task-related decision processes but not lexical retrieval. Many deaf signers have delayed first language acquisition, and we found that language deprivation had lasting, negative effects on phonological processing and sign recognition speed and accuracy. This work indicates that the lexicons of both spoken and signed languages are organized by form, that lexical recognition occurs through form-based competition (most evident for low frequency items), and that form-meaning mappings do not drive lexical access even when iconicity is pervasive in the lexicon.",
      "abstract": "Most of what is known about the mental lexicon comes from studies of spoken language and their written forms. Signs differ from spoken/written words in two important ways that may affect lexical recognition: their phonological composition is unique (e.g., more simultaneous than serial structure; few minimal pairs) and many signs are iconic. Using an unprimed lexical decision task in American Sign Language (ASL) and the first available estimates of phonological neighborhood density for any sign language, we found that phonological neighborhood density had an inhibitory effect on latency among low frequency signs. This is the first clear evidence that phonological neighbors spontaneously compete during sign recognition. Iconicity negatively affected accuracy but not reaction times, suggesting that iconicity plays a role in task-related decision processes but not lexical retrieval. Many deaf signers have delayed first language acquisition, and we found that language deprivation had lasting, negative effects on phonological processing and sign recognition speed and accuracy. This work indicates that the lexicons of both spoken and signed languages are organized by form, that lexical recognition occurs through form-based competition (most evident for low frequency items), and that form-meaning mappings do not drive lexical access even when iconicity is pervasive in the lexicon.",
      "doi": "https://doi.org/10.1016/j.jml.2021.104282",
      "openalex_id": "https://openalex.org/W3188819639",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Data Representativeness in Accessibility Datasets: A Meta-Analysis",
      "summary": "As data-driven systems are increasingly deployed at scale, ethical concerns have arisen around unfair and discriminatory outcomes for historically marginalized groups that are underrepresented in training data. In response, work around AI fairness and inclusion has called for datasets that are representative of various demographic groups. In this paper, we contribute an analysis of the representativeness of age, gender, and race & ethnicity in accessibility datasets-datasets sourced from people with disabilities and older adults-that can potentially play an important role in mitigating bias for inclusive AI-infused applications. We examine the current state of representation within datasets sourced by people with disabilities by reviewing publicly-available information of 190 datasets, we call these accessibility datasets. We find that accessibility datasets represent diverse ages, but have gender and race representation gaps. Additionally, we investigate how the sensitive and complex nature of demographic variables makes classification difficult and inconsistent (<i>e.g.</i>, gender, race & ethnicity), with the source of labeling often unknown. By reflecting on the current challenges and opportunities for representation of disabled data contributors, we hope our effort expands the space of possibility for greater inclusion of marginalized communities in AI-infused systems.",
      "abstract": "As data-driven systems are increasingly deployed at scale, ethical concerns have arisen around unfair and discriminatory outcomes for historically marginalized groups that are underrepresented in training data. In response, work around AI fairness and inclusion has called for datasets that are representative of various demographic groups. In this paper, we contribute an analysis of the representativeness of age, gender, and race & ethnicity in accessibility datasets-datasets sourced from people with disabilities and older adults-that can potentially play an important role in mitigating bias for inclusive AI-infused applications. We examine the current state of representation within datasets sourced by people with disabilities by reviewing publicly-available information of 190 datasets, we call these accessibility datasets. We find that accessibility datasets represent diverse ages, but have gender and race representation gaps. Additionally, we investigate how the sensitive and complex nature of demographic variables makes classification difficult and inconsistent (<i>e.g.</i>, gender, race & ethnicity), with the source of labeling often unknown. By reflecting on the current challenges and opportunities for representation of disabled data contributors, we hope our effort expands the space of possibility for greater inclusion of marginalized communities in AI-infused systems.",
      "doi": "https://doi.org/10.1145/3517428.3544826",
      "openalex_id": "https://openalex.org/W4297888912",
      "arxiv_id": "",
      "publication_date": "2022-10-22",
      "published": "2022-10-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attitudes Toward Signing Avatars Vary Depending on Hearing Status, Age of Signed Language Acquisition, and Avatar Type",
      "summary": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer's preferred or native language. As the development of signing avatars has gained traction in recent years, researchers have come up with many different methods of creating signing avatars. The resulting avatars vary widely in their appearance, the naturalness of their movements, and facial expressions—all of which may potentially impact users' acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars while also examining the extent to which people's own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by (1) a live human signer (Human), (2) an avatar made using computer-synthesized animation (CS Avatar), and (3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf ( N = 83), Hard-of-Hearing ( N = 34), and Hearing ( N = 67) groups. Participants rated the three signers on multiple dimensions, which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but we did not see this association for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users' ratings of signing avatars and show that signed language users with earlier age of ASL acquisition are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars consider retaining the fluid movement qualities integral to signed languages.",
      "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer's preferred or native language. As the development of signing avatars has gained traction in recent years, researchers have come up with many different methods of creating signing avatars. The resulting avatars vary widely in their appearance, the naturalness of their movements, and facial expressions—all of which may potentially impact users' acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars while also examining the extent to which people's own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by (1) a live human signer (Human), (2) an avatar made using computer-synthesized animation (CS Avatar), and (3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf ( N = 83), Hard-of-Hearing ( N = 34), and Hearing ( N = 67) groups. Participants rated the three signers on multiple dimensions, which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but we did not see this association for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users' ratings of signing avatars and show that signed language users with earlier age of ASL acquisition are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars consider retaining the fluid movement qualities integral to signed languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.730917",
      "openalex_id": "https://openalex.org/W4211140127",
      "arxiv_id": "",
      "publication_date": "2022-02-10",
      "published": "2022-02-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CV-SincNet: Learning Complex Sinc Filters From Raw Radar Data for Computationally Efficient Human Motion Recognition",
      "summary": "The utilization of radio-frequency (RF) sensing in cyber-physical human systems, such as human-computer interfaces or smart environments, is an emerging application that requires real-time human motion recognition. However, current state-of-the-art radar-based recognition techniques rely on computing various RF data representations, such as range-Doppler or range-Angle maps, micro-Doppler signatures, or higher dimensional representations, which have great computational complexity. Consequently, classification of raw radar data has garnered increasing interest, while remaining limited in the accuracy that can be attained for recognition of even simple gross motor activities. To help address this challenge, this paper proposes a more interpretable complex-valued neural network design. Complex sinc filters are designed to learn frequency-based relationships directly from the complex raw radar data in the initial layer of the proposed model. The complex-valued sinc layer consists of windowed band-pass filters that learn the center frequency and bandwidth of each filter. A challenging RF dataset consisting of 100 words from American Sign Language (ASL) is selected to verify the model. About 40% improvement in classification accuracy was achieved over the application of a 1D CNN on raw RF data, while 8% improvement was achieved compared to real-valued SincNet. Our proposed approach achieved a 4% improvement in accuracy over that attained with a 2D CNN applied to micro-Doppler spectrograms, while also reducing the overall computational latency by 71%.",
      "abstract": "The utilization of radio-frequency (RF) sensing in cyber-physical human systems, such as human-computer interfaces or smart environments, is an emerging application that requires real-time human motion recognition. However, current state-of-the-art radar-based recognition techniques rely on computing various RF data representations, such as range-Doppler or range-Angle maps, micro-Doppler signatures, or higher dimensional representations, which have great computational complexity. Consequently, classification of raw radar data has garnered increasing interest, while remaining limited in the accuracy that can be attained for recognition of even simple gross motor activities. To help address this challenge, this paper proposes a more interpretable complex-valued neural network design. Complex sinc filters are designed to learn frequency-based relationships directly from the complex raw radar data in the initial layer of the proposed model. The complex-valued sinc layer consists of windowed band-pass filters that learn the center frequency and bandwidth of each filter. A challenging RF dataset consisting of 100 words from American Sign Language (ASL) is selected to verify the model. About 40% improvement in classification accuracy was achieved over the application of a 1D CNN on raw RF data, while 8% improvement was achieved compared to real-valued SincNet. Our proposed approach achieved a 4% improvement in accuracy over that attained with a 2D CNN applied to micro-Doppler spectrograms, while also reducing the overall computational latency by 71%.",
      "doi": "https://doi.org/10.1109/trs.2023.3310894",
      "openalex_id": "https://openalex.org/W4386320441",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Psycholinguistic norms for more than 300 lexical signs in German Sign Language (DGS)",
      "summary": "Abstract Sign language offers a unique perspective on the human faculty of language by illustrating that linguistic abilities are not bound to speech and writing. In studies of spoken and written language processing, lexical variables such as, for example, age of acquisition have been found to play an important role, but such information is not as yet available for German Sign Language ( Deutsche Gebärdensprache , DGS). Here, we present a set of norms for frequency, age of acquisition, and iconicity for more than 300 lexical DGS signs, derived from subjective ratings by 32 deaf signers. We also provide additional norms for iconicity and transparency for the same set of signs derived from ratings by 30 hearing non-signers. In addition to empirical norming data, the dataset includes machine-readable information about a sign’s correspondence in German and English, as well as annotations of lexico-semantic and phonological properties: one-handed vs. two-handed, place of articulation, most likely lexical class, animacy, verb type, (potential) homonymy, and potential dialectal variation. Finally, we include information about sign onset and offset for all stimulus clips from automated motion-tracking data. All norms, stimulus clips, data, as well as code used for analysis are made available through the Open Science Framework in the hope that they may prove to be useful to other researchers: 10.17605/OSF.IO/MZ8J4",
      "abstract": "Abstract Sign language offers a unique perspective on the human faculty of language by illustrating that linguistic abilities are not bound to speech and writing. In studies of spoken and written language processing, lexical variables such as, for example, age of acquisition have been found to play an important role, but such information is not as yet available for German Sign Language ( Deutsche Gebärdensprache , DGS). Here, we present a set of norms for frequency, age of acquisition, and iconicity for more than 300 lexical DGS signs, derived from subjective ratings by 32 deaf signers. We also provide additional norms for iconicity and transparency for the same set of signs derived from ratings by 30 hearing non-signers. In addition to empirical norming data, the dataset includes machine-readable information about a sign’s correspondence in German and English, as well as annotations of lexico-semantic and phonological properties: one-handed vs. two-handed, place of articulation, most likely lexical class, animacy, verb type, (potential) homonymy, and potential dialectal variation. Finally, we include information about sign onset and offset for all stimulus clips from automated motion-tracking data. All norms, stimulus clips, data, as well as code used for analysis are made available through the Open Science Framework in the hope that they may prove to be useful to other researchers: 10.17605/OSF.IO/MZ8J4",
      "doi": "https://doi.org/10.3758/s13428-020-01524-y",
      "openalex_id": "https://openalex.org/W2980432777",
      "arxiv_id": "",
      "publication_date": "2021-02-11",
      "published": "2021-02-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effect of Automatic Sign Recognition Performance on the Usability of Video-Based Search Interfaces for Sign Language Dictionaries",
      "summary": "Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.",
      "abstract": "Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.",
      "doi": "https://doi.org/10.1145/3308561.3353791",
      "openalex_id": "https://openalex.org/W2982201827",
      "arxiv_id": "",
      "publication_date": "2019-10-24",
      "published": "2019-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iconicity ratings really do measure iconicity, and they open a new window onto the nature of language",
      "summary": "Abstract This paper reviews recent research using participant ratings to measure the iconicity (form-meaning resemblance) of words and signs. This method, by enabling wide coverage of lexical items and cross-linguistic comparison, has revealed systematic patterns in how iconicity is distributed across the vocabularies of different languages. These findings are consistent with established linguistic and psychological theory on iconicity, and they connect iconicity to factors like learning and acquisition, semantics, pragmatic aspects of language like playfulness, and to the semantic neighborhood density of words and signs. After taking stock of this research, we look critically at the construct validity of iconicity ratings, considering an alternative account of iconicity ratings recently put forward by Thompson, Arthur Lewis, Kimi Akita &amp; Youngah Do. 2020a. Iconicity ratings across the Japanese lexicon: A comparative study with English. Linguistics Vanguard 6. 20190088. They propose that, for most vocabulary, participants might rate the iconicity of different words based on their meaning alone – specifically the degree to which it relates to the senses – independently of actual form-meaning resemblance. We argue that their hypothesis cannot account for many of the various, theory-driven results from this line of research, which strongly support the conclusion that the ratings really do measure iconicity.",
      "abstract": "Abstract This paper reviews recent research using participant ratings to measure the iconicity (form-meaning resemblance) of words and signs. This method, by enabling wide coverage of lexical items and cross-linguistic comparison, has revealed systematic patterns in how iconicity is distributed across the vocabularies of different languages. These findings are consistent with established linguistic and psychological theory on iconicity, and they connect iconicity to factors like learning and acquisition, semantics, pragmatic aspects of language like playfulness, and to the semantic neighborhood density of words and signs. After taking stock of this research, we look critically at the construct validity of iconicity ratings, considering an alternative account of iconicity ratings recently put forward by Thompson, Arthur Lewis, Kimi Akita &amp; Youngah Do. 2020a. Iconicity ratings across the Japanese lexicon: A comparative study with English. Linguistics Vanguard 6. 20190088. They propose that, for most vocabulary, participants might rate the iconicity of different words based on their meaning alone – specifically the degree to which it relates to the senses – independently of actual form-meaning resemblance. We argue that their hypothesis cannot account for many of the various, theory-driven results from this line of research, which strongly support the conclusion that the ratings really do measure iconicity.",
      "doi": "https://doi.org/10.1515/lingvan-2020-0135",
      "openalex_id": "https://openalex.org/W3173157255",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition With RF Sensors",
      "summary": "RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.",
      "abstract": "RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.",
      "doi": "https://doi.org/10.1109/taes.2021.3139848",
      "openalex_id": "https://openalex.org/W4206328226",
      "arxiv_id": "",
      "publication_date": "2022-01-04",
      "published": "2022-01-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos",
      "summary": "The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"translate\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.",
      "abstract": "The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"translate\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.",
      "doi": "https://doi.org/10.1145/3596261",
      "openalex_id": "https://openalex.org/W4380361332",
      "arxiv_id": "",
      "publication_date": "2023-06-12",
      "published": "2023-06-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper",
      "summary": "Language processing is influenced by sensorimotor experiences. Here, we review behavioral evidence for embodied and grounded influences in language processing across six linguistic levels of granularity. We examine (a) sub-word features, discussing grounded influences on iconicity (systematic associations between word form and meaning); (b) words, discussing boundary conditions and generalizations for the simulation of color, sensory modality, and spatial position; (c) sentences, discussing boundary conditions and applications of action direction simulation; (d) texts, discussing how the teaching of simulation can improve comprehension in beginning readers; (e) conversations, discussing how multi-modal cues improve turn taking and alignment; and (f) text corpora, discussing how distributional semantic models can reveal how grounded and embodied knowledge is encoded in texts. These approaches are converging on a convincing account of the psychology of language, but at the same time, there are important criticisms of the embodied approach and of specific experimental paradigms. The surest way forward requires the adoption of a wide array of scientific methods. By providing complimentary evidence, a combination of multiple methods on various levels of granularity can help us gain a more complete understanding of the role of embodiment and grounding in language processing.",
      "abstract": "Language processing is influenced by sensorimotor experiences. Here, we review behavioral evidence for embodied and grounded influences in language processing across six linguistic levels of granularity. We examine (a) sub-word features, discussing grounded influences on iconicity (systematic associations between word form and meaning); (b) words, discussing boundary conditions and generalizations for the simulation of color, sensory modality, and spatial position; (c) sentences, discussing boundary conditions and applications of action direction simulation; (d) texts, discussing how the teaching of simulation can improve comprehension in beginning readers; (e) conversations, discussing how multi-modal cues improve turn taking and alignment; and (f) text corpora, discussing how distributional semantic models can reveal how grounded and embodied knowledge is encoded in texts. These approaches are converging on a convincing account of the psychology of language, but at the same time, there are important criticisms of the embodied approach and of specific experimental paradigms. The surest way forward requires the adoption of a wide array of scientific methods. By providing complimentary evidence, a combination of multiple methods on various levels of granularity can help us gain a more complete understanding of the role of embodiment and grounding in language processing.",
      "doi": "https://doi.org/10.5334/joc.231",
      "openalex_id": "https://openalex.org/W4387507685",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Impoverished language in early childhood affects the development of complex sentence structure",
      "summary": "Abstract The hypothesis that impoverished language experience affects complex sentence structure development around the end of early childhood was tested using a fully randomized, sentence‐to‐picture matching study in American Sign Language (ASL). The participants were ASL signers who had impoverished or typical access to language in early childhood. Deaf signers whose access to language was highly impoverished in early childhood ( N = 11) primarily comprehended structures consisting of a single verb and argument (Subject or Object), agreeing verbs, and the spatial relation or path of semantic classifiers. They showed difficulty comprehending more complex sentence structures involving dual lexical arguments or multiple verbs. As predicted, participants with typical language access in early childhood, deaf native signers ( N = 17) or hearing second‐language learners ( N = 10), comprehended the range of 12 ASL sentence structures, independent of the subjective iconicity or frequency of the stimulus lexical items, or length of ASL experience and performance on non‐verbal cognitive tasks. The results show that language experience in early childhood is necessary for the development of complex syntax. Research Highlights Previous research with deaf signers suggests an inflection point around the end of early childhood for sentence structure development. Deaf signers who experienced impoverished language until the age of 9 or older comprehend several basic sentence structures but few complex structures. Language experience in early childhood is necessary for the development of complex sentence structure.",
      "abstract": "Abstract The hypothesis that impoverished language experience affects complex sentence structure development around the end of early childhood was tested using a fully randomized, sentence‐to‐picture matching study in American Sign Language (ASL). The participants were ASL signers who had impoverished or typical access to language in early childhood. Deaf signers whose access to language was highly impoverished in early childhood ( N = 11) primarily comprehended structures consisting of a single verb and argument (Subject or Object), agreeing verbs, and the spatial relation or path of semantic classifiers. They showed difficulty comprehending more complex sentence structures involving dual lexical arguments or multiple verbs. As predicted, participants with typical language access in early childhood, deaf native signers ( N = 17) or hearing second‐language learners ( N = 10), comprehended the range of 12 ASL sentence structures, independent of the subjective iconicity or frequency of the stimulus lexical items, or length of ASL experience and performance on non‐verbal cognitive tasks. The results show that language experience in early childhood is necessary for the development of complex syntax. Research Highlights Previous research with deaf signers suggests an inflection point around the end of early childhood for sentence structure development. Deaf signers who experienced impoverished language until the age of 9 or older comprehend several basic sentence structures but few complex structures. Language experience in early childhood is necessary for the development of complex sentence structure.",
      "doi": "https://doi.org/10.1111/desc.13416",
      "openalex_id": "https://openalex.org/W4378783225",
      "arxiv_id": "",
      "publication_date": "2023-05-31",
      "published": "2023-05-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Complex SincNet for More Interpretable Radar Based Activity Recognition",
      "summary": "Radio frequency (RF) sensing has been increasingly used in many applications such as fall-motion recognition, human-machine interfacing, gesture controlled home appliances, and American sign language (ASL) recognition. However, most current activity classification techniques employ a two-stage process. First, the micro-Doppler spectrograms or a temporal sequence of range-Doppler (RD) maps are created. Then, in the second stage, the created representation is used with deep learning (DL) or machine learning (ML) techniques. In this paper, we propose a more interpretable complex-valued neural network architecture to directly classify human activities from the complex-valued raw radar data. The one-dimensional (1D) slow-time radar data is used as the raw radar input, and the complex sinc function is used as the first layer of the proposed model. The sinc filter is a windowed band-pass filter in which the model learns only the lower and higher frequencies of the bandpass filter. To verify our model, an American Sign Language (ASL) dataset consisting of 15 activity classes is selected. The proposed complex-valued SincNet (CV-SincNet) provides higher classification accuracy compared to direct application of a convolutional neural network (CNN) or real-valued SincNet. Enhanced results are observed compared to standard CNN models applied on the Micro-Doppler spectrogram images.",
      "abstract": "Radio frequency (RF) sensing has been increasingly used in many applications such as fall-motion recognition, human-machine interfacing, gesture controlled home appliances, and American sign language (ASL) recognition. However, most current activity classification techniques employ a two-stage process. First, the micro-Doppler spectrograms or a temporal sequence of range-Doppler (RD) maps are created. Then, in the second stage, the created representation is used with deep learning (DL) or machine learning (ML) techniques. In this paper, we propose a more interpretable complex-valued neural network architecture to directly classify human activities from the complex-valued raw radar data. The one-dimensional (1D) slow-time radar data is used as the raw radar input, and the complex sinc function is used as the first layer of the proposed model. The sinc filter is a windowed band-pass filter in which the model learns only the lower and higher frequencies of the bandpass filter. To verify our model, an American Sign Language (ASL) dataset consisting of 15 activity classes is selected. The proposed complex-valued SincNet (CV-SincNet) provides higher classification accuracy compared to direct application of a convolutional neural network (CNN) or real-valued SincNet. Enhanced results are observed compared to standard CNN models applied on the Micro-Doppler spectrogram images.",
      "doi": "https://doi.org/10.1109/radarconf2351548.2023.10149682",
      "openalex_id": "https://openalex.org/W4381745561",
      "arxiv_id": "",
      "publication_date": "2023-05-01",
      "published": "2023-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HRSpecNET: A Deep Learning-Based High-Resolution Radar Micro-Doppler Signature Reconstruction for Improved HAR Classification",
      "summary": "Micro-Doppler signatures (μ-DS) are widely used for human activity recognition (HAR) using radar. However, traditional methods for generating μ-DS, such as the Short-Time Fourier Transform (STFT), suffer from limitations, such as the trade-off between time and frequency resolution, noise sensitivity, and parameter calibration. To address these limitations, we propose a novel deep learning-based approach to reconstruct high-resolution μ-DS directly from 1D complex time-domain signal. Our deep learning architecture consists of an autoencoder block to improve signal-to-noise ratio (SNR), an STFT block to learn frequency transformations to generate pseudo spectrograms, and finally, a UNET block to reconstruct high-resolution spectrogram images. We evaluated our proposed architecture on both synthetic and real-world data. For synthetic data, we generated 1D complex time domain signals with multiple time-varying frequencies and evaluated and compared the ability of our network to generate high-resolution μ-DS and perform in different SNR levels. For real-world data, a challenging radar-based American Sign Language (ASL) dataset consisting of 100 words was used to evaluate the classification performance achieved using the μ-DS generated by the proposed approach. The results showed that the proposed approach outperforms the classification accuracy of traditional STFT-based μ-DS by 3.48%. Both synthetic and experimental μ-DS show that the proposed approach learns to reconstruct higher-resolution and sparser spectrograms.",
      "abstract": "Micro-Doppler signatures (μ-DS) are widely used for human activity recognition (HAR) using radar. However, traditional methods for generating μ-DS, such as the Short-Time Fourier Transform (STFT), suffer from limitations, such as the trade-off between time and frequency resolution, noise sensitivity, and parameter calibration. To address these limitations, we propose a novel deep learning-based approach to reconstruct high-resolution μ-DS directly from 1D complex time-domain signal. Our deep learning architecture consists of an autoencoder block to improve signal-to-noise ratio (SNR), an STFT block to learn frequency transformations to generate pseudo spectrograms, and finally, a UNET block to reconstruct high-resolution spectrogram images. We evaluated our proposed architecture on both synthetic and real-world data. For synthetic data, we generated 1D complex time domain signals with multiple time-varying frequencies and evaluated and compared the ability of our network to generate high-resolution μ-DS and perform in different SNR levels. For real-world data, a challenging radar-based American Sign Language (ASL) dataset consisting of 100 words was used to evaluate the classification performance achieved using the μ-DS generated by the proposed approach. The results showed that the proposed approach outperforms the classification accuracy of traditional STFT-based μ-DS by 3.48%. Both synthetic and experimental μ-DS show that the proposed approach learns to reconstruct higher-resolution and sparser spectrograms.",
      "doi": "https://doi.org/10.1109/trs.2024.3396172",
      "openalex_id": "https://openalex.org/W4396594900",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introduction and Analysis of an Event-Based Sign Language Dataset",
      "summary": "Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.",
      "abstract": "Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.",
      "doi": "https://doi.org/10.1109/fg47880.2020.00069",
      "openalex_id": "https://openalex.org/W3128431487",
      "arxiv_id": "",
      "publication_date": "2020-11-01",
      "published": "2020-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonological and semantic priming in American Sign Language: N300 and N400 effects",
      "summary": "This study investigated the electrophysiological signatures of phonological and semantic priming in American Sign Language (ASL). Deaf signers made semantic relatedness judgments to pairs of ASL signs separated by a 1300 ms prime-target SOA. Phonologically related sign pairs shared two of three phonological parameters (handshape, location, and movement). Target signs preceded by phonologically related and semantically related prime signs elicited smaller negativities within the N300 and N400 windows than those preceded by unrelated primes. N300 effects, typically reported in studies of picture processing, are interpreted to reflect the mapping from the visual features of the signs to more abstract linguistic representations. N400 effects, consistent with rhyme priming effects in the spoken language literature, are taken to index lexico-semantic processes that appear to be largely modality independent. Together, these results highlight both the unique visual-manual nature of sign languages and the linguistic processing characteristics they share with spoken languages.",
      "abstract": "This study investigated the electrophysiological signatures of phonological and semantic priming in American Sign Language (ASL). Deaf signers made semantic relatedness judgments to pairs of ASL signs separated by a 1300 ms prime-target SOA. Phonologically related sign pairs shared two of three phonological parameters (handshape, location, and movement). Target signs preceded by phonologically related and semantically related prime signs elicited smaller negativities within the N300 and N400 windows than those preceded by unrelated primes. N300 effects, typically reported in studies of picture processing, are interpreted to reflect the mapping from the visual features of the signs to more abstract linguistic representations. N400 effects, consistent with rhyme priming effects in the spoken language literature, are taken to index lexico-semantic processes that appear to be largely modality independent. Together, these results highlight both the unique visual-manual nature of sign languages and the linguistic processing characteristics they share with spoken languages.",
      "doi": "https://doi.org/10.1080/23273798.2018.1446543",
      "openalex_id": "https://openalex.org/W2793554983",
      "arxiv_id": "",
      "publication_date": "2018-03-07",
      "published": "2018-03-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The role of iconicity, construal, and proficiency in the online processing of handshape",
      "summary": "abstract Iconicity has traditionally been considered an objective, fixed, unidimensional property of language forms, often operationalized as transparency for experimental purposes. Within a Cognitive Linguistics framework, iconicity is a mapping between an individual’s construal of form and construal of meaning, such that iconicity is subjective, dynamic, and multidimensional. We test the latter alternative by asking signers who differed in ASL proficiency to complete a handshape monitoring task in which we manipulated the number of form–meaning construals that target handshapes participated in. We estimated the interaction of iconicity, proficiency, and construal density using mixed-effects models for response time and accuracy with crossed random effects for participants and items. Results show a significant three-way interaction between iconicity, proficiency, and construal density such that less-proficient signers detected handshapes in more iconic signs faster than less iconic signs regardless of the handshape they were monitoring, but highly proficient signers’ performance was only improved by iconicity for handshapes that participate in many construals. Taken in conjunction with growing evidence of the subjectivity of iconicity, we interpret these results as support for the claim that construal is a core mechanism underlying iconicity, both for transparent and systematic language-internal form–meaning mappings.",
      "abstract": "abstract Iconicity has traditionally been considered an objective, fixed, unidimensional property of language forms, often operationalized as transparency for experimental purposes. Within a Cognitive Linguistics framework, iconicity is a mapping between an individual’s construal of form and construal of meaning, such that iconicity is subjective, dynamic, and multidimensional. We test the latter alternative by asking signers who differed in ASL proficiency to complete a handshape monitoring task in which we manipulated the number of form–meaning construals that target handshapes participated in. We estimated the interaction of iconicity, proficiency, and construal density using mixed-effects models for response time and accuracy with crossed random effects for participants and items. Results show a significant three-way interaction between iconicity, proficiency, and construal density such that less-proficient signers detected handshapes in more iconic signs faster than less iconic signs regardless of the handshape they were monitoring, but highly proficient signers’ performance was only improved by iconicity for handshapes that participate in many construals. Taken in conjunction with growing evidence of the subjectivity of iconicity, we interpret these results as support for the claim that construal is a core mechanism underlying iconicity, both for transparent and systematic language-internal form–meaning mappings.",
      "doi": "https://doi.org/10.1017/langcog.2020.1",
      "openalex_id": "https://openalex.org/W3009084745",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sensorimotor characteristics of sign translations modulate EEG when deaf signers read English",
      "summary": "Bilingual individuals automatically translate written words from one language to another. While this process is established in spoken-language bilinguals, there is less known about its occurrence in deaf bilinguals who know signed and spoken languages. Since sign language uses motion and space to convey linguistic content, it is possible that action simulation in the brain's sensorimotor system plays a role in this process. We recorded EEG from deaf participants fluent in ASL as they read individual English words and found significant differences in alpha and beta EEG at central electrode sites during the reading of English words whose ASL translations use two hands, compared to English words whose ASL translations use one hand. Hearing non-signers did not show any differences between conditions. These results demonstrate the involvement of the sensorimotor system in cross-linguistic, cross-modal translation, and suggest that covert action simulation processes are involved when deaf signers read.",
      "abstract": "Bilingual individuals automatically translate written words from one language to another. While this process is established in spoken-language bilinguals, there is less known about its occurrence in deaf bilinguals who know signed and spoken languages. Since sign language uses motion and space to convey linguistic content, it is possible that action simulation in the brain's sensorimotor system plays a role in this process. We recorded EEG from deaf participants fluent in ASL as they read individual English words and found significant differences in alpha and beta EEG at central electrode sites during the reading of English words whose ASL translations use two hands, compared to English words whose ASL translations use one hand. Hearing non-signers did not show any differences between conditions. These results demonstrate the involvement of the sensorimotor system in cross-linguistic, cross-modal translation, and suggest that covert action simulation processes are involved when deaf signers read.",
      "doi": "https://doi.org/10.1016/j.bandl.2018.10.001",
      "openalex_id": "https://openalex.org/W2884597521",
      "arxiv_id": "",
      "publication_date": "2018-11-03",
      "published": "2018-11-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Higher Order Feature Extraction and Selection for Robust Human Gesture Recognition using CSI of COTS Wi-Fi Devices",
      "summary": "Device-free human gesture recognition (HGR) using commercial off the shelf (COTS) Wi-Fi devices has gained attention with recent advances in wireless technology. HGR recognizes the human activity performed, by capturing the reflections of Wi-Fi signals from moving humans and storing them as raw channel state information (CSI) traces. Existing work on HGR applies noise reduction and transformation to pre-process the raw CSI traces. However, these methods fail to capture the non-Gaussian information in the raw CSI data due to its limitation to deal with linear signal representation alone. The proposed higher order statistics-based recognition (HOS-Re) model extracts higher order statistical (HOS) features from raw CSI traces and selects a robust feature subset for the recognition task. HOS-Re addresses the limitations in the existing methods, by extracting third order cumulant features that maximizes the recognition accuracy. Subsequently, feature selection methods derived from information theory construct a robust and highly informative feature subset, fed as input to the multilevel support vector machine (SVM) classifier in order to measure the performance. The proposed methodology is validated using a public database SignFi, consisting of 276 gestures with 8280 gesture instances, out of which 5520 are from the laboratory and 2760 from the home environment using a 10 × 5 cross-validation. HOS-Re achieved an average recognition accuracy of 97.84%, 98.26% and 96.34% for the lab, home and lab + home environment respectively. The average recognition accuracy for 150 sign gestures with 7500 instances, collected from five different users was 96.23% in the laboratory environment.",
      "abstract": "Device-free human gesture recognition (HGR) using commercial off the shelf (COTS) Wi-Fi devices has gained attention with recent advances in wireless technology. HGR recognizes the human activity performed, by capturing the reflections of Wi-Fi signals from moving humans and storing them as raw channel state information (CSI) traces. Existing work on HGR applies noise reduction and transformation to pre-process the raw CSI traces. However, these methods fail to capture the non-Gaussian information in the raw CSI data due to its limitation to deal with linear signal representation alone. The proposed higher order statistics-based recognition (HOS-Re) model extracts higher order statistical (HOS) features from raw CSI traces and selects a robust feature subset for the recognition task. HOS-Re addresses the limitations in the existing methods, by extracting third order cumulant features that maximizes the recognition accuracy. Subsequently, feature selection methods derived from information theory construct a robust and highly informative feature subset, fed as input to the multilevel support vector machine (SVM) classifier in order to measure the performance. The proposed methodology is validated using a public database SignFi, consisting of 276 gestures with 8280 gesture instances, out of which 5520 are from the laboratory and 2760 from the home environment using a 10 × 5 cross-validation. HOS-Re achieved an average recognition accuracy of 97.84%, 98.26% and 96.34% for the lab, home and lab + home environment respectively. The average recognition accuracy for 150 sign gestures with 7500 instances, collected from five different users was 96.23% in the laboratory environment.",
      "doi": "https://doi.org/10.3390/s19132959",
      "openalex_id": "https://openalex.org/W2953413701",
      "arxiv_id": "",
      "publication_date": "2019-07-04",
      "published": "2019-07-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A data-driven approach to the semantics of iconicity in American Sign Language and English",
      "summary": "abstract A growing body of research shows that both signed and spoken languages display regular patterns of iconicity in their vocabularies. We compared iconicity in the lexicons of American Sign Language (ASL) and English by combining previously collected ratings of ASL signs (Caselli, Sevcikova Sehyr, Cohen-Goldberg, &amp; Emmorey, 2017) and English words (Winter, Perlman, Perry, &amp; Lupyan, 2017) with the use of data-driven semantic vectors derived from English. Our analyses show that models of spoken language lexical semantics drawn from large text corpora can be useful for predicting the iconicity of signs as well as words. Compared to English, ASL has a greater number of regions of semantic space with concentrations of highly iconic vocabulary. There was an overall negative relationship between semantic density and the iconicity of both English words and ASL signs. This negative relationship disappeared for highly iconic signs, suggesting that iconic forms may be more easily discriminable in ASL than in English. Our findings contribute to an increasingly detailed picture of how iconicity is distributed across different languages.",
      "abstract": "abstract A growing body of research shows that both signed and spoken languages display regular patterns of iconicity in their vocabularies. We compared iconicity in the lexicons of American Sign Language (ASL) and English by combining previously collected ratings of ASL signs (Caselli, Sevcikova Sehyr, Cohen-Goldberg, &amp; Emmorey, 2017) and English words (Winter, Perlman, Perry, &amp; Lupyan, 2017) with the use of data-driven semantic vectors derived from English. Our analyses show that models of spoken language lexical semantics drawn from large text corpora can be useful for predicting the iconicity of signs as well as words. Compared to English, ASL has a greater number of regions of semantic space with concentrations of highly iconic vocabulary. There was an overall negative relationship between semantic density and the iconicity of both English words and ASL signs. This negative relationship disappeared for highly iconic signs, suggesting that iconic forms may be more easily discriminable in ASL than in English. Our findings contribute to an increasingly detailed picture of how iconicity is distributed across different languages.",
      "doi": "https://doi.org/10.1017/langcog.2019.52",
      "openalex_id": "https://openalex.org/W3010194027",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Age, frequency, and iconicity in early sign language acquisition: Evidence from the Israeli Sign Language MacArthur–Bates Communicative Developmental Inventory",
      "summary": "Abstract The current study described the development of the MacArthur–Bates Communicative Developmental Inventory (CDI) for Israeli Sign Language (ISL) and investigated the effects of age, sign iconicity, and sign frequency on lexical acquisition of bimodal-bilingual toddlers acquiring ISL. Previous findings bring inconclusive evidence on the role of sign iconicity (the relationship between form and meaning) and sign frequency (how often a word/sign is used in the language) on the acquisition of signs. The ISL-CDI consisted of 563 video clips. Iconicity ratings from 41 sign-naïve Hebrew-speaking adults (Study 1A) and sign frequency ratings from 19 native ISL adult signers (Study 1B) were collected. ISL vocabulary was evaluated in 34 toddlers, native signers (Study 2). Results indicated significant effects of age, strong correlations between parental ISL ratings and ISL size even when age was controlled for, and strong correlations between naturalistic data and ISL-CDI scores, supporting the validity of the ISL-CDI. Moreover, the results revealed effects of iconicity, frequency, and interactions between age and the iconicity and frequency factors, suggesting that both iconicity and frequency are modulated by age. The findings contribute to the field of sign language acquisition and to our understanding of potential factors affecting human language acquisition beyond language modality.",
      "abstract": "Abstract The current study described the development of the MacArthur–Bates Communicative Developmental Inventory (CDI) for Israeli Sign Language (ISL) and investigated the effects of age, sign iconicity, and sign frequency on lexical acquisition of bimodal-bilingual toddlers acquiring ISL. Previous findings bring inconclusive evidence on the role of sign iconicity (the relationship between form and meaning) and sign frequency (how often a word/sign is used in the language) on the acquisition of signs. The ISL-CDI consisted of 563 video clips. Iconicity ratings from 41 sign-naïve Hebrew-speaking adults (Study 1A) and sign frequency ratings from 19 native ISL adult signers (Study 1B) were collected. ISL vocabulary was evaluated in 34 toddlers, native signers (Study 2). Results indicated significant effects of age, strong correlations between parental ISL ratings and ISL size even when age was controlled for, and strong correlations between naturalistic data and ISL-CDI scores, supporting the validity of the ISL-CDI. Moreover, the results revealed effects of iconicity, frequency, and interactions between age and the iconicity and frequency factors, suggesting that both iconicity and frequency are modulated by age. The findings contribute to the field of sign language acquisition and to our understanding of potential factors affecting human language acquisition beyond language modality.",
      "doi": "https://doi.org/10.1017/s0142716420000247",
      "openalex_id": "https://openalex.org/W3044174589",
      "arxiv_id": "",
      "publication_date": "2020-07-01",
      "published": "2020-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multi-Frequency RF Sensor Fusion for Word-Level Fluent ASL Recognition",
      "summary": "Deaf spaces are unique indoor environments designed to optimize visual communication and Deaf cultural expression. However, much of the technological research geared towards the deaf involve use of video or wearables for American sign language (ASL) translation, with little consideration for Deaf perspective on privacy and usability of the technology. In contrast to video, RF sensors offer the avenue for ambient ASL recognition while also preserving privacy for Deaf signers. Methods: This paper investigates the RF transmit waveform parameters required for effective measurement of ASL signs and their effect on word-level classification accuracy attained with transfer learning and convolutional autoencoders (CAE). A multi-frequency fusion network is proposed to exploit data from all sensors in an RF sensor network and improve the recognition accuracy of fluent ASL signing. Results: For fluent signers, CAEs yield a 20-sign classification accuracy of %76 at 77 GHz and %73 at 24 GHz, while at X-band (10 Ghz) accuracy drops to 67%. For hearing imitation signers, signs are more separable, resulting in a 96% accuracy with CAEs. Further, fluent ASL recognition accuracy is significantly increased with use of the multi-frequency fusion network, which boosts the 20-sign fluent ASL recognition accuracy to 95%, surpassing conventional feature level fusion by 12%. Implications: Signing involves finer spatiotemporal dynamics than typical hand gestures, and thus requires interrogation with a transmit waveform that has a rapid succession of pulses and high bandwidth. Millimeter wave RF frequencies also yield greater accuracy due to the increased Doppler spread of the radar backscatter. Comparative analysis of articulation dynamics also shows that imitation signing is not representative of fluent signing, and not effective in pre-training networks for fluent ASL classification. Deep neural networks employing multi-frequency fusion capture both shared, as well as sensor-specific features and thus offer significant performance gains in comparison to using a single sensor or feature-level fusion.",
      "abstract": "Deaf spaces are unique indoor environments designed to optimize visual communication and Deaf cultural expression. However, much of the technological research geared towards the deaf involve use of video or wearables for American sign language (ASL) translation, with little consideration for Deaf perspective on privacy and usability of the technology. In contrast to video, RF sensors offer the avenue for ambient ASL recognition while also preserving privacy for Deaf signers. Methods: This paper investigates the RF transmit waveform parameters required for effective measurement of ASL signs and their effect on word-level classification accuracy attained with transfer learning and convolutional autoencoders (CAE). A multi-frequency fusion network is proposed to exploit data from all sensors in an RF sensor network and improve the recognition accuracy of fluent ASL signing. Results: For fluent signers, CAEs yield a 20-sign classification accuracy of %76 at 77 GHz and %73 at 24 GHz, while at X-band (10 Ghz) accuracy drops to 67%. For hearing imitation signers, signs are more separable, resulting in a 96% accuracy with CAEs. Further, fluent ASL recognition accuracy is significantly increased with use of the multi-frequency fusion network, which boosts the 20-sign fluent ASL recognition accuracy to 95%, surpassing conventional feature level fusion by 12%. Implications: Signing involves finer spatiotemporal dynamics than typical hand gestures, and thus requires interrogation with a transmit waveform that has a rapid succession of pulses and high bandwidth. Millimeter wave RF frequencies also yield greater accuracy due to the increased Doppler spread of the radar backscatter. Comparative analysis of articulation dynamics also shows that imitation signing is not representative of fluent signing, and not effective in pre-training networks for fluent ASL classification. Deep neural networks employing multi-frequency fusion capture both shared, as well as sensor-specific features and thus offer significant performance gains in comparison to using a single sensor or feature-level fusion.",
      "doi": "https://doi.org/10.1109/jsen.2021.3078339",
      "openalex_id": "https://openalex.org/W3163591631",
      "arxiv_id": "",
      "publication_date": "2021-05-07",
      "published": "2021-05-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perceptual optimization of language: Evidence from American Sign Language",
      "summary": "If language has evolved for communication, languages should be structured such that they maximize the efficiency of processing. What is efficient for communication in the visual-gestural modality is different from the auditory-oral modality, and we ask here whether sign languages have adapted to the affordances and constraints of the signed modality. During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (high frequency signs, signs with common handshapes) can be produced further from the face because precise visual resolution is not necessary for recognition. Using pose estimation algorithms, we examined the structure of over 2000 American Sign Language lexical signs to identify whether lexical frequency and handshape probability affect the position of the wrist in 2D space. We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs. Together these results provide empirical support for anecdotal assertions that the phonological structure of sign language is shaped by the properties of the human visual and motor systems.",
      "abstract": "If language has evolved for communication, languages should be structured such that they maximize the efficiency of processing. What is efficient for communication in the visual-gestural modality is different from the auditory-oral modality, and we ask here whether sign languages have adapted to the affordances and constraints of the signed modality. During sign perception, perceivers look almost exclusively at the lower face, rarely looking down at the hands. This means that signs articulated far from the lower face must be perceived through peripheral vision, which has less acuity than central vision. We tested the hypothesis that signs that are more predictable (high frequency signs, signs with common handshapes) can be produced further from the face because precise visual resolution is not necessary for recognition. Using pose estimation algorithms, we examined the structure of over 2000 American Sign Language lexical signs to identify whether lexical frequency and handshape probability affect the position of the wrist in 2D space. We found that frequent signs with rare handshapes tended to occur closer to the signer's face than frequent signs with common handshapes, and that frequent signs are generally more likely to be articulated further from the face than infrequent signs. Together these results provide empirical support for anecdotal assertions that the phonological structure of sign language is shaped by the properties of the human visual and motor systems.",
      "doi": "https://doi.org/10.1016/j.cognition.2022.105040",
      "openalex_id": "https://openalex.org/W4213262042",
      "arxiv_id": "",
      "publication_date": "2022-02-19",
      "published": "2022-02-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iconicity in American Sign Language–English translation recognition",
      "summary": "abstract Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.",
      "abstract": "abstract Reaction times for a translation recognition study are reported where novice to expert English–ASL bilinguals rejected English translation distractors for ASL signs that were related to the correct translations through phonology, semantics, or both form and meaning (diagrammatic iconicity). Imageability ratings of concepts impacted performance in all conditions; when imageability was high, participants showed interference for phonologically related distractors, and when imageability was low participants showed interference for semantically related distractors, regardless of proficiency. For diagrammatically related distractors high imageability caused interference in experts, but low imageability caused interference in novices. These patterns suggest that imageability and diagrammaticity interact with proficiency – experts process diagrammatic related distractors phonologically, but novices process them semantically. This implies that motivated signs are dependent on the entrenchment of language systematicity; rather than decreasing their impact on language processing as proficiency grows, they build on the original benefit conferred by iconic mappings.",
      "doi": "https://doi.org/10.1017/langcog.2019.51",
      "openalex_id": "https://openalex.org/W3010109748",
      "arxiv_id": "",
      "publication_date": "2020-03-01",
      "published": "2020-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tracking the time course of sign recognition using ERP repetition priming",
      "summary": "Abstract Repetition priming and event‐related potentials (ERPs) were used to investigate the time course of sign recognition in deaf users of American Sign Language. Signers performed a go/no‐go semantic categorization task to rare probe signs referring to people; critical target items were repeated and unrelated signs. In Experiment 1, ERPs were time‐locked either to the onset of the video or to sign onset within the video; in Experiment 2, the same full videos were clipped so that video and sign onset were aligned (removing transitional movements), and ERPs were time‐locked to video/sign onset. All analyses revealed an N400 repetition priming effect (less negativity for repeated than unrelated signs) but differed in the timing and/or duration of the N400 effect. Results from Experiment 1 revealed that repetition priming effects began before sign onset within a video, suggesting that signers are sensitive to linguistic information within the transitional movement to sign onset. The timing and duration of the N400 for clipped videos were more parallel to that observed previously for auditorily presented words and was 200 ms shorter than either time‐locking analysis from Experiment 1. We conclude that time‐locking to full video onset is optimal when early ERP components or sensitivity to transitional movements are of interest and that time‐locking to the onset of clipped videos is optimal for priming studies with fluent signers.",
      "abstract": "Abstract Repetition priming and event‐related potentials (ERPs) were used to investigate the time course of sign recognition in deaf users of American Sign Language. Signers performed a go/no‐go semantic categorization task to rare probe signs referring to people; critical target items were repeated and unrelated signs. In Experiment 1, ERPs were time‐locked either to the onset of the video or to sign onset within the video; in Experiment 2, the same full videos were clipped so that video and sign onset were aligned (removing transitional movements), and ERPs were time‐locked to video/sign onset. All analyses revealed an N400 repetition priming effect (less negativity for repeated than unrelated signs) but differed in the timing and/or duration of the N400 effect. Results from Experiment 1 revealed that repetition priming effects began before sign onset within a video, suggesting that signers are sensitive to linguistic information within the transitional movement to sign onset. The timing and duration of the N400 for clipped videos were more parallel to that observed previously for auditorily presented words and was 200 ms shorter than either time‐locking analysis from Experiment 1. We conclude that time‐locking to full video onset is optimal when early ERP components or sensitivity to transitional movements are of interest and that time‐locking to the onset of clipped videos is optimal for priming studies with fluent signers.",
      "doi": "https://doi.org/10.1111/psyp.13975",
      "openalex_id": "https://openalex.org/W3212089245",
      "arxiv_id": "",
      "publication_date": "2021-11-17",
      "published": "2021-11-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Two measures are better than one: combining iconicity ratings and guessing experiments for a more nuanced picture of iconicity in the lexicon",
      "summary": "Abstract Iconicity in language is receiving increased attention from many fields, but our understanding of iconicity is only as good as the measures we use to quantify it. We collected iconicity measures for 304 Japanese words from English-speaking participants, using rating and guessing tasks. The words included ideophones (structurally marked depictive words) along with regular lexical items from similar semantic domains (e.g., fuwafuwa ‘fluffy’, jawarakai ‘soft’). The two measures correlated, speaking to their validity. However, ideophones received consistently higher iconicity ratings than other items, even when guessed at the same accuracies, suggesting the rating task is more sensitive to cues like structural markedness that frame words as iconic. These cues did not always guide participants to the meanings of ideophones in the guessing task, but they did make them more confident in their guesses, even when they were wrong. Consistently poor guessing results reflect the role different experiences play in shaping construals of iconicity. Using multiple measures in tandem allows us to explore the interplay between iconicity and these external factors. To facilitate this, we introduce a reproducible workflow for creating rating and guessing tasks from standardised wordlists, while also making improvements to the robustness, sensitivity and discriminability of previous approaches.",
      "abstract": "Abstract Iconicity in language is receiving increased attention from many fields, but our understanding of iconicity is only as good as the measures we use to quantify it. We collected iconicity measures for 304 Japanese words from English-speaking participants, using rating and guessing tasks. The words included ideophones (structurally marked depictive words) along with regular lexical items from similar semantic domains (e.g., fuwafuwa ‘fluffy’, jawarakai ‘soft’). The two measures correlated, speaking to their validity. However, ideophones received consistently higher iconicity ratings than other items, even when guessed at the same accuracies, suggesting the rating task is more sensitive to cues like structural markedness that frame words as iconic. These cues did not always guide participants to the meanings of ideophones in the guessing task, but they did make them more confident in their guesses, even when they were wrong. Consistently poor guessing results reflect the role different experiences play in shaping construals of iconicity. Using multiple measures in tandem allows us to explore the interplay between iconicity and these external factors. To facilitate this, we introduce a reproducible workflow for creating rating and guessing tasks from standardised wordlists, while also making improvements to the robustness, sensitivity and discriminability of previous approaches.",
      "doi": "https://doi.org/10.1017/langcog.2023.9",
      "openalex_id": "https://openalex.org/W4364379271",
      "arxiv_id": "",
      "publication_date": "2023-04-11",
      "published": "2023-04-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The organization of the American Sign Language lexicon: Comparing one- and two-parameter ERP phonological priming effects across tasks",
      "summary": "We used phonological priming and ERPs to investigate the organization of the lexicon in American Sign Language. Across go/no-go repetition detection and semantic categorization tasks, targets in related pairs that shared handshape and location elicited smaller N400s than targets in unrelated pairs, indicative of facilitated processing. Handshape-related targets also elicited smaller N400s than unrelated targets, but only in the repetition task. The location priming effect reversed direction across tasks, with slightlylargeramplitude N400s for targets in related versus unrelated pairs in the semantic task, indicative of interference. These patterns imply that handshape and location play different roles during sign recognition and that there is a hierarchical organization for the sign lexicon. Similar to interactive-activation models of word recognition, we argue for differentiation between sublexical facilitation and lexical competition. Lexical competition is primarily driven by the location parameter and is more engaged when identification of single lexico-semantic entries is required.",
      "abstract": "We used phonological priming and ERPs to investigate the organization of the lexicon in American Sign Language. Across go/no-go repetition detection and semantic categorization tasks, targets in related pairs that shared handshape and location elicited smaller N400s than targets in unrelated pairs, indicative of facilitated processing. Handshape-related targets also elicited smaller N400s than unrelated targets, but only in the repetition task. The location priming effect reversed direction across tasks, with slightlylargeramplitude N400s for targets in related versus unrelated pairs in the semantic task, indicative of interference. These patterns imply that handshape and location play different roles during sign recognition and that there is a hierarchical organization for the sign lexicon. Similar to interactive-activation models of word recognition, we argue for differentiation between sublexical facilitation and lexical competition. Lexical competition is primarily driven by the location parameter and is more engaged when identification of single lexico-semantic entries is required.",
      "doi": "https://doi.org/10.1016/j.bandl.2021.104960",
      "openalex_id": "https://openalex.org/W3157566248",
      "arxiv_id": "",
      "publication_date": "2021-04-30",
      "published": "2021-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical selection in bimodal bilinguals: ERP evidence from picture-word interference",
      "summary": "The picture word interference (PWI) paradigm and ERPs were used to investigate whether lexical selection in deaf and hearing ASL-English bilinguals occurs via lexical competition or whether the response exclusion hypothesis (REH) for PWI effects is supported. The REH predicts that semantic interference should not occur for bimodal bilinguals because sign and word responses do not compete within an output buffer. Bimodal bilinguals named pictures in ASL, preceded by either a translation equivalent, semantically-related, or unrelated English written word. In both the translation and semantically-related conditions bimodal bilinguals showed facilitation effects: reduced RTs and N400 amplitudes for related compared to unrelated prime conditions. We also observed an unexpected focal left anterior positivity that was stronger in the translation condition, which we speculate may be due to articulatory priming. Overall, the results support the REH and models of bilingual language production that assume lexical selection occurs without competition between languages.",
      "abstract": "The picture word interference (PWI) paradigm and ERPs were used to investigate whether lexical selection in deaf and hearing ASL-English bilinguals occurs via lexical competition or whether the response exclusion hypothesis (REH) for PWI effects is supported. The REH predicts that semantic interference should not occur for bimodal bilinguals because sign and word responses do not compete within an output buffer. Bimodal bilinguals named pictures in ASL, preceded by either a translation equivalent, semantically-related, or unrelated English written word. In both the translation and semantically-related conditions bimodal bilinguals showed facilitation effects: reduced RTs and N400 amplitudes for related compared to unrelated prime conditions. We also observed an unexpected focal left anterior positivity that was stronger in the translation condition, which we speculate may be due to articulatory priming. Overall, the results support the REH and models of bilingual language production that assume lexical selection occurs without competition between languages.",
      "doi": "https://doi.org/10.1080/23273798.2020.1821905",
      "openalex_id": "https://openalex.org/W3088249661",
      "arxiv_id": "",
      "publication_date": "2020-09-21",
      "published": "2020-09-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effect of Sign-recognition Performance on the Usability of Sign-language Dictionary Search",
      "summary": "Advances in sign-language recognition technology have enabled researchers to investigate various methods that can assist users in searching for an unfamiliar sign in ASL using sign-recognition technology. Users can generate a query by submitting a video of themselves performing the sign they believe they encountered somewhere and obtain a list of possible matches. However, there is disagreement among developers of such technology on how to report the performance of their systems, and prior research has not examined the relationship between the performance of search technology and users’ subjective judgements for this task. We conducted three studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that, in addition to the position of the desired word in a list of results, the placement of the desired word above or below the fold and the similarity of the other words in the results list affected users’ judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users’ judgements than did metrics currently reported in prior ASL dictionary research.",
      "abstract": "Advances in sign-language recognition technology have enabled researchers to investigate various methods that can assist users in searching for an unfamiliar sign in ASL using sign-recognition technology. Users can generate a query by submitting a video of themselves performing the sign they believe they encountered somewhere and obtain a list of possible matches. However, there is disagreement among developers of such technology on how to report the performance of their systems, and prior research has not examined the relationship between the performance of search technology and users’ subjective judgements for this task. We conducted three studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that, in addition to the position of the desired word in a list of results, the placement of the desired word above or below the fold and the similarity of the other words in the results list affected users’ judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users’ judgements than did metrics currently reported in prior ASL dictionary research.",
      "doi": "https://doi.org/10.1145/3470650",
      "openalex_id": "https://openalex.org/W3206077713",
      "arxiv_id": "",
      "publication_date": "2021-10-15",
      "published": "2021-10-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mapping Word to World in ASL: Evidence from a Human Simulation Paradigm",
      "summary": "Abstract Across languages, children map words to meaning with great efficiency, despite a seemingly unconstrained space of potential mappings. The literature on how children do this is primarily limited to spoken language. This leaves a gap in our understanding of sign language acquisition, because several of the hypothesized mechanisms that children use are visual (e.g., visual attention to the referent), and sign languages are perceived in the visual modality. Here, we used the Human Simulation Paradigm in American Sign Language (ASL) to determine potential cues to word learning. Sign‐naïve adult participants viewed video clips of parent–child interactions in ASL, and at a designated point, had to guess what ASL sign the parent produced. Across two studies, we demonstrate that referential clarity in ASL interactions is characterized by access to information about word class and referent presence (for verbs), similarly to spoken language. Unlike spoken language, iconicity is a cue to word meaning in ASL, although this is not always a fruitful cue. We also present evidence that verbs are highlighted well in the input, relative to spoken English. The results shed light on both similarities and differences in the information that learners may have access to in acquiring signed versus spoken languages.",
      "abstract": "Abstract Across languages, children map words to meaning with great efficiency, despite a seemingly unconstrained space of potential mappings. The literature on how children do this is primarily limited to spoken language. This leaves a gap in our understanding of sign language acquisition, because several of the hypothesized mechanisms that children use are visual (e.g., visual attention to the referent), and sign languages are perceived in the visual modality. Here, we used the Human Simulation Paradigm in American Sign Language (ASL) to determine potential cues to word learning. Sign‐naïve adult participants viewed video clips of parent–child interactions in ASL, and at a designated point, had to guess what ASL sign the parent produced. Across two studies, we demonstrate that referential clarity in ASL interactions is characterized by access to information about word class and referent presence (for verbs), similarly to spoken language. Unlike spoken language, iconicity is a cue to word meaning in ASL, although this is not always a fruitful cue. We also present evidence that verbs are highlighted well in the input, relative to spoken English. The results shed light on both similarities and differences in the information that learners may have access to in acquiring signed versus spoken languages.",
      "doi": "https://doi.org/10.1111/cogs.13061",
      "openalex_id": "https://openalex.org/W3215451835",
      "arxiv_id": "",
      "publication_date": "2021-12-01",
      "published": "2021-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Support in the Moment: Benefits and use of video-span selection and search for sign-language video comprehension among ASL learners",
      "summary": "As they develop comprehension skills, American Sign Language (ASL) learners often view challenging ASL videos, which may contain unfamiliar signs. Current dictionary tools require students to isolate a single sign they do not understand and input a search query, by selecting linguistic properties or by performing the sign into a webcam. Students may struggle with extracting and re-creating an unfamiliar sign, and they must leave the video-watching task to use an external dictionary tool. We investigate a technology that enables users, in the moment, i.e., while they are viewing a video, to select a span of one or more signs that they do not understand, to view dictionary results. We interviewed 14 American Sign Language (ASL) learners about their challenges in understanding ASL video and workarounds for unfamiliar vocabulary. We then conducted a comparative study and an in-depth analysis with 15 ASL learners to investigate the benefits of using video sub-spans for searching, and their interactions with a Wizard-of-Oz prototype during a video-comprehension task. Our findings revealed benefits of our tool in terms of quality of video translation produced and perceived workload to produce translations. Our in-depth analysis also revealed benefits of an integrated search tool and use of span-selection to constrain video play. These findings inform future designers of such systems, computer vision researchers working on the underlying sign matching technologies, and sign language educators.",
      "abstract": "As they develop comprehension skills, American Sign Language (ASL) learners often view challenging ASL videos, which may contain unfamiliar signs. Current dictionary tools require students to isolate a single sign they do not understand and input a search query, by selecting linguistic properties or by performing the sign into a webcam. Students may struggle with extracting and re-creating an unfamiliar sign, and they must leave the video-watching task to use an external dictionary tool. We investigate a technology that enables users, in the moment, i.e., while they are viewing a video, to select a span of one or more signs that they do not understand, to view dictionary results. We interviewed 14 American Sign Language (ASL) learners about their challenges in understanding ASL video and workarounds for unfamiliar vocabulary. We then conducted a comparative study and an in-depth analysis with 15 ASL learners to investigate the benefits of using video sub-spans for searching, and their interactions with a Wizard-of-Oz prototype during a video-comprehension task. Our findings revealed benefits of our tool in terms of quality of video translation produced and perceived workload to produce translations. Our in-depth analysis also revealed benefits of an integrated search tool and use of span-selection to constrain video play. These findings inform future designers of such systems, computer vision researchers working on the underlying sign matching technologies, and sign language educators.",
      "doi": "https://doi.org/10.1145/3517428.3544883",
      "openalex_id": "https://openalex.org/W4307095343",
      "arxiv_id": "",
      "publication_date": "2022-10-22",
      "published": "2022-10-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Simultaneous structures in sign languages: Acquisition and emergence",
      "summary": "The visual-gestural modality affords its users simultaneous movement of several independent articulators and thus lends itself to simultaneous encoding of information. Much research has focused on the fact that sign languages coordinate two manual articulators in addition to a range of non-manual articulators to present different types of linguistic information simultaneously, from phonological contrasts to inflection, spatial relations, and information structure. Children and adults acquiring a signed language arguably thus need to comprehend and produce simultaneous structures to a greater extent than individuals acquiring a spoken language. In this paper, we discuss the simultaneous encoding that is found in emerging and established sign languages; we also discuss places where sign languages are unexpectedly sequential. We explore potential constraints on simultaneity in cognition and motor coordination that might impact the acquisition and use of simultaneous structures.",
      "abstract": "The visual-gestural modality affords its users simultaneous movement of several independent articulators and thus lends itself to simultaneous encoding of information. Much research has focused on the fact that sign languages coordinate two manual articulators in addition to a range of non-manual articulators to present different types of linguistic information simultaneously, from phonological contrasts to inflection, spatial relations, and information structure. Children and adults acquiring a signed language arguably thus need to comprehend and produce simultaneous structures to a greater extent than individuals acquiring a spoken language. In this paper, we discuss the simultaneous encoding that is found in emerging and established sign languages; we also discuss places where sign languages are unexpectedly sequential. We explore potential constraints on simultaneity in cognition and motor coordination that might impact the acquisition and use of simultaneous structures.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.992589",
      "openalex_id": "https://openalex.org/W4312188350",
      "arxiv_id": "",
      "publication_date": "2022-12-22",
      "published": "2022-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonological Proximity in Costa Rican Sign Language",
      "summary": "The study of phonological proximity makes it possible to establish a basis for future decision-making in the treatment of sign languages. Knowing how close a set of signs are allows the interested party to decide more easily its study by clustering, as well as the teaching of the language to third parties based on similarities. In addition, it lays the foundation for strengthening disambiguation modules in automatic recognition systems. To the best of our knowledge, this is the first study of its kind for Costa Rican Sign Language (LESCO, for its Spanish acronym), and forms the basis for one of the modules of the already operational system of sign and speech editing called the International Platform for Sign Language Edition (PIELS). A database of 2665 signs, grouped into eight contexts, is used, and a comparison of similarity measures is made, using standard statistical formulas to measure their degree of correlation. This corpus will be especially useful in machine learning approaches. In this work, we have proposed an analysis of different similarity measures between signs in order to find out the phonological proximity between them. After analyzing the results obtained, we can conclude that LESCO is a sign language with high levels of phonological proximity, particularly in the orientation and location components, but they are noticeably lower in the form component. We have also concluded as an outstanding contribution of our research that automatic recognition systems can take as a basis for their first prototypes the contexts or sign domains that map to clusters with lower levels of similarity. As mentioned, the results obtained have multiple applications such as in the teaching area or the Natural Language Processing area for automatic recognition tasks.",
      "abstract": "The study of phonological proximity makes it possible to establish a basis for future decision-making in the treatment of sign languages. Knowing how close a set of signs are allows the interested party to decide more easily its study by clustering, as well as the teaching of the language to third parties based on similarities. In addition, it lays the foundation for strengthening disambiguation modules in automatic recognition systems. To the best of our knowledge, this is the first study of its kind for Costa Rican Sign Language (LESCO, for its Spanish acronym), and forms the basis for one of the modules of the already operational system of sign and speech editing called the International Platform for Sign Language Edition (PIELS). A database of 2665 signs, grouped into eight contexts, is used, and a comparison of similarity measures is made, using standard statistical formulas to measure their degree of correlation. This corpus will be especially useful in machine learning approaches. In this work, we have proposed an analysis of different similarity measures between signs in order to find out the phonological proximity between them. After analyzing the results obtained, we can conclude that LESCO is a sign language with high levels of phonological proximity, particularly in the orientation and location components, but they are noticeably lower in the form component. We have also concluded as an outstanding contribution of our research that automatic recognition systems can take as a basis for their first prototypes the contexts or sign domains that map to clusters with lower levels of similarity. As mentioned, the results obtained have multiple applications such as in the teaching area or the Natural Language Processing area for automatic recognition tasks.",
      "doi": "https://doi.org/10.3390/electronics9081302",
      "openalex_id": "https://openalex.org/W3048407964",
      "arxiv_id": "",
      "publication_date": "2020-08-13",
      "published": "2020-08-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cleaning up the Brickyard: How Theory and Methodology Shape Experiments in Cognitive Neuroscience of Language",
      "summary": "Abstract The capacity for language is a defining property of our species, yet despite decades of research, evidence on its neural basis is still mixed and a generalized consensus is difficult to achieve. We suggest that this is partly caused by researchers defining “language” in different ways, with focus on a wide range of phenomena, properties, and levels of investigation. Accordingly, there is very little agreement among cognitive neuroscientists of language on the operationalization of fundamental concepts to be investigated in neuroscientific experiments. Here, we review chains of derivation in the cognitive neuroscience of language, focusing on how the hypothesis under consideration is defined by a combination of theoretical and methodological assumptions. We first attempt to disentangle the complex relationship between linguistics, psychology, and neuroscience in the field. Next, we focus on how conclusions that can be drawn from any experiment are inherently constrained by auxiliary assumptions, both theoretical and methodological, on which the validity of conclusions drawn rests. These issues are discussed in the context of classical experimental manipulations as well as study designs that employ novel approaches such as naturalistic stimuli and computational modeling. We conclude by proposing that a highly interdisciplinary field such as the cognitive neuroscience of language requires researchers to form explicit statements concerning the theoretical definitions, methodological choices, and other constraining factors involved in their work.",
      "abstract": "Abstract The capacity for language is a defining property of our species, yet despite decades of research, evidence on its neural basis is still mixed and a generalized consensus is difficult to achieve. We suggest that this is partly caused by researchers defining “language” in different ways, with focus on a wide range of phenomena, properties, and levels of investigation. Accordingly, there is very little agreement among cognitive neuroscientists of language on the operationalization of fundamental concepts to be investigated in neuroscientific experiments. Here, we review chains of derivation in the cognitive neuroscience of language, focusing on how the hypothesis under consideration is defined by a combination of theoretical and methodological assumptions. We first attempt to disentangle the complex relationship between linguistics, psychology, and neuroscience in the field. Next, we focus on how conclusions that can be drawn from any experiment are inherently constrained by auxiliary assumptions, both theoretical and methodological, on which the validity of conclusions drawn rests. These issues are discussed in the context of classical experimental manipulations as well as study designs that employ novel approaches such as naturalistic stimuli and computational modeling. We conclude by proposing that a highly interdisciplinary field such as the cognitive neuroscience of language requires researchers to form explicit statements concerning the theoretical definitions, methodological choices, and other constraining factors involved in their work.",
      "doi": "https://doi.org/10.1162/jocn_a_02058",
      "openalex_id": "https://openalex.org/W4386763304",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Do parents modify child-directed signing to emphasize iconicity?",
      "summary": "Iconic signs are overrepresented in the vocabularies of young deaf children, but it is unclear why. It is possible that iconic signs are easier for children to learn, but it is also possible that adults use iconic signs in child-directed signing in ways that make them more learnable, either by using them more often than less iconic signs or by lengthening them. We analyzed videos of naturalistic play sessions between parents and deaf children ( n = 24 dyads) aged 9–60 months. To determine whether iconic signs are overrepresented during child-directed signing, we compared the iconicity of actual parent productions to the iconicity of simulated vocabularies designed to estimate chance levels of iconicity. For almost all dyads, parent sign types and tokens were not more iconic than the simulated vocabularies, suggesting that parents do not select more iconic signs during child-directed signing. To determine whether iconic signs are more likely to be lengthened, we ran a linear regression predicting sign duration, and found an interaction between age and iconicity: while parents of younger children produced non-iconic and iconic signs with similar durations, parents of older children produced non-iconic signs with shorter durations than iconic signs. Thus, parents sign more quickly with older children than younger children, and iconic signs appear to resist that reduction in sign length. It is possible that iconic signs are perceptually available longer, and their availability is a candidate hypothesis as to why iconic signs are overrepresented in children’s vocabularies.",
      "abstract": "Iconic signs are overrepresented in the vocabularies of young deaf children, but it is unclear why. It is possible that iconic signs are easier for children to learn, but it is also possible that adults use iconic signs in child-directed signing in ways that make them more learnable, either by using them more often than less iconic signs or by lengthening them. We analyzed videos of naturalistic play sessions between parents and deaf children ( n = 24 dyads) aged 9–60 months. To determine whether iconic signs are overrepresented during child-directed signing, we compared the iconicity of actual parent productions to the iconicity of simulated vocabularies designed to estimate chance levels of iconicity. For almost all dyads, parent sign types and tokens were not more iconic than the simulated vocabularies, suggesting that parents do not select more iconic signs during child-directed signing. To determine whether iconic signs are more likely to be lengthened, we ran a linear regression predicting sign duration, and found an interaction between age and iconicity: while parents of younger children produced non-iconic and iconic signs with similar durations, parents of older children produced non-iconic signs with shorter durations than iconic signs. Thus, parents sign more quickly with older children than younger children, and iconic signs appear to resist that reduction in sign length. It is possible that iconic signs are perceptually available longer, and their availability is a candidate hypothesis as to why iconic signs are overrepresented in children’s vocabularies.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.920729",
      "openalex_id": "https://openalex.org/W4293104800",
      "arxiv_id": "",
      "publication_date": "2022-08-25",
      "published": "2022-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Impact of face swapping and data augmentation on sign language recognition",
      "summary": "Abstract This study addresses the challenge of improving communication between the deaf and hearing community by exploring different sign language recognition (SLR) techniques. Due to privacy issues and the need for validation by interpreters, creating large-scale sign language (SL) datasets can be difficult. The authors address this by presenting a new Spanish isolated sign language recognition dataset, CALSE-1000, consisting of 5000 videos representing 1000 glosses, with various signers and scenarios. The study also proposes using different computer vision techniques, such as face swapping and affine transformations, to augment the SL dataset and improve the accuracy of the model I3D trained using them. The results show that the inclusion of these augmentations during training leads to an improvement in accuracy in top-1 metrics by up to 11.7 points, top-5 by up to 8.8 points and top-10 by up to 9 points. This has great potential to improve the state of the art in other datasets and other models. Furthermore, the analysis confirms the importance of facial expressions in the model by testing with a facial omission dataset and shows how face swapping can be used to include new anonymous signers without the costly and time-consuming process of recording.",
      "abstract": "Abstract This study addresses the challenge of improving communication between the deaf and hearing community by exploring different sign language recognition (SLR) techniques. Due to privacy issues and the need for validation by interpreters, creating large-scale sign language (SL) datasets can be difficult. The authors address this by presenting a new Spanish isolated sign language recognition dataset, CALSE-1000, consisting of 5000 videos representing 1000 glosses, with various signers and scenarios. The study also proposes using different computer vision techniques, such as face swapping and affine transformations, to augment the SL dataset and improve the accuracy of the model I3D trained using them. The results show that the inclusion of these augmentations during training leads to an improvement in accuracy in top-1 metrics by up to 11.7 points, top-5 by up to 8.8 points and top-10 by up to 9 points. This has great potential to improve the state of the art in other datasets and other models. Furthermore, the analysis confirms the importance of facial expressions in the model by testing with a facial omission dataset and shows how face swapping can be used to include new anonymous signers without the costly and time-consuming process of recording.",
      "doi": "https://doi.org/10.1007/s10209-024-01133-y",
      "openalex_id": "https://openalex.org/W4400965347",
      "arxiv_id": "",
      "publication_date": "2024-07-24",
      "published": "2024-07-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural effects differ for learning highly iconic versus non-iconic signs in hearing adults",
      "summary": "Abstract Little is known about the neural changes that accompany sign language learning by hearing adults. We used ERPs and a word-sign matching task to assess how learning impacted the N400 priming effect (reduced negativity for translations compared to unrelated trials). English monolinguals (N = 32) learned 100 ASL signs – half highly iconic (meaning was guessable), half non-iconic. In contrast to non-iconic signs, little learning was needed for the highly iconic signs as translation accuracy was similar pre- and post-learning. Prior to learning, an N400 priming effect was observed only for iconic signs. After learning, the size of the priming effect increased for non-iconic signs (replicating word learning studies) but decreased for iconic signs. For deaf ASL signers (N = 20), iconicity did not modulate the size of the N400 priming effect. We conclude that the impact of iconicity on lexico-semantic processing is reduced following learning, as signs are integrated into an emerging visual-manual lexicon.",
      "abstract": "Abstract Little is known about the neural changes that accompany sign language learning by hearing adults. We used ERPs and a word-sign matching task to assess how learning impacted the N400 priming effect (reduced negativity for translations compared to unrelated trials). English monolinguals (N = 32) learned 100 ASL signs – half highly iconic (meaning was guessable), half non-iconic. In contrast to non-iconic signs, little learning was needed for the highly iconic signs as translation accuracy was similar pre- and post-learning. Prior to learning, an N400 priming effect was observed only for iconic signs. After learning, the size of the priming effect increased for non-iconic signs (replicating word learning studies) but decreased for iconic signs. For deaf ASL signers (N = 20), iconicity did not modulate the size of the N400 priming effect. We conclude that the impact of iconicity on lexico-semantic processing is reduced following learning, as signs are integrated into an emerging visual-manual lexicon.",
      "doi": "https://doi.org/10.1017/s1366728923000809",
      "openalex_id": "https://openalex.org/W4388928887",
      "arxiv_id": "",
      "publication_date": "2023-11-23",
      "published": "2023-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Interactive learning of natural sign language with radar",
      "summary": "Abstract Over the past decade, there have been great advancements in radio frequency sensor technology for human–computer interaction applications, such as gesture recognition, and human activity recognition more broadly. While there is a significant amount of study on these topics, in most cases, experimental data are acquired in controlled settings by directing participants what motion to articulate. However, especially for communicative motions, such as sign language, such directed data sets do not accurately capture natural, in situ articulations. This results in a difference in the distribution of directed American Sign Language (ASL) versus natural ASL, which severely degrades natural sign language recognition in real‐world scenarios. To overcome these challenges and acquire more representative data for training deep models, the authors develop an interactive gaming environment, ChessSIGN, which records video and radar data of participants as they play the game without any external direction . The authors investigate various ways of generating synthetic samples from directed ASL data, but show that ultimately such data does not offer much improvement over just initialising using imagery from ImageNet. In contrast, an interactive learning paradigm is proposed by the authors in which model training is shown to improve as more and more natural ASL samples are acquired and augmented via synthetic samples generated from a physics‐aware generative adversarial network. The authors show that the proposed approach enables the recognition of natural ASL in a real‐world setting, achieving an accuracy of 69% for 29 ASL signs—a 60% improvement over conventional training with directed ASL data.",
      "abstract": "Abstract Over the past decade, there have been great advancements in radio frequency sensor technology for human–computer interaction applications, such as gesture recognition, and human activity recognition more broadly. While there is a significant amount of study on these topics, in most cases, experimental data are acquired in controlled settings by directing participants what motion to articulate. However, especially for communicative motions, such as sign language, such directed data sets do not accurately capture natural, in situ articulations. This results in a difference in the distribution of directed American Sign Language (ASL) versus natural ASL, which severely degrades natural sign language recognition in real‐world scenarios. To overcome these challenges and acquire more representative data for training deep models, the authors develop an interactive gaming environment, ChessSIGN, which records video and radar data of participants as they play the game without any external direction . The authors investigate various ways of generating synthetic samples from directed ASL data, but show that ultimately such data does not offer much improvement over just initialising using imagery from ImageNet. In contrast, an interactive learning paradigm is proposed by the authors in which model training is shown to improve as more and more natural ASL samples are acquired and augmented via synthetic samples generated from a physics‐aware generative adversarial network. The authors show that the proposed approach enables the recognition of natural ASL in a real‐world setting, achieving an accuracy of 69% for 29 ASL signs—a 60% improvement over conventional training with directed ASL data.",
      "doi": "https://doi.org/10.1049/rsn2.12565",
      "openalex_id": "https://openalex.org/W4396829806",
      "arxiv_id": "",
      "publication_date": "2024-05-10",
      "published": "2024-05-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Literacy Instruction for Students Who are Deaf and Hard of Hearing",
      "summary": "Abstract Responsive, high-quality literacy instruction is critical for deaf and hard-of-hearing (DHH) students as they may not be in an environment that provides full access to language and/or receive insufficient instruction. This second edition of Literacy Instruction for Students Who Are Deaf or Hard-of-Hearing updates previous findings and describes current, evidence-based practices in teaching literacy to DHH learners. In addition to application examples across chapters, a stand-alone appendix provides brief explanations of each strategy, the areas of literacy to which they relate, and references in support of each strategy. Beal, Dostal, and Easterbrooks provide educators and parents with a process for determining which literacy and language assessments are appropriate for individual DHH learners and whether an instructional practice is supported by evidence or causal factors. They describe the literacy process with an overview of related learning theories, language and literacy assessments, and evidence-based instructional strategies across the following domains of literacy: phonemic awareness, phonics, vocabulary, fluency, and comprehension. They also weave evidence-based writing strategies and case vignettes that highlight application of assessments and instructional approaches within each of these literacy areas. Finally, they review lingering questions related to literacy instruction for DHH learners. Educators and caregivers who provide literacy instruction to DHH learners will benefit from the breadth and depth of literacy content provided in this concise literacy textbook.",
      "abstract": "Abstract Responsive, high-quality literacy instruction is critical for deaf and hard-of-hearing (DHH) students as they may not be in an environment that provides full access to language and/or receive insufficient instruction. This second edition of Literacy Instruction for Students Who Are Deaf or Hard-of-Hearing updates previous findings and describes current, evidence-based practices in teaching literacy to DHH learners. In addition to application examples across chapters, a stand-alone appendix provides brief explanations of each strategy, the areas of literacy to which they relate, and references in support of each strategy. Beal, Dostal, and Easterbrooks provide educators and parents with a process for determining which literacy and language assessments are appropriate for individual DHH learners and whether an instructional practice is supported by evidence or causal factors. They describe the literacy process with an overview of related learning theories, language and literacy assessments, and evidence-based instructional strategies across the following domains of literacy: phonemic awareness, phonics, vocabulary, fluency, and comprehension. They also weave evidence-based writing strategies and case vignettes that highlight application of assessments and instructional approaches within each of these literacy areas. Finally, they review lingering questions related to literacy instruction for DHH learners. Educators and caregivers who provide literacy instruction to DHH learners will benefit from the breadth and depth of literacy content provided in this concise literacy textbook.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.001.0001",
      "openalex_id": "https://openalex.org/W4401771534",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pre-output Language Monitoring in Sign Production",
      "summary": "Abstract A domain-general monitoring mechanism is proposed to be involved in overt speech monitoring. This mechanism is reflected in a medial frontal component, the error negativity (Ne), present in both errors and correct trials (Ne-like wave) but larger in errors than correct trials. In overt speech production, this negativity starts to rise before speech onset and is therefore associated with inner speech monitoring. Here, we investigate whether the same monitoring mechanism is involved in sign language production. Twenty deaf signers (American Sign Language [ASL] dominant) and 16 hearing signers (English dominant) participated in a picture–word interference paradigm in ASL. As in previous studies, ASL naming latencies were measured using the keyboard release time. EEG results revealed a medial frontal negativity peaking within 15 msec after keyboard release in the deaf signers. This negativity was larger in errors than correct trials, as previously observed in spoken language production. No clear negativity was present in the hearing signers. In addition, the slope of the Ne was correlated with ASL proficiency (measured by the ASL Sentence Repetition Task) across signers. Our results indicate that a similar medial frontal mechanism is engaged in preoutput language monitoring in sign and spoken language production. These results suggest that the monitoring mechanism reflected by the Ne/Ne-like wave is independent of output modality (i.e., spoken or signed) and likely monitors prearticulatory representations of language. Differences between groups may be linked to several factors including differences in language proficiency or more variable lexical access to motor programming latencies for hearing than deaf signers.",
      "abstract": "Abstract A domain-general monitoring mechanism is proposed to be involved in overt speech monitoring. This mechanism is reflected in a medial frontal component, the error negativity (Ne), present in both errors and correct trials (Ne-like wave) but larger in errors than correct trials. In overt speech production, this negativity starts to rise before speech onset and is therefore associated with inner speech monitoring. Here, we investigate whether the same monitoring mechanism is involved in sign language production. Twenty deaf signers (American Sign Language [ASL] dominant) and 16 hearing signers (English dominant) participated in a picture–word interference paradigm in ASL. As in previous studies, ASL naming latencies were measured using the keyboard release time. EEG results revealed a medial frontal negativity peaking within 15 msec after keyboard release in the deaf signers. This negativity was larger in errors than correct trials, as previously observed in spoken language production. No clear negativity was present in the hearing signers. In addition, the slope of the Ne was correlated with ASL proficiency (measured by the ASL Sentence Repetition Task) across signers. Our results indicate that a similar medial frontal mechanism is engaged in preoutput language monitoring in sign and spoken language production. These results suggest that the monitoring mechanism reflected by the Ne/Ne-like wave is independent of output modality (i.e., spoken or signed) and likely monitors prearticulatory representations of language. Differences between groups may be linked to several factors including differences in language proficiency or more variable lexical access to motor programming latencies for hearing than deaf signers.",
      "doi": "https://doi.org/10.1162/jocn_a_01542",
      "openalex_id": "https://openalex.org/W3004786620",
      "arxiv_id": "",
      "publication_date": "2020-02-06",
      "published": "2020-02-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On The Dynamics of Lexical Access In Two or More Languages",
      "summary": "Abstract The most provocative finding about bilingualism in the last two decades is that both languages are active even when bilinguals intend to use one language alone. When bilinguals hear, read, or speak words in one language, form, or translation, relatives of those words in the other language become momentarily available. The way bilingual speakers negotiate the competition produced by cross-language interactions has profound consequences for language processing and for the cognitive and neural mechanisms that it engages. In this chapter, we review the most exciting of these new discoveries. We consider how the context of language immersion induces dynamic changes in lexical access, how the native language may change, how new learning is influenced by language experience, and how brain activity reflects these consequences. These findings expose the way that bilingualism reveals the relations between language and cognition in a manner that is impenetrable in speakers of one language alone.",
      "abstract": "Abstract The most provocative finding about bilingualism in the last two decades is that both languages are active even when bilinguals intend to use one language alone. When bilinguals hear, read, or speak words in one language, form, or translation, relatives of those words in the other language become momentarily available. The way bilingual speakers negotiate the competition produced by cross-language interactions has profound consequences for language processing and for the cognitive and neural mechanisms that it engages. In this chapter, we review the most exciting of these new discoveries. We consider how the context of language immersion induces dynamic changes in lexical access, how the native language may change, how new learning is influenced by language experience, and how brain activity reflects these consequences. These findings expose the way that bilingualism reveals the relations between language and cognition in a manner that is impenetrable in speakers of one language alone.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.26",
      "openalex_id": "https://openalex.org/W4213267748",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign language learning and assessment in German Switzerland: Exploring the potential of vocabulary size tests for Swiss German Sign Language",
      "summary": "In German Switzerland the learning and assessment of Swiss German Sign Language (Deutschschweizerische GebÃ¤rdensprache, DSGS) takes place in different contexts, for example, in tertiary education or in continuous education courses. By way of the still ongoing implementation of the Common European Framework of Reference for DSGS, different tests and assessment procedures are currently been developed and their potential are explored to support the learning and assessment of DSGS. Examples of this are two vocabulary size tests. The first is a web-delivered Yes/No Test, the second a Translation Test from written German to DSGS. For both tests, the same set of items was used. The items were sampled from DSGS teaching materials. For the development of the two vocabulary size tests, 20 DSGS adult learners of ages 24 to 55 (M = 39.3) were recruited as test takers. An item analysis of the test results yielded candidates for removal from the item set. Cronbach’s Alpha showed good results for both tests (&gt;.90), and inter-rater reliability of the translation test also indicated promising results (Cohen’s Kappa = .613, p &lt;.001). Evidence contributing to content validity was collected based on the sampling method of the test items. Due to the lack of a second DSGS vocabulary test that could be used to establish concurrent validity, external variables were identified and investigated as possible external criteria contributing to the performance of the test takers. One variable, number of courses attended, showed a significant correlation with the test results.",
      "abstract": "In German Switzerland the learning and assessment of Swiss German Sign Language (Deutschschweizerische GebÃ¤rdensprache, DSGS) takes place in different contexts, for example, in tertiary education or in continuous education courses. By way of the still ongoing implementation of the Common European Framework of Reference for DSGS, different tests and assessment procedures are currently been developed and their potential are explored to support the learning and assessment of DSGS. Examples of this are two vocabulary size tests. The first is a web-delivered Yes/No Test, the second a Translation Test from written German to DSGS. For both tests, the same set of items was used. The items were sampled from DSGS teaching materials. For the development of the two vocabulary size tests, 20 DSGS adult learners of ages 24 to 55 (M = 39.3) were recruited as test takers. An item analysis of the test results yielded candidates for removal from the item set. Cronbach’s Alpha showed good results for both tests (&gt;.90), and inter-rater reliability of the translation test also indicated promising results (Cohen’s Kappa = .613, p &lt;.001). Evidence contributing to content validity was collected based on the sampling method of the test items. Due to the lack of a second DSGS vocabulary test that could be used to establish concurrent validity, external variables were identified and investigated as possible external criteria contributing to the performance of the test takers. One variable, number of courses attended, showed a significant correlation with the test results.",
      "doi": "https://doi.org/10.29140/lea.v2n1.85",
      "openalex_id": "https://openalex.org/W2943831203",
      "arxiv_id": "",
      "publication_date": "2019-04-30",
      "published": "2019-04-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Manual and Spoken Cues in French Sign Language’s Lexical Access: Evidence From Mouthing in a Sign-Picture Priming Paradigm",
      "summary": "Although Sign Languages are gestural languages, the fact remains that some linguistic information can also be conveyed by spoken components as mouthing. Mouthing usually tend to reproduce the more relevant phonetic part of the equivalent spoken word matching with the manual sign. Therefore, one crucial issue in sign language is to understand whether mouthing is part of the signs themselves or not, and to which extent it contributes to the construction of signs meaning. Another question is to know whether mouthing patterns constitute a phonological or a semantic cue in the lexical sign entry. This study aimed to investigate the role of mouthing on the processing of lexical signs in French Sign Language (LSF), according the type of bilingualism (intramodal vs. bimodal). For this purpose, a behavioral sign-picture lexical decision experiment was designed. Intramodal signers (native deaf adults) and Bimodal signers (fluent hearing adults) have to decide as fast as possible whether a picture matched with the sign seen just before. Five experimental conditions in which the pair sign-mouthing were congruent or incongruent were created. Our results showed a strong interference effect when the sign-mouthing matching was incongruent, reflected by higher error rates and lengthened reaction times compared with the congruent condition. This finding suggests that both groups of signers use the available lexical information contained in mouthing during accessing the sign meaning. In addition, deaf intramodal signers were strongly interfered than hearing bimodal signers. Taken together, our data indicate that mouthing is a determining factor in LSF lexical access, specifically in deaf signers.",
      "abstract": "Although Sign Languages are gestural languages, the fact remains that some linguistic information can also be conveyed by spoken components as mouthing. Mouthing usually tend to reproduce the more relevant phonetic part of the equivalent spoken word matching with the manual sign. Therefore, one crucial issue in sign language is to understand whether mouthing is part of the signs themselves or not, and to which extent it contributes to the construction of signs meaning. Another question is to know whether mouthing patterns constitute a phonological or a semantic cue in the lexical sign entry. This study aimed to investigate the role of mouthing on the processing of lexical signs in French Sign Language (LSF), according the type of bilingualism (intramodal vs. bimodal). For this purpose, a behavioral sign-picture lexical decision experiment was designed. Intramodal signers (native deaf adults) and Bimodal signers (fluent hearing adults) have to decide as fast as possible whether a picture matched with the sign seen just before. Five experimental conditions in which the pair sign-mouthing were congruent or incongruent were created. Our results showed a strong interference effect when the sign-mouthing matching was incongruent, reflected by higher error rates and lengthened reaction times compared with the congruent condition. This finding suggests that both groups of signers use the available lexical information contained in mouthing during accessing the sign meaning. In addition, deaf intramodal signers were strongly interfered than hearing bimodal signers. Taken together, our data indicate that mouthing is a determining factor in LSF lexical access, specifically in deaf signers.",
      "doi": "https://doi.org/10.3389/fpsyg.2021.655168",
      "openalex_id": "https://openalex.org/W3164299976",
      "arxiv_id": "",
      "publication_date": "2021-05-25",
      "published": "2021-05-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Operationalization of Sign Language Phonological Similarity and its Effects on Lexical Access",
      "summary": "Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features-similar to the majority of extant definitions in the literature.",
      "abstract": "Cognitive mechanisms for sign language lexical access are fairly unknown. This study investigated whether phonological similarity facilitates lexical retrieval in sign languages using measures from a new lexical database for American Sign Language. Additionally, it aimed to determine which similarity metric best fits the present data in order to inform theories of how phonological similarity is constructed within the lexicon and to aid in the operationalization of phonological similarity in sign language. Sign repetition latencies and accuracy were obtained when native signers were asked to reproduce a sign displayed on a computer screen. Results indicated that, as predicted, phonological similarity facilitated repetition latencies and accuracy as long as there were no strict constraints on the type of sublexical features that overlapped. The data converged to suggest that one similarity measure, MaxD, defined as the overlap of any 4 sublexical features, likely best represents mechanisms of phonological similarity in the mental lexicon. Together, these data suggest that lexical access in sign language is facilitated by phonologically similar lexical representations in memory and the optimal operationalization is defined as liberal constraints on overlap of 4 out of 5 sublexical features-similar to the majority of extant definitions in the literature.",
      "doi": "https://doi.org/10.1093/deafed/enx014",
      "openalex_id": "https://openalex.org/W2617407762",
      "arxiv_id": "",
      "publication_date": "2017-04-12",
      "published": "2017-04-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical Recognition in Deaf Children Learning American Sign Language: Activation of Semantic and Phonological Features of Signs",
      "summary": "Abstract Children learning language efficiently process single words and activate semantic, phonological, and other features of words during recognition. We investigated lexical recognition in deaf children acquiring American Sign Language (ASL) to determine how perceiving language in the visual–spatial modality affects lexical recognition. Twenty native or early‐exposed signing deaf children (ages 4 to 8 years) participated in a visual world eye‐tracking study. Participants were presented with a single ASL sign, target picture, and three competitor pictures that varied in their phonological and semantic relationship to the target. Participants shifted gaze to the target picture shortly after sign offset. Participants showed robust evidence for activation of semantic but not phonological features of signs. However, in their behavioral responses, participants were most susceptible to phonological competitors. Results demonstrated that single word recognition in ASL is largely parallel to spoken language recognition among children who are developing a mature lexicon.",
      "abstract": "Abstract Children learning language efficiently process single words and activate semantic, phonological, and other features of words during recognition. We investigated lexical recognition in deaf children acquiring American Sign Language (ASL) to determine how perceiving language in the visual–spatial modality affects lexical recognition. Twenty native or early‐exposed signing deaf children (ages 4 to 8 years) participated in a visual world eye‐tracking study. Participants were presented with a single ASL sign, target picture, and three competitor pictures that varied in their phonological and semantic relationship to the target. Participants shifted gaze to the target picture shortly after sign offset. Participants showed robust evidence for activation of semantic but not phonological features of signs. However, in their behavioral responses, participants were most susceptible to phonological competitors. Results demonstrated that single word recognition in ASL is largely parallel to spoken language recognition among children who are developing a mature lexicon.",
      "doi": "https://doi.org/10.1111/lang.12409",
      "openalex_id": "https://openalex.org/W3033933301",
      "arxiv_id": "",
      "publication_date": "2020-06-03",
      "published": "2020-06-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Visual form of ASL verb signs predicts non-signer judgment of transitivity",
      "summary": "Longstanding cross-linguistic work on event representations in spoken languages have argued for a robust mapping between an event’s underlying representation and its syntactic encoding, such that–for example–the agent of an event is most frequently mapped to subject position. In the same vein, sign languages have long been claimed to construct signs that visually represent their meaning, i.e., signs that are iconic. Experimental research on linguistic parameters such as plurality and aspect has recently shown some of them to be visually universal in sign, i.e. recognized by non-signers as well as signers, and have identified specific visual cues that achieve this mapping. However, little is known about what makes action representations in sign language iconic, or whether and how the mapping of underlying event representations to syntactic encoding is visually apparent in the form of a verb sign. To this end, we asked what visual cues non-signers may use in evaluating transitivity (i.e., the number of entities involved in an action). To do this, we correlated non-signer judgments about transitivity of verb signs from American Sign Language (ASL) with phonological characteristics of these signs. We found that non-signers did not accurately guess the transitivity of the signs, but that non-signer transitivity judgments can nevertheless be predicted from the signs’ visual characteristics. Further, non-signers cue in on just those features that code event representations across sign languages, despite interpreting them differently. This suggests the existence of visual biases that underlie detection of linguistic categories, such as transitivity, which may uncouple from underlying conceptual representations over time in mature sign languages due to lexicalization processes.",
      "abstract": "Longstanding cross-linguistic work on event representations in spoken languages have argued for a robust mapping between an event’s underlying representation and its syntactic encoding, such that–for example–the agent of an event is most frequently mapped to subject position. In the same vein, sign languages have long been claimed to construct signs that visually represent their meaning, i.e., signs that are iconic. Experimental research on linguistic parameters such as plurality and aspect has recently shown some of them to be visually universal in sign, i.e. recognized by non-signers as well as signers, and have identified specific visual cues that achieve this mapping. However, little is known about what makes action representations in sign language iconic, or whether and how the mapping of underlying event representations to syntactic encoding is visually apparent in the form of a verb sign. To this end, we asked what visual cues non-signers may use in evaluating transitivity (i.e., the number of entities involved in an action). To do this, we correlated non-signer judgments about transitivity of verb signs from American Sign Language (ASL) with phonological characteristics of these signs. We found that non-signers did not accurately guess the transitivity of the signs, but that non-signer transitivity judgments can nevertheless be predicted from the signs’ visual characteristics. Further, non-signers cue in on just those features that code event representations across sign languages, despite interpreting them differently. This suggests the existence of visual biases that underlie detection of linguistic categories, such as transitivity, which may uncouple from underlying conceptual representations over time in mature sign languages due to lexicalization processes.",
      "doi": "https://doi.org/10.1371/journal.pone.0262098",
      "openalex_id": "https://openalex.org/W4214745554",
      "arxiv_id": "",
      "publication_date": "2022-02-25",
      "published": "2022-02-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Role of Modality in L2 Learning: The Importance of Learners Acquiring a Second Sign Language (M2L2 and M1L2 Learners)",
      "summary": "Abstract Second language acquisition (SLA) research offers valuable insight on how languages are learned and how they coexist and influence each other. Sign language learners offer unique perspectives on SLA, allowing researchers to test theories that are otherwise constrained by access to only one modality. Current literature on sign language learning focuses primarily on bimodal bilinguals, mostly hearing adults learning their first sign language (M2L2 learners). However, other groups of L2 signers exist, including deaf learners who have previously acquired a sign language and are learning a new one (M1L2 learners). M1L2 acquisition offers unique insights into complex interactions including multilingualism, modality, and timing of acquisition. We argue that M1L2 signers are a key comparison group for investigations of various L2 and so‐called modality effects and also represent a crucial test case for re‐examining the traditional constructs of “native speaker/signer” and the effects of initial language delay or deprivation on subsequent language acquisition.",
      "abstract": "Abstract Second language acquisition (SLA) research offers valuable insight on how languages are learned and how they coexist and influence each other. Sign language learners offer unique perspectives on SLA, allowing researchers to test theories that are otherwise constrained by access to only one modality. Current literature on sign language learning focuses primarily on bimodal bilinguals, mostly hearing adults learning their first sign language (M2L2 learners). However, other groups of L2 signers exist, including deaf learners who have previously acquired a sign language and are learning a new one (M1L2 learners). M1L2 acquisition offers unique insights into complex interactions including multilingualism, modality, and timing of acquisition. We argue that M1L2 signers are a key comparison group for investigations of various L2 and so‐called modality effects and also represent a crucial test case for re‐examining the traditional constructs of “native speaker/signer” and the effects of initial language delay or deprivation on subsequent language acquisition.",
      "doi": "https://doi.org/10.1111/lang.12607",
      "openalex_id": "https://openalex.org/W4387575129",
      "arxiv_id": "",
      "publication_date": "2023-10-10",
      "published": "2023-10-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "RF Micro-Doppler Classification with Multiple Spectrograms from Angular Subspace Projections",
      "summary": "Radio Frequency (RF) sensors present distinct ad-vantages over cameras or wearables for hand gesture recognition providing high resolution radial range and velocity measurement, being able to operate in dark and through the objects with high temporal and frequency resolutions. Moreover, the flexibility of the complex formatted data allows users to develop their own algorithms to generate various data representations such as time-frequency (Micro-Doppler - &#x03BC;D) maps, or range-Doppler or - angle as a function of time. However, conventional &#x03BC;-D generation does not regard the angular information of the multiple targets existing in the RF data. Hence, multiple targets with different &#x03BC;-D signatures at various angular positions create a mixed spec-trogram output reducing recognition performance. This paper proposes an angular projection approach on radar data cubes (RDCs) to generate raw radar data for defined angular subspaces. Hence multiple &#x03BC;-D spectrograms for each angular subspace can be constructed from the projected data. The proposed approach has been tested on RF data for gross body movement and American Sign Language (ASL) recognition. It has been showed that the utilization of angular projected spectrograms increases classification accuracy for ASL and achieves recognition accuracy of 92.6&#x0025; for 20 word ASL signs.",
      "abstract": "Radio Frequency (RF) sensors present distinct ad-vantages over cameras or wearables for hand gesture recognition providing high resolution radial range and velocity measurement, being able to operate in dark and through the objects with high temporal and frequency resolutions. Moreover, the flexibility of the complex formatted data allows users to develop their own algorithms to generate various data representations such as time-frequency (Micro-Doppler - &#x03BC;D) maps, or range-Doppler or - angle as a function of time. However, conventional &#x03BC;-D generation does not regard the angular information of the multiple targets existing in the RF data. Hence, multiple targets with different &#x03BC;-D signatures at various angular positions create a mixed spec-trogram output reducing recognition performance. This paper proposes an angular projection approach on radar data cubes (RDCs) to generate raw radar data for defined angular subspaces. Hence multiple &#x03BC;-D spectrograms for each angular subspace can be constructed from the projected data. The proposed approach has been tested on RF data for gross body movement and American Sign Language (ASL) recognition. It has been showed that the utilization of angular projected spectrograms increases classification accuracy for ASL and achieves recognition accuracy of 92.6&#x0025; for 20 word ASL signs.",
      "doi": "https://doi.org/10.1109/radarconf2248738.2022.9763904",
      "openalex_id": "https://openalex.org/W4229371428",
      "arxiv_id": "",
      "publication_date": "2022-03-21",
      "published": "2022-03-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Functional and structural brain asymmetries in sign language processing",
      "summary": "The capacity for language constitutes a cornerstone of human cognition and distinguishes our species from other animals. Research in the cognitive sciences has demonstrated that this capacity is not bound to speech but can also be externalized in the form of sign language. Sign languages are the naturally occurring languages of the deaf and rely on movements and configurations of hands, arms, face, and torso in space. This chapter reviews the functional and structural organization of the neural substrates of sign language, as identified by neuroimaging research over the past decades. Most aspects of sign language processing in adult deaf signers markedly mirror the well-known, functional left-lateralization of spoken and written language. However, both hemispheres exhibit a certain equipotentiality for processing linguistic information and the right hemisphere seems to specifically support processing of some constructions unique to the signed modality. Crucially, the so-called \"core language network\" in the left hemisphere constitutes a functional and structural asymmetry in typically developed deaf and hearing populations alike: This network is (i) pivotal for processing complex syntax independent of the modality of language use, (ii) matures in accordance with a genetically determined biologic matrix, and (iii) may have constituted an evolutionary prerequisite for the emergence of the human capacity for language.",
      "abstract": "The capacity for language constitutes a cornerstone of human cognition and distinguishes our species from other animals. Research in the cognitive sciences has demonstrated that this capacity is not bound to speech but can also be externalized in the form of sign language. Sign languages are the naturally occurring languages of the deaf and rely on movements and configurations of hands, arms, face, and torso in space. This chapter reviews the functional and structural organization of the neural substrates of sign language, as identified by neuroimaging research over the past decades. Most aspects of sign language processing in adult deaf signers markedly mirror the well-known, functional left-lateralization of spoken and written language. However, both hemispheres exhibit a certain equipotentiality for processing linguistic information and the right hemisphere seems to specifically support processing of some constructions unique to the signed modality. Crucially, the so-called \"core language network\" in the left hemisphere constitutes a functional and structural asymmetry in typically developed deaf and hearing populations alike: This network is (i) pivotal for processing complex syntax independent of the modality of language use, (ii) matures in accordance with a genetically determined biologic matrix, and (iii) may have constituted an evolutionary prerequisite for the emergence of the human capacity for language.",
      "doi": "https://doi.org/10.1016/b978-0-443-15646-5.00021-x",
      "openalex_id": "https://openalex.org/W4408282267",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Iconicity as an organizing principle of the lexicon",
      "summary": "The view that words are arbitrary is a foundational assumption about language, used to set human languages apart from nonhuman communication. We present here a study of the alignment between the semantic and phonological structure (systematicity) of American Sign Language (ASL), and for comparison, two spoken languages—English and Spanish. Across all three languages, words that are semantically related are more likely to be phonologically related, highlighting systematic alignment between word form and word meaning. Critically, there is a significant effect of iconicity (a perceived physical resemblance between word form and word meaning) on this alignment: words are most likely to be phonologically related when they are semantically related and iconic. This phenomenon is particularly widespread in ASL: half of the signs in the ASL lexicon are iconically related to other signs, i.e., there is a nonarbitrary relationship between form and meaning that is shared across signs. Taken together, the results reveal that iconicity can act as a driving force behind the alignment between the semantic and phonological structure of spoken and signed languages, but languages may differ in the extent that iconicity structures the lexicon. Theories of language must account for iconicity as a possible organizing principle of the lexicon.",
      "abstract": "The view that words are arbitrary is a foundational assumption about language, used to set human languages apart from nonhuman communication. We present here a study of the alignment between the semantic and phonological structure (systematicity) of American Sign Language (ASL), and for comparison, two spoken languages—English and Spanish. Across all three languages, words that are semantically related are more likely to be phonologically related, highlighting systematic alignment between word form and word meaning. Critically, there is a significant effect of iconicity (a perceived physical resemblance between word form and word meaning) on this alignment: words are most likely to be phonologically related when they are semantically related and iconic. This phenomenon is particularly widespread in ASL: half of the signs in the ASL lexicon are iconically related to other signs, i.e., there is a nonarbitrary relationship between form and meaning that is shared across signs. Taken together, the results reveal that iconicity can act as a driving force behind the alignment between the semantic and phonological structure of spoken and signed languages, but languages may differ in the extent that iconicity structures the lexicon. Theories of language must account for iconicity as a possible organizing principle of the lexicon.",
      "doi": "https://doi.org/10.1073/pnas.2401041122",
      "openalex_id": "https://openalex.org/W4409416063",
      "arxiv_id": "",
      "publication_date": "2025-04-14",
      "published": "2025-04-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The neural basis of word production",
      "summary": "Abstract Traditionally, psycholinguistic models and models of speech motor control have approached the word production process from different angles and have remained, to a large extent, separate from one another. Ultimately, however, the process entails the entire pathway from semantic processing to articulation: producing a word from meaning requires activating a concept, retrieving the word, selecting its segments, ordering those segments into the right sequence, and finally articulating them with the correct timing. This chapter reviews the neural basis of word production from the complementary perspectives of the psycholinguistic literature and the speech motor control literature.",
      "abstract": "Abstract Traditionally, psycholinguistic models and models of speech motor control have approached the word production process from different angles and have remained, to a large extent, separate from one another. Ultimately, however, the process entails the entire pathway from semantic processing to articulation: producing a word from meaning requires activating a concept, retrieving the word, selecting its segments, ordering those segments into the right sequence, and finally articulating them with the correct timing. This chapter reviews the neural basis of word production from the complementary perspectives of the psycholinguistic literature and the speech motor control literature.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.12",
      "openalex_id": "https://openalex.org/W4213326696",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AfricaSign -- A Crowd-sourcing Platform for the Documentation of STEM Vocabulary in African Sign Languages",
      "summary": "Research in sign languages, in general, is still a relatively new topic of study when compared to research into spoken languages. Most of African sign languages are endangered and severely under-studied [11]. In an attempt to (lexically) document as many endangered sign languages in Africa as possible, we have developed a low-barrier, online crowd-sourcing platform (AfricaSign) that enables the African deaf communities to document their sign languages. AfricaSign offers to users multiple input modes, accommodates regional variation in multiple sign languages and allows the use of avatar technology to describe signs. It is likely that this research will uncover typological features exhibited by African sign languages. Documentation of STEM vocabulary will also help facilitate access to education for the Deaf community.",
      "abstract": "Research in sign languages, in general, is still a relatively new topic of study when compared to research into spoken languages. Most of African sign languages are endangered and severely under-studied [11]. In an attempt to (lexically) document as many endangered sign languages in Africa as possible, we have developed a low-barrier, online crowd-sourcing platform (AfricaSign) that enables the African deaf communities to document their sign languages. AfricaSign offers to users multiple input modes, accommodates regional variation in multiple sign languages and allows the use of avatar technology to describe signs. It is likely that this research will uncover typological features exhibited by African sign languages. Documentation of STEM vocabulary will also help facilitate access to education for the Deaf community.",
      "doi": "https://doi.org/10.1145/3308561.3354592",
      "openalex_id": "https://openalex.org/W2981337119",
      "arxiv_id": "",
      "publication_date": "2019-10-24",
      "published": "2019-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonology Recognition in American Sign Language",
      "summary": "Inspired by recent developments in natural language processing, we propose a novel approach to sign language processing based on phonological properties validated by American Sign Language users. By taking advantage of datasets composed of phonological data and people speaking sign language, we use a pretrained deep model based on mesh reconstruction to extract the 3D coordinates of the signers keypoints. Then, we train standard statistical and deep machine learning models in order to assign phonological classes to each temporal sequence of coordinates. Our paper introduces the idea of exploiting the phonological properties manually assigned by sign language users to classify videos of people performing signs by regressing a 3D mesh. We establish a new baseline for this problem based on the statistical distribution of 725 different signs. Our best-performing models achieve a micro-averaged F1-score of 58% for the major location class and 70% for the sign type using statistical and deep learning algorithms, compared to their corresponding baselines of 35% and 39%.",
      "abstract": "Inspired by recent developments in natural language processing, we propose a novel approach to sign language processing based on phonological properties validated by American Sign Language users. By taking advantage of datasets composed of phonological data and people speaking sign language, we use a pretrained deep model based on mesh reconstruction to extract the 3D coordinates of the signers keypoints. Then, we train standard statistical and deep machine learning models in order to assign phonological classes to each temporal sequence of coordinates. Our paper introduces the idea of exploiting the phonological properties manually assigned by sign language users to classify videos of people performing signs by regressing a 3D mesh. We establish a new baseline for this problem based on the statistical distribution of 725 different signs. Our best-performing models achieve a micro-averaged F1-score of 58% for the major location class and 70% for the sign type using statistical and deep learning algorithms, compared to their corresponding baselines of 35% and 39%.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747212",
      "openalex_id": "https://openalex.org/W3204590994",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural substrates of sign language vocabulary processing in less-skilled hearing M2L2 signers: Evidence for difficult phonological movement perception",
      "summary": "No previous research has investigated the neural correlates of vocabulary acquisition in second language learners of sign language. The present study investigated whether poor vocabulary knowledge engaged similar prefrontal lexico-semantic regions as seen in unimodal L2 learners. Behavioral improvements in vocabulary knowledge in a cohort of M2L2 learners were quantified. Results indicated that there is significant increase in vocabulary knowledge after one semester, but stabilized in the second semester. A longitudinal fMRI analysis was implemented for a subset of learners who were followed for the entire 10 months during initial sign language acquisition. The results indicated that learners who had poor sign vocabulary knowledge consistently showed greater activation in regions involved in motor simulation, salience, biological motion and spatial processing, and lexico-semantic retrieval. In conclusion, poor vocabulary knowledge requires greater engagement of modality-independent and modality-dependent regions, which could account for behavioral evidence of difficulty in visual phonology processing.",
      "abstract": "No previous research has investigated the neural correlates of vocabulary acquisition in second language learners of sign language. The present study investigated whether poor vocabulary knowledge engaged similar prefrontal lexico-semantic regions as seen in unimodal L2 learners. Behavioral improvements in vocabulary knowledge in a cohort of M2L2 learners were quantified. Results indicated that there is significant increase in vocabulary knowledge after one semester, but stabilized in the second semester. A longitudinal fMRI analysis was implemented for a subset of learners who were followed for the entire 10 months during initial sign language acquisition. The results indicated that learners who had poor sign vocabulary knowledge consistently showed greater activation in regions involved in motor simulation, salience, biological motion and spatial processing, and lexico-semantic retrieval. In conclusion, poor vocabulary knowledge requires greater engagement of modality-independent and modality-dependent regions, which could account for behavioral evidence of difficulty in visual phonology processing.",
      "doi": "https://doi.org/10.1017/s1366728917000347",
      "openalex_id": "https://openalex.org/W2727952944",
      "arxiv_id": "",
      "publication_date": "2017-07-06",
      "published": "2017-07-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Earlier and more robust sensorimotor discrimination of ASL signs in deaf signers during imitation",
      "summary": "Prior research suggests that the amount of experience an individual has with an action influences the degree to which the sensorimotor systems of their brain are involved in the subsequent perception of those actions. Less is known about how action experience and semantic knowledge impact sensorimotor involvement during imitation. To address this question, we collected electroencephalograms (EEG) while deaf signers and hearing non-signers imitated one-handed and two-handed ASL signs. During observation, deaf signers showed early differentiation in alpha/beta power between the one- and two-handed sign conditions, whereas hearing non-signers showed this discrimination only later. During sign imitation, deaf signers showed desynchronisation of alpha/beta EEG signals, while hearing non-signers showed increased power. Thus, in an imitative context, deaf signers engage anticipatory motor preparation in advance of action production, while hearing non-signers engage slower, more memory-related processes to help them complete the complex task.",
      "abstract": "Prior research suggests that the amount of experience an individual has with an action influences the degree to which the sensorimotor systems of their brain are involved in the subsequent perception of those actions. Less is known about how action experience and semantic knowledge impact sensorimotor involvement during imitation. To address this question, we collected electroencephalograms (EEG) while deaf signers and hearing non-signers imitated one-handed and two-handed ASL signs. During observation, deaf signers showed early differentiation in alpha/beta power between the one- and two-handed sign conditions, whereas hearing non-signers showed this discrimination only later. During sign imitation, deaf signers showed desynchronisation of alpha/beta EEG signals, while hearing non-signers showed increased power. Thus, in an imitative context, deaf signers engage anticipatory motor preparation in advance of action production, while hearing non-signers engage slower, more memory-related processes to help them complete the complex task.",
      "doi": "https://doi.org/10.1080/23273798.2021.1925712",
      "openalex_id": "https://openalex.org/W3160446378",
      "arxiv_id": "",
      "publication_date": "2021-05-12",
      "published": "2021-05-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Attitudes toward signing human avatars vary depending on hearing status, age of signed language exposure, and avatar type",
      "summary": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer’s preferred or native language. As development of signing avatars has gained traction in recent years, many different methods of creating signing avatars have been developed, and the resulting avatars vary widely in their appearance, the naturalness of their movements, and their facial expressions--all of which may potentially impact users’ acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars, while also examining the extent to which people’s own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by 1) a live human signer (Human), 2) an avatar made using computer-synthesized animation (CS Avatar), and 3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf (N = 83), Hard-of-Hearing (N = 34), and Hearing (N= 67) groups. Participants rated the three signers on multiple dimensions which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of, and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but this association was not seen for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users’ ratings of signing avatars, and show that signed language users with earlier age of ASL exposure are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars be considerate of retaining the fluid movement qualities which are integral to signed languages.",
      "abstract": "The use of virtual humans (i.e., avatars) holds the potential for interactive, automated interaction in domains such as remote communication, customer service, or public announcements. For signed language users, signing avatars could potentially provide accessible content by sharing information in the signer’s preferred or native language. As development of signing avatars has gained traction in recent years, many different methods of creating signing avatars have been developed, and the resulting avatars vary widely in their appearance, the naturalness of their movements, and their facial expressions--all of which may potentially impact users’ acceptance of the avatars. We designed a study to test the effects of these intrinsic properties of different signing avatars, while also examining the extent to which people’s own language experiences change their responses to signing avatars. We created video stimuli showing individual signs produced by 1) a live human signer (Human), 2) an avatar made using computer-synthesized animation (CS Avatar), and 3) an avatar made using high-fidelity motion capture (Mocap avatar). We surveyed 191 American Sign Language users, including Deaf (N = 83), Hard-of-Hearing (N = 34), and Hearing (N= 67) groups. Participants rated the three signers on multiple dimensions which were then combined to form ratings of Attitudes, Impressions, Comprehension, and Naturalness. Analyses demonstrated that the Mocap avatar was rated significantly more positively than the CS avatar on all primary variables. Correlations revealed that signers who acquire sign language later in life are more accepting of, and likely to have positive impressions of signing avatars. Finally, those who learned ASL earlier were more likely to give lower, more negative ratings to the CS avatar, but this association was not seen for the Mocap avatar or the Human signer. Together, these findings suggest that movement quality and appearance significantly impact users’ ratings of signing avatars, and show that signed language users with earlier age of ASL exposure are the most sensitive to movement quality issues seen in computer-generated avatars. We suggest that future efforts to develop signing avatars be considerate of retaining the fluid movement qualities which are integral to signed languages.",
      "doi": "https://doi.org/10.31234/osf.io/g2wuc",
      "openalex_id": "https://openalex.org/W3174389016",
      "arxiv_id": "",
      "publication_date": "2021-06-25",
      "published": "2021-06-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Managing Semantic Norms for Cognitive Linguistics, Corpus Linguistics, and Lexicon Studies",
      "summary": "examples where researchers commonly do not use norms to elucidate some of the problems that may arise for norm-less semantics with respect to the reproducibility of these studies (section 2); followed by an overview of some common norm data sets (section 3); and, finally, a discussion of methodological challenges of norm-based research and how norm-based linguistics fits within contemporary efforts to facilitate reproducible research (section 4). Norm-less semantics: Two examples Corpus linguistics: Semantic prosodyIt is widely known that word meaning depends on context.Take, for example, the verb to cause, which, when seen in isolation appears to be a rather neutral term.However, when using a corpus to look at the contexts this term tends to occur in (e.g., via a concordancer), it becomes apparent that by and large, only bad things get caused (Stubbs 2001), which is exemplified by the concordance lines from the Corpus of Contemporary American English (COCA; Davies 2009) shown in table 42.1.The idea that words consistently occur in certain types of attitudinal or emotional contexts has been dubbed semantic prosody in the British tradition of corpus linguistics (Hunston 2007;Louw 1993;Stewart 2010;Whitsitt 2005), and the fact that language users can so greatly mischaracterize the \"connotation\" of words when they introspect on them in isolation is a major argument for using corpus methods when looking at word meaning.In this field, other headwords that have been studied with respect to semantic prosody include to set in (Sinclair 1991) and utterly (Louw 1993), both of which tend to occur in negative contexts.As another example, consider the plural form days, for which Louw (1993) claims",
      "abstract": "examples where researchers commonly do not use norms to elucidate some of the problems that may arise for norm-less semantics with respect to the reproducibility of these studies (section 2); followed by an overview of some common norm data sets (section 3); and, finally, a discussion of methodological challenges of norm-based research and how norm-based linguistics fits within contemporary efforts to facilitate reproducible research (section 4). Norm-less semantics: Two examples Corpus linguistics: Semantic prosodyIt is widely known that word meaning depends on context.Take, for example, the verb to cause, which, when seen in isolation appears to be a rather neutral term.However, when using a corpus to look at the contexts this term tends to occur in (e.g., via a concordancer), it becomes apparent that by and large, only bad things get caused (Stubbs 2001), which is exemplified by the concordance lines from the Corpus of Contemporary American English (COCA; Davies 2009) shown in table 42.1.The idea that words consistently occur in certain types of attitudinal or emotional contexts has been dubbed semantic prosody in the British tradition of corpus linguistics (Hunston 2007;Louw 1993;Stewart 2010;Whitsitt 2005), and the fact that language users can so greatly mischaracterize the \"connotation\" of words when they introspect on them in isolation is a major argument for using corpus methods when looking at word meaning.In this field, other headwords that have been studied with respect to semantic prosody include to set in (Sinclair 1991) and utterly (Louw 1993), both of which tend to occur in negative contexts.As another example, consider the plural form days, for which Louw (1993) claims",
      "doi": "https://doi.org/10.7551/mitpress/12200.003.0047",
      "openalex_id": "https://openalex.org/W4205719504",
      "arxiv_id": "",
      "publication_date": "2022-01-04",
      "published": "2022-01-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Development of a Technology-Assisted Assessment for Sign Language Learning",
      "summary": "The goal of a recently concluded project in Switzerland was to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Gebärdensprache, DSGS) that relies on automatic sign language recognition. The assessment system gives adult L2 learners of DSGS feedback on the correctness of the manual parameters of signing (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system includes automatic feedback for a subset of a DSGS vocabulary size production test consisting of approximately 100 lexical items at CEFR level A1. The paper at hand reports on the process of selecting the items for the test, compiling training data for the SLR system, and linguistically analyzing errors in the resulting video recordings.",
      "abstract": "The goal of a recently concluded project in Switzerland was to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Gebärdensprache, DSGS) that relies on automatic sign language recognition. The assessment system gives adult L2 learners of DSGS feedback on the correctness of the manual parameters of signing (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system includes automatic feedback for a subset of a DSGS vocabulary size production test consisting of approximately 100 lexical items at CEFR level A1. The paper at hand reports on the process of selecting the items for the test, compiling training data for the SLR system, and linguistically analyzing errors in the resulting video recordings.",
      "doi": "https://doi.org/10.3991/ijet.v17i06.26959",
      "openalex_id": "https://openalex.org/W4221060801",
      "arxiv_id": "",
      "publication_date": "2022-03-29",
      "published": "2022-03-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Identifying the Correlations Between the Semantics and the Phonology of American Sign Language and British Sign Language: A Vector Space Approach",
      "summary": "Over the history of research on sign languages, much scholarship has highlighted the pervasive presence of signs whose forms relate to their meaning in a non-arbitrary way. The presence of these forms suggests that sign language vocabularies are shaped, at least in part, by a pressure toward maintaining a link between form and meaning in wordforms. We use a vector space approach to test the ways this pressure might shape sign language vocabularies, examining how non-arbitrary forms are distributed within the lexicons of two unrelated sign languages. Vector space models situate the representations of words in a multi-dimensional space where the distance between words indexes their relatedness in meaning. Using phonological information from the vocabularies of American Sign Language (ASL) and British Sign Language (BSL), we tested whether increased similarity between the semantic representations of signs corresponds to increased phonological similarity. The results of the computational analysis showed a significant positive relationship between phonological form and semantic meaning for both sign languages, which was strongest when the sign language lexicons were organized into clusters of semantically related signs. The analysis also revealed variation in the strength of patterns across the form-meaning relationships seen between phonological parameters within each sign language, as well as between the two languages. This shows that while the connection between form and meaning is not entirely language specific, there are cross-linguistic differences in how these mappings are realized for signs in each language, suggesting that arbitrariness as well as cognitive or cultural influences may play a role in how these patterns are realized. The results of this analysis not only contribute to our understanding of the distribution of non-arbitrariness in sign language lexicons, but also demonstrate a new way that computational modeling can be harnessed in lexicon-wide investigations of sign languages.",
      "abstract": "Over the history of research on sign languages, much scholarship has highlighted the pervasive presence of signs whose forms relate to their meaning in a non-arbitrary way. The presence of these forms suggests that sign language vocabularies are shaped, at least in part, by a pressure toward maintaining a link between form and meaning in wordforms. We use a vector space approach to test the ways this pressure might shape sign language vocabularies, examining how non-arbitrary forms are distributed within the lexicons of two unrelated sign languages. Vector space models situate the representations of words in a multi-dimensional space where the distance between words indexes their relatedness in meaning. Using phonological information from the vocabularies of American Sign Language (ASL) and British Sign Language (BSL), we tested whether increased similarity between the semantic representations of signs corresponds to increased phonological similarity. The results of the computational analysis showed a significant positive relationship between phonological form and semantic meaning for both sign languages, which was strongest when the sign language lexicons were organized into clusters of semantically related signs. The analysis also revealed variation in the strength of patterns across the form-meaning relationships seen between phonological parameters within each sign language, as well as between the two languages. This shows that while the connection between form and meaning is not entirely language specific, there are cross-linguistic differences in how these mappings are realized for signs in each language, suggesting that arbitrariness as well as cognitive or cultural influences may play a role in how these patterns are realized. The results of this analysis not only contribute to our understanding of the distribution of non-arbitrariness in sign language lexicons, but also demonstrate a new way that computational modeling can be harnessed in lexicon-wide investigations of sign languages.",
      "doi": "https://doi.org/10.3389/fpsyg.2022.806471",
      "openalex_id": "https://openalex.org/W4221068510",
      "arxiv_id": "",
      "publication_date": "2022-03-16",
      "published": "2022-03-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Using transitional information in sign and gesture perception",
      "summary": "For sign languages, transitional movements of the hands are fully visible and may be used to predict upcoming linguistic input. We investigated whether and how deaf signers and hearing nonsigners use transitional information to detect a target item in a string of either pseudosigns or grooming gestures, as well as whether motor imagery ability was related to this skill. Transitional information between items was either intact (Normal videos), digitally altered such that the hands were selectively blurred (Blurred videos), or edited to only show the frame prior to the transition which was frozen for the entire transition period, removing all transitional information (Static videos). For both pseudosigns and gestures, signers and nonsigners had faster target detection times for Blurred than Static videos, indicating similar use of movement transition cues. For linguistic stimuli (pseudosigns), only signers made use of transitional handshape information, as evidenced by faster target detection times for Normal than Blurred videos. This result indicates that signers can use their linguistic knowledge to interpret transitional handshapes to predict the upcoming signal. Signers and nonsigners did not differ in motor imagery abilities, but only non-signers exhibited evidence of using motor imagery as a prediction strategy. Overall, these results suggest that signers use transitional movement and handshape cues to facilitate sign recognition.",
      "abstract": "For sign languages, transitional movements of the hands are fully visible and may be used to predict upcoming linguistic input. We investigated whether and how deaf signers and hearing nonsigners use transitional information to detect a target item in a string of either pseudosigns or grooming gestures, as well as whether motor imagery ability was related to this skill. Transitional information between items was either intact (Normal videos), digitally altered such that the hands were selectively blurred (Blurred videos), or edited to only show the frame prior to the transition which was frozen for the entire transition period, removing all transitional information (Static videos). For both pseudosigns and gestures, signers and nonsigners had faster target detection times for Blurred than Static videos, indicating similar use of movement transition cues. For linguistic stimuli (pseudosigns), only signers made use of transitional handshape information, as evidenced by faster target detection times for Normal than Blurred videos. This result indicates that signers can use their linguistic knowledge to interpret transitional handshapes to predict the upcoming signal. Signers and nonsigners did not differ in motor imagery abilities, but only non-signers exhibited evidence of using motor imagery as a prediction strategy. Overall, these results suggest that signers use transitional movement and handshape cues to facilitate sign recognition.",
      "doi": "https://doi.org/10.1016/j.actpsy.2023.103923",
      "openalex_id": "https://openalex.org/W4366714453",
      "arxiv_id": "",
      "publication_date": "2023-04-21",
      "published": "2023-04-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Visual Representation of Abstract Verbs: Merging Verb Classification with Iconicity in Sign Language",
      "summary": "Theories like the picture superiority effect state that the visual modality has substantial advantage over the other human senses. This makes visual information vital in the acquisition of knowledge, such as in the learning of a language. Words can be graphically represented to illustrate the meaning of a message and facilitate its understanding. This method, however, becomes a limitation in the case of abstract words, like accept, belong, integrate and agree, which have no visual referent. The current research turns to sign languages to explore the common semantic elements that link words to each other. Such visual languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of creating clusters of iconic meanings along with their respective graphic representation. By using sign language insight and VerbNet's organisation of verb predicates, this study presents a novel organisation of 506 English abstract verbs classified by visual shape. Graphic animation was used to visually represent the 20 classes of abstract verbs developed. To build confidence on the resulting product, which can be accessed on www.vroav.online, an online survey was created to achieve judgements on the visuals' representativeness. Considerable agreement between participants was found, suggesting a positive way forward for this work, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.",
      "abstract": "Theories like the picture superiority effect state that the visual modality has substantial advantage over the other human senses. This makes visual information vital in the acquisition of knowledge, such as in the learning of a language. Words can be graphically represented to illustrate the meaning of a message and facilitate its understanding. This method, however, becomes a limitation in the case of abstract words, like accept, belong, integrate and agree, which have no visual referent. The current research turns to sign languages to explore the common semantic elements that link words to each other. Such visual languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of creating clusters of iconic meanings along with their respective graphic representation. By using sign language insight and VerbNet's organisation of verb predicates, this study presents a novel organisation of 506 English abstract verbs classified by visual shape. Graphic animation was used to visually represent the 20 classes of abstract verbs developed. To build confidence on the resulting product, which can be accessed on www.vroav.online, an online survey was created to achieve judgements on the visuals' representativeness. Considerable agreement between participants was found, suggesting a positive way forward for this work, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.",
      "doi": "https://doi.org/10.1109/iccc.2019.00025",
      "openalex_id": "https://openalex.org/W2970711556",
      "arxiv_id": "",
      "publication_date": "2019-07-01",
      "published": "2019-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Infants’ Learning of Speech Sounds and Word-Forms",
      "summary": "Abstract How do infants start learning their native language? This chapter reviews the conventional understanding of this problem, illustrated by a review of the most important studies in this area, and suggests that this conventional understanding mischaracterizes the problem infants solve and the developmental process by which they solve it. Recent experimental and modeling work from several labs suggest new ways to think about the beginnings of language learning and the emergence of the lexicon.",
      "abstract": "Abstract How do infants start learning their native language? This chapter reviews the conventional understanding of this problem, illustrated by a review of the most important studies in this area, and suggests that this conventional understanding mischaracterizes the problem infants solve and the developmental process by which they solve it. Recent experimental and modeling work from several labs suggest new ways to think about the beginnings of language learning and the emergence of the lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.6",
      "openalex_id": "https://openalex.org/W4212928203",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Taboo in Sign Languages",
      "summary": "Taboo topics in deaf communities include the usual ones found in spoken languages, as well as ones particular to deaf experiences, both in how deaf people relate to hearing people and how deaf people interact with other deaf people. Attention to these topics can help linguists understand better the consequences of field method choices and lead them to adopt better ones. Taboo expressions in American Sign Language are innovative regarding the linguistic structures they play with. This creativity is evident across the grammar in non-taboo expressions, but seems to revel in profane ones. When it comes to the syntax, however, certain types of structures occur in taboo expressions that are all but absent elsewhere, showing grammatical possibilities that might have gone unnoticed without attention to taboo. Taboo expressions are innovative, as well, in how they respond to changing culture, where lexical items that are incoherent with community sensibilities are 'corrected'.",
      "abstract": "Taboo topics in deaf communities include the usual ones found in spoken languages, as well as ones particular to deaf experiences, both in how deaf people relate to hearing people and how deaf people interact with other deaf people. Attention to these topics can help linguists understand better the consequences of field method choices and lead them to adopt better ones. Taboo expressions in American Sign Language are innovative regarding the linguistic structures they play with. This creativity is evident across the grammar in non-taboo expressions, but seems to revel in profane ones. When it comes to the syntax, however, certain types of structures occur in taboo expressions that are all but absent elsewhere, showing grammatical possibilities that might have gone unnoticed without attention to taboo. Taboo expressions are innovative, as well, in how they respond to changing culture, where lexical items that are incoherent with community sensibilities are 'corrected'.",
      "doi": "https://doi.org/10.1017/9781009291972",
      "openalex_id": "https://openalex.org/W4387328060",
      "arxiv_id": "",
      "publication_date": "2023-10-05",
      "published": "2023-10-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Units of sub-sign meaning in NGT",
      "summary": "Abstract This paper provides an overview of all the meaningful sub-sign form units (form-meaning units; FMUs) in lexical signs in Sign Language of the Netherlands (NGT). We investigated the potential meaning of all form features that were previously established in analyses of NGT form by analyzing their distribution in lexical signs. The data set consisted of 500 NGT signs in the lexical database Global Signbank, and a set of 163 elicited newly-formed lexical signs. All features in these data sets appear to bear meaning (at least once). No completely arbitrary features were found, and some features appeared to be always associated to a specific meaning. This toolkit and the set of FMUs in NGT provides a possible basis for cross-linguistic study and for a more fine-grained approach in various research disciplines, for instance psycholinguistics and acquisition, and it may thus advance the theoretical and applied study of sign languages.",
      "abstract": "Abstract This paper provides an overview of all the meaningful sub-sign form units (form-meaning units; FMUs) in lexical signs in Sign Language of the Netherlands (NGT). We investigated the potential meaning of all form features that were previously established in analyses of NGT form by analyzing their distribution in lexical signs. The data set consisted of 500 NGT signs in the lexical database Global Signbank, and a set of 163 elicited newly-formed lexical signs. All features in these data sets appear to bear meaning (at least once). No completely arbitrary features were found, and some features appeared to be always associated to a specific meaning. This toolkit and the set of FMUs in NGT provides a possible basis for cross-linguistic study and for a more fine-grained approach in various research disciplines, for instance psycholinguistics and acquisition, and it may thus advance the theoretical and applied study of sign languages.",
      "doi": "https://doi.org/10.1075/sll.20009.van",
      "openalex_id": "https://openalex.org/W4387879108",
      "arxiv_id": "",
      "publication_date": "2023-10-23",
      "published": "2023-10-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Winter, Woodin &amp;amp; Perlman - Defining iconicity for the cognitive sciences",
      "summary": "Iconicity has become an increasingly hot topic in the cognitive sciences, but research is riddled with inconsistencies in the use of key terminology, including “iconicity” itself as well as other related terms. This chapter presents a precise definition of iconicity that reflects the wide range of research on the topic in cognitive science and linguistics, but also provides clarity moving into the future. We also clarify common terminological confusions with respect to related notions including sound symbolism, systematicity, and indexicality. Our discussion puts the spotlight on aspects of iconicity that are under-appreciated in cognitive science, especially its context-dependent nature, and the fact that iconicity is underpinned by multiple rather than monolithic cognitive mechanisms.",
      "abstract": "Iconicity has become an increasingly hot topic in the cognitive sciences, but research is riddled with inconsistencies in the use of key terminology, including “iconicity” itself as well as other related terms. This chapter presents a precise definition of iconicity that reflects the wide range of research on the topic in cognitive science and linguistics, but also provides clarity moving into the future. We also clarify common terminological confusions with respect to related notions including sound symbolism, systematicity, and indexicality. Our discussion puts the spotlight on aspects of iconicity that are under-appreciated in cognitive science, especially its context-dependent nature, and the fact that iconicity is underpinned by multiple rather than monolithic cognitive mechanisms.",
      "doi": "https://doi.org/10.31219/osf.io/5e3rc",
      "openalex_id": "https://openalex.org/W4388931422",
      "arxiv_id": "",
      "publication_date": "2023-11-22",
      "published": "2023-11-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Evidence of Zipfian distributions in three sign languages",
      "summary": "One striking commonality between languages is their Zipfian distributions: A power-law distribution of word frequency. This distribution is found across languages, speech genres, and within different parts of speech. The recurrence of such distributions is thought to reflect cognitive and/or communicative pressures and to facilitate language learning. However, research on Zipfian distributions has mostly been limited to spoken languages. In this study, we ask whether Zipfian distributions are also found across signed languages, as expected if they reflect a universal property of human language. We find that sign frequencies and ranks in three sign language corpora (BSL, DGS and NGT) show a Zipfian relationship, similar to that found in spoken languages. These findings highlight the commonalities between spoken and signed languages, add to our understanding of the use of signs, and show the prevalence of Zipfian distributions across language modalities, supporting the idea that they facilitate language learning and communication.",
      "abstract": "One striking commonality between languages is their Zipfian distributions: A power-law distribution of word frequency. This distribution is found across languages, speech genres, and within different parts of speech. The recurrence of such distributions is thought to reflect cognitive and/or communicative pressures and to facilitate language learning. However, research on Zipfian distributions has mostly been limited to spoken languages. In this study, we ask whether Zipfian distributions are also found across signed languages, as expected if they reflect a universal property of human language. We find that sign frequencies and ranks in three sign language corpora (BSL, DGS and NGT) show a Zipfian relationship, similar to that found in spoken languages. These findings highlight the commonalities between spoken and signed languages, add to our understanding of the use of signs, and show the prevalence of Zipfian distributions across language modalities, supporting the idea that they facilitate language learning and communication.",
      "doi": "https://doi.org/10.1075/gest.23014.kim",
      "openalex_id": "https://openalex.org/W4393435033",
      "arxiv_id": "",
      "publication_date": "2023-12-31",
      "published": "2023-12-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Repetition Preferences in Two-Handed Balanced Signs: Vestigial Locomotor Central Pattern Generators Shape Sign Language Phonetics and Phonology",
      "summary": "Language is produced by bodies that evolved to fulfill a variety of functions, most of them non-communicative. Vestigial influences of adaptation for quadrupedal locomotion are still affecting bimanual actions, and have consequences on manual communication systems such as sign languages of the deaf. We discuss how central pattern generators (CPGs), networks of nerve cells in the spinal cord that drive locomotion, influence bimanual actions with alternating movements to be produced with repeated motion. We demonstrate this influence with data from three unrelated sign languages, American Sign Language, British Sign Language, and Hong Kong Sign Language: in all three sign languages two-handed balanced signs produced with alternating movements have a tendency to be repeated, whereas other types of two-handed balanced signs show the opposite tendency for single movements. These tendencies cannot be fully explained by factors such as iconicity. We propose a motoric account for these results: as alternating bimanual movements are influenced by locomotor patterns, they favor repeated movements.",
      "abstract": "Language is produced by bodies that evolved to fulfill a variety of functions, most of them non-communicative. Vestigial influences of adaptation for quadrupedal locomotion are still affecting bimanual actions, and have consequences on manual communication systems such as sign languages of the deaf. We discuss how central pattern generators (CPGs), networks of nerve cells in the spinal cord that drive locomotion, influence bimanual actions with alternating movements to be produced with repeated motion. We demonstrate this influence with data from three unrelated sign languages, American Sign Language, British Sign Language, and Hong Kong Sign Language: in all three sign languages two-handed balanced signs produced with alternating movements have a tendency to be repeated, whereas other types of two-handed balanced signs show the opposite tendency for single movements. These tendencies cannot be fully explained by factors such as iconicity. We propose a motoric account for these results: as alternating bimanual movements are influenced by locomotor patterns, they favor repeated movements.",
      "doi": "https://doi.org/10.3389/fcomm.2020.612973",
      "openalex_id": "https://openalex.org/W3128753683",
      "arxiv_id": "",
      "publication_date": "2021-01-28",
      "published": "2021-01-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The distribution of handshapes in the established lexicon of Israeli Sign Language (ISL)",
      "summary": "Abstract Our study focuses on the perception of the iconicity of handshapes – one of the formational parameters of the sign in signed language. Seventy Hebrew speakers were asked to match handshapes to Hebrew translations of 45 signs (that varied in degree of iconicity), which are specified for one of the handshapes in Israeli Sign Language (ISL). The results show that participants reliably match handshapes to corresponding sign translations for highly iconic signs, but are less accurate for less iconic signs. This demonstrates that there is a notable degree of iconicity in the lexicon of ISL, which is recognizable even to non-signers. The ability of non-signers to detect handshape to form is explained by the fact that word meanings are understood by both deaf and hearing peoples via the mental elaboration of simple iconic sources in which handshape meanings are grounded. The results suggest that while language external iconic mapping could ease the learning of direct iconic forms, it has a more limited capacity to help hearing non-signers learn indirect and opaque forms. The full semiotic distribution of handshapes in the lexicon and their use in language remain difficult for hearing non-signers to understand and depends on more specific language and cultural knowledge.",
      "abstract": "Abstract Our study focuses on the perception of the iconicity of handshapes – one of the formational parameters of the sign in signed language. Seventy Hebrew speakers were asked to match handshapes to Hebrew translations of 45 signs (that varied in degree of iconicity), which are specified for one of the handshapes in Israeli Sign Language (ISL). The results show that participants reliably match handshapes to corresponding sign translations for highly iconic signs, but are less accurate for less iconic signs. This demonstrates that there is a notable degree of iconicity in the lexicon of ISL, which is recognizable even to non-signers. The ability of non-signers to detect handshape to form is explained by the fact that word meanings are understood by both deaf and hearing peoples via the mental elaboration of simple iconic sources in which handshape meanings are grounded. The results suggest that while language external iconic mapping could ease the learning of direct iconic forms, it has a more limited capacity to help hearing non-signers learn indirect and opaque forms. The full semiotic distribution of handshapes in the lexicon and their use in language remain difficult for hearing non-signers to understand and depends on more specific language and cultural knowledge.",
      "doi": "https://doi.org/10.1515/sem-2019-0049",
      "openalex_id": "https://openalex.org/W3190329100",
      "arxiv_id": "",
      "publication_date": "2021-08-11",
      "published": "2021-08-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Contributions of pragmatics to word learning and interpretation",
      "summary": "Abstract This chapter focuses on the role of pragmatic reasoning in language learning and interpretation. The first half of the chapter assesses the extent to which young children use pragmatic mechanisms of intention recognition to build a mental lexicon (i.e., to learn new words). The second half discusses the extent to which children use pragmatic inference to employ their mental lexicon in conversation (i.e., to interpret known words). The evidence reviewed points to rich and massive effects of pragmatic reasoning in both domains. The sophistication of children’s pragmatic system and its relation to the mature adult system are discussed throughout the chapter.",
      "abstract": "Abstract This chapter focuses on the role of pragmatic reasoning in language learning and interpretation. The first half of the chapter assesses the extent to which young children use pragmatic mechanisms of intention recognition to build a mental lexicon (i.e., to learn new words). The second half discusses the extent to which children use pragmatic inference to employ their mental lexicon in conversation (i.e., to interpret known words). The evidence reviewed points to rich and massive effects of pragmatic reasoning in both domains. The sophistication of children’s pragmatic system and its relation to the mature adult system are discussed throughout the chapter.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.21",
      "openalex_id": "https://openalex.org/W4212770460",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Effect of Iconicity on Weak Hand Drop in American Sign Language",
      "summary": "The research community generally accepts that signed and spoken languages contain both iconicity and arbitrariness. Iconicity's impact on statistical distributions of motivated forms throughout signed language lexicons is clear (e.g. Occhino, 2016). However, there has been little work to determine whether the iconic links between form and meaning are relevant only to a sign's initial formation, or if these links are stored as part of lexical representations. In the present study, 40 Deaf signers of American Sign Language were asked to rate two-handed signs in their citation form and in one-handed (reduced) forms. Twelve signs were highly iconic. For each of these highly iconic sign, a less iconic but phonologically similar sign of the same grammatical category was also chosen. Signs were presented in carrier sentences and in isolation. Participants preferred one-handed forms of the highly iconic signs over one-handed forms of their phonolgogically similar but less iconic counterparts. Thus, iconicity impacted the application of a synchronic phonological process. This finding suggests that lexical representations retain iconic form-meaning links and that these links are accessible to the phonological grammar.",
      "abstract": "The research community generally accepts that signed and spoken languages contain both iconicity and arbitrariness. Iconicity's impact on statistical distributions of motivated forms throughout signed language lexicons is clear (e.g. Occhino, 2016). However, there has been little work to determine whether the iconic links between form and meaning are relevant only to a sign's initial formation, or if these links are stored as part of lexical representations. In the present study, 40 Deaf signers of American Sign Language were asked to rate two-handed signs in their citation form and in one-handed (reduced) forms. Twelve signs were highly iconic. For each of these highly iconic sign, a less iconic but phonologically similar sign of the same grammatical category was also chosen. Signs were presented in carrier sentences and in isolation. Participants preferred one-handed forms of the highly iconic signs over one-handed forms of their phonolgogically similar but less iconic counterparts. Thus, iconicity impacted the application of a synchronic phonological process. This finding suggests that lexical representations retain iconic form-meaning links and that these links are accessible to the phonological grammar.",
      "doi": "https://doi.org/10.3765/amp.v9i0.5305",
      "openalex_id": "https://openalex.org/W4289914217",
      "arxiv_id": "",
      "publication_date": "2022-08-05",
      "published": "2022-08-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "W(h)ither the ASL corpus?",
      "summary": "Abstract In this chapter, we discuss logistic and ideological practices used by signed language corpus projects around the world to develop their corpora, starting with Johnston’s early call to develop signed language corpora in 2004. We then outline a brief history of existing American Sign Language (ASL) corpora or corpus-like collections, most of which are specialized and/or inaccessible. We unpack the lessons we have learned as we, North American linguists interested in signed language research and corpora, have sought to create a national-level corpus following the standards set by earlier projects. We conclude that our contexts are somewhat different, and describe projects that we think would be better suited for our ASL communities. In other words, though our dreams of having a single national-level ASL corpus have ‘withered’, we remain optimistic that corpus methods can be used to catalog and analyze the wealth of ASL signing data currently available.",
      "abstract": "Abstract In this chapter, we discuss logistic and ideological practices used by signed language corpus projects around the world to develop their corpora, starting with Johnston’s early call to develop signed language corpora in 2004. We then outline a brief history of existing American Sign Language (ASL) corpora or corpus-like collections, most of which are specialized and/or inaccessible. We unpack the lessons we have learned as we, North American linguists interested in signed language research and corpora, have sought to create a national-level corpus following the standards set by earlier projects. We conclude that our contexts are somewhat different, and describe projects that we think would be better suited for our ASL communities. In other words, though our dreams of having a single national-level ASL corpus have ‘withered’, we remain optimistic that corpus methods can be used to catalog and analyze the wealth of ASL signing data currently available.",
      "doi": "https://doi.org/10.1075/scl.108.11hoc",
      "openalex_id": "https://openalex.org/W4323661081",
      "arxiv_id": "",
      "publication_date": "2023-03-09",
      "published": "2023-03-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EdGCon: Auto-assigner of Iconicity Ratings Grounded by Lexical Properties to Aid in Generation of Technical Gestures",
      "summary": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
      "abstract": "Gestures that share similarities in their forms and are related in their meanings, should be easier for learners to recognize and incorporate into their existing lexicon. In that regard, to be more readily accepted as standard by the Deaf and Hard of Hearing community, technical gestures in American Sign Language (ASL) will optimally share similar forms with their lexical neighbors. We utilize a lexical database of ASL, ASL-LEX, to identify lexical relations within a set of technical gestures. We use automated identification for 3 unique sub-lexical properties in ASL- location, handshape and movement. EdGCon assigned an iconicity rating based on the lexical property similarities of the new gesture with an existing set of technical gestures and the relatedness of the meaning of the new technical word to that of the existing set of technical words. We collected 30 ad hoc crowdsourced technical gestures from different internet websites and tested them against 31 gestures from the DeafTEC technical corpus. We found that EdGCon was able to correctly auto-assign the iconicity ratings 80.76% of the time.",
      "doi": "https://doi.org/10.1145/3555776.3577623",
      "openalex_id": "https://openalex.org/W4379620202",
      "arxiv_id": "",
      "publication_date": "2023-03-27",
      "published": "2023-03-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Feeling signs: Motor encoding enhances sign language learning in hearing adults",
      "summary": "Abstract Manual production enhances learning and recall of signs by hearing second language learners; however, the mechanisms enabling this effect are unclear. We examined whether the motor encoding (somatosensory feedback) that occurs during sign production benefits learning and whether it interacts with sign iconicity, which also enhances learning. American Sign Language (ASL) signs varying in iconicity were learned either via production (repetition) with the eyes closed or via observation without production. Signs learned via production were recalled more accurately than signs learned via observation, indicating that motor encoding from manual production enriches the representations of signs. Moreover, the effect of motor encoding interacted with iconicity, suggesting that motor encoding may particularly enhance the recall of signs low in iconicity. Together, these results reveal the importance of somatosensory feedback as a key mechanism underlying the beneficial effect of production on sign learning, demonstrating that feeling one’s own signing promotes learning and recall of signs.",
      "abstract": "Abstract Manual production enhances learning and recall of signs by hearing second language learners; however, the mechanisms enabling this effect are unclear. We examined whether the motor encoding (somatosensory feedback) that occurs during sign production benefits learning and whether it interacts with sign iconicity, which also enhances learning. American Sign Language (ASL) signs varying in iconicity were learned either via production (repetition) with the eyes closed or via observation without production. Signs learned via production were recalled more accurately than signs learned via observation, indicating that motor encoding from manual production enriches the representations of signs. Moreover, the effect of motor encoding interacted with iconicity, suggesting that motor encoding may particularly enhance the recall of signs low in iconicity. Together, these results reveal the importance of somatosensory feedback as a key mechanism underlying the beneficial effect of production on sign learning, demonstrating that feeling one’s own signing promotes learning and recall of signs.",
      "doi": "https://doi.org/10.1017/s0272263124000196",
      "openalex_id": "https://openalex.org/W4396228599",
      "arxiv_id": "",
      "publication_date": "2024-04-29",
      "published": "2024-04-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Playing With Language in the Manual Modality: Which Motions Do Signers Gradiently Modify?",
      "summary": "Abstract Language is traditionally characterized as an arbitrary, symbolic system, made up of discrete, categorical forms. But iconicity and gradience are pervasive in communication. For example, in spoken languages, word forms can be “played with” in iconic gradient ways by varying vowel length, pitch, or speed (e.g., “It's been a loooooooong day”). However, little is known about this process in sign languages. Here, we (1) explore gradient modification in three dimensions of motion in American Sign Language (ASL), and (2) ask whether the three dimensions are equally likely to be modified. We asked deaf signers of ASL ( n = 11, mean age = 49.3) to describe an event manipulated along speed, direction, or path, and observed their use of gradient modification in lexical and depicting signs. We found that signers alter the forms of both types of signs to enhance meaning. However, the three motion dimensions were not modified equally in lexical signs, suggesting constraints on gradient modification. These constraints may be linguistic in nature, found only in signers. Alternatively, the constraints could reflect difficulties in using the hands to convey particular modifications and, if so, should be found in speakers as well as signers.",
      "abstract": "Abstract Language is traditionally characterized as an arbitrary, symbolic system, made up of discrete, categorical forms. But iconicity and gradience are pervasive in communication. For example, in spoken languages, word forms can be “played with” in iconic gradient ways by varying vowel length, pitch, or speed (e.g., “It's been a loooooooong day”). However, little is known about this process in sign languages. Here, we (1) explore gradient modification in three dimensions of motion in American Sign Language (ASL), and (2) ask whether the three dimensions are equally likely to be modified. We asked deaf signers of ASL ( n = 11, mean age = 49.3) to describe an event manipulated along speed, direction, or path, and observed their use of gradient modification in lexical and depicting signs. We found that signers alter the forms of both types of signs to enhance meaning. However, the three motion dimensions were not modified equally in lexical signs, suggesting constraints on gradient modification. These constraints may be linguistic in nature, found only in signers. Alternatively, the constraints could reflect difficulties in using the hands to convey particular modifications and, if so, should be found in speakers as well as signers.",
      "doi": "https://doi.org/10.1111/cogs.70051",
      "openalex_id": "https://openalex.org/W4409087116",
      "arxiv_id": "",
      "publication_date": "2025-04-01",
      "published": "2025-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Automatic Classification of Handshapes in Russian Sign Language",
      "summary": "Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as an inventory is necessary for generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, creating an inventory is a very time-consuming process, thus only a handful of sign languages have them. Therefore, in this work we firstly test an unsupervised approach with the aim to automatically generate a handshape inventory. The process includes hand detection, cropping, and clustering techniques, which we apply to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verify the data to be able to apply supervised learning to classify new data.",
      "abstract": "Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as an inventory is necessary for generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, creating an inventory is a very time-consuming process, thus only a handful of sign languages have them. Therefore, in this work we firstly test an unsupervised approach with the aim to automatically generate a handshape inventory. The process includes hand detection, cropping, and clustering techniques, which we apply to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verify the data to be able to apply supervised learning to classify new data.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3088439434",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "What the frequency list can teach us about Turkish sign language?",
      "summary": "Abstract Recent studies on linguistics, cognitive science and psychology have shown that describing lexical frequency characteristics can answer many critical questions on language acquisition, mental lexicon and language use. Given the importance of corpus-based methodology, this study reports the preliminary findings from the objective lexical frequency list in TİD based on 103.087 sign tokens. This paper shows that frequency occurrence has a very decisive role on the linguistics categories and language in use. With respect to the multi-functionality of pointing in signed languages, the top ranked ID-gloss occurrences are mostly shaped by the pronominal references. Moreover, when compared to previous studies in terms of lexical density and lexical diversity, TİD shares both similar and different statistical features observed in other signed languages.",
      "abstract": "Abstract Recent studies on linguistics, cognitive science and psychology have shown that describing lexical frequency characteristics can answer many critical questions on language acquisition, mental lexicon and language use. Given the importance of corpus-based methodology, this study reports the preliminary findings from the objective lexical frequency list in TİD based on 103.087 sign tokens. This paper shows that frequency occurrence has a very decisive role on the linguistics categories and language in use. With respect to the multi-functionality of pointing in signed languages, the top ranked ID-gloss occurrences are mostly shaped by the pronominal references. Moreover, when compared to previous studies in terms of lexical density and lexical diversity, TİD shares both similar and different statistical features observed in other signed languages.",
      "doi": "https://doi.org/10.1515/psicl-2021-0022",
      "openalex_id": "https://openalex.org/W4200357742",
      "arxiv_id": "",
      "publication_date": "2021-12-01",
      "published": "2021-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Pragmatics and the Lexicon",
      "summary": "Abstract The present chapter reviews how contextually driven inferences interact with the lexical encoding of meaning. A central question is whether (and to what extent) a given ingredient of meaning associated with the use of a particular expression in context should be seen as directly encoded in the lexicon, or whether (and to what extent) it is derived via general reasoning in context. The discussion focuses on three key phenomena that serve as case studies to illustrate the task of spelling out the division of labor between semantics and pragmatics: scalar implicatures, scalar adjectives, and presuppositions. While many details in the theoretical analysis of these phenomena are still up for debate, they all serve to illustrate that key choice points for proposals at the semantics-pragmatics interface concern the question of what type of information has to be included at the lexical level in order to ground pragmatic phenomena in semantics.",
      "abstract": "Abstract The present chapter reviews how contextually driven inferences interact with the lexical encoding of meaning. A central question is whether (and to what extent) a given ingredient of meaning associated with the use of a particular expression in context should be seen as directly encoded in the lexicon, or whether (and to what extent) it is derived via general reasoning in context. The discussion focuses on three key phenomena that serve as case studies to illustrate the task of spelling out the division of labor between semantics and pragmatics: scalar implicatures, scalar adjectives, and presuppositions. While many details in the theoretical analysis of these phenomena are still up for debate, they all serve to illustrate that key choice points for proposals at the semantics-pragmatics interface concern the question of what type of information has to be included at the lexical level in order to ground pragmatic phenomena in semantics.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.18",
      "openalex_id": "https://openalex.org/W4213186527",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Corrective Feedback to Second Language Learners of American Sign Language",
      "summary": "This study examined the corrective feedback Deaf teachers used to target handshape, movement, and place-of-articulation errors in introductory American Sign Language (ASL) classes for hearing students. Although feedback is underresearched in bimodal second language (M2-L2) pedagogy, there is some evidence that teacher practices may differ from those observed in spoken L2 classes, notably in the more frequent use of direct corrections. Willoughby et al.'s (2015) study of Auslan (Australian Sign Language) reports that the teachers' preference for this type of correction stemmed from beliefs about the challenges of learning signed language phonology. Spoken L2 research suggests that a reliance on this type of feedback may limit students' opportunities to learn from their errors, as the nontarget form is corrected for the student and is not often followed by student \"repair\" of the original error (Panova and Lyster 2002). As student response to teacher feedback was not examined in Willoughby et al.'s (2015) study, we do not know if M2-L2 students show similar behavior. The current study was designed to address this issue, examining both teacher feedback practices and student responses to feedback.",
      "abstract": "This study examined the corrective feedback Deaf teachers used to target handshape, movement, and place-of-articulation errors in introductory American Sign Language (ASL) classes for hearing students. Although feedback is underresearched in bimodal second language (M2-L2) pedagogy, there is some evidence that teacher practices may differ from those observed in spoken L2 classes, notably in the more frequent use of direct corrections. Willoughby et al.'s (2015) study of Auslan (Australian Sign Language) reports that the teachers' preference for this type of correction stemmed from beliefs about the challenges of learning signed language phonology. Spoken L2 research suggests that a reliance on this type of feedback may limit students' opportunities to learn from their errors, as the nontarget form is corrected for the student and is not often followed by student \"repair\" of the original error (Panova and Lyster 2002). As student response to teacher feedback was not examined in Willoughby et al.'s (2015) study, we do not know if M2-L2 students show similar behavior. The current study was designed to address this issue, examining both teacher feedback practices and student responses to feedback.",
      "doi": "https://doi.org/10.1353/sls.2022.0009",
      "openalex_id": "https://openalex.org/W3116954592",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Overlapping Semantic Representations of Sign and Speech in Novice Sign Language Learners",
      "summary": "The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.",
      "abstract": "The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.",
      "doi": "https://doi.org/10.31234/osf.io/2tjsg",
      "openalex_id": "https://openalex.org/W4281558366",
      "arxiv_id": "",
      "publication_date": "2022-05-25",
      "published": "2022-05-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Defining Nodes and Edges in Other Languages in Cognitive Network Science—Moving beyond Single-Layer Networks",
      "summary": "Cognitive network science has increased our understanding of how the mental lexicon is structured and how that structure at the micro-, meso-, and macro-levels influences language and cognitive processes. Most of the research using this approach has used single-layer networks of English words. We consider two fundamental concepts in network science—nodes and connections (or edges)—in the context of two lesser-studied languages (American Sign Language and Kaqchikel) to see if a single-layer network can model phonological similarities among words in each of those languages. The analyses of those single-layer networks revealed several differences in network architecture that may challenge the cognitive network approach. We discuss several directions for future research using different network architectures that could address these challenges and also increase our understanding of how language processing might vary across languages. Such work would also provide a common framework for research in the language sciences, despite the variation among human languages. The methodological and theoretical tools of network science may also make it easier to integrate research of various language processes, such as typical and delayed development, acquired disorders, and the interaction of phonological and semantic information. Finally, coupling the cognitive network science approach with investigations of languages other than English might further advance our understanding of cognitive processing in general.",
      "abstract": "Cognitive network science has increased our understanding of how the mental lexicon is structured and how that structure at the micro-, meso-, and macro-levels influences language and cognitive processes. Most of the research using this approach has used single-layer networks of English words. We consider two fundamental concepts in network science—nodes and connections (or edges)—in the context of two lesser-studied languages (American Sign Language and Kaqchikel) to see if a single-layer network can model phonological similarities among words in each of those languages. The analyses of those single-layer networks revealed several differences in network architecture that may challenge the cognitive network approach. We discuss several directions for future research using different network architectures that could address these challenges and also increase our understanding of how language processing might vary across languages. Such work would also provide a common framework for research in the language sciences, despite the variation among human languages. The methodological and theoretical tools of network science may also make it easier to integrate research of various language processes, such as typical and delayed development, acquired disorders, and the interaction of phonological and semantic information. Finally, coupling the cognitive network science approach with investigations of languages other than English might further advance our understanding of cognitive processing in general.",
      "doi": "https://doi.org/10.3390/info15070401",
      "openalex_id": "https://openalex.org/W4400583186",
      "arxiv_id": "",
      "publication_date": "2024-07-12",
      "published": "2024-07-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonological variation and lexical form",
      "summary": "Abstract It is well documented that words are produced differently by different individuals (interspeaker variation) and by a single individual on different occasions (intraspeaker variation). However, exactly how this variation is processed in the brain, and therefore how to situate it in psycholinguistic models of word production and recognition remains an area of active research and debate. This chapter ties the wealth of sociolinguistic knowledge on phonological variables to the study of the mental lexicon. First, the existing experimental evidence concerning the relationship between phonological variation and lexical access is laid out. Second, to better understand conflicting results and guide further research, this chapter includes an overview of relevant structural and representational properties that differentiate phonological variables. Finally, the advantages of incorporating phonological variation into models of the mental lexicon are discussed.",
      "abstract": "Abstract It is well documented that words are produced differently by different individuals (interspeaker variation) and by a single individual on different occasions (intraspeaker variation). However, exactly how this variation is processed in the brain, and therefore how to situate it in psycholinguistic models of word production and recognition remains an area of active research and debate. This chapter ties the wealth of sociolinguistic knowledge on phonological variables to the study of the mental lexicon. First, the existing experimental evidence concerning the relationship between phonological variation and lexical access is laid out. Second, to better understand conflicting results and guide further research, this chapter includes an overview of relevant structural and representational properties that differentiate phonological variables. Finally, the advantages of incorporating phonological variation into models of the mental lexicon are discussed.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.15",
      "openalex_id": "https://openalex.org/W4212855166",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disorders of Lexical Access And Production",
      "summary": "Abstract Disorders of lexical access are characterized by inconsistent lexical access such that individuals successfully comprehend or produce a word in some contexts but fail on other occasions. Therefore, the lexical representations are thought to be intact, but their retrieval or activation is impaired and/or competing representations are not effectively managed. Lexical access deficits are most well-studied in individuals with aphasia, though some degree of lexical access difficulty can occur in a wide variety of neurogenic and developmental disorders, as well as in typical aging. This chapter focuses on the intersections of language, cognitive control, and memory: (1) how inhibition of lexical competitors and selection among competitors may explain some lexical access deficit phenomena, and (2) learning and retrieval processes in lexical access deficits from both basic research and translational application perspectives.",
      "abstract": "Abstract Disorders of lexical access are characterized by inconsistent lexical access such that individuals successfully comprehend or produce a word in some contexts but fail on other occasions. Therefore, the lexical representations are thought to be intact, but their retrieval or activation is impaired and/or competing representations are not effectively managed. Lexical access deficits are most well-studied in individuals with aphasia, though some degree of lexical access difficulty can occur in a wide variety of neurogenic and developmental disorders, as well as in typical aging. This chapter focuses on the intersections of language, cognitive control, and memory: (1) how inhibition of lexical competitors and selection among competitors may explain some lexical access deficit phenomena, and (2) learning and retrieval processes in lexical access deficits from both basic research and translational application perspectives.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.28",
      "openalex_id": "https://openalex.org/W4212986620",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical representation and access in sign languages",
      "summary": "Abstract The focus of this chapter is on how signed words are processed in relation to spoken ones, and whether the profound differences in how the two word types are articulated and perceived affect these processes. The form of words in sign languages, signs, is characterized by an architecture identical to that of spoken words, namely coordinated patterning between two language levels, meaning and articulatory patterning. The lexical architecture of sign differentiates it from that of gesture, which lacks this coordinated, bi-level linguistic structure. The patterning of sign phonology arises from the fine motor patterns of the fingers with the larger ones of the arms, torso, and head. Children’s lexical acquisition reveals how their analytic skills in phonological and morphological acquisition are tied to their early control of the visual field for inter-personal communication. Adult sign access and processing are governed by lexical structure and significantly modulated by prior linguistic experience.",
      "abstract": "Abstract The focus of this chapter is on how signed words are processed in relation to spoken ones, and whether the profound differences in how the two word types are articulated and perceived affect these processes. The form of words in sign languages, signs, is characterized by an architecture identical to that of spoken words, namely coordinated patterning between two language levels, meaning and articulatory patterning. The lexical architecture of sign differentiates it from that of gesture, which lacks this coordinated, bi-level linguistic structure. The patterning of sign phonology arises from the fine motor patterns of the fingers with the larger ones of the arms, torso, and head. Children’s lexical acquisition reveals how their analytic skills in phonological and morphological acquisition are tied to their early control of the visual field for inter-personal communication. Adult sign access and processing are governed by lexical structure and significantly modulated by prior linguistic experience.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.27",
      "openalex_id": "https://openalex.org/W4213226374",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EEG analysis based on dynamic visual stimuli",
      "summary": "This paper reviews best practices for experimental design and analysis for sign language research using neurophysiological methods, such as electroencephalography (EEG) and other methods with high temporal resolution, as well as identifies methodological challenges in neurophysiological research on natural sign language processing. In particular, we outline the considerations for generating linguistically and physically well-controlled stimuli accounting for 1) the layering of manual and non-manual information at different timescales, 2) possible unknown linguistic and non-linguistic visual cues that can affect processing, 3) variability across linguistic stimuli, and 4) predictive processing. Two specific concerns with regard to the analysis and interpretation of observed event related potential (ERP) effects for dynamic stimuli are discussed in detail. First, we discuss the “trigger/effect assignment problem”, which describes the difficulty of determining the time point for calculating ERPs. This issue is related to the problem of determining the onset of a critical sign (i.e., stimulus onset time), and the lack of clarity as to how the border between lexical (sign) and transitional movement (motion trajectory between individual signs) should be defined. Second, we discuss possible differences in the dynamics within signing that might influence ERP patterns and should be controlled for when creating natural sign language material for ERP studies. In addition, we outline alternative approaches to EEG data analyses for natural signing stimuli, such as the timestamping of continuous EEG with trigger markers for each potentially relevant cue in dynamic stimuli. Throughout the discussion, we present empirical evidence for the need to account for dynamic, multi-channel, and multi-timescale visual signal that characterizes sign languages in order to ensure the ecological validity of neurophysiological research in sign languages.",
      "abstract": "This paper reviews best practices for experimental design and analysis for sign language research using neurophysiological methods, such as electroencephalography (EEG) and other methods with high temporal resolution, as well as identifies methodological challenges in neurophysiological research on natural sign language processing. In particular, we outline the considerations for generating linguistically and physically well-controlled stimuli accounting for 1) the layering of manual and non-manual information at different timescales, 2) possible unknown linguistic and non-linguistic visual cues that can affect processing, 3) variability across linguistic stimuli, and 4) predictive processing. Two specific concerns with regard to the analysis and interpretation of observed event related potential (ERP) effects for dynamic stimuli are discussed in detail. First, we discuss the “trigger/effect assignment problem”, which describes the difficulty of determining the time point for calculating ERPs. This issue is related to the problem of determining the onset of a critical sign (i.e., stimulus onset time), and the lack of clarity as to how the border between lexical (sign) and transitional movement (motion trajectory between individual signs) should be defined. Second, we discuss possible differences in the dynamics within signing that might influence ERP patterns and should be controlled for when creating natural sign language material for ERP studies. In addition, we outline alternative approaches to EEG data analyses for natural signing stimuli, such as the timestamping of continuous EEG with trigger markers for each potentially relevant cue in dynamic stimuli. Throughout the discussion, we present empirical evidence for the need to account for dynamic, multi-channel, and multi-timescale visual signal that characterizes sign languages in order to ensure the ecological validity of neurophysiological research in sign languages.",
      "doi": "https://doi.org/10.31299/hrri.58.si.13",
      "openalex_id": "https://openalex.org/W4304823005",
      "arxiv_id": "",
      "publication_date": "2022-10-12",
      "published": "2022-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recent works in Sign Language Recognition using deep learning approach - A Survey",
      "summary": "Across the globe, different countries have their own sign language used by hearing and speech-impaired people. The American, Arabic, British, Brazilian, and Indian Sign Languages have gained popularity and piqued researchers and engineers to develop solutions that will help in the task of Sign Language Recognition and Machine translation. Just like spoken languages, sign languages also have dialects, and hence multiple gestures can exist for one word. The task of Sign Language Recognition is complex and has been carried out for the last three decades. In this paper, we survey, compare, and summarize the proposed techniques of researchers in sign language and provide a proposed plan for our imminent work. An extensive survey of CNN networks, Transformer models, Sequence models, Keypoint-based methods, and Hardware-assisted methods applied on various sign language open datasets is presented in the paper which can form a basis of the literature survey for the research.",
      "abstract": "Across the globe, different countries have their own sign language used by hearing and speech-impaired people. The American, Arabic, British, Brazilian, and Indian Sign Languages have gained popularity and piqued researchers and engineers to develop solutions that will help in the task of Sign Language Recognition and Machine translation. Just like spoken languages, sign languages also have dialects, and hence multiple gestures can exist for one word. The task of Sign Language Recognition is complex and has been carried out for the last three decades. In this paper, we survey, compare, and summarize the proposed techniques of researchers in sign language and provide a proposed plan for our imminent work. An extensive survey of CNN networks, Transformer models, Sequence models, Keypoint-based methods, and Hardware-assisted methods applied on various sign language open datasets is presented in the paper which can form a basis of the literature survey for the research.",
      "doi": "https://doi.org/10.1109/ocit59427.2023.10430576",
      "openalex_id": "https://openalex.org/W4391936583",
      "arxiv_id": "",
      "publication_date": "2023-12-13",
      "published": "2023-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Gamification of RF Data Acquisition for Classification of Natural Human Gestures",
      "summary": "In recent years, there have been significant developments in radio frequency (RF) sensor technology used in human-computer interaction (HCI) applications, specifically in areas like gesture recognition and more broadly, human activity recognition. Although extensive research has been conducted on these subjects, most experiments involve controlled settings where participants are instructed on how to perform specific movements. However, when such experiments are conducted on sign language recognition they lack capturing dialectal and background-related diversities. In this work, we explore the differences in RF datasets acquired under controlled experimental settings and in free form environments where users were not constrained by the experimental instructions and limitations. We show that directed (i.e., controlled) data acquisition approaches result in over-optimistic performances which do not perform well on naturally acquired data samples in a real-world use case. We evaluate different approaches on generating synthetic samples from directed dataset, but show that such methods do not offer much benefit over collecting natural data. Therefore, we propose an interactive data acquisition paradigm through gamification. We show that the proposed approach enables the recognition of American Sign Language (ASL) in real-world settings by achieving 69% accuracy on 29 words.",
      "abstract": "In recent years, there have been significant developments in radio frequency (RF) sensor technology used in human-computer interaction (HCI) applications, specifically in areas like gesture recognition and more broadly, human activity recognition. Although extensive research has been conducted on these subjects, most experiments involve controlled settings where participants are instructed on how to perform specific movements. However, when such experiments are conducted on sign language recognition they lack capturing dialectal and background-related diversities. In this work, we explore the differences in RF datasets acquired under controlled experimental settings and in free form environments where users were not constrained by the experimental instructions and limitations. We show that directed (i.e., controlled) data acquisition approaches result in over-optimistic performances which do not perform well on naturally acquired data samples in a real-world use case. We evaluate different approaches on generating synthetic samples from directed dataset, but show that such methods do not offer much benefit over collecting natural data. Therefore, we propose an interactive data acquisition paradigm through gamification. We show that the proposed approach enables the recognition of American Sign Language (ASL) in real-world settings by achieving 69% accuracy on 29 words.",
      "doi": "https://doi.org/10.1109/radarconf2458775.2024.10548148",
      "openalex_id": "https://openalex.org/W4399621143",
      "arxiv_id": "",
      "publication_date": "2024-05-06",
      "published": "2024-05-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring the Benefits and Applications of Video-Span Selection and Search for Real-Time Support in Sign Language Video Comprehension among ASL Learners",
      "summary": "People learning American Sign Language (ASL) and practicing their comprehension skills will often encounter complex ASL videos that may contain unfamiliar signs. Existing dictionary tools require users to isolate a single unknown sign before initiating a search by selecting linguistic properties or performing the sign in front of a webcam. This process presents challenges in extracting and reproducing unfamiliar signs, disrupting the video-watching experience, and requiring learners to rely on external dictionaries. We explore a technology that allows users to select and view dictionary results for one or more unfamiliar signs while watching a video. We interviewed 14 ASL learners to understand their challenges in understanding ASL videos, strategies for dealing with unfamiliar vocabulary, and expectations for an in situ dictionary system. We then conducted an in-depth analysis with eight learners to examine their interactions with a Wizard-of-Oz prototype during a video comprehension task. Finally, we conducted a comparative study with six additional ASL learners to evaluate the speed, accuracy, and workload benefits of an embedded dictionary-search feature within a video player. Our tool outperformed a baseline in the form of an existing online dictionary across all three metrics. The integration of a search tool and span selection offered advantages for video comprehension. Our findings have implications for designers, computer vision researchers, and sign language educators.",
      "abstract": "People learning American Sign Language (ASL) and practicing their comprehension skills will often encounter complex ASL videos that may contain unfamiliar signs. Existing dictionary tools require users to isolate a single unknown sign before initiating a search by selecting linguistic properties or performing the sign in front of a webcam. This process presents challenges in extracting and reproducing unfamiliar signs, disrupting the video-watching experience, and requiring learners to rely on external dictionaries. We explore a technology that allows users to select and view dictionary results for one or more unfamiliar signs while watching a video. We interviewed 14 ASL learners to understand their challenges in understanding ASL videos, strategies for dealing with unfamiliar vocabulary, and expectations for an in situ dictionary system. We then conducted an in-depth analysis with eight learners to examine their interactions with a Wizard-of-Oz prototype during a video comprehension task. Finally, we conducted a comparative study with six additional ASL learners to evaluate the speed, accuracy, and workload benefits of an embedded dictionary-search feature within a video player. Our tool outperformed a baseline in the form of an existing online dictionary across all three metrics. The integration of a search tool and span selection offered advantages for video comprehension. Our findings have implications for designers, computer vision researchers, and sign language educators.",
      "doi": "https://doi.org/10.1145/3690647",
      "openalex_id": "https://openalex.org/W4402132261",
      "arxiv_id": "",
      "publication_date": "2024-09-02",
      "published": "2024-09-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Semantic processing of iconic signs is not automatic: Neural evidence from hearing non-signers",
      "summary": "Abstract Iconicity facilitates learning signs, but it is unknown whether recognition of meaning from the sign form occurs automatically. We recorded ERPs to highly iconic (transparent) and non-iconic ASL signs presented to one group who knew they would be taught signs (learners) and another group with no such expectations (non-learners). Participants watched sign videos and detected an occasional grooming gesture (no semantic processing required). Before sign onset, learners showed a greater frontal negativity compared to non-learners for both sign types, possibly due to greater motivation to attend to signs. During the N400 window, learners showed greater negativity to iconic than non-iconic signs, indicating more semantic processing for iconic signs. The non-learners showed a later and much weaker iconicity effect. The groups did not differ in task performance or in P3 amplitude. We conclude that comprehending the form-meaning mapping of highly iconic signs is not automatic and requires motivation and attention.",
      "abstract": "Abstract Iconicity facilitates learning signs, but it is unknown whether recognition of meaning from the sign form occurs automatically. We recorded ERPs to highly iconic (transparent) and non-iconic ASL signs presented to one group who knew they would be taught signs (learners) and another group with no such expectations (non-learners). Participants watched sign videos and detected an occasional grooming gesture (no semantic processing required). Before sign onset, learners showed a greater frontal negativity compared to non-learners for both sign types, possibly due to greater motivation to attend to signs. During the N400 window, learners showed greater negativity to iconic than non-iconic signs, indicating more semantic processing for iconic signs. The non-learners showed a later and much weaker iconicity effect. The groups did not differ in task performance or in P3 amplitude. We conclude that comprehending the form-meaning mapping of highly iconic signs is not automatic and requires motivation and attention.",
      "doi": "https://doi.org/10.1017/s1366728924001093",
      "openalex_id": "https://openalex.org/W4407294001",
      "arxiv_id": "",
      "publication_date": "2025-02-10",
      "published": "2025-02-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Relationship Between Community Size and Iconicity in Sign Languages",
      "summary": "Abstract Communication is harder in larger communities. Past research shows that this leads larger communities to create languages that are easier to learn and use. In particular, previous research suggests that spoken languages that are used by larger communities are more sound symbolic than spoken languages used by smaller communities, presumably, because sound symbolism facilitates language acquisition and use. This study tests whether the same principle extends to sign languages as the role of iconicity in the acquisition and use of sign languages is debated. Furthermore, sign languages are more iconic than spoken languages and are argued to lose their iconicity over time. Therefore, they might not show the same pattern. The paper also tests whether iconicity depends on semantic domain. Participants from five different countries guessed the meaning and rated the iconicity of signs from 11 different sign languages: five languages with &gt;500,000 signers and six languages with &lt;3000 signers. Half of the signs referred to social concepts (e.g., friend, shame) and half referred to nonsocial concepts (e.g., garlic, morning). Nonsocial signs from large sign languages were rated as more iconic than nonsocial signs from small sign languages with no difference between the languages for social signs. Results also suggest that rated iconicity and guessing accuracy are more aligned in signs from large sign languages, potentially because smaller sign languages are more likely to rely on culture‐specific iconicity that is not as easily guessed outside of context. Together, this study shows how community size can influence lexical form and how the effect of such social pressures might depend on semantic domain.",
      "abstract": "Abstract Communication is harder in larger communities. Past research shows that this leads larger communities to create languages that are easier to learn and use. In particular, previous research suggests that spoken languages that are used by larger communities are more sound symbolic than spoken languages used by smaller communities, presumably, because sound symbolism facilitates language acquisition and use. This study tests whether the same principle extends to sign languages as the role of iconicity in the acquisition and use of sign languages is debated. Furthermore, sign languages are more iconic than spoken languages and are argued to lose their iconicity over time. Therefore, they might not show the same pattern. The paper also tests whether iconicity depends on semantic domain. Participants from five different countries guessed the meaning and rated the iconicity of signs from 11 different sign languages: five languages with &gt;500,000 signers and six languages with &lt;3000 signers. Half of the signs referred to social concepts (e.g., friend, shame) and half referred to nonsocial concepts (e.g., garlic, morning). Nonsocial signs from large sign languages were rated as more iconic than nonsocial signs from small sign languages with no difference between the languages for social signs. Results also suggest that rated iconicity and guessing accuracy are more aligned in signs from large sign languages, potentially because smaller sign languages are more likely to rely on culture‐specific iconicity that is not as easily guessed outside of context. Together, this study shows how community size can influence lexical form and how the effect of such social pressures might depend on semantic domain.",
      "doi": "https://doi.org/10.1111/cogs.70074",
      "openalex_id": "https://openalex.org/W4411134337",
      "arxiv_id": "",
      "publication_date": "2025-06-01",
      "published": "2025-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The relationship between verbal form and event structure in sign languages",
      "summary": "Whether predicates describe events as inherently bounded (telic) or unbounded (atelic) is usually understood to be an emergent property that depends on several factors; few, if any, spoken languages have dedicated morphology to mark the distinction. It is thus surprising that sign languages have been proposed to have dedicated morphology for telicity, and moreover that it takes a form which iconically reflects the underlying event structure – this is known as the “Event Visibility Hypothesis” (EVH) (Wilbur 2008). The EVH has been extended with claims about its universality in sign languages (Wilbur 2008; Malaia &amp;amp; Wilbur 2012), its gradient nature (Kuhn 2017), and its iconic transparency (Strickland et al. 2015). However, in this paper we argue that the status of this relationship between form and meaning remains an open question due to (a) lack of independent tests for telicity, (b) lack of lexical coverage, (c) lack of demonstration that formal expressions of telicity are morphological in nature, rather than a lexical property, and (d) inability to sufficiently dissociate telicity and perfectivity. We present new data coming from verbs that alternate in both form and meaning in ASL that is in line with the EVH, and conclude that while there is evidence supporting a morphological marker, the proposed form and telicity are not isomorphic in their distribution, significantly limiting the “visibility” of the event structure. We further propose that much of the related iconicity is the result of several independent factors also found in spoken languages, so that sign languages may be more similar to spoken languages than typically implied in this domain.",
      "abstract": "Whether predicates describe events as inherently bounded (telic) or unbounded (atelic) is usually understood to be an emergent property that depends on several factors; few, if any, spoken languages have dedicated morphology to mark the distinction. It is thus surprising that sign languages have been proposed to have dedicated morphology for telicity, and moreover that it takes a form which iconically reflects the underlying event structure – this is known as the “Event Visibility Hypothesis” (EVH) (Wilbur 2008). The EVH has been extended with claims about its universality in sign languages (Wilbur 2008; Malaia &amp;amp; Wilbur 2012), its gradient nature (Kuhn 2017), and its iconic transparency (Strickland et al. 2015). However, in this paper we argue that the status of this relationship between form and meaning remains an open question due to (a) lack of independent tests for telicity, (b) lack of lexical coverage, (c) lack of demonstration that formal expressions of telicity are morphological in nature, rather than a lexical property, and (d) inability to sufficiently dissociate telicity and perfectivity. We present new data coming from verbs that alternate in both form and meaning in ASL that is in line with the EVH, and conclude that while there is evidence supporting a morphological marker, the proposed form and telicity are not isomorphic in their distribution, significantly limiting the “visibility” of the event structure. We further propose that much of the related iconicity is the result of several independent factors also found in spoken languages, so that sign languages may be more similar to spoken languages than typically implied in this domain.",
      "doi": "https://doi.org/10.5334/gjgl.924",
      "openalex_id": "https://openalex.org/W2989221848",
      "arxiv_id": "",
      "publication_date": "2019-11-11",
      "published": "2019-11-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Construals of iconicity: experimental approaches to form-meaning resemblances in language",
      "summary": "While speculations on form-meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well-defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well-equipped to contribute to a comprehensive science of language.",
      "abstract": "While speculations on form-meaning resemblances in language go back millennia, the experimental study of iconicity is only about a century old. Here we take stock of experimental work on iconicity and present a double special issue with a diverse set of new contributions. We contextualise the work by introducing a typology of approaches to iconicity in language. Some approaches construe iconicity as a discrete property that is either present or absent; others treat it as involving semiotic relationships that come in kinds; and yet others see it as a gradient substance that comes in degrees. We show the benefits and limitations that come with each of these construals and stress the importance of developing accounts that can fluently switch between them. With operationalisations of iconicity that are well-defined yet flexible enough to deal with differences in tasks, modalities, and levels of analysis, experimental research on iconicity is well-equipped to contribute to a comprehensive science of language.",
      "doi": "https://doi.org/10.31234/osf.io/9qb6a",
      "openalex_id": "https://openalex.org/W4254548727",
      "arxiv_id": "",
      "publication_date": "2019-12-03",
      "published": "2019-12-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The effect of bilingualism on lexical learning and memory across two language modalities: some evidence for a domain-specific, but not general, advantage",
      "summary": "The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material – no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults.",
      "abstract": "The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material – no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults.",
      "doi": "https://doi.org/10.1080/20445911.2019.1634080",
      "openalex_id": "https://openalex.org/W2953716057",
      "arxiv_id": "",
      "publication_date": "2019-06-24",
      "published": "2019-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Children’s Use of Syntax In Word Learning",
      "summary": "Abstract This chapter investigates the role that syntax plays in guiding the acquisition of word meaning. It reviews data that reveal how children can use the syntactic distribution of a word as evidence for its meaning and discusses the principles of grammar that license such inferences. We delineate the role of thematic linking generalizations in the acquisition of action verbs, arguing that children use specific links between subject and agent and between object and patient to guide initial verb learning. In the domain of attitude verbs, we show that children’s knowledge of abstract links between subclasses of attitude verbs and their syntactic distribution enable learners to identify the meanings of their initial attitude verbs, such as think and want. Finally, we show that syntactic bootstrapping effects are not limited to verb learning but extend across the lexicon.",
      "abstract": "Abstract This chapter investigates the role that syntax plays in guiding the acquisition of word meaning. It reviews data that reveal how children can use the syntactic distribution of a word as evidence for its meaning and discusses the principles of grammar that license such inferences. We delineate the role of thematic linking generalizations in the acquisition of action verbs, arguing that children use specific links between subject and agent and between object and patient to guide initial verb learning. In the domain of attitude verbs, we show that children’s knowledge of abstract links between subclasses of attitude verbs and their syntactic distribution enable learners to identify the meanings of their initial attitude verbs, such as think and want. Finally, we show that syntactic bootstrapping effects are not limited to verb learning but extend across the lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.10",
      "openalex_id": "https://openalex.org/W4212781277",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Real-time lexical comprehension in young children learning American Sign Language",
      "summary": "When children interpret spoken language in real time, linguistic information drives rapidshifts in visual attention to objects in the visual world. This language-vision interaction canprovide insights into children's developing efficiency in language comprehension. But how doeslanguage influence visual attention when the linguistic signal and the visual world are bothprocessed via the visual channel? Here, we measured eye movements during real-timecomprehension of a visual-manual language, American Sign Language (ASL), by 29 nativeASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. Allsigners showed evidence of rapid, incremental language comprehension, tending to initiate aneye movement before sign offset. Deaf and hearing ASL-learners showed similar gaze patterns,suggesting that the in-the-moment dynamics of eye movements during ASL processing areshaped by the constraints of processing a visual language in real time and not by differentialaccess to auditory information in day-to-day life. Finally, variation in children’s ASL processingwas positively correlated with age and vocabulary size. Thus, despite competition for attentionwithin a single modality, the timing and accuracy of visual fixations during ASL comprehensionreflect information processing skills that are fundamental for language acquisition regardless oflanguage modality.",
      "abstract": "When children interpret spoken language in real time, linguistic information drives rapidshifts in visual attention to objects in the visual world. This language-vision interaction canprovide insights into children's developing efficiency in language comprehension. But how doeslanguage influence visual attention when the linguistic signal and the visual world are bothprocessed via the visual channel? Here, we measured eye movements during real-timecomprehension of a visual-manual language, American Sign Language (ASL), by 29 nativeASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. Allsigners showed evidence of rapid, incremental language comprehension, tending to initiate aneye movement before sign offset. Deaf and hearing ASL-learners showed similar gaze patterns,suggesting that the in-the-moment dynamics of eye movements during ASL processing areshaped by the constraints of processing a visual language in real time and not by differentialaccess to auditory information in day-to-day life. Finally, variation in children’s ASL processingwas positively correlated with age and vocabulary size. Thus, despite competition for attentionwithin a single modality, the timing and accuracy of visual fixations during ASL comprehensionreflect information processing skills that are fundamental for language acquisition regardless oflanguage modality.",
      "doi": "https://doi.org/10.31234/osf.io/zht6g",
      "openalex_id": "https://openalex.org/W4240668972",
      "arxiv_id": "",
      "publication_date": "2017-10-12",
      "published": "2017-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perception and production of language in the visual modality",
      "summary": "Abstract Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "abstract": "Abstract Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "doi": "https://doi.org/10.1075/tilar.25.08dye",
      "openalex_id": "https://openalex.org/W4245659432",
      "arxiv_id": "",
      "publication_date": "2020-01-30",
      "published": "2020-01-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical processing in sign language comprehension and production – experimental perspectives",
      "summary": "The study of sign language has received increasing interest in the last decades. Within this growing field, research on sign language processing – including both comprehension and production – has also received a remarkable boost in recent years. At initial stages of research, efforts were concentrated on demonstrating universal aspects of language processing; thus, little attention was paid to the differences between modalities or to the specific aspects of the sign-modality. However, the wide recognition of sign languages as natural languages has supported a greater interest in furthering our understanding of modality specific factors (e.g., the use of proprioceptive and spatial information for phonological encoding or the greater potential for iconicity). This chapter offers a comprehensive overview of the most relevant studies of sign language comprehension and production that focus on the lexical level of processing. Results from behavioural studies, as well as evidence of similar neural substrates underlying speech and sign processing, have led to the widely accepted assumption that universal language processing principles can explain lexical access in both signed and spoken languages. However, although psycholinguistic and cognitive mechanisms as well as neural networks underlying speech and sign processing are strikingly similar, they are not identical. We propose that the study of the differences in processing of speech and signs can lead to a more complete picture of human language processing. Acknowledging these differences can also point researchers to factors influencing spoken language processing that might have been under-researched so far.",
      "abstract": "The study of sign language has received increasing interest in the last decades. Within this growing field, research on sign language processing – including both comprehension and production – has also received a remarkable boost in recent years. At initial stages of research, efforts were concentrated on demonstrating universal aspects of language processing; thus, little attention was paid to the differences between modalities or to the specific aspects of the sign-modality. However, the wide recognition of sign languages as natural languages has supported a greater interest in furthering our understanding of modality specific factors (e.g., the use of proprioceptive and spatial information for phonological encoding or the greater potential for iconicity). This chapter offers a comprehensive overview of the most relevant studies of sign language comprehension and production that focus on the lexical level of processing. Results from behavioural studies, as well as evidence of similar neural substrates underlying speech and sign processing, have led to the widely accepted assumption that universal language processing principles can explain lexical access in both signed and spoken languages. However, although psycholinguistic and cognitive mechanisms as well as neural networks underlying speech and sign processing are strikingly similar, they are not identical. We propose that the study of the differences in processing of speech and signs can lead to a more complete picture of human language processing. Acknowledging these differences can also point researchers to factors influencing spoken language processing that might have been under-researched so far.",
      "doi": "https://doi.org/10.31219/osf.io/qr769",
      "openalex_id": "https://openalex.org/W4248506213",
      "arxiv_id": "",
      "publication_date": "2019-09-23",
      "published": "2019-09-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Efficient Communication and The Organization of The Lexicon",
      "summary": "Abstract This chapter applies a language design perspective to the lexicon. It reviews and synthesizes a body of work in cognitive science and linguistics that uses ideas from computer science, specifically information theory, to explore how structural features of lexicons can be explained by principles of efficient communication. It pays particular attention to four major properties of lexicons. The first is the structure of word frequency distributions, particularly the Zipfian structure of these distributions and the way that individual semantic spaces are carved up so as to be maximally efficient. The second is the relationship between word frequency and properties like word length and phonotactic probability. The third concerns lexical arbitrariness: the extent to which word forms contain information about their meanings. Finally, the chapter considers how lexicons are structured for child language learning.",
      "abstract": "Abstract This chapter applies a language design perspective to the lexicon. It reviews and synthesizes a body of work in cognitive science and linguistics that uses ideas from computer science, specifically information theory, to explore how structural features of lexicons can be explained by principles of efficient communication. It pays particular attention to four major properties of lexicons. The first is the structure of word frequency distributions, particularly the Zipfian structure of these distributions and the way that individual semantic spaces are carved up so as to be maximally efficient. The second is the relationship between word frequency and properties like word length and phonotactic probability. The third concerns lexical arbitrariness: the extent to which word forms contain information about their meanings. Finally, the chapter considers how lexicons are structured for child language learning.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.19",
      "openalex_id": "https://openalex.org/W4289716311",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Wifi Sensing And Networking With Channel State Information",
      "summary": "In recent years, WiFi has a very rapid growth due to its high throughput, high efficiency, and low costs. Multiple-Input Multiple-Output (MIMO) and Orthogonal Frequency-Division Multiplexing (OFDM) are two key technologies for providing high throughput and efficiency for WiFi systems. MIMO-OFDM provides Channel State Information (CSI) which represents the amplitude attenuation and phase shift of each transmit-receiver antenna pair of each carrier frequency. CSI helps WiFi achieve high throughput to meet the growing demands of wireless data traffic. CSI captures how wireless signals travel through the surrounding environment, so it can also be used for wireless sensing purposes. This dissertation presents how to improve WiFi sensing and networking with CSI. More specifically, this dissertation proposes deep learning models to improve the performance and capability of WiFi sensing and presents network protocols to reduce CSI feedback overhead for high efficiency WiFi networking. For WiFi sensing, there are many wireless sensing applications using CSI as the input in recent years. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this dissertation presents a survey of signal processing techniques, algorithms, applications, performance results, challenges, and future trends of CSI-based WiFi sensing. CSI is widely used for gesture recognition and sign language recognition. Existing methods for WiFi-based sign language recognition have low accuracy and high costs when there are more than 200 sign gestures. The dissertation presents SignFi for sign language recognition using CSI and Convolutional Neural Networks (CNNs). SignFi provides high accuracy and low costs for run-time testing for 276 sign gestures in the lab and home environments. For WiFi networking, although CSI provides high throughput for WiFi networks, it also introduces high overhead. WiFi transmitters need CSI feedback for transmit beamforming and rate adaptation. The size of CSI packets is very large and it grows very fast with respect to the number of antennas and channel width. CSI feedback introduces high overhead which reduces the performance and efficiency of WiFi systems, especially mobile and hand-held WiFi devices. This dissertation presents RoFi to reduce CSI feedback overhead based on the mobility status of WiFi receivers. CSI feedback compression reduces overhead, but WiFi receivers still need to send CSI feedback to the WiFi transmitter. The dissertation presents EliMO for eliminating CSI feedback without sacrificing beamforming gains.",
      "abstract": "In recent years, WiFi has a very rapid growth due to its high throughput, high efficiency, and low costs. Multiple-Input Multiple-Output (MIMO) and Orthogonal Frequency-Division Multiplexing (OFDM) are two key technologies for providing high throughput and efficiency for WiFi systems. MIMO-OFDM provides Channel State Information (CSI) which represents the amplitude attenuation and phase shift of each transmit-receiver antenna pair of each carrier frequency. CSI helps WiFi achieve high throughput to meet the growing demands of wireless data traffic. CSI captures how wireless signals travel through the surrounding environment, so it can also be used for wireless sensing purposes. This dissertation presents how to improve WiFi sensing and networking with CSI. More specifically, this dissertation proposes deep learning models to improve the performance and capability of WiFi sensing and presents network protocols to reduce CSI feedback overhead for high efficiency WiFi networking. For WiFi sensing, there are many wireless sensing applications using CSI as the input in recent years. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this dissertation presents a survey of signal processing techniques, algorithms, applications, performance results, challenges, and future trends of CSI-based WiFi sensing. CSI is widely used for gesture recognition and sign language recognition. Existing methods for WiFi-based sign language recognition have low accuracy and high costs when there are more than 200 sign gestures. The dissertation presents SignFi for sign language recognition using CSI and Convolutional Neural Networks (CNNs). SignFi provides high accuracy and low costs for run-time testing for 276 sign gestures in the lab and home environments. For WiFi networking, although CSI provides high throughput for WiFi networks, it also introduces high overhead. WiFi transmitters need CSI feedback for transmit beamforming and rate adaptation. The size of CSI packets is very large and it grows very fast with respect to the number of antennas and channel width. CSI feedback introduces high overhead which reduces the performance and efficiency of WiFi systems, especially mobile and hand-held WiFi devices. This dissertation presents RoFi to reduce CSI feedback overhead based on the mobility status of WiFi receivers. CSI feedback compression reduces overhead, but WiFi receivers still need to send CSI feedback to the WiFi transmitter. The dissertation presents EliMO for eliminating CSI feedback without sacrificing beamforming gains.",
      "doi": "https://doi.org/10.21220/s2-dwgg-3j27",
      "openalex_id": "https://openalex.org/W3080097142",
      "arxiv_id": "",
      "publication_date": "2020-06-25",
      "published": "2020-06-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Systematicity and Arbitrariness In Language",
      "summary": "Abstract This chapter reassesses two traditional ideas about the nature of language—Saussurean arbitrariness and Humboldtian infinity—in light of the modern study of linguistic productivity, language acquisition, and cognitive science. It reviews evidence from child language that productivity is categorical, which enables the language user to overcome the arbitrariness of words with systematic generalizations. Such a conception of productivity place severe constraints on the language learning mechanism that children use to form productive generalizations from a finite number of examples. Special focus is given to the Tolerance Principle, which appears to underlie the acquisition of rules in phonology, morphology, and syntax. The connection between a learning-theoretic approach to language and the structural theory of language is discussed, which may also elucidate the role of language acquisition in children’s conceptual development.",
      "abstract": "Abstract This chapter reassesses two traditional ideas about the nature of language—Saussurean arbitrariness and Humboldtian infinity—in light of the modern study of linguistic productivity, language acquisition, and cognitive science. It reviews evidence from child language that productivity is categorical, which enables the language user to overcome the arbitrariness of words with systematic generalizations. Such a conception of productivity place severe constraints on the language learning mechanism that children use to form productive generalizations from a finite number of examples. Special focus is given to the Tolerance Principle, which appears to underlie the acquisition of rules in phonology, morphology, and syntax. The connection between a learning-theoretic approach to language and the structural theory of language is discussed, which may also elucidate the role of language acquisition in children’s conceptual development.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.9",
      "openalex_id": "https://openalex.org/W4212864350",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Language and thought",
      "summary": "Abstract This chapter explores the relationship between language and thought, focusing on whether changes in non-linguistic thought follow from learning a particular language (e.g., English vs. Greek) and/or from learning any natural language at all. The chapter considers several different hypotheses about the possible effects of language on our non-linguistic representation, including classical and neo-classical versions of Whorf’s hypothesis as well as more recent views. The latter include one proposal suggesting that learning a language causes massive reorganization of our non-linguistic thought, and one suggesting that language more likely causes pervasive momentary on-line effects of linguistic understanding while leaving non-linguistic representations intact. These views are considered across a number of different domains including color, space, number, and theory of mind.",
      "abstract": "Abstract This chapter explores the relationship between language and thought, focusing on whether changes in non-linguistic thought follow from learning a particular language (e.g., English vs. Greek) and/or from learning any natural language at all. The chapter considers several different hypotheses about the possible effects of language on our non-linguistic representation, including classical and neo-classical versions of Whorf’s hypothesis as well as more recent views. The latter include one proposal suggesting that learning a language causes massive reorganization of our non-linguistic thought, and one suggesting that language more likely causes pervasive momentary on-line effects of linguistic understanding while leaving non-linguistic representations intact. These views are considered across a number of different domains including color, space, number, and theory of mind.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.5",
      "openalex_id": "https://openalex.org/W4213305108",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Effects of Adjuvant Chemotherapy on Insulin Resistance in Patients with Early Breast Cancer",
      "summary": "Objective: To assess the effect of adjuvant chemotherapy on insulin resistance in patients with early breast cancer. Material and Methods: Twenty-three non-diabetic patients were included. Patients were prospectively evaluated before, during, and after chemotherapy. Demographic, anthropometric, histopathological features, and treatment data were recorded. Blood samples were taken to evaluate fasting blood glucose, fasting insulin levels, and HbA1c. Homeostatic model assessment for insulin resistance (HOMA-IR) score measured using fasting blood glucose and fasting insulin levels. Results: Overall, pre- and post-chemotherapy mean weights were comparable (70.17 kg vs. 71.43). Prechemotherapy mean HOMA-IR was 4.99 and significantly higher than the control group of the healthy population (p=0.008). The mean values of the HOMA-IR score before, during, and after chemotherapy were 4.99, 3.47, and 3.13, respectively. Although the mean HOMA-IR decreased after chemotherapy, these decreases were not statistically significant (p=0.089). The mean fasting glucose levels before, during, and after chemotherapy were 95.5, 101.9, and 94.1 mg/dL, respectively. Before, during, and after chemotherapy, the mean fasting insulin levels were 21.43, 13.32, and 13.28 &#956;IU/mL, respectively. Conclusion: In the study, we observed a higher rate of insulin resistance in patients with breast cancer. The mean values of the HOMA-IR score decreased during and after chemotherapy.",
      "abstract": "Objective: To assess the effect of adjuvant chemotherapy on insulin resistance in patients with early breast cancer. Material and Methods: Twenty-three non-diabetic patients were included. Patients were prospectively evaluated before, during, and after chemotherapy. Demographic, anthropometric, histopathological features, and treatment data were recorded. Blood samples were taken to evaluate fasting blood glucose, fasting insulin levels, and HbA1c. Homeostatic model assessment for insulin resistance (HOMA-IR) score measured using fasting blood glucose and fasting insulin levels. Results: Overall, pre- and post-chemotherapy mean weights were comparable (70.17 kg vs. 71.43). Prechemotherapy mean HOMA-IR was 4.99 and significantly higher than the control group of the healthy population (p=0.008). The mean values of the HOMA-IR score before, during, and after chemotherapy were 4.99, 3.47, and 3.13, respectively. Although the mean HOMA-IR decreased after chemotherapy, these decreases were not statistically significant (p=0.089). The mean fasting glucose levels before, during, and after chemotherapy were 95.5, 101.9, and 94.1 mg/dL, respectively. Before, during, and after chemotherapy, the mean fasting insulin levels were 21.43, 13.32, and 13.28 &#956;IU/mL, respectively. Conclusion: In the study, we observed a higher rate of insulin resistance in patients with breast cancer. The mean values of the HOMA-IR score decreased during and after chemotherapy.",
      "doi": "https://doi.org/10.4274/csmedj.galenos.2022.2022-1-7",
      "openalex_id": "https://openalex.org/W4225502224",
      "arxiv_id": "",
      "publication_date": "2022-04-08",
      "published": "2022-04-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign learning of hearing children in inclusive day care centers—does iconicity matter?",
      "summary": "An increasing number of experimental studies suggest that signs and gestures can scaffold vocabulary learning for children with and without special educational needs and/or disabilities (SEND). However, little research has been done on the extent to which iconicity plays a role in sign learning, particularly in inclusive day care centers. This current study investigated the role of iconicity in the sign learning of 145 hearing children (2;1 to 6;3 years) from inclusive day care centers with educators who started using sign-supported speech after a training module. Children’s sign use was assessed via a questionnaire completed by their educators. We found that older children were more likely to learn signs with a higher degree of iconicity, whereas the learning of signs by younger children was less affected by iconicity. Children with SEND did not benefit more from iconicity than children without SEND. These results suggest that whether iconicity plays a role in sign learning depends on the age of the children.",
      "abstract": "An increasing number of experimental studies suggest that signs and gestures can scaffold vocabulary learning for children with and without special educational needs and/or disabilities (SEND). However, little research has been done on the extent to which iconicity plays a role in sign learning, particularly in inclusive day care centers. This current study investigated the role of iconicity in the sign learning of 145 hearing children (2;1 to 6;3 years) from inclusive day care centers with educators who started using sign-supported speech after a training module. Children’s sign use was assessed via a questionnaire completed by their educators. We found that older children were more likely to learn signs with a higher degree of iconicity, whereas the learning of signs by younger children was less affected by iconicity. Children with SEND did not benefit more from iconicity than children without SEND. These results suggest that whether iconicity plays a role in sign learning depends on the age of the children.",
      "doi": "https://doi.org/10.3389/fpsyg.2023.1196114",
      "openalex_id": "https://openalex.org/W4385950012",
      "arxiv_id": "",
      "publication_date": "2023-08-16",
      "published": "2023-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign Language Production: A Review",
      "summary": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
      "abstract": "Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.",
      "doi": "https://doi.org/10.1109/cvprw53098.2021.00384",
      "openalex_id": "https://openalex.org/W3141790718",
      "arxiv_id": "",
      "publication_date": "2021-06-01",
      "published": "2021-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language",
      "summary": "Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.",
      "abstract": "Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.",
      "doi": "https://doi.org/10.48550/arxiv.1812.01053",
      "openalex_id": "https://openalex.org/W2903314716",
      "arxiv_id": "",
      "publication_date": "2018-12-03",
      "published": "2018-12-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language",
      "summary": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
      "abstract": "Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\", \"disgust\", and \"happy\". We also considered the \"None\" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.",
      "doi": "https://doi.org/10.48550/arxiv.2003.08759",
      "openalex_id": "https://openalex.org/W3084410123",
      "arxiv_id": "",
      "publication_date": "2020-03-03",
      "published": "2020-03-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Give Me A Sign: Using Data Gloves For Static Hand Shape Recognition",
      "summary": "Human-to-human communication via the computer is mainly done using a keyboard or microphone. In the field of Virtual Reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g. silent commands during task force training) or simply not possible (e.g. if the user has a hearing loss). Data gloves help to increase immersion within the VR as they correspond to our natural interaction. At the same time, they offer the possibility to accurately capture hand shapes, such as those used in non-verbal communication (e.g. thumbs up, okay gesture, ...) and in sign language. In this paper, we present a hand shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an Outlier Detection and a Feature Selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial Data Augmentation, i.e., we create new artificial data from the recorded and filtered data to augment the training dataset. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. Voting Meta-Classifier (VL2) has proven to be the most accurate, albeit slowest, classifier. A good alternative is Random Forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. Outlier Detection has proven to be a effective approach, especially in improving classification time. Overall, we have shown that our hand shape recognition system using data gloves is suitable for communication within VR.",
      "abstract": "Human-to-human communication via the computer is mainly done using a keyboard or microphone. In the field of Virtual Reality (VR), where the most immersive experience possible is desired, the use of a keyboard contradicts this goal, while the use of a microphone is not always desirable (e.g. silent commands during task force training) or simply not possible (e.g. if the user has a hearing loss). Data gloves help to increase immersion within the VR as they correspond to our natural interaction. At the same time, they offer the possibility to accurately capture hand shapes, such as those used in non-verbal communication (e.g. thumbs up, okay gesture, ...) and in sign language. In this paper, we present a hand shape recognition system using Manus Prime X data gloves, including data acquisition, data preprocessing, and data classification to enable nonverbal communication within VR. We investigate the impact on accuracy and classification time of using an Outlier Detection and a Feature Selection approach in our data preprocessing. To obtain a more generalized approach, we also studied the impact of artificial Data Augmentation, i.e., we create new artificial data from the recorded and filtered data to augment the training dataset. With our approach, 56 different hand shapes could be distinguished with an accuracy of up to 93.28%. With a reduced number of 27 hand shapes, an accuracy of up to 95.55% could be achieved. Voting Meta-Classifier (VL2) has proven to be the most accurate, albeit slowest, classifier. A good alternative is Random Forest (RF), which was even able to achieve better accuracy values in a few cases and was generally somewhat faster. Outlier Detection has proven to be a effective approach, especially in improving classification time. Overall, we have shown that our hand shape recognition system using data gloves is suitable for communication within VR.",
      "doi": "https://doi.org/10.20944/preprints202311.1385.v1",
      "openalex_id": "https://openalex.org/W4388911562",
      "arxiv_id": "",
      "publication_date": "2023-11-22",
      "published": "2023-11-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonological Abstraction in The Mental Lexicon",
      "summary": "Abstract In this chapter, we examine the nature of the long-term memory representation of the pronunciations of words. A fundamental question concerns how abstract these representations are vis à vis the physical manifestation of words, both as gestures and as physical percepts. We consider this question and related issues within the traditions of linguistic cognition and generative phonology. We first explore the general nature of abstraction, and then review the arguments in generative phonology for positing that the units of speech stored in long-term memory (so called ‘underlying forms’) abstract away from many phonetic details. Motivations for concepts such as phonemes and distinctive phonological features are given. We then visit the open question regarding how abstract underlying forms may be allowed to be. We conclude by highlighting the contributions that evidence from neuroscience and sign language linguistics brings to these issues of phonological abstraction in the mental lexicon.",
      "abstract": "Abstract In this chapter, we examine the nature of the long-term memory representation of the pronunciations of words. A fundamental question concerns how abstract these representations are vis à vis the physical manifestation of words, both as gestures and as physical percepts. We consider this question and related issues within the traditions of linguistic cognition and generative phonology. We first explore the general nature of abstraction, and then review the arguments in generative phonology for positing that the units of speech stored in long-term memory (so called ‘underlying forms’) abstract away from many phonetic details. Motivations for concepts such as phonemes and distinctive phonological features are given. We then visit the open question regarding how abstract underlying forms may be allowed to be. We conclude by highlighting the contributions that evidence from neuroscience and sign language linguistics brings to these issues of phonological abstraction in the mental lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.14",
      "openalex_id": "https://openalex.org/W4212983170",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language",
      "summary": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
      "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
      "doi": "https://doi.org/10.48550/arxiv.2203.06096",
      "openalex_id": "https://openalex.org/W4221167884",
      "arxiv_id": "",
      "publication_date": "2022-03-11",
      "published": "2022-03-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Folk Definitions as a Model for Sign Language Dictionary Definitions: A User-Focused Study of the Online Dictionary of New Zealand Sign Language",
      "summary": "&lt;p&gt;This thesis addresses the question whether signed definitions, made possible by advances in electronic lexicography, should be introduced to sign language dictionaries. The thesis comprises four interrelated studies investigating different aspects of this question through a user-focused case study of the Online Dictionary of New Zealand Sign Language (ODNZSL). A preliminary study investigated current use of the ODNZSL in order to identify what user needs signed definitions might fulfil. The study drew on two data sets: website log data for the ODNZSL, and a think-aloud protocol and interview with representatives of user groups. Results showed that in addition to a large volume of casual browsers, the most frequent and intensive users of the dictionary are beginner and intermediate students of New Zealand Sign Language (NZSL). These (hearing) language learners mostly search for frequent vocabulary with the aims of language production and vocabulary learning. Findings also identified reasons for unsuccessful dictionary consultations that may impact on the effectiveness of definitions. In the second study, a review of ODNZSL entries highlighted categories of lexical items for which the current description through English glosses, examples, and usage notes is inadequate. A test was developed to assess whether these categories of signs were problematic for the user group identified in the first study: hearing intermediate learners of NZSL. Twenty-one participants took a computer-based error correction test with both comprehension and production sections comprising fifty items in six different categories: culture-bound; idiomatic; polysemous; metaphoric/metonymic; vocabulary type / word class; and other. Quantitative results indicated that a small number of test items were problematic, but that none of the test categories were good predictors of the difficulties learners experienced. A qualitative examination identified linguistic factors and issues with the current dictionary information that may be improved by the addition of signed definitions. The central proposition tested in the third study was that folk definitions—informal explanations of sign meaning by Deaf sign language users—can be applied as a template for dictionary definitions. This study took fifteen of the signs that were identified as problematic for learners in the previous study, and asked thirteen Deaf NZSL users to explain the meaning of these signs. A qualitative analysis found that the folk definitions by different NZSL users shared common semantic categories and embedded information about situational and sociolinguistic variation as well as grammatical structures. Some semantic relationships that occur frequently in spoken language folk definitions, such as exemplification and synonymy, were also common in signed folk definitions. Other semantic relationships such as attribution, function, operation, and spatial relationships occurred less frequently because they were inherent in the sign construction. Due to the bilingual status of the participants, many folk definitions included reference to English words in the form of mouth patterns and fingerspelling. In the fourth study, twelve pilot dictionary definitions were created on the basis of common features found in the folk definitions and an evaluation of definition formats by Deaf NZSL users. The error correction test from the second study was repeated with a new cohort of intermediate NZSL learners. This time twelve test items were accompanied by a pilot definition; for the remaining items participants were shown a video example sentence from the ODNZSL entry. Results showed no significant improvements in scores for the test items with definitions. However, feedback from test participants showed that the definitions were comprehensible and perceived as valuable for language learning. The overall conclusion of these studies is that a selective approach should be taken to introducing signed definitions in existing multifunctional sign language dictionaries. For hearing learners of sign language, signed definitions do not meet immediate communicative (comprehension and production) needs, but they may contribute to wider vocabulary learning goals. The main contribution of this thesis is that it suggests a user-focused methodology for creating signed definitions, driven by evidence from the first empirical user study of an online sign language dictionary and therefore taking into account the particular challenges of sign language lexicography. Furthermore, the analysis of features of signed folk definitions contributes to the semantic description of sign languages.&lt;/p&gt;",
      "abstract": "&lt;p&gt;This thesis addresses the question whether signed definitions, made possible by advances in electronic lexicography, should be introduced to sign language dictionaries. The thesis comprises four interrelated studies investigating different aspects of this question through a user-focused case study of the Online Dictionary of New Zealand Sign Language (ODNZSL). A preliminary study investigated current use of the ODNZSL in order to identify what user needs signed definitions might fulfil. The study drew on two data sets: website log data for the ODNZSL, and a think-aloud protocol and interview with representatives of user groups. Results showed that in addition to a large volume of casual browsers, the most frequent and intensive users of the dictionary are beginner and intermediate students of New Zealand Sign Language (NZSL). These (hearing) language learners mostly search for frequent vocabulary with the aims of language production and vocabulary learning. Findings also identified reasons for unsuccessful dictionary consultations that may impact on the effectiveness of definitions. In the second study, a review of ODNZSL entries highlighted categories of lexical items for which the current description through English glosses, examples, and usage notes is inadequate. A test was developed to assess whether these categories of signs were problematic for the user group identified in the first study: hearing intermediate learners of NZSL. Twenty-one participants took a computer-based error correction test with both comprehension and production sections comprising fifty items in six different categories: culture-bound; idiomatic; polysemous; metaphoric/metonymic; vocabulary type / word class; and other. Quantitative results indicated that a small number of test items were problematic, but that none of the test categories were good predictors of the difficulties learners experienced. A qualitative examination identified linguistic factors and issues with the current dictionary information that may be improved by the addition of signed definitions. The central proposition tested in the third study was that folk definitions—informal explanations of sign meaning by Deaf sign language users—can be applied as a template for dictionary definitions. This study took fifteen of the signs that were identified as problematic for learners in the previous study, and asked thirteen Deaf NZSL users to explain the meaning of these signs. A qualitative analysis found that the folk definitions by different NZSL users shared common semantic categories and embedded information about situational and sociolinguistic variation as well as grammatical structures. Some semantic relationships that occur frequently in spoken language folk definitions, such as exemplification and synonymy, were also common in signed folk definitions. Other semantic relationships such as attribution, function, operation, and spatial relationships occurred less frequently because they were inherent in the sign construction. Due to the bilingual status of the participants, many folk definitions included reference to English words in the form of mouth patterns and fingerspelling. In the fourth study, twelve pilot dictionary definitions were created on the basis of common features found in the folk definitions and an evaluation of definition formats by Deaf NZSL users. The error correction test from the second study was repeated with a new cohort of intermediate NZSL learners. This time twelve test items were accompanied by a pilot definition; for the remaining items participants were shown a video example sentence from the ODNZSL entry. Results showed no significant improvements in scores for the test items with definitions. However, feedback from test participants showed that the definitions were comprehensible and perceived as valuable for language learning. The overall conclusion of these studies is that a selective approach should be taken to introducing signed definitions in existing multifunctional sign language dictionaries. For hearing learners of sign language, signed definitions do not meet immediate communicative (comprehension and production) needs, but they may contribute to wider vocabulary learning goals. The main contribution of this thesis is that it suggests a user-focused methodology for creating signed definitions, driven by evidence from the first empirical user study of an online sign language dictionary and therefore taking into account the particular challenges of sign language lexicography. Furthermore, the analysis of features of signed folk definitions contributes to the semantic description of sign languages.&lt;/p&gt;",
      "doi": "https://doi.org/10.26686/wgtn.17060042.v1",
      "openalex_id": "https://openalex.org/W2613542438",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perception and production of language in the visual modality:: Implications for sign language development",
      "summary": "Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "abstract": "Sign language acquisition requires learning how to comprehend and produce a linguistic system that is visual in nature, as opposed to spoken language acquisition which uses the auditory-visual modality. In this chapter, we consider the impact this has for a child acquiring a sign language. We summarize the research literature on sign language production and comprehension, and attempt to integrate psycholinguistic studies with work documenting the visual perceptual abilities of deaf children. While much of this research emphasizes the experience-dependent nature of language processing abilities, reinforcing the importance of early exposure for native-like acquisition, we caution against overgeneralizing from studies of adult processing and call for more child-specific language studies related to comprehension and production within varying acquisition environments.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3004712147",
      "arxiv_id": "",
      "publication_date": "2020-02-01",
      "published": "2020-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "5. Childhood Autism and Sign Communication",
      "summary": "Chapter 5 provides a comprehensive, historical overview of the defining characteristics of autism spectrum disorder (ASD) along with the various language therapies employed to improve the communicative success of minimally verbal individuals. The various strengths and weaknesses of such approaches are analyzed as a basis for helping to determine which methods are likely to be the most successful. The history of signing in persons with ASD is examined, with a special focus on the relatively recent realization that motor skills (both gross and fine) and imitation abilities in such individuals may be severely impaired, thus limiting the effectiveness of sign interventions that do not take into consideration the motor complexity of the signs used. Since many parents and caregivers may also express reluctance to adopt a strategy that uses signs out of fear that this will prevent their child’s acquisition of speech, research dispelling this myth is provided. In addition to coverage of sign-communication interventions and strategies for promoting spontaneous communication and generalization of sign use to multiple settings, other non-oral approaches employed with persons with disabilities are presented as options, including the use of real objects, the Picture Exchange Communication System (PECS), Blissymbols, speech-generating devices, and software applications.",
      "abstract": "Chapter 5 provides a comprehensive, historical overview of the defining characteristics of autism spectrum disorder (ASD) along with the various language therapies employed to improve the communicative success of minimally verbal individuals. The various strengths and weaknesses of such approaches are analyzed as a basis for helping to determine which methods are likely to be the most successful. The history of signing in persons with ASD is examined, with a special focus on the relatively recent realization that motor skills (both gross and fine) and imitation abilities in such individuals may be severely impaired, thus limiting the effectiveness of sign interventions that do not take into consideration the motor complexity of the signs used. Since many parents and caregivers may also express reluctance to adopt a strategy that uses signs out of fear that this will prevent their child’s acquisition of speech, research dispelling this myth is provided. In addition to coverage of sign-communication interventions and strategies for promoting spontaneous communication and generalization of sign use to multiple settings, other non-oral approaches employed with persons with disabilities are presented as options, including the use of real objects, the Picture Exchange Communication System (PECS), Blissymbols, speech-generating devices, and software applications.",
      "doi": "https://doi.org/10.11647/obp.0205.05",
      "openalex_id": "https://openalex.org/W3037410390",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "2. Use of Manual Signs and Gestures by Hearing Persons",
      "summary": "Chapter 2 presents multiple accounts of the widespread use of manual signs by hearing persons in diverse settings throughout history. From an initial theoretical focus on the origins of language in humans, and the potential that language first emerged from gestural or manual communication, the reader is introduced to the views of various historical scholars who believed that signs and gestures are a natural means of communication and could potentially even be a universal form of communication. Such a universal form of communication, however, meets with a substantial obstacle in that gestures may vary widely in meaning and usage cross-culturally. Nevertheless, such a system was developed once before by the Indigenous peoples of North America, who spoke hundreds of different languages. Native Americans used signs as a lingua franca across a wide geographical area to overcome the numerous spoken language barriers they encountered. Also covered in this chapter are the use of signs in early contact situations and interactions between Native Americans and Europeans, and the development of signs by various monastic orders in Europe.",
      "abstract": "Chapter 2 presents multiple accounts of the widespread use of manual signs by hearing persons in diverse settings throughout history. From an initial theoretical focus on the origins of language in humans, and the potential that language first emerged from gestural or manual communication, the reader is introduced to the views of various historical scholars who believed that signs and gestures are a natural means of communication and could potentially even be a universal form of communication. Such a universal form of communication, however, meets with a substantial obstacle in that gestures may vary widely in meaning and usage cross-culturally. Nevertheless, such a system was developed once before by the Indigenous peoples of North America, who spoke hundreds of different languages. Native Americans used signs as a lingua franca across a wide geographical area to overcome the numerous spoken language barriers they encountered. Also covered in this chapter are the use of signs in early contact situations and interactions between Native Americans and Europeans, and the development of signs by various monastic orders in Europe.",
      "doi": "https://doi.org/10.11647/obp.0205.02",
      "openalex_id": "https://openalex.org/W3037624045",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "9. Application and Use of the Simplified Sign System with Persons with Disabilities",
      "summary": "In Chapter 9, various approaches to teaching signs to non-speaking or minimally verbal persons are examined, including general exposure, an incidental or milieu approach, games and group activities, and specific training sessions. Learning goals are identified not only for the main or primary user of the system, but also for that person’s communication partners (family members, caregivers, friends) and persons in the wider environment. Guidelines for using the Simplified Sign System with the target populations are provided in order to establish good and consistent communication practices that will help to maximize users’ success with the system. Such guidelines or strategies include ensuring a positive signing environment, establishing visual contact, using key word signing, accepting errors in sign formation from the main user, rewarding progress, using facial expressions and environmental cues or contextual information to enhance vocabulary acquisition, and adapting the rate and frequency of signing. Finally, the authors anticipate and address many of the questions or concerns that teachers or caregivers may have as they embark on a program of using Simplified Signs.",
      "abstract": "In Chapter 9, various approaches to teaching signs to non-speaking or minimally verbal persons are examined, including general exposure, an incidental or milieu approach, games and group activities, and specific training sessions. Learning goals are identified not only for the main or primary user of the system, but also for that person’s communication partners (family members, caregivers, friends) and persons in the wider environment. Guidelines for using the Simplified Sign System with the target populations are provided in order to establish good and consistent communication practices that will help to maximize users’ success with the system. Such guidelines or strategies include ensuring a positive signing environment, establishing visual contact, using key word signing, accepting errors in sign formation from the main user, rewarding progress, using facial expressions and environmental cues or contextual information to enhance vocabulary acquisition, and adapting the rate and frequency of signing. Finally, the authors anticipate and address many of the questions or concerns that teachers or caregivers may have as they embark on a program of using Simplified Signs.",
      "doi": "https://doi.org/10.11647/obp.0205.09",
      "openalex_id": "https://openalex.org/W3037687057",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "6. Sign-Communication Intervention in Adults and Children with Aphasia",
      "summary": "Chapter 6 focuses on the language and communication impairments of adults and children that may be acquired after suffering a head injury, stroke, brain infection, tumor, or other similar trauma. Such persons may have had intact language abilities before the trauma, but often present with varying degrees of severity of aphasia that may temporarily or permanently affect their receptive understanding of language and/or their production of language. Although most of the literature examines such deficits in hearing persons and the resulting impact on spoken language, deaf persons may also experience aphasic impairments to their production and understanding of sign language. An examination of apraxia, which often co-occurs with aphasia, provides another dimension that needs to be analyzed when addressing strategies for language rehabilitation. The outcomes of sign-communication interventions in persons with aphasia are presented, along with a focus on Amer-Ind, pantomime skills, and the use of signing to facilitate speech. Finally, the authors address speech and language disorders such as acquired childhood aphasia (Landau-Kleffner syndrome), developmental language disorder (formerly known as specific language impairment) in both hearing and deaf children, and childhood apraxia of speech.",
      "abstract": "Chapter 6 focuses on the language and communication impairments of adults and children that may be acquired after suffering a head injury, stroke, brain infection, tumor, or other similar trauma. Such persons may have had intact language abilities before the trauma, but often present with varying degrees of severity of aphasia that may temporarily or permanently affect their receptive understanding of language and/or their production of language. Although most of the literature examines such deficits in hearing persons and the resulting impact on spoken language, deaf persons may also experience aphasic impairments to their production and understanding of sign language. An examination of apraxia, which often co-occurs with aphasia, provides another dimension that needs to be analyzed when addressing strategies for language rehabilitation. The outcomes of sign-communication interventions in persons with aphasia are presented, along with a focus on Amer-Ind, pantomime skills, and the use of signing to facilitate speech. Finally, the authors address speech and language disorders such as acquired childhood aphasia (Landau-Kleffner syndrome), developmental language disorder (formerly known as specific language impairment) in both hearing and deaf children, and childhood apraxia of speech.",
      "doi": "https://doi.org/10.11647/obp.0205.06",
      "openalex_id": "https://openalex.org/W3037714746",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Simplified Signs: A Manual Sign-Communication System for Special Populations, Volume 2",
      "summary": "Simplified Signs presents a system of manual sign communication intended for special populations who have had limited success mastering spoken or full sign languages. It is the culmination of over twenty years of research and development by the authors. The Simplified Sign System has been developed and tested for ease of sign comprehension, memorization, and formation by limiting the complexity of the motor skills required to form each sign, and by ensuring that each sign visually resembles the meaning it conveys. Volume 1 outlines the research underpinning and informing the project, and places the Simplified Sign System in a wider context of sign usage, historically and by different populations. Volume 2 presents the lexicon of signs, totalling approximately 1000 signs, each with a clear illustration and a written description of how the sign is formed, as well as a memory aid that connects the sign visually to the meaning that it conveys. While the Simplified Sign System originally was developed to meet the needs of persons with intellectual disabilities, cerebral palsy, autism, or aphasia, it may also assist the communication needs of a wider audience – such as healthcare professionals, aid workers, military personnel, travellers or parents, and children who have not yet mastered spoken language. The system also has been shown to enhance learning for individuals studying a foreign language. Lucid and comprehensive, this work constitutes a valuable resource that will enhance the communicative interactions of many different people, and will be of great interest to researchers and educators alike.",
      "abstract": "Simplified Signs presents a system of manual sign communication intended for special populations who have had limited success mastering spoken or full sign languages. It is the culmination of over twenty years of research and development by the authors. The Simplified Sign System has been developed and tested for ease of sign comprehension, memorization, and formation by limiting the complexity of the motor skills required to form each sign, and by ensuring that each sign visually resembles the meaning it conveys. Volume 1 outlines the research underpinning and informing the project, and places the Simplified Sign System in a wider context of sign usage, historically and by different populations. Volume 2 presents the lexicon of signs, totalling approximately 1000 signs, each with a clear illustration and a written description of how the sign is formed, as well as a memory aid that connects the sign visually to the meaning that it conveys. While the Simplified Sign System originally was developed to meet the needs of persons with intellectual disabilities, cerebral palsy, autism, or aphasia, it may also assist the communication needs of a wider audience – such as healthcare professionals, aid workers, military personnel, travellers or parents, and children who have not yet mastered spoken language. The system also has been shown to enhance learning for individuals studying a foreign language. Lucid and comprehensive, this work constitutes a valuable resource that will enhance the communicative interactions of many different people, and will be of great interest to researchers and educators alike.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3037808925",
      "arxiv_id": "",
      "publication_date": "2020-06-22",
      "published": "2020-06-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "4. Sign Communication in Persons with an Intellectual Disability or with Cerebral Palsy",
      "summary": "In Chapter 4, the authors begin an in-depth discussion of the use of signs with special populations, including an early study that occurred in the West of England in the 1840s with deaf students with intellectual disabilities. Various types of intellectual disability are identified, including fragile X syndrome, Williams syndrome, Down syndrome, and Angelman syndrome. The successes and failures of speech-based and sign-based interventions are covered for individuals with these syndromes (particularly the latter two) as well as in persons with multiple disabilities. The authors next move on to a discussion of the relatively sparse research related to teaching signs to children who have cerebral palsy. Recommendations for enhancing the sign-learning environment are provided so that all persons who use signs as an augmentative or alternative means of communication may derive the greatest benefit from their communicative interactions. In addition to maximizing the positive atmosphere in which signing individuals interact with others at school, at home, and in public, the authors suggest that the types of signs employed may also have an impact on whether or not signing is successful.",
      "abstract": "In Chapter 4, the authors begin an in-depth discussion of the use of signs with special populations, including an early study that occurred in the West of England in the 1840s with deaf students with intellectual disabilities. Various types of intellectual disability are identified, including fragile X syndrome, Williams syndrome, Down syndrome, and Angelman syndrome. The successes and failures of speech-based and sign-based interventions are covered for individuals with these syndromes (particularly the latter two) as well as in persons with multiple disabilities. The authors next move on to a discussion of the relatively sparse research related to teaching signs to children who have cerebral palsy. Recommendations for enhancing the sign-learning environment are provided so that all persons who use signs as an augmentative or alternative means of communication may derive the greatest benefit from their communicative interactions. In addition to maximizing the positive atmosphere in which signing individuals interact with others at school, at home, and in public, the authors suggest that the types of signs employed may also have an impact on whether or not signing is successful.",
      "doi": "https://doi.org/10.11647/obp.0205.04",
      "openalex_id": "https://openalex.org/W3037841061",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "3. Deaf Persons and Sign Languages",
      "summary": "Chapter 3 introduces the reader to various aspects of sign languages, including their historical development and use within educational contexts by Deaf communities in Europe and the United States. Also covered is the initiation of the field of sign language linguistics by William C. Stokoe, a linguist who systematically proved that American Sign Language (ASL) was indeed a language with its own distinct structure and properties that differed from any spoken language. The phonological parameters of signs receive considerable attention, highlighting ways in which the unique properties of sign languages allow them to represent meaning in ways that are more consistently transparent and iconic than similar phenomena in the speech modality. Despite these similarities across sign languages, the differences among the sign languages of the world led Deaf persons to create and develop the lingua franca of International Sign (previously Gestuno) for use at international conventions. Finally, the similarities and distinctions between the processes of language development and acquisition across the modalities of speech and sign are discussed, as well as how signing benefits the learning of spoken language vocabulary by hearing children.",
      "abstract": "Chapter 3 introduces the reader to various aspects of sign languages, including their historical development and use within educational contexts by Deaf communities in Europe and the United States. Also covered is the initiation of the field of sign language linguistics by William C. Stokoe, a linguist who systematically proved that American Sign Language (ASL) was indeed a language with its own distinct structure and properties that differed from any spoken language. The phonological parameters of signs receive considerable attention, highlighting ways in which the unique properties of sign languages allow them to represent meaning in ways that are more consistently transparent and iconic than similar phenomena in the speech modality. Despite these similarities across sign languages, the differences among the sign languages of the world led Deaf persons to create and develop the lingua franca of International Sign (previously Gestuno) for use at international conventions. Finally, the similarities and distinctions between the processes of language development and acquisition across the modalities of speech and sign are discussed, as well as how signing benefits the learning of spoken language vocabulary by hearing children.",
      "doi": "https://doi.org/10.11647/obp.0205.03",
      "openalex_id": "https://openalex.org/W3037863649",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "8. Development of the Simplified Sign System",
      "summary": "Chapter 8 provides background information on the development of the Simplified Sign System. These steps are included so that investigators may replicate research findings and/or develop additional signs for their own sign-intervention programs. The authors first discuss efforts to find highly iconic or representative gestures in the dictionaries of various sign languages and sign systems from around the world. If necessary, signs were then modified to make them easier to produce based on the results of prior studies of signing errors made by students with autism, the sign-learning children of Deaf parents, and undergraduate students unfamiliar with any sign language. These potential signs were then tested with different undergraduate students to determine whether the signs were sufficiently memorable and accurately formed. Signs that did not meet criterion were either dropped from the system or subsequently modified and re-tested. Initial results from comparison studies between Simplified Signs and ASL signs and between Simplified Signs and Amer-Ind signs are presented as well. Finally, feedback from users influenced the course of the project. Memory aids were developed, especially for those persons who have less familiarity with sign languages, to help explain the ties between each sign and its referent in case that relationship is not readily or immediately apparent to a potential learner.",
      "abstract": "Chapter 8 provides background information on the development of the Simplified Sign System. These steps are included so that investigators may replicate research findings and/or develop additional signs for their own sign-intervention programs. The authors first discuss efforts to find highly iconic or representative gestures in the dictionaries of various sign languages and sign systems from around the world. If necessary, signs were then modified to make them easier to produce based on the results of prior studies of signing errors made by students with autism, the sign-learning children of Deaf parents, and undergraduate students unfamiliar with any sign language. These potential signs were then tested with different undergraduate students to determine whether the signs were sufficiently memorable and accurately formed. Signs that did not meet criterion were either dropped from the system or subsequently modified and re-tested. Initial results from comparison studies between Simplified Signs and ASL signs and between Simplified Signs and Amer-Ind signs are presented as well. Finally, feedback from users influenced the course of the project. Memory aids were developed, especially for those persons who have less familiarity with sign languages, to help explain the ties between each sign and its referent in case that relationship is not readily or immediately apparent to a potential learner.",
      "doi": "https://doi.org/10.11647/obp.0205.08",
      "openalex_id": "https://openalex.org/W3037943431",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "7. Use of Manual Signs and Gestures by Hearing Persons",
      "summary": "In Chapter 7, the authors change focus from the use of signs by deaf persons and with individuals with disabilities to how signing may enhance the learning and processing of spoken language by typically developing hearing children and adults. The first topic examined is the use of signs to foster infants’ and young children’s acquisition of their principal spoken language. Signs may further serve as an effective intervention strategy in academic settings for children with ADHD or as a means to improving vocabulary acquisition and reading comprehension for children who lag behind their age group on various language performance measures. Iconic signs and representative gestures may also be used to facilitate the acquisition of foreign language vocabulary when the signs are paired with the to-be-learned words. Finally, various studies concerning the positive benefits of learning to sign promote the possibility that using the visual-gestural modality may confer increased skills in various cognitive domains such as spatial memory, mental rotation, and facial discrimination.",
      "abstract": "In Chapter 7, the authors change focus from the use of signs by deaf persons and with individuals with disabilities to how signing may enhance the learning and processing of spoken language by typically developing hearing children and adults. The first topic examined is the use of signs to foster infants’ and young children’s acquisition of their principal spoken language. Signs may further serve as an effective intervention strategy in academic settings for children with ADHD or as a means to improving vocabulary acquisition and reading comprehension for children who lag behind their age group on various language performance measures. Iconic signs and representative gestures may also be used to facilitate the acquisition of foreign language vocabulary when the signs are paired with the to-be-learned words. Finally, various studies concerning the positive benefits of learning to sign promote the possibility that using the visual-gestural modality may confer increased skills in various cognitive domains such as spatial memory, mental rotation, and facial discrimination.",
      "doi": "https://doi.org/10.11647/obp.0205.07",
      "openalex_id": "https://openalex.org/W3037999947",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sign Language Phonological Variation and Change",
      "summary": "This chapter covers factors that affect variation in phonological or prosodic form synchronically and diachronically. Such factors can include grammatical context, phonetic factors such as ease of articulation and perception, as well as a range of sociolinguistic factors, such as age, gender, region, and ethnicity.",
      "abstract": "This chapter covers factors that affect variation in phonological or prosodic form synchronically and diachronically. Such factors can include grammatical context, phonetic factors such as ease of articulation and perception, as well as a range of sociolinguistic factors, such as age, gender, region, and ethnicity.",
      "doi": "https://doi.org/10.1017/9781316286401.008",
      "openalex_id": "https://openalex.org/W3161496272",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Design of Signed Language Tests for Deaf L1 Children",
      "summary": "Abstract Recent changes in the earlier diagnosis of deafness and improved amplification options have meant that deaf children increasingly have better opportunities to develop spoken language. Nevertheless, a significant proportion of children continue to use signed language as a first language (L1), including deaf and hearing children in deaf signing families and deaf children in hearing families where families use signed language in the home. For both groups, mastery of sign language as an L1 is important because it paves the way to communication and also because it provides the basis for development of spoken language, in either its oral or written form, as a second language (L2). It is crucial that signed language development proceeds in an age-appropriate manner, and assessments of signed language are therefore important to ensure that this is the case. However, the development of effective tests of signed language acquisition is not without challenges. This chapter presents these challenges and other issues and gives examples of how available tests seek to overcome them.",
      "abstract": "Abstract Recent changes in the earlier diagnosis of deafness and improved amplification options have meant that deaf children increasingly have better opportunities to develop spoken language. Nevertheless, a significant proportion of children continue to use signed language as a first language (L1), including deaf and hearing children in deaf signing families and deaf children in hearing families where families use signed language in the home. For both groups, mastery of sign language as an L1 is important because it paves the way to communication and also because it provides the basis for development of spoken language, in either its oral or written form, as a second language (L2). It is crucial that signed language development proceeds in an age-appropriate manner, and assessments of signed language are therefore important to ensure that this is the case. However, the development of effective tests of signed language acquisition is not without challenges. This chapter presents these challenges and other issues and gives examples of how available tests seek to overcome them.",
      "doi": "https://doi.org/10.1093/oso/9780190885052.003.0003",
      "openalex_id": "https://openalex.org/W4200391881",
      "arxiv_id": "",
      "publication_date": "2021-12-21",
      "published": "2021-12-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Emergence of Phonology",
      "summary": "In this chapter, issues concerning the emergence of phonology will be addressed by tracing the paths of phonology and morphophonology as they move from gesture, to homesign, and across multiple stages in cohorts (or generations) of young sign languages. The material covered in the first four chapters of this volume will provide theoretical context for the emergence of phonology. Relevant work on spoken languages, which has observed and modeled processes of emergence or mapped them typologically, will be discussed, and some of the principles of phonological systems will be articulated, such as paradigm uniformity, conventionalization, symmetry of the phonological inventory, and well-formedness constraints on phonological constituents. Based on ongoing work we can also address some of the social factors that may be important in different rates of emergence in different social contexts or \"language ecologies.\"",
      "abstract": "In this chapter, issues concerning the emergence of phonology will be addressed by tracing the paths of phonology and morphophonology as they move from gesture, to homesign, and across multiple stages in cohorts (or generations) of young sign languages. The material covered in the first four chapters of this volume will provide theoretical context for the emergence of phonology. Relevant work on spoken languages, which has observed and modeled processes of emergence or mapped them typologically, will be discussed, and some of the principles of phonological systems will be articulated, such as paradigm uniformity, conventionalization, symmetry of the phonological inventory, and well-formedness constraints on phonological constituents. Based on ongoing work we can also address some of the social factors that may be important in different rates of emergence in different social contexts or \"language ecologies.\"",
      "doi": "https://doi.org/10.1017/9781316286401.005",
      "openalex_id": "https://openalex.org/W4211179059",
      "arxiv_id": "",
      "publication_date": "2019-11-04",
      "published": "2019-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Structure of The Lexical Item and Sentence Meaning Composition",
      "summary": "Abstract This chapter examines the full-entry model according to which lexical meaning is a generalization that results from, and is maintained by, continued exposure to a conceptual structure through linguistics means. It focuses on the iteration reading in “durative” for, as in “Sam jumped for an hour,” and the observation that such iteration has no overt morphophonological support, raising the question of its source. Composition of the for-adverbial exerts greater computational load than the non-iterative counterpart. The root of this cost is explained as the real-time search for a partition measure demanded by for’s meaning. Hence, for’s meaning determines the context that is relevant for the utterance construal. In this way, the lexical item built in a full-entry fashion captures the semantic combinatorial and generative burden by capitalizing on a simplified lexically-driven constraint-satisfaction dynamic for the processor and a lexicon-based grammar.",
      "abstract": "Abstract This chapter examines the full-entry model according to which lexical meaning is a generalization that results from, and is maintained by, continued exposure to a conceptual structure through linguistics means. It focuses on the iteration reading in “durative” for, as in “Sam jumped for an hour,” and the observation that such iteration has no overt morphophonological support, raising the question of its source. Composition of the for-adverbial exerts greater computational load than the non-iterative counterpart. The root of this cost is explained as the real-time search for a partition measure demanded by for’s meaning. Hence, for’s meaning determines the context that is relevant for the utterance construal. In this way, the lexical item built in a full-entry fashion captures the semantic combinatorial and generative burden by capitalizing on a simplified lexically-driven constraint-satisfaction dynamic for the processor and a lexicon-based grammar.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.13",
      "openalex_id": "https://openalex.org/W4212814378",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning and Using Written Word Forms",
      "summary": "Abstract Over most of human history, knowing a word has involved knowing its phonological form. Nowadays, for people who are literate, knowing a word means knowing its written form as well. The goal of this chapter is to discuss how people learn and use these forms. The chapter begins by considering how writing systems represent language in a visual form. The next section of the chapter takes up the processes that are involved in skilled reading, considering how experienced readers perform the secondary linguistic task of reading as well and quickly as they do. How children learn to read and spell is also considered. The final section of the chapter discusses how the learning of orthographic representations can affect the mental lexicon.",
      "abstract": "Abstract Over most of human history, knowing a word has involved knowing its phonological form. Nowadays, for people who are literate, knowing a word means knowing its written form as well. The goal of this chapter is to discuss how people learn and use these forms. The chapter begins by considering how writing systems represent language in a visual form. The next section of the chapter takes up the processes that are involved in skilled reading, considering how experienced readers perform the secondary linguistic task of reading as well and quickly as they do. How children learn to read and spell is also considered. The final section of the chapter discusses how the learning of orthographic representations can affect the mental lexicon.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.24",
      "openalex_id": "https://openalex.org/W4212849484",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Neural encoding of speech and word forms",
      "summary": "Abstract Speech perception is a collection of procedures that make possible the mapping from continuously varying acoustic input to discrete mental representations that form the basis for stored words in the mental lexicon. The chapter focuses on experiments supporting abstractionist versus episodic approaches to the representation of speech and words. Studies using electrophysiological approaches are reviewed that are consistent with both the generation of abstract categories and the maintenance of episodic information, suggesting that both types of representations are carried forward during the recognition process. The functional neuroanatomy that underpins speech recognition is reviewed, and both neurophysiological and neuroimaging data are discussed that, jointly, are consistent with a perspective that privileges abstract representations but allows for the concurrent incorporation of episodic or indexical information in speech perception. The integrative, neurally inspired model that emerges accommodates both abstractionist and episodicist approaches.",
      "abstract": "Abstract Speech perception is a collection of procedures that make possible the mapping from continuously varying acoustic input to discrete mental representations that form the basis for stored words in the mental lexicon. The chapter focuses on experiments supporting abstractionist versus episodic approaches to the representation of speech and words. Studies using electrophysiological approaches are reviewed that are consistent with both the generation of abstract categories and the maintenance of episodic information, suggesting that both types of representations are carried forward during the recognition process. The functional neuroanatomy that underpins speech recognition is reviewed, and both neurophysiological and neuroimaging data are discussed that, jointly, are consistent with a perspective that privileges abstract representations but allows for the concurrent incorporation of episodic or indexical information in speech perception. The integrative, neurally inspired model that emerges accommodates both abstractionist and episodicist approaches.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.16",
      "openalex_id": "https://openalex.org/W4212906825",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Dynamics of Word Production",
      "summary": "Abstract The common sense notion of lexicon as a dictionary implies a static, fixed repository of information about the properties of individual words. This chapter discusses evidence from speech production suggesting that the lexicon in production is best characterized as a process: lexical access. This process involves the dynamic interaction of information from multiple lexical representations, resulting in the production of variable word forms. The corresponding theoretical framework is outlined within the context of single word production. This chapter then discusses a relatively less-well explored area: how lexical access changes when speakers plan and produce multiple words in connected speech. The conclusion points to open theoretical issues raised by new findings in connected speech.",
      "abstract": "Abstract The common sense notion of lexicon as a dictionary implies a static, fixed repository of information about the properties of individual words. This chapter discusses evidence from speech production suggesting that the lexicon in production is best characterized as a process: lexical access. This process involves the dynamic interaction of information from multiple lexical representations, resulting in the production of variable word forms. The corresponding theoretical framework is outlined within the context of single word production. This chapter then discusses a relatively less-well explored area: how lexical access changes when speakers plan and produce multiple words in connected speech. The conclusion points to open theoretical issues raised by new findings in connected speech.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.25",
      "openalex_id": "https://openalex.org/W4212957703",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How Learners Move From Sound To Morphology",
      "summary": "Abstract This chapter investigates the phenomenon of how children acquire grammatical morphology, including both function words and inflectional morphemes. In particular, it shows that the phonology and prosodic structure of a language interact with how and when grammatical morphemes are perceived/comprehended and produced. With respect to function words such as articles, it shows that those that can be prosodified as part of a foot/prosodic word tend to be produced first, as do inflectional morphemes occurring at the ends of phrases/utterances. The fact that similar patterns of prosodic interactions between the perception/production of grammatical morphology and the lexicon appear crosslinguistically suggests that these are robust phenomena. This has both theoretical implications for understanding the interactions between children’s developing linguistic competencies at the phonology/syntax interface, as well as practical implications for clinicians working with children with language delay.",
      "abstract": "Abstract This chapter investigates the phenomenon of how children acquire grammatical morphology, including both function words and inflectional morphemes. In particular, it shows that the phonology and prosodic structure of a language interact with how and when grammatical morphemes are perceived/comprehended and produced. With respect to function words such as articles, it shows that those that can be prosodified as part of a foot/prosodic word tend to be produced first, as do inflectional morphemes occurring at the ends of phrases/utterances. The fact that similar patterns of prosodic interactions between the perception/production of grammatical morphology and the lexicon appear crosslinguistically suggests that these are robust phenomena. This has both theoretical implications for understanding the interactions between children’s developing linguistic competencies at the phonology/syntax interface, as well as practical implications for clinicians working with children with language delay.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.8",
      "openalex_id": "https://openalex.org/W4212986196",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Early Logic and Language",
      "summary": "Abstract The present chapter charts the course of the acquisition of logical expressions in child language. The focus is on the meanings that child language learners initially assign to logical expressions, how children’s meanings compare with the meanings assigned by adults, and how the meanings of the logical expressions of natural language correspond to the meanings that are assigned to the corresponding vocabulary of classical logic. We review the findings of several cross-linguistic experimental studies investigating children’s interpretation of sentences that contain different combinations of logical expressions. In general, the findings of research indicate a strong overlap in the meanings that children assign to logical expressions and the meanings assigned to the corresponding expressions in classical logic.",
      "abstract": "Abstract The present chapter charts the course of the acquisition of logical expressions in child language. The focus is on the meanings that child language learners initially assign to logical expressions, how children’s meanings compare with the meanings assigned by adults, and how the meanings of the logical expressions of natural language correspond to the meanings that are assigned to the corresponding vocabulary of classical logic. We review the findings of several cross-linguistic experimental studies investigating children’s interpretation of sentences that contain different combinations of logical expressions. In general, the findings of research indicate a strong overlap in the meanings that children assign to logical expressions and the meanings assigned to the corresponding expressions in classical logic.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.11",
      "openalex_id": "https://openalex.org/W4213062342",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Syntax and The Lexicon",
      "summary": "Abstract Taking as point of departure Chomsky’s Remarks, this chapter discusses the relationship between syntax and the lexicon and how this has changed in the last 50 years. It then examines the syntax-lexicon relationship from the perspective of Distributed Morphology, focusing on transitivity alternations, nominalization, and adjectival passives. The main question addressed is the locus of introduction of arguments, focusing on external and internal arguments as well as on different types of external arguments. The Distributed Morphology perspective is compared to Ramchand’s (2008, 2018) First Phase Syntax framework, and Borer’s (2005a, b, 2013) Exoskeletal model. The chapter is structured from the perspective of a theory of grammar that dispenses with the lexicon component and discusses the consequences of this choice for argument introduction.",
      "abstract": "Abstract Taking as point of departure Chomsky’s Remarks, this chapter discusses the relationship between syntax and the lexicon and how this has changed in the last 50 years. It then examines the syntax-lexicon relationship from the perspective of Distributed Morphology, focusing on transitivity alternations, nominalization, and adjectival passives. The main question addressed is the locus of introduction of arguments, focusing on external and internal arguments as well as on different types of external arguments. The Distributed Morphology perspective is compared to Ramchand’s (2008, 2018) First Phase Syntax framework, and Borer’s (2005a, b, 2013) Exoskeletal model. The chapter is structured from the perspective of a theory of grammar that dispenses with the lexicon component and discusses the consequences of this choice for argument introduction.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.2",
      "openalex_id": "https://openalex.org/W4213124799",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Differences in vocabulary growth across groups and individuals",
      "summary": "Abstract This chapter describes causes and consequences of individual differences in young children’s word learning. For decades, research has documented qualitative and quantitative differences in children’s language input, and it has been convincingly demonstrated that across different communities, children’s vocabulary growth can be linked to their language experiences. However, children also actively shape their own learning environment, and it is important to consider how their cognitive abilities, as well as their interests, shape their language experiences and their learning. Only by examining children with a wide range of experiences (e.g., children growing up in multilingual communities) and abilities (e.g., children with developmental delays or disorders) will it be possible to develop theories that adequately capture and explain differences in children’s word learning and vocabulary growth.",
      "abstract": "Abstract This chapter describes causes and consequences of individual differences in young children’s word learning. For decades, research has documented qualitative and quantitative differences in children’s language input, and it has been convincingly demonstrated that across different communities, children’s vocabulary growth can be linked to their language experiences. However, children also actively shape their own learning environment, and it is important to consider how their cognitive abilities, as well as their interests, shape their language experiences and their learning. Only by examining children with a wide range of experiences (e.g., children growing up in multilingual communities) and abilities (e.g., children with developmental delays or disorders) will it be possible to develop theories that adequately capture and explain differences in children’s word learning and vocabulary growth.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.22",
      "openalex_id": "https://openalex.org/W4213154137",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Logic and The Lexicon",
      "summary": "Abstract This chapter focuses on a special instance of logical vocabulary, namely modal words, like “might” or “must,” which express possibility and necessity. Modal statements involve a complex interplay of morphology, syntax, semantics, and pragmatics, which make it particularly challenging to identify what lexical meanings the modal words encode. This chapter surveys how possibilities and necessities are expressed in natural language, with an eye toward cross-linguistic similarity and variation, and introduces the framework that formal semantics inherits from modal logic to analyze modal statements. It then turns to the challenges—for both the semanticist and for the child learner—of figuring out the right division of labor between semantics and pragmatics for modal statements, and the exact lexical contributions of the modal words themselves.",
      "abstract": "Abstract This chapter focuses on a special instance of logical vocabulary, namely modal words, like “might” or “must,” which express possibility and necessity. Modal statements involve a complex interplay of morphology, syntax, semantics, and pragmatics, which make it particularly challenging to identify what lexical meanings the modal words encode. This chapter surveys how possibilities and necessities are expressed in natural language, with an eye toward cross-linguistic similarity and variation, and introduces the framework that formal semantics inherits from modal logic to analyze modal statements. It then turns to the challenges—for both the semanticist and for the child learner—of figuring out the right division of labor between semantics and pragmatics for modal statements, and the exact lexical contributions of the modal words themselves.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.4",
      "openalex_id": "https://openalex.org/W4213190168",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Compositionality of concepts",
      "summary": "Abstract This chapter focuses on the problem of concept composition: to obtain a complex concept such as RED SQUARE, the mind has to be able to combine simple concepts, RED and SQUARE. It is argued here that compositionality constraint is a necessary element of any cognitively plausible theory of concepts. The chapter provides an overview of the theories of concepts that have been particularly influential in cognitive science, such as the Inferential Roles Semantics and the Prototype theory. At the same time, it aims to show how these theories still fall short of providing a satisfying solution for compositionality constraint. The chapter concludes by emphasizing the importance of compositionality constraint by stating that it should be regarded as a critical (rather than a secondary) concern that guides both theoretical and empirical research on concept representations.",
      "abstract": "Abstract This chapter focuses on the problem of concept composition: to obtain a complex concept such as RED SQUARE, the mind has to be able to combine simple concepts, RED and SQUARE. It is argued here that compositionality constraint is a necessary element of any cognitively plausible theory of concepts. The chapter provides an overview of the theories of concepts that have been particularly influential in cognitive science, such as the Inferential Roles Semantics and the Prototype theory. At the same time, it aims to show how these theories still fall short of providing a satisfying solution for compositionality constraint. The chapter concludes by emphasizing the importance of compositionality constraint by stating that it should be regarded as a critical (rather than a secondary) concern that guides both theoretical and empirical research on concept representations.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780198845003.013.20",
      "openalex_id": "https://openalex.org/W4213352237",
      "arxiv_id": "",
      "publication_date": "2022-02-14",
      "published": "2022-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Preface and Acknowledgments",
      "summary": "The Preface and Acknowledgements section provides a personal look by the primary author into the initial request for a simplified form of signing to use with persons with autism at the Grafton School in Virginia in the 1980s. Since little research into the development of sign language skills in children had been done at that time, the primary author felt that investigators first needed to document sign language acquisition by the typically developing signing children of Deaf parents. After this initial foundation of knowledge was established, then the focus could turn to sign language acquisition by atypical populations. The onset of the actual Simplified Sign project in 1998, and its subsequent development over the decades since, are covered with a focus on the contributions made by the authors as well as by the many University of Virginia students involved in the sign testing and by various persons who provided feedback on earlier versions of the chapters in this manuscript. Finally, a short Postscript by the primary author’s brother summarizes the work done on the Simplified Sign project since the passing of Dr. John Bonvillian in 2018.",
      "abstract": "The Preface and Acknowledgements section provides a personal look by the primary author into the initial request for a simplified form of signing to use with persons with autism at the Grafton School in Virginia in the 1980s. Since little research into the development of sign language skills in children had been done at that time, the primary author felt that investigators first needed to document sign language acquisition by the typically developing signing children of Deaf parents. After this initial foundation of knowledge was established, then the focus could turn to sign language acquisition by atypical populations. The onset of the actual Simplified Sign project in 1998, and its subsequent development over the decades since, are covered with a focus on the contributions made by the authors as well as by the many University of Virginia students involved in the sign testing and by various persons who provided feedback on earlier versions of the chapters in this manuscript. Finally, a short Postscript by the primary author’s brother summarizes the work done on the Simplified Sign project since the passing of Dr. John Bonvillian in 2018.",
      "doi": "https://doi.org/10.11647/obp.0205.10",
      "openalex_id": "https://openalex.org/W4230164465",
      "arxiv_id": "",
      "publication_date": "2020-06-24",
      "published": "2020-06-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Case Study of Suspected Childhood Apraxia of Sign",
      "summary": "Purpose: We provide a case report of “Zoe,” a 4-year-old deaf child from a deaf signing family, who presented with a possible case of Childhood Apraxia of Sign (CASign). Method: The description is based on reports from the child's speech-language pathologist, her Individualized Education Program report, and a clinician-created sign imitation task. Results: Zoe's sign articulation errors in American Sign Language differed from those reported for typically developing deaf children and were parallel to the types of errors observed for hearing children with childhood apraxia of speech. Specifically, Zoe produced inconsistent errors across signs, substituted more complex handshapes for simple handshapes, made errors on both unmarked (common) and marked (less common) forms, produced articulatory distortions (rather than substitutions), and exhibited “groping” behaviors (a sequence of attempts to move her hands into the correct position). In addition, Zoe sometimes self-corrected her errors by manipulating her own hands, for example, using her left hand to move the thumb of her right hand into the correct position. Conclusion: Zoe's pattern of sign errors is consistent with an underlying deficit in motor planning and/or programming and may constitute the first reported case of CASign.",
      "abstract": "Purpose: We provide a case report of “Zoe,” a 4-year-old deaf child from a deaf signing family, who presented with a possible case of Childhood Apraxia of Sign (CASign). Method: The description is based on reports from the child's speech-language pathologist, her Individualized Education Program report, and a clinician-created sign imitation task. Results: Zoe's sign articulation errors in American Sign Language differed from those reported for typically developing deaf children and were parallel to the types of errors observed for hearing children with childhood apraxia of speech. Specifically, Zoe produced inconsistent errors across signs, substituted more complex handshapes for simple handshapes, made errors on both unmarked (common) and marked (less common) forms, produced articulatory distortions (rather than substitutions), and exhibited “groping” behaviors (a sequence of attempts to move her hands into the correct position). In addition, Zoe sometimes self-corrected her errors by manipulating her own hands, for example, using her left hand to move the thumb of her right hand into the correct position. Conclusion: Zoe's pattern of sign errors is consistent with an underlying deficit in motor planning and/or programming and may constitute the first reported case of CASign.",
      "doi": "https://doi.org/10.1044/2024_persp-24-00042",
      "openalex_id": "https://openalex.org/W4401386165",
      "arxiv_id": "",
      "publication_date": "2024-08-07",
      "published": "2024-08-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Text Comprehension and Grammar",
      "summary": "Abstract Chapter 5 provides an overview of grammar knowledge and its relation to reading comprehension. A structure for reading instruction is provided, including use of all languages across guided, shared, and independent reading and writing. Evidence-based strategies for all areas of instruction are embedded across the chapter, such as explicit instruction, the language experience approach, directed reading and thinking, writer’s workshop, narrative story grammar, story maps, visualization, and reflections. Case vignettes include application of prediction, visualization, and summarizing during reading with diverse deaf/hard-of-hearing (DHH) learners. Tables include examples of complex grammatical structures, story grammar elements, sentence-parsing strategies, strategies used within multilingual deaf education classrooms, how to teach strategies, and a review of story signing and story reading.",
      "abstract": "Abstract Chapter 5 provides an overview of grammar knowledge and its relation to reading comprehension. A structure for reading instruction is provided, including use of all languages across guided, shared, and independent reading and writing. Evidence-based strategies for all areas of instruction are embedded across the chapter, such as explicit instruction, the language experience approach, directed reading and thinking, writer’s workshop, narrative story grammar, story maps, visualization, and reflections. Case vignettes include application of prediction, visualization, and summarizing during reading with diverse deaf/hard-of-hearing (DHH) learners. Tables include examples of complex grammatical structures, story grammar elements, sentence-parsing strategies, strategies used within multilingual deaf education classrooms, how to teach strategies, and a review of story signing and story reading.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0005",
      "openalex_id": "https://openalex.org/W4401771202",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introduction and Overview",
      "summary": "Abstract Chapter 1 provides an overview of the diversity of DHH learners. This range of diversity is wide and includes language(s), listening technology, literacy outcomes, early intervention, and learning environments. The chapter then shows how literacy theories and models are applied to DHH learners and it goes on to present considerations for literacy instruction, such as learner motivation, learning standards, accommodations and modifications, and learning environment. A discussion of criteria that define evidence-based instructional practices and causal factors is presented. Finally, a review of curricula for DHH learners and how to make informed choices during instruction with DHH learners closes the chapter.",
      "abstract": "Abstract Chapter 1 provides an overview of the diversity of DHH learners. This range of diversity is wide and includes language(s), listening technology, literacy outcomes, early intervention, and learning environments. The chapter then shows how literacy theories and models are applied to DHH learners and it goes on to present considerations for literacy instruction, such as learner motivation, learning standards, accommodations and modifications, and learning environment. A discussion of criteria that define evidence-based instructional practices and causal factors is presented. Finally, a review of curricula for DHH learners and how to make informed choices during instruction with DHH learners closes the chapter.",
      "doi": "https://doi.org/10.1093/oso/9780198879114.003.0001",
      "openalex_id": "https://openalex.org/W4401775482",
      "arxiv_id": "",
      "publication_date": "2024-05-20",
      "published": "2024-05-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Early Production of Imperceptible Words by Infants and Toddlers Born Deaf or Blind",
      "summary": "Abstract We investigate the roles of linguistic and sensory experience in the early-produced visual, auditory, and abstract words of congenitally-blind toddlers, deaf toddlers, and typically-sighted/hearing peers. We also assess the role of language access by comparing early word production in children learning English or American Sign Language (ASL) from birth, versus at a delay. Using parental report data on child word production from the MacArthur-Bates Communicative Development Inventory, we found evidence that while children produced words referring to imperceptible referents before age 2, such words were less likely to be produced relative to words with perceptible referents. For instance, blind (vs. sighted) children said fewer highly visual words like “blue” or “see”; deaf signing (vs. hearing) children produced fewer auditory signs like hear. Additionally, in spoken English and ASL, children who received delayed language access were less likely to produce words overall. These results demonstrate and begin to quantify how linguistic and sensory access may influence which words young children produce.",
      "abstract": "Abstract We investigate the roles of linguistic and sensory experience in the early-produced visual, auditory, and abstract words of congenitally-blind toddlers, deaf toddlers, and typically-sighted/hearing peers. We also assess the role of language access by comparing early word production in children learning English or American Sign Language (ASL) from birth, versus at a delay. Using parental report data on child word production from the MacArthur-Bates Communicative Development Inventory, we found evidence that while children produced words referring to imperceptible referents before age 2, such words were less likely to be produced relative to words with perceptible referents. For instance, blind (vs. sighted) children said fewer highly visual words like “blue” or “see”; deaf signing (vs. hearing) children produced fewer auditory signs like hear. Additionally, in spoken English and ASL, children who received delayed language access were less likely to produce words overall. These results demonstrate and begin to quantify how linguistic and sensory access may influence which words young children produce.",
      "doi": "https://doi.org/10.1162/opmi_a_00197",
      "openalex_id": "https://openalex.org/W4409260542",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Initial signs of learning: Decoding newly-learned vocabulary from neural patterns in novice sign language learners",
      "summary": "How do novice language learners represent semantic information in their new language? The extent to which multiple languages are supported by divergent or overlapping semantic representations in bilinguals has been well-studied, but less is known about how new knowledge is integrated into established representational networks at the earliest stages of acquisition. Furthermore, examining language across modality (sign vs. speech) can provide unique insight into language unconfounded by perceptual features. We present two experiments in which hearing non-signers underwent brief training in American Sign Language (ASL) followed by fMRI scanning. Across both datasets (N=50), we use representational similarity analysis (RSA) to identify brain regions where neural patterns reflect semantic relationships between stimuli. In Study 2 (N=40) we show that multivariate neural measures of semantic representation in several frontal, temporal, and occipital regions reflect individual participant-level comprehension. These results demonstrate the role of frontal and temporal regions, especially bilateral superior temporal sulcus, in representing semantic content across language and modality in novice learners.",
      "abstract": "How do novice language learners represent semantic information in their new language? The extent to which multiple languages are supported by divergent or overlapping semantic representations in bilinguals has been well-studied, but less is known about how new knowledge is integrated into established representational networks at the earliest stages of acquisition. Furthermore, examining language across modality (sign vs. speech) can provide unique insight into language unconfounded by perceptual features. We present two experiments in which hearing non-signers underwent brief training in American Sign Language (ASL) followed by fMRI scanning. Across both datasets (N=50), we use representational similarity analysis (RSA) to identify brain regions where neural patterns reflect semantic relationships between stimuli. In Study 2 (N=40) we show that multivariate neural measures of semantic representation in several frontal, temporal, and occipital regions reflect individual participant-level comprehension. These results demonstrate the role of frontal and temporal regions, especially bilateral superior temporal sulcus, in representing semantic content across language and modality in novice learners.",
      "doi": "https://doi.org/10.1101/2025.04.11.648265",
      "openalex_id": "https://openalex.org/W4409723404",
      "arxiv_id": "",
      "publication_date": "2025-04-17",
      "published": "2025-04-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Methods to study evolution of iconicity in sign languages",
      "summary": "Abstract Sign languages—the conventional languages of deaf communities—have been considered to provide a window into answering some questions regarding language emergence and evolution. In particular, iconicity, defined as the ‘existence of a structure-preserving mapping between mental models of linguistic form and meaning’, is generally regarded as a precursor to the arbitrary and segmental categorical structures found in spoken languages. However, iconic structures are omnipresent in sign languages at all levels of linguistic organization. Thus, there is a necessity for a more nuanced understanding of iconicity and its trajectory in language evolution. In this chapter, we outline different quantitative and qualitative methods to study iconicity and how one can operationalize them at lexical and discourse levels to investigate the role of iconicity in the evolution of sign languages.",
      "abstract": "Abstract Sign languages—the conventional languages of deaf communities—have been considered to provide a window into answering some questions regarding language emergence and evolution. In particular, iconicity, defined as the ‘existence of a structure-preserving mapping between mental models of linguistic form and meaning’, is generally regarded as a precursor to the arbitrary and segmental categorical structures found in spoken languages. However, iconic structures are omnipresent in sign languages at all levels of linguistic organization. Thus, there is a necessity for a more nuanced understanding of iconicity and its trajectory in language evolution. In this chapter, we outline different quantitative and qualitative methods to study iconicity and how one can operationalize them at lexical and discourse levels to investigate the role of iconicity in the evolution of sign languages.",
      "doi": "https://doi.org/10.1093/oxfordhb/9780192886491.013.11",
      "openalex_id": "https://openalex.org/W4410593440",
      "arxiv_id": "",
      "publication_date": "2025-05-22",
      "published": "2025-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Perceptual-semantic features of words differentially shape early vocabulary in American Sign Language and English",
      "summary": "Abstract How do sensory experiences shape the words we learn first? Most studies of language have focused on hearing children learning spoken languages, making it challenging to know how sound and language modality might contribute to language learning. This study investigates how perceptual and semantic features influence early vocabulary acquisition in deaf children learning American Sign Language and hearing children learning spoken English. Using vocabulary data from parent-report inventories, we analyzed 214 nouns common to both languages to compare the types of meanings associated with earlier Age of Acquisition. Results revealed that while children in both groups were earlier to acquire words that were more strongly related to the senses, the specific types of sensory meaning varied by language modality. Hearing children learned words with sound-related features earlier than other words, while deaf children learned words with visual and touch-related features earlier. This suggests that the easiest words to learn are words with meanings that children can experience first-hand, which varies based on children’s own sensory access and experience. Studying the diverse ways children acquire language, in this case deaf children, is key to developing language learning theories that reflect all learners.",
      "abstract": "Abstract How do sensory experiences shape the words we learn first? Most studies of language have focused on hearing children learning spoken languages, making it challenging to know how sound and language modality might contribute to language learning. This study investigates how perceptual and semantic features influence early vocabulary acquisition in deaf children learning American Sign Language and hearing children learning spoken English. Using vocabulary data from parent-report inventories, we analyzed 214 nouns common to both languages to compare the types of meanings associated with earlier Age of Acquisition. Results revealed that while children in both groups were earlier to acquire words that were more strongly related to the senses, the specific types of sensory meaning varied by language modality. Hearing children learned words with sound-related features earlier than other words, while deaf children learned words with visual and touch-related features earlier. This suggests that the easiest words to learn are words with meanings that children can experience first-hand, which varies based on children’s own sensory access and experience. Studying the diverse ways children acquire language, in this case deaf children, is key to developing language learning theories that reflect all learners.",
      "doi": "https://doi.org/10.1017/s0142716425100210",
      "openalex_id": "https://openalex.org/W4414844883",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning",
      "summary": "Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this paper, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",
      "abstract": "Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this paper, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",
      "doi": "https://doi.org/10.1109/taslp.2020.3038524",
      "openalex_id": "https://openalex.org/W3098557217",
      "arxiv_id": "",
      "publication_date": "2020-11-17",
      "published": "2020-11-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Spectral voice conversion for text-to-speech synthesis",
      "summary": "A new voice conversion algorithm that modifies a source speaker's speech to sound as if produced by a target speaker is presented. It is applied to a residual-excited LPC text-to-speech diphone synthesizer. Spectral parameters are mapped using a locally linear transformation based on Gaussian mixture models whose parameters are trained by joint density estimation. The LPC residuals are adjusted to match the target speakers average pitch. To study effects of the amount of training on performance, data sets of varying sizes are created by automatically selecting subsets of all available diphones by a vector quantization method. In an objective evaluation, the proposed method is found to perform more reliably for small training sets than a previous approach. In perceptual tests, it was shown that nearly optimal spectral conversion performance was achieved, even with a small amount of training data. However, speech quality improved with increases in the training set size.",
      "abstract": "A new voice conversion algorithm that modifies a source speaker's speech to sound as if produced by a target speaker is presented. It is applied to a residual-excited LPC text-to-speech diphone synthesizer. Spectral parameters are mapped using a locally linear transformation based on Gaussian mixture models whose parameters are trained by joint density estimation. The LPC residuals are adjusted to match the target speakers average pitch. To study effects of the amount of training on performance, data sets of varying sizes are created by automatically selecting subsets of all available diphones by a vector quantization method. In an objective evaluation, the proposed method is found to perform more reliably for small training sets than a previous approach. In perceptual tests, it was shown that nearly optimal spectral conversion performance was achieved, even with a small amount of training data. However, speech quality improved with increases in the training set size.",
      "doi": "https://doi.org/10.1109/icassp.1998.674423",
      "openalex_id": "https://openalex.org/W2123003832",
      "arxiv_id": "",
      "publication_date": "2002-11-27",
      "published": "2002-11-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice expression conversion with factorised HMM-TTS models",
      "summary": "This paper proposes a method to modify the expression or emotion in a sample of speech without altering the speaker’s identity. The method exploits a statistical speech model that factorises the speaker identity from expressions using linear transforms. For this approach, the set of transforms that best fit the speaker and expression of the input speech sample are learned. They are then combined with the expression transforms of the desired expression taken from another speaker. Since the combined expression transform is factorised and contains information about expression only, it may be applied to the original speech sample to modify its expression to the desired one without altering the identity of the speaker. Notably, this method may be applied universally to any voice without the need for a parallel training corpus.",
      "abstract": "This paper proposes a method to modify the expression or emotion in a sample of speech without altering the speaker’s identity. The method exploits a statistical speech model that factorises the speaker identity from expressions using linear transforms. For this approach, the set of transforms that best fit the speaker and expression of the input speech sample are learned. They are then combined with the expression transforms of the desired expression taken from another speaker. Since the combined expression transform is factorised and contains information about expression only, it may be applied to the original speech sample to modify its expression to the desired one without altering the identity of the speaker. Notably, this method may be applied universally to any voice without the need for a parallel training corpus.",
      "doi": "https://doi.org/10.21437/interspeech.2014-363",
      "openalex_id": "https://openalex.org/W2403098732",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Alaryngeal Speech Enhancement Based on One-to-Many Eigenvoice Conversion",
      "summary": "In this paper, we present novel speaking-aid systems based on one-to-many eigenvoice conversion (EVC) to enhance three types of alaryngeal speech: esophageal speech, electrolaryngeal speech, and body-conducted silent electrolaryngeal speech. Although alaryngeal speech allows laryngectomees to utter speech sounds, it suffers from the lack of speech quality and speaker individuality. To improve the speech quality of alaryngeal speech, alaryngeal-speech-to-speech (AL-to-Speech) methods based on statistical voice conversion have been proposed. In this paper, one-to-many EVC capable of flexibly controlling the converted voice quality by adapting the conversion model to given target natural voices is further implemented for the AL-to-Speech methods to effectively recover speaker individuality of each type of alaryngeal speech. These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems are capable of effectively addressing the issues of alaryngeal speech, e.g., yielding significant improvements in speech quality of each type of alaryngeal speech.",
      "abstract": "In this paper, we present novel speaking-aid systems based on one-to-many eigenvoice conversion (EVC) to enhance three types of alaryngeal speech: esophageal speech, electrolaryngeal speech, and body-conducted silent electrolaryngeal speech. Although alaryngeal speech allows laryngectomees to utter speech sounds, it suffers from the lack of speech quality and speaker individuality. To improve the speech quality of alaryngeal speech, alaryngeal-speech-to-speech (AL-to-Speech) methods based on statistical voice conversion have been proposed. In this paper, one-to-many EVC capable of flexibly controlling the converted voice quality by adapting the conversion model to given target natural voices is further implemented for the AL-to-Speech methods to effectively recover speaker individuality of each type of alaryngeal speech. These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems are capable of effectively addressing the issues of alaryngeal speech, e.g., yielding significant improvements in speech quality of each type of alaryngeal speech.",
      "doi": "https://doi.org/10.1109/taslp.2013.2286917",
      "openalex_id": "https://openalex.org/W2048646122",
      "arxiv_id": "",
      "publication_date": "2013-10-23",
      "published": "2013-10-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A hybrid approach to electrolaryngeal speech enhancement based on spectral subtraction and statistical voice conversion",
      "summary": "We present a hybrid approach to improving naturalness of electrolaryngeal (EL) speech while minimizing degradation in intelligibility. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, previous work has proposed methods for EL speech enhancement through either noise reduction or voice conversion. The former usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter method significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid method using the noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough. Index Terms: speaking-aid, electrolaryngeal speech, spectral subtraction, voice conversion, hybrid approach",
      "abstract": "We present a hybrid approach to improving naturalness of electrolaryngeal (EL) speech while minimizing degradation in intelligibility. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, previous work has proposed methods for EL speech enhancement through either noise reduction or voice conversion. The former usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter method significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid method using the noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough. Index Terms: speaking-aid, electrolaryngeal speech, spectral subtraction, voice conversion, hybrid approach",
      "doi": "https://doi.org/10.21437/interspeech.2013-669",
      "openalex_id": "https://openalex.org/W2187234408",
      "arxiv_id": "",
      "publication_date": "2013-08-25",
      "published": "2013-08-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice conversion: Factors responsible for quality",
      "summary": "A flexible analysis-synthesis system with signal dependent features is described and used to realize some desired voice characteristics in synthesized speech. The intelligibility of synthetic speech appears to depend on the ability to reproduce dynamic sounds such as stops, whereas the quality of voice is mainly determined by the true reproduction of voiced segments. We describe our work in converting the speech of one speaker to sound like that of another. A number of factors are important for maintaining the quality of the voice during this conversion process. These factors are derived from both the speech and electroglottograph signals.",
      "abstract": "A flexible analysis-synthesis system with signal dependent features is described and used to realize some desired voice characteristics in synthesized speech. The intelligibility of synthetic speech appears to depend on the ability to reproduce dynamic sounds such as stops, whereas the quality of voice is mainly determined by the true reproduction of voiced segments. We describe our work in converting the speech of one speaker to sound like that of another. A number of factors are important for maintaining the quality of the voice during this conversion process. These factors are derived from both the speech and electroglottograph signals.",
      "doi": "https://doi.org/10.1109/icassp.1985.1168479",
      "openalex_id": "https://openalex.org/W2100819376",
      "arxiv_id": "",
      "publication_date": "2005-03-23",
      "published": "2005-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Continuous probabilistic transform for voice conversion",
      "summary": "Voice conversion, as considered in this paper, is defined as modifying the speech signal of one speaker (source speaker) so that it sounds as if it had been pronounced by a different speaker (target speaker). Our contribution includes the design of a new methodology for representing the relationship between two sets of spectral envelopes. The proposed method is based on the use of a Gaussian mixture model of the source speaker spectral envelopes. The conversion itself is represented by a continuous parametric function which takes into account the probabilistic classification provided by the mixture model. The parameters of the conversion function are estimated by least squares optimization on the training data. This conversion method is implemented in the context of the HNM (harmonic+noise model) system, which allows high-quality modifications of speech signals. Compared to earlier methods based on vector quantization, the proposed conversion scheme results in a much better match between the converted envelopes and the target envelopes. Evaluation by objective tests and formal listening tests shows that the proposed transform greatly improves the quality and naturalness of the converted speech signals compared with previous proposed conversion methods.",
      "abstract": "Voice conversion, as considered in this paper, is defined as modifying the speech signal of one speaker (source speaker) so that it sounds as if it had been pronounced by a different speaker (target speaker). Our contribution includes the design of a new methodology for representing the relationship between two sets of spectral envelopes. The proposed method is based on the use of a Gaussian mixture model of the source speaker spectral envelopes. The conversion itself is represented by a continuous parametric function which takes into account the probabilistic classification provided by the mixture model. The parameters of the conversion function are estimated by least squares optimization on the training data. This conversion method is implemented in the context of the HNM (harmonic+noise model) system, which allows high-quality modifications of speech signals. Compared to earlier methods based on vector quantization, the proposed conversion scheme results in a much better match between the converted envelopes and the target envelopes. Evaluation by objective tests and formal listening tests shows that the proposed transform greatly improves the quality and naturalness of the converted speech signals compared with previous proposed conversion methods.",
      "doi": "https://doi.org/10.1109/89.661472",
      "openalex_id": "https://openalex.org/W2156142001",
      "arxiv_id": "",
      "publication_date": "1998-03-01",
      "published": "1998-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice Conversion Based on Maximum-Likelihood Estimation of Spectral Parameter Trajectory",
      "summary": "In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.",
      "abstract": "In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.",
      "doi": "https://doi.org/10.1109/tasl.2007.907344",
      "openalex_id": "https://openalex.org/W2120605154",
      "arxiv_id": "",
      "publication_date": "2007-10-15",
      "published": "2007-10-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice conversion from non-parallel corpora using variational auto-encoder",
      "summary": "We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.",
      "abstract": "We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.",
      "doi": "https://doi.org/10.1109/apsipa.2016.7820786",
      "openalex_id": "https://openalex.org/W2532494225",
      "arxiv_id": "",
      "publication_date": "2016-12-01",
      "published": "2016-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
      "summary": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios.In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages.In this case, one possible, although indirect, solution is to build a generative model for speech.Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment.In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model.Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.",
      "abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios.In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages.In this case, one possible, although indirect, solution is to build a generative model for speech.Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment.In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model.Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.",
      "doi": "https://doi.org/10.21437/interspeech.2017-63",
      "openalex_id": "https://openalex.org/W2962896155",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
      "summary": "We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.",
      "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.",
      "doi": "https://doi.org/10.21437/interspeech.2019-3232",
      "openalex_id": "https://openalex.org/W2972374322",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
      "summary": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.",
      "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2663",
      "openalex_id": "https://openalex.org/W2972659941",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Non-Parallel Sequence-to-Sequence Voice Conversion With Disentangled Linguistic and Speaker Representations",
      "summary": "This paper presents a method of sequence-to-sequence (seq2seq) voice\\nconversion using non-parallel training data. In this method, disentangled\\nlinguistic and speaker representations are extracted from acoustic features,\\nand voice conversion is achieved by preserving the linguistic representations\\nof source utterances while replacing the speaker representations with the\\ntarget ones. Our model is built under the framework of encoder-decoder neural\\nnetworks. A recognition encoder is designed to learn the disentangled\\nlinguistic representations with two strategies. First, phoneme transcriptions\\nof training data are introduced to provide the references for leaning\\nlinguistic representations of audio signals. Second, an adversarial training\\nstrategy is employed to further wipe out speaker information from the\\nlinguistic representations. Meanwhile, speaker representations are extracted\\nfrom audio signals by a speaker encoder. The model parameters are estimated by\\ntwo-stage training, including a pretraining stage using a multi-speaker dataset\\nand a fine-tuning stage using the dataset of a specific conversion pair. Since\\nboth the recognition encoder and the decoder for recovering acoustic features\\nare seq2seq neural networks, there are no constrains of frame alignment and\\nframe-by-frame conversion in our proposed method. Experimental results showed\\nthat our method obtained higher similarity and naturalness than the best\\nnon-parallel voice conversion method in Voice Conversion Challenge 2018.\\nBesides, the performance of our proposed method was closed to the\\nstate-of-the-art parallel seq2seq voice conversion method.\\n",
      "abstract": "This paper presents a method of sequence-to-sequence (seq2seq) voice\\nconversion using non-parallel training data. In this method, disentangled\\nlinguistic and speaker representations are extracted from acoustic features,\\nand voice conversion is achieved by preserving the linguistic representations\\nof source utterances while replacing the speaker representations with the\\ntarget ones. Our model is built under the framework of encoder-decoder neural\\nnetworks. A recognition encoder is designed to learn the disentangled\\nlinguistic representations with two strategies. First, phoneme transcriptions\\nof training data are introduced to provide the references for leaning\\nlinguistic representations of audio signals. Second, an adversarial training\\nstrategy is employed to further wipe out speaker information from the\\nlinguistic representations. Meanwhile, speaker representations are extracted\\nfrom audio signals by a speaker encoder. The model parameters are estimated by\\ntwo-stage training, including a pretraining stage using a multi-speaker dataset\\nand a fine-tuning stage using the dataset of a specific conversion pair. Since\\nboth the recognition encoder and the decoder for recovering acoustic features\\nare seq2seq neural networks, there are no constrains of frame alignment and\\nframe-by-frame conversion in our proposed method. Experimental results showed\\nthat our method obtained higher similarity and naturalness than the best\\nnon-parallel voice conversion method in Voice Conversion Challenge 2018.\\nBesides, the performance of our proposed method was closed to the\\nstate-of-the-art parallel seq2seq voice conversion method.\\n",
      "doi": "https://doi.org/10.1109/taslp.2019.2960721",
      "openalex_id": "https://openalex.org/W2996414377",
      "arxiv_id": "",
      "publication_date": "2019-12-19",
      "published": "2019-12-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Sequence-to-Sequence Baseline for the Voice Conversion Challenge 2020: Cascading ASR and TTS",
      "summary": "This paper presents the sequence-to-sequence (seq2seq) baseline system for the voice conversion challenge (VCC) 2020. We consider a naive approach for voice conversion (VC), which is to first transcribe the input speech with an automatic speech recognition (ASR) model, followed using the transcriptions to generate the voice of the target with a text-to-speech (TTS) model. We revisit this method under a sequence-to-sequence (seq2seq) framework by utilizing ESPnet, an open-source end-to-end speech processing toolkit, and the many well-configured pretrained models provided by the community. Official evaluation results show that our system comes out top among the participating systems in terms of conversion similarity, demonstrating the promising ability of seq2seq models to convert speaker identity. The implementation is made open-source at: this https URL.",
      "abstract": "This paper presents the sequence-to-sequence (seq2seq) baseline system for the voice conversion challenge (VCC) 2020. We consider a naive approach for voice conversion (VC), which is to first transcribe the input speech with an automatic speech recognition (ASR) model, followed using the transcriptions to generate the voice of the target with a text-to-speech (TTS) model. We revisit this method under a sequence-to-sequence (seq2seq) framework by utilizing ESPnet, an open-source end-to-end speech processing toolkit, and the many well-configured pretrained models provided by the community. Official evaluation results show that our system comes out top among the participating systems in terms of conversion similarity, demonstrating the promising ability of seq2seq models to convert speaker identity. The implementation is made open-source at: this https URL.",
      "doi": "https://doi.org/10.21437/vccbc.2020-24",
      "openalex_id": "https://openalex.org/W3092368332",
      "arxiv_id": "",
      "publication_date": "2020-10-30",
      "published": "2020-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voice Conversion by Cascading Automatic Speech Recognition and Text-to-Speech Synthesis with Prosody Transfer",
      "summary": "With the development of automatic speech recognition (ASR) and text-to-speech synthesis (TTS) technique, it's intuitive to construct a voice conversion system by cascading an ASR and TTS system. In this paper, we present a ASR-TTS method for voice conversion, which used iFLYTEK ASR engine to transcribe the source speech into text and a Transformer TTS model with WaveNet vocoder to synthesize the converted speech from the decoded text. For the TTS model, we proposed to use a prosody code to describe the prosody information other than text and speaker information contained in speech. A prosody encoder is used to extract the prosody code. During conversion, the source prosody is transferred to converted speech by conditioning the Transformer TTS model with its code. Experiments were conducted to demonstrate the effectiveness of our proposed method. Our system also obtained the best naturalness and similarity in the mono-lingual task of Voice Conversion Challenge 2020.",
      "abstract": "With the development of automatic speech recognition (ASR) and text-to-speech synthesis (TTS) technique, it's intuitive to construct a voice conversion system by cascading an ASR and TTS system. In this paper, we present a ASR-TTS method for voice conversion, which used iFLYTEK ASR engine to transcribe the source speech into text and a Transformer TTS model with WaveNet vocoder to synthesize the converted speech from the decoded text. For the TTS model, we proposed to use a prosody code to describe the prosody information other than text and speaker information contained in speech. A prosody encoder is used to extract the prosody code. During conversion, the source prosody is transferred to converted speech by conditioning the Transformer TTS model with its code. Experiments were conducted to demonstrate the effectiveness of our proposed method. Our system also obtained the best naturalness and similarity in the mono-lingual task of Voice Conversion Challenge 2020.",
      "doi": "https://doi.org/10.21437/vccbc.2020-16",
      "openalex_id": "https://openalex.org/W3081565196",
      "arxiv_id": "",
      "publication_date": "2020-10-30",
      "published": "2020-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
      "summary": "Wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning.It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases.In this work, we attempt to extend the self-supervised framework to speaker verification and language identification.First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language.Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively.For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset.For language identification, we obtain an EER of 12.02% on the 1 second condition and an EER of 3.47% on the full-length condition of the AP17-OLR dataset.Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.",
      "abstract": "Wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning.It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases.In this work, we attempt to extend the self-supervised framework to speaker verification and language identification.First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language.Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively.For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset.For language identification, we obtain an EER of 12.02% on the 1 second condition and an EER of 3.47% on the full-length condition of the AP17-OLR dataset.Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1280",
      "openalex_id": "https://openalex.org/W3198275944",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations",
      "summary": "This paper introduces S3PRL-VC, an open-source voice conversion (VC) framework based on the S3PRL toolkit. In the context of recognition-synthesis VC, self-supervised speech representation (S3R) is valuable in its potential to replace the expensive supervised representation adopted by state-of-the-art VC systems. Moreover, we claim that VC is a good probing task for S3R analysis. In this work, we provide a series of in-depth analyses by benchmarking on the two tasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as an any-to-any (A2A) setting. We also provide comparisons between not only different S3Rs but also top systems in VCC2020 with supervised representations. Systematic objective and subjective evaluation were conducted, and we show that S3R is comparable with VCC2020 top systems in the A2O setting in terms of similarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the extensive analysis, as well as the toolkit itself, contribute to not only the S3R community but also the VC community. The codebase is now open-sourced <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "This paper introduces S3PRL-VC, an open-source voice conversion (VC) framework based on the S3PRL toolkit. In the context of recognition-synthesis VC, self-supervised speech representation (S3R) is valuable in its potential to replace the expensive supervised representation adopted by state-of-the-art VC systems. Moreover, we claim that VC is a good probing task for S3R analysis. In this work, we provide a series of in-depth analyses by benchmarking on the two tasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as an any-to-any (A2A) setting. We also provide comparisons between not only different S3Rs but also top systems in VCC2020 with supervised representations. Systematic objective and subjective evaluation were conducted, and we show that S3R is comparable with VCC2020 top systems in the A2O setting in terms of similarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the extensive analysis, as well as the toolkit itself, contribute to not only the S3R community but also the VC community. The codebase is now open-sourced <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746430",
      "openalex_id": "https://openalex.org/W3207300132",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "summary": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-475",
      "openalex_id": "https://openalex.org/W3140429000",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Any-to-One Sequence-to-Sequence Voice Conversion Using Self-Supervised Discrete Speech Representations",
      "summary": "We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.",
      "abstract": "We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9415079",
      "openalex_id": "https://openalex.org/W3161695192",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations",
      "summary": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training.Various any-to-any VC approaches have been proposed like AUTOVC, AdaINVC, and FragmentVC.AUTOVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features.Frag-mentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content.Moreover, pretrained features are adopted.AUTOVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information.Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model.Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features.The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC.",
      "abstract": "Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training.Various any-to-any VC approaches have been proposed like AUTOVC, AdaINVC, and FragmentVC.AUTOVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features.Frag-mentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content.Moreover, pretrained features are adopted.AUTOVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information.Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model.Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features.The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1356",
      "openalex_id": "https://openalex.org/W3197763626",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Prosody Modeling for ASR+TTS Based Voice Conversion",
      "summary": "In voice conversion (VC), an approach showing promising results in the latest voice conversion challenge (VCC) 2020 is to first use an automatic speech recognition (ASR) model to transcribe the source speech into the underlying linguistic contents; these are then used as input by a text-to-speech (TTS) system to generate the converted speech. Such a paradigm, referred to as ASR+TTS, overlooks the modeling of prosody, which plays an important role in speech naturalness and conversion similarity. Although some researchers have considered transferring prosodic clues from the source speech, there arises a speaker mismatch during training and conversion. To address this issue, in this work, we propose to directly predict prosody from the linguistic representation in a target-speaker-dependent manner, referred to as target text prediction (TTP). We evaluate both methods on the VCC2020 benchmark and consider different linguistic representations. The results demonstrate the effectiveness of TTP in both objective and subjective evaluations.",
      "abstract": "In voice conversion (VC), an approach showing promising results in the latest voice conversion challenge (VCC) 2020 is to first use an automatic speech recognition (ASR) model to transcribe the source speech into the underlying linguistic contents; these are then used as input by a text-to-speech (TTS) system to generate the converted speech. Such a paradigm, referred to as ASR+TTS, overlooks the modeling of prosody, which plays an important role in speech naturalness and conversion similarity. Although some researchers have considered transferring prosodic clues from the source speech, there arises a speaker mismatch during training and conversion. To address this issue, in this work, we propose to directly predict prosody from the linguistic representation in a target-speaker-dependent manner, referred to as target text prediction (TTP). We evaluate both methods on the VCC2020 benchmark and consider different linguistic representations. The results demonstrate the effectiveness of TTP in both objective and subjective evaluations.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688010",
      "openalex_id": "https://openalex.org/W4210774711",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transferring Source Style in Non-Parallel Voice Conversion",
      "summary": "Voice conversion (VC) techniques aim to modify speaker identity of an utterance while preserving the underlying linguistic information.Most VC approaches ignore modeling of the speaking style (e.g.emotion and emphasis), which may contain the factors intentionally added by the speaker and should be retained during conversion.This study proposes a sequence-tosequence based non-parallel VC approach, which has the capability of transferring the speaking style from the source speech to the converted speech by explicitly modeling.Objective evaluation and subjective listening tests show superiority of the proposed VC approach in terms of speech naturalness and speaker similarity of the converted speech.Experiments are also conducted to show the source-style transferability of the proposed approach.",
      "abstract": "Voice conversion (VC) techniques aim to modify speaker identity of an utterance while preserving the underlying linguistic information.Most VC approaches ignore modeling of the speaking style (e.g.emotion and emphasis), which may contain the factors intentionally added by the speaker and should be retained during conversion.This study proposes a sequence-tosequence based non-parallel VC approach, which has the capability of transferring the speaking style from the source speech to the converted speech by explicitly modeling.Objective evaluation and subjective listening tests show superiority of the proposed VC approach in terms of speech naturalness and speaker similarity of the converted speech.Experiments are also conducted to show the source-style transferability of the proposed approach.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2412",
      "openalex_id": "https://openalex.org/W3094635600",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
      "summary": "This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.",
      "abstract": "This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.",
      "doi": "https://doi.org/10.1109/icme.2016.7552917",
      "openalex_id": "https://openalex.org/W2518172956",
      "arxiv_id": "",
      "publication_date": "2016-07-01",
      "published": "2016-07-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Sequence-to-Sequence Acoustic Modeling for Voice Conversion",
      "summary": "In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition (ASR) model are appended as auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as acoustic models. This proposed method also outperformed our previous work which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.",
      "abstract": "In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition (ASR) model are appended as auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as acoustic models. This proposed method also outperformed our previous work which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.",
      "doi": "https://doi.org/10.1109/taslp.2019.2892235",
      "openalex_id": "https://openalex.org/W2897353073",
      "arxiv_id": "",
      "publication_date": "2019-01-10",
      "published": "2019-01-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "One-Shot Voice Conversion by Vector Quantization",
      "summary": "In this paper, we propose a vector quantization (VQ) based one-shot voice conversion (VC) approach without any supervision on speaker label. We model the content embedding as a series of discrete codes and take the difference between quantize-before and quantize-after vector as the speaker embedding. We show that this approach has a strong ability to disentangle the content and speaker information with reconstruction loss only, and one-shot VC is thus achieved.",
      "abstract": "In this paper, we propose a vector quantization (VQ) based one-shot voice conversion (VC) approach without any supervision on speaker label. We model the content embedding as a series of discrete codes and take the difference between quantize-before and quantize-after vector as the speaker embedding. We show that this approach has a strong ability to disentangle the content and speaker information with reconstruction loss only, and one-shot VC is thus achieved.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053854",
      "openalex_id": "https://openalex.org/W3015434413",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Academia Sinica Systems of Voice Conversion for VCC2020",
      "summary": "This paper describes the Academia Sinica systems for the two tasks of Voice Conversion Challenge 2020, namely voice conversion within the same language (Task 1) and cross-lingual voice conversion (Task 2).For both tasks, we followed the cascaded ASR+TTS structure, using phonetic tokens as the TTS input instead of the text or characters.For Task 1, we used the international phonetic alphabet (IPA) as the input of the TTS model.For Task 2, we used unsupervised phonetic symbols extracted by the vector-quantized variational autoencoder (VQ-VAE).In the evaluation, the listening test showed that our systems performed well in the VCC2020 challenge.",
      "abstract": "This paper describes the Academia Sinica systems for the two tasks of Voice Conversion Challenge 2020, namely voice conversion within the same language (Task 1) and cross-lingual voice conversion (Task 2).For both tasks, we followed the cascaded ASR+TTS structure, using phonetic tokens as the TTS input instead of the text or characters.For Task 1, we used the international phonetic alphabet (IPA) as the input of the TTS model.For Task 2, we used unsupervised phonetic symbols extracted by the vector-quantized variational autoencoder (VQ-VAE).In the evaluation, the listening test showed that our systems performed well in the VCC2020 challenge.",
      "doi": "https://doi.org/10.21437/vccbc.2020-28",
      "openalex_id": "https://openalex.org/W3091890228",
      "arxiv_id": "",
      "publication_date": "2020-10-30",
      "published": "2020-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generative Pre-Training for Speech with Autoregressive Predictive Coding",
      "summary": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
      "abstract": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054438",
      "openalex_id": "https://openalex.org/W3016011332",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders",
      "summary": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
      "abstract": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054458",
      "openalex_id": "https://openalex.org/W2982223350",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
      "summary": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
      "abstract": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
      "doi": "https://doi.org/10.1109/taslp.2021.3095662",
      "openalex_id": "https://openalex.org/W3041561163",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?",
      "summary": "Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.",
      "abstract": "Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414460",
      "openalex_id": "https://openalex.org/W3160799772",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "summary": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178964",
      "openalex_id": "https://openalex.org/W1494198834",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
      "summary": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9052942",
      "openalex_id": "https://openalex.org/W2995181338",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An autoregressive recurrent mixture density network for parametric speech synthesis",
      "summary": "Neural-network-based generative models, such as mixture density networks, are potential solutions for speech synthesis. In this paper we follow this path and propose a recurrent mixture density network that incorporates a trainable autoregressive model. An advantage of incorporating an autoregressive model is that the time dependency within acoustic feature trajectories can be modeled without using the conventional dynamic features. More interestingly, experiments show that this autoregressive model learns to be a filter that emphasizes the high frequency components of the target acoustic feature trajectories in the training stage. In the synthesis stage, it boosts the low frequency components of the generated feature trajectories and hence increases their global variance. Experimental results show that the proposed model achieved higher likelihood on the training data and generated speech with better quality than other models when dynamic features were not utilized in any model.",
      "abstract": "Neural-network-based generative models, such as mixture density networks, are potential solutions for speech synthesis. In this paper we follow this path and propose a recurrent mixture density network that incorporates a trainable autoregressive model. An advantage of incorporating an autoregressive model is that the time dependency within acoustic feature trajectories can be modeled without using the conventional dynamic features. More interestingly, experiments show that this autoregressive model learns to be a filter that emphasizes the high frequency components of the target acoustic feature trajectories in the training stage. In the synthesis stage, it boosts the low frequency components of the generated feature trajectories and hence increases their global variance. Experimental results show that the proposed model achieved higher likelihood on the training data and generated speech with better quality than other models when dynamic features were not utilized in any model.",
      "doi": "https://doi.org/10.1109/icassp.2017.7953087",
      "openalex_id": "https://openalex.org/W2595110011",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Autoregressive Neural F0 Model for Statistical Parametric Speech Synthesis",
      "summary": "Recurrent neural networks (RNNs) have been successfully used as fundamental frequency (F0) models for textto-speech synthesis. However, this study showed that a normal RNN may not take into account the statistical dependency of the F0 data across frames and consequently only generate noisy F0 contours when F0 values are sampled from the model. A better model may take into account the causal dependency of the current F0 datum on the previous frames’ F0 data. One such model is the shallow autoregressive (AR) recurrent mixture density network (SAR) that we recently proposed. However, as this study showed, an SAR is equivalent to the combination of trainable linear filters and a conventional RNN. It is still weak for F0 modeling.<br/>To better model the temporal dependency in F0 contours, we propose a deep AR model (DAR). On the basis of an RNN, this DAR propagates the previous frame’s F0 value through the RNN, which allows non-linear AR dependency to be achieved. We also propose F0 quantization and data dropout strategies for the DAR. Experiments on a Japanese corpus demonstrated that this DAR can generate appropriate F0 contours by using the random-sampling-based generation method, which is impossible for the baseline RNN and SAR. When a conventional mean-based generation method was used in the proposed DAR and other experimental models, the DAR generated accurate and less oversmoothed F0 contours and achieved a better mean-opinion-score in a subjective evaluation test.<br/><i><b>Index Terms</b></i>—fundamental frequency, F0, pitch, speech synthesis, neural network, autoregressive model",
      "abstract": "Recurrent neural networks (RNNs) have been successfully used as fundamental frequency (F0) models for textto-speech synthesis. However, this study showed that a normal RNN may not take into account the statistical dependency of the F0 data across frames and consequently only generate noisy F0 contours when F0 values are sampled from the model. A better model may take into account the causal dependency of the current F0 datum on the previous frames’ F0 data. One such model is the shallow autoregressive (AR) recurrent mixture density network (SAR) that we recently proposed. However, as this study showed, an SAR is equivalent to the combination of trainable linear filters and a conventional RNN. It is still weak for F0 modeling.<br/>To better model the temporal dependency in F0 contours, we propose a deep AR model (DAR). On the basis of an RNN, this DAR propagates the previous frame’s F0 value through the RNN, which allows non-linear AR dependency to be achieved. We also propose F0 quantization and data dropout strategies for the DAR. Experiments on a Japanese corpus demonstrated that this DAR can generate appropriate F0 contours by using the random-sampling-based generation method, which is impossible for the baseline RNN and SAR. When a conventional mean-based generation method was used in the proposed DAR and other experimental models, the DAR generated accurate and less oversmoothed F0 contours and achieved a better mean-opinion-score in a subjective evaluation test.<br/><i><b>Index Terms</b></i>—fundamental frequency, F0, pitch, speech synthesis, neural network, autoregressive model",
      "doi": "https://doi.org/10.1109/taslp.2018.2828650",
      "openalex_id": "https://openalex.org/W2801493797",
      "arxiv_id": "",
      "publication_date": "2018-04-19",
      "published": "2018-04-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Tacotron: Towards End-to-End Speech Synthesis",
      "summary": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.Building these components often requires extensive domain expertise and may contain brittle design choices.In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.Given <text, audio> pairs, the model can be trained completely from scratch with random initialization.We present several key techniques to make the sequence-tosequence framework perform well for this challenging task.Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
      "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.Building these components often requires extensive domain expertise and may contain brittle design choices.In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.Given <text, audio> pairs, the model can be trained completely from scratch with random initialization.We present several key techniques to make the sequence-tosequence framework perform well for this challenging task.Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
      "doi": "https://doi.org/10.21437/interspeech.2017-1452",
      "openalex_id": "https://openalex.org/W2963609956",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
      "summary": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461368",
      "openalex_id": "https://openalex.org/W2964243274",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep neural networks for small footprint text-dependent speaker verification",
      "summary": "In this paper we investigate the use of deep neural networks (DNNs) for a small footprint text-dependent speaker verification task. At development stage, a DNN is trained to classify speakers at the framelevel. During speaker enrollment, the trained DNN is used to extract speaker specific features from the last hidden layer. The average of these speaker features, or d-vector, is taken as the speaker model. At evaluation stage, a d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision. Experimental results show the DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task. In addition, the DNN based system is more robust to additive noise and outperforms the i-vector system at low False Rejection operating points. Finally the combined system outperforms the i-vector system by 14% and 25% relative in equal error rate (EER) for clean and noisy conditions respectively.",
      "abstract": "In this paper we investigate the use of deep neural networks (DNNs) for a small footprint text-dependent speaker verification task. At development stage, a DNN is trained to classify speakers at the framelevel. During speaker enrollment, the trained DNN is used to extract speaker specific features from the last hidden layer. The average of these speaker features, or d-vector, is taken as the speaker model. At evaluation stage, a d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision. Experimental results show the DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task. In addition, the DNN based system is more robust to additive noise and outperforms the i-vector system at low False Rejection operating points. Finally the combined system outperforms the i-vector system by 14% and 25% relative in equal error rate (EER) for clean and noisy conditions respectively.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854363",
      "openalex_id": "https://openalex.org/W2046056978",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "summary": "The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million 'real-world' utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.",
      "abstract": "The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million 'real-world' utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.",
      "doi": "https://doi.org/10.1016/j.csl.2019.101027",
      "openalex_id": "https://openalex.org/W2981087920",
      "arxiv_id": "",
      "publication_date": "2019-10-16",
      "published": "2019-10-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition",
      "summary": "We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.",
      "abstract": "We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053176",
      "openalex_id": "https://openalex.org/W3015265920",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications",
      "summary": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.",
      "abstract": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.",
      "doi": "https://doi.org/10.1587/transinf.2015edp7457",
      "openalex_id": "https://openalex.org/W2471520273",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Predictions of Subjective Ratings and Spoofing Assessments of Voice Conversion Challenge 2020 Submissions",
      "summary": "The Voice Conversion Challenge 2020 is the third edition under its flagship that promotes intra-lingual semiparallel and crosslingual voice conversion (VC).While the primary evaluation of the challenge submissions was done through crowd-sourced listening tests, we also performed an objective assessment of the submitted systems.The aim of the objective assessment is to provide complementary performance analysis that may be more beneficial than the time-consuming listening tests.In this study, we examined five types of objective assessments using automatic speaker verification (ASV), neural speaker embeddings, spoofing countermeasures, predicted mean opinion scores (MOS), and automatic speech recognition (ASR).Each of these objective measures assesses the VC output along different aspects.We observed that the correlations of these objective assessments with the subjective results were high for ASV, neural speaker embedding, and ASR, which makes them more influential for predicting subjective test results.In addition, we performed spoofing assessments on the submitted systems and identified some of the VC methods showing a potentially high security risk.",
      "abstract": "The Voice Conversion Challenge 2020 is the third edition under its flagship that promotes intra-lingual semiparallel and crosslingual voice conversion (VC).While the primary evaluation of the challenge submissions was done through crowd-sourced listening tests, we also performed an objective assessment of the submitted systems.The aim of the objective assessment is to provide complementary performance analysis that may be more beneficial than the time-consuming listening tests.In this study, we examined five types of objective assessments using automatic speaker verification (ASV), neural speaker embeddings, spoofing countermeasures, predicted mean opinion scores (MOS), and automatic speech recognition (ASR).Each of these objective measures assesses the VC output along different aspects.We observed that the correlations of these objective assessments with the subjective results were high for ASV, neural speaker embedding, and ASR, which makes them more influential for predicting subjective test results.In addition, we performed spoofing assessments on the submitted systems and identified some of the VC methods showing a potentially high security risk.",
      "doi": "https://doi.org/10.21437/vccbc.2020-15",
      "openalex_id": "https://openalex.org/W3083776549",
      "arxiv_id": "",
      "publication_date": "2020-10-30",
      "published": "2020-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An Open Source Implementation of ITU-T Recommendation P.808 with Validation",
      "summary": "The ITU-T Recommendation P.808 provides a crowdsourcing approach for\\nconducting a subjective assessment of speech quality using the Absolute\\nCategory Rating (ACR) method. We provide an open-source implementation of the\\nITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended\\nour implementation to include Degradation Category Ratings (DCR) and Comparison\\nCategory Ratings (CCR) test methods. We also significantly speed up the test\\nprocess by integrating the participant qualification step into the main rating\\ntask compared to a two-stage qualification and rating solution. We provide\\nprogram scripts for creating and executing the subjective test, and data\\ncleansing and analyzing the answers to avoid operational errors. To validate\\nthe implementation, we compare the Mean Opinion Scores (MOS) collected through\\nour implementation with MOS values from a standard laboratory experiment\\nconducted based on the ITU-T Rec. P.800. We also evaluate the reproducibility\\nof the result of the subjective speech quality assessment through crowdsourcing\\nusing our implementation. Finally, we quantify the impact of parts of the\\nsystem designed to improve the reliability: environmental tests, gold and\\ntrapping questions, rating patterns, and a headset usage test.\\n",
      "abstract": "The ITU-T Recommendation P.808 provides a crowdsourcing approach for\\nconducting a subjective assessment of speech quality using the Absolute\\nCategory Rating (ACR) method. We provide an open-source implementation of the\\nITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended\\nour implementation to include Degradation Category Ratings (DCR) and Comparison\\nCategory Ratings (CCR) test methods. We also significantly speed up the test\\nprocess by integrating the participant qualification step into the main rating\\ntask compared to a two-stage qualification and rating solution. We provide\\nprogram scripts for creating and executing the subjective test, and data\\ncleansing and analyzing the answers to avoid operational errors. To validate\\nthe implementation, we compare the Mean Opinion Scores (MOS) collected through\\nour implementation with MOS values from a standard laboratory experiment\\nconducted based on the ITU-T Rec. P.800. We also evaluate the reproducibility\\nof the result of the subjective speech quality assessment through crowdsourcing\\nusing our implementation. Finally, we quantify the impact of parts of the\\nsystem designed to improve the reliability: environmental tests, gold and\\ntrapping questions, rating patterns, and a headset usage test.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-2665",
      "openalex_id": "https://openalex.org/W3025844872",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An investigation of multi-speaker training for wavenet vocoder",
      "summary": "In this paper, we investigate the effectiveness of multi-speaker training for WaveNet vocoder. In our previous work, we have demonstrated that our proposed speaker-dependent (SD) WaveNet vocoder, which is trained with a single speaker's speech data, is capable of modeling temporal waveform structure, such as phase information, and makes it possible to generate more naturally sounding synthetic voices compared to conventional high-quality vocoder, STRAIGHT. However, it is still difficult to generate synthetic voices of various speakers using the SD-WaveNet due to its speaker-dependent property. Towards the development of speaker-independent WaveNet vocoder, we apply multi-speaker training techniques to the WaveNet vocoder and investigate its effectiveness. The experimental results demonstrate that 1) the multispeaker WaveNet vocoder still outperforms STRAIGHT in generating known speakers' voices but it is comparable to STRAIGHT in generating unknown speakers' voices, and 2) the multi-speaker training is effective for developing the WaveNet vocoder capable of speech modification.",
      "abstract": "In this paper, we investigate the effectiveness of multi-speaker training for WaveNet vocoder. In our previous work, we have demonstrated that our proposed speaker-dependent (SD) WaveNet vocoder, which is trained with a single speaker's speech data, is capable of modeling temporal waveform structure, such as phase information, and makes it possible to generate more naturally sounding synthetic voices compared to conventional high-quality vocoder, STRAIGHT. However, it is still difficult to generate synthetic voices of various speakers using the SD-WaveNet due to its speaker-dependent property. Towards the development of speaker-independent WaveNet vocoder, we apply multi-speaker training techniques to the WaveNet vocoder and investigate its effectiveness. The experimental results demonstrate that 1) the multispeaker WaveNet vocoder still outperforms STRAIGHT in generating known speakers' voices but it is comparable to STRAIGHT in generating unknown speakers' voices, and 2) the multi-speaker training is effective for developing the WaveNet vocoder capable of speech modification.",
      "doi": "https://doi.org/10.1109/asru.2017.8269007",
      "openalex_id": "https://openalex.org/W2786868129",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Refined WaveNet Vocoder for Variational Autoencoder Based Voice Conversion",
      "summary": "This paper presents a refinement framework of WaveNet vocoders for variational autoencoder (VAE) based voice conversion (VC), which reduces the quality distortion caused by the mismatch between the training data and testing data. Conventional WaveNet vocoders are trained with natural acoustic features but conditioned on the converted features in the conversion stage for VC, and such a mismatch often causes significant quality and similarity degradation. In this work, we take advantage of the particular structure of VAEs to refine WaveNet vocoders with the self-reconstructed features generated by VAE, which are of similar characteristics with the converted features while having the same temporal structure with the target natural features. We analyze these features and show that the self-reconstructed features are similar to the converted features. Objective and subjective experimental results demonstrate the effectiveness of our proposed framework.",
      "abstract": "This paper presents a refinement framework of WaveNet vocoders for variational autoencoder (VAE) based voice conversion (VC), which reduces the quality distortion caused by the mismatch between the training data and testing data. Conventional WaveNet vocoders are trained with natural acoustic features but conditioned on the converted features in the conversion stage for VC, and such a mismatch often causes significant quality and similarity degradation. In this work, we take advantage of the particular structure of VAEs to refine WaveNet vocoders with the self-reconstructed features generated by VAE, which are of similar characteristics with the converted features while having the same temporal structure with the target natural features. We analyze these features and show that the self-reconstructed features are similar to the converted features. Objective and subjective experimental results demonstrate the effectiveness of our proposed framework.",
      "doi": "https://doi.org/10.23919/eusipco.2019.8902651",
      "openalex_id": "https://openalex.org/W2903365642",
      "arxiv_id": "",
      "publication_date": "2019-09-01",
      "published": "2019-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "summary": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "doi": "https://doi.org/10.48550/arxiv.1807.03748",
      "openalex_id": "https://openalex.org/W4297808394",
      "arxiv_id": "",
      "publication_date": "2018-07-10",
      "published": "2018-07-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Influence of lips on the production of vowels based on finite element simulations and experiments",
      "summary": "Three-dimensional (3-D) numerical approaches for voice production are currently being investigated and developed. Radiation losses produced when sound waves emanate from the mouth aperture are one of the key aspects to be modeled. When doing so, the lips are usually removed from the vocal tract geometry in order to impose a radiation impedance on a closed cross-section, which speeds up the numerical simulations compared to free-field radiation solutions. However, lips may play a significant role. In this work, the lips' effects on vowel sounds are investigated by using 3-D vocal tract geometries generated from magnetic resonance imaging. To this aim, two configurations for the vocal tract exit are considered: with lips and without lips. The acoustic behavior of each is analyzed and compared by means of time-domain finite element simulations that allow free-field wave propagation and experiments performed using 3-D-printed mechanical replicas. The results show that the lips should be included in order to correctly model vocal tract acoustics not only at high frequencies, as commonly accepted, but also in the low frequency range below 4 kHz, where plane wave propagation occurs.",
      "abstract": "Three-dimensional (3-D) numerical approaches for voice production are currently being investigated and developed. Radiation losses produced when sound waves emanate from the mouth aperture are one of the key aspects to be modeled. When doing so, the lips are usually removed from the vocal tract geometry in order to impose a radiation impedance on a closed cross-section, which speeds up the numerical simulations compared to free-field radiation solutions. However, lips may play a significant role. In this work, the lips' effects on vowel sounds are investigated by using 3-D vocal tract geometries generated from magnetic resonance imaging. To this aim, two configurations for the vocal tract exit are considered: with lips and without lips. The acoustic behavior of each is analyzed and compared by means of time-domain finite element simulations that allow free-field wave propagation and experiments performed using 3-D-printed mechanical replicas. The results show that the lips should be included in order to correctly model vocal tract acoustics not only at high frequencies, as commonly accepted, but also in the low frequency range below 4 kHz, where plane wave propagation occurs.",
      "doi": "https://doi.org/10.1121/1.4950698",
      "openalex_id": "https://openalex.org/W2404836908",
      "arxiv_id": "",
      "publication_date": "2016-05-01",
      "published": "2016-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "summary": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
      "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
      "doi": "https://doi.org/10.1109/tpami.2018.2798607",
      "openalex_id": "https://openalex.org/W2619383789",
      "arxiv_id": "",
      "publication_date": "2018-01-25",
      "published": "2018-01-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "OpenFace: An open source facial behavior analysis toolkit",
      "summary": "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.",
      "abstract": "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.",
      "doi": "https://doi.org/10.1109/wacv.2016.7477553",
      "openalex_id": "https://openalex.org/W2395639500",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Representation Learning: A Review and New Perspectives",
      "summary": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
      "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
      "doi": "https://doi.org/10.1109/tpami.2013.50",
      "openalex_id": "https://openalex.org/W2163922914",
      "arxiv_id": "",
      "publication_date": "2013-05-31",
      "published": "2013-05-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis",
      "summary": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.",
      "abstract": "This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.",
      "doi": "https://doi.org/10.1162/neco.2008.04-08-771",
      "openalex_id": "https://openalex.org/W2039844283",
      "arxiv_id": "",
      "publication_date": "2008-09-11",
      "published": "2008-09-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Identifying independence in bayesian networks",
      "summary": "Abstract An important feature of Bayesian networks is that they facilitate explicit encoding of information about independencies in the domain, information that is indispensable for efficient inferencing. This article characterizes all independence assertions that logically follow from the topology of a network and develops a linear time algorithm that identifies these assertions. The algorithm's correctness is based on the soundness of a graphical criterion, called d ‐separation, and its optimality stems from the completeness of d ‐separation. An enhanced version of d ‐separation, called D ‐separation, is defined, extending the algorithm to networks that encode functional dependencies. Finally, the algorithm is shown to work for a broad class of nonprobabilistic independencies.",
      "abstract": "Abstract An important feature of Bayesian networks is that they facilitate explicit encoding of information about independencies in the domain, information that is indispensable for efficient inferencing. This article characterizes all independence assertions that logically follow from the topology of a network and develops a linear time algorithm that identifies these assertions. The algorithm's correctness is based on the soundness of a graphical criterion, called d ‐separation, and its optimality stems from the completeness of d ‐separation. An enhanced version of d ‐separation, called D ‐separation, is defined, extending the algorithm to networks that encode functional dependencies. Finally, the algorithm is shown to work for a broad class of nonprobabilistic independencies.",
      "doi": "https://doi.org/10.1002/net.3230200504",
      "openalex_id": "https://openalex.org/W2034009564",
      "arxiv_id": "",
      "publication_date": "1990-08-01",
      "published": "1990-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Dynamical Variational Autoencoders: A Comprehensive Review",
      "summary": "Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this monograph, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The monograph concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.",
      "abstract": "Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this monograph, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The monograph concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.",
      "doi": "https://doi.org/10.1561/2200000089",
      "openalex_id": "https://openalex.org/W3217536461",
      "arxiv_id": "",
      "publication_date": "2021-12-02",
      "published": "2021-12-02",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Training Products of Experts by Minimizing Contrastive Divergence",
      "summary": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.",
      "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.",
      "doi": "https://doi.org/10.1162/089976602760128018",
      "openalex_id": "https://openalex.org/W2116064496",
      "arxiv_id": "",
      "publication_date": "2002-08-01",
      "published": "2002-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "summary": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "abstract": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "doi": "https://doi.org/10.1371/journal.pone.0196391",
      "openalex_id": "https://openalex.org/W2803193013",
      "arxiv_id": "",
      "publication_date": "2018-05-16",
      "published": "2018-05-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "summary": "Automated affective computing in the wild setting is a challenging problem in\\ncomputer vision. Existing annotated databases of facial expressions in the wild\\nare small and mostly cover discrete emotions (aka the categorical model). There\\nare very limited annotated facial databases for affective computing in the\\ncontinuous dimensional model (e.g., valence and arousal). To meet this need, we\\ncollected, annotated, and prepared for public distribution a new database of\\nfacial emotions in the wild (called AffectNet). AffectNet contains more than\\n1,000,000 facial images from the Internet by querying three major search\\nengines using 1250 emotion related keywords in six different languages. About\\nhalf of the retrieved images were manually annotated for the presence of seven\\ndiscrete facial expressions and the intensity of valence and arousal. AffectNet\\nis by far the largest database of facial expression, valence, and arousal in\\nthe wild enabling research in automated facial expression recognition in two\\ndifferent emotion models. Two baseline deep neural networks are used to\\nclassify images in the categorical model and predict the intensity of valence\\nand arousal. Various evaluation metrics show that our deep neural network\\nbaselines can perform better than conventional machine learning methods and\\noff-the-shelf facial expression recognition systems.\\n",
      "abstract": "Automated affective computing in the wild setting is a challenging problem in\\ncomputer vision. Existing annotated databases of facial expressions in the wild\\nare small and mostly cover discrete emotions (aka the categorical model). There\\nare very limited annotated facial databases for affective computing in the\\ncontinuous dimensional model (e.g., valence and arousal). To meet this need, we\\ncollected, annotated, and prepared for public distribution a new database of\\nfacial emotions in the wild (called AffectNet). AffectNet contains more than\\n1,000,000 facial images from the Internet by querying three major search\\nengines using 1250 emotion related keywords in six different languages. About\\nhalf of the retrieved images were manually annotated for the presence of seven\\ndiscrete facial expressions and the intensity of valence and arousal. AffectNet\\nis by far the largest database of facial expression, valence, and arousal in\\nthe wild enabling research in automated facial expression recognition in two\\ndifferent emotion models. Two baseline deep neural networks are used to\\nclassify images in the categorical model and predict the intensity of valence\\nand arousal. Various evaluation metrics show that our deep neural network\\nbaselines can perform better than conventional machine learning methods and\\noff-the-shelf facial expression recognition systems.\\n",
      "doi": "https://doi.org/10.1109/taffc.2017.2740923",
      "openalex_id": "https://openalex.org/W2745497104",
      "arxiv_id": "",
      "publication_date": "2017-08-21",
      "published": "2017-08-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PyFeat: a Python-based effective feature generation tool for DNA, RNA and protein sequences",
      "summary": "Abstract Motivation Extracting useful feature set which contains significant discriminatory information is a critical step in effectively presenting sequence data to predict structural, functional, interaction and expression of proteins, DNAs and RNAs. Also, being able to filter features with significant information and avoid sparsity in the extracted features require the employment of efficient feature selection techniques. Here we present PyFeat as a practical and easy to use toolkit implemented in Python for extracting various features from proteins, DNAs and RNAs. To build PyFeat we mainly focused on extracting features that capture information about the interaction of neighboring residues to be able to provide more local information. We then employ AdaBoost technique to select features with maximum discriminatory information. In this way, we can significantly reduce the number of extracted features and enable PyFeat to represent the combination of effective features from large neighboring residues. As a result, PyFeat is able to extract features from 13 different techniques and represent context free combination of effective features. The source code for PyFeat standalone toolkit and employed benchmarks with a comprehensive user manual explaining its system and workflow in a step by step manner are publicly available. Results https://github.com/mrzResearchArena/PyFeat/blob/master/RESULTS.md. Availability and implementation Toolkit, source code and manual to use PyFeat: https://github.com/mrzResearchArena/PyFeat/ Supplementary information Supplementary data are available at Bioinformatics online.",
      "abstract": "Abstract Motivation Extracting useful feature set which contains significant discriminatory information is a critical step in effectively presenting sequence data to predict structural, functional, interaction and expression of proteins, DNAs and RNAs. Also, being able to filter features with significant information and avoid sparsity in the extracted features require the employment of efficient feature selection techniques. Here we present PyFeat as a practical and easy to use toolkit implemented in Python for extracting various features from proteins, DNAs and RNAs. To build PyFeat we mainly focused on extracting features that capture information about the interaction of neighboring residues to be able to provide more local information. We then employ AdaBoost technique to select features with maximum discriminatory information. In this way, we can significantly reduce the number of extracted features and enable PyFeat to represent the combination of effective features from large neighboring residues. As a result, PyFeat is able to extract features from 13 different techniques and represent context free combination of effective features. The source code for PyFeat standalone toolkit and employed benchmarks with a comprehensive user manual explaining its system and workflow in a step by step manner are publicly available. Results https://github.com/mrzResearchArena/PyFeat/blob/master/RESULTS.md. Availability and implementation Toolkit, source code and manual to use PyFeat: https://github.com/mrzResearchArena/PyFeat/ Supplementary information Supplementary data are available at Bioinformatics online.",
      "doi": "https://doi.org/10.1093/bioinformatics/btz165",
      "openalex_id": "https://openalex.org/W2921372942",
      "arxiv_id": "",
      "publication_date": "2019-03-06",
      "published": "2019-03-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio-Visual Emotion Recognition in Video Clips",
      "summary": "This paper presents a multimodal emotion recognition system, which is based on the analysis of audio and visual cues. From the audio channel, Mel-Frequency Cepstral Coefficients, Filter Bank Energies and prosodic features are extracted. For the visual part, two strategies are considered. First, facial landmarks' geometric relations, i.e., distances and angles, are computed. Second, we summarize each emotional video into a reduced set of key-frames, which are taught to visually discriminate between the emotions. In order to do so, a convolutional neural network is applied to key-frames summarizing videos. Finally, confidence outputs of all the classifiers from all the modalities are used to define a new feature space to be learned for final emotion label prediction, in a late fusion/stacking fashion. The experiments conducted on the SAVEE, eNTERFACE'05, and RML databases show significant performance improvements by our proposed system in comparison to current alternatives, defining the current state-of-the-art in all three databases.",
      "abstract": "This paper presents a multimodal emotion recognition system, which is based on the analysis of audio and visual cues. From the audio channel, Mel-Frequency Cepstral Coefficients, Filter Bank Energies and prosodic features are extracted. For the visual part, two strategies are considered. First, facial landmarks' geometric relations, i.e., distances and angles, are computed. Second, we summarize each emotional video into a reduced set of key-frames, which are taught to visually discriminate between the emotions. In order to do so, a convolutional neural network is applied to key-frames summarizing videos. Finally, confidence outputs of all the classifiers from all the modalities are used to define a new feature space to be learned for final emotion label prediction, in a late fusion/stacking fashion. The experiments conducted on the SAVEE, eNTERFACE'05, and RML databases show significant performance improvements by our proposed system in comparison to current alternatives, defining the current state-of-the-art in all three databases.",
      "doi": "https://doi.org/10.1109/taffc.2017.2713783",
      "openalex_id": "https://openalex.org/W2624340939",
      "arxiv_id": "",
      "publication_date": "2017-06-09",
      "published": "2017-06-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Multimodal Learning: A Survey on Recent Advances and Trends",
      "summary": "The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.",
      "abstract": "The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.",
      "doi": "https://doi.org/10.1109/msp.2017.2738401",
      "openalex_id": "https://openalex.org/W2767290858",
      "arxiv_id": "",
      "publication_date": "2017-11-01",
      "published": "2017-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A survey of multimodal deep generative models",
      "summary": "Multimodal learning is a framework for building models that make predictions\\nbased on different types of modalities. Important challenges in multimodal\\nlearning are the inference of shared representations from arbitrary modalities\\nand cross-modal generation via these representations; however, achieving this\\nrequires taking the heterogeneous nature of multimodal data into account. In\\nrecent years, deep generative models, i.e., generative models in which\\ndistributions are parameterized by deep neural networks, have attracted much\\nattention, especially variational autoencoders, which are suitable for\\naccomplishing the above challenges because they can consider heterogeneity and\\ninfer good representations of data. Therefore, various multimodal generative\\nmodels based on variational autoencoders, called multimodal deep generative\\nmodels, have been proposed in recent years. In this paper, we provide a\\ncategorized survey of studies on multimodal deep generative models.\\n",
      "abstract": "Multimodal learning is a framework for building models that make predictions\\nbased on different types of modalities. Important challenges in multimodal\\nlearning are the inference of shared representations from arbitrary modalities\\nand cross-modal generation via these representations; however, achieving this\\nrequires taking the heterogeneous nature of multimodal data into account. In\\nrecent years, deep generative models, i.e., generative models in which\\ndistributions are parameterized by deep neural networks, have attracted much\\nattention, especially variational autoencoders, which are suitable for\\naccomplishing the above challenges because they can consider heterogeneity and\\ninfer good representations of data. Therefore, various multimodal generative\\nmodels based on variational autoencoders, called multimodal deep generative\\nmodels, have been proposed in recent years. In this paper, we provide a\\ncategorized survey of studies on multimodal deep generative models.\\n",
      "doi": "https://doi.org/10.1080/01691864.2022.2035253",
      "openalex_id": "https://openalex.org/W4213299273",
      "arxiv_id": "",
      "publication_date": "2022-02-21",
      "published": "2022-02-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning?",
      "summary": "A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",
      "abstract": "A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",
      "doi": "https://doi.org/10.48550/arxiv.1905.12506",
      "openalex_id": "https://openalex.org/W2946896833",
      "arxiv_id": "",
      "publication_date": "2019-05-29",
      "published": "2019-05-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Image quality assessment: from error visibility to structural similarity",
      "summary": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
      "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
      "doi": "https://doi.org/10.1109/tip.2003.819861",
      "openalex_id": "https://openalex.org/W2133665775",
      "arxiv_id": "",
      "publication_date": "2004-04-01",
      "published": "2004-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "summary": "Emotion recognition is the ability to identify what people would think someone is feeling from moment to moment and understand the connection between his/her feelings and expressions. In today's world, human–computer interaction (HCI) interface undoubtedly plays an important role in our daily life. Toward harmonious HCI interface, automated analysis and recognition of human emotion has attracted increasing attention from the researchers in multidisciplinary research fields. In this paper, a survey on the theoretical and practical work offering new and broad views of the latest research in emotion recognition from bimodal information including facial and vocal expressions is provided. First, the currently available audiovisual emotion databases are described. Facial and vocal features and audiovisual bimodal data fusion methods for emotion recognition are then surveyed and discussed. Specifically, this survey also covers the recent emotion challenges in several conferences. Conclusions outline and address some of the existing emotion recognition issues.",
      "abstract": "Emotion recognition is the ability to identify what people would think someone is feeling from moment to moment and understand the connection between his/her feelings and expressions. In today's world, human–computer interaction (HCI) interface undoubtedly plays an important role in our daily life. Toward harmonious HCI interface, automated analysis and recognition of human emotion has attracted increasing attention from the researchers in multidisciplinary research fields. In this paper, a survey on the theoretical and practical work offering new and broad views of the latest research in emotion recognition from bimodal information including facial and vocal expressions is provided. First, the currently available audiovisual emotion databases are described. Facial and vocal features and audiovisual bimodal data fusion methods for emotion recognition are then surveyed and discussed. Specifically, this survey also covers the recent emotion challenges in several conferences. Conclusions outline and address some of the existing emotion recognition issues.",
      "doi": "https://doi.org/10.1017/atsip.2014.11",
      "openalex_id": "https://openalex.org/W2314395941",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust Lightweight Facial Expression Recognition Network with Label Distribution Training",
      "summary": "This paper presents an efficiently robust facial expression recognition (FER) network, named EfficientFace, which holds much fewer parameters but more robust to the FER in the wild. Firstly, to improve the robustness of the lightweight network, a local-feature extractor and a channel-spatial modulator are designed, in which the depthwise convolution is employed. As a result, the network is aware of local and global-salient facial features. Then, considering the fact that most emotions occur as combinations, mixtures, or compounds of the basic emotions, we introduce a simple but efficient label distribution learning (LDL) method as a novel training strategy. Experiments conducted on realistic occlusion and pose variation datasets demonstrate that the proposed EfficientFace is robust under occlusion and pose variation conditions. Moreover, the proposed method achieves state-of-the-art results on RAF-DB, CAER-S, and AffectNet-7 datasets with accuracies of 88.36%, 85.87%, and 63.70%, respectively, and a comparable result on the AffectNet-8 dataset with an accuracy of 59.89%. The code is public available at https://github.com/zengqunzhao/EfficientFace.",
      "abstract": "This paper presents an efficiently robust facial expression recognition (FER) network, named EfficientFace, which holds much fewer parameters but more robust to the FER in the wild. Firstly, to improve the robustness of the lightweight network, a local-feature extractor and a channel-spatial modulator are designed, in which the depthwise convolution is employed. As a result, the network is aware of local and global-salient facial features. Then, considering the fact that most emotions occur as combinations, mixtures, or compounds of the basic emotions, we introduce a simple but efficient label distribution learning (LDL) method as a novel training strategy. Experiments conducted on realistic occlusion and pose variation datasets demonstrate that the proposed EfficientFace is robust under occlusion and pose variation conditions. Moreover, the proposed method achieves state-of-the-art results on RAF-DB, CAER-S, and AffectNet-7 datasets with accuracies of 88.36%, 85.87%, and 63.70%, respectively, and a comparable result on the AffectNet-8 dataset with an accuracy of 59.89%. The code is public available at https://github.com/zengqunzhao/EfficientFace.",
      "doi": "https://doi.org/10.1609/aaai.v35i4.16465",
      "openalex_id": "https://openalex.org/W3175546442",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Handbook of Psychotherapy Integration",
      "summary": "This online resource is a good overview of the growth of psychotherapy integration into a mature, empirically supported, and international movement. It provides a state-of-the-art, comprehensive description of psychotherapy and its clinical practices by leading proponents. It covers topics such as cognitive-analytic therapy, integrative psychotherapy with culturally diverse clients, cognitive-behavioural analysis system, and blending spirituality with psychotherapy. It also addresses assimilative integration, reviews the empirical research on integrative and eclectic treatments, provides guidelines that facilitate comparative analyses and ensure comprehensiveness, and includes a summary outline to help readers compare the integrative approaches.",
      "abstract": "This online resource is a good overview of the growth of psychotherapy integration into a mature, empirically supported, and international movement. It provides a state-of-the-art, comprehensive description of psychotherapy and its clinical practices by leading proponents. It covers topics such as cognitive-analytic therapy, integrative psychotherapy with culturally diverse clients, cognitive-behavioural analysis system, and blending spirituality with psychotherapy. It also addresses assimilative integration, reviews the empirical research on integrative and eclectic treatments, provides guidelines that facilitate comparative analyses and ensure comprehensiveness, and includes a summary outline to help readers compare the integrative approaches.",
      "doi": "https://doi.org/10.1093/med:psych/9780195165791.001.0001",
      "openalex_id": "https://openalex.org/W2409603808",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Private-Shared Disentangled Multimodal VAE for Learning of Hybrid Latent Representations",
      "summary": "Multi-modal generative models represent an important family of deep models, whose goal is to facilitate representation learning on data with multiple views or modalities. However, current deep multi-modal models focus on the inference of shared representations, while neglecting the important private aspects of data within individual modalities. In this paper, we introduce a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. We specifically consider the instance where the latent factor may be of both continuous and discrete nature, leading to the family of general hybrid DMVAE models. We demonstrate the utility of DMVAE on a semi-supervised learning task, where one of the modalities contains partial data labels, both relevant and irrelevant to the other modality. Our experiments on several benchmarks indicate the importance of the private-shared disentanglement as well as the hybrid latent representation.",
      "abstract": "Multi-modal generative models represent an important family of deep models, whose goal is to facilitate representation learning on data with multiple views or modalities. However, current deep multi-modal models focus on the inference of shared representations, while neglecting the important private aspects of data within individual modalities. In this paper, we introduce a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. We specifically consider the instance where the latent factor may be of both continuous and discrete nature, leading to the family of general hybrid DMVAE models. We demonstrate the utility of DMVAE on a semi-supervised learning task, where one of the modalities contains partial data labels, both relevant and irrelevant to the other modality. Our experiments on several benchmarks indicate the importance of the private-shared disentanglement as well as the hybrid latent representation.",
      "doi": "https://doi.org/10.48550/arxiv.2012.13024",
      "openalex_id": "https://openalex.org/W3114644868",
      "arxiv_id": "",
      "publication_date": "2020-12-23",
      "published": "2020-12-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NVAE: A Deep Hierarchical Variational Autoencoder",
      "summary": "Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .",
      "abstract": "Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .",
      "doi": "https://doi.org/10.48550/arxiv.2007.03898",
      "openalex_id": "https://openalex.org/W3041956526",
      "arxiv_id": "",
      "publication_date": "2020-07-08",
      "published": "2020-07-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning",
      "summary": "A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",
      "abstract": "A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2971127577",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Advances in Neural Information Processing Systems 19",
      "summary": "Papers from the 2006 flagship meeting on neural computation, with contributions from physicists, neuroscientists, mathematicians, statisticians, and computer scientists. The annual Neural Information Processing Systems (NIPS) conference is the flagship meeting on neural computation and machine learning. It draws a diverse group of attendees—physicists, neuroscientists, mathematicians, statisticians, and computer scientists—interested in theoretical and applied aspects of modeling, simulating, and building neural-like or intelligent systems. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning, and applications. Only twenty-five percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains the papers presented at the December 2006 meeting, held in Vancouver. Bradford Books imprint",
      "abstract": "Papers from the 2006 flagship meeting on neural computation, with contributions from physicists, neuroscientists, mathematicians, statisticians, and computer scientists. The annual Neural Information Processing Systems (NIPS) conference is the flagship meeting on neural computation and machine learning. It draws a diverse group of attendees—physicists, neuroscientists, mathematicians, statisticians, and computer scientists—interested in theoretical and applied aspects of modeling, simulating, and building neural-like or intelligent systems. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning, and applications. Only twenty-five percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains the papers presented at the December 2006 meeting, held in Vancouver. Bradford Books imprint",
      "doi": "https://doi.org/10.7551/mitpress/7503.001.0001",
      "openalex_id": "https://openalex.org/W2990138404",
      "arxiv_id": "",
      "publication_date": "2007-09-07",
      "published": "2007-09-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep\\n Generative Models",
      "summary": "Learning generative models that span multiple data modalities, such as vision\\nand language, is often motivated by the desire to learn more useful,\\ngeneralisable representations that faithfully capture common underlying factors\\nbetween the modalities. In this work, we characterise successful learning of\\nsuch models as the fulfillment of four criteria: i) implicit latent\\ndecomposition into shared and private subspaces, ii) coherent joint generation\\nover all modalities, iii) coherent cross-generation across individual\\nmodalities, and iv) improved model learning for individual modalities through\\nmulti-modal integration. Here, we propose a mixture-of-experts multimodal\\nvariational autoencoder (MMVAE) to learn generative models on different sets of\\nmodalities, including a challenging image-language dataset, and demonstrate its\\nability to satisfy all four criteria, both qualitatively and quantitatively.\\n",
      "abstract": "Learning generative models that span multiple data modalities, such as vision\\nand language, is often motivated by the desire to learn more useful,\\ngeneralisable representations that faithfully capture common underlying factors\\nbetween the modalities. In this work, we characterise successful learning of\\nsuch models as the fulfillment of four criteria: i) implicit latent\\ndecomposition into shared and private subspaces, ii) coherent joint generation\\nover all modalities, iii) coherent cross-generation across individual\\nmodalities, and iv) improved model learning for individual modalities through\\nmulti-modal integration. Here, we propose a mixture-of-experts multimodal\\nvariational autoencoder (MMVAE) to learn generative models on different sets of\\nmodalities, including a challenging image-language dataset, and demonstrate its\\nability to satisfy all four criteria, both qualitatively and quantitatively.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1911.03393",
      "openalex_id": "https://openalex.org/W2987472543",
      "arxiv_id": "",
      "publication_date": "2019-11-08",
      "published": "2019-11-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "International Conference on Pattern Recognition",
      "summary": "In this paper, we explore a new approach for enriching the HoG method for pedestrian detection in an unconstrained outdoor environment.The proposed algorithm is based on using gait motion since the rhythmic footprint pattern for walking people is considered the stable and characteristic feature for the detection of walking people.The novelty of our approach is motivated by the latest research for people identification using gait.The experimental results confirmed the robustness of our method to enhance HoG to detect walking people as well as to discriminate between single walking subject, groups of people and vehicles with a detection rate of 100%.Furthermore, the results revealed the potential of our method to be used in visual surveillance systems for identity tracking over different camera views.",
      "abstract": "In this paper, we explore a new approach for enriching the HoG method for pedestrian detection in an unconstrained outdoor environment.The proposed algorithm is based on using gait motion since the rhythmic footprint pattern for walking people is considered the stable and characteristic feature for the detection of walking people.The novelty of our approach is motivated by the latest research for people identification using gait.The experimental results confirmed the robustness of our method to enhance HoG to detect walking people as well as to discriminate between single walking subject, groups of people and vehicles with a detection rate of 100%.Furthermore, the results revealed the potential of our method to be used in visual surveillance systems for identity tracking over different camera views.",
      "doi": "https://doi.org/10.1109/c-m.1971.216793",
      "openalex_id": "https://openalex.org/W2601530120",
      "arxiv_id": "",
      "publication_date": "1971-05-01",
      "published": "1971-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Joint Multimodal Learning with Deep Generative Models",
      "summary": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.",
      "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.",
      "doi": "https://doi.org/10.48550/arxiv.1611.01891",
      "openalex_id": "https://openalex.org/W2556013083",
      "arxiv_id": "",
      "publication_date": "2016-11-07",
      "published": "2016-11-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
      "summary": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.",
      "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned\f beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2753738274",
      "arxiv_id": "",
      "publication_date": "2017-04-24",
      "published": "2017-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal Generative Models for Scalable Weakly-Supervised Learning",
      "summary": "Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization, segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.",
      "abstract": "Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization, segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.",
      "doi": "https://doi.org/10.48550/arxiv.1802.05335",
      "openalex_id": "https://openalex.org/W2786541991",
      "arxiv_id": "",
      "publication_date": "2018-02-14",
      "published": "2018-02-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentangling by Partitioning: A Representation Learning Framework for\\n Multimodal Sensory Data",
      "summary": "Multimodal sensory data resembles the form of information perceived by humans\\nfor learning, and are easy to obtain in large quantities. Compared to unimodal\\ndata, synchronization of concepts between modalities in such data provides\\nsupervision for disentangling the underlying explanatory factors of each\\nmodality. Previous work leveraging multimodal data has mainly focused on\\nretaining only the modality-invariant factors while discarding the rest. In\\nthis paper, we present a partitioned variational autoencoder (PVAE) and several\\ntraining objectives to learn disentangled representations, which encode not\\nonly the shared factors, but also modality-dependent ones, into separate latent\\nvariables. Specifically, PVAE integrates a variational inference framework and\\na multimodal generative model that partitions the explanatory factors and\\nconditions only on the relevant subset of them for generation. We evaluate our\\nmodel on two parallel speech/image datasets, and demonstrate its ability to\\nlearn disentangled representations by qualitatively exploring within-modality\\nand cross-modality conditional generation with semantics and styles specified\\nby examples. For quantitative analysis, we evaluate the classification accuracy\\nof automatically discovered semantic units. Our PVAE can achieve over 99%\\naccuracy on both modalities.\\n",
      "abstract": "Multimodal sensory data resembles the form of information perceived by humans\\nfor learning, and are easy to obtain in large quantities. Compared to unimodal\\ndata, synchronization of concepts between modalities in such data provides\\nsupervision for disentangling the underlying explanatory factors of each\\nmodality. Previous work leveraging multimodal data has mainly focused on\\nretaining only the modality-invariant factors while discarding the rest. In\\nthis paper, we present a partitioned variational autoencoder (PVAE) and several\\ntraining objectives to learn disentangled representations, which encode not\\nonly the shared factors, but also modality-dependent ones, into separate latent\\nvariables. Specifically, PVAE integrates a variational inference framework and\\na multimodal generative model that partitions the explanatory factors and\\nconditions only on the relevant subset of them for generation. We evaluate our\\nmodel on two parallel speech/image datasets, and demonstrate its ability to\\nlearn disentangled representations by qualitatively exploring within-modality\\nand cross-modality conditional generation with semantics and styles specified\\nby examples. For quantitative analysis, we evaluate the classification accuracy\\nof automatically discovered semantic units. Our PVAE can achieve over 99%\\naccuracy on both modalities.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1805.11264",
      "openalex_id": "https://openalex.org/W4294491235",
      "arxiv_id": "",
      "publication_date": "2018-05-29",
      "published": "2018-05-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation",
      "summary": "We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.",
      "abstract": "We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.391",
      "openalex_id": "https://openalex.org/W4367841185",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Training and Pre-Training are Complementary for Speech Recognition",
      "summary": "Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves word error rates (WER) of 2.8%/4.8% on the clean and other test sets of Librispeech – rivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5%/3.1%.",
      "abstract": "Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves word error rates (WER) of 2.8%/4.8% on the clean and other test sets of Librispeech – rivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5%/3.1%.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414641",
      "openalex_id": "https://openalex.org/W3160525311",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Textless Speech-to-Speech Translation on Real Data",
      "summary": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.63",
      "openalex_id": "https://openalex.org/W4287854499",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
      "summary": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019.",
      "abstract": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019.",
      "doi": "https://doi.org/10.18653/v1/n19-4009",
      "openalex_id": "https://openalex.org/W2933138175",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Denoising Pre-training for Neural Machine Translation",
      "summary": "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019 ). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. 1",
      "abstract": "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019 ). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. 1",
      "doi": "https://doi.org/10.1162/tacl_a_00343",
      "openalex_id": "https://openalex.org/W3001434439",
      "arxiv_id": "",
      "publication_date": "2020-11-25",
      "published": "2020-11-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "summary": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.",
      "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.",
      "doi": "https://doi.org/10.21437/interspeech.2022-143",
      "openalex_id": "https://openalex.org/W3213029956",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Comparative Layer-Wise Analysis of Self-Supervised Speech Models",
      "summary": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096149",
      "openalex_id": "https://openalex.org/W4375869259",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
      "summary": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.",
      "abstract": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1951",
      "openalex_id": "https://openalex.org/W2972495969",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Speech-to-Speech Translation With Discrete Units",
      "summary": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.235",
      "openalex_id": "https://openalex.org/W3180374548",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "summary": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "abstract": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.naacl-main.41",
      "openalex_id": "https://openalex.org/W3169483174",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multilingual Speech Translation from Efficient Finetuning of Pretrained Models",
      "summary": "Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "abstract": "Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-long.68",
      "openalex_id": "https://openalex.org/W3173767661",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "summary": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "doi": "https://doi.org/10.1109/taslp.2021.3122291",
      "openalex_id": "https://openalex.org/W3169320628",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "summary": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "doi": "https://doi.org/10.48550/arxiv.2010.05646",
      "openalex_id": "https://openalex.org/W3092028330",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "mSLAM: Massively multilingual joint pre-training for speech and text",
      "summary": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
      "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
      "doi": "https://doi.org/10.48550/arxiv.2202.01374",
      "openalex_id": "https://openalex.org/W4221155340",
      "arxiv_id": "",
      "publication_date": "2022-02-03",
      "published": "2022-02-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CVSS Corpus and Massively Multilingual Speech-to-Speech Translation",
      "summary": "We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",
      "abstract": "We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",
      "doi": "https://doi.org/10.48550/arxiv.2201.03713",
      "openalex_id": "https://openalex.org/W4226444650",
      "arxiv_id": "",
      "publication_date": "2022-01-11",
      "published": "2022-01-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation",
      "summary": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers' voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker's voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker's privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.",
      "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers' voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker's voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker's privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.",
      "doi": "https://doi.org/10.48550/arxiv.2107.08661",
      "openalex_id": "https://openalex.org/W4287072252",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models",
      "summary": "Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.",
      "abstract": "Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.",
      "doi": "https://doi.org/10.1145/3592458",
      "openalex_id": "https://openalex.org/W4377010269",
      "arxiv_id": "",
      "publication_date": "2023-07-26",
      "published": "2023-07-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents",
      "summary": "The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.",
      "abstract": "The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.",
      "doi": "https://doi.org/10.1145/3592097",
      "openalex_id": "https://openalex.org/W4385284180",
      "arxiv_id": "",
      "publication_date": "2023-07-26",
      "published": "2023-07-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Playing a part: speaker verification at the movies",
      "summary": "The goal of this work is to investigate the performance of popular speaker recognition models on speech segments from movies, where often actors intentionally disguise their voice to play a character. We make the following three contributions: (i) We collect a novel, challenging speaker recognition dataset called VoxMovies, with speech for 856 identities from almost 4000 movie clips. VoxMovies contains utterances with varying emotion, accents and background noise, and therefore comprises an entirely different domain to the interview-style, emotionally calm utterances in current speaker recognition datasets such as VoxCeleb; (ii) We provide a number of domain adaptation evaluation sets, and benchmark the performance of state-of-the-art speaker recognition models on these evaluation pairs. We demonstrate that both speaker verification and identification performance drops steeply on this new data, showing the challenge in transferring models across domains; and finally (iii) We show that simple domain adaptation paradigms improve performance, but there is still large room for improvement.",
      "abstract": "The goal of this work is to investigate the performance of popular speaker recognition models on speech segments from movies, where often actors intentionally disguise their voice to play a character. We make the following three contributions: (i) We collect a novel, challenging speaker recognition dataset called VoxMovies, with speech for 856 identities from almost 4000 movie clips. VoxMovies contains utterances with varying emotion, accents and background noise, and therefore comprises an entirely different domain to the interview-style, emotionally calm utterances in current speaker recognition datasets such as VoxCeleb; (ii) We provide a number of domain adaptation evaluation sets, and benchmark the performance of state-of-the-art speaker recognition models on these evaluation pairs. We demonstrate that both speaker verification and identification performance drops steeply on this new data, showing the challenge in transferring models across domains; and finally (iii) We show that simple domain adaptation paradigms improve performance, but there is still large room for improvement.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3160743249",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-Driven Holistic 3D Expression and Gesture Generation",
      "summary": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",
      "abstract": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",
      "doi": "https://doi.org/10.1109/cvpr52733.2024.00702",
      "openalex_id": "https://openalex.org/W4402703119",
      "arxiv_id": "",
      "publication_date": "2024-06-16",
      "published": "2024-06-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Executing your Commands via Motion Diffusion in Latent Space",
      "summary": "We study a challenging task, conditional human motion generation, which produces plausible human motion sequences according to various conditional inputs, such as action classes or textual descriptors. Since human motions are highly diverse and have a property of quite different distribution from conditional modalities, such as textual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modality to the human motion sequences. Besides, the raw motion data from the motion capture system might be redundant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and conditional modalities would need a heavy computational over-head and might result in artifacts introduced by the captured noises. To learn a better representation of the various human motion sequences, we first design a powerful Variational AutoEncoder (VAE) and arrive at a representative and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves significant improvements over the state-of-the-art methods among extensive human motion generation tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences.",
      "abstract": "We study a challenging task, conditional human motion generation, which produces plausible human motion sequences according to various conditional inputs, such as action classes or textual descriptors. Since human motions are highly diverse and have a property of quite different distribution from conditional modalities, such as textual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modality to the human motion sequences. Besides, the raw motion data from the motion capture system might be redundant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and conditional modalities would need a heavy computational over-head and might result in artifacts introduced by the captured noises. To learn a better representation of the various human motion sequences, we first design a powerful Variational AutoEncoder (VAE) and arrive at a representative and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves significant improvements over the state-of-the-art methods among extensive human motion generation tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01726",
      "openalex_id": "https://openalex.org/W4386065848",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotional Speech-Driven Animation with Content-Emotion Disentanglement",
      "summary": "To be widely adopted, 3D facial avatars must be animated easily,\\nrealistically, and directly from speech signals. While the best recent methods\\ngenerate 3D animations that are synchronized with the input audio, they largely\\nignore the impact of emotions on facial expressions. Realistic facial animation\\nrequires lip-sync together with the natural expression of emotion. To that end,\\nwe propose EMOTE (Expressive Model Optimized for Talking with Emotion), which\\ngenerates 3D talking-head avatars that maintain lip-sync from speech while\\nenabling explicit control over the expression of emotion. To achieve this, we\\nsupervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion.\\nThese losses are based on two key observations: (1) deformations of the face\\ndue to speech are spatially localized around the mouth and have high temporal\\nfrequency, whereas (2) facial expressions may deform the whole face and occur\\nover longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss\\nto preserve the speech-dependent content, while supervising emotion at the\\nsequence level. Furthermore, we employ a content-emotion exchange mechanism in\\norder to supervise different emotions on the same audio, while maintaining the\\nlip motion synchronized with the speech. To employ deep perceptual losses\\nwithout getting undesirable artifacts, we devise a motion prior in the form of\\na temporal VAE. Due to the absence of high-quality aligned emotional 3D face\\ndatasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted\\nfrom an emotional video dataset (i.e., MEAD). Extensive qualitative and\\nperceptual evaluations demonstrate that EMOTE produces speech-driven facial\\nanimations with better lip-sync than state-of-the-art methods trained on the\\nsame data, while offering additional, high-quality emotional control.\\n",
      "abstract": "To be widely adopted, 3D facial avatars must be animated easily,\\nrealistically, and directly from speech signals. While the best recent methods\\ngenerate 3D animations that are synchronized with the input audio, they largely\\nignore the impact of emotions on facial expressions. Realistic facial animation\\nrequires lip-sync together with the natural expression of emotion. To that end,\\nwe propose EMOTE (Expressive Model Optimized for Talking with Emotion), which\\ngenerates 3D talking-head avatars that maintain lip-sync from speech while\\nenabling explicit control over the expression of emotion. To achieve this, we\\nsupervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion.\\nThese losses are based on two key observations: (1) deformations of the face\\ndue to speech are spatially localized around the mouth and have high temporal\\nfrequency, whereas (2) facial expressions may deform the whole face and occur\\nover longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss\\nto preserve the speech-dependent content, while supervising emotion at the\\nsequence level. Furthermore, we employ a content-emotion exchange mechanism in\\norder to supervise different emotions on the same audio, while maintaining the\\nlip motion synchronized with the speech. To employ deep perceptual losses\\nwithout getting undesirable artifacts, we devise a motion prior in the form of\\na temporal VAE. Due to the absence of high-quality aligned emotional 3D face\\ndatasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted\\nfrom an emotional video dataset (i.e., MEAD). Extensive qualitative and\\nperceptual evaluations demonstrate that EMOTE produces speech-driven facial\\nanimations with better lip-sync than state-of-the-art methods trained on the\\nsame data, while offering additional, high-quality emotional control.\\n",
      "doi": "https://doi.org/10.1145/3610548.3618183",
      "openalex_id": "https://openalex.org/W4380994134",
      "arxiv_id": "",
      "publication_date": "2023-12-10",
      "published": "2023-12-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "JALI-Driven Expressive Facial Animation and Multilingual Speech in Cyberpunk 2077",
      "summary": "Cyberpunk 2077 is a highly anticipated massive open-world video game, with a complex, branching narrative. This talk details new research and innovative workflow contributions, developed by jali, toward the generation of an unprecedented number of hours of realistic, expressive speech animation in ten languages, often with multiple languages interleaved within individual sentences. The speech animation workflow is largely automatic but remains under animator control, using a combination of audio and tagged text transcripts. We use insights from anatomy, perception, and the psycho-linguistic literature to develop independent and combined language models that drive procedural animation of the mouth and paralingual (speech supportive non-verbal expression) motion of the neck, brows and eyes. Directorial tags in the speech transcript further enable the integration of performance capture driven facial emotion. The entire workflow is animator-centric, allowing efficient key-frame customization and editing of the resulting facial animation on any typical facs-like face rig. The talk will focus equally on technical contributions and its integration and creative use within the animation pipeline of the highly anticipated aaa game title: Cyberpunk 2077.",
      "abstract": "Cyberpunk 2077 is a highly anticipated massive open-world video game, with a complex, branching narrative. This talk details new research and innovative workflow contributions, developed by jali, toward the generation of an unprecedented number of hours of realistic, expressive speech animation in ten languages, often with multiple languages interleaved within individual sentences. The speech animation workflow is largely automatic but remains under animator control, using a combination of audio and tagged text transcripts. We use insights from anatomy, perception, and the psycho-linguistic literature to develop independent and combined language models that drive procedural animation of the mouth and paralingual (speech supportive non-verbal expression) motion of the neck, brows and eyes. Directorial tags in the speech transcript further enable the integration of performance capture driven facial emotion. The entire workflow is animator-centric, allowing efficient key-frame customization and editing of the resulting facial animation on any typical facs-like face rig. The talk will focus equally on technical contributions and its integration and creative use within the animation pipeline of the highly anticipated aaa game title: Cyberpunk 2077.",
      "doi": "https://doi.org/10.1145/3388767.3407339",
      "openalex_id": "https://openalex.org/W3080485275",
      "arxiv_id": "",
      "publication_date": "2020-08-17",
      "published": "2020-08-17",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers",
      "summary": "Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typically focus on learning phoneme-level features of short audio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, Face-Former, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes. To cope with the data scarcity issue, we integrate the self-supervised pre-trained speech representations. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy. The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code and the video are available at: https://evelynfan.github.io/audio2face/",
      "abstract": "Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typically focus on learning phoneme-level features of short audio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, Face-Former, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes. To cope with the data scarcity issue, we integrate the self-supervised pre-trained speech representations. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy. The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code and the video are available at: https://evelynfan.github.io/audio2face/",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.01821",
      "openalex_id": "https://openalex.org/W4200630629",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A 3-D Audio-Visual Corpus of Affective Communication",
      "summary": "Communication between humans deeply relies on the capability of expressing and recognizing feelings. For this reason, research on human-machine interaction needs to focus on the recognition and simulation of emotional states, prerequisite of which is the collection of affective corpora. Currently available datasets still represent a bottleneck for the difficulties arising during the acquisition and labeling of affective data. In this work, we present a new audio-visual corpus for possibly the two most important modalities used by humans to communicate their emotional states, namely speech and facial expression in the form of dense dynamic 3-D face geometries. We acquire high-quality data by working in a controlled environment and resort to video clips to induce affective states. The annotation of the speech signal includes: transcription of the corpus text into the phonological representation, accurate phone segmentation, fundamental frequency extraction, and signal intensity estimation of the speech signals. We employ a real-time 3-D scanner to acquire dense dynamic facial geometries and track the faces throughout the sequences, achieving full spatial and temporal correspondences. The corpus is a valuable tool for applications like affective visual speech synthesis or view-independent facial expression recognition.",
      "abstract": "Communication between humans deeply relies on the capability of expressing and recognizing feelings. For this reason, research on human-machine interaction needs to focus on the recognition and simulation of emotional states, prerequisite of which is the collection of affective corpora. Currently available datasets still represent a bottleneck for the difficulties arising during the acquisition and labeling of affective data. In this work, we present a new audio-visual corpus for possibly the two most important modalities used by humans to communicate their emotional states, namely speech and facial expression in the form of dense dynamic 3-D face geometries. We acquire high-quality data by working in a controlled environment and resort to video clips to induce affective states. The annotation of the speech signal includes: transcription of the corpus text into the phonological representation, accurate phone segmentation, fundamental frequency extraction, and signal intensity estimation of the speech signals. We employ a real-time 3-D scanner to acquire dense dynamic facial geometries and track the faces throughout the sequences, achieving full spatial and temporal correspondences. The corpus is a valuable tool for applications like affective visual speech synthesis or view-independent facial expression recognition.",
      "doi": "https://doi.org/10.1109/tmm.2010.2052239",
      "openalex_id": "https://openalex.org/W2154961933",
      "arxiv_id": "",
      "publication_date": "2010-09-15",
      "published": "2010-09-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning an animatable detailed 3D face model from in-the-wild images",
      "summary": "While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.",
      "abstract": "While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.",
      "doi": "https://doi.org/10.1145/3450626.3459936",
      "openalex_id": "https://openalex.org/W3180794345",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\\n Prediction of Hidden Units",
      "summary": "Self-supervised approaches for speech representation learning are challenged\\nby three unique problems: (1) there are multiple sound units in each input\\nutterance, (2) there is no lexicon of input sound units during the pre-training\\nphase, and (3) sound units have variable lengths with no explicit segmentation.\\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\\napproach for self-supervised speech representation learning, which utilizes an\\noffline clustering step to provide aligned target labels for a BERT-like\\nprediction loss. A key ingredient of our approach is applying the prediction\\nloss over the masked regions only, which forces the model to learn a combined\\nacoustic and language model over the continuous inputs. HuBERT relies primarily\\non the consistency of the unsupervised clustering step rather than the\\nintrinsic quality of the assigned cluster labels. Starting with a simple\\nk-means teacher of 100 clusters, and using two iterations of clustering, the\\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\\ndev-other and test-other evaluation subsets.\\n",
      "abstract": "Self-supervised approaches for speech representation learning are challenged\\nby three unique problems: (1) there are multiple sound units in each input\\nutterance, (2) there is no lexicon of input sound units during the pre-training\\nphase, and (3) sound units have variable lengths with no explicit segmentation.\\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\\napproach for self-supervised speech representation learning, which utilizes an\\noffline clustering step to provide aligned target labels for a BERT-like\\nprediction loss. A key ingredient of our approach is applying the prediction\\nloss over the masked regions only, which forces the model to learn a combined\\nacoustic and language model over the continuous inputs. HuBERT relies primarily\\non the consistency of the unsupervised clustering step rather than the\\nintrinsic quality of the assigned cluster labels. Starting with a simple\\nk-means teacher of 100 clusters, and using two iterations of clustering, the\\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\\ndev-other and test-other evaluation subsets.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2106.07447",
      "openalex_id": "https://openalex.org/W4287119707",
      "arxiv_id": "",
      "publication_date": "2021-06-14",
      "published": "2021-06-14",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model",
      "summary": "Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate one-shot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zero- and first-order key-points motion. Then through exploring the motion model's properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.",
      "abstract": "Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate one-shot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zero- and first-order key-points motion. Then through exploring the motion model's properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.",
      "doi": "https://doi.org/10.1145/3528233.3530745",
      "openalex_id": "https://openalex.org/W4281730245",
      "arxiv_id": "",
      "publication_date": "2022-07-20",
      "published": "2022-07-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
      "summary": "We present a machine learning technique for driving 3D facial animation by audio input in real time and with low latency. Our deep neural network learns a mapping from input waveforms to the 3D vertex coordinates of a face model, and simultaneously discovers a compact, latent code that disambiguates the variations in facial expression that cannot be explained by the audio alone. During inference, the latent code can be used as an intuitive control for the emotional state of the face puppet. We train our network with 3--5 minutes of high-quality animation data obtained using traditional, vision-based performance capture methods. Even though our primary goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language, as we demonstrate with a user study. The results are applicable to in-game dialogue, low-cost localization, virtual reality avatars, and telepresence.",
      "abstract": "We present a machine learning technique for driving 3D facial animation by audio input in real time and with low latency. Our deep neural network learns a mapping from input waveforms to the 3D vertex coordinates of a face model, and simultaneously discovers a compact, latent code that disambiguates the variations in facial expression that cannot be explained by the audio alone. During inference, the latent code can be used as an intuitive control for the emotional state of the face puppet. We train our network with 3--5 minutes of high-quality animation data obtained using traditional, vision-based performance capture methods. Even though our primary goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language, as we demonstrate with a user study. The results are applicable to in-game dialogue, low-cost localization, virtual reality avatars, and telepresence.",
      "doi": "https://doi.org/10.1145/3072959.3073658",
      "openalex_id": "https://openalex.org/W2739192055",
      "arxiv_id": "",
      "publication_date": "2017-07-20",
      "published": "2017-07-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning a model of facial shape and expression from 4D scans",
      "summary": "The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).",
      "abstract": "The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).",
      "doi": "https://doi.org/10.1145/3130800.3130813",
      "openalex_id": "https://openalex.org/W2769666294",
      "arxiv_id": "",
      "publication_date": "2017-11-20",
      "published": "2017-11-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep appearance models for face rendering",
      "summary": "We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).",
      "abstract": "We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).",
      "doi": "https://doi.org/10.1145/3197517.3201401",
      "openalex_id": "https://openalex.org/W2811426698",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion",
      "summary": "We present a framework for modeling interactional communication in dyadic conversations: given multimodal inputs of a speaker, we autoregressively output multiple possibilities of corresponding listener motion. We combine the motion and speech audio of the speaker using a motion-audio cross attention transformer. Furthermore, we enable non-deterministic prediction by learning a discrete latent representation of realistic listener motion with a novel motion-encoding VQ-VAE. Our method organically captures the multimodal and non-deterministic nature of nonverbal dyadic interactions. Moreover, it produces realistic 3D listener facial motion synchronous with the speaker (see video). We demonstrate that our method outperforms baselines qualitatively and quantitatively via a rich suite of experiments. To facilitate this line of research, we introduce a novel and large in-the-wild dataset of dyadic conversations. Code, data, and videos available at https://evonneng.github.io/learning2listen/",
      "abstract": "We present a framework for modeling interactional communication in dyadic conversations: given multimodal inputs of a speaker, we autoregressively output multiple possibilities of corresponding listener motion. We combine the motion and speech audio of the speaker using a motion-audio cross attention transformer. Furthermore, we enable non-deterministic prediction by learning a discrete latent representation of realistic listener motion with a novel motion-encoding VQ-VAE. Our method organically captures the multimodal and non-deterministic nature of nonverbal dyadic interactions. Moreover, it produces realistic 3D listener facial motion synchronous with the speaker (see video). We demonstrate that our method outperforms baselines qualitatively and quantitatively via a rich suite of experiments. To facilitate this line of research, we introduce a novel and large in-the-wild dataset of dyadic conversations. Code, data, and videos available at https://evonneng.github.io/learning2listen/",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.01975",
      "openalex_id": "https://openalex.org/W4312309978",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
      "summary": "Speech-driven 3D face animation aims to generate realistic facial expressions that match the speech content and emotion. However, existing methods often neglect emotional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emotions in speech so as to generate rich 3D facial expressions. Specifically, we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face with enhanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to generate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate that our approach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watching the supplementary video: https://ziqiaopeng.github.io/emotalk",
      "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions that match the speech content and emotion. However, existing methods often neglect emotional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emotions in speech so as to generate rich 3D facial expressions. Specifically, we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face with enhanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to generate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate that our approach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watching the supplementary video: https://ziqiaopeng.github.io/emotalk",
      "doi": "https://doi.org/10.1109/iccv51070.2023.01891",
      "openalex_id": "https://openalex.org/W4390872742",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model",
      "summary": "We propose a simple and novel method for generating 3D human motion from complex natural language sentences, which describe different velocity, direction and composition of all kinds of actions. Different from existing methods that use classical generative architecture, we apply the Denoising Diffusion Probabilistic Model to this task, synthesizing diverse motion results under the guidance of texts. The diffusion model converts white noise into structured 3D motion by a Markov process with a series of denoising steps and is efficiently trained by optimizing a variational lower bound. To achieve the goal of text-conditioned image synthesis, we use the classifier-free guidance strategy to add text embedding into the model during training. Our experiments demonstrate that our model achieves competitive results on HumanML3D test set quantitatively and can generate more visually natural and diverse examples. We also show with experiments that our model is capable of zero-shot generation of motions for unseen text guidance.",
      "abstract": "We propose a simple and novel method for generating 3D human motion from complex natural language sentences, which describe different velocity, direction and composition of all kinds of actions. Different from existing methods that use classical generative architecture, we apply the Denoising Diffusion Probabilistic Model to this task, synthesizing diverse motion results under the guidance of texts. The diffusion model converts white noise into structured 3D motion by a Markov process with a series of denoising steps and is efficiently trained by optimizing a variational lower bound. To achieve the goal of text-conditioned image synthesis, we use the classifier-free guidance strategy to add text embedding into the model during training. Our experiments demonstrate that our model achieves competitive results on HumanML3D test set quantitatively and can generate more visually natural and diverse examples. We also show with experiments that our model is capable of zero-shot generation of motions for unseen text guidance.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096441",
      "openalex_id": "https://openalex.org/W4372341521",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement",
      "summary": "This paper presents a generic method for generating full facial 3D animation from speech. Existing approaches to audio-driven facial animation exhibit uncanny or static upper face animation, fail to produce accurate and plausible co-articulation or rely on person-specific models that limit their scalability. To improve upon existing models, we propose a generic audio-driven facial animation approach that achieves highly realistic motion synthesis results for the entire face. At the core of our approach is a categorical latent space for facial animation that disentangles audio-correlated and audio-uncorrelated information based on a novel cross-modality loss. Our approach ensures highly accurate lip motion, while also synthesizing plausible animation of the parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. We demonstrate that our approach outperforms several baselines and obtains state-of-the-art quality both qualitatively and quantitatively. A perceptual user study demonstrates that our approach is deemed more realistic than the current state-of-the-art in over 75% of cases. We recommend watching the supplemental video before reading the paper: https://github.com/facebookresearch/meshtalk",
      "abstract": "This paper presents a generic method for generating full facial 3D animation from speech. Existing approaches to audio-driven facial animation exhibit uncanny or static upper face animation, fail to produce accurate and plausible co-articulation or rely on person-specific models that limit their scalability. To improve upon existing models, we propose a generic audio-driven facial animation approach that achieves highly realistic motion synthesis results for the entire face. At the core of our approach is a categorical latent space for facial animation that disentangles audio-correlated and audio-uncorrelated information based on a novel cross-modality loss. Our approach ensures highly accurate lip motion, while also synthesizing plausible animation of the parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. We demonstrate that our approach outperforms several baselines and obtains state-of-the-art quality both qualitatively and quantitatively. A perceptual user study demonstrates that our approach is deemed more realistic than the current state-of-the-art in over 75% of cases. We recommend watching the supplemental video before reading the paper: https://github.com/facebookresearch/meshtalk",
      "doi": "https://doi.org/10.1109/iccv48922.2021.00121",
      "openalex_id": "https://openalex.org/W3154411171",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation",
      "summary": "Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/wacv57701.2024.00502",
      "openalex_id": "https://openalex.org/W4394597549",
      "arxiv_id": "",
      "publication_date": "2024-01-03",
      "published": "2024-01-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models",
      "summary": "The generation of stylistic 3D facial animations driven by speech presents a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. In particular, our style includes the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset are at https://diffposetalk.github.io.",
      "abstract": "The generation of stylistic 3D facial animations driven by speech presents a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. In particular, our style includes the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset are at https://diffposetalk.github.io.",
      "doi": "https://doi.org/10.1145/3658221",
      "openalex_id": "https://openalex.org/W4400818936",
      "arxiv_id": "",
      "publication_date": "2024-07-19",
      "published": "2024-07-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A deep learning approach for generalized speech animation",
      "summary": "We introduce a simple and effective deep learning approach to automatically generate natural looking speech animation that synchronizes to input speech. Our approach uses a sliding window predictor that learns arbitrary nonlinear mappings from phoneme label input sequences to mouth movements in a way that accurately captures natural motion and visual coarticulation effects. Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches. One important focus of our work is to develop an effective approach for speech animation that can be easily integrated into existing production pipelines. We provide a detailed description of our end-to-end approach, including machine learning design decisions. Generalized speech animation results are demonstrated over a wide range of animation clips on a variety of characters and voices, including singing and foreign language input. Our approach can also generate on-demand speech animation in real-time from user speech input.",
      "abstract": "We introduce a simple and effective deep learning approach to automatically generate natural looking speech animation that synchronizes to input speech. Our approach uses a sliding window predictor that learns arbitrary nonlinear mappings from phoneme label input sequences to mouth movements in a way that accurately captures natural motion and visual coarticulation effects. Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches. One important focus of our work is to develop an effective approach for speech animation that can be easily integrated into existing production pipelines. We provide a detailed description of our end-to-end approach, including machine learning design decisions. Generalized speech animation results are demonstrated over a wide range of animation clips on a variety of characters and voices, including singing and foreign language input. Our approach can also generate on-demand speech animation in real-time from user speech input.",
      "doi": "https://doi.org/10.1145/3072959.3073699",
      "openalex_id": "https://openalex.org/W2737658251",
      "arxiv_id": "",
      "publication_date": "2017-07-20",
      "published": "2017-07-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generating Holistic 3D Human Motion from Speech",
      "summary": "This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ- VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de/.",
      "abstract": "This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ- VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de/.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.00053",
      "openalex_id": "https://openalex.org/W4386075984",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation",
      "summary": "Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> The code and demo videos are available at https://sadtalker.github.io.",
      "abstract": "Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> The code and demo videos are available at https://sadtalker.github.io.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.00836",
      "openalex_id": "https://openalex.org/W4386072021",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset",
      "summary": "One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driving audio and arbitrary single face image as the source. Current works fail to generate over 256×256 resolution realistic-looking videos due to the lack of an appropriate high-resolution audio-visual dataset, and the limitation of the sparse facial landmarks in providing poor expression details. To synthesize high-definition videos, we build a large in-the-wild high-resolution audio-visual dataset and propose a novel flow-guided talking face generation framework. The new dataset is collected from youtube and consists of about 16 hours 720P or 1080P videos. We leverage the facial 3D morphable model (3DMM) to split the framework into two cascaded modules instead of learning a direct mapping from audio to video. In the first module, we propose a novel animation generator to produce the movements of mouth, eyebrow and head pose simultaneously. In the second module, we transform animation into dense flow to provide more expression details and carefully design a novel flow-guided video generator to synthesize videos. Our method is able to produce high-definition videos and outperforms state-of-the-art works in objective and subjective comparisons <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*</sup> .",
      "abstract": "One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driving audio and arbitrary single face image as the source. Current works fail to generate over 256×256 resolution realistic-looking videos due to the lack of an appropriate high-resolution audio-visual dataset, and the limitation of the sparse facial landmarks in providing poor expression details. To synthesize high-definition videos, we build a large in-the-wild high-resolution audio-visual dataset and propose a novel flow-guided talking face generation framework. The new dataset is collected from youtube and consists of about 16 hours 720P or 1080P videos. We leverage the facial 3D morphable model (3DMM) to split the framework into two cascaded modules instead of learning a direct mapping from audio to video. In the first module, we propose a novel animation generator to produce the movements of mouth, eyebrow and head pose simultaneously. In the second module, we transform animation into dense flow to provide more expression details and carefully design a novel flow-guided video generator to synthesize videos. Our method is able to produce high-definition videos and outperforms state-of-the-art works in objective and subjective comparisons <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*</sup> .",
      "doi": "https://doi.org/10.1109/cvpr46437.2021.00366",
      "openalex_id": "https://openalex.org/W3197199219",
      "arxiv_id": "",
      "publication_date": "2021-06-01",
      "published": "2021-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance",
      "summary": "The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of flexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotion and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.",
      "abstract": "The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of flexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotion and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.",
      "doi": "https://doi.org/10.1145/3641519.3657413",
      "openalex_id": "https://openalex.org/W4400582137",
      "arxiv_id": "",
      "publication_date": "2024-07-12",
      "published": "2024-07-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Human Motion Generation: A Survey",
      "summary": "Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications. Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation. Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts. While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals. In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field. We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation. Additionally, we provide an overview of common datasets and evaluation metrics. Lastly, we discuss open problems and outline potential future research directions. We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.",
      "abstract": "Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications. Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation. Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts. While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals. In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field. We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation. Additionally, we provide an overview of common datasets and evaluation metrics. Lastly, we discuss open problems and outline potential future research directions. We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.",
      "doi": "https://doi.org/10.1109/tpami.2023.3330935",
      "openalex_id": "https://openalex.org/W4388505252",
      "arxiv_id": "",
      "publication_date": "2023-11-08",
      "published": "2023-11-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone",
      "summary": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "abstract": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "doi": "https://doi.org/10.48550/arxiv.2112.02418",
      "openalex_id": "https://openalex.org/W4200631896",
      "arxiv_id": "",
      "publication_date": "2021-12-04",
      "published": "2021-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
      "summary": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2022-489",
      "openalex_id": "https://openalex.org/W4226132755",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "summary": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "abstract": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854049",
      "openalex_id": "https://openalex.org/W2085628288",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
      "summary": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "abstract": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "doi": "https://doi.org/10.48550/arxiv.2111.14822",
      "openalex_id": "https://openalex.org/W3217345456",
      "arxiv_id": "",
      "publication_date": "2021-11-29",
      "published": "2021-11-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "doi": "https://doi.org/10.48550/arxiv.2005.08100",
      "openalex_id": "https://openalex.org/W3025165719",
      "arxiv_id": "",
      "publication_date": "2020-05-16",
      "published": "2020-05-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
      "summary": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "doi": "https://doi.org/10.48550/arxiv.2005.11129",
      "openalex_id": "https://openalex.org/W3026874504",
      "arxiv_id": "",
      "publication_date": "2020-05-22",
      "published": "2020-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
      "summary": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "doi": "https://doi.org/10.48550/arxiv.2105.06337",
      "openalex_id": "https://openalex.org/W3162673269",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition",
      "summary": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461375",
      "openalex_id": "https://openalex.org/W2890964092",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
      "summary": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02584",
      "openalex_id": "https://openalex.org/W3204550533",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
      "summary": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "doi": "https://doi.org/10.21437/interspeech.2022-245",
      "openalex_id": "https://openalex.org/W4283722828",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ESPnet: End-to-End Speech Processing Toolkit",
      "summary": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1456",
      "openalex_id": "https://openalex.org/W2962780374",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
      "summary": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2441",
      "openalex_id": "https://openalex.org/W2972359262",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
      "summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "doi": "https://doi.org/10.48550/arxiv.2005.05957",
      "openalex_id": "https://openalex.org/W3025528898",
      "arxiv_id": "",
      "publication_date": "2020-05-12",
      "published": "2020-05-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
      "summary": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "doi": "https://doi.org/10.48550/arxiv.2006.04558",
      "openalex_id": "https://openalex.org/W3033411150",
      "arxiv_id": "",
      "publication_date": "2020-06-08",
      "published": "2020-06-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "summary": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1910.05453",
      "openalex_id": "https://openalex.org/W2979476256",
      "arxiv_id": "",
      "publication_date": "2019-10-12",
      "published": "2019-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "summary": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2006.11477",
      "openalex_id": "https://openalex.org/W3036601975",
      "arxiv_id": "",
      "publication_date": "2020-06-20",
      "published": "2020-06-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adam: A Method for Stochastic Optimization",
      "summary": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "doi": "https://doi.org/10.48550/arxiv.1412.6980",
      "openalex_id": "https://openalex.org/W1522301498",
      "arxiv_id": "",
      "publication_date": "2014-12-22",
      "published": "2014-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AudioLM: A Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "doi": "https://doi.org/10.1109/taslp.2023.3288409",
      "openalex_id": "https://openalex.org/W4381786045",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
      "summary": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "doi": "https://doi.org/10.48550/arxiv.2304.09116",
      "openalex_id": "https://openalex.org/W4366460484",
      "arxiv_id": "",
      "publication_date": "2023-04-18",
      "published": "2023-04-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "summary": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688253",
      "openalex_id": "https://openalex.org/W4226033575",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "textless-lib: a Library for Textless Spoken Language Processing",
      "summary": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "abstract": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-demo.1",
      "openalex_id": "https://openalex.org/W4287887366",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
      "summary": "Unsupervised models of representations based on Contrastive Predictive Coding\\n(CPC)[1] are primarily used in spoken language modelling in that they encode\\nphonetic information. In this study, we ask what other types of information are\\npresent in CPC speech representations. We focus on three categories: phone\\nclass, gender and language, and compare monolingual and bilingual models. Using\\nqualitative and quantitative tools, we find that both gender and phone class\\ninformation are present in both types of models. Language information, however,\\nis very salient in the bilingual model only, suggesting CPC models learn to\\ndiscriminate languages when trained on multiple languages. Some language\\ninformation can also be retrieved from monolingual models, but it is more\\ndiffused across all features. These patterns hold when analyses are carried on\\nthe discrete units from a downstream clustering model. However, although there\\nis no effect of the number of target clusters on phone class and language\\ninformation, more gender information is encoded with more clusters. Finally, we\\nfind that there is some cost to being exposed to two languages on a downstream\\nphoneme discrimination task.\\n",
      "abstract": "Unsupervised models of representations based on Contrastive Predictive Coding\\n(CPC)[1] are primarily used in spoken language modelling in that they encode\\nphonetic information. In this study, we ask what other types of information are\\npresent in CPC speech representations. We focus on three categories: phone\\nclass, gender and language, and compare monolingual and bilingual models. Using\\nqualitative and quantitative tools, we find that both gender and phone class\\ninformation are present in both types of models. Language information, however,\\nis very salient in the bilingual model only, suggesting CPC models learn to\\ndiscriminate languages when trained on multiple languages. Some language\\ninformation can also be retrieved from monolingual models, but it is more\\ndiffused across all features. These patterns hold when analyses are carried on\\nthe discrete units from a downstream clustering model. However, although there\\nis no effect of the number of target clusters on phone class and language\\ninformation, more gender information is encoded with more clusters. Finally, we\\nfind that there is some cost to being exposed to two languages on a downstream\\nphoneme discrimination task.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2022-373",
      "openalex_id": "https://openalex.org/W4225726571",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Voronoi diagrams—a survey of a fundamental geometric data structure",
      "summary": "article Free Access Share on Voronoi diagrams—a survey of a fundamental geometric data structure Author: Franz Aurenhammer Technische Univ. Graz, Schiebbstattgasse, Austria Technische Univ. Graz, Schiebbstattgasse, AustriaView Profile Authors Info & Claims ACM Computing SurveysVolume 23Issue 3Sept. 1991 pp 345–405https://doi.org/10.1145/116873.116880Published:01 September 1991Publication History 2,817citation20,462DownloadsMetricsTotal Citations2,817Total Downloads20,462Last 12 Months1,262Last 6 weeks220 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "abstract": "article Free Access Share on Voronoi diagrams—a survey of a fundamental geometric data structure Author: Franz Aurenhammer Technische Univ. Graz, Schiebbstattgasse, Austria Technische Univ. Graz, Schiebbstattgasse, AustriaView Profile Authors Info & Claims ACM Computing SurveysVolume 23Issue 3Sept. 1991 pp 345–405https://doi.org/10.1145/116873.116880Published:01 September 1991Publication History 2,817citation20,462DownloadsMetricsTotal Citations2,817Total Downloads20,462Last 12 Months1,262Last 6 weeks220 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "doi": "https://doi.org/10.1145/116873.116880",
      "openalex_id": "https://openalex.org/W1967005434",
      "arxiv_id": "",
      "publication_date": "1991-09-01",
      "published": "1991-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
      "summary": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.",
      "abstract": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10884",
      "openalex_id": "https://openalex.org/W4297841405",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
      "summary": "We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
      "abstract": "We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2138615112",
      "arxiv_id": "",
      "publication_date": "2007-06-01",
      "published": "2007-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
      "summary": "This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training.Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc.A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription.For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h.For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%.The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality.Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.",
      "abstract": "This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training.Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc.A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription.For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h.For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%.The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality.Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1965",
      "openalex_id": "https://openalex.org/W3198694222",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "summary": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
      "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
      "doi": "https://doi.org/10.48550/arxiv.2205.14135",
      "openalex_id": "https://openalex.org/W4281758439",
      "arxiv_id": "",
      "publication_date": "2022-05-27",
      "published": "2022-05-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechCraft: A Fine-Grained Expressive Speech Dataset with Natural Language Description",
      "summary": "Speech-language multi-modal learning presents a significant challenge due to\\nthe fine nuanced information inherent in speech styles. Therefore, a\\nlarge-scale dataset providing elaborate comprehension of speech style is\\nurgently needed to facilitate insightful interplay between speech audio and\\nnatural language. However, constructing such datasets presents a major\\ntrade-off between large-scale data collection and high-quality annotation. To\\ntackle this challenge, we propose an automatic speech annotation system for\\nexpressiveness interpretation that annotates in-the-wild speech clips with\\nexpressive and vivid human language descriptions. Initially, speech audios are\\nprocessed by a series of expert classifiers and captioning models to capture\\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\\nannotation generation. Unlike previous tag/templet-based annotation frameworks\\nwith limited information and diversity, our system provides in-depth\\nunderstandings of speech style through tailored natural language descriptions,\\nthereby enabling accurate and voluminous data generation for large model\\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\\nexpressive speech dataset. It is distinguished by highly descriptive natural\\nlanguage style prompts, containing approximately 2,000 hours of audio data and\\nencompassing over two million speech clips. Extensive experiments demonstrate\\nthat the proposed dataset significantly boosts speech-language task performance\\nin stylist speech synthesis and speech style understanding.\\n",
      "abstract": "Speech-language multi-modal learning presents a significant challenge due to\\nthe fine nuanced information inherent in speech styles. Therefore, a\\nlarge-scale dataset providing elaborate comprehension of speech style is\\nurgently needed to facilitate insightful interplay between speech audio and\\nnatural language. However, constructing such datasets presents a major\\ntrade-off between large-scale data collection and high-quality annotation. To\\ntackle this challenge, we propose an automatic speech annotation system for\\nexpressiveness interpretation that annotates in-the-wild speech clips with\\nexpressive and vivid human language descriptions. Initially, speech audios are\\nprocessed by a series of expert classifiers and captioning models to capture\\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\\nannotation generation. Unlike previous tag/templet-based annotation frameworks\\nwith limited information and diversity, our system provides in-depth\\nunderstandings of speech style through tailored natural language descriptions,\\nthereby enabling accurate and voluminous data generation for large model\\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\\nexpressive speech dataset. It is distinguished by highly descriptive natural\\nlanguage style prompts, containing approximately 2,000 hours of audio data and\\nencompassing over two million speech clips. Extensive experiments demonstrate\\nthat the proposed dataset significantly boosts speech-language task performance\\nin stylist speech synthesis and speech style understanding.\\n",
      "doi": "https://doi.org/10.1145/3664647.3681674",
      "openalex_id": "https://openalex.org/W4402703639",
      "arxiv_id": "",
      "publication_date": "2024-10-26",
      "published": "2024-10-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody",
      "summary": "Disentangled speech representation learning aims to separate different factors of variation from speech into disjoint representations. This paper focuses on disentangling speech into representations for three factors: spoken content, speaker timbre, and speech prosody. Many previous methods for speech disentanglement have focused on separating spoken content and speaker timbre. However, the lack of explicit modeling of prosodic information leads to degraded speech generation performance and uncontrollable prosody leakage into content and/or speaker representations. While some recent methods have utilized explicit speaker labels or pre-trained models to facilitate triple-factor disentanglement, there are no end-to-end methods to simultaneously disentangle three factors using only unsupervised or self-supervised learning objectives. This paper introduces SpeechTripleNet, an end-to-end method to disentangle speech into representations for content, timbre, and prosody. Based on VAE, SpeechTripleNet restricts the structures of the latent variables and the amount of information captured in them to induce disentanglement. It is a pure unsupervised/self-supervised learning method that only requires speech data and no additional labels. Our qualitative and quantitative results demonstrate that SpeechTripleNet is effective in achieving triple-factor speech disentanglement, as well as controllable speech editing concerning different factors.",
      "abstract": "Disentangled speech representation learning aims to separate different factors of variation from speech into disjoint representations. This paper focuses on disentangling speech into representations for three factors: spoken content, speaker timbre, and speech prosody. Many previous methods for speech disentanglement have focused on separating spoken content and speaker timbre. However, the lack of explicit modeling of prosodic information leads to degraded speech generation performance and uncontrollable prosody leakage into content and/or speaker representations. While some recent methods have utilized explicit speaker labels or pre-trained models to facilitate triple-factor disentanglement, there are no end-to-end methods to simultaneously disentangle three factors using only unsupervised or self-supervised learning objectives. This paper introduces SpeechTripleNet, an end-to-end method to disentangle speech into representations for content, timbre, and prosody. Based on VAE, SpeechTripleNet restricts the structures of the latent variables and the amount of information captured in them to induce disentanglement. It is a pure unsupervised/self-supervised learning method that only requires speech data and no additional labels. Our qualitative and quantitative results demonstrate that SpeechTripleNet is effective in achieving triple-factor speech disentanglement, as well as controllable speech editing concerning different factors.",
      "doi": "https://doi.org/10.1145/3581783.3612485",
      "openalex_id": "https://openalex.org/W4388233464",
      "arxiv_id": "",
      "publication_date": "2023-10-26",
      "published": "2023-10-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-To-Speech Using Natural Language Descriptions",
      "summary": "We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic features of diverse speakers. Our subjective evaluation results show that the proposed method can better control speaker characteristics than the methods without the speaker prompt. Audio samples are available at https://reppy4620.github.io/demo.promptttspp/.",
      "abstract": "We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic features of diverse speakers. Our subjective evaluation results show that the proposed method can better control speaker characteristics than the methods without the speaker prompt. Audio samples are available at https://reppy4620.github.io/demo.promptttspp/.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448173",
      "openalex_id": "https://openalex.org/W4392908903",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WENETSPEECH: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition",
      "summary": "In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation – Dev for cross-validation purpose in training, Test_Net, collected from Internet for matched test, and Test_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.",
      "abstract": "In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation – Dev for cross-validation purpose in training, Test_Net, collected from Internet for matched test, and Test_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746682",
      "openalex_id": "https://openalex.org/W3203407300",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Proceedings of the 24th international conference on Machine learning",
      "summary": "This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Schölkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",
      "abstract": "This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Schölkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",
      "doi": "https://doi.org/10.1145/1273496",
      "openalex_id": "https://openalex.org/W1583837637",
      "arxiv_id": "",
      "publication_date": "2007-06-20",
      "published": "2007-06-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "An audiovisual and contextual approach for categorical and continuous\\n emotion recognition in-the-wild",
      "summary": "In this work we tackle the task of video-based audio-visual emotion\\nrecognition, within the premises of the 2nd Workshop and Competition on\\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\\nhead/body orientation and low image resolution constitute factors that can\\npotentially hinder performance in case of methodologies that solely rely on the\\nextraction and analysis of facial features. In order to alleviate this problem,\\nwe leverage both bodily and contextual features, as part of a broader emotion\\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\\nApart from learning through the RGB input modality, we construct an aural\\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\\nof the human body and scene context, as aspects of the emotion recognition\\nprocess that have been left relatively unexplored up to this point. All the\\ncode was implemented using PyTorch and is publicly available.\\n",
      "abstract": "In this work we tackle the task of video-based audio-visual emotion\\nrecognition, within the premises of the 2nd Workshop and Competition on\\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\\nhead/body orientation and low image resolution constitute factors that can\\npotentially hinder performance in case of methodologies that solely rely on the\\nextraction and analysis of facial features. In order to alleviate this problem,\\nwe leverage both bodily and contextual features, as part of a broader emotion\\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\\nApart from learning through the RGB input modality, we construct an aural\\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\\nof the human body and scene context, as aspects of the emotion recognition\\nprocess that have been left relatively unexplored up to this point. All the\\ncode was implemented using PyTorch and is publicly available.\\n",
      "doi": "https://doi.org/10.1109/iccvw54120.2021.00407",
      "openalex_id": "https://openalex.org/W3209058072",
      "arxiv_id": "",
      "publication_date": "2021-07-07",
      "published": "2021-07-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "summary": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "doi": "https://doi.org/10.48550/arxiv.2202.03555",
      "openalex_id": "https://openalex.org/W4221145109",
      "arxiv_id": "",
      "publication_date": "2022-02-07",
      "published": "2022-02-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "How Far are We from Solving the 2D &amp; 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)",
      "summary": "This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from https://www.adrianbulat.com/face-alignment/\\n",
      "abstract": "This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from https://www.adrianbulat.com/face-alignment/\\n",
      "doi": "https://doi.org/10.1109/iccv.2017.116",
      "openalex_id": "https://openalex.org/W2949662773",
      "arxiv_id": "",
      "publication_date": "2017-10-01",
      "published": "2017-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "summary": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "doi": "https://doi.org/10.48550/arxiv.2002.05709",
      "openalex_id": "https://openalex.org/W3005680577",
      "arxiv_id": "",
      "publication_date": "2020-02-13",
      "published": "2020-02-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "summary": "In this paper, we consider the problem of multi-modal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.",
      "abstract": "In this paper, we consider the problem of multi-modal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.",
      "doi": "https://doi.org/10.1109/icpr56361.2022.9956592",
      "openalex_id": "https://openalex.org/W4312292725",
      "arxiv_id": "",
      "publication_date": "2022-08-21",
      "published": "2022-08-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "doi": "https://doi.org/10.48550/arxiv.1810.04805",
      "openalex_id": "https://openalex.org/W2896457183",
      "arxiv_id": "",
      "publication_date": "2018-10-11",
      "published": "2018-10-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Co-Separating Sounds of Visual Objects",
      "summary": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
      "abstract": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
      "doi": "https://doi.org/10.1109/iccv.2019.00398",
      "openalex_id": "https://openalex.org/W2988200020",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal Attention-Mechanism For Temporal Emotion Recognition",
      "summary": "Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition.",
      "abstract": "Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition.",
      "doi": "https://doi.org/10.1109/icip40778.2020.9191019",
      "openalex_id": "https://openalex.org/W3090471441",
      "arxiv_id": "",
      "publication_date": "2020-09-30",
      "published": "2020-09-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
      "summary": "In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-art performance on both datasets, and showing a significant impact of multimodal and temporal emotion perception.",
      "abstract": "In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-art performance on both datasets, and showing a significant impact of multimodal and temporal emotion perception.",
      "doi": "https://doi.org/10.1109/acii.2019.8925444",
      "openalex_id": "https://openalex.org/W2977259558",
      "arxiv_id": "",
      "publication_date": "2019-09-01",
      "published": "2019-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "summary": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
      "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",
      "doi": "https://doi.org/10.48550/arxiv.1803.07728",
      "openalex_id": "https://openalex.org/W2785325870",
      "arxiv_id": "",
      "publication_date": "2018-03-21",
      "published": "2018-03-21",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Robust Audiovisual Emotion Recognition: Aligning Modalities, Capturing Temporal Information, and Handling Missing Features",
      "summary": "Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.",
      "abstract": "Emotion recognition using audiovisual features is a challenging task for human-machine interaction systems. Under ideal conditions (perfect illumination, clean speech signals, and non-occluded visual data) many systems are able to achieve reliable results. However, few studies have considered developing multimodal systems and training strategies to build systems that can perform well under non ideal conditions. Audiovisual models still face challenging problems such as misalignment of modalities, lack of temporal modeling, and missing features due to noise or occlusions. In this article, we implement a model that combines auxiliary networks, a transformer architecture, and an optimized training mechanism to achieve a robust system for audiovisual emotion recognition that addresses, in a principled way, these challenges. Our evaluation analyzes how well this model performs in ideal conditions and when modalities are missing. We contrast this method with other multimodal fusion methods for emotion recognition. Our experimental results based on two audiovisual databases demonstrate that the proposed framework achieves: 1) improvements in emotion recognition accuracy, 2) better alignment and fusion of audiovisual features at the model level, 3) awareness of temporal information, and 4) robustness to non-ideal scenarios.",
      "doi": "https://doi.org/10.1109/taffc.2022.3216993",
      "openalex_id": "https://openalex.org/W4312976151",
      "arxiv_id": "",
      "publication_date": "2022-10-01",
      "published": "2022-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Gradient Descent Ascent for Minimax Problems on Riemannian Manifolds",
      "summary": "In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of O(κ<sup>2</sup>ϵ<sup>-2</sup>) for finding an ϵ-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where κ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of O(κ<sup>4</sup>ϵ<sup>-4</sup>) for finding an ϵ-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of ~O(κ<sup>4</sup>ϵ<sup>-3</sup>) in searching for an ϵ-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.",
      "abstract": "In the paper, we study a class of useful minimax problems on Riemanian manifolds and propose a class of effective Riemanian gradient-based methods to solve these minimax problems. Specifically, we propose an effective Riemannian gradient descent ascent (RGDA) algorithm for the deterministic minimax optimization. Moreover, we prove that our RGDA has a sample complexity of O(κ<sup>2</sup>ϵ<sup>-2</sup>) for finding an ϵ-stationary solution of the Geodesically-Nonconvex Strongly-Concave (GNSC) minimax problems, where κ denotes the condition number. At the same time, we present an effective Riemannian stochastic gradient descent ascent (RSGDA) algorithm for the stochastic minimax optimization, which has a sample complexity of O(κ<sup>4</sup>ϵ<sup>-4</sup>) for finding an ϵ-stationary solution. To further reduce the sample complexity, we propose an accelerated Riemannian stochastic gradient descent ascent (Acc-RSGDA) algorithm based on the momentum-based variance-reduced technique. We prove that our Acc-RSGDA algorithm achieves a lower sample complexity of ~O(κ<sup>4</sup>ϵ<sup>-3</sup>) in searching for an ϵ-stationary solution of the GNSC minimax problems. Extensive experimental results on the robust distributional optimization and robust Deep Neural Networks (DNNs) training over Stiefel manifold demonstrate efficiency of our algorithms.",
      "doi": "https://doi.org/10.1109/tpami.2023.3234160",
      "openalex_id": "https://openalex.org/W4313591702",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video",
      "summary": "This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.",
      "abstract": "This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.",
      "doi": "https://doi.org/10.1109/fg57933.2023.10042638",
      "openalex_id": "https://openalex.org/W4321020767",
      "arxiv_id": "",
      "publication_date": "2023-01-05",
      "published": "2023-01-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Asymmetric Loss For Multi-Label Classification",
      "summary": "In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (\"ASL\"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",
      "abstract": "In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (\"ASL\"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",
      "doi": "https://doi.org/10.1109/iccv48922.2021.00015",
      "openalex_id": "https://openalex.org/W4214673031",
      "arxiv_id": "",
      "publication_date": "2021-10-01",
      "published": "2021-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder",
      "summary": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.",
      "abstract": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.",
      "doi": "https://doi.org/10.1109/icassp49660.2025.10887856",
      "openalex_id": "https://openalex.org/W4406273203",
      "arxiv_id": "",
      "publication_date": "2025-03-12",
      "published": "2025-03-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Vector Quantized Masked Autoencoder for Speech Emotion Recognition",
      "summary": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
      "abstract": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193151",
      "openalex_id": "https://openalex.org/W4385484923",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Pre-Trained Audio-Visual Transformer for Emotion Recognition",
      "summary": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
      "abstract": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9747278",
      "openalex_id": "https://openalex.org/W4225959162",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SimMIM: a Simple Framework for Masked Image Modeling",
      "summary": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.",
      "abstract": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.",
      "doi": "https://doi.org/10.1109/cvpr52688.2022.00943",
      "openalex_id": "https://openalex.org/W4312804044",
      "arxiv_id": "",
      "publication_date": "2022-06-01",
      "published": "2022-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Sound of Motions",
      "summary": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
      "abstract": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
      "doi": "https://doi.org/10.1109/iccv.2019.00182",
      "openalex_id": "https://openalex.org/W2981851635",
      "arxiv_id": "",
      "publication_date": "2019-10-01",
      "published": "2019-10-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Emotion Recognition for In-the-wild Videos",
      "summary": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
      "abstract": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
      "doi": "https://doi.org/10.48550/arxiv.2002.05447",
      "openalex_id": "https://openalex.org/W3005997049",
      "arxiv_id": "",
      "publication_date": "2020-02-13",
      "published": "2020-02-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improved Baselines with Momentum Contrastive Learning",
      "summary": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
      "abstract": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
      "doi": "https://doi.org/10.48550/arxiv.2003.04297",
      "openalex_id": "https://openalex.org/W3009561768",
      "arxiv_id": "",
      "publication_date": "2020-03-09",
      "published": "2020-03-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "summary": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
      "abstract": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
      "doi": "https://doi.org/10.48550/arxiv.1706.02677",
      "openalex_id": "https://openalex.org/W2622263826",
      "arxiv_id": "",
      "publication_date": "2017-06-08",
      "published": "2017-06-08",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond",
      "summary": "Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
      "abstract": "Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
      "doi": "https://doi.org/10.48550/arxiv.2208.00173",
      "openalex_id": "https://openalex.org/W4289644862",
      "arxiv_id": "",
      "publication_date": "2022-07-30",
      "published": "2022-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MizAR 60 for Mizar 50",
      "summary": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "doi": "https://doi.org/10.4230/lipics.itp.2023.19",
      "openalex_id": "https://openalex.org/W4385245566",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Masked Autoencoders that Listen",
      "summary": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "doi": "https://doi.org/10.48550/arxiv.2207.06405",
      "openalex_id": "https://openalex.org/W4285483774",
      "arxiv_id": "",
      "publication_date": "2022-07-13",
      "published": "2022-07-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild",
      "summary": "Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.",
      "abstract": "Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.",
      "doi": "https://doi.org/10.1109/tcsvt.2023.3312858",
      "openalex_id": "https://openalex.org/W4386699383",
      "arxiv_id": "",
      "publication_date": "2023-09-13",
      "published": "2023-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition",
      "summary": "Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.",
      "abstract": "Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.",
      "doi": "https://doi.org/10.48550/arxiv.2107.04187",
      "openalex_id": "https://openalex.org/W3180874665",
      "arxiv_id": "",
      "publication_date": "2021-07-09",
      "published": "2021-07-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "summary": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
      "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
      "doi": "https://doi.org/10.48550/arxiv.2104.11178",
      "openalex_id": "https://openalex.org/W3154596443",
      "arxiv_id": "",
      "publication_date": "2021-04-22",
      "published": "2021-04-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Augmenting Convolutional networks with attention-based aggregation",
      "summary": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
      "abstract": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
      "doi": "https://doi.org/10.48550/arxiv.2112.13692",
      "openalex_id": "https://openalex.org/W4226487397",
      "arxiv_id": "",
      "publication_date": "2021-12-27",
      "published": "2021-12-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
      "summary": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
      "abstract": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
      "doi": "https://doi.org/10.48550/arxiv.2107.10834",
      "openalex_id": "https://openalex.org/W3184087575",
      "arxiv_id": "",
      "publication_date": "2021-07-22",
      "published": "2021-07-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Masked Autoencoders As Spatiotemporal Learners",
      "summary": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
      "abstract": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
      "doi": "https://doi.org/10.48550/arxiv.2205.09113",
      "openalex_id": "https://openalex.org/W4280490805",
      "arxiv_id": "",
      "publication_date": "2022-05-18",
      "published": "2022-05-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "summary": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",
      "abstract": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",
      "doi": "https://doi.org/10.48550/arxiv.2203.12602",
      "openalex_id": "https://openalex.org/W4221167396",
      "arxiv_id": "",
      "publication_date": "2022-03-23",
      "published": "2022-03-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
      "summary": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
      "abstract": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
      "doi": "https://doi.org/10.48550/arxiv.2111.02735",
      "openalex_id": "https://openalex.org/W3211224152",
      "arxiv_id": "",
      "publication_date": "2021-11-04",
      "published": "2021-11-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "summary": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
      "abstract": "The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.",
      "doi": "https://doi.org/10.48550/arxiv.2111.02172",
      "openalex_id": "https://openalex.org/W3209494353",
      "arxiv_id": "",
      "publication_date": "2021-11-03",
      "published": "2021-11-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition",
      "summary": "Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale \"in-the-wild\" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.",
      "abstract": "Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years. However, human emotional states are quite complex. To appraise such states displayed in real-world settings, we need expressive emotional descriptors that are capable of capturing and describing this complexity. The circumplex model of affect, which is described in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the activation of the emotion), can be used for this purpose. Recent progress in the emotion recognition domain has been achieved through the development of deep neural architectures and the availability of very large training databases. To this end, Aff-Wild has been the first large-scale \"in-the-wild\" database, containing around 1,200,000 frames. In this paper, we build upon this database, extending it with 260 more subjects and 1,413,000 new video frames. We call the union of Aff-Wild with the additional data, Aff-Wild2. The videos are downloaded from Youtube and have large variations in pose, age, illumination conditions, ethnicity and profession. Both database-specific as well as cross-database experiments are performed in this paper, by utilizing the Aff-Wild2, along with the RECOLA database. The developed deep neural architectures are based on the joint training of state-of-the-art convolutional and recurrent neural networks with attention mechanism; thus exploiting both the invariant properties of convolutional features, while modeling temporal dynamics that arise in human behaviour via the recurrent layers. The obtained results show premise for utilization of the extended Aff-Wild, as well as of the developed deep neural architectures for visual analysis of human behaviour in terms of continuous emotion dimensions.",
      "doi": "https://doi.org/10.48550/arxiv.1811.07770",
      "openalex_id": "https://openalex.org/W2901836079",
      "arxiv_id": "",
      "publication_date": "2018-11-11",
      "published": "2018-11-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The role of syllables in sign language production",
      "summary": "The aim of the present study was to investigate the functional role of syllables in sign language and how the different phonological combinations influence sign production. Moreover, the influence of age of acquisition was evaluated. Deaf signers (native and non-native) of Catalan Signed Language (LSC) were asked in a picture-sign interference task to sign picture names while ignoring distractor-signs with which they shared two phonological parameters (out of three of the main sign parameters: Location, Movement, and Handshape). The results revealed a different impact of the three phonological combinations. While no effect was observed for the phonological combination Handshape-Location, the combination Handshape-Movement slowed down signing latencies, but only in the non-native group. A facilitatory effect was observed for both groups when pictures and distractors shared Location-Movement. Importantly, linguistic models have considered this phonological combination to be a privileged unit in the composition of signs, as syllables are in spoken languages. Thus, our results support the functional role of syllable units during phonological articulation in sign language production.",
      "abstract": "The aim of the present study was to investigate the functional role of syllables in sign language and how the different phonological combinations influence sign production. Moreover, the influence of age of acquisition was evaluated. Deaf signers (native and non-native) of Catalan Signed Language (LSC) were asked in a picture-sign interference task to sign picture names while ignoring distractor-signs with which they shared two phonological parameters (out of three of the main sign parameters: Location, Movement, and Handshape). The results revealed a different impact of the three phonological combinations. While no effect was observed for the phonological combination Handshape-Location, the combination Handshape-Movement slowed down signing latencies, but only in the non-native group. A facilitatory effect was observed for both groups when pictures and distractors shared Location-Movement. Importantly, linguistic models have considered this phonological combination to be a privileged unit in the composition of signs, as syllables are in spoken languages. Thus, our results support the functional role of syllable units during phonological articulation in sign language production.",
      "doi": "https://doi.org/10.3389/fpsyg.2014.01254",
      "openalex_id": "https://openalex.org/W1994683737",
      "arxiv_id": "",
      "publication_date": "2014-11-13",
      "published": "2014-11-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Inflected words in production: Evidence for a morphologically rich lexicon",
      "summary": "Current evidence suggests that there is a difference between the representations of multimorphemic words in production and perception. In perception, it is widely believed that both whole-word and root representations exist, while in production there is little evidence for whole-word representations. The present investigation demonstrates that whole-word and root frequency independently predict the duration of words suffixed with –ing, –ed, and –s, which reveals that both root and word representations play a role in the production of inflected English words. In a second line of analysis, we find that the number of inflected phonological neighbours independently predicts the duration of monomorphemic words, which extends these results and suggests that whole-word representations exist at the lexical level. Together these results suggest that both root and word representations of inflected words are stored in the lexicon and are relevant for the production of both monomorphemic and multimorphemic words.",
      "abstract": "Current evidence suggests that there is a difference between the representations of multimorphemic words in production and perception. In perception, it is widely believed that both whole-word and root representations exist, while in production there is little evidence for whole-word representations. The present investigation demonstrates that whole-word and root frequency independently predict the duration of words suffixed with –ing, –ed, and –s, which reveals that both root and word representations play a role in the production of inflected English words. In a second line of analysis, we find that the number of inflected phonological neighbours independently predicts the duration of monomorphemic words, which extends these results and suggests that whole-word representations exist at the lexical level. Together these results suggest that both root and word representations of inflected words are stored in the lexicon and are relevant for the production of both monomorphemic and multimorphemic words.",
      "doi": "https://doi.org/10.1080/17470218.2015.1054847",
      "openalex_id": "https://openalex.org/W1606571944",
      "arxiv_id": "",
      "publication_date": "2015-05-27",
      "published": "2015-05-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical access in sign language: a computational model",
      "summary": "PSYCHOLINGUISTIC THEORIES HAVE PREDOMINANTLY BEEN BUILT UPON DATA FROM SPOKEN LANGUAGE, WHICH LEAVES OPEN THE QUESTION: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either (1) the time course of sign perception or (2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.",
      "abstract": "PSYCHOLINGUISTIC THEORIES HAVE PREDOMINANTLY BEEN BUILT UPON DATA FROM SPOKEN LANGUAGE, WHICH LEAVES OPEN THE QUESTION: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either (1) the time course of sign perception or (2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.",
      "doi": "https://doi.org/10.3389/fpsyg.2014.00428",
      "openalex_id": "https://openalex.org/W2111112092",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Abstract and Lexically Specific Information in Sound Patterns: Evidence from /r/-sandhi in Rhotic and Non-rhotic Varieties of English",
      "summary": "Phonological theories differ as to whether phonological knowledge is abstract (e.g., phonemic), concrete (e.g., exemplar-based), or some combination of the two. The abstractness/concreteness of phonological knowledge was examined by analyzing the process of /r/-sandhi in two corpora of spoken English. Two predictions of exemplar-based theories were examined: the extent to which a word manifests a particular sound pattern like /r/-deletion should be influenced by (1) its lexical frequency and (2) its distribution in the language with respect to the sound pattern’s conditioning environment. Lexical frequency was found to influence /r/-sandhi in a corpus of rhotic American English but not in a corpus of predominantly non-rhotic British English. No effect of a word’s long-term distribution was found in either corpus. These results support theories proposing that phonological knowledge is both word-specific and abstract and indicate that speakers do not store all phonetic detail that is in principle available to them. The factors that may favor the use of word-specific versus abstract representations are discussed.",
      "abstract": "Phonological theories differ as to whether phonological knowledge is abstract (e.g., phonemic), concrete (e.g., exemplar-based), or some combination of the two. The abstractness/concreteness of phonological knowledge was examined by analyzing the process of /r/-sandhi in two corpora of spoken English. Two predictions of exemplar-based theories were examined: the extent to which a word manifests a particular sound pattern like /r/-deletion should be influenced by (1) its lexical frequency and (2) its distribution in the language with respect to the sound pattern’s conditioning environment. Lexical frequency was found to influence /r/-sandhi in a corpus of rhotic American English but not in a corpus of predominantly non-rhotic British English. No effect of a word’s long-term distribution was found in either corpus. These results support theories proposing that phonological knowledge is both word-specific and abstract and indicate that speakers do not store all phonetic detail that is in principle available to them. The factors that may favor the use of word-specific versus abstract representations are discussed.",
      "doi": "https://doi.org/10.1177/0023830914567168",
      "openalex_id": "https://openalex.org/W2036289461",
      "arxiv_id": "",
      "publication_date": "2015-02-18",
      "published": "2015-02-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The MRC Psycholinguistic Database",
      "summary": "This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",
      "abstract": "This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",
      "doi": "https://doi.org/10.1080/14640748108400805",
      "openalex_id": "https://openalex.org/W1985449126",
      "arxiv_id": "",
      "publication_date": "1981-11-01",
      "published": "1981-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Psycholinguistic investigations of phonological structure in ASL",
      "summary": "Linguistic categories (e.g. segment, syllable, etc.) have long enabled cogent descriptions of the systematic patterns apparent in spoken languages. Beginning with the seminal work of William Stokoe (1960; 1965), research on the structure of American Sign Language (ASL) has demonstrated that linguistic categories are useful in capturing extant patterns found in a signed language. For example, recognition of a syllable unit permits accounts of morphophonological processes and places constraints on sign forms (Brentari 1990; Perlmutter 1993; Sandler 1993; Corina 1996). Acknowledgment of Movement and Location segments permits descriptions of infixation processes (Liddell and Johnson 1985; Sandler 1986). Feature hierarchies provide accounts of assimilations that are observed in the language and also help to explain those that do not occur (Corina and Sandler 1993). These investigations of linguistic structure have led to a better understanding of both the similarities and differences between signed and spoken language.",
      "abstract": "Linguistic categories (e.g. segment, syllable, etc.) have long enabled cogent descriptions of the systematic patterns apparent in spoken languages. Beginning with the seminal work of William Stokoe (1960; 1965), research on the structure of American Sign Language (ASL) has demonstrated that linguistic categories are useful in capturing extant patterns found in a signed language. For example, recognition of a syllable unit permits accounts of morphophonological processes and places constraints on sign forms (Brentari 1990; Perlmutter 1993; Sandler 1993; Corina 1996). Acknowledgment of Movement and Location segments permits descriptions of infixation processes (Liddell and Johnson 1985; Sandler 1986). Feature hierarchies provide accounts of assimilations that are observed in the language and also help to explain those that do not occur (Corina and Sandler 1993). These investigations of linguistic structure have led to a better understanding of both the similarities and differences between signed and spoken language.",
      "doi": "https://doi.org/10.1017/cbo9780511486777.005",
      "openalex_id": "https://openalex.org/W2198101096",
      "arxiv_id": "",
      "publication_date": "2002-10-24",
      "published": "2002-10-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A constraint -based account of handshape contrast in sign languages",
      "summary": "The main goal of this dissertation is to explore the nature of contrast in sign language handshapes. I first demonstrate that the distribution of handshape contrasts is not homogeneous, either within or across sign languages. By using a variety of methodologies (examination of dictionary data, elicited data, and psycholinguistic experimentation), I present examples of differences related to type of contrast (distinctive, active, and prominent—following Clements, 2001); position in the lexical substrata (following the work of Ito and Mester, 1995a, and Brentari and Padden, 2001); iconic relationships (e.g. shape, size, arrangement of parts); and cross-linguistic variation (comparing American Sign Language, Swiss German Sign Language, and Hong Kong Sign Language). I also propose that the distributional differences in handshape contrasts can be explained in terms of a confluence of pressures on language. Using the tenets of Optimality Theory (OT), these differences can be explained by determining how various languages—or lexical components within languages—rank constraints related to those pressures. Specifically, I follow Flemming's (2002) version of OT (Dispersion Theory) in which grammars balance the pressures of articulatory ease and perceptual distinctiveness, as well as the desire to maximize the number of contrasts available for word formation. To this, I propose an additional pressure—one to maintain contrasts borrowed into the language from external sources. These external contrasts can be borrowed from other languages (directly from other sign languages, or indirectly from spoken languages via systems such as fingerspelling), or they can be borrowed from visual aspects of the real world.",
      "abstract": "The main goal of this dissertation is to explore the nature of contrast in sign language handshapes. I first demonstrate that the distribution of handshape contrasts is not homogeneous, either within or across sign languages. By using a variety of methodologies (examination of dictionary data, elicited data, and psycholinguistic experimentation), I present examples of differences related to type of contrast (distinctive, active, and prominent—following Clements, 2001); position in the lexical substrata (following the work of Ito and Mester, 1995a, and Brentari and Padden, 2001); iconic relationships (e.g. shape, size, arrangement of parts); and cross-linguistic variation (comparing American Sign Language, Swiss German Sign Language, and Hong Kong Sign Language). I also propose that the distributional differences in handshape contrasts can be explained in terms of a confluence of pressures on language. Using the tenets of Optimality Theory (OT), these differences can be explained by determining how various languages—or lexical components within languages—rank constraints related to those pressures. Specifically, I follow Flemming's (2002) version of OT (Dispersion Theory) in which grammars balance the pressures of articulatory ease and perceptual distinctiveness, as well as the desire to maximize the number of contrasts available for word formation. To this, I propose an additional pressure—one to maintain contrasts borrowed into the language from external sources. These external contrasts can be borrowed from other languages (directly from other sign languages, or indirectly from spoken languages via systems such as fingerspelling), or they can be borrowed from visual aspects of the real world.",
      "doi": "",
      "openalex_id": "https://openalex.org/W1485561458",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bimodal Bilingualism and the Frequency-Lag Hypothesis",
      "summary": "The frequency-lag hypothesis proposes that bilinguals have slowed lexical retrieval relative to monolinguals and in their nondominant language relative to their dominant language, particularly for low-frequency words. These effects arise because bilinguals divide their language use between 2 languages and use their nondominant language less frequently. We conducted a picture-naming study with hearing American Sign Language (ASL)-English bilinguals (bimodal bilinguals), deaf signers, and English-speaking monolinguals. As predicted by the frequency-lag hypothesis, bimodal bilinguals were slower, less accurate, and exhibited a larger frequency effect when naming pictures in ASL as compared with English (their dominant language) and as compared with deaf signers. For English there was no difference in naming latencies, error rates, or frequency effects for bimodal bilinguals as compared with monolinguals. Neither age of ASL acquisition nor interpreting experience affected the results; picture-naming accuracy and frequency effects were equivalent for deaf signers and English monolinguals. Larger frequency effects in ASL relative to English for bimodal bilinguals suggests that they are affected by a frequency lag in ASL. The absence of a lag for English could reflect the use of mouthing and/or code-blending, which may shield bimodal bilinguals from the lexical slowing observed for spoken language bilinguals in the dominant language.",
      "abstract": "The frequency-lag hypothesis proposes that bilinguals have slowed lexical retrieval relative to monolinguals and in their nondominant language relative to their dominant language, particularly for low-frequency words. These effects arise because bilinguals divide their language use between 2 languages and use their nondominant language less frequently. We conducted a picture-naming study with hearing American Sign Language (ASL)-English bilinguals (bimodal bilinguals), deaf signers, and English-speaking monolinguals. As predicted by the frequency-lag hypothesis, bimodal bilinguals were slower, less accurate, and exhibited a larger frequency effect when naming pictures in ASL as compared with English (their dominant language) and as compared with deaf signers. For English there was no difference in naming latencies, error rates, or frequency effects for bimodal bilinguals as compared with monolinguals. Neither age of ASL acquisition nor interpreting experience affected the results; picture-naming accuracy and frequency effects were equivalent for deaf signers and English monolinguals. Larger frequency effects in ASL relative to English for bimodal bilinguals suggests that they are affected by a frequency lag in ASL. The absence of a lag for English could reflect the use of mouthing and/or code-blending, which may shield bimodal bilinguals from the lexical slowing observed for spoken language bilinguals in the dominant language.",
      "doi": "https://doi.org/10.1093/deafed/ens034",
      "openalex_id": "https://openalex.org/W2135389530",
      "arxiv_id": "",
      "publication_date": "2012-10-16",
      "published": "2012-10-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Building BSL SignBank: The Lemma Dilemma Revisited",
      "summary": "One key criterion when creating a representation of the lexicon of any language within a dictionary or lexical database is that it must be decided which groups of idiosyncratic and systematically modified variants together form a lexeme. Few researchers have, however, attempted to outline such principles as they might apply to sign languages. As a consequence, some sign language dictionaries and lexical databases appear to be mixed collections of phonetic, phonological, morphological, and lexical variants of lexical signs (e.g. Brien 1992) which have not addressed what may be termed as the lemma dilemma. In this paper, we outline the lemmatisation practices used in the creation of BSL SignBank (Fenlon, Cormier et al. 2014), a lexical database and dictionary of British Sign Language based on signs identified within the British Sign Language Corpus (http://www.bslcorpusproject.org). We argue that the principles outlined here should be considered in the creation of any sign language lexical database and ultimately any sign language dictionary and reference grammar.",
      "abstract": "One key criterion when creating a representation of the lexicon of any language within a dictionary or lexical database is that it must be decided which groups of idiosyncratic and systematically modified variants together form a lexeme. Few researchers have, however, attempted to outline such principles as they might apply to sign languages. As a consequence, some sign language dictionaries and lexical databases appear to be mixed collections of phonetic, phonological, morphological, and lexical variants of lexical signs (e.g. Brien 1992) which have not addressed what may be termed as the lemma dilemma. In this paper, we outline the lemmatisation practices used in the creation of BSL SignBank (Fenlon, Cormier et al. 2014), a lexical database and dictionary of British Sign Language based on signs identified within the British Sign Language Corpus (http://www.bslcorpusproject.org). We argue that the principles outlined here should be considered in the creation of any sign language lexical database and ultimately any sign language dictionary and reference grammar.",
      "doi": "https://doi.org/10.1093/ijl/ecv008",
      "openalex_id": "https://openalex.org/W2073421084",
      "arxiv_id": "",
      "publication_date": "2015-04-24",
      "published": "2015-04-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Arbitrariness and Iconicity: Historical Change in American Sign Language",
      "summary": "Grammarians since Saussure have insisted that language symbols are arbitrary, though conventionalized, in form. Sign languages in general, however, and American Sign Language (ASL) in particular, have been noted for their pantomimic or iconic nature. This paper examines some historical processes in ASL, and shows that there is a strong tendency for signs to change in the direction of arbitrariness, rather than maintaining a level of iconicity. Changes at the formational level can be seen as contributing to language-internal consistency, at the expense of transparency.*",
      "abstract": "Grammarians since Saussure have insisted that language symbols are arbitrary, though conventionalized, in form. Sign languages in general, however, and American Sign Language (ASL) in particular, have been noted for their pantomimic or iconic nature. This paper examines some historical processes in ASL, and shows that there is a strong tendency for signs to change in the direction of arbitrariness, rather than maintaining a level of iconicity. Changes at the formational level can be seen as contributing to language-internal consistency, at the expense of transparency.*",
      "doi": "https://doi.org/10.2307/412894",
      "openalex_id": "https://openalex.org/W2037715905",
      "arxiv_id": "",
      "publication_date": "1975-09-01",
      "published": "1975-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Time and Thyme Are not Homophones: The Effect of Lemma Frequency on Word Durations in Spontaneous Speech",
      "summary": "Frequent words tend to shorten. But do homophone pairs, such as time and thyme , shorten equally if one member of the pair is frequent? This study reports an analysis of roughly 90,000 tokens of homophones in the Switchboard corpus of American English telephone conversations, in which it was found that high-frequency words like time are significantly shorter than their low-frequency homophones like thyme . The effect of lemma frequency persisted when local speaking rate, predictability from neighboring words, position relative to pauses, syntactic category, and orthographic regularity were brought under statistical control. These findings have theoretical implications for the locus of frequency information in linguistic competence and in models of language production, and for the role of articulatory routinization in shortening.",
      "abstract": "Frequent words tend to shorten. But do homophone pairs, such as time and thyme , shorten equally if one member of the pair is frequent? This study reports an analysis of roughly 90,000 tokens of homophones in the Switchboard corpus of American English telephone conversations, in which it was found that high-frequency words like time are significantly shorter than their low-frequency homophones like thyme . The effect of lemma frequency persisted when local speaking rate, predictability from neighboring words, position relative to pauses, syntactic category, and orthographic regularity were brought under statistical control. These findings have theoretical implications for the locus of frequency information in linguistic competence and in models of language production, and for the role of articulatory routinization in shortening.",
      "doi": "https://doi.org/10.1353/lan.0.0035",
      "openalex_id": "https://openalex.org/W2094008654",
      "arxiv_id": "",
      "publication_date": "2008-09-01",
      "published": "2008-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Transition From Fingerspelling to English Print: Facilitating English Decoding",
      "summary": "Fingerspelling is an integral part of American Sign Language (ASL) and it is also an important aspect of becoming bilingual in English and ASL. Even though fingerspelling is based on English orthography, the development of fingerspelling does not parallel the development of reading in hearing children. Research reveals that deaf children may initially treat fingerspelled words as lexical items rather than a series of letters that represent English orthography and only later begin to learn to link handshapes to English graphemes. The purpose of this study is to determine whether a training method that uses fingerspelling and phonological patterns that resemble those found in lexicalized fingerspelling to teach deaf students unknown English vocabulary would increase their ability to learn the fingerspelled and orthographic version of a word. There were 21 deaf students (aged 4-14 years) who participated. Results show that students were better able to recognize and write the printed English word as well as fingerspell the word, when training incorporated fingerspelling that is more lexicalized. The discussion focuses on the degree to which fingerspelling can serve as a visual phonological bridge as an aid to decode English print.",
      "abstract": "Fingerspelling is an integral part of American Sign Language (ASL) and it is also an important aspect of becoming bilingual in English and ASL. Even though fingerspelling is based on English orthography, the development of fingerspelling does not parallel the development of reading in hearing children. Research reveals that deaf children may initially treat fingerspelled words as lexical items rather than a series of letters that represent English orthography and only later begin to learn to link handshapes to English graphemes. The purpose of this study is to determine whether a training method that uses fingerspelling and phonological patterns that resemble those found in lexicalized fingerspelling to teach deaf students unknown English vocabulary would increase their ability to learn the fingerspelled and orthographic version of a word. There were 21 deaf students (aged 4-14 years) who participated. Results show that students were better able to recognize and write the printed English word as well as fingerspell the word, when training incorporated fingerspelling that is more lexicalized. The discussion focuses on the degree to which fingerspelling can serve as a visual phonological bridge as an aid to decode English print.",
      "doi": "https://doi.org/10.1093/deafed/enm003",
      "openalex_id": "https://openalex.org/W2142638134",
      "arxiv_id": "",
      "publication_date": "2007-02-25",
      "published": "2007-02-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Calculating Frequency of Occurrence of ASL handshapes",
      "summary": "&lt;p&gt;Here we discuss an investigation of handshape markedness based on frequency of occurrence in an ASL&lt;br /&gt;database. Using a database of the most frequently used signs in a corpus of child language and other&lt;br /&gt;early-acquired signs we examined the handshapes of approximately 1000 signs by using two annotation&lt;br /&gt;systems, BTS and Stokoe annotation. Results indicate that the distribution of handshape use on the&lt;br /&gt;dominant and non-dominant hands is consistent with the predictions set forth by previous researchers&lt;br /&gt;in their examinations of handshape markedness. Our findings are also consistent with investigations&lt;br /&gt;handshape frequency in other sign languages, suggesting some cross-linguistic comparability in handshape&lt;br /&gt;markedness.&lt;/p&gt;",
      "abstract": "&lt;p&gt;Here we discuss an investigation of handshape markedness based on frequency of occurrence in an ASL&lt;br /&gt;database. Using a database of the most frequently used signs in a corpus of child language and other&lt;br /&gt;early-acquired signs we examined the handshapes of approximately 1000 signs by using two annotation&lt;br /&gt;systems, BTS and Stokoe annotation. Results indicate that the distribution of handshape use on the&lt;br /&gt;dominant and non-dominant hands is consistent with the predictions set forth by previous researchers&lt;br /&gt;in their examinations of handshape markedness. Our findings are also consistent with investigations&lt;br /&gt;handshape frequency in other sign languages, suggesting some cross-linguistic comparability in handshape&lt;br /&gt;markedness.&lt;/p&gt;",
      "doi": "https://doi.org/10.3765/exabs.v0i0.764",
      "openalex_id": "https://openalex.org/W2425058634",
      "arxiv_id": "",
      "publication_date": "2013-05-07",
      "published": "2013-05-07",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Lexical Frequency in Sign Languages",
      "summary": "Measures of lexical frequency presuppose the existence of corpora, but true machine-readable corpora of sign languages (SLs) are only now being created. Lexical frequency ratings for SLs are needed because there has been a heavy reliance on the interpretation of results of psycholinguistic and neurolinguistic experiments in the SL research literature; yet, these experiments have been conducted without the benefit of such measures. In addition, measures of lexical frequency can also guide SL teachers by identifying which signs would be prioritized in early language instruction. I begin by a discussion of lexicalization and sign types in order to explain what constitutes a lexical sign in SLs. I then present the annotation method and results. In the discussion, I raise the potential limitations of previous studies of lexical frequency in terms of the discrimination of lexical signs from other kinds of signs, consistent lemma glossing, part of speech tagging, and the systematic treatment of depicting signs. I conclude in cautioning that descriptions of SL grammars that do not accommodate typical mixtures and sequences of signs as shown in data are likely to be unreliable.",
      "abstract": "Measures of lexical frequency presuppose the existence of corpora, but true machine-readable corpora of sign languages (SLs) are only now being created. Lexical frequency ratings for SLs are needed because there has been a heavy reliance on the interpretation of results of psycholinguistic and neurolinguistic experiments in the SL research literature; yet, these experiments have been conducted without the benefit of such measures. In addition, measures of lexical frequency can also guide SL teachers by identifying which signs would be prioritized in early language instruction. I begin by a discussion of lexicalization and sign types in order to explain what constitutes a lexical sign in SLs. I then present the annotation method and results. In the discussion, I raise the potential limitations of previous studies of lexical frequency in terms of the discrimination of lexical signs from other kinds of signs, consistent lemma glossing, part of speech tagging, and the systematic treatment of depicting signs. I conclude in cautioning that descriptions of SL grammars that do not accommodate typical mixtures and sequences of signs as shown in data are likely to be unreliable.",
      "doi": "https://doi.org/10.1093/deafed/enr036",
      "openalex_id": "https://openalex.org/W2153317408",
      "arxiv_id": "",
      "publication_date": "2011-08-12",
      "published": "2011-08-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The reluctant oracle: using strategic annotations to add value to, and extract value from, a signed language corpus",
      "summary": "In this paper, I discuss the ways in which multimedia annotation software is being used to transform an archive of Auslan recordings into a true machine-readable language corpus. After the basic structure of the annotation files in the Auslan corpus is described and the exercise differentiated from transcription, the glossing and annotation conventions are explained. Following this, I exemplify the searching and pattern-matching at different levels of linguistic organisation that these annotations make possible. The paper shows how, in the creation of signed language corpora, it is important to be clear about the difference between transcription and annotation. Without an awareness of this distinction – and despite time consuming and expensive processing of the video recordings – we may not be able to discern the types of patterns in our corpora that we hope to. The conventions are designed to ensure that the annotations really do enable researchers to identify regularities at different levels of linguistic organisation in the corpus and, thus, to test, or build on, existing descriptions of the language.",
      "abstract": "In this paper, I discuss the ways in which multimedia annotation software is being used to transform an archive of Auslan recordings into a true machine-readable language corpus. After the basic structure of the annotation files in the Auslan corpus is described and the exercise differentiated from transcription, the glossing and annotation conventions are explained. Following this, I exemplify the searching and pattern-matching at different levels of linguistic organisation that these annotations make possible. The paper shows how, in the creation of signed language corpora, it is important to be clear about the difference between transcription and annotation. Without an awareness of this distinction – and despite time consuming and expensive processing of the video recordings – we may not be able to discern the types of patterns in our corpora that we hope to. The conventions are designed to ensure that the annotations really do enable researchers to identify regularities at different levels of linguistic organisation in the corpus and, thus, to test, or build on, existing descriptions of the language.",
      "doi": "https://doi.org/10.3366/cor.2014.0056",
      "openalex_id": "https://openalex.org/W2012075454",
      "arxiv_id": "",
      "publication_date": "2014-11-01",
      "published": "2014-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Segmental Framework for Representing Signs Phonetically",
      "summary": "The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments.",
      "abstract": "The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments.",
      "doi": "https://doi.org/10.1353/sls.2011.0002",
      "openalex_id": "https://openalex.org/W2054973881",
      "arxiv_id": "",
      "publication_date": "2011-03-01",
      "published": "2011-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "American Sign Language: The Phonological Base",
      "summary": "This paper has the ambitious goal of outlining the phonological structures and processes we have analyzed in American Sign Language (ASL). In order to do this we have divided the paper into five parts. In section 1 we detail the types of sequential phenomena found in the production of individual signs, allowing us to argue that ASL signs are composed of sequences of phonological segments, just as are words in spoken languages. Section 2 provides the details of a segmental phonetic transcription system. Using the descriptions made available by the transcription system, Section 3 briefly discusses both paradigmatic and syntagmatic contrast in ASL signs. Section 4 deals with the various types of phonological processes at work in the language, processes remarkable in their similarity to phonological processes found in spoken languages. We conclude the paper with an overview of the major typed of phonological effects of ASL’s rich system of morphological processes. We realize that the majority of readers will come to this paper with neither sign language proficiency nor a knowledge of sign language structure. As a result, many will encounter reference to ASL signs without knowing their form. Although we have been unable to illustrate all the examples, we hope we have provided sufficient illustrations to make the paper more accessible.",
      "abstract": "This paper has the ambitious goal of outlining the phonological structures and processes we have analyzed in American Sign Language (ASL). In order to do this we have divided the paper into five parts. In section 1 we detail the types of sequential phenomena found in the production of individual signs, allowing us to argue that ASL signs are composed of sequences of phonological segments, just as are words in spoken languages. Section 2 provides the details of a segmental phonetic transcription system. Using the descriptions made available by the transcription system, Section 3 briefly discusses both paradigmatic and syntagmatic contrast in ASL signs. Section 4 deals with the various types of phonological processes at work in the language, processes remarkable in their similarity to phonological processes found in spoken languages. We conclude the paper with an overview of the major typed of phonological effects of ASL’s rich system of morphological processes. We realize that the majority of readers will come to this paper with neither sign language proficiency nor a knowledge of sign language structure. As a result, many will encounter reference to ASL signs without knowing their form. Although we have been unable to illustrate all the examples, we hope we have provided sufficient illustrations to make the paper more accessible.",
      "doi": "https://doi.org/10.1353/sls.1989.0027",
      "openalex_id": "https://openalex.org/W1965498301",
      "arxiv_id": "",
      "publication_date": "1989-09-01",
      "published": "1989-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Recognizing Spoken Words: The Neighborhood Activation Model",
      "summary": "The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.",
      "abstract": "The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the effects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.",
      "doi": "https://doi.org/10.1097/00003446-199802000-00001",
      "openalex_id": "https://openalex.org/W2026992087",
      "arxiv_id": "",
      "publication_date": "1998-02-01",
      "published": "1998-02-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Distribution of Signs in New Zealand Sign Language",
      "summary": "Until now, teachers and learners of NZSL have not had access to information on the most frequently used signs in the Deaf community. This article describes the first study of the distribution of signs in New Zealand Sign Language (NZSL). We hope that it will help teachers of NZSL make decisions about which signs to teach first and suggest questions for investigation into other signed languages using a corpus analysis approach.",
      "abstract": "Until now, teachers and learners of NZSL have not had access to information on the most frequently used signs in the Deaf community. This article describes the first study of the distribution of signs in New Zealand Sign Language (NZSL). We hope that it will help teachers of NZSL make decisions about which signs to teach first and suggest questions for investigation into other signed languages using a corpus analysis approach.",
      "doi": "https://doi.org/10.1353/sls.2006.0027",
      "openalex_id": "https://openalex.org/W2154212878",
      "arxiv_id": "",
      "publication_date": "2006-06-01",
      "published": "2006-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Bilingual Deaf Students’ Phonological Awareness in ASL and Reading Skills in English",
      "summary": "The sources of knowledge that individuals use to make similarity judgments about words are thought to tap underlying phonological representations. This study addresses the issue of segmental representation by investigating bilingual deaf students’ (a) awareness of American Sign Language (ASL) phonological structure; (b) the relationships between ASL phonological awareness (ASL-PA) and written English word recognition and reading comprehension skill, and (c) the question of whether age and/or reading ability would differentially affect performance on an ASL-PA task in fifty bilingual deaf children (ages 7–18) attending schools for deaf children in Western Canada. In the ASL-PA task, minimal contrasts between ASL parameters (handshape, movement, and location; H, M, and L, respectively) were systematically manipulated. The results show significant differences in deaf students’ ASL phonological awareness, with discrimination accuracy improving with age and reading ability. Significant relationships between children’s second language (L2) reading skills and first language (L1) phonological awareness skills were found. Evidence of rich metalinguistic knowledge that children with developing L1 phonological skills bring to the acquisition of L2 reading skills may have practical implications for the education of bilingual deaf children.",
      "abstract": "The sources of knowledge that individuals use to make similarity judgments about words are thought to tap underlying phonological representations. This study addresses the issue of segmental representation by investigating bilingual deaf students’ (a) awareness of American Sign Language (ASL) phonological structure; (b) the relationships between ASL phonological awareness (ASL-PA) and written English word recognition and reading comprehension skill, and (c) the question of whether age and/or reading ability would differentially affect performance on an ASL-PA task in fifty bilingual deaf children (ages 7–18) attending schools for deaf children in Western Canada. In the ASL-PA task, minimal contrasts between ASL parameters (handshape, movement, and location; H, M, and L, respectively) were systematically manipulated. The results show significant differences in deaf students’ ASL phonological awareness, with discrimination accuracy improving with age and reading ability. Significant relationships between children’s second language (L2) reading skills and first language (L1) phonological awareness skills were found. Evidence of rich metalinguistic knowledge that children with developing L1 phonological skills bring to the acquisition of L2 reading skills may have practical implications for the education of bilingual deaf children.",
      "doi": "https://doi.org/10.1353/sls.2013.0028",
      "openalex_id": "https://openalex.org/W2030728229",
      "arxiv_id": "",
      "publication_date": "2013-09-01",
      "published": "2013-09-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Introduction to WordNet: An On-line Lexical Database<sup>*</sup>",
      "summary": "WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",
      "abstract": "WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",
      "doi": "https://doi.org/10.1093/ijl/3.4.235",
      "openalex_id": "https://openalex.org/W2102381086",
      "arxiv_id": "",
      "publication_date": "1990-01-01",
      "published": "1990-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Frequency Characteristics of American Sign Language",
      "summary": "When signers communicate with one another, they use some signs, such as finish, more frequently than others, such as eagle. The frequency of occurrence affects both the way that languages are processed and the way they change over time. It is important to be aware of the frequency characteristics of a language when pursuing either psycholinguistic or linguistic analyses. This article reports the findings of a pilot study of sign frequency in American Sign Language. A corpus of over four thousand signs was analyzed, and some of the frequency characteristics that were uncovered are reported here. Appendix 1 lists the most frequent signs in the database.",
      "abstract": "When signers communicate with one another, they use some signs, such as finish, more frequently than others, such as eagle. The frequency of occurrence affects both the way that languages are processed and the way they change over time. It is important to be aware of the frequency characteristics of a language when pursuing either psycholinguistic or linguistic analyses. This article reports the findings of a pilot study of sign frequency in American Sign Language. A corpus of over four thousand signs was analyzed, and some of the frequency characteristics that were uncovered are reported here. Appendix 1 lists the most frequent signs in the database.",
      "doi": "https://doi.org/10.1353/sls.2003.0003",
      "openalex_id": "https://openalex.org/W1966925283",
      "arxiv_id": "",
      "publication_date": "2003-12-01",
      "published": "2003-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Role of Iconicity in Early Sign Language Acquisition",
      "summary": "A longitudinal study of sign language acquisition was conducted with 13 very young children (median age 10 months at outset of study) of deaf parents. The children's sign language lexicons were examined for their percentages of iconic signs at two early stages of vocabulary development. Iconic signs are those that clearly resemble the action, object, or characteristic they represent. Analysis of the subjects' vocabularies revealed that iconic signs comprised 30.8% of the first 10 signs they acquired. At age 18 months, the proportion of iconic signs was found to be 33.7%. The finding that a majority of signs in the subjects' early vocabularies were not iconic suggests that the role of iconicity in young children's acquisition of signs may have been overrated by some investigators, and that other formational features may be of greater importance in influencing young children's ability to acquire signs.",
      "abstract": "A longitudinal study of sign language acquisition was conducted with 13 very young children (median age 10 months at outset of study) of deaf parents. The children's sign language lexicons were examined for their percentages of iconic signs at two early stages of vocabulary development. Iconic signs are those that clearly resemble the action, object, or characteristic they represent. Analysis of the subjects' vocabularies revealed that iconic signs comprised 30.8% of the first 10 signs they acquired. At age 18 months, the proportion of iconic signs was found to be 33.7%. The finding that a majority of signs in the subjects' early vocabularies were not iconic suggests that the role of iconicity in young children's acquisition of signs may have been overrated by some investigators, and that other formational features may be of greater importance in influencing young children's ability to acquire signs.",
      "doi": "https://doi.org/10.1044/jshd.4903.287",
      "openalex_id": "https://openalex.org/W2011095659",
      "arxiv_id": "",
      "publication_date": "1984-08-01",
      "published": "1984-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Patterned iconicity in sign language lexicons",
      "summary": "Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity . In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.",
      "abstract": "Iconicity is an acknowledged property of both gesture and sign language. In contrast to the familiar definition of iconicity as a correspondence between individual forms and their referents, we explore iconicity as a shared property among groups of signs, in what we call patterned iconicity . In this paper, we focus on iconic strategies used by hearing silent gesturers and by signers of three unrelated sign languages in an elicitation task featuring pictures of hand-held manufactured tools. As in previous gesture literature, we find that silent gesturers largely prefer a handling strategy, though some use an instrument strategy, in which the handshape represents the shape of the tool. There are additional differences in use of handling and instrument strategies for hand-held tools across the different sign languages, suggesting typological differences in iconic patterning. Iconic patterning in each of the three sign languages demonstrates how gestural iconic resources are organized in the grammars of sign languages.",
      "doi": "https://doi.org/10.1075/gest.13.3.03pad",
      "openalex_id": "https://openalex.org/W2019268934",
      "arxiv_id": "",
      "publication_date": "2013-12-31",
      "published": "2013-12-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Language from the Body",
      "summary": "What is the role of meaning in linguistic theory? Generative linguists have severely limited the influence of meaning, claiming that language is not affected by other cognitive processes and that semantics does not influence linguistic form. Conversely, cognitivist and functionalist linguists believe that meaning pervades and motivates all levels of linguistic structure. This dispute can be resolved conclusively by evidence from signed languages. Signed languages are full of iconic linguistic items: words, inflections, and even syntactic constructions with structural similarities between their physical form and their referents' form. Iconic items can have concrete meanings and also abstract meanings through conceptual metaphors. Language from the Body rebuts the generativist linguistic theories which separate form and meaning and asserts that iconicity can only be described in a cognitivist framework where meaning can influence form.",
      "abstract": "What is the role of meaning in linguistic theory? Generative linguists have severely limited the influence of meaning, claiming that language is not affected by other cognitive processes and that semantics does not influence linguistic form. Conversely, cognitivist and functionalist linguists believe that meaning pervades and motivates all levels of linguistic structure. This dispute can be resolved conclusively by evidence from signed languages. Signed languages are full of iconic linguistic items: words, inflections, and even syntactic constructions with structural similarities between their physical form and their referents' form. Iconic items can have concrete meanings and also abstract meanings through conceptual metaphors. Language from the Body rebuts the generativist linguistic theories which separate form and meaning and asserts that iconicity can only be described in a cognitivist framework where meaning can influence form.",
      "doi": "https://doi.org/10.1017/cbo9780511509629",
      "openalex_id": "https://openalex.org/W4247720119",
      "arxiv_id": "",
      "publication_date": "2001-02-26",
      "published": "2001-02-26",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The link between form and meaning in American Sign Language: Lexical processing effects.",
      "summary": "Signed languages exploit iconicity (the transparent relationship between meaning and form) to a greater extent than spoken languages. where it is largely limited to onomatopoeia. In a picture-sign matching experiment measuring reaction times, the authors examined the potential advantage of iconicity both for 1st- and 2nd-language learners of American Sign Language (ASL). The results show that native ASL signers are faster to respond when a specific property iconically represented in a sign is made salient in the corresponding picture, thus providing evidence that a closer mapping between meaning and form can aid in lexical retrieval. While late 2nd-language learners appear to use iconicity as an aid to learning sign (R. Campbell, P. Martin, & T. White, 1992), they did not show the same facilitation effect as native ASL signers, suggesting that the task tapped into more automatic language processes. Overall, the findings suggest that completely arbitrary mappings between meaning and form may not be more advantageous in language and that, rather, arbitrariness may simply be an accident of modality.",
      "abstract": "Signed languages exploit iconicity (the transparent relationship between meaning and form) to a greater extent than spoken languages. where it is largely limited to onomatopoeia. In a picture-sign matching experiment measuring reaction times, the authors examined the potential advantage of iconicity both for 1st- and 2nd-language learners of American Sign Language (ASL). The results show that native ASL signers are faster to respond when a specific property iconically represented in a sign is made salient in the corresponding picture, thus providing evidence that a closer mapping between meaning and form can aid in lexical retrieval. While late 2nd-language learners appear to use iconicity as an aid to learning sign (R. Campbell, P. Martin, & T. White, 1992), they did not show the same facilitation effect as native ASL signers, suggesting that the task tapped into more automatic language processes. Overall, the findings suggest that completely arbitrary mappings between meaning and form may not be more advantageous in language and that, rather, arbitrariness may simply be an accident of modality.",
      "doi": "https://doi.org/10.1037/a0014547",
      "openalex_id": "https://openalex.org/W2108026270",
      "arxiv_id": "",
      "publication_date": "2009-03-01",
      "published": "2009-03-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Prevalence of Poor Reading in Dutch Special Elementary Education",
      "summary": "The relative frequency of poor readers in Dutch general elementary education (GEE) and special elementary education (SEE) and the characteristics of their reading performance were investigated using a lexical decision procedure. According to the same norms that identified 9% of students as poor readers in GEE, no less than 73% of the students in SEE were classified as poor readers. On average, the GEE poor readers were better readers than those in SEE, but the findings do not point to substantial differences in reading processes between the two reader groups. Hypotheses about the nature of the referral process that may cause this surprisingly strong relation between poor reading ability and SEE placement are advanced.",
      "abstract": "The relative frequency of poor readers in Dutch general elementary education (GEE) and special elementary education (SEE) and the characteristics of their reading performance were investigated using a lexical decision procedure. According to the same norms that identified 9% of students as poor readers in GEE, no less than 73% of the students in SEE were classified as poor readers. On average, the GEE poor readers were better readers than those in SEE, but the findings do not point to substantial differences in reading processes between the two reader groups. Hypotheses about the nature of the referral process that may cause this surprisingly strong relation between poor reading ability and SEE placement are advanced.",
      "doi": "https://doi.org/10.1177/00222194060390060101",
      "openalex_id": "https://openalex.org/W2106794954",
      "arxiv_id": "",
      "publication_date": "2006-11-01",
      "published": "2006-11-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Units in the analysis of signs",
      "summary": "The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on ‘spoken phonetics’.",
      "abstract": "The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on ‘spoken phonetics’.",
      "doi": "https://doi.org/10.1017/s095267570000004x",
      "openalex_id": "https://openalex.org/W2117522905",
      "arxiv_id": "",
      "publication_date": "1993-08-01",
      "published": "1993-08-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Cognitive Neuropsychological Approach to Assessment and Intervention in Aphasia",
      "summary": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. The opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention. This new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. The book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "doi": "https://doi.org/10.4324/9781315852447",
      "openalex_id": "https://openalex.org/W4254658607",
      "arxiv_id": "",
      "publication_date": "2014-01-03",
      "published": "2014-01-03",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Signs of Language",
      "summary": "Introduction PART I: The Two Faces of Sign 1. Iconicity in Signs and Signing 2. Properties of Symbols in a Silent Language 3. Historical Change: From Iconic to Arbitrary PART II: The Structure of the Sign 4. Remembering without Words: Manual Memory 5. Slips of the Hands 6. A Comparison of Chinese and American Signs 7. A Feature Analysis of Handshapes 8. The Rate of Speaking and Signing PART III: Grammatical Processes 9. On the Creation of New Lexical Items by Compounding 10. Linguistic Expression of Category Levels 11. Aspectual Modulations on Adjectival Predicates 12. The Structured Use of Space and Movement: Morphological Processes PART IV: The Heightened Use of Language 13. Wit and Plays on Signs 14. Poetry and Song in a Language without Sound Appendix A: Notation Appendix B: Conventions Employed in Illustrations Notes References Index",
      "abstract": "Introduction PART I: The Two Faces of Sign 1. Iconicity in Signs and Signing 2. Properties of Symbols in a Silent Language 3. Historical Change: From Iconic to Arbitrary PART II: The Structure of the Sign 4. Remembering without Words: Manual Memory 5. Slips of the Hands 6. A Comparison of Chinese and American Signs 7. A Feature Analysis of Handshapes 8. The Rate of Speaking and Signing PART III: Grammatical Processes 9. On the Creation of New Lexical Items by Compounding 10. Linguistic Expression of Category Levels 11. Aspectual Modulations on Adjectival Predicates 12. The Structured Use of Space and Movement: Morphological Processes PART IV: The Heightened Use of Language 13. Wit and Plays on Signs 14. Poetry and Song in a Language without Sound Appendix A: Notation Appendix B: Conventions Employed in Illustrations Notes References Index",
      "doi": "https://doi.org/10.2307/413507",
      "openalex_id": "https://openalex.org/W1980502471",
      "arxiv_id": "",
      "publication_date": "1980-12-01",
      "published": "1980-12-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Verbal and signed languages : comparing structures, constructs and methodologies",
      "summary": "This book is the first to explore how much of knowledge based on research on spoken languages needs to be refined in the light of the growing field of sign linguistics. Drawing upon a broad cross-linguistic perspective, the contributors focus on topics of general theoretical interest: linearity and arbitrariness principles, definition of units and levels of analysis, expression of grammatical categories, semantic relations, and cohesion mechanisms. The book is of interest to language typologists, theoretical and descriptive linguists, scholars in the fields of semiotics, anthropology, gesture studies, and cognitive sciences at large.",
      "abstract": "This book is the first to explore how much of knowledge based on research on spoken languages needs to be refined in the light of the growing field of sign linguistics. Drawing upon a broad cross-linguistic perspective, the contributors focus on topics of general theoretical interest: linearity and arbitrariness principles, definition of units and levels of analysis, expression of grammatical categories, semantic relations, and cohesion mechanisms. The book is of interest to language typologists, theoretical and descriptive linguists, scholars in the fields of semiotics, anthropology, gesture studies, and cognitive sciences at large.",
      "doi": "",
      "openalex_id": "https://openalex.org/W614258797",
      "arxiv_id": "",
      "publication_date": "2007-01-01",
      "published": "2007-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Phonological priming in British Sign Language",
      "summary": "Models of lexical access seek to explain how incoming language data is mapped onto long-term lexical representations. The experiment reported here aims to provide insight into which elements of language input are used for mapping onto a sign language lexicon. Rather than using the organs of the vocal tract, sign languages use the arms, hands, body and face to create meaning, combining handshapes, locations and movements to create meaningful words (signs). This study aims to determine whether these parameters are also used in lexical access processes. Twelve deaf native and twelve deaf non-native signers of British Sign Language (BSL) were presented with a primed lexical decision task. They were required to make a lexical decision about a target sign after viewing a preceding prime that was phonologically related to the target. Analysis of the data suggests that native signers use phonological information in signs in order to access their mental lexicon. Moreover, it appears that the salient parameter in the input is a combination of location and movement – only when both these parameters are shared by prime and target is facilitatory priming observed. There was no evidence that non-native (deaf) signers used phonological parameters to access their lexicon, despite a high degree of success in the lexical decision task at a speed comparable to that of native signers. These findings are discussed in relation to sign language acquisition and the development of phonological theories of signed languages.",
      "abstract": "Models of lexical access seek to explain how incoming language data is mapped onto long-term lexical representations. The experiment reported here aims to provide insight into which elements of language input are used for mapping onto a sign language lexicon. Rather than using the organs of the vocal tract, sign languages use the arms, hands, body and face to create meaning, combining handshapes, locations and movements to create meaningful words (signs). This study aims to determine whether these parameters are also used in lexical access processes. Twelve deaf native and twelve deaf non-native signers of British Sign Language (BSL) were presented with a primed lexical decision task. They were required to make a lexical decision about a target sign after viewing a preceding prime that was phonologically related to the target. Analysis of the data suggests that native signers use phonological information in signs in order to access their mental lexicon. Moreover, it appears that the salient parameter in the input is a combination of location and movement – only when both these parameters are shared by prime and target is facilitatory priming observed. There was no evidence that non-native (deaf) signers used phonological parameters to access their lexicon, despite a high degree of success in the lexical decision task at a speed comparable to that of native signers. These findings are discussed in relation to sign language acquisition and the development of phonological theories of signed languages.",
      "doi": "https://doi.org/10.1515/9783110197211.1.241",
      "openalex_id": "https://openalex.org/W129019996",
      "arxiv_id": "",
      "publication_date": "2006-06-20",
      "published": "2006-06-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A cognitive neuropsychological approach to assessment and intervention in aphasia: A clinician's guide",
      "summary": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. \n\nThe opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention.\n\nThis new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. \n\nThe book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "abstract": "This is a second edition of the highly popular volume used by clinicians and students in the assessment and intervention of aphasia. It provides both a theoretical and practical reference to cognitive neuropsychological approaches for speech-language pathologists and therapists working with people with aphasia. Having evolved from the activity of a group of clinicians working with aphasia, it interprets the theoretical literature as it relates to aphasia, identifying available assessments and published intervention studies, and draws together a complex literature for the practicing clinician. \n\nThe opening section of the book outlines the cognitive neuropsychological approach, and explains how it can be applied to assessment and interpretation of language processing impairments. Part 2 describes the deficits which can arise from impairments at different stages of language processing, and also provides an accessible guide to the use of assessment tools in identifying underlying impairments. The final part of the book provides systematic summaries of therapies reported in the literature, followed by a comprehensive synopsis of the current themes and issues confronting clinicians when drawing on cognitive neuropsychological theory in planning and evaluating intervention.\n\nThis new edition has been updated and expanded to include the assessment and treatment of verbs as well as nouns, presenting recently published assessments and intervention studies. It also includes a principled discussion on how to conduct robust evaluations of intervention within the clinical and research settings. \n\nThe book has been written by clinicians with hands-on experience. Like its predecessor, it will remain an invaluable resource for clinicians and students of speech-language pathology and related disciplines, in working with people with aphasia.",
      "doi": "https://doi.org/10.1080/02687030600742046",
      "openalex_id": "https://openalex.org/W1549850236",
      "arxiv_id": "",
      "publication_date": "2008-01-11",
      "published": "2008-01-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A Prosodic Model of Sign Language Phonology",
      "summary": "Superior to any other book on the subject that I have seen. I can see it being used as a class text or reference for current theory in sign language phonology. This book is intended in part to provide linguists and cognitive scientists who do not know sign language with a point of entry into the study of sign language phonology. At the same time, it presents a comprehensive theory of American Sign Language (ASL) phonology, while reviewing and building on alternative theories. One claim of this theoretical framework is that, because of sign language's visual/gestural phonetic basis, the consonant-like units and vowel-like units are expressed simultaneously with one another, rather than sequentially as in spoken languages. A second claim is that movements operate as the most basic prosodic units of the language. The author is concerned to show both the similarities and differences between signed and spoken languages, and to indicate some directions for future work in cognitive science that can be derived from her phonological model. Bradford Books imprint",
      "abstract": "Superior to any other book on the subject that I have seen. I can see it being used as a class text or reference for current theory in sign language phonology. This book is intended in part to provide linguists and cognitive scientists who do not know sign language with a point of entry into the study of sign language phonology. At the same time, it presents a comprehensive theory of American Sign Language (ASL) phonology, while reviewing and building on alternative theories. One claim of this theoretical framework is that, because of sign language's visual/gestural phonetic basis, the consonant-like units and vowel-like units are expressed simultaneously with one another, rather than sequentially as in spoken languages. A second claim is that movements operate as the most basic prosodic units of the language. The author is concerned to show both the similarities and differences between signed and spoken languages, and to indicate some directions for future work in cognitive science that can be derived from her phonological model. Bradford Books imprint",
      "doi": "https://doi.org/10.7551/mitpress/5644.001.0001",
      "openalex_id": "https://openalex.org/W1566439858",
      "arxiv_id": "",
      "publication_date": "1999-02-16",
      "published": "1999-02-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Access to the Internal Lexicon",
      "summary": "In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the \"no\" response latency, but not the \"yes\" response latency, and it is argued that this result favors the view that lexical access is \"direct,\" rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say \"no\" to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.",
      "abstract": "In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the \"no\" response latency, but not the \"yes\" response latency, and it is argued that this result favors the view that lexical access is \"direct,\" rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say \"no\" to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.",
      "doi": "https://doi.org/10.4324/9781003309734-29",
      "openalex_id": "https://openalex.org/W1519165391",
      "arxiv_id": "",
      "publication_date": "2022-05-19",
      "published": "2022-05-19",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
      "summary": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446933",
      "openalex_id": "https://openalex.org/W4392931281",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
      "summary": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094723",
      "openalex_id": "https://openalex.org/W4372259964",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
      "summary": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.872",
      "openalex_id": "https://openalex.org/W4385570550",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Looking to listen at the cocktail party",
      "summary": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "doi": "https://doi.org/10.1145/3197517.3201357",
      "openalex_id": "https://openalex.org/W4289665794",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "summary": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2826",
      "openalex_id": "https://openalex.org/W3095410713",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "summary": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "doi": "https://doi.org/10.5281/zenodo.1188976",
      "openalex_id": "https://openalex.org/W3081192838",
      "arxiv_id": "",
      "publication_date": "2018-04-05",
      "published": "2018-04-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
      "summary": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683277",
      "openalex_id": "https://openalex.org/W2935711438",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "summary": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "doi": "https://doi.org/10.1109/icassp.2009.4960497",
      "openalex_id": "https://openalex.org/W2107740512",
      "arxiv_id": "",
      "publication_date": "2009-04-01",
      "published": "2009-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "summary": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "doi": "https://doi.org/10.21437/interspeech.2022-439",
      "openalex_id": "https://openalex.org/W4225956675",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
      "summary": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "doi": "https://doi.org/10.1109/qomex48832.2020.9123150",
      "openalex_id": "https://openalex.org/W3037038648",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    }
  }
]